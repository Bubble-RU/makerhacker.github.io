<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2001-The Method of Quantum Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-185" href="#">nips2001-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2001-The Method of Quantum Clustering</h1>
<br/><p>Source: <a title="nips-2001-185-pdf" href="http://papers.nips.cc/paper/2083-the-method-of-quantum-clustering.pdf">pdf</a></p><p>Author: David Horn, Assaf Gottlieb</p><p>Abstract: We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scalespace probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ dinger equation of o which the probability function is a solution. This Schr¨ dinger equation o contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ dinger potential to the locations of data points, o we can apply this method to problems in high dimensions.</p><p>Reference: <a title="nips-2001-185-reference" href="../nips2001_reference/nips-2001-The_Method_of_Quantum_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scalespace probability function. [sent-2, score-0.117]
</p><p>2 The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ dinger equation of o which the probability function is a solution. [sent-3, score-0.445]
</p><p>3 This Schr¨ dinger equation o contains a potential function that can be derived analytically from the probability function. [sent-4, score-0.589]
</p><p>4 We associate minima of the potential with cluster centers. [sent-5, score-0.496]
</p><p>5 By limiting the evaluation of the Schr¨ dinger potential to the locations of data points, o we can apply this method to problems in high dimensions. [sent-8, score-0.696]
</p><p>6 1 Introduction Methods of data clustering are usually based on geometric or probabilistic considerations [1, 2, 3]. [sent-9, score-0.344]
</p><p>7 The problem of unsupervised learning of clusters based on locations of points in data-space, is in general ill deﬁned. [sent-10, score-0.201]
</p><p>8 Hence intuition based on other ﬁelds of study may be useful in formulating new heuristic procedures. [sent-11, score-0.028]
</p><p>9 The example of [4] shows how intuition derived from statistical mechanics leads to successful results. [sent-12, score-0.088]
</p><p>10 Here we propose a model based on tools that are borrowed from quantum mechanics. [sent-13, score-0.335]
</p><p>11 Using a Gaussian kernel, one generates from the data points in a Euclidean space of dimension a probability distribution given by, up to an overall normalization, the expression     ¡  (1)  1 ) 2) 0)(&$"! [sent-15, score-0.129]
</p><p>12 #     © § ¥£   ¥  where are the data points. [sent-17, score-0.044]
</p><p>13 It seems quite natural [5] to associate maxima of this function with cluster centers. [sent-18, score-0.22]
</p><p>14 ¥     Here we will also consider a Hilbert space, but, in contradistinction with kernel methods where the Hilbert space is implicit, here we work with a Schr¨ dinger equation that serves o as the basic framework of the Hilbert space. [sent-20, score-0.51]
</p><p>15 Its main emphasis is on the Schr¨ dinger potential, o whose minima will determine the cluster centers. [sent-22, score-0.764]
</p><p>16 This potential is part of the Schr¨ dinger o equation that is a solution of. [sent-23, score-0.589]
</p><p>17 ¢  2 The Schr¨ dinger Potential o We deﬁne[7] the Schr¨ dinger equation o  § ¦£ ¢ ¥  (2)   ©  $ © %¡ ©   ¥  ¥    ¥ ¡ § ¨©§ ¦¤££ ¢¢    § ¦£ ¢ § § (£ ¥ ¥  § (£ ¢ ¥  for which is a solution, or eigenstate. [sent-24, score-0.854]
</p><p>18 This quadratic function, whose center lies at , is known as the harmonic potential in quantum mechanics (see, e. [sent-27, score-0.512]
</p><p>19 Its eigenvalue is the lowest possible eigenvalue of , hence the Gaussian function is said to describe the ground state of . [sent-30, score-0.084]
</p><p>20 §  ©       § ¥¦¤¢ £ § ¦¤¢ ¥£ § ¦£  ¥  ¢  Conventionally, in quantum mechanics, one is given and one searches for solutions, or eigenfunctions, . [sent-32, score-0.308]
</p><p>21 Here, we have already , as determined by the data points, we ask therefore for the whose solution is the given . [sent-33, score-0.044]
</p><p>22 1 Crab Data To show the power of our new method we discuss the crab data set taken from Ripley’s book [9]. [sent-43, score-0.18]
</p><p>23 This data set is deﬁned over a ﬁve-dimensional parameter space. [sent-44, score-0.044]
</p><p>24 When analyzed in terms of the 2nd and 3rd principal components of the correlation matrix one observes a nice separation of the 200 instances into their four classes. [sent-45, score-0.301]
</p><p>25 1 we show the data as well as the Parzen probability distribution using the width parameter . [sent-48, score-0.106]
</p><p>26 It is quite obvious that this width is not small enough to deduce the correct clustering according to the approach of [5]. [sent-49, score-0.362]
</p><p>27 2 shows the required four minima for the same width parameter. [sent-51, score-0.3]
</p><p>28 One needs, however, the quantum clustering approach, to bring it out. [sent-53, score-0.608]
</p><p>29 A ( © B$  ©  § (£ ¢ ¥  ¥  D  C  1  (the Hamiltonian) and (potential energy) are conventional quantum mechanical operators, rescaled so that depends on one parameter, . [sent-54, score-0.383]
</p><p>30 F E  C  2 1  2  1  0  0 −1  −2  −1 −3  −2 PC3  PC2  Figure 1: A plot of Roberts’ probability distribution for Ripley’s crab data [9] as deﬁned over the 2nd and 3rd principal components of the correlation matrix. [sent-56, score-0.336]
</p><p>31 Using a Gaussian width of we observe only one maximum. [sent-57, score-0.062]
</p><p>32 2  2 0 2  1 1  0  0 −1 PC2  −2  −1 −3  −2 PC3  Figure 2: A plot of the Schr¨ dinger potential for the same problem as Fig. [sent-64, score-0.553]
</p><p>33 2 that the potential grows quadratically outside the domain over which the data are located. [sent-69, score-0.188]
</p><p>34 If the width is decreased more structure is to be expected. [sent-73, score-0.118]
</p><p>35 Thus, for , two more minima appear, as seen in Fig. [sent-74, score-0.194]
</p><p>36 Nonetheless, they lie high and contain only a few data points. [sent-76, score-0.133]
</p><p>37 2 Iris Data Our second example consists of the iris data set [10], which is a standard benchmark obtainable from the UCI repository [11]. [sent-80, score-0.308]
</p><p>38 Here we use the ﬁrst two principal components to deﬁne the two dimensions in which we apply our method. [sent-81, score-0.163]
</p><p>39 4, which shows the case for , provides an almost perfect separation of the 150 instances into the three classes into which they should belong. [sent-83, score-0.133]
</p><p>40 ©) 7    ¡  ©  ¥  4  Application of Quantum Clustering  The examples displayed in the previous section show that, if the spatial representation of the data allows for meaningful clustering using geometric information, quantum clustering (QC) will do the job. [sent-84, score-1.012]
</p><p>41 1 Varying  In the crabs-data we ﬁnd that as is decreased to , the previous minima of get deeper and two new minima are formed. [sent-90, score-0.471]
</p><p>42 However the latter are insigniﬁcant, in the sense that they lie at high values (of order ), as shown in Fig. [sent-91, score-0.089]
</p><p>43 Thus, if we classify data-points , roughly the same to clusters according to their topographic location on the surface of clustering assignment is expected for as for . [sent-93, score-0.408]
</p><p>44 By the way, the wave function acquires only one additional maximum at . [sent-94, score-0.06]
</p><p>45 As is being further decreased, more and more maxima are expected in and an ever increasing number of minima (limited by ) in . [sent-95, score-0.256]
</p><p>46 § ¦£ ¥    §¥    ¢  §    § ¥  ©§ © ¥    ¢  ¥       The one parameter of our problem, , signiﬁes the distance that we probe. [sent-96, score-0.037]
</p><p>47 Accordingly we expect to ﬁnd clusters relevant to proximity information of the same order of magnitude. [sent-97, score-0.077]
</p><p>48 One may therefore vary continuously and look for stability of cluster solutions, or limit oneself to relatively high values of and decide to stop the search once a few clusters are being uncovered. [sent-98, score-0.225]
</p><p>49 2 Higher Dimensions In the iris problem we obtained excellent clustering results using the ﬁrst two principal components, whereas in the crabs problem, clustering that depicts correctly the classiﬁcation necessitates components 2 and 3. [sent-100, score-0.933]
</p><p>50 However, once this is realized, it does not harm to add the 1st component. [sent-101, score-0.03]
</p><p>51 This requires working in a 3-dimensional space, spanned by the three leading PCs. [sent-102, score-0.034]
</p><p>52 Calculating on a ﬁne computational grid becomes a heavy task in high dimensions. [sent-103, score-0.03]
</p><p>53 To cut down complexity, we propose using the analytic expression of Eq. [sent-104, score-0.027]
</p><p>54 3 and evaluating the potential on data points only. [sent-105, score-0.243]
</p><p>55 This should be good enough to give a close estimate of where the minima lie, and it reduces the complexity to irrespective of dimension. [sent-106, score-0.194]
</p><p>56 In the gradient-descent algorithm described below, we will require further computations, also restricted to well deﬁned locations in space. [sent-107, score-0.069]
</p><p>57 2  2 1  0 2  0  1  0  −1  −1  −2  −2  −3  PC2  PC3  ©$  ©  ¥  Figure 3: The potential for the crab data with displays two additional, but insigniﬁcant, minima. [sent-113, score-0.324]
</p><p>58 The four deep minima are roughly at the same locations as in Fig. [sent-114, score-0.307]
</p><p>59 8  PC1  ©) 7    ¡  ©  ¥  Figure 4: Quantum clustering of the iris data for in a space spanned by the ﬁrst two principal components. [sent-128, score-0.679]
</p><p>60 Equipotential lines are drawn at  ) (   ¤ )   £ )   ¢ ) ¡© )    ©   $ § ¦¥£   ¢ ¡ ¥ £  ¥ £© ¡    ¢      © § (£ ¥   When restricted to the locations of data points, we evaluate on a discrete set of points . [sent-130, score-0.168]
</p><p>61 We can then express in terms of the distance matrix as   ©§) )% 21)¥   ¡ ¤ § ( ¥ ©   © ¡ £' ¨§) )% 1 )¥¦  ¡ §   ¡ ¤     ©         with  (6)  chosen appropriately so that min =0. [sent-131, score-0.037]
</p><p>62 All problems that we have used as examples were such that data were given in some space, and we have exercised our freedom to deﬁne a metric, using the PCA approach, as the basis for distance calculations. [sent-132, score-0.081]
</p><p>63 The previous analysis tells us that QC can also be applied to data for which only the distance information is known. [sent-133, score-0.124]
</p><p>64 3 Principal Component Metrics The QC algorithm starts from distance information. [sent-135, score-0.037]
</p><p>65 The question how the distances are calculated is another - very important - piece of the clustering procedure. [sent-136, score-0.3]
</p><p>66 The PCA approach deﬁnes a metric that is intrinsic to the data, determined by their second order statistics. [sent-137, score-0.053]
</p><p>67 Principal component decomposition can be applied both to the correlation matrix and to the covariance matrix. [sent-139, score-0.042]
</p><p>68 The PCA approach that we have used is based on a whitened correlation matrix. [sent-141, score-0.042]
</p><p>69 This turns out to lead to the good separation of crab-data in PC2-PC3 and of iris-data in PC1-PC2. [sent-142, score-0.041]
</p><p>70 Since our aim was to convince the reader that once a good metric is found, QC conveys the correct information, we have used the best preprocessing before testing QC. [sent-143, score-0.083]
</p><p>71 5 The Gradient Descent Algorithm After discovering the cluster centers we are faced with the problem of allocating the data points to the different clusters. [sent-144, score-0.244]
</p><p>72 We propose using a gradient descent algorithm for this purpose. [sent-145, score-0.057]
</p><p>73 Deﬁning we deﬁne the process (7)     § § £  £        § £   ¥ § 7 £ ©    £ § £   © §   £           letting the points reach an asymptotic ﬁxed value coinciding with a cluster center. [sent-146, score-0.203]
</p><p>74 To demonstrate the results of this algorithm, as well as the application of QC to higher dimensions, we analyze the iris data in 4 dimensions. [sent-148, score-0.233]
</p><p>75 We use the original data space with only one modiﬁcation: all axes are normalized to lie within a uniﬁed range of variation. [sent-149, score-0.195]
</p><p>76 Shown here are different windows for the four different axes, within which we display the values of the points after descending the potential surface and reaching its minima, whose values are shown in the ﬁfth window. [sent-152, score-0.274]
</p><p>77 Applying QC to data space without normalization of the different axes, leads to misclassiﬁcations of the order of 15 instances, similar to the clustering quality of [4]. [sent-154, score-0.41]
</p><p>78   6 Discussion In the literature of image analysis one often looks for the curve on which the Laplacian of the Gaussian ﬁlter of an image vanishes[13]. [sent-155, score-0.08]
</p><p>79 This is known as zero-crossing and serves as  dim 1  1. [sent-156, score-0.143]
</p><p>80 5  50  100  150  0  dim 2  0  50  100  150  0  50  100  150  0  50  100  150  1  dim 3  2 1 0  dim 4  2 1 0  V/E  0. [sent-158, score-0.324]
</p><p>81 1 0  20  40  60  80 serial number  100  120  140  Figure 5: The ﬁxed points of the four-dimensional iris problem following the gradientdescent algorithm. [sent-160, score-0.244]
</p><p>82 The results show almost perfect clustering into the three families of 50 instances each for . [sent-161, score-0.392]
</p><p>83 Clearly each such contour can also be viewed as surrounding maxima of the probability function, and therefore representing some kind of cluster boundary, although different from the conventional one [5]. [sent-164, score-0.24]
</p><p>84 Clearly they surround the data but do not give a satisfactory indication of where the clusters are. [sent-170, score-0.166]
</p><p>85 One may therefore speculate that equipotential levels of may serve as alternatives to curves in future applications to image analysis. [sent-172, score-0.142]
</p><p>86 Since the Schr¨ dinger potential, the function that plays the major role in our o analysis, has minima that lie in the neighborhood of data points, we ﬁnd that it sufﬁces to evaluate it at these points. [sent-176, score-0.736]
</p><p>87 This enables us to deal with clustering in high dimensional spaces. [sent-177, score-0.33]
</p><p>88 They show that the basic idea, as well as the gradient-descent algorithm of data allocation to clusters, work well. [sent-180, score-0.044]
</p><p>89 Quantum clustering does not presume any particular shape or any speciﬁc number of clusters. [sent-181, score-0.33]
</p><p>90 It can be used in conjunction with other clustering methods. [sent-182, score-0.3]
</p><p>91 This would be one example where not all points are given the same weight in the construction of the Parzen probability distribution. [sent-184, score-0.055]
</p><p>92 It may seem strange to see the Schr¨ dinger equation in the context of machine learning. [sent-185, score-0.445]
</p><p>93 The potential represents the attractive force that tries to concentrate  the distribution around its minima. [sent-188, score-0.144]
</p><p>94 The Laplacian has the opposite effect of spreading the wave-function. [sent-189, score-0.058]
</p><p>95 In a clustering analysis we implicitly assume that two such effects exist. [sent-190, score-0.3]
</p><p>96 Its success proves that this equation can o serve as the basic tool of a clustering method. [sent-192, score-0.37]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dinger', 0.409), ('schr', 0.409), ('quantum', 0.308), ('clustering', 0.3), ('qc', 0.213), ('minima', 0.194), ('iris', 0.189), ('potential', 0.144), ('crab', 0.136), ('cluster', 0.118), ('hilbert', 0.112), ('dim', 0.108), ('horn', 0.102), ('ripley', 0.089), ('principal', 0.082), ('clusters', 0.077), ('locations', 0.069), ('assaf', 0.068), ('aviv', 0.068), ('equipotential', 0.068), ('svc', 0.068), ('tel', 0.068), ('axes', 0.062), ('maxima', 0.062), ('width', 0.062), ('instances', 0.06), ('displayed', 0.06), ('mechanics', 0.06), ('insigni', 0.059), ('lie', 0.059), ('decreased', 0.056), ('de', 0.055), ('points', 0.055), ('parzen', 0.054), ('metric', 0.053), ('ne', 0.051), ('dimensions', 0.049), ('laplacian', 0.047), ('uci', 0.045), ('rescaled', 0.045), ('satisfactory', 0.045), ('repository', 0.045), ('data', 0.044), ('four', 0.044), ('misclassi', 0.043), ('tells', 0.043), ('emphasis', 0.043), ('pca', 0.042), ('correlation', 0.042), ('eigenvalue', 0.042), ('contours', 0.041), ('nonetheless', 0.041), ('separation', 0.041), ('image', 0.04), ('associate', 0.04), ('distance', 0.037), ('normalization', 0.036), ('equation', 0.036), ('pattern', 0.036), ('serves', 0.035), ('serve', 0.034), ('spanned', 0.034), ('perfect', 0.032), ('components', 0.032), ('surface', 0.031), ('opposite', 0.031), ('major', 0.03), ('space', 0.03), ('high', 0.03), ('presume', 0.03), ('convince', 0.03), ('hamiltonian', 0.03), ('crabs', 0.03), ('wave', 0.03), ('astronomy', 0.03), ('surrounding', 0.03), ('fth', 0.03), ('harm', 0.03), ('jain', 0.03), ('puts', 0.03), ('acquires', 0.03), ('obtainable', 0.03), ('coinciding', 0.03), ('conventional', 0.03), ('descent', 0.03), ('gaussian', 0.028), ('symbols', 0.028), ('intuition', 0.028), ('propose', 0.027), ('allocating', 0.027), ('answered', 0.027), ('spreading', 0.027), ('cliffs', 0.027), ('englewood', 0.027), ('siegelmann', 0.027), ('conventionally', 0.027), ('operators', 0.027), ('roberts', 0.027), ('deeper', 0.027), ('recipes', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="185-tfidf-1" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>Author: David Horn, Assaf Gottlieb</p><p>Abstract: We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scalespace probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ dinger equation of o which the probability function is a solution. This Schr¨ dinger equation o contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ dinger potential to the locations of data points, o we can apply this method to problems in high dimensions.</p><p>2 0.14452419 <a title="185-tfidf-2" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan, Yair Weiss</p><p>Abstract: Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1</p><p>3 0.13240677 <a title="185-tfidf-3" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<p>Author: Takashi Morie, Tomohiro Matsuura, Makoto Nagata, Atsushi Iwata</p><p>Abstract: This paper describes a clustering algorithm for vector quantizers using a “stochastic association model”. It offers a new simple and powerful softmax adaptation rule. The adaptation process is the same as the on-line K-means clustering method except for adding random ﬂuctuation in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efﬁcient adaptation as high as the “neural gas” algorithm, which is reported as one of the most efﬁcient clustering methods. It is a key to add uncorrelated random ﬂuctuation in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses ﬂuctuation in quantum mechanical tunneling processes.</p><p>4 0.13205059 <a title="185-tfidf-4" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>Author: Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, Horst D. Simon</p><p>Abstract: The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function. 1</p><p>5 0.086552076 <a title="185-tfidf-5" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>6 0.083104074 <a title="185-tfidf-6" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>7 0.078595817 <a title="185-tfidf-7" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>8 0.072113454 <a title="185-tfidf-8" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>9 0.072059259 <a title="185-tfidf-9" href="./nips-2001-Blind_Source_Separation_via_Multinode_Sparse_Representation.html">44 nips-2001-Blind Source Separation via Multinode Sparse Representation</a></p>
<p>10 0.070836499 <a title="185-tfidf-10" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>11 0.066851899 <a title="185-tfidf-11" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>12 0.065808393 <a title="185-tfidf-12" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>13 0.065669976 <a title="185-tfidf-13" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>14 0.064737387 <a title="185-tfidf-14" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>15 0.059759893 <a title="185-tfidf-15" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>16 0.059641536 <a title="185-tfidf-16" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>17 0.057960179 <a title="185-tfidf-17" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>18 0.057879988 <a title="185-tfidf-18" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>19 0.057725094 <a title="185-tfidf-19" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>20 0.053638931 <a title="185-tfidf-20" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.18), (1, 0.046), (2, -0.027), (3, -0.134), (4, 0.076), (5, -0.05), (6, -0.077), (7, 0.008), (8, 0.012), (9, -0.066), (10, 0.111), (11, -0.063), (12, -0.044), (13, 0.074), (14, -0.056), (15, 0.045), (16, 0.005), (17, -0.066), (18, 0.02), (19, -0.009), (20, 0.073), (21, -0.052), (22, 0.133), (23, -0.011), (24, 0.204), (25, 0.12), (26, 0.015), (27, 0.101), (28, -0.046), (29, 0.028), (30, -0.059), (31, -0.086), (32, -0.054), (33, 0.018), (34, -0.069), (35, -0.117), (36, 0.001), (37, 0.022), (38, 0.02), (39, 0.092), (40, -0.147), (41, 0.042), (42, 0.14), (43, 0.008), (44, 0.128), (45, 0.011), (46, 0.194), (47, 0.027), (48, -0.079), (49, -0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94682276 <a title="185-lsi-1" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>Author: David Horn, Assaf Gottlieb</p><p>Abstract: We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scalespace probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ dinger equation of o which the probability function is a solution. This Schr¨ dinger equation o contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ dinger potential to the locations of data points, o we can apply this method to problems in high dimensions.</p><p>2 0.74533653 <a title="185-lsi-2" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<p>Author: Takashi Morie, Tomohiro Matsuura, Makoto Nagata, Atsushi Iwata</p><p>Abstract: This paper describes a clustering algorithm for vector quantizers using a “stochastic association model”. It offers a new simple and powerful softmax adaptation rule. The adaptation process is the same as the on-line K-means clustering method except for adding random ﬂuctuation in the distortion error evaluation process. Simulation results demonstrate that the new algorithm can achieve efﬁcient adaptation as high as the “neural gas” algorithm, which is reported as one of the most efﬁcient clustering methods. It is a key to add uncorrelated random ﬂuctuation in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation is described by a single-electron circuit. It positively uses ﬂuctuation in quantum mechanical tunneling processes.</p><p>3 0.63966238 <a title="185-lsi-3" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>Author: Ran El-Yaniv, Oren Souroujon</p><p>Abstract: We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically ﬁnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiﬁcantly more accurate classiﬁcation. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the eﬀectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples. 1</p><p>4 0.63523585 <a title="185-lsi-4" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>Author: Wheeler Ruml</p><p>Abstract: If the promise of computational modeling is to be fully realized in higherlevel cognitive domains such as language processing, principled methods must be developed to construct the semantic representations used in such models. In this paper, we propose the use of an established formalism from mathematical psychology, additive clustering, as a means of automatically constructing binary representations for objects using only pairwise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large problems. We present a new algorithm for additive clustering, based on a novel heuristic technique for combinatorial optimization. The algorithm is simpler than previous formulations and makes fewer independence assumptions. Extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems. By making additive clustering practical, we take a signiﬁcant step toward scaling connectionist models beyond hand-coded examples.</p><p>5 0.59396732 <a title="185-lsi-5" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan, Yair Weiss</p><p>Abstract: Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1</p><p>6 0.58115399 <a title="185-lsi-6" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>7 0.42863998 <a title="185-lsi-7" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>8 0.3998239 <a title="185-lsi-8" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>9 0.37996033 <a title="185-lsi-9" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>10 0.3442142 <a title="185-lsi-10" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>11 0.33302623 <a title="185-lsi-11" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>12 0.33130035 <a title="185-lsi-12" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>13 0.32108662 <a title="185-lsi-13" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>14 0.31005168 <a title="185-lsi-14" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>15 0.30972201 <a title="185-lsi-15" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>16 0.30717549 <a title="185-lsi-16" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>17 0.30537978 <a title="185-lsi-17" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>18 0.30169669 <a title="185-lsi-18" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>19 0.29582271 <a title="185-lsi-19" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>20 0.29231349 <a title="185-lsi-20" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.043), (17, 0.034), (19, 0.037), (27, 0.116), (30, 0.097), (36, 0.012), (38, 0.036), (59, 0.03), (72, 0.091), (79, 0.027), (83, 0.02), (91, 0.127), (99, 0.25)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82139015 <a title="185-lda-1" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>Author: David Horn, Assaf Gottlieb</p><p>Abstract: We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scalespace probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schr¨ dinger equation of o which the probability function is a solution. This Schr¨ dinger equation o contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schr¨ dinger potential to the locations of data points, o we can apply this method to problems in high dimensions.</p><p>2 0.75624382 <a title="185-lda-2" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>Author: Ran El-Yaniv, Oren Souroujon</p><p>Abstract: We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically ﬁnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiﬁcantly more accurate classiﬁcation. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the eﬀectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples. 1</p><p>3 0.66815466 <a title="185-lda-3" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: Mário Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiﬁcation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>4 0.66468567 <a title="185-lda-4" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>5 0.66424125 <a title="185-lda-5" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>6 0.66227412 <a title="185-lda-6" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>7 0.65969789 <a title="185-lda-7" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>8 0.65807384 <a title="185-lda-8" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>9 0.65800941 <a title="185-lda-9" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>10 0.65461814 <a title="185-lda-10" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>11 0.65422869 <a title="185-lda-11" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>12 0.65292639 <a title="185-lda-12" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>13 0.65247184 <a title="185-lda-13" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>14 0.65198106 <a title="185-lda-14" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>15 0.65145618 <a title="185-lda-15" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>16 0.6508373 <a title="185-lda-16" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>17 0.65029651 <a title="185-lda-17" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>18 0.65012151 <a title="185-lda-18" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>19 0.64990807 <a title="185-lda-19" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>20 0.64932573 <a title="185-lda-20" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
