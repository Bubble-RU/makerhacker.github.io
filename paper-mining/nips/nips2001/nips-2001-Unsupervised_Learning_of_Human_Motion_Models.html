<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>193 nips-2001-Unsupervised Learning of Human Motion Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-193" href="#">nips2001-193</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>193 nips-2001-Unsupervised Learning of Human Motion Models</h1>
<br/><p>Source: <a title="nips-2001-193-pdf" href="http://papers.nips.cc/paper/2106-unsupervised-learning-of-human-motion-models.pdf">pdf</a></p><p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>Reference: <a title="nips-2001-193-reference" href="../nips2001_reference/nips-2001-Unsupervised_Learning_of_Human_Motion_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu    ¡  Abstract This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. [sent-3, score-0.898]
</p><p>2 The distinguished part of this work is that it is based on unlabeled data, i. [sent-4, score-0.184]
</p><p>3 , the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. [sent-6, score-1.13]
</p><p>4 We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. [sent-7, score-1.141]
</p><p>5 In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. [sent-8, score-0.304]
</p><p>6 A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. [sent-9, score-0.641]
</p><p>7 The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences. [sent-10, score-0.558]
</p><p>8 1 Introduction Human motion detection and labeling is a very important but difﬁcult problem in computer vision. [sent-11, score-0.426]
</p><p>9 Given a video sequence, we need to assign appropriate labels to the different regions of the image (labeling) and decide whether a person is in the image (detection). [sent-12, score-0.161]
</p><p>10 To detect and label a moving human body, a feature detector/tracker (such as corner detector) is ﬁrst run to obtain the candidate features from a pair of frames. [sent-14, score-0.355]
</p><p>11 The combination of features is then selected based on maximum likelihood by using the joint probability density function formed by the position and motion of the body. [sent-15, score-0.391]
</p><p>12 One key factor in the method is the probabilistic model of human motion. [sent-18, score-0.117]
</p><p>13 In order to avoid exponential combinatorial search, we use conditional independence property of body parts. [sent-19, score-0.249]
</p><p>14 target objects and background clutter with no identity attached to each feature. [sent-25, score-0.276]
</p><p>15 This case is interesting because the candidate features can be acquired automatically. [sent-26, score-0.185]
</p><p>16 Our algorithm leads to systems able to learn models of human motion completely automatically from real image sequences - unlabeled training features with clutter and occlusion. [sent-27, score-0.941]
</p><p>17 We restrict our attention to triangulated models, since they both account for much correlation between the random variables that represent the position and motion of each body part, and they yield efﬁcient algorithms. [sent-28, score-0.909]
</p><p>18 Our goal is to learn the best triangulated model, i. [sent-29, score-0.633]
</p><p>19 The distinguished part of this paper is that it is an unsupervised learning method based on unlabeled data, i. [sent-33, score-0.263]
</p><p>20 , the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. [sent-35, score-1.13]
</p><p>21 Although we work on triangulated models here, the unsupervised technique is not limited to this type of graph. [sent-36, score-0.631]
</p><p>22 In section 2 we summarize the main facts about the triangulated probability model. [sent-38, score-0.552]
</p><p>23 In section 3 we address the learning problem when the training features are labeled, i. [sent-39, score-0.11]
</p><p>24 , the parts of the model and the correspondence between the parts and observed features are known. [sent-41, score-0.566]
</p><p>25 In section 4 we address the learning problem when the training features are unlabeled. [sent-42, score-0.11]
</p><p>26 2 Decomposable triangulated graphs Discovering the probability structure (conditional independence) among variables is important since it makes efﬁcient learning and testing possible, hence some computationally intractable problems become tractable. [sent-44, score-0.665]
</p><p>27 The decomposable triangulated graph is another type of graph which has been demonstrated to be useful for biological motion detection and labeling [8, 1]. [sent-46, score-1.685]
</p><p>28 A decomposable triangulated graph [1] is a collection of cliques of size three, where there is an elimination order of vertices such that when a vertex is deleted, it is only contained in one triangle and the remaining subgraph is again a collection of triangles until only one triangle left. [sent-47, score-1.325]
</p><p>29 Decomposable triangulated graphs are more powerful than trees since each node can be thought of as having two parents. [sent-48, score-0.632]
</p><p>30 Conditional independence among random variables (parts) can be described by a decombe the set of parts, and posable triangulated graph. [sent-50, score-0.64]
</p><p>31 If the joint probability density function can be decomposed as a decomposable triangulated graph, it can  )  0 )     1#! [sent-52, score-0.962]
</p><p>32 We want to ﬁnd the decomposable triangulated graph , such that is maximized. [sent-61, score-1.075]
</p><p>33 is the probability of graph being the ’correct’ one given the observed data . [sent-62, score-0.146]
</p><p>34 Here we use to denote both the decomposable graph and the conditional (in)dependence depicted by the graph. [sent-63, score-0.566]
</p><p>35 From the previous section, a decomposable triangulated graph is represented by , then can be computed as follows,     8 D 7  ) S E U S C  Q  ) E ! [sent-65, score-1.075]
</p><p>36  DC  %¤9  3  Q    @  @ I@   (2)  w  1) I         R UE   where is differential entropy or conditional differential entropy [3] (we consider continuous random variables here). [sent-66, score-0.449]
</p><p>37 Equation (2) is an approximation which converges to equality for due to the weak Law of Large numbers and deﬁnitions and properties of differential entropy [3, 2, 4, 5, 6]. [sent-67, score-0.189]
</p><p>38 1 Greedy search Though for tree cases, the optimal structure can be obtained efﬁciently by the maximum spanning tree algorithm [2, 6], for decomposable triangulated graphs, there is no existing algorithm which runs in polynomial time and guarantees to the optimal solution [9]. [sent-70, score-1.108]
</p><p>39 We develop a greedy algorithm to grow the graph by the property of decomposable graphs. [sent-71, score-0.677]
</p><p>40 For each possible choice of (the last vertex of the last triangle), ﬁnd the best which , then get the best child of edge as , i. [sent-72, score-0.15]
</p><p>41 The next vertex is added one by one to the existing graph by choosing the best child of all the edges (legal parents) of the existing graph until all the vertices are added to the graph. [sent-75, score-0.426]
</p><p>42 For each choice of , one such graph can be grown, so there are candidate graphs. [sent-76, score-0.221]
</p><p>43 The ﬁnal result is the graph with the highest among the graphs. [sent-77, score-0.146]
</p><p>44 The algorithm is a greedy algorithm, with no guarantee that the global optimal solution could be found. [sent-80, score-0.154]
</p><p>45 2 Computation of differential entropy - translation invariance  R @ f 2 @ d G2E  R @ f 2 @ d 2 @ ` E2 g  In the greedy search algorithm, we need to compute and , . [sent-83, score-0.374]
</p><p>46 If we assume that they are jointly Gaussian, then the differential entropy can be computed by , where is the dimension and is the covariance matrix. [sent-84, score-0.223]
</p><p>47 }    s  R E" HI  x x} q k i yxh { z |$  7 F 7  a r v  In our applications, position and velocity are used as measurements for each body part, but humans can be present at different locations of the scene. [sent-85, score-0.22]
</p><p>48 , we can take one body part (for example ) as system for each triangle the origin, and use relative positions for other body parts. [sent-89, score-0.443]
</p><p>49       £    V    1    s  ¨ ©  In the greedy search algorithm, the differential entropy of all the possible triplets are needed and different triplets are with different origins. [sent-93, score-0.442]
</p><p>50 "  1 and  (4)  (  From the above equations, we can ﬁrst estimate the mean and covariance of (including all the body parts and without removing translation), then take the dimensions corresponding to the triangle and use equations (4) and (5) to get the mean and covariance for . [sent-97, score-0.483]
</p><p>51 }  )   R @ f 2 @ d 2 @ e E2 g  s W  s X s W  4 Unsupervised learning of the decomposable graph i 0 r2 ''%&%'% I 2 H 2 1   In this section, we consider the case when only unlabeled data are available. [sent-99, score-0.642]
</p><p>52 Each sample , , is a group of detected features which contains the target object, but is unlabeled, which means the correspondence between the candidate features and the parts of the object is unknown. [sent-101, score-0.712]
</p><p>53 For example when we run a feature detector (such as Lucas-Tomasi-Kanade detector [10]) on real image sequences, the detected features can be from target objects and background clutter with no identity attached to each feature. [sent-102, score-0.789]
</p><p>54 We want to select the useful composite parts of the object and learn the probability structure from . [sent-103, score-0.358]
</p><p>55 1 All foreground parts observed Here we ﬁrst assume that all the foreground parts are observed for each sample. [sent-105, score-0.63]
</p><p>56 If the labeling for each is taken as a hidden variable, then the EM algorithm can be used to learn the probability structure and parameters. [sent-106, score-0.363]
</p><p>57 Our method was developed from [11], but here we learn the probabilistic independence structure and all the candidate features are with the same type. [sent-107, score-0.371]
</p><p>58 If contains features, then is an -dimensional vector with each element taken a value from ( is the background clutter label). [sent-109, score-0.272]
</p><p>59 The observations for the EM algorithm are , the hidden variables are , and the parameters to optimize are the probability (in)dependence structure (i. [sent-110, score-0.152]
</p><p>60 the decomposable triangulated graph) and parameters for its associated probability density function. [sent-112, score-0.929]
</p><p>61 Under the labeling hypoth, is divided into the foreground features , which are parts of the esis object, and background (clutter) . [sent-119, score-0.667]
</p><p>62 If the foreground features are independent of , then, clutter  2  (9)  5q  )  P) I     G H%  2  2  xw q   GB%  G H%  P)   I #%"  P)   I  ! [sent-120, score-0.397]
</p><p>63 %          G H%  G B% )   P)   I %         1  5q Q  %$q R H g E  R w 5 R 5 FD E  )  1  2 5q   q  %    G H%         q  q   2  For simplicity, we will assume the priors are the same for different , and are the same for different graph structures. [sent-121, score-0.146]
</p><p>64 If we assume uniform background densities [11, 8], then , where is the volume of the space a background feature lies in, is the same for different . [sent-122, score-0.161]
</p><p>65 However, if there is one hypothesis labeling that is much better than other hypotheses,, i. [sent-126, score-0.221]
</p><p>66  4¤% C I 3¤% 9    3  £   #  %  5' @ q f 2 ' @5 5 q d 2 ' @5 q g 2   )  7 I 7   ¡ 1 5 q  5 ) 'q  where and are measurements corresponding to the best labeling . [sent-131, score-0.294]
</p><p>67 Comparing with equation (2) and also by the weak law of large numbers, we know for iteration , if the best hypothesis is used as the ’true’ labeling, then the decomposable triangulated graph structure can be obtained through the algorithm described in section 3. [sent-132, score-1.322]
</p><p>68 One approximation we make here is that the best hypothesis labeling for each is really dominant among all the possible labelings so that hard assignment for labelings can be used. [sent-133, score-0.384]
</p><p>69 2 Dealing with missing parts (occlusion) So far we assume that all the parts are observed. [sent-140, score-0.458]
</p><p>70 In the case of some parts missing, the measurements for the missing parts can be taken as additional hidden variables [11], and the EM algorithm can be modiﬁed to handle the missing parts. [sent-141, score-0.709]
</p><p>71 2  q£  2  q¢  For each hypothesis , let denote the measurements of the observed parts, be the measurements for the missing parts, and be the measurements of the whole object (to reduce clutter in the notation, we assume that the dimensions can be and to obtain sorted in this way). [sent-142, score-0.559]
</p><p>72 For each EM iteration, we need to compute the differential entropies and then with its parameters. [sent-143, score-0.184]
</p><p>73 %      ¢  %  (  Where  (12)  All the expectations are conditional expectations with respect to and decomposable graph structure . [sent-149, score-0.617]
</p><p>74 Therefore, are the measurements of the observed foreground parts under . [sent-150, score-0.389]
</p><p>75 Since is Gaussian distributed, conditional expectation and can be computed from observed parts and the mean and covariance matrix of . [sent-151, score-0.282]
</p><p>76 ' q¢  w 2 H s    R q £ E2    5 Experiments We tested the greedy algorithm on labeled motion capture data (Johansson displays) as in [8], and the EM-like algorithm on unlabeled detected features from real image sequences. [sent-152, score-0.808]
</p><p>77 1 Motion capture data Our motion capture data consist of the 3-D positions of 14 markers ﬁxed rigidly on a subject’s body. [sent-154, score-0.233]
</p><p>78 From the estimated mean and covariance, we can compute differential entropies for all the possible triplets and pairs and further run the greedy search algorithm to ﬁnd the approximated best triangulated model. [sent-157, score-1.06]
</p><p>79 Figure 2(a) shows the expected likelihood (differential entropy) of the estimated joint pdf, of the best triangulated model from the greedy algorithm, of the hand-constructed model from [8], and of randomly generated models. [sent-158, score-0.806]
</p><p>80 The greedy model is clearly superior to the hand-constructed model and the random models. [sent-159, score-0.111]
</p><p>81 The gap to the original joint pdf is partly due to the strong conditional independence assumptions of the triangulated model, which are an approximation of the true data’s pdf. [sent-160, score-0.731]
</p><p>82 Since these datasets were generated from 50 triangulated models, the greedy algorithm (solid  curve) can match the true model (dashed curve) extremely well. [sent-162, score-0.706]
</p><p>83 The solid line with error bars are the expected likelihoods of random triangulated models. [sent-163, score-0.552]
</p><p>84 2 Real image sequences There are three types of sequences used here: (I) a subject walks from left to right (Figure 3(a,b)); (II) a subject walks from right to left; (III) a subject rides a bike from left to right (Figure3(c,d)). [sent-166, score-0.333]
</p><p>85 Left-to-right walking motion models were learned from type I sequences and tested on all types of sequences to see if the learned model can detect left-to-right walking and label the body parts correctly. [sent-167, score-1.02]
</p><p>86 The candidate features were obtained from a Lucas-Tomasi-Kanade algorithm [10] on two frames. [sent-168, score-0.228]
</p><p>87 We used two frames to simulate the difﬁcult situation, where due to extreme body motion or to loose and textured clothing and occlusion, tracking is extremely unreliable and each feature’s lifetime is short. [sent-169, score-0.365]
</p><p>88 1, one approximation we made is taking the best hypothesis labeling instead of summing over all the possible hypotheses (equation (11)). [sent-172, score-0.266]
</p><p>89 Figure 4(c) shows the learned decomposable triangulated probabilistic structure. [sent-180, score-0.968]
</p><p>90 The red dots (with letter labels) are the maximum likelihood conﬁguration from the left-to-right walking model. [sent-183, score-0.31]
</p><p>91 The horizontal bar at the bottom left of each frame shows the likelihood of the best conﬁguration. [sent-184, score-0.155]
</p><p>92 The dots (either ﬁlled or empty) are the features selected by Tomasi-Kanade algorithm [10] on two frames. [sent-187, score-0.191]
</p><p>93 The ﬁlled dots (with letter labels) are the maximum likelihood conﬁguration from the left-to-right walking model. [sent-188, score-0.31]
</p><p>94 The horizontal bar at the bottom left of each frame shows the likelihood of the best conﬁguration. [sent-189, score-0.155]
</p><p>95 the likelihood is greater than the threshold, a left-to-right walking person is detected. [sent-195, score-0.232]
</p><p>96 The detection rate is 100% for the left-to-right walking vs. [sent-196, score-0.235]
</p><p>97 right-to-left walking, and 87% for the left-to-right walking vs. [sent-197, score-0.167]
</p><p>98 6 Conclusions In this paper we have described a method for learning the structure and parameters of a decomposable triangulated graph in an unsupervised fashion from unlabeled data. [sent-199, score-1.324]
</p><p>99 We have applied this method to learn models of biological motion that can be used to reliably detect and label biological motion. [sent-200, score-0.327]
</p><p>100 Perona, “Monocular perception of biological motion in johansson displays”, Computer Vision and Image Understanding, 81:303–327, 2001. [sent-238, score-0.266]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('triangulated', 0.552), ('decomposable', 0.377), ('parts', 0.205), ('motion', 0.183), ('clutter', 0.177), ('labeling', 0.175), ('walking', 0.167), ('graph', 0.146), ('body', 0.146), ('differential', 0.129), ('unlabeled', 0.119), ('greedy', 0.111), ('foreground', 0.11), ('features', 0.11), ('detected', 0.1), ('efd', 0.099), ('fd', 0.097), ('unsupervised', 0.079), ('human', 0.078), ('em', 0.077), ('candidate', 0.075), ('measurements', 0.074), ('perona', 0.072), ('detector', 0.072), ('detection', 0.068), ('background', 0.067), ('image', 0.066), ('object', 0.066), ('likelihood', 0.065), ('triangle', 0.064), ('entropy', 0.06), ('sequences', 0.06), ('independence', 0.06), ('vertex', 0.06), ('labelings', 0.059), ('entropies', 0.055), ('structure', 0.051), ('positions', 0.05), ('hr', 0.05), ('triplets', 0.05), ('missing', 0.048), ('correspondence', 0.046), ('trees', 0.046), ('hypothesis', 0.046), ('bar', 0.045), ('goncalves', 0.045), ('johansson', 0.045), ('weber', 0.045), ('best', 0.045), ('algorithm', 0.043), ('pdf', 0.043), ('song', 0.043), ('conditional', 0.043), ('search', 0.042), ('letter', 0.04), ('occlusion', 0.039), ('feng', 0.039), ('caltech', 0.039), ('chow', 0.039), ('probabilistic', 0.039), ('biological', 0.038), ('dots', 0.038), ('part', 0.037), ('dependence', 0.036), ('automatically', 0.036), ('frames', 0.036), ('tracker', 0.036), ('learn', 0.036), ('covariance', 0.034), ('graphs', 0.034), ('lled', 0.033), ('elimination', 0.033), ('walks', 0.033), ('real', 0.033), ('run', 0.033), ('joint', 0.033), ('ir', 0.032), ('translation', 0.032), ('detect', 0.032), ('fv', 0.032), ('attached', 0.032), ('gw', 0.032), ('iteration', 0.031), ('ef', 0.031), ('equation', 0.031), ('fc', 0.03), ('hidden', 0.03), ('con', 0.03), ('labels', 0.029), ('sh', 0.029), ('vertices', 0.029), ('qh', 0.029), ('nd', 0.029), ('taken', 0.028), ('variables', 0.028), ('distinguished', 0.028), ('guration', 0.028), ('subject', 0.027), ('feature', 0.027), ('yw', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="193-tfidf-1" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>2 0.15976356 <a title="193-tfidf-2" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm that induces a class of models with thin junction trees—models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efﬁcient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efﬁciently with the ﬁnal model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection.</p><p>3 0.1593339 <a title="193-tfidf-3" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>Author: Rómer Rosales, Stan Sclaroff</p><p>Abstract: A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion. 1</p><p>4 0.10140464 <a title="193-tfidf-4" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>Author: Sanjoy Dasgupta, Michael L. Littman, David A. McAllester</p><p>Abstract: The rule-based bootstrapping introduced by Yarowsky, and its cotraining variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justiﬁes both the use of conﬁdences — partial rules and partial labeling of the unlabeled data — and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for . £ ¡ ¤¢</p><p>5 0.096181862 <a title="193-tfidf-5" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>6 0.093192019 <a title="193-tfidf-6" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>7 0.08668752 <a title="193-tfidf-7" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>8 0.079301603 <a title="193-tfidf-8" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>9 0.076735042 <a title="193-tfidf-9" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>10 0.07507728 <a title="193-tfidf-10" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>11 0.074503481 <a title="193-tfidf-11" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>12 0.067252919 <a title="193-tfidf-12" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>13 0.063816302 <a title="193-tfidf-13" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>14 0.063402653 <a title="193-tfidf-14" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>15 0.062094793 <a title="193-tfidf-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.061570667 <a title="193-tfidf-16" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>17 0.061542816 <a title="193-tfidf-17" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>18 0.059990015 <a title="193-tfidf-18" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>19 0.058173981 <a title="193-tfidf-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.056830324 <a title="193-tfidf-20" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.201), (1, -0.005), (2, -0.031), (3, 0.012), (4, -0.097), (5, -0.083), (6, -0.186), (7, -0.006), (8, -0.03), (9, -0.027), (10, 0.022), (11, -0.015), (12, 0.101), (13, -0.012), (14, -0.061), (15, -0.113), (16, -0.092), (17, -0.009), (18, -0.08), (19, 0.024), (20, -0.059), (21, 0.1), (22, -0.152), (23, 0.011), (24, 0.059), (25, 0.019), (26, 0.116), (27, -0.065), (28, 0.002), (29, -0.002), (30, 0.095), (31, 0.119), (32, 0.21), (33, -0.036), (34, -0.036), (35, 0.094), (36, -0.035), (37, -0.079), (38, -0.074), (39, 0.038), (40, -0.106), (41, 0.086), (42, 0.126), (43, -0.109), (44, 0.134), (45, -0.162), (46, -0.05), (47, -0.119), (48, -0.063), (49, 0.146)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94968426 <a title="193-lsi-1" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>2 0.75774568 <a title="193-lsi-2" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>Author: Rómer Rosales, Stan Sclaroff</p><p>Abstract: A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion. 1</p><p>3 0.5624209 <a title="193-lsi-3" href="./nips-2001-Receptive_field_structure_of_flow_detectors_for_heading_perception.html">158 nips-2001-Receptive field structure of flow detectors for heading perception</a></p>
<p>Author: J. A. Beintema, M. Lappe, Alexander C. Berg</p><p>Abstract: Observer translation relative to the world creates image flow that expands from the observer's direction of translation (heading) from which the observer can recover heading direction. Yet, the image flow is often more complex, depending on rotation of the eye, scene layout and translation velocity. A number of models [1-4] have been proposed on how the human visual system extracts heading from flow in a neurophysiologic ally plausible way. These models represent heading by a set of neurons that respond to large image flow patterns and receive input from motion sensed at different image locations. We analysed these models to determine the exact receptive field of these heading detectors. We find most models predict that, contrary to widespread believe, the contribut ing motion sensors have a preferred motion directed circularly rather than radially around the detector's preferred heading. Moreover, the results suggest to look for more refined structure within the circular flow, such as bi-circularity or local motion-opponency.</p><p>4 0.53243089 <a title="193-lsi-4" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>Author: Francis R. Bach, Michael I. Jordan</p><p>Abstract: We present an algorithm that induces a class of models with thin junction trees—models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efﬁcient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efﬁciently with the ﬁnal model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection.</p><p>5 0.39532053 <a title="193-lsi-5" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>Author: Sanjoy Dasgupta, Michael L. Littman, David A. McAllester</p><p>Abstract: The rule-based bootstrapping introduced by Yarowsky, and its cotraining variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justiﬁes both the use of conﬁdences — partial rules and partial labeling of the unlabeled data — and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for . £ ¡ ¤¢</p><p>6 0.39225468 <a title="193-lsi-6" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>7 0.38013455 <a title="193-lsi-7" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>8 0.37905762 <a title="193-lsi-8" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>9 0.35844633 <a title="193-lsi-9" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>10 0.34774971 <a title="193-lsi-10" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>11 0.34090909 <a title="193-lsi-11" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>12 0.32615197 <a title="193-lsi-12" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>13 0.323228 <a title="193-lsi-13" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>14 0.32276776 <a title="193-lsi-14" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>15 0.31117043 <a title="193-lsi-15" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>16 0.3009319 <a title="193-lsi-16" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>17 0.30089423 <a title="193-lsi-17" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>18 0.30026394 <a title="193-lsi-18" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>19 0.29330656 <a title="193-lsi-19" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>20 0.29198965 <a title="193-lsi-20" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.033), (17, 0.018), (19, 0.027), (27, 0.123), (30, 0.1), (36, 0.257), (38, 0.041), (59, 0.051), (67, 0.01), (72, 0.058), (79, 0.033), (83, 0.019), (88, 0.017), (91, 0.142)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8999579 <a title="193-lda-1" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>same-paper 2 0.86101329 <a title="193-lda-2" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>3 0.84087771 <a title="193-lda-3" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>4 0.7290833 <a title="193-lda-4" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>5 0.66949117 <a title="193-lda-5" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>Author: Paul Viola, Michael Jones</p><p>Abstract: This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classiﬁers each trained to achieve high detection rates and modest false positive rates can yield a ﬁnal detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classiﬁers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields signiﬁcant improvements in performance over conventional AdaBoost. The ﬁnal face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000.</p><p>6 0.66518819 <a title="193-lda-6" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>7 0.66390723 <a title="193-lda-7" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>8 0.66072088 <a title="193-lda-8" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>9 0.65572059 <a title="193-lda-9" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>10 0.65536034 <a title="193-lda-10" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>11 0.65388203 <a title="193-lda-11" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>12 0.65238678 <a title="193-lda-12" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>13 0.64996243 <a title="193-lda-13" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>14 0.64923 <a title="193-lda-14" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>15 0.6459499 <a title="193-lda-15" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>16 0.64564615 <a title="193-lda-16" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>17 0.64474988 <a title="193-lda-17" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>18 0.64424908 <a title="193-lda-18" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>19 0.6441313 <a title="193-lda-19" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>20 0.6434561 <a title="193-lda-20" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
