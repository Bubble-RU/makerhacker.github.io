<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-81" href="#">nips2001-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</h1>
<br/><p>Source: <a title="nips-2001-81-pdf" href="http://papers.nips.cc/paper/2136-generalization-performance-of-some-learning-problems-in-hilbert-functional-spaces.pdf">pdf</a></p><p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>Reference: <a title="nips-2001-81-reference" href="../nips2001_reference/nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We investigate the generalization performance of some learning problems in Hilbert functional Spaces. [sent-5, score-0.257]
</p><p>2 We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. [sent-6, score-0.806]
</p><p>3 This estimate allows us to derive generalization bounds on some learning formulations. [sent-7, score-0.21]
</p><p>4 1 Introduction     In machine learning, our goal is often to predict an unobserved output value based on an observed input vector . [sent-8, score-0.105]
</p><p>5 This requires us to estimate a functional relationship from a set of example pairs of . [sent-9, score-0.129]
</p><p>6 Usually the quality of the predictor can be that is problem dependent. [sent-10, score-0.367]
</p><p>7 In machine learning, measured by a loss function we assume that the data are drawn from an underlying distribution which is not so that the expected true loss of given below is as small known. [sent-11, score-0.438]
</p><p>8 §¤¦ ¨ ¦ #¨ ¦  ¨ ¦ ¡ ¤ ¤  ¨ ¡  ¨ ¡    ¦ ¨ ¨    §§¦¦¡ ¤¦  ¡  In order to estimate a good predictor from a set of training data randomly drawn from , it is necessary to start with a model of the functional relationship. [sent-13, score-0.535]
</p><p>9 In this paper, we consider models that are subsets in some Hilbert functional spaces . [sent-14, score-0.17]
</p><p>10 Denote by the norm in , we consider models in the set , where is a parameter that can be used to control the size of the underlying model family. [sent-15, score-0.051]
</p><p>11 We would like to ﬁnd the best model in which is given by:  B    Y  (1)  By introducing a non-negative Lagrangian multiplier problem as:  , we may rewrite the above  (2)  We shall only consider this equivalent formulation in this paper. [sent-16, score-0.204]
</p><p>12 In addition, for technical reasons, we also assume that is a convex function of . [sent-17, score-0.151]
</p><p>13 ¦ £ b ¤ ¦ £ ¥ i uhs f e d # § F P9§ ©¡ ¨ ¨ £   ¦ b ¤ ¤¡ ¦ 8VV¡   u¢¡ ¦ £  AAA¨ ¡  , we consider the following estimation :  A  D V¨ ! [sent-19, score-0.154]
</p><p>14 ¤ C  C ¦  b¤  Given training examples method to approximate the optimal predictor  (3)     The goal of this paper is to show that as , in probability under appropriate regularity conditions. [sent-20, score-0.507]
</p><p>15 Furthermore, we obtain an estimate on the rate of convergence. [sent-21, score-0.11]
</p><p>16 Consequences of this result in some speciﬁc learning formulations are examined. [sent-22, score-0.136]
</p><p>17 2 Convergence of the estimated predictor  ¤  Assume that input belongs to a set . [sent-23, score-0.415]
</p><p>18 We make the reasonable assumption that is pointwise continuous under the topology: , where is in the sense that . [sent-24, score-0.087]
</p><p>19 The condition implies that each data point can be regarded as a bounded linear functional on such that : . [sent-26, score-0.411]
</p><p>20 For notational simplicity, we shall deﬁned as for all , where denotes the inner product of . [sent-28, score-0.107]
</p><p>21 C H) F# C ¤ D B G B ¦ C¤ # ¨ ©¡ ¦"¦ ¡ ¨ 4 2 s 2 Ed Uf wd # C ¤ D h e B ¡ §¤ )B B B P ¨ ¦ ¤ ©¡ ¤ # " ) B B P )B ¡  B ¤ )B B ¨ # ©¡ ¦ a¨ ) B B P ¤  B )B ¡ ¤ ¤¦  AP ©@9 ¡   &VC; $ C © D D C C " ¤ %¤ #¨ ¦  c©¡ §¤ ! [sent-32, score-0.041]
</p><p>22 s s rh0i P   ¡  ¡  8 ¨ 9© ( ¦¡ 6 ¤ ¡ 7542 s 31¡0¨2 )' " ¤¤ © ¤ ¦ #¤ "  It is clear that can be regarded as a representing feature vector of in . [sent-33, score-0.119]
</p><p>23 Since can now be considered as a linear functional using the feature space representation of , we can use the idea from [6] to analyze the convergence behavior of in . [sent-38, score-0.23]
</p><p>24 Following [6], using the linear representation of , we differentiate (2) at , which leads to the following ﬁrst order condition: the optimal solution (4)  where is the derivative of with respect to if is smooth; it denotes a subgradient (see [4]) otherwise. [sent-39, score-0.303]
</p><p>25 Since we have assumed that is a convex function . [sent-40, score-0.104]
</p><p>26 This implies the following of , we know that inequality:  ¡  ¡  §  ¡  . [sent-41, score-0.163]
</p><p>27 In (5), we have already bounded the convergence of to in terms to its of the convergence of the empirical expectation of a random vector mean. [sent-44, score-0.264]
</p><p>28 For example, if its variance can be bounded, then we may use the Chebyshev inequality to obtain a probability bound. [sent-46, score-0.477]
</p><p>29 In this paper, we are interested in obtaining an exponential probability bound. [sent-47, score-0.062]
</p><p>30 In order to do so, similar to the analysis in [6], we use the following form of concentration inequality which can be found in [5], page 95:  b¤   ) B ¨   @¨¡ ¦§b ¤ ¦ ¡Q  b¤ £  §  £  b¤ A¨ £  £ "¨ #4R  © '3¦ G     2   S¦ 0 D W 2 VC b ¤ $ £ b ¤ C ¦  ¨ D $   ¤ ! [sent-48, score-0.436]
</p><p>31 1 ([5]) Let be zero-mean independent random vectors in a Hilbert space If there exists such that for all natural numbers : Then for all : . [sent-52, score-0.049]
</p><p>32 We may now use the following form of Jensen’s inequality to bound the moments of the zero-mean random vector :  From inequality (5) and Theorem 2. [sent-55, score-0.866]
</p><p>33 2 If there exists  such that for all natural numbers . [sent-57, score-0.049]
</p><p>34 2 is quite general, the quantity and on the right hand side of the bound depend on the optimal predictor which requires to be estimated. [sent-59, score-0.67]
</p><p>35 In order to obtain a bound that does not require any knowledge of the true distribution , we may impose the following assumptions: both and are bounded. [sent-60, score-0.405]
</p><p>36 Observe that , we obtain the following result:    D B VC ) ! [sent-61, score-0.152]
</p><p>37 Also assume that the loss , then :  A ¨ @¨"24 £ 8 ¤ BA   ¨   ¦ ¡Q   Corollary 2. [sent-64, score-0.217]
</p><p>38 1 compares the performance of the computed function with that of the optimal predictor in (1). [sent-66, score-0.453]
</p><p>39 This style of analysis has been extensively used in the literature. [sent-67, score-0.172]
</p><p>40 In order to compare with their results, we can rewrite Theorem 3. [sent-69, score-0.163]
</p><p>41 1 in another form as: with probability of at least ,  B ¦F P b ¤ ¥  A¨ @    8 ¦  " 8£ b ¤ C 4R§¨   @©¡ §b ¤ ¦  3 w) % W ¨   @©¡ ¦ £ b ¤ ¦  3 w) %  DC    ¨ ¦ 1 ¨ 1 G ¨  £  8  3 G      $ 0 D 5 t 0 ( W ¡ §¤ ¡ ¨ 4 2 s 2 7D us 1)' S¦ ! [sent-70, score-0.137]
</p><p>42 Assume that , with probability of at least  , then , we have  b¤  It is clear that the right-hand side of the above inequality does not depend on the unobserved function . [sent-73, score-0.609]
</p><p>43 ¡ ¤ $ ¤¦ $ ¨   u¡ ¤¦  $ ¨    ¤¦   ¤ ¡ ¤    Using this inequality and (4), we obtain:     ¤  for all  A    £ W @$¨    ¤¦ ¡Q  $  £¡ $ T ££ W $$    T¤¤ $$ ¤¢$ $ ¨     T$$ ¤ $ ¤¦ ¡ £   $  It is clear that is continuous differentiable and not hard to check that and :  if if  and . [sent-78, score-0.366]
</p><p>44 It is also (6)  # U¨    ¤¦   We consider the following type of Huber’s robust loss function: 3. [sent-79, score-0.212]
</p><p>45 1 Regression  We study some consequences of Corollary 2. [sent-80, score-0.065]
</p><p>46 1, which bounds the convergence rate of the estimated predictor to the best predictor. [sent-81, score-0.638]
</p><p>47 Note that the constant in their depends on the pseudodimension, which can be inﬁnity for problems considered in this paper. [sent-83, score-0.04]
</p><p>48 It is possible to employ their analysis using some covering number bounds for general Hilbert spaces. [sent-84, score-0.246]
</p><p>49 However, such an analysis would have led to a result of the following form for our problems:  A ¢  @¨ ¡¡¡ ¨ 0q §  q $   0    ©  ¨ ¦ 1 ¨ 1 ¦ ¦ ¨   @©¡ b ¤ ¦  3 w) % W ¨   @©¡ ¦ £ b ¤ ¦  3 w) %  It is also interesting to compare Theorem 3. [sent-85, score-0.262]
</p><p>50 1 since the right hand side includes an extra term of . [sent-88, score-0.09]
</p><p>51 Using the analysis in this paper, we may obtain a similar result from (7) which leads to an average bound of the form:  It is clear that the term resulted in our paper is not as good as from [7]. [sent-89, score-0.564]
</p><p>52 However analysis in this paper leads to probability bounds while the leave-one-out analysis in [7] only gives average bounds. [sent-90, score-0.326]
</p><p>53 It is also worth mentioning that it is possible to reﬁne the analysis presented in this section to obtain a probability bound which when averaged, , rather than in the current analysis. [sent-91, score-0.485]
</p><p>54 gives a bound with the correct term of However due to the space limitation, we shall skip this more elaborated derivation. [sent-92, score-0.446]
</p><p>55 ¨ £¡ £ ¦ ©  ¨ £¡ £ ¦ ©  In addition to the above style bounds, it is also interesting to compare the generalization performance of the computed function to the empirical error of the computed function. [sent-93, score-0.275]
</p><p>56 Assume that , with probability of at least  §  8£ W C8 ©    §  §  Unlike Theorem 3. [sent-98, score-0.137]
</p><p>57 2 contains a term which relies on the unknown optimal predictor . [sent-100, score-0.446]
</p><p>58 1, we know that this term does not affect the performance of the estimated function when   ¨   ¨ ¡ b ¤ ¦   ¦  ¡ ¨  £  § ¡£  b¤  compared with the performance of . [sent-102, score-0.131]
</p><p>59 In order for us to compare with the bound in [1] obtained from an algorithmic stability point of view, we make the additional assumption for all . [sent-103, score-0.462]
</p><p>60 Note that this assumption is also required in [1]. [sent-104, score-0.043]
</p><p>61 2, we have with probability of at least      1 A    ¥ £ rq ¨    8 3  ¦8 ¦  £    £  8 W ¨   @¨ ¡ ¦ £ b ¤ ¦   ¤  § §  ¡ ¨   $ £ §¦ ¥  ¨   ¡ ¦ £ b ¤ ¦  3 w) % ¨ 1  3. [sent-106, score-0.137]
</p><p>62 Given a continu, we consider the following prediction rule: predict if , and ous model predict otherwise. [sent-108, score-0.189]
</p><p>63 In fact, even in many other popular methods, such as logistic regression and support vector machines, some kind of convex formulations have to be employed. [sent-110, score-0.237]
</p><p>64 In this case, denotes a subgradient rather than gradient since is non-smooth: at ; when and when . [sent-112, score-0.097]
</p><p>65 As a result, the original bound in their paper was in a form equivalent to the one we cite here with replaced by . [sent-114, score-0.252]
</p><p>66 Assume that , with probability of at least  ¨ 42 s2  We also have with probability of at least  ,  , we have  , then    C8 ©  ¡  § ¨$ ¥  We may obtain from Theorem 3. [sent-118, score-0.384]
</p><p>67 3 the following result: with probability of at least  ,  ¢  A   0q  ¤  £  §  §   8¨        8  ¢  £     0q   ¥ ¤  3  $  §  ¨ ¡ ¦ £ b¤¦  ¡ ¨   £ §¦ ¥  ©  W ¨   ¡ ¦ £ b ¤ ¦ © 3 2) % ¨ 1  It is interesting to compare this result with margin percentile style bounds from VC analysis. [sent-119, score-0.632]
</p><p>68 Clearly, this implies that our analysis has some advantages over VC analysis due to the fact that we directly analyze the numerical formulation of support vector classiﬁcation. [sent-123, score-0.178]
</p><p>69 4 Conclusion In this paper, we have introduced a notion of the convergence of the estimated predictor to the best underlying predictor for some learning problems in Hilbert spaces. [sent-124, score-0.974]
</p><p>70 We derived generalization bounds for some regression and classiﬁcation problems. [sent-126, score-0.254]
</p><p>71 We have shown that results from our analysis compare favorably with a number of earlier studies. [sent-127, score-0.189]
</p><p>72 This indicates that the concept introduced in this paper can lead to valuable insights into certain numerical formulations of learning problems. [sent-128, score-0.138]
</p><p>73 The importance of convexity in learning with squared loss. [sent-141, score-0.096]
</p><p>74 A leave-one-out cross validation bound for kernel methods with applications in learning. [sent-156, score-0.214]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predictor', 0.367), ('inequality', 0.305), ('theorem', 0.25), ('bound', 0.214), ('hilbert', 0.214), ('rq', 0.178), ('loss', 0.17), ('vc', 0.158), ('corollary', 0.131), ('functional', 0.129), ('dc', 0.124), ('hoeffding', 0.124), ('style', 0.122), ('bounds', 0.122), ('tb', 0.117), ('uhs', 0.112), ('tong', 0.111), ('obtain', 0.11), ('shall', 0.107), ('convex', 0.104), ('convergence', 0.101), ('classi', 0.099), ('subgradient', 0.097), ('formulations', 0.089), ('uts', 0.089), ('generalization', 0.088), ('condition', 0.084), ('implies', 0.078), ('least', 0.075), ('covering', 0.074), ('favorably', 0.074), ('consequences', 0.065), ('compare', 0.065), ('bounded', 0.062), ('probability', 0.062), ('princeton', 0.061), ('clear', 0.061), ('rewrite', 0.059), ('cation', 0.058), ('regarded', 0.058), ('separable', 0.058), ('led', 0.058), ('unobserved', 0.056), ('algorithmic', 0.055), ('margin', 0.053), ('squared', 0.052), ('underlying', 0.051), ('analysis', 0.05), ('side', 0.05), ('predict', 0.049), ('exists', 0.049), ('tzhang', 0.049), ('wee', 0.049), ('yorktown', 0.049), ('andr', 0.049), ('mentioning', 0.049), ('aaa', 0.049), ('chebyshev', 0.049), ('igf', 0.049), ('insights', 0.049), ('ous', 0.049), ('estimated', 0.048), ('result', 0.047), ('assume', 0.047), ('compares', 0.047), ('stability', 0.046), ('uh', 0.044), ('pointwise', 0.044), ('bousquet', 0.044), ('percentile', 0.044), ('convexity', 0.044), ('differentiate', 0.044), ('elaborated', 0.044), ('regression', 0.044), ('assumption', 0.043), ('know', 0.043), ('leads', 0.042), ('following', 0.042), ('together', 0.042), ('heights', 0.041), ('skip', 0.041), ('watson', 0.041), ('huber', 0.041), ('wd', 0.041), ('comparable', 0.041), ('spaces', 0.041), ('term', 0.04), ('problems', 0.04), ('optimal', 0.039), ('averaged', 0.039), ('regularity', 0.039), ('sun', 0.039), ('uf', 0.039), ('olivier', 0.039), ('vg', 0.039), ('nello', 0.039), ('volker', 0.039), ('order', 0.039), ('equivalent', 0.038), ('jensen', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="81-tfidf-1" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>2 0.37801287 <a title="81-tfidf-2" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>3 0.22489342 <a title="81-tfidf-3" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>4 0.1865111 <a title="81-tfidf-4" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>Author: John Langford, Rich Caruana</p><p>Abstract: We present a new approach to bounding the true error rate of a continuous valued classiﬁer based upon PAC-Bayes bounds. The method ﬁrst constructs a distribution over classiﬁers by determining how sensitive each parameter in the model is to noise. The true error rate of the stochastic classiﬁer found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artiﬁcial neural networks with results of a order of magnitude improvement vs. the best deterministic neural net bounds. £ ¡ ¤¢</p><p>5 0.17876296 <a title="81-tfidf-5" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>6 0.16327584 <a title="81-tfidf-6" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>7 0.14562941 <a title="81-tfidf-7" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>8 0.14292824 <a title="81-tfidf-8" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>9 0.14266935 <a title="81-tfidf-9" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>10 0.13492323 <a title="81-tfidf-10" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>11 0.12065418 <a title="81-tfidf-11" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>12 0.11695819 <a title="81-tfidf-12" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>13 0.11221624 <a title="81-tfidf-13" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>14 0.11149647 <a title="81-tfidf-14" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>15 0.10166986 <a title="81-tfidf-15" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>16 0.099584378 <a title="81-tfidf-16" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>17 0.097057685 <a title="81-tfidf-17" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>18 0.09544763 <a title="81-tfidf-18" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>19 0.091614142 <a title="81-tfidf-19" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>20 0.090352513 <a title="81-tfidf-20" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.283), (1, 0.155), (2, 0.057), (3, 0.277), (4, 0.182), (5, -0.149), (6, 0.066), (7, 0.08), (8, -0.028), (9, -0.099), (10, -0.064), (11, 0.189), (12, -0.012), (13, 0.051), (14, -0.13), (15, 0.137), (16, -0.007), (17, -0.034), (18, -0.049), (19, -0.02), (20, 0.027), (21, 0.062), (22, 0.056), (23, -0.063), (24, -0.065), (25, 0.051), (26, -0.057), (27, 0.074), (28, 0.016), (29, 0.0), (30, 0.015), (31, -0.054), (32, 0.041), (33, 0.083), (34, 0.031), (35, -0.077), (36, 0.048), (37, -0.045), (38, 0.01), (39, 0.036), (40, -0.088), (41, -0.021), (42, -0.032), (43, -0.09), (44, -0.004), (45, 0.05), (46, -0.12), (47, 0.111), (48, -0.197), (49, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97736448 <a title="81-lsi-1" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>2 0.85203302 <a title="81-lsi-2" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>3 0.67590445 <a title="81-lsi-3" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>4 0.66270143 <a title="81-lsi-4" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>Author: Sanjoy Dasgupta, Michael L. Littman, David A. McAllester</p><p>Abstract: The rule-based bootstrapping introduced by Yarowsky, and its cotraining variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justiﬁes both the use of conﬁdences — partial rules and partial labeling of the unlabeled data — and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for . £ ¡ ¤¢</p><p>5 0.65459359 <a title="81-lsi-5" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>6 0.6372503 <a title="81-lsi-6" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>7 0.62764895 <a title="81-lsi-7" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>8 0.57476616 <a title="81-lsi-8" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>9 0.55254024 <a title="81-lsi-9" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>10 0.55195397 <a title="81-lsi-10" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>11 0.52863079 <a title="81-lsi-11" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>12 0.44133425 <a title="81-lsi-12" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>13 0.42847934 <a title="81-lsi-13" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>14 0.422259 <a title="81-lsi-14" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>15 0.41707793 <a title="81-lsi-15" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>16 0.41400328 <a title="81-lsi-16" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>17 0.41145894 <a title="81-lsi-17" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>18 0.40273562 <a title="81-lsi-18" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>19 0.38881844 <a title="81-lsi-19" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>20 0.3736344 <a title="81-lsi-20" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.073), (17, 0.048), (19, 0.036), (27, 0.142), (30, 0.056), (36, 0.319), (59, 0.027), (72, 0.081), (79, 0.034), (83, 0.018), (91, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82882547 <a title="81-lda-1" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>same-paper 2 0.81491172 <a title="81-lda-2" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>3 0.75101012 <a title="81-lda-3" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>4 0.65431517 <a title="81-lda-4" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>5 0.57484955 <a title="81-lda-5" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>Author: Sanjoy Dasgupta, Michael L. Littman, David A. McAllester</p><p>Abstract: The rule-based bootstrapping introduced by Yarowsky, and its cotraining variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justiﬁes both the use of conﬁdences — partial rules and partial labeling of the unlabeled data — and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for . £ ¡ ¤¢</p><p>6 0.57294285 <a title="81-lda-6" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>7 0.57289135 <a title="81-lda-7" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>8 0.56683648 <a title="81-lda-8" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>9 0.55107963 <a title="81-lda-9" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>10 0.54495668 <a title="81-lda-10" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>11 0.54176331 <a title="81-lda-11" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>12 0.54105568 <a title="81-lda-12" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>13 0.53991508 <a title="81-lda-13" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>14 0.53804702 <a title="81-lda-14" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>15 0.53732461 <a title="81-lda-15" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>16 0.5345158 <a title="81-lda-16" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>17 0.53443468 <a title="81-lda-17" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>18 0.53369993 <a title="81-lda-18" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>19 0.53335047 <a title="81-lda-19" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>20 0.53243202 <a title="81-lda-20" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
