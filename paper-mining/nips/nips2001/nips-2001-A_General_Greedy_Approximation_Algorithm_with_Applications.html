<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2001-A General Greedy Approximation Algorithm with Applications</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-8" href="#">nips2001-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2001-A General Greedy Approximation Algorithm with Applications</h1>
<br/><p>Source: <a title="nips-2001-8-pdf" href="http://papers.nips.cc/paper/2051-a-general-greedy-approximation-algorithm-with-applications.pdf">pdf</a></p><p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>Reference: <a title="nips-2001-8-reference" href="../nips2001_reference/nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. [sent-5, score-0.424]
</p><p>2 In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. [sent-6, score-0.772]
</p><p>3 We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases. [sent-7, score-0.45]
</p><p>4 1 Introduction The goal of machine learning is to obtain a certain input/output functional relationship from a set of training examples. [sent-8, score-0.19]
</p><p>5 In order to do so, we need to start with a model of the functional relationship. [sent-9, score-0.125]
</p><p>6 In practice, it is often desirable to ﬁnd the simplest model that can explain the data. [sent-10, score-0.078]
</p><p>7 In addition, the philosophy of Occam’s Razor implies that the simplest solution is likely to be the best solution among all possible solutions, In this paper, we are interested in composite models that can be expressed as linear combinations of basic models. [sent-12, score-0.505]
</p><p>8 In this framework, it is natural to measure the simplicity of a composite model by the number of its basic model components. [sent-13, score-0.269]
</p><p>9 Since a composite model in our framework corresponds to a linear weight over the basic model space, therefore our measurement of model simplicity corresponds to the sparsity of the linear weight representation. [sent-14, score-0.593]
</p><p>10 In this paper, we are interested in achieving sparsity through a greedy optimization algorithm which we propose in the next section. [sent-15, score-0.56]
</p><p>11 This algorithm is closely related to a number of previous works. [sent-16, score-0.195]
</p><p>12 The basic idea was originated in [5], where Jones observed that if a target vector in a Hilbert space is a convex combination of a library of basic vectors, then using with basic library vecgreedy approximation, one can achieve an error rate of tors. [sent-17, score-1.049]
</p><p>13 The idea has been reﬁned in [1] to analyze the approximation property of sigmoidal functions including neural networks. [sent-18, score-0.249]
</p><p>14 §  © § ¥£¡ ¨¦¤¢   The above methods can be regarded as greedy sparse algorithms for functional approximation, which is the noise-free case of regression problems. [sent-19, score-0.982]
</p><p>15 A similar greedy algorithm can also be used to solve general regression problems under noisy conditions [6]. [sent-20, score-0.737]
</p><p>16 In addition to regression, greedy approximation can also be applied to classiﬁcation problems. [sent-21, score-0.512]
</p><p>17 The resulting algorithm is closely related to boosting [2] under the additive model point of view [3]. [sent-22, score-0.568]
</p><p>18 This paper shows how to generalize the method in [5, 1] for analyzing greedy algorithms (in their case, for functional approximation problems) and apply it to boosting. [sent-23, score-0.637]
</p><p>19 Our method can also be used to obtain sparse kernel representations for regression problems. [sent-25, score-0.528]
</p><p>20 Such a sparse representation is what support vector regression machines try to achieve. [sent-26, score-0.449]
</p><p>21 In this regard, the method given in this paper complements some recently proposed greedy kernel methods for Gaussian processes such as [9, 10]. [sent-27, score-0.425]
</p><p>22 The proposed greedy approximation method can also be applied to other prediction problems with different loss functions. [sent-28, score-0.739]
</p><p>23 For example, in density estimation, the goal is to ﬁnd a model that has the smallest negative log-likelihood. [sent-29, score-0.13]
</p><p>24 Similar approximation bounds can be directly obtained under the general framework proposed in this paper. [sent-31, score-0.322]
</p><p>25 Section 2 formalizes the general class of problems considered in this paper, and proposes a greedy algorithm to solve the formulation. [sent-33, score-0.546]
</p><p>26 The convergence rate of the algorithm is investigated in Section 3. [sent-34, score-0.202]
</p><p>27 2 General Algorithm In machine learning, our goal is often to predict an unobserved output value based on an observed input vector . [sent-37, score-0.078]
</p><p>28 This requires us to estimate a functional relationship from a set of example pairs of . [sent-38, score-0.125]
</p><p>29 Usually the quality of the predictor can be measured by a loss function that is problem dependent. [sent-39, score-0.286]
</p><p>30 This family of models can be regarded as additive models in statistics [4]. [sent-41, score-0.266]
</p><p>31 Formally, each basic model can be regarded as a vector in a linear functional space. [sent-42, score-0.358]
</p><p>32 Our problem in its most general form can thus be described as to ﬁnd a vector in the convex hull of to minimize a functional of that measures the quality of . [sent-43, score-0.642]
</p><p>33 This functional of plays the role of loss function for learning problems. [sent-44, score-0.311]
</p><p>34 © © ¡  More formally, we consider a linear vector space the convex hull of :  to denote the set of positive integers. [sent-47, score-0.377]
</p><p>35 We consider the following optimization problem on  In this paper, we assume that  :  is a differentiable convex function on  We propose the following algorithm to approximately solved (1). [sent-48, score-0.401]
</p><p>36 (1)   ¨ £   D " © w©  §¤£¡    ¦D ¨ © ©  £   ¤  D " ¤¥R  ©  a1 £ ¡  ¡ £ ( ¦ 0§  £  G  5 ¦ 4 ¢ ¦ £  § © 0 ¡ GV1 ¡   and  D  © ¨¡  let end  that minimize  © ¨¡  given for ﬁnd  D  For simplicity, we assume that the minimization of in Algorithm 2. [sent-51, score-0.046]
</p><p>37 However due to the space limitation, we shall not consider this generalization. [sent-54, score-0.056]
</p><p>38 For convenience, we introduce the following quantity  R  ! [sent-55, score-0.046]
</p><p>39 hg © ¡ ( 7© ¡ (       In the next section, we show that under appropriate regularity conditions, as , where is computed from Algorithm 2. [sent-57, score-0.115]
</p><p>40 In addition, the convergence rate can be bounded as . [sent-59, score-0.156]
</p><p>41 #§  3 Approximation bound Given any convex function , we have the following proposition, which is a direct consequence of the deﬁnition of convexity. [sent-61, score-0.343]
</p><p>42 In convex analysis, The gradient can be replaced by the concept of subgradient, which we do not consider in this paper for simplicity. [sent-62, score-0.267]
</p><p>43 1 Consider a convex function  , we have  The following lemma is the main theoretical result of the paper, which bounds the performance of each greedy solution step in Algorithm 2. [sent-65, score-0.92]
</p><p>44 For all vectors  E  , we have    ¦ ©  ¡ (   ¤ © )  , we have the following inequality   6©  )  7¡ 6¢  C   © ¡ ( &  y r  q 5 5   Ar 9 @  C  wDF¤£¡ ¡ (  pBi 'h" g ©C ¦  ) 7 8¤  © ¡ (   Proof. [sent-70, score-0.175]
</p><p>45 hg £ C £ ¢ ) y r q £ ( q ( ¤ h¡ i ¦ ©  ¡ ip'g ¡ C  © ¡ ©   C  w©D§£ ¡ ¡ ( p'hg  C ¦ 54 D 0 6V1 " §%@C  C© ¡  C  i wC §¤£¡ ¡ ( pC 'hg © ¦ C ¤  © ¡ ( ¦ © ¡ C  C   Now by setting the lemma. [sent-74, score-0.115]
</p><p>46 1, we obtain  in the above inequality, we obtain  Using the above lemma and note that , it is easy to obtain the following theorem by induction. [sent-77, score-0.446]
</p><p>47 1 approximately solves (1), and the rate of convergence for is given by  ©  ¡ (  £ Q §  )  ¤  )  , then we also have  ¥  ¦ G § )    ¨! [sent-82, score-0.164]
</p><p>48 We show that the general formulation considered in this paper includes some previous formulations as special cases. [sent-85, score-0.276]
</p><p>49 1 Regression  © ¡¡ ¤  so that the expected loss of  6 ¡ ¤ ©© ¡  ¦  In regression, we would like to approximate as   ¡     ! [sent-88, score-0.186]
</p><p>50 5  © © ¡ ¤ ¡ (    is small, where we use the squared loss for simplicity (this choice is obviously not crucial in our framework). [sent-89, score-0.24]
</p><p>51 is the expectation over and , which often corresponds to the empirical distribution of pairs. [sent-90, score-0.097]
</p><p>52 Given a set of basis functions with , we may consider the following regression formulation that is slightly different from (1):  " §' 6 ¡ © © ¨¦  ¡ ¤  #  B  ¦ ¥¤ £  # £ " ! [sent-93, score-0.257]
</p><p>53 ¤  where is a positive regularization parameter which is used to control the size of the weight vector . [sent-98, score-0.086]
</p><p>54 The above formulation can be readily converted into (1) by considering the following set of basic vectors:    ¡  6 © ¨¦ ¡ ¤   6 ¤ ¢ 41 0  3 ) R    R    #   ¡ ¦ ¤  d ©1  ¦ ¨¤ £ £¦ I © %¦ ¡ ¤ §8  0  ) in Algorithm 2. [sent-99, score-0.185]
</p><p>55 ¤ ¨¤   #   (  )  #  0  We may start with can be bounded as  This implies that the sparse solution in Algorithm 2. [sent-103, score-0.375]
</p><p>56 1, represented as weight and ( ), satisﬁes the following inequality:  §   6  " §%C ¡ © © ¨¦  C ¡ ¤ C # @ B    ¡ ! [sent-104, score-0.047]
</p><p>57 This leads to the original functional approximation results in [1, 5] and its generalization in [6]. [sent-107, score-0.265]
</p><p>58 The sparse regression algorithm studied in this section can also be applied to kernel methods. [sent-108, score-0.549]
</p><p>59 In this case, corresponds to the input training data space , and the basis . [sent-109, score-0.058]
</p><p>60 Clearly, this corresponds to a special case of predictors are of the form (2). [sent-110, score-0.242]
</p><p>61 A sparse kernel representation can be obtained easily from Algorithm 2. [sent-111, score-0.272]
</p><p>62 Our sparse kernel regression formulation is related to Gaussian processes, where greedy style algorithms have also been proposed [9, 10]. [sent-113, score-0.963]
</p><p>63 The bound given here is comparable to the bound given in [10] where a sparse approximation rate of the form was obtained. [sent-114, score-0.573]
</p><p>64 In fact, even in many other popular methods, such as logistic regression and support vector machines, some kind of convex formulations have to be employed. [sent-118, score-0.618]
</p><p>65 Although it is possible for us to analyze their formulations, in this section, we only consider the following form of loss that is closely related to Adaboost [2]:  ¦ H 1 © ¨¦ ¡ ¤ ¡ I P£ £ ¦ ¦ © ©   © ¡ ¡ ¤ ¤ (¦¡ 3 ¥£! [sent-119, score-0.295]
</p><p>66 Again, we consider a set of basis predictors , which are often called weak learners in the boosting literature. [sent-121, score-0.482]
</p><p>67 We would like to ﬁnd a strong learner as a convex combination of weak learners to approximately minimize the above loss:  © ¡¡ ¤  " ! [sent-122, score-0.47]
</p><p>68 (4)  (5)  This can be written as formulation (1) with  d£ ¤  ¡ 8 ¦ ¤ R I © ¨¦ ¡ ¤ ¦  0  Using simple algebra, it is easy to verify that  "§%C ¡ ¡ ¤   5 " i   ¢  6 © ¥¦¤ G §    © ¨¦  C ¡ ¤ C # @ B ¤ (¡ 3 ¦¢ ! [sent-127, score-0.115]
</p><p>69 hg ¤ ©   © %¦  ¡ ¤  # ¦ § ¦ G  ¦ £      # R    x t f'y6wvur  § 5 § ¤¢ ¤ ¦£! [sent-129, score-0.115]
</p><p>70 1 implies that the sparse solution represented as weight and ( ), satisﬁes the following inequality:  ,  (6)  £ Q §  #  . [sent-135, score-0.382]
</p><p>71 Now we consider the for all special situation that there exists such that (7)  0 R ©¨  " §%C ¡  ¨ ¢ Q   © %¦  C ¡ ¤ C # @ B   R 0 ¨ © ¨¦ ¡ ¡ £ ¤ " # " §%C ¡ ¤ ¤ 5 ¢  6© ¤ ¨ ¢ (¦¡ 3 ¥¢ ¤ ©   © ¨¦ C ¡ ¤ C # @ B ¤ (¦¡ 3 ¥¢ ! [sent-137, score-0.084]
</p><p>72 5    " pi 'hg    This condition will be satisﬁed in the large margin linearly separable case where there exists and such that and for all data ,    C  ¦ SQ C # R  Now, under (7), we obtain from (6) that    ! [sent-138, score-0.258]
</p><p>73 ¥ "£ ¥ © ¦ ¨ ¤ (¦¡ 3 ¦¢ ¤ © ¨ ¤ ¤  ¡§ ¨  ¤ " §'   ¡ © %¦  ¡ ¤  #  B ¡  ©  , we can choose   6© ¥ 6 ¤ G §  to obtain  " ! [sent-139, score-0.065]
</p><p>74  ¡ ¤  © ¢ ¥ ¥ © ¦ ¡§ 6 ¨ (¦¡ 3 ¥¢ ¤ © ¨ ¤   © ¨¦  ¡ ¤  #  B ¡ © ¥  £ Q §  Fix any  (8)  This implies that the misclassiﬁcation error rate decays exponentially. [sent-140, score-0.171]
</p><p>75 The exponential decay of misclassiﬁcation error is the original motivation of Adaboost [2]. [sent-141, score-0.174]
</p><p>76 Boosting was later  viewed as greedy approximation in the additive model framework [3]. [sent-142, score-0.715]
</p><p>77 From the learning theory perspective, the good generalization ability of boosting is related to its tendency to improve the misclassiﬁcation error under a positive margin [8]. [sent-143, score-0.41]
</p><p>78 From this point of view, inequality (8) gives a much more explicit margin error bound (which decreases exponentially) than a related result in [8]. [sent-144, score-0.431]
</p><p>79 In the framework of additive models, Adaboost corresponds to the exponential loss (3) analyzed in this section. [sent-145, score-0.513]
</p><p>80 As pointed out in [3], other loss functions can also be used. [sent-146, score-0.186]
</p><p>81 Using our analysis, we may also obtain sparse approximation bounds for these different loss functions. [sent-147, score-0.685]
</p><p>82 However, it is also easy to observe that they will not lead to the exponential decay of classiﬁcation error in the separable case. [sent-148, score-0.298]
</p><p>83 Although the exponential loss in (3) is attractive for separable problems due to the exponential decay of margin error, it is very sensitive to outliers in the non-separable case. [sent-149, score-0.576]
</p><p>84 We shall mention that an interesting aspect of boosting is the concept of adaptive resampling or sample reweighting. [sent-150, score-0.353]
</p><p>85 Although this idea has dominated the interpretation of boosting algorithms, it has been argued in [3] that adaptive resampling is only a computational by-product. [sent-151, score-0.347]
</p><p>86 The idea corresponds to a Newton step approximation in the sparse greedy solution of in Algorithm 2. [sent-152, score-0.888]
</p><p>87 1 under the additive model framework which we consider here. [sent-153, score-0.203]
</p><p>88 Our analysis further conﬁrmed that the greedy sparse solution of an additive model in (1), rather than reweighting itself is the key component in boosting. [sent-154, score-0.825]
</p><p>89 In our framework, it is also much easier to related the idea of boosting to the greedy function approximation method outlined in [1, 5]. [sent-155, score-0.854]
</p><p>90 3 Mixture density estimation In mixture density estimation, the output is the probability density function of the input vector at . [sent-157, score-0.415]
</p><p>91 The following negative log-likelihood is commonly used as loss function:  ¡     ¦ © © ¡ ¤ ¡ (  is a probability density function. [sent-158, score-0.316]
</p><p>92 ¦ © ¡¡ ¤ h      ¡ © %¦ ¡ ¤  R Q © ¡¡ ¤  where  Again, we consider a set of basis predictors , which are often called mixture comas a convex components. [sent-159, score-0.518]
</p><p>93 We would like to ﬁnd a mixture probability density model bination of mixture components to approximately minimize the negative log-likelihood:  © ¡¡ ¤  Q  R 7 # ¦ £   #  B " §! [sent-160, score-0.364]
</p><p>94 1 can be computed  6 %¦ 6 ¤ ¡  x t  5   6 ¡ 6 f'yGwvu r x 5  x  ¡  6 ©© %¦ " ¡¡ ¤   420  6 ©© ¡ ¡¡ "   3 1 3 1 420  ¥ ¦  £  ¥  ¡ £ ¢ ¤   ¡ ¢  )     An approximation bound can now be directly obtained from Theorem 3. [sent-165, score-0.216]
</p><p>95 5 Conclusion This paper studies a formalization of a general class of prediction problems in machine learning, where the goal is to approximate the best model as a convex combination of  a family of basic models. [sent-168, score-0.56]
</p><p>96 The quality of the approximation can be measured by a loss function which we want to minimize. [sent-169, score-0.373]
</p><p>97 We proposed a greedy algorithm to solve the problem, and we have shown that for a variety of loss functions, a convergence rate of can be achieved using a convex combination of basic models. [sent-170, score-1.184]
</p><p>98 We have illustrated the consequence of this general algorithm in regression, classiﬁcation and density estimation, and related the resulting algorithms to previous methods. [sent-171, score-0.284]
</p><p>99 Universal approximation bounds for superpositions of a sigmoidal function. [sent-175, score-0.274]
</p><p>100 A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training. [sent-201, score-0.914]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('greedy', 0.372), ('convex', 0.267), ('boosting', 0.23), ('sparse', 0.219), ('regression', 0.191), ('loss', 0.186), ('inequality', 0.175), ('lemma', 0.157), ('additive', 0.143), ('predictors', 0.142), ('approximation', 0.14), ('functional', 0.125), ('basic', 0.119), ('hg', 0.115), ('cation', 0.1), ('library', 0.097), ('spc', 0.097), ('composite', 0.096), ('adaboost', 0.091), ('misclassi', 0.091), ('density', 0.089), ('algorithm', 0.086), ('classi', 0.085), ('wee', 0.084), ('proposition', 0.079), ('formulations', 0.077), ('margin', 0.076), ('bound', 0.076), ('bounds', 0.075), ('regarded', 0.075), ('separable', 0.075), ('hull', 0.071), ('learners', 0.071), ('mixture', 0.07), ('resampling', 0.067), ('sun', 0.067), ('implies', 0.067), ('formulation', 0.066), ('exponential', 0.066), ('decay', 0.066), ('obtain', 0.065), ('tong', 0.064), ('rate', 0.062), ('related', 0.062), ('pc', 0.061), ('nd', 0.061), ('framework', 0.06), ('satis', 0.059), ('sigmoidal', 0.059), ('corresponds', 0.058), ('ar', 0.056), ('shall', 0.056), ('sparsity', 0.054), ('convergence', 0.054), ('simplicity', 0.054), ('kernel', 0.053), ('hilbert', 0.053), ('predictor', 0.053), ('sq', 0.053), ('hastie', 0.051), ('idea', 0.05), ('limitation', 0.05), ('solution', 0.049), ('easy', 0.049), ('freund', 0.048), ('interested', 0.048), ('family', 0.048), ('approximately', 0.048), ('weight', 0.047), ('closely', 0.047), ('general', 0.047), ('quality', 0.047), ('minimize', 0.046), ('robert', 0.046), ('quantity', 0.046), ('theorem', 0.045), ('peter', 0.044), ('logistic', 0.044), ('includes', 0.044), ('lee', 0.043), ('exists', 0.042), ('special', 0.042), ('error', 0.042), ('newton', 0.042), ('reweighting', 0.042), ('subgradient', 0.042), ('tzhang', 0.042), ('yorktown', 0.042), ('problems', 0.041), ('negative', 0.041), ('bounded', 0.04), ('often', 0.039), ('simplest', 0.039), ('estimation', 0.039), ('vector', 0.039), ('combination', 0.038), ('eighteenth', 0.038), ('occam', 0.038), ('philosophy', 0.038), ('yoav', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="8-tfidf-1" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>2 0.37801287 <a title="8-tfidf-2" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>3 0.26016915 <a title="8-tfidf-3" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>4 0.2216938 <a title="8-tfidf-4" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>5 0.2044199 <a title="8-tfidf-5" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>6 0.17969246 <a title="8-tfidf-6" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>7 0.17081137 <a title="8-tfidf-7" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>8 0.16259389 <a title="8-tfidf-8" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>9 0.15337233 <a title="8-tfidf-9" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>10 0.15264194 <a title="8-tfidf-10" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>11 0.15242842 <a title="8-tfidf-11" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>12 0.1490345 <a title="8-tfidf-12" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>13 0.12694982 <a title="8-tfidf-13" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>14 0.12661752 <a title="8-tfidf-14" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>15 0.12660585 <a title="8-tfidf-15" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>16 0.11589137 <a title="8-tfidf-16" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>17 0.11514585 <a title="8-tfidf-17" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>18 0.10810811 <a title="8-tfidf-18" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>19 0.10635613 <a title="8-tfidf-19" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>20 0.10567056 <a title="8-tfidf-20" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.343), (1, 0.179), (2, 0.062), (3, 0.324), (4, 0.158), (5, -0.144), (6, 0.071), (7, 0.031), (8, 0.03), (9, -0.16), (10, 0.003), (11, 0.182), (12, -0.004), (13, 0.003), (14, -0.054), (15, -0.003), (16, -0.045), (17, -0.048), (18, -0.085), (19, 0.152), (20, 0.144), (21, -0.01), (22, -0.025), (23, 0.031), (24, 0.03), (25, 0.057), (26, -0.013), (27, 0.053), (28, -0.023), (29, 0.015), (30, 0.073), (31, -0.041), (32, 0.007), (33, 0.069), (34, 0.048), (35, -0.013), (36, 0.071), (37, -0.066), (38, 0.009), (39, 0.021), (40, -0.074), (41, -0.077), (42, -0.069), (43, -0.102), (44, -0.034), (45, -0.027), (46, -0.127), (47, 0.084), (48, -0.107), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97451699 <a title="8-lsi-1" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>2 0.8415888 <a title="8-lsi-2" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>3 0.76725054 <a title="8-lsi-3" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>4 0.69993228 <a title="8-lsi-4" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>5 0.66463727 <a title="8-lsi-5" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>6 0.66165513 <a title="8-lsi-6" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>7 0.60097903 <a title="8-lsi-7" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>8 0.59657699 <a title="8-lsi-8" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>9 0.56811816 <a title="8-lsi-9" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>10 0.53814387 <a title="8-lsi-10" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>11 0.51401484 <a title="8-lsi-11" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>12 0.51385355 <a title="8-lsi-12" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>13 0.50258189 <a title="8-lsi-13" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>14 0.47957739 <a title="8-lsi-14" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>15 0.45392269 <a title="8-lsi-15" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>16 0.44558349 <a title="8-lsi-16" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>17 0.43786469 <a title="8-lsi-17" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>18 0.42817891 <a title="8-lsi-18" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>19 0.41610906 <a title="8-lsi-19" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>20 0.40990236 <a title="8-lsi-20" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.085), (17, 0.018), (19, 0.038), (27, 0.17), (30, 0.077), (36, 0.095), (38, 0.022), (59, 0.026), (72, 0.109), (79, 0.052), (83, 0.023), (91, 0.116), (92, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91700149 <a title="8-lda-1" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>2 0.9149729 <a title="8-lda-2" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>3 0.89468628 <a title="8-lda-3" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>Author: K. Wong, F. Li</p><p>Abstract: We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion. 1</p><p>4 0.87842625 <a title="8-lda-4" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>Author: Yang Song, Luis Goncalves, Pietro Perona</p><p>Abstract: This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.</p><p>5 0.85292697 <a title="8-lda-5" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>6 0.85208589 <a title="8-lda-6" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>7 0.84851378 <a title="8-lda-7" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>8 0.84671873 <a title="8-lda-8" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>9 0.84608042 <a title="8-lda-9" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>10 0.84032381 <a title="8-lda-10" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>11 0.83956534 <a title="8-lda-11" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>12 0.83560896 <a title="8-lda-12" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>13 0.83507895 <a title="8-lda-13" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>14 0.83506942 <a title="8-lda-14" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>15 0.83476663 <a title="8-lda-15" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>16 0.83381021 <a title="8-lda-16" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>17 0.82975078 <a title="8-lda-17" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>18 0.82965595 <a title="8-lda-18" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>19 0.82882106 <a title="8-lda-19" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>20 0.82721686 <a title="8-lda-20" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
