<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-125" href="#">nips2001-125</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</h1>
<br/><p>Source: <a title="nips-2001-125-pdf" href="http://papers.nips.cc/paper/1974-modularity-in-the-motor-system-decomposition-of-muscle-patterns-as-combinations-of-time-varying-synergies.pdf">pdf</a></p><p>Author: A. D'avella, M. C. Tresch</p><p>Abstract: The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a speciﬁc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques.</p><p>Reference: <a title="nips-2001-125-reference" href="../nips2001_reference/nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies  Andrea d’Avella and Matthew C. [sent-1, score-1.264]
</p><p>2 edu    ¡  Abstract The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. [sent-4, score-0.171]
</p><p>3 coordinated patterns of muscle activity, have been proposed as possible building blocks. [sent-7, score-0.402]
</p><p>4 Here we propose a model based on combinations of muscle synergies with a speciﬁc amplitude and temporal structure. [sent-8, score-1.219]
</p><p>5 Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. [sent-9, score-0.773]
</p><p>6 To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques. [sent-10, score-0.811]
</p><p>7 1 Introduction In order to produce movement, every vertebrate has to coordinate the large number of degrees of freedom in the musculoskeletal apparatus. [sent-11, score-0.015]
</p><p>8 How this coordination is accomplished by the central nervous system is a long standing question in the study of motor control. [sent-12, score-0.149]
</p><p>9 According to one common proposal, this task might be simpliﬁed by a modular organization of the neural systems controlling movement [1, 2, 3, 4]. [sent-13, score-0.137]
</p><p>10 In this scheme, speciﬁc output modules would control different but overlapping sets of degrees of freedom, thereby decreasing the number of variables controlled by the nervous system. [sent-14, score-0.062]
</p><p>11 By activating different output modules simultaneously but independently, the system may achieve the ﬂexibility necessary to control a variety of behaviors. [sent-15, score-0.037]
</p><p>12 Several studies have sought evidence for such a modular controller by examining the patterns of muscle activity during movement, in particular looking for the presence of muscle synergies. [sent-16, score-0.939]
</p><p>13 A muscle synergy is a functional unit coordinating the activity of a number of muscles. [sent-17, score-0.771]
</p><p>14 The simplest model for such a unit would be the synchronous activation of a set of muscles with a speciﬁc activity balance, i. [sent-18, score-0.329]
</p><p>15 Using techniques such as the correlation between pairs of muscles, these studies have generally failed to provide strong evidence in support of such units. [sent-21, score-0.017]
</p><p>16 However, using a new analysis that allows for simultaneous combinations of more than one synergy, our group has recently provided evidence in support of this basic hypothesis of the neural control of movement. [sent-22, score-0.1]
</p><p>17 We used a non-negative matrix factorization algorithm to examine the composition of muscle activation patterns in spinalized frogs [5, 6]. [sent-23, score-0.5]
</p><p>18 This algorithm, similarly to that developed independently by others [7], extracts a small number of non-negative 1 factors which can be combined to reconstruct a set of high-dimensional data. [sent-24, score-0.015]
</p><p>19 However, this analysis assumed that the muscle synergies consisted of a set of muscles which were activated synchronously. [sent-25, score-1.303]
</p><p>20 In examinations of complex behaviors produced by intact animals, it became clear that muscles within a putative synergy were often activated asynchronously. [sent-26, score-0.644]
</p><p>21 In these cases, although the temporal delay between muscles was nonzero, the dispersion around this delay was very small. [sent-27, score-0.381]
</p><p>22 These observations suggested that the basic units of motor production might involve not only a ﬁxed coordination of relative muscle activation amplitudes, but also a coordination of relative muscle activation timings. [sent-28, score-0.95]
</p><p>23 We therefore have developed a new algorithm to factorize muscle activation patterns produced during movement into combinations of such time-varying muscle synergies. [sent-29, score-0.911]
</p><p>24 2 Combinations of time-varying muscle synergies We model the output of the neural controller as a linear combination of muscle patterns with a speciﬁc time course in the activity of each muscle. [sent-30, score-1.566]
</p><p>25 In discrete time, we can represent each pattern, or time-varying synergy, as a sequence of vectors in muscle activity space. [sent-31, score-0.417]
</p><p>26 The data set which we consider here consists of episodes of a given behavior, e. [sent-32, score-0.051]
</p><p>27 In a particular episode , each synergy is scaled by an amplitude coefﬁcient and timeshifted by a delay . [sent-35, score-0.571]
</p><p>28 The sequence of muscle activity for that episode is then given by:      ©  (1)  ¦¢ §¥¤£¡    ¦  ¤$"#¤¥¢ ¡  © ! [sent-36, score-0.488]
</p><p>29 1 illustrates the model with an example of the construction of a muscle pattern by combinations of three synergies. [sent-38, score-0.425]
</p><p>30 Compared to the model based on combinations of synchronous muscle synergies this model has more parameters describing each synergy ( vs. [sent-39, score-1.509]
</p><p>31 , with muscles and maximum number of time steps in a synergy) but less overall parameters. [sent-40, score-0.233]
</p><p>32 In fact, with synchronous synergies there is a combination coefﬁcient for each time step and each synergy, whereas with time-varying synergies there are only two parameters ( and ) for each episode and each synergy. [sent-41, score-1.546]
</p><p>33 Synergy  1  1 2  1  3 4  Muscles  5  1  3  2  Synergy  2  2 3 4  4 5  5 10  1  20  30  40  3  Synergy  50  60  70  80  90  100  Time  2  C1  3  T1  4  C2 T2  5  C3 T3  Figure 1: An example of construction of a muscle pattern by the combinations of three time-varying synergies. [sent-44, score-0.425]
</p><p>34 In this example, each time-varying synergy (left) is constituted by a sequence of 50 activation levels in 5 muscles chosen as samples from Gaussian functions with different centers, widths, and amplitudes. [sent-45, score-0.62]
</p><p>35 To construct the muscle pattern (top right, shaded area), the activity levels of each synergy are ﬁrst scaled by an amplitude coefﬁcient ( , represented in the bottom right by the height of an horizontal bar) and shifted in time by a delay ( , represented by the position of the same bar). [sent-46, score-0.973]
</p><p>36 Then, at each time step, the scaled and shifted components (top right, broken lines) are summed together. [sent-47, score-0.06]
</p><p>37 ( EI       (    7 ¦ ¢ ¡ ¡ ¢  with  After initializing synergies and coefﬁcients to random positive values, we minimize the error by iterating the following steps:        0  and the scaling coefﬁcients , ﬁnd the 1. [sent-49, score-0.761]
</p><p>38 For each episode, given the synergies delays using a nested matching procedure based on the cross-correlation of the synergies with the data (see 3. [sent-50, score-1.549]
</p><p>39 ©  , update the scaling coef-        ¤    ¤  2. [sent-52, score-0.046]
</p><p>40 For each episode, given the synergies and the delays ﬁcients by gradient descent  Q P  ¦ §¥  "      ¨ ©  U  ©  Here and below, we enforce non-negativity by setting to zero any negative value. [sent-53, score-0.811]
</p><p>41 Given delays and scaling coefﬁcients, update the synergy elements by gradient descent  Q RP  "   $ " %#! [sent-55, score-0.496]
</p><p>42 1 Matching the synergy delays To ﬁnd the best delay of each synergy in each episode we use the following procedure: i. [sent-57, score-0.947]
</p><p>43 Compute the sum of the scalar products between the s -th data episode and the i -th synergy time-shifted by  ¤ ¦ §¤`" ¢ ¡ S ¦ ¢    §¥¤¢    ¦   ¤   (2)  ¡ ¢  ¡ ¢    or scalar product cross-correlation at delay , for all possible delays. [sent-58, score-0.531]
</p><p>44 Select the synergy and the delay with highest cross-correlation. [sent-60, score-0.442]
</p><p>45 Subtract from the data the selected synergy (after scaling and time-shifting by the selected delay). [sent-62, score-0.4]
</p><p>46 4 Results We tested the algorithm on simulated data in order to evaluate its performance and then applied it to EMG recordings from 13 hindlimb muscles of intact bullfrogs during several episodes of natural behaviors [8]. [sent-65, score-0.385]
</p><p>47 1 Simulated data We ﬁrst tested whether the algorithm could reconstruct known synergies and coefﬁcients from a dataset generated by those same synergies and coefﬁcients. [sent-67, score-1.43]
</p><p>48 The ﬁrst type was generated using a Gaussian function of different center, width, and amplitude for each muscle. [sent-69, score-0.052]
</p><p>49 The second type consisted of synergies generated by uniformly distributed random activities. [sent-70, score-0.715]
</p><p>50 For each type, we generated sets of three synergies involving ﬁve muscles with a duration of 15 time steps. [sent-71, score-0.964]
</p><p>51 Using these synergies, 50 episodes of duration 30 time steps were generated by scaling each synergy with random coefﬁcients and shifting it by random delays . [sent-72, score-0.621]
</p><p>52     ¤  ©  ¦ §¥¤¢    ¡  In ﬁgure 2 the results of a run with Gaussian synergies are shown. [sent-73, score-0.715]
</p><p>53 Generating and reconstructed synergy activations are shown side by side on the left, in gray scale. [sent-75, score-0.486]
</p><p>54 reconstructed scaling coefﬁcients and temporal delays are shown in the center and on the right respectively. [sent-77, score-0.292]
</p><p>55 Both synergies and coefﬁcients were accurately reconstructed by the algorithm. [sent-78, score-0.849]
</p><p>56 Q  7D  ¥ £ ¦¤  ¢   © § ¨§  97  Q  ¢  In table 1, a summary of the results from 10 runs with Gaussian and random synergies is presented. [sent-79, score-0.734]
</p><p>57 We used the maximum of the scalar product cross-correlation between two normalized synergies (see eq. [sent-80, score-0.732]
</p><p>58 We compared two sets of synergies by matching the pairs in each set with the highest similarity and computing the mean similarity ( ) between these pairs. [sent-82, score-0.798]
</p><p>59 All the synergy sets that we reconstructed ( ) had a high similarity with the generating set ( ). [sent-83, score-0.516]
</p><p>60 We also compared the generating and reconstructed scaling coefﬁcients using their correlation coefﬁcient , and delays by counting the number of delay coefﬁcients that were reconstructed correctly after compensating for possible lags in the synergies ( ). [sent-84, score-1.178]
</p><p>61 The match in scaling coefﬁcients and delays was in general very good. [sent-85, score-0.142]
</p><p>62 Only in a few runs with Gaussian synergies were the data correctly reconstructed (high ) but with synergies slightly different from the generating ones (as indicated by the lower ) and consequently not perfectly matching coefﬁcients (lower and ). [sent-86, score-1.612]
</p><p>63 Trec  Figure 2: An example of reconstruction of known synergies and coefﬁcients from simulated data. [sent-90, score-0.771]
</p><p>64 The ﬁrst column ( ) shows three time-varying synergies, generated from Gaussian functions, as three matrices each representing, in gray scale, the activity of 5 muscles (rows) over 15 time steps (columns). [sent-91, score-0.314]
</p><p>65 The second column ( ) shows the three synergies reconstructed by the algorithm: they accurately match the generating synergies (except for a temporal shift compensated by an opposite shift in the reconstructed delays). [sent-92, score-1.745]
</p><p>66 The third and fourth columns show scatter plots of generating vs. [sent-93, score-0.047]
</p><p>67 reconstructed scaling coefﬁcients and delays in 50 simulated episodes. [sent-94, score-0.275]
</p><p>68 Both sets of coefﬁcients are accurately reconstructed in almost all episodes. [sent-95, score-0.134]
</p><p>69 2 Time-varying muscle synergies in frog’s muscle patterns We then applied the algorithm to EMG recordings of a large set ( ) of hindlimb kicks, a defensive reﬂex that frogs use to remove noxious stimuli from the foot. [sent-98, score-1.557]
</p><p>70 Each kick consists of a fast extension followed by a slower ﬂexion to return the leg to a crouched posture. [sent-99, score-0.041]
</p><p>71 The trajectory of the foot varies with the location of the stimulation on the skin and, as a consequence, the set of kicks spans a wide range of the workspace of the frog. [sent-100, score-0.072]
</p><p>72 Correspondingly, across different episodes the muscle activity patterns in the 13 muscles that we recorded showed considerable amplitude and timing variations that we sought to explain by combinations of time-varying synergies. [sent-101, score-0.833]
</p><p>73 DDD      After rectifying and integrating the EMGs over 10 ms intervals, we performed the optimization procedure with sets of synergies, with . [sent-102, score-0.018]
</p><p>74 We chose the maximum duration of each synergy to be 20 time steps, i. [sent-103, score-0.407]
</p><p>75 200 ms, a duration larger than the duration of a typical muscle burst observed in this behavior. [sent-105, score-0.433]
</p><p>76 2591 Random synergies  3  Q       max median min  V  561 451 297  V  max median min  Gaussian synergies  Table 1: Comparison between generated and reconstructed synergies and coefﬁcients for 10 runs with Gaussian and random synergies. [sent-131, score-2.335]
</p><p>77 Q  In ﬁgure 3 the result of the extraction of four synergies with the highest is shown. [sent-133, score-0.731]
</p><p>78 The convergence criterion of a change in smaller than for 20 iterations was reached after 100 iterations with a ﬁnal . [sent-134, score-0.03]
</p><p>79 The synergies extracted in the other nine runs were in general very similar to this set, as indicated by a mean similarity ( ) ranging from to (median ) and a correlation between scaling coefﬁcients ranging from to (median ). [sent-135, score-0.836]
</p><p>80 In the case with the lowest similarity, only one synergy in the set shown in ﬁgure 3 was not properly matched. [sent-136, score-0.354]
</p><p>81 § ¨§  97    ¢    ¡£  Q  7 )D   ¤ £¢ ¢ ¢    97  Q  ¢  ¢  §  9 7¢  §  97  97  ©   §  ¤ ¨§ §  97  97  The four synergies captured the basic features of the muscle patterns observed during different kicks. [sent-137, score-1.117]
</p><p>82 The ﬁrst synergy, recruiting all the major knee extensor muscles (VI, RA, and VE), is highly activated in laterally directed kicks, as seen in the ﬁrst kick shown in ﬁgure 3, which involved a large knee extension. [sent-138, score-0.395]
</p><p>83 The second synergy, recruiting two large hip extensor muscles (RI and SM) and an ankle extensor muscle (GA), is highly activated in caudally and medially directed kicks, i. [sent-139, score-0.722]
</p><p>84 The third synergy involves a speciﬁc temporal sequencing of several muscles: BI and VE ﬁrst, followed by RI, SM, and GA, and then by AD and VI at the end. [sent-142, score-0.395]
</p><p>85 The fourth synergy has long activation proﬁles in many ﬂexor muscles, i. [sent-143, score-0.414]
</p><p>86 those involved in the return phase of the kick, with a speciﬁc temporal pattern (long activation of IP; BI and SA before TA). [sent-145, score-0.087]
</p><p>87 When this set of EMGs was reconstructed using different numbers of muscle synergies, we found that the synergies identiﬁed using N synergies were generally preserved in the synergies identiﬁed using N+1 synergies. [sent-146, score-2.613]
</p><p>88 For instance, the ﬁrst two synergies shown in ﬁgure 3 were seen in all sets of synergies, from to . [sent-147, score-0.715]
</p><p>89 Therefore, increasing the number of synergies allowed the data to be reconstructed more accurately (as seen by a higher ) but without a complete reorganization of the synergies. [sent-148, score-0.849]
</p><p>90 ¤      ¡      Q  ¢  5 Discussion The algorithm that we introduced here represents a new analytical tool for the investigation of the organization of the motor system. [sent-149, score-0.1]
</p><p>91 This algorithm is an extension of previous nonnegative matrix factorization procedures, providing a means of capturing structure in a set of data not only in the amplitude domain but also in the temporal domain. [sent-150, score-0.117]
</p><p>92 Such temporal structure is a natural description of motor systems where many behaviors are characterized by a particular temporal organization. [sent-151, score-0.203]
</p><p>93 The analysis applied to behaviors produced by the frog, as described here, was able to capture signiﬁcant physiologically relevant characteristics in the patterns of muscle activations. [sent-152, score-0.443]
</p><p>94 The motor system is not unique, however, in having structure in both amplitude and temporal domains and the techniques used here could easily be extended to other systems. [sent-153, score-0.173]
</p><p>95 Top right: the observed EMGs (thin line and shaded area) and their reconstruction (thick line) by combinations of the four synergies, scaled in amplitude ( ) and shifted in time ( ). [sent-156, score-0.214]
</p><p>96   (       Our model can be naturally extended to include temporal scaling of the synergies, i. [sent-157, score-0.087]
</p><p>97 allowing different durations of a synergy in different episodes. [sent-159, score-0.354]
</p><p>98 We will also address the issue of how to identify time-varying muscle synergies from continuous recordings of EMG patterns, without any manual segmentation into different episodes. [sent-161, score-1.099]
</p><p>99 Finally, future work will aim to the development of a probabilistic model to address the issue of the dimensionality of the synergy set in terms of Bayesian model selection [10]. [sent-163, score-0.354]
</p><p>100 Muscle synergies encoded within the spinal cord: evidence from focal intraspinal nmda iontophoresis in the frog. [sent-203, score-0.754]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('synergies', 0.715), ('muscle', 0.359), ('synergy', 0.354), ('muscles', 0.196), ('coef', 0.11), ('reconstructed', 0.109), ('cients', 0.097), ('delays', 0.096), ('motor', 0.08), ('sm', 0.073), ('delay', 0.072), ('kicks', 0.072), ('episode', 0.071), ('ra', 0.067), ('modular', 0.065), ('ga', 0.064), ('activity', 0.058), ('pe', 0.057), ('bi', 0.057), ('ta', 0.056), ('emgs', 0.055), ('ve', 0.055), ('ad', 0.053), ('sa', 0.053), ('amplitude', 0.052), ('movement', 0.052), ('combinations', 0.052), ('episodes', 0.051), ('ri', 0.05), ('ip', 0.046), ('scaling', 0.046), ('activation', 0.046), ('emg', 0.044), ('patterns', 0.043), ('extensor', 0.041), ('kick', 0.041), ('saltiel', 0.041), ('tresch', 0.041), ('temporal', 0.041), ('vi', 0.041), ('behaviors', 0.041), ('duration', 0.037), ('st', 0.034), ('activated', 0.033), ('reconstruction', 0.032), ('median', 0.031), ('generating', 0.031), ('coordination', 0.03), ('synchronous', 0.029), ('avella', 0.028), ('bizzi', 0.028), ('frog', 0.028), ('frogs', 0.028), ('hindlimb', 0.028), ('internus', 0.028), ('knee', 0.028), ('recruiting', 0.028), ('rectus', 0.028), ('vastus', 0.028), ('recordings', 0.025), ('nervous', 0.025), ('accurately', 0.025), ('factorization', 0.024), ('hip', 0.024), ('constituted', 0.024), ('lewicki', 0.024), ('simulated', 0.024), ('matching', 0.023), ('gray', 0.023), ('gure', 0.022), ('similarity', 0.022), ('scaled', 0.022), ('shifted', 0.022), ('sought', 0.022), ('spinal', 0.022), ('steps', 0.021), ('intact', 0.02), ('anterior', 0.02), ('modules', 0.02), ('organization', 0.02), ('runs', 0.019), ('speci', 0.019), ('shaded', 0.018), ('ms', 0.018), ('control', 0.017), ('scalar', 0.017), ('ranging', 0.017), ('evidence', 0.017), ('scatter', 0.016), ('rst', 0.016), ('time', 0.016), ('highest', 0.016), ('controller', 0.016), ('freedom', 0.015), ('factors', 0.015), ('iterations', 0.015), ('decomposition', 0.015), ('long', 0.014), ('construction', 0.014), ('simultaneous', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="125-tfidf-1" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>Author: A. D'avella, M. C. Tresch</p><p>Abstract: The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a speciﬁc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques.</p><p>2 0.10675156 <a title="125-tfidf-2" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>3 0.080563568 <a title="125-tfidf-3" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>4 0.059583135 <a title="125-tfidf-4" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Driven by the progress in the ﬁeld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming ﬁnger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100–230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiﬁcation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiﬁers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).</p><p>5 0.051278166 <a title="125-tfidf-5" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>Author: Yun Gao, Michael J. Black, Elie Bienenstock, Shy Shoham, John P. Donoghue</p><p>Abstract: Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural ﬁring conditioned on hand kinematics. We learn a nonparametric representation of this ﬁring activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned ﬁring models of multiple cells are used to deﬁne a nonGaussian likelihood term which is combined with a prior probability for the kinematics. A particle ﬁltering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear ﬁltering methods; the results suggest that it may be appropriate for neural prosthetic applications.</p><p>6 0.047241788 <a title="125-tfidf-6" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>7 0.042794362 <a title="125-tfidf-7" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>8 0.040064 <a title="125-tfidf-8" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>9 0.034624729 <a title="125-tfidf-9" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>10 0.030888049 <a title="125-tfidf-10" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>11 0.030161409 <a title="125-tfidf-11" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>12 0.028315289 <a title="125-tfidf-12" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>13 0.028285949 <a title="125-tfidf-13" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>14 0.027990356 <a title="125-tfidf-14" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>15 0.027818931 <a title="125-tfidf-15" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>16 0.026757887 <a title="125-tfidf-16" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>17 0.026676634 <a title="125-tfidf-17" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>18 0.026152976 <a title="125-tfidf-18" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>19 0.026038967 <a title="125-tfidf-19" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>20 0.025725208 <a title="125-tfidf-20" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.084), (1, -0.056), (2, -0.019), (3, 0.018), (4, -0.024), (5, -0.0), (6, -0.038), (7, 0.057), (8, 0.05), (9, 0.013), (10, -0.014), (11, -0.02), (12, -0.097), (13, -0.025), (14, -0.007), (15, 0.054), (16, -0.129), (17, 0.004), (18, 0.078), (19, 0.023), (20, 0.051), (21, 0.004), (22, 0.002), (23, 0.048), (24, -0.052), (25, -0.08), (26, 0.039), (27, 0.07), (28, -0.015), (29, -0.011), (30, 0.015), (31, -0.054), (32, 0.071), (33, 0.054), (34, -0.08), (35, 0.035), (36, -0.005), (37, 0.072), (38, 0.06), (39, -0.006), (40, 0.022), (41, 0.048), (42, 0.08), (43, -0.034), (44, -0.038), (45, 0.027), (46, 0.066), (47, 0.067), (48, 0.123), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94497049 <a title="125-lsi-1" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>Author: A. D'avella, M. C. Tresch</p><p>Abstract: The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a speciﬁc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques.</p><p>2 0.68181109 <a title="125-lsi-2" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>3 0.67315388 <a title="125-lsi-3" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>Author: O. Donchin, Reza Shadmehr</p><p>Abstract: Reaching movements require the brain to generate motor commands that rely on an internal model of the task’s dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reﬂect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we ﬁnd a surprisingly good ﬁt accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force ﬁeld, and in a random sequence of force ﬁelds where learning is not possible. Remarkably, we ﬁnd that the basis remains invariant. 1</p><p>4 0.43014368 <a title="125-lsi-4" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Gabriel Curio, Klaus-Robert Müller</p><p>Abstract: Driven by the progress in the ﬁeld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming ﬁnger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100–230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiﬁcation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiﬁers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).</p><p>5 0.38687679 <a title="125-lsi-5" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>6 0.35379615 <a title="125-lsi-6" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>7 0.33656836 <a title="125-lsi-7" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>8 0.29896456 <a title="125-lsi-8" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>9 0.28153402 <a title="125-lsi-9" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>10 0.27714694 <a title="125-lsi-10" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>11 0.25132525 <a title="125-lsi-11" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>12 0.2483891 <a title="125-lsi-12" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>13 0.24454713 <a title="125-lsi-13" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>14 0.23984306 <a title="125-lsi-14" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>15 0.23901615 <a title="125-lsi-15" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<p>16 0.22504196 <a title="125-lsi-16" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>17 0.21694472 <a title="125-lsi-17" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>18 0.21667199 <a title="125-lsi-18" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>19 0.21434368 <a title="125-lsi-19" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>20 0.21014652 <a title="125-lsi-20" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.015), (19, 0.021), (27, 0.081), (30, 0.113), (32, 0.324), (38, 0.031), (59, 0.018), (72, 0.035), (79, 0.051), (81, 0.016), (83, 0.054), (91, 0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81115335 <a title="125-lda-1" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>Author: A. D'avella, M. C. Tresch</p><p>Abstract: The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a speciﬁc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques.</p><p>2 0.64038187 <a title="125-lda-2" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>Author: Daniela Farias, Benjamin V. Roy</p><p>Abstract: The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large- scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach</p><p>3 0.50742424 <a title="125-lda-3" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>Author: Eran Segal, Daphne Koller, Dirk Ormoneit</p><p>Abstract: Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in “nearby” classes in the taxonomy are similar. In this paper, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a probabilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchical agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text.</p><p>4 0.5033806 <a title="125-lda-4" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>5 0.49917233 <a title="125-lda-5" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>6 0.49661934 <a title="125-lda-6" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>7 0.49638143 <a title="125-lda-7" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>8 0.49612728 <a title="125-lda-8" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>9 0.4944241 <a title="125-lda-9" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>10 0.49422726 <a title="125-lda-10" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>11 0.49213606 <a title="125-lda-11" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>12 0.49107853 <a title="125-lda-12" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>13 0.49106836 <a title="125-lda-13" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>14 0.4910602 <a title="125-lda-14" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>15 0.49097276 <a title="125-lda-15" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>16 0.48933169 <a title="125-lda-16" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>17 0.4885183 <a title="125-lda-17" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>18 0.48841345 <a title="125-lda-18" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>19 0.48810783 <a title="125-lda-19" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>20 0.48767352 <a title="125-lda-20" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
