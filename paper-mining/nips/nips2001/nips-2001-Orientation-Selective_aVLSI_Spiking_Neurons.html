<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-141" href="#">nips2001-141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</h1>
<br/><p>Source: <a title="nips-2001-141-pdf" href="http://papers.nips.cc/paper/2045-orientation-selective-avlsi-spiking-neurons.pdf">pdf</a></p><p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>Reference: <a title="nips-2001-141-reference" href="../nips2001_reference/nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. [sent-2, score-0.967]
</p><p>2 The circuit on this multi-neuron chip approximates a cortical microcircuit. [sent-3, score-0.453]
</p><p>3 The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. [sent-4, score-0.558]
</p><p>4 The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. [sent-5, score-0.481]
</p><p>5 We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. [sent-6, score-0.576]
</p><p>6 1 Introduction The sheer number of cortical neurons and the vast connectivity within the cortex are difﬁcult to duplicate in either hardware or software. [sent-9, score-0.492]
</p><p>7 Simulations of a network consisting of thousands of neurons with a connectivity that is representative of cortical neurons can take minutes to hours on a fast Pentium, particularly if spiking behavior is simulated. [sent-10, score-0.963]
</p><p>8 We have taken initial steps in mitigating the simulation time of neuronal networks by developing a multi-chip VLSI system that can support spike-based cortical processing models. [sent-12, score-0.176]
</p><p>9 The connectivity between neurons on different chips and between neurons on the same chip are reconﬁgurable. [sent-13, score-1.085]
</p><p>10 The receptive ﬁelds are effected by appropriate mapping of the spikes from source neurons to target neurons. [sent-14, score-0.649]
</p><p>11 In this work, we show how we synthesized orientation-tuned spiking neurons using the multi-chip system in Figure 1. [sent-16, score-0.506]
</p><p>12 The retina communicates through the AER protocol to the PIC when it has an active pixel. [sent-18, score-0.405]
</p><p>13 The PIC communicates with the multi-neuron chip if the retina address falls into one of its stored templates. [sent-19, score-0.729]
</p><p>14 The address of the active neuron on this array can also be communicated off-chip to another receiver/transceiver. [sent-21, score-0.366]
</p><p>15 the retina to the target neurons on the multi-neuron transceiver chip is achieved with a PIC microcontroller and an asynchronous event-driven communication protocol. [sent-22, score-1.369]
</p><p>16 The circuit on this multi-neuron chip approximates a cortical microcircuit (Douglas and Martin, 1991). [sent-23, score-0.489]
</p><p>17 We explored different models that have been proposed for the generation of orientation tuning in neurons of the V1 cortical area. [sent-24, score-0.634]
</p><p>18 There have been earlier attempts to use multichip systems for creating orientation-selective neurons (Boahen et al. [sent-25, score-0.368]
</p><p>19 Visual cortical neurons receive inputs from the lateral geniculate nucleus (LGN) neurons which are not orientation-selective. [sent-31, score-0.822]
</p><p>20 Models for the emergence of orientation-selectivity in cortical neurons can be divided into two groups; feedforward models and feedback models. [sent-32, score-0.627]
</p><p>21 In a feedforward model, the orientation selectivity of a cortical neuron is conferred by the spatial alignment of the LGN neurons that are presynaptic to the cortical neuron (Hubel and Wiesel, 1962). [sent-33, score-1.265]
</p><p>22 In a feedback model, a weak orientation bias provided by the LGN input is sharpened by the intracortical excitatory and/or inhibitory feedback (Somers et al. [sent-34, score-0.55]
</p><p>23 In this work, we quantify the tuning curves of neurons created using a feedforward model and a feedback model with global inhibition. [sent-38, score-0.685]
</p><p>24 2 System Architecture The multi-chip system (Figure 1) in this work consists of a 16 16 silicon ON/OFF retina, a PIC microcontroller, and a transceiver chip with a ring of 16 integrate-and-ﬁre neurons and a global inhibitory neuron. [sent-39, score-1.117]
</p><p>25 The PIC and the multi-neuron chip are both transceivers: They can both receive events and send events (Liu et al. [sent-43, score-0.308]
</p><p>26 The retina with an on-chip arbiter can only send events. [sent-45, score-0.475]
</p><p>27 Each pixel is composed of an adaptive photoreceptor that has a rectifying temporal differentiator (Kramer, 2001) in its feedback loop as shown in Figure 2. [sent-46, score-0.223]
</p><p>28 The outputs are then coded in the form of asynchronous binary pulses by two neurons within the pixel. [sent-48, score-0.483]
</p><p>29 These asynchronous pulses  Arbiter ON REQ  ON ACK  neuron ON M1  OFF REQ  OFF ACK  neuron OFF  bias  M3  temporal differentiator  M2  Figure 2: Pixel of the transient imager. [sent-49, score-0.699]
</p><p>30 The circuit contains a photodiode with a transistor in a a source-follower conﬁguration with a high-gain inverting ampliﬁer ( , ) in a negative feedback loop. [sent-50, score-0.264]
</p><p>31 A rectifying temporal differentiator in the feedback loop extracts transient ON and OFF signals. [sent-51, score-0.216]
</p><p>32 These signals go to individual neurons that generate the REQ signals to the arbiter. [sent-52, score-0.368]
</p><p>33 The duration of the ACK signal from the X-arbiter is extended within the pixel by a global refractory bias. [sent-54, score-0.2]
</p><p>34 A global parameter sets the minimum time (or refractory period) between subsequent pulses from the same output. [sent-57, score-0.2]
</p><p>35 The position of a pixel is encoded with a 4-bit column address (X address) and a 4-bit row address (Y address) as shown in Figure 3. [sent-60, score-0.265]
</p><p>36 An active neuron makes a request to the on-chip arbiter. [sent-61, score-0.271]
</p><p>37 If the neuron is selected by the arbiter, then the X and Y addresses which code the location of this neuron are placed on the output address bus of the chip. [sent-62, score-0.695]
</p><p>38 The multi-neuron chip has an on-chip address decoder for the incoming events and an onchip arbiter to send events. [sent-64, score-0.516]
</p><p>39 The X address to the chip codes the identity of the neuron and the Y address codes the input synapse used to stimulate the neuron. [sent-65, score-0.795]
</p><p>40 Each neuron can be stimulated externally through an excitatory synapse or an inhibitory synapse. [sent-66, score-0.586]
</p><p>41 The excitatory neurons of this array are mutually connected via hard-wired excitatory synapses. [sent-67, score-0.648]
</p><p>42 These excitatory neurons also excite a global inhibitory neuron which in turn inhibits all the excitatory neurons. [sent-68, score-1.001]
</p><p>43 The membrane potentials of the neurons can be monitored by an on-chip scanner and the output spikes of the neurons can be monitored by the chip’s AER output. [sent-69, score-0.978]
</p><p>44 In this work, the excitatory neurons on the multi-neuron chip model the orientation tuning properties of simple cells in the visual cortex and the global inhibitory neuron models an inhibitory interneuron in the visual cortex. [sent-71, score-1.418]
</p><p>45 The receptive ﬁelds of the neurons are created by conﬁguring the connections from a subset of the source pixels on the retina onto the appropriate target neurons on the multi-neuron transceiver chip through a PIC 16C74 microcontroller. [sent-72, score-1.782]
</p><p>46 The subsets of retina pixels are determined by user-supplied templates. [sent-73, score-0.416]
</p><p>47 The light-shaded triangles mark the somas of the excitatory neurons and the dark-shaded triangle marks the soma of the global inhibitory neuron. [sent-75, score-0.642]
</p><p>48 decide if it lies in one or more of the receptive ﬁelds (RFs) of the neurons on the receiver. [sent-77, score-0.461]
</p><p>49 The retina and transceiver chips can handle handshaking cycle times on the order of 100 ns. [sent-81, score-0.605]
</p><p>50 3 Neuron Circuit The circuit of a neuron and an excitatory synapse on the transceiver chip is shown in Figure 4. [sent-82, score-1.02]
</p><p>51 The synapse circuit (M1–M4) in the left box of the ﬁgure was originally described in (Boahen, 1996). [sent-83, score-0.226]
</p><p>52 ¤ ¥¡  ¢ £¡  The circuit in the right box of Figure 4 implements a linear threshold integrate and ﬁre neuron with an adjustable voltage threshold, spike pulse width and refractory period. [sent-86, score-0.665]
</p><p>53 When the memexceeds a threshold voltage , the output of the transconductance brane potential ampliﬁer M5–M9 switches to a voltage close to . [sent-88, score-0.277]
</p><p>54 As long as the gate voltage of M21 is sufﬁciently high, the neuron is in its refractory period. [sent-99, score-0.385]
</p><p>55 The spike output of the circuit is taken from the output of the ﬁrst to the magnitude of inverter. [sent-101, score-0.277]
</p><p>56 The spike addresses and spike times generated by the retina and the multi-neuron chip at an image speed of 7. [sent-108, score-0.915]
</p><p>57 Each pixel of the retina responded with only one spike to the transition of an edge of the stimulus because the refractory period of the pixel was set to 500 s. [sent-111, score-0.907]
</p><p>58 The spike addresses during the time of travel of the OFF edge of a 0 deg oriented stimulus through the entire array (Figure 5(a)) indicates that almost all the pixels along a row transmitted their addresses sequentially as the edge passed by. [sent-112, score-0.735]
</p><p>59 This sequential ordering can be seen because the stimulus was oriented slightly different from 0 deg. [sent-113, score-0.231]
</p><p>60 If the stimulus was perfectly at 0 deg, then there would be a random ordering of the pixel addresses within each row. [sent-114, score-0.265]
</p><p>61 The same observation can be made for the OFF-transient spikes recorded in response to a 90 deg oriented stimulus (Figure 5(b)). [sent-115, score-0.595]
</p><p>62 The receptive ﬁelds of two orientation-selective neurons were synthesized by mapping the OFF transient outputs of a selected set of pixels on the retina as shown in Figure 3. [sent-116, score-0.937]
</p><p>63 The local excitatory coupling between the neurons was disabled. [sent-118, score-0.492]
</p><p>64 There is no self excitation to each neuron so we explored only a feedforward model and a feedback model using global inhibition. [sent-119, score-0.465]
</p><p>65 We varied the size and aspect ratio of the receptive ﬁelds of the neurons by changing the template size used in the mapping of the retina spikes to the transceiver chip. [sent-120, score-1.295]
</p><p>66 The template size and aspect ratio determine the orientation responses of the neurons. [sent-121, score-0.256]
</p><p>67 The orientation response of these neurons also depends on the time constant of the neuron. [sent-122, score-0.532]
</p><p>68 Instead, we generated a leak current through in Figure 4 by controlling the source voltage of ,  ¡ ¨¢   ¡ ¢   250  Retina address for OFF spikes  Retina address for OFF spikes  250  200  150  100  50  0  200  150  100  50  0 7. [sent-124, score-0.562]
</p><p>69 02  −50  0  50  100  Time (s)  (a)  150  Time (ms)  (b)  Figure 5: The spike addresses from the retina were recorded when a 0 deg (a) and a 90 deg (b) oriented stimulus moved across the retina. [sent-131, score-1.17]
</p><p>70 The ﬁgure shows the time progression of the stimulated pixels (OFF spikes are marked with circles) as the 0 deg oriented stimulus (see Figure 3 for the orientation deﬁnition) passed over each row in (a). [sent-132, score-0.783]
</p><p>71 A similar observation is true of (b) for the ordering of the OFF-transient spikes when each column on the retina was stimulated by the 90 deg oriented stimulus. [sent-134, score-0.825]
</p><p>72 Because the neuron charges up to threshold through the summation of the incoming EPSPs, it can only spike if the ISIs of the incoming spikes are small enough. [sent-137, score-0.564]
</p><p>73 The synaptic weight determines the number of EPSPs needed to drive the neuron above threshold. [sent-138, score-0.235]
</p><p>74 (¢ )© ¡  (¢ 8© ¡  We ﬁrst investigated the feedforward model by using a template size of 5 7 (3 deg 4. [sent-139, score-0.347]
</p><p>75 ) The time constant of the neuron and synaptic gain and strength were adjusted so that both neurons responded optimally to the stimulus. [sent-145, score-0.689]
</p><p>76 The connection from the global inhibitory neuron to the two excitatory neurons was disabled. [sent-146, score-0.877]
</p><p>77 Data was collected from the multi-neuron chip for different orientations of the drum (and hence of the stimulus). [sent-147, score-0.405]
</p><p>78 Since the orientation-selective neurons responded with only 1–3 spikes every time the stimulus moved over the retina, we normalized the total number of spikes collected in these experiments to the number of stimulus presentations. [sent-149, score-1.091]
</p><p>79 The results are shown as a polar plot in Figure 6(a) for the two neurons that are sensitive to orthogonal orientations. [sent-150, score-0.404]
</p><p>80 Each neuron was more sensitive to a stimulus at its preferred orientation than the nonpreferred orientations. [sent-151, score-0.533]
</p><p>81 The neuron responded more to the orthogonal orientation than to the in-between orientations because there were a small number of retina spikes that arrived with a small ISI when the orthogonally-oriented stimulus moved across the template space of the retina (see Figure 3). [sent-152, score-1.684]
</p><p>82 We used an orientation-selective (OS) index to quantify the orientation selectivity of the neuron. [sent-153, score-0.184]
</p><p>83 As an example, R(preferred) for neuron 5, which is sensitive to vertical orientations, is R(90)+R(270) and R(nonpreferred) is R(0)+R(180). [sent-155, score-0.235]
</p><p>84 In the presence of global inhibition, the multi-  90  90  60  120 150  120 150  30  180  0  210  330 240  300  60 30  180  0  330  210 300  240  270  270  (a)  (b)  Figure 6: Orientation tuning curves of the two neurons in the (a) absence and (b) presence of global inhibition. [sent-157, score-0.533]
</p><p>85 The responses of the neurons were measured by the number of spikes collected per stimulus presentation. [sent-158, score-0.687]
</p><p>86 The data was collected for stimulus orientations spaced at 30 deg intervals. [sent-160, score-0.422]
</p><p>87 The neuron that responded preferably to a 90 deg oriented stimulus (solid curve) also had a small response to a stimulus at 0 deg orientation (OS=0. [sent-161, score-1.221]
</p><p>88 The same observation is true for the other neuron (dashed curve) (OS=0. [sent-163, score-0.235]
</p><p>89 In the presence of global inhibition, each neuron responded less to the non-preferred orientation due to the suppression from the other neuron (cross-orientation inhibition). [sent-165, score-0.742]
</p><p>90 We tuned the coupling strengths between the excitatory neurons and the inhibitory neuron so that we obtained the optimal response to the same stimulus presentations as in the feedforward case. [sent-171, score-1.063]
</p><p>91 The non-preferred response of a neuron was suppressed by the other neuron through the recurrent inhibition (cross-orientation inhibition). [sent-173, score-0.572]
</p><p>92 The spiking neurons can be conﬁgured for different computational properties. [sent-176, score-0.471]
</p><p>93 Interchip and intrachip connectivity between neurons can be programmed using the AER protocol. [sent-177, score-0.406]
</p><p>94 In this work, we created receptive ﬁelds for orientation-tuned spiking neurons by mapping the transient spikes from a silicon retina onto the neurons using a microcontroller. [sent-178, score-1.664]
</p><p>95 We have not mapped onto all the neurons on the transceiver chip because the PIC microcontroller we used is not fast enough to create receptive ﬁelds for more neurons without distorting the ISI distribution of the incoming retina spikes. [sent-179, score-1.819]
</p><p>96 We evaluated the responses of the orientation-tuned spiking neurons for different receptive ﬁeld sizes and aspect ratios and also in the absence and presence of feedback inhibition. [sent-180, score-0.701]
</p><p>97 In  a feedforward model, the aVLSI spiking neurons show orientation selectivity similar to digital simulations of continuous-valued neurons. [sent-181, score-0.758]
</p><p>98 Adding inhibition increased the selectivity of the spiking neurons between orthogonal orientations. [sent-182, score-0.629]
</p><p>99 Horiuchi for the original design of the transceiver chip and David Lawrence for the software driver development in this work. [sent-187, score-0.468]
</p><p>100 An emergent model of orientation selectivity in cat visual cortex simple cells. [sent-266, score-0.184]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neurons', 0.368), ('retina', 0.362), ('chip', 0.268), ('neuron', 0.235), ('transceiver', 0.2), ('deg', 0.184), ('pic', 0.174), ('spikes', 0.145), ('stimulus', 0.137), ('orientation', 0.129), ('excitatory', 0.124), ('douglas', 0.121), ('spike', 0.112), ('boahen', 0.109), ('spiking', 0.103), ('feedback', 0.102), ('address', 0.099), ('circuit', 0.099), ('silicon', 0.096), ('kramer', 0.095), ('synapse', 0.094), ('oriented', 0.094), ('receptive', 0.093), ('inhibitory', 0.093), ('template', 0.092), ('aer', 0.091), ('microcontroller', 0.091), ('req', 0.091), ('cortical', 0.086), ('responded', 0.086), ('elds', 0.082), ('ack', 0.079), ('refractory', 0.076), ('voltage', 0.074), ('arbiter', 0.073), ('telluride', 0.073), ('whatley', 0.073), ('feedforward', 0.071), ('pixel', 0.067), ('inhibition', 0.067), ('pulses', 0.067), ('vdd', 0.067), ('orientations', 0.064), ('transistor', 0.063), ('addresses', 0.061), ('transient', 0.06), ('global', 0.057), ('vlsi', 0.057), ('selectivity', 0.055), ('neuronal', 0.055), ('avlsi', 0.054), ('differentiator', 0.054), ('transconductance', 0.054), ('pixels', 0.054), ('os', 0.054), ('ampli', 0.054), ('tuning', 0.051), ('asynchronous', 0.048), ('zurich', 0.047), ('neuromorphic', 0.047), ('effected', 0.043), ('lgn', 0.043), ('chips', 0.043), ('protocol', 0.043), ('switches', 0.042), ('stimulated', 0.04), ('send', 0.04), ('virtual', 0.04), ('connectivity', 0.038), ('collected', 0.037), ('incoming', 0.036), ('created', 0.036), ('delbruck', 0.036), ('drum', 0.036), ('hubel', 0.036), ('indiveri', 0.036), ('irradiance', 0.036), ('lazzaro', 0.036), ('microcircuit', 0.036), ('request', 0.036), ('somers', 0.036), ('wiesel', 0.036), ('orthogonal', 0.036), ('koch', 0.036), ('moved', 0.036), ('pulse', 0.036), ('martin', 0.036), ('aspect', 0.035), ('system', 0.035), ('response', 0.035), ('output', 0.033), ('box', 0.033), ('onto', 0.033), ('communication', 0.032), ('array', 0.032), ('digital', 0.032), ('liu', 0.032), ('nonpreferred', 0.032), ('monitored', 0.032), ('bus', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="141-tfidf-1" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>2 0.27441451 <a title="141-tfidf-2" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>3 0.23162323 <a title="141-tfidf-3" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>4 0.18198276 <a title="141-tfidf-4" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>5 0.16822392 <a title="141-tfidf-5" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>6 0.16713753 <a title="141-tfidf-6" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>7 0.16374825 <a title="141-tfidf-7" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>8 0.15761307 <a title="141-tfidf-8" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>9 0.15643939 <a title="141-tfidf-9" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>10 0.15523662 <a title="141-tfidf-10" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>11 0.1494047 <a title="141-tfidf-11" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>12 0.14089341 <a title="141-tfidf-12" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>13 0.13714123 <a title="141-tfidf-13" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>14 0.1333473 <a title="141-tfidf-14" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>15 0.10816702 <a title="141-tfidf-15" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>16 0.10507582 <a title="141-tfidf-16" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>17 0.10051191 <a title="141-tfidf-17" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>18 0.10048239 <a title="141-tfidf-18" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>19 0.089277633 <a title="141-tfidf-19" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>20 0.085347563 <a title="141-tfidf-20" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.157), (1, -0.397), (2, -0.231), (3, 0.062), (4, 0.175), (5, 0.09), (6, 0.069), (7, -0.039), (8, -0.041), (9, -0.006), (10, 0.046), (11, -0.08), (12, 0.096), (13, 0.044), (14, -0.065), (15, -0.052), (16, 0.001), (17, 0.081), (18, 0.024), (19, 0.069), (20, -0.053), (21, -0.062), (22, 0.031), (23, -0.108), (24, 0.056), (25, 0.041), (26, -0.003), (27, -0.109), (28, 0.022), (29, 0.023), (30, -0.057), (31, -0.146), (32, 0.016), (33, -0.038), (34, -0.047), (35, -0.001), (36, 0.005), (37, 0.081), (38, 0.028), (39, -0.049), (40, 0.031), (41, -0.04), (42, 0.077), (43, -0.073), (44, -0.129), (45, 0.065), (46, -0.067), (47, 0.021), (48, -0.006), (49, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9817552 <a title="141-lsi-1" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>2 0.74472171 <a title="141-lsi-2" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>3 0.57866395 <a title="141-lsi-3" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>Author: Ádám Kepecs, S. Raghavachari</p><p>Abstract: Neurons receive excitatory inputs via both fast AMPA and slow NMDA type receptors. We find that neurons receiving input via NMDA receptors can have two stable membrane states which are input dependent. Action potentials can only be initiated from the higher voltage state. Similar observations have been made in several brain areas which might be explained by our model. The interactions between the two kinds of inputs lead us to suggest that some neurons may operate in 3 states: disabled, enabled and firing. Such enabled, but non-firing modes can be used to introduce context-dependent processing in neural networks. We provide a simple example and discuss possible implications for neuronal processing and response variability. 1</p><p>4 0.55444121 <a title="141-lsi-4" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>5 0.54898137 <a title="141-lsi-5" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>6 0.54798847 <a title="141-lsi-6" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>7 0.52766657 <a title="141-lsi-7" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>8 0.51747626 <a title="141-lsi-8" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>9 0.4956933 <a title="141-lsi-9" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>10 0.47627026 <a title="141-lsi-10" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>11 0.47503009 <a title="141-lsi-11" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>12 0.45628694 <a title="141-lsi-12" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>13 0.45353806 <a title="141-lsi-13" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>14 0.45175794 <a title="141-lsi-14" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>15 0.4382315 <a title="141-lsi-15" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>16 0.43452576 <a title="141-lsi-16" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>17 0.41952676 <a title="141-lsi-17" href="./nips-2001-Analog_Soft-Pattern-Matching_Classifier_using_Floating-Gate_MOS_Technology.html">34 nips-2001-Analog Soft-Pattern-Matching Classifier using Floating-Gate MOS Technology</a></p>
<p>18 0.41824889 <a title="141-lsi-18" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>19 0.38448808 <a title="141-lsi-19" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>20 0.3408201 <a title="141-lsi-20" href="./nips-2001-Bayesian_morphometry_of_hippocampal_cells_suggests_same-cell_somatodendritic_repulsion.html">42 nips-2001-Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.043), (17, 0.012), (19, 0.022), (27, 0.071), (30, 0.115), (34, 0.016), (38, 0.062), (48, 0.016), (59, 0.013), (72, 0.028), (74, 0.368), (79, 0.029), (81, 0.011), (91, 0.119)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84694505 <a title="141-lda-1" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>2 0.68916905 <a title="141-lda-2" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>Author: Peter E. Latham</p><p>Abstract: Almost two decades ago , Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy; doing so without destabilizing the background is not. Previous work [2, 3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented. One of the most important features of the nervous system is its ability to perform associative memory. It is generally believed that associative memory is implemented using attractor networks - experimental studies point in that direction [4- 7], and there are virtually no competing theoretical models. Perhaps surprisingly, however, it is still an open theoretical question whether attractors can exist in realistic neuronal networks. The</p><p>3 0.46276125 <a title="141-lda-3" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>4 0.46041882 <a title="141-lda-4" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>Author: Jesper Tegnér, Ádám Kepecs</p><p>Abstract: Hebbian learning rules are generally formulated as static rules. Under changing condition (e.g. neuromodulation, input statistics) most rules are sensitive to parameters. In particular, recent work has focused on two different formulations of spike-timing-dependent plasticity rules. Additive STDP [1] is remarkably versatile but also very fragile, whereas multiplicative STDP [2, 3] is more robust but lacks attractive features such as synaptic competition and rate stabilization. Here we address the problem of robustness in the additive STDP rule. We derive an adaptive control scheme, where the learning function is under fast dynamic control by postsynaptic activity to stabilize learning under a variety of conditions. Such a control scheme can be implemented using known biophysical mechanisms of synapses. We show that this adaptive rule makes the addit ive STDP more robust. Finally, we give an example how meta plasticity of the adaptive rule can be used to guide STDP into different type of learning regimes. 1</p><p>5 0.4472 <a title="141-lda-5" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><p>6 0.44179362 <a title="141-lda-6" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>7 0.44177616 <a title="141-lda-7" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>8 0.44072163 <a title="141-lda-8" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>9 0.43985471 <a title="141-lda-9" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>10 0.43713763 <a title="141-lda-10" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>11 0.43623191 <a title="141-lda-11" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>12 0.43463349 <a title="141-lda-12" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>13 0.43309933 <a title="141-lda-13" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>14 0.43130818 <a title="141-lda-14" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>15 0.43028116 <a title="141-lda-15" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>16 0.43025488 <a title="141-lda-16" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>17 0.42990029 <a title="141-lda-17" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>18 0.42805791 <a title="141-lda-18" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>19 0.42681152 <a title="141-lda-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.4251256 <a title="141-lda-20" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
