<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 nips-2001-Contextual Modulation of Target Saliency</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-54" href="#">nips2001-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 nips-2001-Contextual Modulation of Target Saliency</h1>
<br/><p>Source: <a title="nips-2001-54-pdf" href="http://papers.nips.cc/paper/2074-contextual-modulation-of-target-saliency.pdf">pdf</a></p><p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>Reference: <a title="nips-2001-54-reference" href="../nips2001_reference/nips-2001-Contextual_Modulation_of_Target_Saliency_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. [sent-4, score-0.814]
</p><p>2 In such approaches, an object is defined by means of local features. [sent-5, score-0.637]
</p><p>3 fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. [sent-6, score-1.207]
</p><p>4 We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. [sent-7, score-0.638]
</p><p>5 1  Introduction  Although there is growing evidence of the role of contextual information in human perception [1], research in computational vision is dominated by object-based representations [5,9,10,15]. [sent-8, score-0.475]
</p><p>6 In real-world scenes, intrinsic object information is often degraded due to occlusion, low contrast, and poor resolution. [sent-9, score-0.626]
</p><p>7 In such situations, the object recognition problem based on intrinsic object representations is ill-posed. [sent-10, score-1.233]
</p><p>8 A more comprehensive representation of an object should include contextual information [11,13]: Obj. [sent-11, score-1.017]
</p><p>9 In this representation, an object is defined by 1) a model of the intrinsic properties of the object and 2) a model of the typical contexts in which the object is immersed. [sent-15, score-1.79]
</p><p>10 Here we show how incorporating contextual models can enhance target object saliency and provide an estimate of its likelihood and intrinsic properties. [sent-16, score-1.385]
</p><p>11 2  Target saliency and object likelihood  Image information can be partitioned into two sets of features: local features, VL, that are intrinsic to an object, and contextual features, rUe which encode structural properties of the background. [sent-17, score-1.333]
</p><p>12 In a statistical framework, object detection requires evaluation of the likelihood function (target saliency function): P(O IVL, va) which provides the probability of presence of the object 0 given a set of local and contextual measurements. [sent-18, score-2.065]
</p><p>13 0 is the set of parameters that define an object immersed in a scene: 0 == {on, x, y, i} with on==object class, (x,y)==location in image coordinates  and bobject appearance parameters. [sent-19, score-0.803]
</p><p>14 It provides a measure of how unlikely it is to find a set of local measurements VL within the context va. [sent-21, score-0.241]
</p><p>15 We can define local saliency as S(x,y) == l/P(vL(x,y) Iva). [sent-22, score-0.259]
</p><p>16 The second factor, P(VL 10, va), gives the likelihood of the local measurements VL when the object is present at such location in a particular context. [sent-24, score-0.739]
</p><p>17 We can write P(VL 10, va) ~ P(VL 10), which is a convenient approximation when the aspect of the target object is fully determined by the parameters given by the description O. [sent-25, score-0.728]
</p><p>18 This factor represents the top-down knowledge of the target· appearance and how it contributes to the search. [sent-26, score-0.111]
</p><p>19 Regions of the image with features unlikely to belong to the target object are vetoed. [sent-27, score-0.933]
</p><p>20 The third factor, the PDF P(O I va), provides context-based priors on object class, location and scale. [sent-29, score-0.64]
</p><p>21 It is of capital importance for insuring reliable inferences in situations where the local image measurements VL produce ambiguous interpretations. [sent-30, score-0.251]
</p><p>22 This factor does not depend on local measurements and target models [8,13]. [sent-31, score-0.248]
</p><p>23 Therefore, the term P(O Iva) modulates the saliency of local image properties when looking for an object of the class On. [sent-32, score-1.08]
</p><p>24 If P( On Iva) is very small, then object search need not be initiated (we do not need to look for cars in a living room). [sent-34, score-0.799]
</p><p>25 On  Contextual control of focus of attention: P(x, y I On, va)· This PDDF gives the most likely locations for the presence of object On given context information, and it allocates computational resources into relevant scene regions. [sent-35, score-0.962]
</p><p>26 This gives the likely (prototypical) shapes (point of views, size, aspect ratio, object aspect) of the object On in the context Va- Here t == {a, p}, with a==scale and p==aspect ratio. [sent-38, score-1.284]
</p><p>27 Other parameters describing the appearance of an object in an image can be added. [sent-39, score-0.803]
</p><p>28 In such a representation [8], v(i,k) is the output magnitude- at the location i of a complex Gabor filter tuned to the spatial frequency f~. [sent-53, score-0.1]
</p><p>29 The variable k indexes filters tuned to different spatial frequencies and orientations. [sent-54, score-0.069]
</p><p>30 On the other ,hand, contextual features have to summarize the structure of the whole image. [sent-55, score-0.49]
</p><p>31 It has been shown that a holistic low-dimensional encoding of the local image features conveys enough information for a semantic categorization of the scene/context [8] and can be used for contextual priming in object recognition tasks [13]. [sent-56, score-1.492]
</p><p>32 Such a representation can be achieved by decomposing the image features into the basis functions provided by PCA:  an ==  L L v{x, k) 1/ln{x, k) x  k  N  v(x, k) ~  L an1/ln(x, k)  (4)  n=l  We propose to use the decomposition coefficients vc == {a n }n=l,N as context features. [sent-57, score-0.405]
</p><p>33 By using only a reduced set of components (N == 60 for the rest of the paper), the coefficients {a n }n=l,N encode the main spectral characteristics of the scene with a coarse description of their spatial arrangement. [sent-59, score-0.203]
</p><p>34 In essence, {a n }n=l,N is a holistic representation as all the regions of the image contribute to all the coefficients, and objects are not encoded individually [8]. [sent-60, score-0.318]
</p><p>35 In the rest of the paper we show the efficacy of this set of features in context modeling for object detection tasks. [sent-61, score-0.879]
</p><p>36 3  Contextual object priming  The PDF P( On Iva) gives the probability of presence of the object class On given contextual information. [sent-62, score-1.76]
</p><p>37 In other words, the PDF P{on Ive) evaluates the consistency of the object On with the context vc. [sent-63, score-0.672]
</p><p>38 For instance, a car has a high probability of presence in a highway scene but it is inconsistent with an indoor environment. [sent-64, score-0.293]
</p><p>39 The goal of P(on Ive) is to cut down the number of possible object categories to deal with before- expending computational resources in the object recognition process. [sent-65, score-1.257]
</p><p>40 The model parameters (bi,n, Vi,n, Vi,n) for the object class On are obtained using the EM algorithm [3]. [sent-67, score-0.592]
</p><p>41 presence/absence of four objects categories (i-people, 2-furniture, 3-vehicles and 4-trees). [sent-70, score-0.111]
</p><p>42 1 shows some typical results from the priming model on the four superordinate categories of objects defined. [sent-72, score-0.261]
</p><p>43 that the probability function P(on Ive) provides information about the probable presence of one object without scanning the picture. [sent-74, score-0.65]
</p><p>44 If P( On Ive) > 1th then we can predict that the target is present. [sent-75, score-0.118]
</p><p>45 On the other hand, if P( On Ive) < th we can predict that the object is likely to be absent before exploring the image. [sent-76, score-0.571]
</p><p>46 The number of scenes in which the system may be able to take high confidence decisions will depend on different factors such as: the strength of the relationship between the target object and its context and the ability of ve for efficiently characterizing the context. [sent-77, score-0.934]
</p><p>47 Figure 1 shows some typical results from the priming model for a set of super-ordinate categories of objects. [sent-78, score-0.199]
</p><p>48 5) the presence/absence of the objects was correctly predicted by the model on 81 % of the scenes of the test set. [sent-80, score-0.131]
</p><p>49 For each object category, high confidence predictions (th == . [sent-81, score-0.593]
</p><p>50 1) were made in at least 50% of the tested scene pictures and the presence/absence of each object class was correctly predicted by the model on 95% of those images. [sent-82, score-0.818]
</p><p>51 Therefore, for those images, we do not need to use local image analysis to decide about the presence/absence of the object. [sent-83, score-0.214]
</p><p>52 4  Contextual control of focus of attention  One of the strategies that biological visual systems use to deal with the analysis of real-world scenes is to focus attention (and, therefore, computational resources) onto the important image regions while neglecting others. [sent-84, score-0.585]
</p><p>53 Current computational models of visual attention (saliency maps anQ target detection) rely exclusively on local information or intrinsic object models [6,7,9,14,16]. [sent-85, score-0.961]
</p><p>54 The control of the focus of attention by contextual information that we propose. [sent-86, score-0.598]
</p><p>55 here is both task driven (looking for object on) and context driven (given global context information: ve). [sent-87, score-0.731]
</p><p>56 However, it does riot include any model of the target object at this stage. [sent-88, score-0.689]
</p><p>57 In our framework, the problem of contextual control of the focus of attention involves the  S••  10 . [sent-89, score-0.598]
</p><p>58 4  oReal pose 1  Figure 3: Estimation results of object scale and pose based on contextual features. [sent-121, score-1.247]
</p><p>59 The training data is {Vt}t==l,Nt and {Xt}t==l,Nt where Vt are the contextual features of the picture t of the training set and Xt is the location of object On in the image. [sent-125, score-1.099]
</p><p>60 We used 1200 pictures for training and a separate set of 1200 pictures for testing. [sent-127, score-0.176]
</p><p>61 The success of the PDF in narrowing the region of the focus of attention will depend on the consistency of the relationship between the object and the context. [sent-128, score-0.76]
</p><p>62 2 shows several examples of images and the selected regions based on contextual features when looking for cars and faces. [sent-130, score-0.889]
</p><p>63 From the PDF P(x, Vo IOn) we selected the region with the highest probability (33% of the image size on average). [sent-131, score-0.174]
</p><p>64 87% of the heads present in the test pictures were inside the selected regions. [sent-132, score-0.159]
</p><p>65 5  Contextual selection of object appearance models  One major problem for computational approaches to object detection is the large variability in object appearance. [sent-133, score-1.989]
</p><p>66 The classical solution is to explore the space of possible shapes looking for the best match. [sent-134, score-0.104]
</p><p>67 The main sources of variability in object appearance are size, pose and intra-class shape variability (deformations, style, etc. [sent-135, score-0.821]
</p><p>68 We show here that including contextual information can reduce at le. [sent-137, score-0.421]
</p><p>69 For instance, the expected size of people in an image differs greatly between an indoor environment and a perspective view of a street. [sent-139, score-0.183]
</p><p>70 Both environments produce different patterns of contextual features vo [8]. [sent-140, score-0.579]
</p><p>71 For the second factor, pose, in the case of cars, there is a strong relationship between the possible orientations of the object and the scene configuration. [sent-141, score-0.709]
</p><p>72 For instance, looking down a highway, we expect to see the back of the cars, however, in a street view, looking towards the buildings, lateral views of cars are more likely. [sent-142, score-0.434]
</p><p>73 The expected scale and pose of the target object can be estimated by a regression procedure. [sent-143, score-0.859]
</p><p>74 The training database used for building the regression is a set of 1000 images in which the target object On is present. [sent-144, score-0.744]
</p><p>75 For each training image the target  Figure 4: Selection of prototypical object appearances based on contextual cues. [sent-145, score-1.368]
</p><p>76 For faces and cars we define the u == scale as the height of the selected window and the P == pose as the ratio between the horizontal and vertical dimensions of the window (~y/ ~x). [sent-147, score-0.451]
</p><p>77 On average, this definition of pose provides a good estimation of the orientation for cars but not for heads. [sent-148, score-0.365]
</p><p>78 Here we used regression using a mixture of gaussians for estimating the conditional PDFs between scale, pose and contextual features: P(u I Va, on) and PCP I va, on). [sent-149, score-0.57]
</p><p>79 3 show that context is a strong cue for scale selection for the face detection task but less important for the car detection task. [sent-151, score-0.45]
</p><p>80 On the other hand, context introduces strong constraints on the prototypical point of views of cars but not at all for heads. [sent-152, score-0.422]
</p><p>81 Once the two parameters (pose and scale) have been estimated, we can build a prototypical model of the target object. [sent-153, score-0.188]
</p><p>82 In the case of a view-based object representation, the model of the object will consist of a collection of templates that correspond to the possible aspects of the target. [sent-154, score-1.142]
</p><p>83 For each image the system produces a collection of views, selected among a database of target examples that have the scale and pose given by eqs. [sent-155, score-0.441]
</p><p>84 In the statistical framework, the object detection requires the evaluation of the function P(VL 10, va). [sent-159, score-0.708]
</p><p>85 The bottom example is an error in detection due to incorrect context identification. [sent-161, score-0.217]
</p><p>86 5 and 6 show the complete chain of operations and some detection results using a simple correlation technique between the image and the generated object models (100 exemplars) at only one scale. [sent-164, score-0.856]
</p><p>87 The last image of each row shows the total object likelihood obtained by multiplying the object saliency maps (obtained by the correlation) and the contextual control of the focus of attention. [sent-165, score-2.036]
</p><p>88 The result shows how the use of context helps reduce false alarms. [sent-166, score-0.08]
</p><p>89 This results in good detection performances despite the simplicity of the matching procedure used. [sent-167, score-0.16]
</p><p>90 6  Conclusion  The contextual schema of a scene provides the likelihood of presence, typical locations and appearances of objects within the scene. [sent-168, score-0.786]
</p><p>91 We have proposed a model for incorporating such contextual cues in the task of object detection. [sent-169, score-0.992]
</p><p>92 The main aspects of our approach are: 1) Progressive reduction of the window of focus of attention: the system reduces the size of the focus of attention by first integrating contextual information and then local information. [sent-170, score-0.732]
</p><p>93 2) Inhibition of target like patterns that are in inconsistent locations. [sent-171, score-0.142]
</p><p>94 3) Faster detection of correctly scaled targets that have a pose in agreement with the context. [sent-172, score-0.263]
</p><p>95 4) No requirement of parsing a scene into individual objects. [sent-173, score-0.118]
</p><p>96 Furthermore, once one object has been detected, it can introduce new contextual information for analyzing the rest of the scene. [sent-174, score-1.014]
</p><p>97 Scene perception: detecting and judging objects undergoing relational violations. [sent-185, score-0.062]
</p><p>98 A model of saliency-based visual attention for rapid scene analysis. [sent-230, score-0.246]
</p><p>99 Modeling the Shape of the Scene: A holistic representation of the spatial envelope. [sent-242, score-0.115]
</p><p>100 Rapid object detection using a boosted cascade of simple features. [sent-295, score-0.708]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('object', 0.571), ('contextual', 0.421), ('cars', 0.228), ('vl', 0.197), ('saliency', 0.193), ('va', 0.16), ('image', 0.148), ('pdf', 0.14), ('detection', 0.137), ('priming', 0.128), ('iva', 0.121), ('target', 0.118), ('scene', 0.118), ('pose', 0.106), ('ive', 0.105), ('attention', 0.095), ('vo', 0.089), ('pictures', 0.088), ('appearance', 0.084), ('looking', 0.081), ('context', 0.08), ('prototypical', 0.07), ('torralba', 0.07), ('features', 0.069), ('local', 0.066), ('objects', 0.062), ('vc', 0.057), ('ion', 0.055), ('intrinsic', 0.055), ('vision', 0.054), ('focus', 0.053), ('holistic', 0.053), ('categories', 0.049), ('scenes', 0.049), ('presence', 0.048), ('ve', 0.045), ('heads', 0.045), ('views', 0.044), ('scale', 0.043), ('appearances', 0.04), ('highway', 0.04), ('ivl', 0.04), ('jauai', 0.04), ('oliva', 0.04), ('aspect', 0.039), ('location', 0.038), ('measurements', 0.037), ('spatial', 0.037), ('recognition', 0.036), ('gk', 0.035), ('indoor', 0.035), ('picard', 0.035), ('sinha', 0.035), ('video', 0.034), ('images', 0.034), ('visual', 0.033), ('locations', 0.033), ('filters', 0.032), ('pdfs', 0.032), ('schema', 0.032), ('ieee', 0.031), ('provides', 0.031), ('variability', 0.03), ('oriented', 0.03), ('regions', 0.03), ('resources', 0.03), ('factors', 0.029), ('control', 0.029), ('ei', 0.029), ('society', 0.029), ('car', 0.028), ('unlikely', 0.027), ('likelihood', 0.027), ('factor', 0.027), ('coefficients', 0.026), ('selected', 0.026), ('exhaustive', 0.026), ('vol', 0.026), ('integration', 0.025), ('selection', 0.025), ('representation', 0.025), ('inconsistent', 0.024), ('window', 0.024), ('maps', 0.023), ('performances', 0.023), ('shapes', 0.023), ('rest', 0.022), ('confidence', 0.022), ('typical', 0.022), ('gaussians', 0.022), ('regression', 0.021), ('class', 0.021), ('modulation', 0.021), ('procedures', 0.021), ('consistency', 0.021), ('vt', 0.021), ('cognitive', 0.02), ('correctly', 0.02), ('integrating', 0.02), ('relationship', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="54-tfidf-1" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>2 0.34792215 <a title="54-tfidf-2" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>3 0.23285052 <a title="54-tfidf-3" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>Author: Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio</p><p>Abstract: We describe an algorithm for automatically learning discriminative components of objects with SVM classiﬁers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiﬁers are then combined in a second stage to yield a hierarchical SVM classiﬁer. Experimental results in face classiﬁcation show considerable robustness against rotations in depth and suggest performance at signiﬁcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiﬁcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiﬁer which may be relevant for biological models of visual recognition.</p><p>4 0.19170409 <a title="54-tfidf-4" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>5 0.10432909 <a title="54-tfidf-5" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, inﬂuenced by input stimuli, and top-down inference, inﬂuenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.</p><p>6 0.099722892 <a title="54-tfidf-6" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>7 0.099298976 <a title="54-tfidf-7" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>8 0.096181862 <a title="54-tfidf-8" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>9 0.094655529 <a title="54-tfidf-9" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>10 0.093055651 <a title="54-tfidf-10" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>11 0.089159146 <a title="54-tfidf-11" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>12 0.080645099 <a title="54-tfidf-12" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>13 0.078290239 <a title="54-tfidf-13" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>14 0.0736292 <a title="54-tfidf-14" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>15 0.06832286 <a title="54-tfidf-15" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>16 0.067618966 <a title="54-tfidf-16" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>17 0.066411905 <a title="54-tfidf-17" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>18 0.059986588 <a title="54-tfidf-18" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>19 0.05886817 <a title="54-tfidf-19" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>20 0.0583391 <a title="54-tfidf-20" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.165), (1, -0.09), (2, -0.103), (3, 0.024), (4, -0.142), (5, 0.052), (6, -0.39), (7, -0.002), (8, 0.059), (9, 0.009), (10, 0.009), (11, 0.155), (12, 0.246), (13, 0.041), (14, -0.083), (15, 0.034), (16, 0.239), (17, 0.012), (18, -0.058), (19, -0.042), (20, 0.066), (21, -0.05), (22, -0.08), (23, -0.014), (24, 0.094), (25, -0.161), (26, -0.145), (27, -0.031), (28, -0.06), (29, 0.086), (30, -0.044), (31, -0.062), (32, 0.186), (33, -0.08), (34, -0.013), (35, -0.099), (36, -0.061), (37, -0.04), (38, 0.04), (39, -0.011), (40, -0.047), (41, -0.002), (42, -0.104), (43, 0.006), (44, 0.019), (45, 0.006), (46, -0.039), (47, -0.043), (48, -0.037), (49, -0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98956031 <a title="54-lsi-1" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>2 0.8416099 <a title="54-lsi-2" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>3 0.55850905 <a title="54-lsi-3" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>4 0.46884051 <a title="54-lsi-4" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>Author: Bernd Heisele, Thomas Serre, Massimiliano Pontil, Thomas Vetter, Tomaso Poggio</p><p>Abstract: We describe an algorithm for automatically learning discriminative components of objects with SVM classiﬁers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classiﬁers are then combined in a second stage to yield a hierarchical SVM classiﬁer. Experimental results in face classiﬁcation show considerable robustness against rotations in depth and suggest performance at signiﬁcantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classiﬁcation experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classiﬁer which may be relevant for biological models of visual recognition.</p><p>5 0.45164183 <a title="54-lsi-5" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>Author: Shimon Edelman, Benjamin P. Hiles, Hwajin Yang, Nathan Intrator</p><p>Abstract: To ﬁnd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow’s criterion of “suspicious coincidence” (the ratio of joint probability to the product of marginals). We then compared the part veriﬁcation response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the signiﬁcance of their co-occurrence as estimated by Barlow’s criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain’s strategies for unsupervised acquisition of structural information in vision. 1 Motivation How does the human visual system decide for which objects it should maintain distinct and persistent internal representations of the kind typically postulated by theories of object recognition? Consider, for example, the image shown in Figure 1, left. This image can be represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer to as and ), a set of strokes, or, trivially, as a collection of pixels. Note that the second option is only available to a system previously exposed to various combinations of Chinese characters. Indeed, a principled decision whether to represent this image as , or otherwise can only be made on the basis of prior exposure to related images. £ ¡ £¦ ¡ £ ¥¨§¢   ¥¤¢   ¢ According to Barlow’s [1] insight, one useful principle is tallying suspicious coincidences: two candidate fragments and should be combined into a composite object if the probability of their joint appearance is much higher than , which is the probability expected in the case of their statistical independence. This criterion may be compared to the Minimum Description Length (MDL) principle, which has been previously discussed in the context of object representation [2, 3]. In a simpliﬁed form [4], MDL calls for representing explicitly as a whole if , just as the principle of suspicious coincidences does. £ ©¢  £  ¢ ¥¤¥  £¦ ¢ ¥  £  ¢   £¦ ¢ ¥¤¥! ¨§¥ £ ¢ £ ©¢  £¦  £ ¨§¢¥ ¡ ¢   While the Barlow/MDL criterion certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used and . One example is the possiin setting the degree of association between ble perfect predictability of from and vice versa, as measured by . If , then and are perfectly predictive of each other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of and that of be established. In comparison, if and are not perfectly predictive of each other ( ), there is a case to be made in favor of coding them separately to allow for a maximally expressive representation, whereas MDL may actually suggest a high degree of association ). In this study we investigated whether the human (if visual system uses a criterion based on alongside MDL while learning (in an unsupervised manner) to represent composite objects. £ £  £ ¢  ¥  ¥ © §¥ ¡ ¢  ¨¦¤</p><p>6 0.43287864 <a title="54-lsi-6" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>7 0.37733588 <a title="54-lsi-7" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>8 0.34202394 <a title="54-lsi-8" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>9 0.30393955 <a title="54-lsi-9" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>10 0.30102611 <a title="54-lsi-10" href="./nips-2001-Receptive_field_structure_of_flow_detectors_for_heading_perception.html">158 nips-2001-Receptive field structure of flow detectors for heading perception</a></p>
<p>11 0.29929256 <a title="54-lsi-11" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>12 0.29602957 <a title="54-lsi-12" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>13 0.28856647 <a title="54-lsi-13" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>14 0.26956066 <a title="54-lsi-14" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>15 0.26508531 <a title="54-lsi-15" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>16 0.26082844 <a title="54-lsi-16" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>17 0.24177362 <a title="54-lsi-17" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>18 0.24045803 <a title="54-lsi-18" href="./nips-2001-A_Hierarchical_Model_of_Complex_Cells_in_Visual_Cortex_for_the_Binocular_Perception_of_Motion-in-Depth.html">10 nips-2001-A Hierarchical Model of Complex Cells in Visual Cortex for the Binocular Perception of Motion-in-Depth</a></p>
<p>19 0.23791002 <a title="54-lsi-19" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>20 0.23074606 <a title="54-lsi-20" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.024), (17, 0.034), (19, 0.048), (27, 0.1), (30, 0.076), (36, 0.01), (38, 0.04), (58, 0.298), (59, 0.041), (72, 0.039), (79, 0.039), (83, 0.013), (91, 0.145)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81522369 <a title="54-lda-1" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>2 0.74370182 <a title="54-lda-2" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>Author: Jonathan L. Shapiro, J. Wearden</p><p>Abstract: Animal data on delayed-reward conditioning experiments shows a striking property - the data for different time intervals collapses into a single curve when the data is scaled by the time interval. This is called the scalar property of interval timing. Here a simple model of a neural clock is presented and shown to give rise to the scalar property. The model is an accumulator consisting of noisy, linear spiking neurons. It is analytically tractable and contains only three parameters. When coupled with reinforcement learning it simulates peak procedure experiments, producing both the scalar property and the pattern of single trial covariances. 1</p><p>3 0.72563565 <a title="54-lda-3" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>Author: André Elisseeff, Jason Weston</p><p>Abstract: This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classiﬁcation problem with positive results.</p><p>4 0.56083322 <a title="54-lda-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>5 0.55898964 <a title="54-lda-5" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>6 0.55794096 <a title="54-lda-6" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>7 0.55686975 <a title="54-lda-7" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>8 0.55539799 <a title="54-lda-8" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>9 0.55523598 <a title="54-lda-9" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>10 0.55471456 <a title="54-lda-10" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>11 0.55398804 <a title="54-lda-11" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>12 0.55249226 <a title="54-lda-12" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>13 0.55248767 <a title="54-lda-13" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>14 0.55208135 <a title="54-lda-14" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>15 0.55058104 <a title="54-lda-15" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>16 0.55012941 <a title="54-lda-16" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>17 0.5489583 <a title="54-lda-17" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>18 0.54863811 <a title="54-lda-18" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>19 0.54816443 <a title="54-lda-19" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>20 0.54808617 <a title="54-lda-20" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
