<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-113" href="#">nips2001-113</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</h1>
<br/><p>Source: <a title="nips-2001-113-pdf" href="http://papers.nips.cc/paper/1996-learning-a-gaussian-process-prior-for-automatically-generating-music-playlists.pdf">pdf</a></p><p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>Reference: <a title="nips-2001-113-reference" href="../nips2001_reference/nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu    ¡  ¢  Abstract This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. [sent-7, score-0.955]
</p><p>2 AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. [sent-8, score-0.445]
</p><p>3 This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. [sent-10, score-0.214]
</p><p>4 For playlist generation, AutoDJ learns a kernel from a large set of albums. [sent-11, score-0.732]
</p><p>5 This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel. [sent-12, score-0.336]
</p><p>6 1 Introduction Digital music is becoming very widespread, as personal collections of music grow to thousands of songs. [sent-13, score-0.379]
</p><p>7 One typical way for a user to interact with a personal music collection is to specify a playlist, an ordered list of music to be played. [sent-14, score-0.717]
</p><p>8 Using existing digital music software, a user can manually construct a playlist by individually choosing each song. [sent-15, score-1.091]
</p><p>9 Alternatively, playlists can be generated by the user specifying a set of rules about songs (e. [sent-16, score-0.893]
</p><p>10 , genre = rock), and the system randomly choosing songs that match those rules. [sent-18, score-0.441]
</p><p>11 Constructing a playlist is a tedious process: it takes time to generate a playlist that matches a particular mood. [sent-19, score-1.2]
</p><p>12 It is also difﬁcult to construct a playlist in advance, as a user may not anticipate all possible music moods and preferences he or she will have in the future. [sent-20, score-1.137]
</p><p>13 AutoDJ is a system for automatically generating playlists at the time that a user wants to listen to music. [sent-21, score-0.575]
</p><p>14 The playlist plays with minimal user intervention: the user hears music that is suitable for his or her current mood, preferences and situation. [sent-22, score-1.458]
</p><p>15 The user selects one or more seed songs for AutoDJ to play. [sent-24, score-0.914]
</p><p>16 AutoDJ then generates a playlist with songs that are similar to the seed songs. [sent-25, score-1.181]
</p><p>17 The user may also review the playlist and add or remove certain songs, if they don’t ﬁt. [sent-26, score-0.909]
</p><p>18 AutoDJ uses a machine learning system that ﬁnds a current user preference function over a feature space of music. [sent-28, score-0.422]
</p><p>19 Every time a user selects a seed song or removes a song from the £  ¤  Current address: Department of Electrical Engineering and Computer Science, University of California at Berkeley  playlist, a training example is generated. [sent-29, score-0.735]
</p><p>20 In general, a user can give an arbitrary preference value to any song. [sent-30, score-0.422]
</p><p>21 By default, we assume that selected songs have target values of 1, while removed songs have target values of 0. [sent-31, score-0.822]
</p><p>22 Given a training set, a full user preference function is inferred by regression. [sent-32, score-0.45]
</p><p>23 The for each song owned by the user is evaluated, and the songs with the highest are placed into the playlist. [sent-33, score-0.857]
</p><p>24 The training set often contains only one training example: a single seed song that the user wishes to listen to. [sent-35, score-0.676]
</p><p>25 Technical details of how to apply GPR to playlist generation are given in section 2. [sent-38, score-0.625]
</p><p>26 We deﬁne the input space to be descriptive metadata about the song. [sent-40, score-0.207]
</p><p>27 Given a training set of user preferences, a user preference function is generated by forming a linear blend of these kernel functions, whose weights are solved via a linear system. [sent-41, score-0.938]
</p><p>28 This user preference function is then used to evaluate all of the songs in the user’s collection. [sent-42, score-0.833]
</p><p>29 During meta-training, a kernel is learned before any training examples are available. [sent-47, score-0.203]
</p><p>30 The kernel is learned from a set of samples from meta-training functions. [sent-48, score-0.196]
</p><p>31 In order to generalize the kernel beyond the meta-training data set, we ﬁt a parameterized kernel to the meta-training data, with many fewer parameters than data points. [sent-50, score-0.307]
</p><p>32 The learned kernel thus reﬂects the similarity of songs on professionally designed albums. [sent-56, score-0.659]
</p><p>33 GPR is then performed using the learned kernel every time a user selects or removes songs from a playlist. [sent-58, score-0.928]
</p><p>34 The learned kernel forms a good prior, which enables AutoDJ to learn a user preference function with a very small number of user training examples. [sent-59, score-0.969]
</p><p>35 1 Previous Work There are several commercial Web sites for playing or recommending music based on one seed song. [sent-61, score-0.36]
</p><p>36 This work is related to Collaborative Filtering (CF) [9] and to building user proﬁles in textual information retrieval [11]. [sent-63, score-0.321]
</p><p>37 However, CF does not use metadata associated with a media object, hence CF will not generalize to new music that has few or no user votes. [sent-64, score-0.729]
</p><p>38 Also, no work has been published on building user proﬁles for music. [sent-65, score-0.321]
</p><p>39 Previous work in GPR [14] learned kernel parameters through Bayesian methods from just the training set, not from meta-training data. [sent-67, score-0.203]
</p><p>40 When AutoDJ generates playlists, the user may select only one training example. [sent-68, score-0.37]
</p><p>41 In this work, using a quadratic program, we ﬁt a parameterized Mercer kernel directly to a meta-training kernel matrix in order to generalize the covariance. [sent-74, score-0.347]
</p><p>42 2 Gaussian Process Regression for Playlist Generation AutoDJ uses GPR to generate a playlist every time a user selects one or more songs. [sent-78, score-0.954]
</p><p>43 In this paper, we assume that the GP over user preference functions is zero mean. [sent-83, score-0.44]
</p><p>44 That is, at any particular time, the user does not want to listen to most of the songs in the world, which leads to a mean preference close enough to zero to approximate as zero. [sent-84, score-0.869]
</p><p>45 of functions :  ¨ ©£  ¥  ¨ §£¦ ¡£   ¥  ¨ §£¦ ¡£   ¥ ¤£¡ ¦ ¡£ ¨ £ ¦¤£¢  ¥ ¡    ¨  £  ¦ © §  ¨¨  £ ¦¤£¡   ¥  ¦  In section 3, we learn a kernel which takes music metadata as and . [sent-86, score-0.574]
</p><p>46 In this paper, whenever we refer to a music metadata vector, we mean a vector consisting of 7 categorical variables: genre, subgenre, style, mood, rhythm type, rhythm description, and vocal code. [sent-87, score-0.531]
</p><p>47 This music metadata vector is assigned by editors to every track of a large corpus of music CDs. [sent-88, score-0.571]
</p><p>48 Our kernel function thus computes the similarity between two metadata vectors corresponding to two songs. [sent-90, score-0.403]
</p><p>49 Let be the metadata vectors for the songs for which the user has expressed a preference by selecting or removing them from the playlist. [sent-95, score-1.059]
</p><p>50 If the user does not express a real-valued preference, is assumed 1 if the user wants to listen to the song and 0 if the user does not. [sent-98, score-1.118]
</p><p>51 ¢  ¢   ¢  ¢   ¡  Let be the underlying true user preference for the th song, of which is a noisy measurement, with Gaussian noise of variance . [sent-100, score-0.44]
</p><p>52 Also, let be a metadata vector of any song that will be considered to be on a playlist: is the (unknown) user preference for that song. [sent-101, score-0.731]
</p><p>53 Since the number of user preferences tends to be small, inverting this matrix is very fast. [sent-107, score-0.402]
</p><p>54 The formula for the log likelihood of the training data given is (3) Every time a playlist is generated, different values of are evaluated and the that generates the highest log likelihood is used. [sent-110, score-0.637]
</p><p>55 In order to generate the playlist, the matrix is computed, and the user preference function is computed for every song that the user owns. [sent-111, score-0.888]
</p><p>56 The songs are then ranked in descending order of . [sent-112, score-0.432]
</p><p>57 The playlist consists of the top songs in the ranked list. [sent-113, score-1.02]
</p><p>58 The playlist can cut off after a ﬁxed number of songs, e. [sent-114, score-0.588]
</p><p>59 It can also cut off if the value of gets too low, so that the playlist only contains songs that the user will enjoy. [sent-117, score-1.32]
</p><p>60 The order of the playlist is the order of the songs in the ranked list. [sent-118, score-1.02]
</p><p>61 This is empirically effective: the playlist typically starts with the selected seed songs, proceeds to songs very similar to the seed songs, and then gradually drifts away from the seed songs towards the end of the list, when the user is paying less attention. [sent-119, score-2.229]
</p><p>62 Here, “effective” is deﬁned as generating playlists that are pleasing to the authors. [sent-121, score-0.183]
</p><p>63 This set of functions should be related to a ﬁnal trained function, since we derive a similarity kernel from the meta-training set of functions. [sent-124, score-0.195]
</p><p>64 We express the kernel as a covariance components model [12]:   ¤£¡  a (£  ¥  ¨ §£¦ ¡£ ¥  (4)  b e fdb  ¤  c § d¨¥ b  § ¨ ¨  ` U  Y  £  §£¦ ¡£   ¥     where are pre-deﬁned Mercer kernels and . [sent-126, score-0.196]
</p><p>65 The cost function in equation (5) is the square of the Frobenius norm of the difference between the empirical matrix and the ﬁt kernel . [sent-138, score-0.2]
</p><p>66 Analogous to [8], we can prove that the Frobenius norm is consistent: as the amount of training data goes to inﬁnity, the empirical Frobenius norm, above, approaches the Frobenius norm of the difference between the true kernel and our ﬁt kernel. [sent-141, score-0.231]
</p><p>67 1 KMT for Music Playlist Generation In this section, we consider the application of the general KMT technique to music playlist generation. [sent-145, score-0.77]
</p><p>68 We decided to use albums to generate a prior for playlist generation, since albums can be considered to be professionally designed playlists. [sent-146, score-0.777]
</p><p>69 For the meta-training function , we use album indicator functions that are 1 for songs on an album , and 0 otherwise. [sent-147, score-0.521]
</p><p>70 If we had simply used the without ﬁtting , the playlist generator would exactly meta-training kernel matrix reproduce one or more albums in the meta-training database. [sent-151, score-0.824]
</p><p>71 Because the album indicator functions are uniquely deﬁned for songs, not for metadata vectors, we cannot simply generate a kernel matrix according to (6). [sent-153, score-0.458]
</p><p>72 Instead, we generate a meta-training kernel matrix using meta-training functions that depend on songs:  ! [sent-154, score-0.205]
</p><p>73 We then ﬁt the according to (5), where the Mercer kernels depend on music metadata vectors that are deﬁned in b  c  £  #    b e  Table 1. [sent-160, score-0.431]
</p><p>74 , which We used 174,577 songs and 14,198 albums to make up the meta-training matrix is dimension 174,577x174,577. [sent-163, score-0.476]
</p><p>75 However, note that the meta-training matrix is very sparse, since most songs only belong to 1 or 2 albums. [sent-164, score-0.43]
</p><p>76 2 Kernels for Categorical Data The kernel learned in section 3 must operate on categorical music metadata. [sent-170, score-0.425]
</p><p>77 Hence, we have designed a new kernel that operates directly on categorical data. [sent-174, score-0.212]
</p><p>78 The vector serves where as a mask: when is 1, then the th component of the two vectors must match in order for the output of the kernel to be 1. [sent-177, score-0.181]
</p><p>79 For playlist generation, the operate on music metadata vectors that are deﬁned in Table 1. [sent-179, score-1.013]
</p><p>80 Thus, the evaluation of from equation (1) on thousands of pieces of music can be done in less than a second on a modern PC. [sent-183, score-0.182]
</p><p>81 We tested AutoDJ on 60 playlists manually designed by users in Microsoft Research. [sent-185, score-0.197]
</p><p>82 We also compare to a playlist which are all of the user’s songs permuted in a random order. [sent-187, score-0.999]
</p><p>83 That is, the similarity between two songs is the number of metadata ﬁelds that they have in common. [sent-189, score-0.651]
</p><p>84 We performed tests using only positive training examples, which emulates users choosing seed songs. [sent-190, score-0.228]
</p><p>85 Let the number of seed songs for an experiment be . [sent-192, score-0.572]
</p><p>86 Each trial chose a playlist at random (out of the playlists that consisted of at least songs), then chose songs at random out of the playlist as a training set. [sent-194, score-1.807]
</p><p>87 The test set of each trial consisted of all of the remaining songs in the playlist, plus all other songs owned by the designer of the playlist. [sent-195, score-0.876]
</p><p>88 This test set thus emulates the possible songs available to the playlist generator. [sent-196, score-1.019]
</p><p>89 This score is summed over all 1000 trials, and normalized:          c    ¥      (12)     ¥¥¥ ¨¨¤ § © ¨§¦¤ § ¥¥¥ § ¨¥   § ¨¥   E 5E    §  ¢      ¡     §    where is the score from (11) if that playlist were perfect (i. [sent-199, score-0.636]
</p><p>90 , all of the true playlist songs were at the head of the list). [sent-201, score-0.999]
</p><p>91 First, all of the experimental systems perform much better than random, so they all capture some notion of playlist generation. [sent-245, score-0.588]
</p><p>92 This is probably due to the work that went into designing the metadata schema. [sent-246, score-0.207]
</p><p>93 This saturation is caused by the fact that exact playlists are hard to predict: there are many appropriate songs that would be valid in a test playlist, even if the user did not choose those songs. [sent-251, score-0.893]
</p><p>94 Qualitative results of the playlist generator are shown in Table 3. [sent-255, score-0.615]
</p><p>95 In that table, two different Eagles songs are selected as single seed songs, and the top 5 playlist songs are shown. [sent-256, score-1.571]
</p><p>96 The seed song is always ﬁrst in the playlist and is not repeated. [sent-257, score-0.851]
</p><p>97 The seed song on the left is softer and leads to a softer playlist, while the seed song on the right is harder rock and leads to a more hard rock play list. [sent-258, score-0.632]
</p><p>98 We have applied KMT to create AutoDJ, which is a system for automatically generating music playlists. [sent-261, score-0.222]
</p><p>99 Experiments with music playlist generation show that KMT leads to better results than a hand-built kernel when the number of training examples is small. [sent-263, score-0.979]
</p><p>100 Learning and revising user proﬁles: The identiﬁcation of interesting web sites. [sent-344, score-0.321]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('playlist', 0.588), ('songs', 0.411), ('user', 0.321), ('kmt', 0.23), ('gpr', 0.207), ('metadata', 0.207), ('autodj', 0.196), ('music', 0.182), ('seed', 0.161), ('playlists', 0.161), ('kernel', 0.144), ('song', 0.102), ('preference', 0.101), ('eagles', 0.058), ('categorical', 0.051), ('album', 0.046), ('albums', 0.046), ('preferences', 0.046), ('mercer', 0.044), ('gp', 0.04), ('frobenius', 0.04), ('generation', 0.037), ('listen', 0.036), ('rap', 0.035), ('rhythm', 0.034), ('similarity', 0.033), ('collaborative', 0.032), ('ned', 0.032), ('table', 0.032), ('learned', 0.031), ('mood', 0.03), ('genre', 0.03), ('rock', 0.03), ('covariance', 0.029), ('training', 0.028), ('generator', 0.027), ('cf', 0.024), ('score', 0.024), ('generate', 0.024), ('learn', 0.023), ('blend', 0.023), ('coast', 0.023), ('gpc', 0.023), ('owned', 0.023), ('professionally', 0.023), ('sad', 0.023), ('softer', 0.023), ('subgenre', 0.023), ('variogram', 0.023), ('vocal', 0.023), ('hamming', 0.023), ('kernels', 0.023), ('microsoft', 0.022), ('generating', 0.022), ('norm', 0.022), ('selects', 0.021), ('quadratic', 0.021), ('ranked', 0.021), ('generates', 0.021), ('samples', 0.021), ('gaussian', 0.02), ('consonant', 0.02), ('boldface', 0.02), ('emulates', 0.02), ('minka', 0.02), ('nips', 0.019), ('generalize', 0.019), ('matrix', 0.019), ('vectors', 0.019), ('les', 0.019), ('users', 0.019), ('de', 0.019), ('functions', 0.018), ('automatically', 0.018), ('love', 0.018), ('metric', 0.018), ('th', 0.018), ('designed', 0.017), ('operate', 0.017), ('elds', 0.017), ('volume', 0.017), ('prior', 0.017), ('sites', 0.017), ('wants', 0.017), ('tting', 0.017), ('list', 0.017), ('trial', 0.017), ('pro', 0.017), ('drawn', 0.016), ('inverting', 0.016), ('decided', 0.016), ('empirical', 0.015), ('process', 0.015), ('empirically', 0.015), ('personal', 0.015), ('consisted', 0.014), ('program', 0.014), ('christopher', 0.014), ('prototype', 0.014), ('creates', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="113-tfidf-1" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>2 0.162614 <a title="113-tfidf-2" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>3 0.12360072 <a title="113-tfidf-3" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>Author: Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor</p><p>Abstract: With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1</p><p>4 0.10433849 <a title="113-tfidf-4" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>Author: Charles Lee Isbell Jr., Christian R. Shelton</p><p>Abstract: We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiﬁng his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 1</p><p>5 0.090403311 <a title="113-tfidf-5" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>6 0.077990085 <a title="113-tfidf-6" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>7 0.072354361 <a title="113-tfidf-7" href="./nips-2001-Tempo_tracking_and_rhythm_quantization_by_sequential_Monte_Carlo.html">179 nips-2001-Tempo tracking and rhythm quantization by sequential Monte Carlo</a></p>
<p>8 0.065920994 <a title="113-tfidf-8" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>9 0.064217865 <a title="113-tfidf-9" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>10 0.064165242 <a title="113-tfidf-10" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>11 0.061605144 <a title="113-tfidf-11" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>12 0.060226802 <a title="113-tfidf-12" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>13 0.057151016 <a title="113-tfidf-13" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>14 0.051633127 <a title="113-tfidf-14" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>15 0.050956789 <a title="113-tfidf-15" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>16 0.047685485 <a title="113-tfidf-16" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>17 0.046683699 <a title="113-tfidf-17" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>18 0.043489516 <a title="113-tfidf-18" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>19 0.040729873 <a title="113-tfidf-19" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>20 0.040721703 <a title="113-tfidf-20" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.124), (1, 0.046), (2, -0.022), (3, -0.083), (4, 0.019), (5, 0.041), (6, -0.002), (7, 0.048), (8, -0.058), (9, 0.068), (10, 0.049), (11, -0.08), (12, -0.128), (13, -0.072), (14, -0.047), (15, 0.099), (16, 0.163), (17, 0.152), (18, -0.13), (19, 0.045), (20, 0.072), (21, -0.001), (22, -0.102), (23, -0.011), (24, -0.218), (25, 0.037), (26, 0.042), (27, -0.209), (28, -0.127), (29, -0.119), (30, -0.001), (31, -0.045), (32, 0.01), (33, -0.07), (34, 0.006), (35, 0.097), (36, -0.04), (37, 0.049), (38, 0.081), (39, 0.092), (40, -0.175), (41, 0.003), (42, 0.055), (43, 0.031), (44, 0.003), (45, -0.001), (46, 0.08), (47, 0.07), (48, -0.05), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91435516 <a title="113-lsi-1" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>2 0.69568604 <a title="113-lsi-2" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>Author: Neil D. Lawrence, Antony I. T. Rowstron, Christopher M. Bishop, Michael J. Taylor</p><p>Abstract: With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. ill this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach. 1</p><p>3 0.67985487 <a title="113-lsi-3" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>Author: Charles Lee Isbell Jr., Christian R. Shelton</p><p>Abstract: We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiﬁng his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment. 1</p><p>4 0.54858732 <a title="113-lsi-4" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>5 0.36950654 <a title="113-lsi-5" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>Author: Judy A. Franklin</p><p>Abstract: This article presents a 2-phase computational learning model and application. As a demonstration, a system has been built, called CHIME for Computer Human Interacting Musical Entity. In phase 1 of training, recurrent back-propagation trains the machine to reproduce 3 jazz melodies. The recurrent network is expanded and is further trained in phase 2 with a reinforcement learning algorithm and a critique produced by a set of basic rules for jazz improvisation. After each phase CHIME can interactively improvise with a human in real time. 1 Foundations Jazz improvisation is the creation of a jazz melody in real time. Charlie Parker, Dizzy Gillespie, Miles Davis, John Coltrane, Charles Mingus, Thelonious Monk, and Sonny Rollins et al. were the founders of bebop and post bop jazz [9] where drummers, bassists, and pianists keep the beat and maintain harmonic structure. Other players improvise over this structure and even take turns improvising for 4 bars at a time. This is called trading fours. Meanwhile, artiﬁcial neural networks have been used in computer music [4, 12]. In particular, the work of (Todd [11]) is the basis for phase 1 of CHIME, a novice machine improvisor that learns to trade fours. Firstly, a recurrent network is trained with back-propagation to play three jazz melodies by Sonny Rollins [1], as described in Section 2. Phase 2 uses actor-critic reinforcement learning and is described in Section 3. This section is on jazz basics. 1.1 Basics: Chords, the ii-V-I Chord Progression and Scales The harmonic structure mentioned above is a series of chords that may be reprated and that are often grouped into standard subsequences. A chord is a group of notes played simultaneously. In the chromatic scale, C-Db-D-Eb-E-F-Gb-G-Ab-A-Bb-B-C, notes are separated by a half step. A ﬂat (b) note is a half step below the original note; a sharp (#) is a half above. Two half steps are a whole step. Two whole steps are a major third. Three half steps are a minor third. A major triad (chord) is the ﬁrst or tonic note, then the note a major third up, then the note a minor third up. When F is the tonic, F major triad is F-A-C. A minor triad (chord) is the tonic ¡ www.cs.smith.edu/˜jfrankli then a minor third, then a major third. F minor triad is F-Ab-C. The diminished triad is the tonic, then a minor third, then a minor third. F diminished triad is F-Ab-Cb. An augmented triad is the tonic, then a major third, then a major third. The F augmented triad is F-A-Db. A third added to the top of a triad forms a seventh chord. A major triad plus a major third is the major seventh chord. F-A-C-E is the F major seventh chord (Fmaj7). A minor triad plus a minor third is a minor seventh chord. For F it is F-Ab-C-Eb (Fm7). A major triad plus a minor third is a dominant seventh chord. For F it is F-A-C-Eb (F7). These three types of chords are used heavily in jazz harmony. Notice that each note in the chromatic scales can be the tonic note for any of these types of chords. A scale, a subset of the chromatic scale, is characterized by note intervals. Let W be a whole step and H be a half. The chromatic scale is HHHHHHHHHHHH. The major scale or ionian mode is WWHWWWH. F major scale is F-G-A-Bb-C-D-E-F. The notes in a scale are degrees; E is the seventh degree of F major. The ﬁrst, third, ﬁfth, and seventh notes of a major scale are the major seventh chord. The ﬁrst, third, ﬁfth, and seventh notes of other modes produce the minor seventh and dominant seventh chords. Roman numerals represent scale degrees and their seventh chords. Upper case implies major or dominant seventh and lower case implies minor seventh [9]. The major seventh chord starting at the scale tonic is the I (one) chord. G is the second degree of F major, and G-Bb-D-F is Gm7, the ii chord, with respect to F. The ii-V-I progression is prevalent in jazz [9], and for F it is Gm7-C7-Fmaj7. The minor ii-V-i progression is obtained using diminished and augmented triads, their seventh chords, and the aeolian mode. Seventh chords can be extended by adding major or minor thirds, e.g. Fmaj9, Fmaj11, Fmaj13, Gm9, Gm11, and Gm13. Any extension can be raised or lowered by 1 step [9] to obtain, e.g. Fmaj7#11, C7#9, C7b9, C7#11. Most jazz compositions are either the 12 bar blues or sectional forms (e.g. ABAB, ABAC, or AABA) [8]. The 3 Rollins songs are 12 bar blues. “Blue 7” has a simple blues form. In “Solid” and “Tenor Madness”, Rollins adds bebop variations to the blues form [1]. ii-V-I and VI-II-V-I progressions are added and G7+9 substitutes for the VI and F7+9 for the V (see section 1.2 below); the II-V in the last bar provides the turnaround to the I of the ﬁrst bar to foster smooth repetition of the form. The result is at left and in Roman numeral notation Bb7 Bb7 Bb7 Bb7 I I I I Eb7 Eb7 Bb7 G7+9 IV IV I VI at right: Cm7 F7 Bb7 G7+9 C7 F7+9 ii V I VI II V 1.2 Scale Substitutions and Rules for Reinforcement Learning First note that the theory and rules derived in this subsection are used in Phase 2, to be described in Section 3. They are presented here since they derive from the jazz basics immediately preceding. One way a novice improvisor can play is to associate one scale with each chord and choose notes from that scale when the chord is presented in the musical score. Therefore, Rule 1 is that an improvisor may choose notes from a “standard” scale associated with a chord. Next, the 4th degree of the scale is often avoided on a major or dominant seventh chord (Rule 3), unless the player can resolve its dissonance. The major 7th is an avoid note on a dominant seventh chord (Rule 4) since a dominant seventh chord and its scale contain the ﬂat 7th, not the major 7th. Rule 2 contains many notes that can be added. A brief rationale is given next. The C7 in Gm7-C7-Fmaj7 may be replaced by a C7#11, a C7+ chord, or a C7b9b5 or C7alt chord [9]. The scales for C7+ and C7#11 make available the raised fourth (ﬂat 5), and ﬂat 6 (ﬂat 13) for improvising. The C7b9b5 and C7alt (C7+9) chords and their scales make available the ﬂat9, raised 9, ﬂat5 and raised 5 [1]. These substitutions provide the notes of Rule 2. These rules (used in phase 2) are stated below, using for reinforcement values very bad (-1.0), bad (-0.5), a little bad (-0.25), ok (0.25), good (0.5), and very good (1.0). The rules are discussed further in Section 4. The Rule Set: 1) Any note in the scale associated with the chord is ok (except as noted in rule 3). 2) On a dominant seventh, hip notes 9, ﬂat9, #9, #11, 13 or ﬂat13 are very good. One hip note 2 times in a row is a little bad. 2 hip notes more than 2 times in a row is a little bad. 3) If the chord is a dominant seventh chord, a natural 4th note is bad. 4) If the chord is a dominant seventh chord, a natural 7th is very bad. 5) A rest is good unless it is held for more than 2 16th notes and then it is very bad. 6) Any note played longer than 1 beat (4 16th notes) is very bad. 7) If two consecutive notes match the human’s, that is good. 2 CHIME Phase 1 In Phase 1, supervised learning is used to train a recurrent network to reproduce the three Sonny Rollins melodies. 2.1 Network Details and Training The recurrent network’s output units are linear. The hidden units are nonlinear (logistic function). Todd [11] used a Jordan recurrent network [6] for classical melody learning and generation. In CHIME, a Jordan net is also used, with the addition of the chord as input (Figure 1. 24 of the 26 outputs are notes (2 chromatic octaves), the 25th is a rest, and the 26th indicates a new note. The output with the highest value above a threshold is the next note, including the rest output. The new note output indicates if this is a new note, or if it is the same note being held for another time step ( note resolution). ¥£ ¡ ¦¤¢  The 12 chord inputs (12 notes in a chromatic scale), are 1 or 0. A chord is represented as its ﬁrst, third, ﬁfth, and seventh notes and it “wraps around” within the 12 inputs. E.g., the Fm7 chord F-Ab-C-Eb is represented as C, Eb, F, Ab or 100101001000. One plan input per song enables distinguishing between songs. The 26 context inputs use eligibility traces, giving the hidden units a decaying history of notes played. CHIME (as did Todd) uses teacher forcing [13], wherein the target outputs for the previous step are used as inputs (so erroneous outputs are not used as inputs). Todd used from 8 to 15 hidden units; CHIME uses 50. The learning rate is 0.075 (Todd used 0.05). The eligibility rate is 0.9 (Todd used 0.8). Differences in values perhaps reﬂect contrasting styles of the songs and available computing power. Todd used 15 output units and assumed a rest when all note units are “turned off.” CHIME uses 24 output note units (2 octaves). Long rests in the Rollins tunes require a dedicated output unit for a rest. Without it, the note outputs learned to turn off all the time. Below are results of four representative experiments. In all experiments, 15,000 presentations of the songs were made. Each song has 192 16th note events. All songs are played at a ﬁxed tempo. Weights are initialized to small random values. The squared error is the average squared error over one complete presentation of the song. “Finessing” the network may improve these values. The songs are easily recognized however, and an exact match could impair the network’s ability to improvise. Figure 2 shows the results for “Solid.” Experiment 1. Song: Blue Seven. Squared error starts at 185, decreases to 2.67. Experiment 2. Song: Tenor Madness. Squared error starts at 218, decreases to 1.05. Experiment 3. Song: Solid. Squared error starts at 184, decreases to 3.77. Experiment 4. Song: All three songs: Squared error starts at 185, decreases to 36. Figure 1: Jordan recurrent net with addition of chord input 2.2 Phase 1 Human Computer Interaction in Real Time In trading fours with the trained network, human note events are brought in via the MIDI interface [7]. Four bars of human notes are recorded then given, one note event at a time to the context inputs (replacing the recurrent inputs). The plan inputs are all 1. The chord inputs follow the “Solid” form. The machine generates its four bars and they are played in real time. Then the human plays again, etc. An accompaniment (drums, bass, and piano), produced by Band-in-a-Box software (PG Music), keeps the beat and provides chords for the human. Figure 3 shows an interaction. The machine’s improvisations are in the second and fourth lines. In bar 5 the ﬂat 9 of the Eb7 appears; the E. This note is used on the Eb7 and Bb7 chords by Rollins in “Blue 7”, as a “passing tone.” D is played in bar 5 on the Eb7. D is the natural 7 over Eb7 (with its ﬂat 7) but is a note that Rollins uses heavily in all three songs, and once over the Eb7. It may be a response to the rest and the Bb played by the human in bar 1. D follows both a rest and a Bb in many places in “Tenor Madness” and “Solid.” In bar 6, the long G and the Ab (the third then fourth of Eb7) ﬁgure prominently in “Solid.” At the beginning of bar 7 is the 2-note sequence Ab-E that appears in exactly the same place in the song “Blue 7.” The focus of bars 7 and 8 is jumping between the 3rd and 4th of Bb7. At the end of bar 8 the machine plays the ﬂat 9 (Ab) then the ﬂat 3 (Bb), of G7+9. In bars 13-16 the tones are longer, as are the human’s in bars 9-12. The tones are the 5th, the root, the 3rd, the root, the ﬂat 7, the 3rd, the 7th, and the raised fourth. Except for the last 2, these are chord tones. 3 CHIME Phase 2 In Phase 2, the network is expanded and trained by reinforcement learning to improvise according to the rules of Section 1.2 and using its knowledge of the Sonny Rollins songs. 3.1 The Expanded Network Figure 4 shows the phase 2 network. The same inputs plus 26 human inputs brings the total to 68. The weights obtained in phase 1 initialize this network. The plan and chord weights Figure 2: At left “Solid” played by a human; at right the song reproduced by the ANN. are the same. The weights connecting context units to the hidden layer are halved. The same weights, halved, connect the 26 human inputs to the hidden layer. Each output unit gets the 100 hidden units’ outputs as input. The original 50 weights are halved and used as initial values of the two sets of 50 hidden unit weights to the output unit. 3.2 SSR and Critic Algorithms Using actor-critic reinforcement learning ([2, 10, 13]), the actor chooses the next note to play. The critic receives a “raw” reinforcement signal from the critique made by the . A rules of Section 1.2. For output j, the SSR (actor) computes mean Gaussian distribution with mean and standard deviation chooses the output . is generated, the critic modiﬁes and produces . is further modiﬁed by a self-scaling algorithm that tracks, via moving average, the maximum and minimum reinforcement and uses them to scale the signal to produce .</p><p>6 0.35788506 <a title="113-lsi-6" href="./nips-2001-A_Bayesian_Network_for_Real-Time_Musical_Accompaniment.html">6 nips-2001-A Bayesian Network for Real-Time Musical Accompaniment</a></p>
<p>7 0.30529371 <a title="113-lsi-7" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>8 0.30201423 <a title="113-lsi-8" href="./nips-2001-Tempo_tracking_and_rhythm_quantization_by_sequential_Monte_Carlo.html">179 nips-2001-Tempo tracking and rhythm quantization by sequential Monte Carlo</a></p>
<p>9 0.29784873 <a title="113-lsi-9" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>10 0.28761116 <a title="113-lsi-10" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>11 0.25955585 <a title="113-lsi-11" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>12 0.25081828 <a title="113-lsi-12" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>13 0.23766463 <a title="113-lsi-13" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>14 0.23543608 <a title="113-lsi-14" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>15 0.23478052 <a title="113-lsi-15" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>16 0.22140558 <a title="113-lsi-16" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>17 0.21718097 <a title="113-lsi-17" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>18 0.21560705 <a title="113-lsi-18" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>19 0.21324483 <a title="113-lsi-19" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>20 0.18948002 <a title="113-lsi-20" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.045), (17, 0.024), (19, 0.014), (26, 0.424), (27, 0.072), (30, 0.062), (59, 0.032), (72, 0.066), (79, 0.039), (83, 0.014), (91, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7619729 <a title="113-lda-1" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>2 0.3441968 <a title="113-lda-2" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>3 0.34405231 <a title="113-lda-3" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>Author: Dieter Fox</p><p>Abstract: Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique.</p><p>4 0.34209073 <a title="113-lda-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>5 0.34125942 <a title="113-lda-5" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>Author: Michael Collins, Nigel Duffy</p><p>Abstract: We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.</p><p>6 0.34117144 <a title="113-lda-6" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>7 0.34016275 <a title="113-lda-7" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>8 0.34013581 <a title="113-lda-8" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>9 0.33978033 <a title="113-lda-9" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>10 0.33880541 <a title="113-lda-10" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>11 0.33838868 <a title="113-lda-11" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>12 0.33811045 <a title="113-lda-12" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>13 0.33750334 <a title="113-lda-13" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>14 0.33714437 <a title="113-lda-14" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>15 0.3371366 <a title="113-lda-15" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>16 0.33680397 <a title="113-lda-16" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>17 0.33532587 <a title="113-lda-17" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>18 0.33478647 <a title="113-lda-18" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>19 0.33471507 <a title="113-lda-19" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>20 0.33471018 <a title="113-lda-20" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
