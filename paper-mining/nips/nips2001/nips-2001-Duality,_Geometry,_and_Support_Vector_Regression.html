<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2001-Duality, Geometry, and Support Vector Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-62" href="#">nips2001-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2001-Duality, Geometry, and Support Vector Regression</h1>
<br/><p>Source: <a title="nips-2001-62-pdf" href="http://papers.nips.cc/paper/2132-duality-geometry-and-support-vector-regression.pdf">pdf</a></p><p>Author: J. Bi, Kristin P. Bennett</p><p>Abstract: We develop an intuitive geometric framework for support vector regression (SVR). By examining when -tubes exist, we show that SVR can be regarded as a classiﬁcation problem in the dual space. Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the eﬀective -tube. In the proposed approach the eﬀects of the choices of all parameters become clear geometrically. 1</p><p>Reference: <a title="nips-2001-62-reference" href="../nips2001_reference/nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We develop an intuitive geometric framework for support vector regression (SVR). [sent-4, score-0.257]
</p><p>2 By examining when -tubes exist, we show that SVR can be regarded as a classiﬁcation problem in the dual space. [sent-5, score-0.3]
</p><p>3 Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . [sent-6, score-1.529]
</p><p>4 A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. [sent-7, score-0.469]
</p><p>5 Maximizing the margin corresponds to shrinking the eﬀective -tube. [sent-8, score-0.116]
</p><p>6 Intuitive geometric formulations exist for the classiﬁcation case addressing both the error metric and capacity control [1, 2]. [sent-11, score-0.182]
</p><p>7 For linearly separable classiﬁcation, the primal SVM ﬁnds the separating plane with maximum hard margin between two sets. [sent-12, score-0.94]
</p><p>8 The equivalent dual SVM computes the closest points in the convex hulls of the data from each class. [sent-13, score-1.111]
</p><p>9 For the inseparable case, the primal SVM optimizes the soft margin of separation between the two classes. [sent-14, score-0.448]
</p><p>10 The corresponding dual SVM ﬁnds the closest points in the reduced convex hulls. [sent-15, score-0.819]
</p><p>11 From the primal perspective, a linear function with no residuals greater than corresponds to an -tube constructed about the data in the space of the data attributes and the response variable [6] (see e. [sent-18, score-0.279]
</p><p>12 The primary contribution of this work is a novel geometric interpretation of SVR from the dual perspective along with a mathematically rigorous derivation of the geometric concepts. [sent-21, score-0.453]
</p><p>13 With duality analysis, the existence of a hard -tube depends on the separability of two sets. [sent-24, score-0.346]
</p><p>14 The two sets consist of the training data augmented with the response variable shifted up and down by . [sent-25, score-0.204]
</p><p>15 In the dual space, regression becomes the classiﬁcation problem of distinguishing between these two sets. [sent-26, score-0.339]
</p><p>16 The geometric formulations developed for the classiﬁcation case [1] become applicable to the regression case. [sent-27, score-0.209]
</p><p>17 We call the resulting formulation convex SVR (C-SVR) since it is based on convex hulls of the augmented training data. [sent-28, score-1.052]
</p><p>18 Much like in SVM classiﬁcation, to compute a hard -tube, C-SVR computes the nearest points in the convex hulls of the augmented classes. [sent-29, score-1.049]
</p><p>19 The corresponding maximum margin (max-margin) planes deﬁne the eﬀective -tube. [sent-30, score-0.168]
</p><p>20 Similarly, to compute a soft -tube, reduced-convex SVR (RC-SVR) ﬁnds the closest points in the reduced convex hulls of the two augmented sets. [sent-32, score-1.153]
</p><p>21 y  y  y  D  ε  y  +  D  +  D  ε  D  -  x  D-  D-  (b)  (a)  +  (d)  (c) x  x  x  Figure 1: The (a) primal hard 0-tube, and dual cases: (b) dual strictly separable (c) dual separable = 0 , and (d) dual inseparable < 0. [sent-42, score-1.617]
</p><p>22 >  0,  SVR constructs a regression model that minimizes some empirical risk measure regularized to control capacity. [sent-43, score-0.197]
</p><p>23 Plotting the points in (x, y) space as in Figure 1(a), we see that for a “perfect” regression model the data fall in a hard -tube about the regression line. [sent-46, score-0.453]
</p><p>24 Let (Xi , yi) be an example where i = 1, 2, · · · , m, Xi is the ith predictor vector, and yi is its response. [sent-47, score-0.15]
</p><p>25 A hard -tube for a ﬁxed > 0 is deﬁned as a plane y = w x + b satisfying − e ≤ y − Xw − be ≤ e where e is an m-dimensional vector of ones. [sent-49, score-0.532]
</p><p>26 Clearly, for  large enough such a tube always  exists for ﬁnite data. [sent-51, score-0.328]
</p><p>27 − e ≤ y − Xw − be ≤ e  (1)  Note that the smallest tube is typically not the -SVR solution. [sent-54, score-0.298]
</p><p>28 D + = {(Xi , yi + ), i = 1, · · · , m} and D − = {(Xi , yi − ), i = 1, · · · , m}. [sent-57, score-0.248]
</p><p>29 For any ﬁxed > 0, there are three possible cases: > 0 in which strict hard -tubes exist, = 0 in which only 0-tubes exist, and < 0 in which no hard -tubes exist. [sent-59, score-0.423]
</p><p>30 A strict hard -tube with no points on the edges of the tube only exists for > 0 . [sent-60, score-0.655]
</p><p>31 Figure 1(b-d) illustrates what happens in the dual space for each case. [sent-61, score-0.238]
</p><p>32 The convex hulls of D+ and D− are drawn along with the max-margin plane in (b) and the supporting plane in (c) for separating the convex hulls. [sent-62, score-1.834]
</p><p>33 Clearly, the existence of the tube is directly related to the separability of D + and D− . [sent-63, score-0.356]
</p><p>34 If > 0 then a strict tube exists and the convex hulls of D + and D− are strictly separable1 . [sent-64, score-1.104]
</p><p>35 One can see that the max-margin plane separating D + and D− corresponds to one such . [sent-66, score-0.478]
</p><p>36 In fact this plane forms an ˆ tube where > ˆ ≥ 0 . [sent-67, score-0.609]
</p><p>37 If = 0, then the convex hulls of D+ and D− are separable but not strictly separable. [sent-68, score-0.815]
</p><p>38 The plane that separates the two convex hulls forms the 0 tube. [sent-69, score-1.04]
</p><p>39 It is easy to show by construction that if a hard -tube exists for a given > 0 then the convex hulls of D + and D− will be separable. [sent-72, score-0.922]
</p><p>40 If a hard -tube exists, then there exists (w, b) such that (y + e) − Xw − be ≥ 0,  (y − e) − Xw − be ≤ 0. [sent-73, score-0.251]
</p><p>41 (2)  X For any convex combination of D + , (y+ e) u where e u = 1, u ≥ 0 of points (Xi , yi + ), i = 1, 2, · · · , m, we have (y + e) u − w (X u) − b ≥ 0. [sent-74, score-0.505]
</p><p>42 Similarly for X D− , (y− e) v where e v = 1, v ≥ 0 of points (Xi , yi − ), i = 1, 2, · · · , m, we have (y − e) v − w (X v) − b ≤ 0. [sent-75, score-0.187]
</p><p>43 Then the plane y = w x + b in the -tube separates the two convex hulls. [sent-76, score-0.687]
</p><p>44 Note the separating plane and the -tube plane are the same. [sent-77, score-0.789]
</p><p>45 If no separating plane exists, then there is no tube. [sent-78, score-0.445]
</p><p>46 1 (Conditions for existence of hard -tube) A hard -tube exists for a given > 0 if and only if the following system in (u, v) has no solution: X u = X v, e u = e v = 1, (y + e) u − (y − e) v < 0, u ≥ 0, v ≥ 0. [sent-81, score-0.48]
</p><p>47 (3)  Proof A hard -tube exists if and only if System (2) has a solution. [sent-82, score-0.251]
</p><p>48 1  We use the following deﬁnitions of separation of convex sets. [sent-85, score-0.318]
</p><p>49 A plane H = {x : w x = α} is said to separate D + and D− if w x ≥ α, ∀x ∈ D + and w x ≤ α, ∀x ∈ D − . [sent-87, score-0.372]
</p><p>50 So as a consequence of this theorem, if D+ and D− are separable, then a hard -tube exists. [sent-92, score-0.188]
</p><p>51 In hard-margin SVM classiﬁcation, the dual SVM formulation constructs the max-margin plane by ﬁnding the two nearest points in the convex hulls of the two classes. [sent-99, score-1.422]
</p><p>52 The max-margin plane is the plane bisecting these two points. [sent-100, score-0.688]
</p><p>53 We know that the existence of the tube is linked to the separability of the shifted sets, D + and D− . [sent-101, score-0.457]
</p><p>54 The key insight is that the regression problem can be regarded as a classiﬁcation problem between D + and D− . [sent-102, score-0.138]
</p><p>55 The only signiﬁcant diﬀerence occurs along the y dimension as the response variable y is shifted up by in D + and down by in D− . [sent-104, score-0.167]
</p><p>56 For > 0, the max-margin separating plane corresponds to a hard ˆ-tube where > ˆ ≥ 0. [sent-105, score-0.666]
</p><p>57 The resulting tube is smaller than but not necessarily the smallest tube. [sent-106, score-0.326]
</p><p>58 Figure 1(b) shows the max-margin plane found for > 0 . [sent-107, score-0.344]
</p><p>59 As in classiﬁcation, we will have a hard and soft -tube case. [sent-109, score-0.344]
</p><p>60 The soft -tube with ≤ 0 is used to obtain good generalization when there are outliers. [sent-110, score-0.156]
</p><p>61 1  The hard -tube case  We now apply the dual convex hull method to constructing the max-margin plane for our augmented sets D + and D− assuming they are strictly separable, i. [sent-112, score-1.332]
</p><p>62 The closest points of D + and D− can be found by solving the following dual C-SVR quadratic program: min u,v  s. [sent-116, score-0.456]
</p><p>63 1 2  X (y+ e)  u−  X (y− e)  v  2  Let the closest points in the convex hulls of D + and D− be c = d =  (4)  e u = 1, e v = 1, u ≥ 0, v ≥ 0. [sent-118, score-0.848]
</p><p>64 The max-margin separating plane bisects these two ˆ points. [sent-120, score-0.487]
</p><p>65 The normal (w, δ) of the plane is the diﬀerence between them, i. [sent-121, score-0.344]
</p><p>66 The threshold, ˆ is the distance from the X u − X v, δ ˆ ˆ ˆ ˆ b, origin to the point halfway between the two closest points along the normal: ˆ = b ˆ ˆ X u +X v ˆ ˆ ˆ y u+y v . [sent-124, score-0.203]
</p><p>67 The separating plane has the equation w x+ δy−ˆ = 0. [sent-125, score-0.445]
</p><p>68 ˆ b w ˆ +δ ˆ 2 2 Rescaling this plane yields the regression function. [sent-126, score-0.445]
</p><p>69 Dual C-SVR (4) can be derived by taking the Wolfe or Lagrangian dual [4] of primal C-SVR (5) and simplifying. [sent-137, score-0.414]
</p><p>70 We prove that the optimal plane from C-SVR bisects the ˆ tube. [sent-138, score-0.386]
</p><p>71 The supporting planes for class D + and class D− determines the lower and upper edges of the ˆ-tube respectively. [sent-139, score-0.213]
</p><p>72 The support vectors from D + and D− correspond to the points along the lower and upper edges of the ˆ-tube. [sent-140, score-0.152]
</p><p>73 1 (C-SVR constructs ˆ-tube) Let the max-margin plane obtained ˆ b by C-SVR (4) be w x+δy−ˆ = 0 where w = X u−X v, δ = (y+ e) u−(y− e) v, and ˆ ˆ ˆ ˆ ˆ ˆ ˆ y u+y v ˆ ˆ ˆ ˆ ˆ = w X u+X v + δ ˆ b ˆ . [sent-143, score-0.411]
</p><p>74 If > 0, then the plane y = w x + b corresponds 2 2 ˆ to an ˆ-tube of training data (Xi , yi), i = 1, 2, · · · , m where w = − w , b = ˆ δ α−β ˆ ˆ ˆ 2δ  ˆ= −  ˆ b ˆ δ  and  < . [sent-144, score-0.377]
</p><p>75 By the Wolfe duality theorem [4], α − β > 0, ˆ since the objective values of (5) and the negative objective value of (4) are equal at optimality. [sent-146, score-0.162]
</p><p>76 By complementarity, the closest points are right on the margin planes ˆ ˆ ˆ w x + δy − α = 0 and w x + δy − β = 0 respectively, so α = w X u + δ(y + e) u and ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ α+β ˆ ˆ ˆ = w X v + δ(y− e) v. [sent-147, score-0.345]
</p><p>77 Hence the plane y = w x + b is in the middle of the ˆ < tube. [sent-158, score-0.344]
</p><p>78 2  The soft -tube case  For < 0 , a hard -tube does not exist. [sent-160, score-0.344]
</p><p>79 In soft-margin classiﬁcation, outliers were handled in the  y    ¨¦¤ £ © § ¥ ^ 2ε     ¡ ¢         x  Figure 3: Soft ˆ-tube found by RC-SVR: left: dual, right: primal space. [sent-162, score-0.203]
</p><p>80 The same strategy works for soft -tubes, see Figure 3. [sent-164, score-0.156]
</p><p>81 Instead of taking the full convex hulls of D + and D− , we reduce the convex hulls away from the diﬃcult boundary cases. [sent-165, score-1.342]
</p><p>82 RC-SVR computes the closest points in the reduced convex hulls u,v  s. [sent-166, score-0.959]
</p><p>83 Parameter D determines the robustness of the solution by reducing the convex hull. [sent-169, score-0.349]
</p><p>84 In this example, every point in the m reduced convex hull must depend on at least two data points since i=1 ui = 1 and 0 ≤ ui ≤ 1/2. [sent-174, score-0.806]
</p><p>85 In general, every point in the reduced convex hull can be written as the convex combination of at least 1/D = ν ∗ m . [sent-175, score-0.845]
</p><p>86 Since these points are exactly the support vectors and there are two reduced convex hulls, 2 ∗ νm is a lower bound on the number of support vectors in RC-SVR. [sent-176, score-0.535]
</p><p>87 By choosing ν suﬃciently large, the inseparable case with ≤ 0 is transformed into a separable case where once again our nearest-points-in-the-convex-hull-problem is well deﬁned. [sent-177, score-0.181]
</p><p>88 As in classiﬁcation, the dual reduced convex hull problem corresponds to computing a soft -tube in the primal space. [sent-178, score-1.13]
</p><p>89 Consider the following soft tube version of the primal C-SVR (7) which has its Wolfe Dual RC-SVR (6): 2  min  1 2  s. [sent-179, score-0.638]
</p><p>90 2 (RC-SVR constructs soft ˆ-tube) Let the soft max-margin ˆ plane obtained by RC-SVR (6) be w x + δy − ˆ = 0 where w = X u − X v, ˆ b ˆ ˆ ˆ y u+y v ˆ ˆ ˆ X u+X v ˆ ˆ ˆ δ = (y + e) u − (y − e) v, and ˆ = ˆ ˆ b w+ ˆ δ. [sent-184, score-0.723]
</p><p>91 If 0 < ≤ 0, then 2 2 ˜  ˜ the plane y = w x + b corresponds to a soft ˆ = − α−β < -tube of training data ˆ 2δ (Xi , yi ), i = 1, 2, · · · , m, i. [sent-185, score-0.657]
</p><p>92 , a ˆ-tube of reduced convex hull of training data where ˆ ˆ b w = − w , b = δ and α = w X u + δ(y + e) u, β = w X v + δ(y − e) v. [sent-187, score-0.527]
</p><p>93 ˜ ˆ ˆ ˆ ˆ ˜ ˆ ˆ ˆ ˆ ˆ ˆ δ  ˜ Notice that the α and β determine the planes parallel to the regression plane and ˜ through the closest points in each reduced convex hull of shifted data. [sent-188, score-1.373]
</p><p>94 In the  inseparable case, these planes are parallel but not necessarily identical to the planes obtained by the primal RC-SVR (7). [sent-189, score-0.521]
</p><p>95 Both RC-SVR and ν-SVR can shrink or grow the tube according to desired robustness. [sent-222, score-0.265]
</p><p>96 5  Conclusion and Discussion  By examining when -tubes exist, we showed that in the dual space SVR can be regarded as a classiﬁcation problem. [sent-224, score-0.3]
</p><p>97 Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . [sent-225, score-1.529]
</p><p>98 We proposed RC-SVR based on choosing the soft max-margin plane between the two shifted datasets. [sent-226, score-0.625]
</p><p>99 The max-margin determines how much the tube can shrink. [sent-228, score-0.296]
</p><p>100 We can show -SVR is equivalent to ﬁnding closest points in a reduced convex hull problem for certain C, but the equivalent problem utilizes a diﬀerent metric in the objective function than RC-SVR. [sent-307, score-0.732]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hulls', 0.353), ('plane', 0.344), ('convex', 0.318), ('tube', 0.265), ('svr', 0.263), ('dual', 0.238), ('xw', 0.209), ('hard', 0.188), ('primal', 0.176), ('soft', 0.156), ('yi', 0.124), ('hull', 0.123), ('planes', 0.123), ('classi', 0.115), ('closest', 0.114), ('ui', 0.108), ('xi', 0.103), ('separating', 0.101), ('shifted', 0.101), ('regression', 0.101), ('svm', 0.1), ('separable', 0.086), ('reduced', 0.086), ('geometric', 0.083), ('inseparable', 0.071), ('cation', 0.069), ('constructs', 0.067), ('mse', 0.067), ('duality', 0.067), ('augmented', 0.063), ('std', 0.063), ('exists', 0.063), ('points', 0.063), ('wolfe', 0.062), ('vi', 0.062), ('strictly', 0.058), ('di', 0.055), ('exist', 0.05), ('separability', 0.05), ('allowable', 0.048), ('gale', 0.048), ('erence', 0.047), ('strict', 0.047), ('xj', 0.047), ('margin', 0.045), ('ective', 0.044), ('bisects', 0.042), ('existence', 0.041), ('min', 0.041), ('response', 0.04), ('theorem', 0.039), ('nearest', 0.039), ('intuitive', 0.039), ('shrinking', 0.038), ('nds', 0.037), ('regarded', 0.037), ('inequalities', 0.036), ('rescaling', 0.035), ('housing', 0.035), ('shrinks', 0.035), ('support', 0.034), ('yj', 0.033), ('geometrically', 0.033), ('corresponds', 0.033), ('smallest', 0.033), ('determines', 0.031), ('constructed', 0.03), ('vol', 0.03), ('bennett', 0.03), ('supporting', 0.03), ('nitely', 0.03), ('risk', 0.029), ('vj', 0.029), ('edges', 0.029), ('objective', 0.028), ('necessarily', 0.028), ('rm', 0.028), ('said', 0.028), ('uj', 0.028), ('outliers', 0.027), ('predictor', 0.026), ('su', 0.026), ('poor', 0.026), ('along', 0.026), ('respectively', 0.026), ('formulations', 0.025), ('examining', 0.025), ('computes', 0.025), ('separates', 0.025), ('erent', 0.025), ('parameter', 0.025), ('choosing', 0.024), ('capacity', 0.024), ('boston', 0.024), ('lkopf', 0.023), ('interpretation', 0.023), ('perfect', 0.023), ('solved', 0.023), ('let', 0.022), ('sch', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="62-tfidf-1" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>Author: J. Bi, Kristin P. Bennett</p><p>Abstract: We develop an intuitive geometric framework for support vector regression (SVR). By examining when -tubes exist, we show that SVR can be regarded as a classiﬁcation problem in the dual space. Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the eﬀective -tube. In the proposed approach the eﬀects of the choices of all parameters become clear geometrically. 1</p><p>2 0.21391572 <a title="62-tfidf-2" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><p>3 0.17969246 <a title="62-tfidf-3" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>4 0.12065418 <a title="62-tfidf-4" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>5 0.11992924 <a title="62-tfidf-5" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>6 0.10517617 <a title="62-tfidf-6" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>7 0.1000285 <a title="62-tfidf-7" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>8 0.095021755 <a title="62-tfidf-8" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>9 0.094307035 <a title="62-tfidf-9" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>10 0.090862848 <a title="62-tfidf-10" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>11 0.088039808 <a title="62-tfidf-11" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>12 0.087852143 <a title="62-tfidf-12" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>13 0.086437188 <a title="62-tfidf-13" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>14 0.084758066 <a title="62-tfidf-14" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>15 0.084455311 <a title="62-tfidf-15" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>16 0.082197458 <a title="62-tfidf-16" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>17 0.080995321 <a title="62-tfidf-17" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>18 0.080065541 <a title="62-tfidf-18" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>19 0.077166751 <a title="62-tfidf-19" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>20 0.07658112 <a title="62-tfidf-20" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.202), (1, 0.147), (2, -0.026), (3, 0.178), (4, 0.099), (5, 0.013), (6, 0.006), (7, -0.052), (8, 0.046), (9, -0.02), (10, 0.084), (11, 0.045), (12, 0.013), (13, 0.08), (14, 0.215), (15, -0.017), (16, -0.065), (17, -0.093), (18, 0.001), (19, 0.086), (20, -0.032), (21, -0.081), (22, -0.024), (23, 0.064), (24, -0.057), (25, -0.036), (26, -0.05), (27, 0.027), (28, 0.051), (29, -0.083), (30, 0.038), (31, -0.102), (32, 0.0), (33, 0.095), (34, 0.025), (35, -0.044), (36, 0.037), (37, -0.012), (38, 0.204), (39, 0.039), (40, -0.145), (41, -0.102), (42, -0.044), (43, -0.116), (44, -0.001), (45, -0.136), (46, -0.161), (47, 0.107), (48, -0.179), (49, -0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97065985 <a title="62-lsi-1" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>Author: J. Bi, Kristin P. Bennett</p><p>Abstract: We develop an intuitive geometric framework for support vector regression (SVR). By examining when -tubes exist, we show that SVR can be regarded as a classiﬁcation problem in the dual space. Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the eﬀective -tube. In the proposed approach the eﬀects of the choices of all parameters become clear geometrically. 1</p><p>2 0.71500731 <a title="62-lsi-2" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><p>3 0.55414903 <a title="62-lsi-3" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>4 0.46209577 <a title="62-lsi-4" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><p>5 0.45565248 <a title="62-lsi-5" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>Author: Pascal Vincent, Yoshua Bengio</p><p>Abstract: Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classiﬁcation tasks. We then propose modiﬁed K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classiﬁcation tasks suggest that the modiﬁed KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs. 1 Motivation The notion of margin for classiﬁcation tasks has been largely popularized by the success of the Support Vector Machine (SVM) [2, 15] approach. The margin of SVMs has a nice geometric interpretation1: it can be deﬁned informally as (twice) the smallest Euclidean distance between the decision surface and the closest training point. The decision surface produced by the original SVM algorithm is the hyperplane that maximizes this distance while still correctly separating the two classes. While the notion of keeping the largest possible safety margin between the decision surface and the data points seems very reasonable and intuitively appealing, questions arise when extending the approach to building more complex, non-linear decision surfaces. Non-linear SVMs usually use the “kernel trick” to achieve their non-linearity. This conceptually corresponds to ﬁrst mapping the input into a higher-dimensional feature space with some non-linear transformation and building a maximum-margin hyperplane (a linear decision surface) there. The “trick” is that this mapping is never computed directly, but implicitly induced by a kernel. In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes inﬁnite dimensional, kernel-induced feature space rather than the original input space. It is less clear whether maximizing the margin in this new space, is meaningful in general (see [16]). 1 for the purpose of this discussion, we consider the original hard-margin SVM algorithm for two linearly separable classes. A different approach is to try and build a non-linear decision surface with maximal distance to the closest data point as measured directly in input space (as proposed in [14]). We could for instance restrict ourselves to a certain class of decision functions and try to ﬁnd the function with maximal margin among this class. But let us take this even further. Extending the idea of building a correctly separating non-linear decision surface as far away as possible from the data points, we deﬁne the notion of local margin as the Euclidean distance, in input space, between a given point on the decision surface and the closest training point. Now would it be possible to ﬁnd an algorithm that could produce a decision surface which correctly separates the classes and such that the local margin is everywhere maximal along its surface? Surprisingly, the plain old Nearest Neighbor algorithm (1NN) [5] does precisely this! So why does 1NN in practice often perform worse than SVMs? One typical explanation, is that it has too much capacity, compared to SVM, that the class of function it can produce is too rich. But, considering it has inﬁnite capacity (VC-dimension), 1NN is still performing quite well... This study is an attempt to better understand what is happening, based on geometrical intuition, and to derive an improved Nearest Neighbor algorithm from this understanding. 2 Fixing a broken Nearest Neighbor algorithm 2.1 Setting and deﬁnitions The setting is that of a classical classiﬁcation problem in (the input space).  wx u  r i q ©</p><p>6 0.44516593 <a title="62-lsi-6" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>7 0.42630783 <a title="62-lsi-7" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>8 0.42201841 <a title="62-lsi-8" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>9 0.40667447 <a title="62-lsi-9" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>10 0.37456325 <a title="62-lsi-10" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>11 0.36602315 <a title="62-lsi-11" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>12 0.3542206 <a title="62-lsi-12" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>13 0.34544826 <a title="62-lsi-13" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>14 0.3353039 <a title="62-lsi-14" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>15 0.31850958 <a title="62-lsi-15" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>16 0.31823769 <a title="62-lsi-16" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>17 0.31660375 <a title="62-lsi-17" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>18 0.31087229 <a title="62-lsi-18" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>19 0.30958229 <a title="62-lsi-19" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>20 0.30704451 <a title="62-lsi-20" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.062), (17, 0.03), (19, 0.037), (20, 0.011), (27, 0.111), (30, 0.063), (38, 0.01), (59, 0.038), (63, 0.307), (72, 0.086), (79, 0.016), (83, 0.024), (91, 0.12)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8503629 <a title="62-lda-1" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>Author: Peter Dayan</p><p>Abstract: The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine, 12,13 into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.</p><p>same-paper 2 0.81858009 <a title="62-lda-2" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>Author: J. Bi, Kristin P. Bennett</p><p>Abstract: We develop an intuitive geometric framework for support vector regression (SVR). By examining when -tubes exist, we show that SVR can be regarded as a classiﬁcation problem in the dual space. Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the eﬀective -tube. In the proposed approach the eﬀects of the choices of all parameters become clear geometrically. 1</p><p>3 0.62747163 <a title="62-lda-3" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>4 0.5841459 <a title="62-lda-4" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><p>5 0.55645466 <a title="62-lda-5" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>6 0.551449 <a title="62-lda-6" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>7 0.54992193 <a title="62-lda-7" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>8 0.548998 <a title="62-lda-8" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>9 0.54816979 <a title="62-lda-9" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>10 0.54794908 <a title="62-lda-10" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>11 0.54767263 <a title="62-lda-11" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>12 0.54712319 <a title="62-lda-12" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>13 0.5459215 <a title="62-lda-13" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>14 0.54582036 <a title="62-lda-14" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>15 0.54551011 <a title="62-lda-15" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>16 0.54515648 <a title="62-lda-16" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>17 0.54348171 <a title="62-lda-17" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>18 0.54343826 <a title="62-lda-18" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>19 0.54322821 <a title="62-lda-19" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>20 0.54303855 <a title="62-lda-20" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
