<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-9" href="#">nips2001-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</h1>
<br/><p>Source: <a title="nips-2001-9-pdf" href="http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf">pdf</a></p><p>Author: Michael Collins, S. Dasgupta, Robert E. Schapire</p><p>Abstract: Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.</p><p>Reference: <a title="nips-2001-9-reference" href="../nips2001_reference/nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. [sent-5, score-0.363]
</p><p>2 This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. [sent-6, score-0.278]
</p><p>3 We describe algorithms for minimizing the loss functions, and give examples on simulated data. [sent-7, score-0.21]
</p><p>4 1 Introduction Principal component analysis (PCA) is a hugely popular dimensionality reduction technique that attempts to ﬁnd a low-dimensional subspace passing close to a given set of . [sent-8, score-0.19]
</p><p>5 More speciﬁcally, in PCA, we ﬁnd a lower dimensional subspace points that minimizes the sum of the squared distances from the data points to their projections in the subspace, i. [sent-9, score-0.621]
</p><p>6 %$%  " %  (1)  This turns out to be equivalent to choosing a subspace that maximizes the sum of the squared lengths of the projections , which is the same as the (empirical) variance of these projections if the data happens to be centered at the origin (so that ). [sent-13, score-0.346]
</p><p>7 In this probabilistic interpretation, each point is thought of as a random draw from some unknown distribution , where denotes a unit Gaussian with mean . [sent-16, score-0.116]
</p><p>8 The purpose then of PCA is to ﬁnd the set of parameters that maximizes the likelihood of the data, subject to the condition that these parameters all lie in a low-dimensional subspace. [sent-17, score-0.158]
</p><p>9 In are considered to be noise-corrupted versions of some true points other words, which lie in a subspace; the goal is to ﬁnd these true points, and the main assumption is that the noise is Gaussian. [sent-18, score-0.156]
</p><p>10 In fact, the Gaussian is only one of the canonical distributions that make up the exponential family, and it is a distribution tailored to real-valued  data. [sent-29, score-0.436]
</p><p>11 It seems natural to consider variants of PCA which are founded upon these other distributions in place of the Gaussian. [sent-31, score-0.184]
</p><p>12 2 ¡  We extend PCA to the rest of the exponential family. [sent-33, score-0.244]
</p><p>13 Let be any parameterized set of distributions from the exponential family, where is the natural parameter of a distribution. [sent-34, score-0.404]
</p><p>14 For instance, a one-dimensional Poisson distribution can be parameterized by , and distribution corresponding to mean Given data , the goal is now to ﬁnd parameters which lie in a low-dimensional subspace and for which the log-likelihood is maximized. [sent-35, score-0.337]
</p><p>15     2  ¤¢ ¥£( ¡  &%  ¡ $ #      £ 8©¦ ¤ £ ¦¨¨¨  ¨ ¦ ©©¦ ¦ ¦ ¢ ¨¨¨  Our uniﬁed approach effortlessly permits hybrid dimensionality reduction schemes in which different types of distributions can be used for different attributes of the data. [sent-42, score-0.159]
</p><p>16 If the data have a few binary attributes and a few integer-valued attributes, then some coordinates of the corresponding can be parameters of binomial distributions while others are parameters of Poisson distributions. [sent-43, score-0.227]
</p><p>17 The dimensionality reduction schemes for non-Gaussian distributions are substantially different from PCA. [sent-46, score-0.092]
</p><p>18 For instance, in PCA the parameters , which are means of Gaussians, lie in a space which coincides with that of the data . [sent-47, score-0.192]
</p><p>19 This is not the case in general, and therefore, although the parameters lie in a linear subspace, they typically correspond to a nonlinear surface in the space of the data. [sent-48, score-0.176]
</p><p>20 The discrepancy and interaction between the space of parameters and the space of the data is a central preoccupation in the study of exponential families, generalized linear models (GLM’s), and Bregman distances. [sent-52, score-0.399]
</p><p>21 In particular, we show that the way in which we generalize PCA is exactly analogous to the manner in which regression is generalized by GLM’s. [sent-54, score-0.184]
</p><p>22 £  We show that the optimization problem we derive can be solved quite naturally by an algorithm that alternately minimizes over the components of the analysis and their coefﬁcients; thus, the algorithm is reminiscent of Csisz´ r and Tusn´ dy’s alternating minization procea a dures [2]. [sent-56, score-0.245]
</p><p>23 In our case, each side of the minimization is a simple convex program that can be interpreted as a projection with respect to a suitable Bregman distance; however, the overall program is not generally convex. [sent-57, score-0.193]
</p><p>24 In the case of Gaussian distributions, our algorithm coincides exactly with the power method for computing eigenvectors; in this sense it is a generalization of one of the oldest algorithms for PCA. [sent-58, score-0.16]
</p><p>25 Although omitted for lack of space, we can show that our procedure converges in that any limit point of the computed coefﬁcients is a stationary point of the loss function. [sent-59, score-0.371]
</p><p>26 1 The Exponential Family and Generalized Linear Models  9  In the exponential family of distributions the conditional probability of a value parameter value takes the following form:  given (2)  0 ¨   H  F9 © ¦ G   E© 9 ¦ C2 A@(' ( ©   % 9 ¦ 2 ! [sent-64, score-0.536]
</p><p>27 We will see that almost all of the concepts of members of the family is the form of the PCA algorithms in this paper stem directly from the deﬁnition of . [sent-78, score-0.262]
</p><p>28 A ﬁrst example is a normal distribution, with mean and unit variance, which has a density . [sent-79, score-0.094]
</p><p>29 It can be veriﬁed that that is usually written as this is a member of the exponential family with , , and . [sent-80, score-0.579]
</p><p>30 The probability of is usually written where is a parameter in . [sent-83, score-0.146]
</p><p>31 This is a member of the exponential family with , , and . [sent-84, score-0.488]
</p><p>32 In the normal distribution, , and in the Bernoulli case . [sent-87, score-0.094]
</p><p>33 In the general case, is referred to as the “expectation parameter”, and deﬁnes a function from the natural parameter values to the expectation parameter values. [sent-88, score-0.205]
</p><p>34 Our generalization of PCA is analogous to the manner in which generalized linear models (GLM’s) [8] provide a uniﬁed treatment of regression for the exponential family by generalizing least-squares regression to loss functions that are more appropriate for other members of this family. [sent-89, score-0.992]
</p><p>35 )(' ( ©   ¦ G %  &   ( ©  ¦ G 0( ( '   £ IH % D © % ¦ B 2 A@' ' # # ( D D ( © 1£ E' D A B ' ¦ 7 5 4 2 ( £( © 8C'   % ¦ ' % ¢ @9860 ¥30 ' &  In GLM’s, is taken to approximate the expectation parameter of the exponential model, where is the inverse of the “link function” [8]. [sent-96, score-0.343]
</p><p>36 A natural choice is to use the “canonical link”, where , being the derivative . [sent-97, score-0.087]
</p><p>37 In this case the natural parameters are directly approximated by , and the log-likelihood is simply . [sent-98, score-0.088]
</p><p>38 In the case of a normal distribution and it follows easily that the maximum-likelihood criwith ﬁxed variance, terion is equivalent to the least squares criterion. [sent-99, score-0.132]
</p><p>39 Another interesting case is logistic regression where , and the negative log-likelihood for parameters is where if , if . [sent-100, score-0.125]
</p><p>40 2 Bregman Distances and the Exponential Family   fa e c a` dbPY  be a differentiable and strictly convex function deﬁned on a closed, convex . [sent-104, score-0.158]
</p><p>41 It can be shown that, in general, every Bregman distance is nonnegative and is equal to zero if and only if its two arguments are equal. [sent-107, score-0.169]
</p><p>42 0 ©   % 9 ¦ 2 A@('  For the exponential family the log-likelihood  is directly related to a Bregman  Bernoulli  Poisson $  I 1H0( ) 2 I 5 3 E E " xE I 2ip hqH0( 1 ) c f c 1 ) ©0( E 1 ) H0( E E  $ E GF  I  " ' & E     T    $ " %# 6 $ " %#! [sent-108, score-0.427]
</p><p>43 Table 1 summarizes various functions of interest for examples of the exponential family. [sent-112, score-0.244]
</p><p>44 We will ﬁnd it useful to extend the idea of Bregman distances to divergences between vectors and matrices. [sent-113, score-0.137]
</p><p>45 (The notion of Bregman distance as well as our generalization of PCA can be extended to vectors in a more general manner; here, for simplicity, we restrict our attention to Bregman distances and PCA problems of this particular form. [sent-115, score-0.232]
</p><p>46 We now generalize PCA to other members of the exponential family. [sent-117, score-0.323]
</p><p>47 We wish to ﬁnd ’s that are “close” to the ’s and which belong to a lower dimensional subspace of parameter space. [sent-118, score-0.203]
</p><p>48 2  §      Let be the matrix whose ’th row is . [sent-122, score-0.112]
</p><p>49 Let be the matrix whose ’th row is , and let be the matrix with elements . [sent-123, score-0.175]
</p><p>50 This is a matrix of natural parameter values which deﬁne the probability of each point in . [sent-125, score-0.201]
</p><p>51   2       Following the discussion in Section 2, we consider the loss function taking the form  7 ©© 7   ¦ G D 7   87 9   ¦ "  "  D  h (  0 7 © 87   % 87 9 ¦ 2 ! [sent-127, score-0.21]
</p><p>52 The loss function varies depending on which member of the exponential family is taken, which simply changes the form of . [sent-130, score-0.698]
</p><p>53 For example, if is a matrix of real values, and the normal distribution is appropriate for the data, then and the loss criterion is the usual squared loss for PCA. [sent-131, score-0.653]
</p><p>54 )(' ( ©   %¦ 0i3 G   (   G © ¦   G  &  '  ' (  ©   ¦    ¦  g  From the relationship between log-likelihood and Bregman distances (see Eq. [sent-136, score-0.137]
</p><p>55 Once and have been found for the data points, the ’th data point can be represented as the vector in the lower dimensional space . [sent-139, score-0.136]
</p><p>56 The optimal value for then minimizes the sum of projection distances: . [sent-142, score-0.1]
</p><p>57 Note that for the normal distribution and the Bregman distance is Euclidean distance so that the projection operation in Eq. [sent-143, score-0.345]
</p><p>58 is also simpliﬁed in the normal case, simply being the hyperplane whose basis is . [sent-145, score-0.094]
</p><p>59 (     is taken to be a matrix of natural parameter values. [sent-147, score-0.169]
</p><p>60 deﬁnes a matrix of expectation parameters,  #  and . [sent-148, score-0.107]
</p><p>61 A Bregman distance    G  e  Y    PCA can also be thought of as search for a matrix is “close” to all the data points. [sent-149, score-0.154]
</p><p>62 that deﬁnes a surface  which    ( ©  ¦ #  The normal distribution is a simple case because , and the divergence is Euclidean distance. [sent-153, score-0.187]
</p><p>63 The projection operation is a linear operation, and is the hyperplane which has as its basis. [sent-154, score-0.095]
</p><p>64 © ¦  £    4 Generic Algorithms for Minimizing the Loss Function We now describe a generic algorithm for minimization of the loss function. [sent-155, score-0.317]
</p><p>65 Thus is alternately minimized with respective to its two arguments, each time optimizing one argument while keeping the other one ﬁxed, reminiscent of Csisz´ r and Tusn´ dy’s alternating minization procedures [2]. [sent-162, score-0.207]
</p><p>66 D §    We can then see that there are optimization problems, and that each one is essentially identical to a GLM regression problem (a very simple one, where there is a single parameter being optimized over). [sent-165, score-0.149]
</p><p>67 These sub-problems are easily solved, as the functions are convex in the argument being optimized over, and the large literature on maximumlikelihood estimation in GLM’s can be directly applied to the problem. [sent-166, score-0.117]
</p><p>68 These updates take a simple form for the normal distribution: , and . [sent-167, score-0.094]
</p><p>69 Thus the generic algorithm generalizes one of the oldest algorithms for solving the regular PCA problem. [sent-170, score-0.187]
</p><p>70 %& %   "  % %    ©    ¦ (  "      h h ¥     ¤    (       $%%   ©  ¤    ¦  (     & $% ¤ %        The loss is convex in either of its arguments with the other ﬁxed, but in general is not convex in the two arguments together. [sent-171, score-0.488]
</p><p>71 The normal distribution is an interesting special case in this respect — the power method is known to converge to the optimal solution, in spite of the non-convex nature of the loss surface. [sent-173, score-0.379]
</p><p>72 It can also be explained by analysis of the Hessian : for any stationary point is not positive semi-deﬁnite. [sent-175, score-0.096]
</p><p>73 Thus these stationary which is not the global minimum, points are saddle points rather than local minima. [sent-176, score-0.24]
</p><p>74 The Hessian for the generalized loss function is more complex; it remains an open problem whether it is also not positive semideﬁnite at stationary points other than the global minimum. [sent-177, score-0.464]
</p><p>75 Moreover, any limit point of the sequence will be a stationary point. [sent-180, score-0.129]
</p><p>76 To avoid such degenerate choices of , we can use a modiﬁed loss    e © B §¦ ¤  # # B C§ ¢ 7 ©© 7   ¦ # s B § ¦ p i £D ©© 7   ¦ # s e 87 9 ¦ p i  " " ¢  where is a small positive constant, and is any value in the range of (and therefore for which is ﬁnite). [sent-182, score-0.21]
</p><p>77 It can be proved, for this modiﬁed loss, that the sequence remains in a bounded region and hence always has at least one limit point which must be a stationary point. [sent-184, score-0.129]
</p><p>78 ) There are various ways to optimize the loss function when there is more than one component. [sent-186, score-0.21]
</p><p>79 Our generalization of PCA behaves rather differently for different members of the exponential family. [sent-228, score-0.359]
</p><p>80 One interesting example is that of the exponential distributions on nonnegative reals. [sent-229, score-0.348]
</p><p>81 For one-dimensional data, these densities are usually written as , where is the mean. [sent-230, score-0.091]
</p><p>82 In the uniform system of notation we have been using, we would instead index each distribution by a single natural parameter (basically, ), and write the density as , where . [sent-231, score-0.144]
</p><p>83 Once is found, we can recover the coefﬁcients The points lie on a line through the origin. [sent-239, score-0.156]
</p><p>84 Normally, we would not expect the points to also lie on a straight line; however, in this case they do, because any point of the form , can be written as and so must lie in the direction . [sent-240, score-0.317]
</p><p>85 ¦ #   ¨ ¤   (      (  ¤     ¤  (  $ ¤    Therefore, we can reasonably ask how the lines found under this exponential assumption differ from those found under a Gaussian assumption (that is, those found by regular PCA), provided all data is nonnegative. [sent-243, score-0.36]
</p><p>86 As a very simple illustration, we conducted two toy experiments with twenty data points in (Figure 1). [sent-244, score-0.104]
</p><p>87 In the second experiment, a few of the points were moved farther aﬁeld, and these outliers had a larger effect upon regular PCA than upon its exponential variant. [sent-246, score-0.476]
</p><p>88 For the Bernoulli distribution, a linear subspace of the space of parameters is typically a nonlinear surface in the space of the data. [sent-248, score-0.2]
</p><p>89 In Figure 2 (left), three points in the three-dimensional hypercube are mapped via our PCA to a onedimensional curve. [sent-249, score-0.125]
</p><p>90 The curve passes through one of the points ( ); the projections of the two other ( and ) are indicated. [sent-250, score-0.156]
</p><p>91 i c i  h  6 Relationship to Previous Work Lee and Seung [6, 7] and Hofmann [4] also describe probabilistic alternatives to PCA, tailored to data types that are not gaussian. [sent-255, score-0.139]
</p><p>92 In contrast to our method, [4, 6, 7] approximate mean parameters underlying the generation of the data points, with constraints on the matrices and ensuring that the elements of are in the correct domain. [sent-256, score-0.12]
</p><p>93 By instead choosing to approximate the natural parameters, in our method the matrices and do not usually need to be constrained—instead, we rely on the link function to give a transformed matrix which lies in the domain of the data points. [sent-257, score-0.297]
</p><p>94    q    #  0 © 87   D 7   A@(' 87 9   ¦ 7     © q ¦ #    More speciﬁcally, Lee and Seung [6] use the loss function (ignoring constant factors, and again deﬁning ). [sent-258, score-0.21]
</p><p>95 This method has a probabilistic interpretation, is generated from a Poisson distribution with mean parameter . [sent-260, score-0.139]
</p><p>96 where each data point For the Poisson distribution, our method uses the loss function , but without any constraints on the matrices and . [sent-261, score-0.325]
</p><p>97 The algorithm in Hofmann [4] uses , where the matrices and are constrained such that a loss function all the ’s are positive, and also such that . [sent-262, score-0.261]
</p><p>98 This work builds upon intuitions about exponential families and Bregman distances obtained largely from interactions with Manfred Warmuth, and from his papers. [sent-266, score-0.459]
</p><p>99 Relative loss bounds for on-line density estimation with the exponential family of distributions. [sent-272, score-0.637]
</p><p>100 Learning the parts of objects with nonnegative matrix factorization. [sent-295, score-0.113]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bregman', 0.447), ('pca', 0.419), ('exponential', 0.244), ('loss', 0.21), ('family', 0.183), ('bernoulli', 0.182), ('glm', 0.17), ('distances', 0.137), ('subspace', 0.108), ('poisson', 0.107), ('normal', 0.094), ('tusn', 0.092), ('generalized', 0.086), ('lie', 0.084), ('regular', 0.084), ('projections', 0.084), ('csisz', 0.08), ('convex', 0.079), ('members', 0.079), ('points', 0.072), ('th', 0.07), ('lee', 0.068), ('attributes', 0.067), ('nes', 0.065), ('stationary', 0.064), ('tipping', 0.064), ('matrix', 0.063), ('alternating', 0.063), ('projection', 0.062), ('member', 0.061), ('minization', 0.061), ('tailored', 0.061), ('cients', 0.06), ('arguments', 0.06), ('distance', 0.059), ('nd', 0.058), ('regression', 0.056), ('hofmann', 0.056), ('seung', 0.056), ('coef', 0.055), ('generic', 0.055), ('surface', 0.055), ('parameter', 0.055), ('link', 0.054), ('distributions', 0.054), ('de', 0.053), ('dasgupta', 0.053), ('hypercube', 0.053), ('jolliffe', 0.053), ('principal', 0.053), ('minimization', 0.052), ('natural', 0.051), ('matrices', 0.051), ('nonnegative', 0.05), ('gf', 0.05), ('row', 0.049), ('oldest', 0.048), ('usually', 0.046), ('probabilistic', 0.046), ('inappropriate', 0.045), ('written', 0.045), ('expectation', 0.044), ('component', 0.044), ('alternately', 0.043), ('dy', 0.043), ('manfred', 0.043), ('manner', 0.042), ('ed', 0.042), ('variants', 0.041), ('families', 0.04), ('reminiscent', 0.04), ('dimensional', 0.04), ('interpretation', 0.039), ('modi', 0.039), ('canonical', 0.039), ('coincides', 0.039), ('cally', 0.038), ('optimized', 0.038), ('distribution', 0.038), ('upon', 0.038), ('squared', 0.038), ('dimensionality', 0.038), ('minimizes', 0.038), ('xed', 0.037), ('veri', 0.037), ('ignoring', 0.037), ('parameters', 0.037), ('power', 0.037), ('generalization', 0.036), ('derivative', 0.036), ('hessian', 0.033), ('park', 0.033), ('operation', 0.033), ('limit', 0.033), ('ne', 0.033), ('point', 0.032), ('data', 0.032), ('global', 0.032), ('negative', 0.032), ('uni', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="9-tfidf-1" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>Author: Michael Collins, S. Dasgupta, Robert E. Schapire</p><p>Abstract: Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.</p><p>2 0.17533915 <a title="9-tfidf-2" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>3 0.17242527 <a title="9-tfidf-3" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>Author: Ming-Hsuan Yang</p><p>Abstract: Principal Component Analysis and Fisher Linear Discriminant methods have demonstrated their success in face detection, recognition, and tracking. The representation in these subspace methods is based on second order statistics of the image set, and does not address higher order statistical dependencies such as the relationships among three or more pixels. Recently Higher Order Statistics and Independent Component Analysis (ICA) have been used as informative low dimensional representations for visual recognition. In this paper, we investigate the use of Kernel Principal Component Analysis and Kernel Fisher Linear Discriminant for learning low dimensional representations for face recognition, which we call Kernel Eigenface and Kernel Fisherface methods. While Eigenface and Fisherface methods aim to find projection directions based on the second order correlation of samples, Kernel Eigenface and Kernel Fisherface methods provide generalizations which take higher order correlations into account. We compare the performance of kernel methods with Eigenface, Fisherface and ICA-based methods for face recognition with variation in pose, scale, lighting and expression. Experimental results show that kernel methods provide better representations and achieve lower error rates for face recognition. 1 Motivation and Approach Subspace methods have been applied successfully in numerous visual recognition tasks such as face localization, face recognition, 3D object recognition, and tracking. In particular, Principal Component Analysis (PCA) [20] [13] ,and Fisher Linear Discriminant (FLD) methods [6] have been applied to face recognition with impressive results. While PCA aims to extract a subspace in which the variance is maximized (or the reconstruction error is minimized), some unwanted variations (due to lighting, facial expressions, viewing points, etc.) may be retained (See [8] for examples). It has been observed that in face recognition the variations between the images of the same face due to illumination and viewing direction are almost always larger than image variations due to the changes in face identity [1]. Therefore, while the PCA projections are optimal in a correlation sense (or for reconstruction</p><p>4 0.15337233 <a title="9-tfidf-4" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>5 0.14977118 <a title="9-tfidf-5" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>6 0.1483527 <a title="9-tfidf-6" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>7 0.14570336 <a title="9-tfidf-7" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>8 0.13203888 <a title="9-tfidf-8" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>9 0.10549009 <a title="9-tfidf-9" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>10 0.099584378 <a title="9-tfidf-10" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>11 0.089722671 <a title="9-tfidf-11" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>12 0.087961905 <a title="9-tfidf-12" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>13 0.087476373 <a title="9-tfidf-13" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>14 0.086126395 <a title="9-tfidf-14" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>15 0.084082864 <a title="9-tfidf-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.080192253 <a title="9-tfidf-16" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>17 0.07835833 <a title="9-tfidf-17" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>18 0.076499715 <a title="9-tfidf-18" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>19 0.074447677 <a title="9-tfidf-19" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>20 0.074271135 <a title="9-tfidf-20" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.262), (1, 0.091), (2, -0.004), (3, 0.022), (4, 0.106), (5, -0.087), (6, -0.016), (7, 0.044), (8, 0.074), (9, -0.096), (10, -0.019), (11, 0.086), (12, 0.016), (13, -0.083), (14, -0.041), (15, -0.132), (16, -0.088), (17, -0.023), (18, -0.05), (19, 0.243), (20, 0.183), (21, -0.041), (22, 0.208), (23, 0.059), (24, -0.088), (25, 0.065), (26, -0.036), (27, 0.058), (28, -0.06), (29, -0.029), (30, -0.106), (31, -0.003), (32, 0.033), (33, -0.036), (34, 0.004), (35, 0.019), (36, 0.016), (37, 0.043), (38, -0.0), (39, 0.061), (40, -0.054), (41, -0.112), (42, 0.003), (43, 0.054), (44, 0.145), (45, 0.038), (46, -0.013), (47, 0.054), (48, 0.04), (49, -0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96277994 <a title="9-lsi-1" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>Author: Michael Collins, S. Dasgupta, Robert E. Schapire</p><p>Abstract: Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.</p><p>2 0.68798167 <a title="9-lsi-2" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.</p><p>3 0.6795615 <a title="9-lsi-3" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>Author: Peter Meinicke, Helge Ritter</p><p>Abstract: We suggest a nonparametric framework for unsupervised learning of projection models in terms of density estimation on quantized sample spaces. The objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data. For the resulting quantizing density estimator (QDE) we present a general method for parameter estimation and model selection. We show how projection sets which correspond to traditional unsupervised methods like vector quantization or PCA appear in the new framework. For a principal component quantizer we present results on synthetic and realworld data, which show that the QDE can improve the generalization of the kernel density estimator although its estimate is based on signiﬁcantly lower-dimensional projection indices of the data.</p><p>4 0.58435869 <a title="9-lsi-4" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>5 0.55476642 <a title="9-lsi-5" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>Author: T. Zhang</p><p>Abstract: Greedy approximation algorithms have been frequently used to obtain sparse solutions to learning problems. In this paper, we present a general greedy algorithm for solving a class of convex optimization problems. We derive a bound on the rate of approximation for this algorithm, and show that our algorithm includes a number of earlier studies as special cases.</p><p>6 0.51281983 <a title="9-lsi-6" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>7 0.502949 <a title="9-lsi-7" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>8 0.47206822 <a title="9-lsi-8" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>9 0.45419118 <a title="9-lsi-9" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>10 0.44383448 <a title="9-lsi-10" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>11 0.44013971 <a title="9-lsi-11" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>12 0.42867839 <a title="9-lsi-12" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>13 0.4278298 <a title="9-lsi-13" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>14 0.41619787 <a title="9-lsi-14" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>15 0.41226438 <a title="9-lsi-15" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>16 0.41198489 <a title="9-lsi-16" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>17 0.3961859 <a title="9-lsi-17" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>18 0.38271493 <a title="9-lsi-18" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>19 0.37764713 <a title="9-lsi-19" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>20 0.37330681 <a title="9-lsi-20" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.05), (17, 0.03), (19, 0.039), (27, 0.222), (30, 0.071), (38, 0.026), (59, 0.061), (70, 0.172), (72, 0.082), (79, 0.057), (83, 0.026), (91, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95002568 <a title="9-lda-1" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>Author: Ralf Herbrich, Robert C. Williamson</p><p>Abstract: In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used. Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space. 1</p><p>2 0.93080521 <a title="9-lda-2" href="./nips-2001-Quantizing_Density_Estimators.html">155 nips-2001-Quantizing Density Estimators</a></p>
<p>Author: Peter Meinicke, Helge Ritter</p><p>Abstract: We suggest a nonparametric framework for unsupervised learning of projection models in terms of density estimation on quantized sample spaces. The objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data. For the resulting quantizing density estimator (QDE) we present a general method for parameter estimation and model selection. We show how projection sets which correspond to traditional unsupervised methods like vector quantization or PCA appear in the new framework. For a principal component quantizer we present results on synthetic and realworld data, which show that the QDE can improve the generalization of the kernel density estimator although its estimate is based on signiﬁcantly lower-dimensional projection indices of the data.</p><p>same-paper 3 0.9192794 <a title="9-lda-3" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>Author: Michael Collins, S. Dasgupta, Robert E. Schapire</p><p>Abstract: Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.</p><p>4 0.82352483 <a title="9-lda-4" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>Author: Gunnar Rätsch, Sebastian Mika, Manfred K. Warmuth</p><p>Abstract: We give an uniﬁed convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-SquareBoost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes -norm regularized cost functions leading to a clean and general way to regularize ensemble learning.</p><p>5 0.81133336 <a title="9-lda-5" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>6 0.81117344 <a title="9-lda-6" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>7 0.81067121 <a title="9-lda-7" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>8 0.80944127 <a title="9-lda-8" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>9 0.80830121 <a title="9-lda-9" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>10 0.8050577 <a title="9-lda-10" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>11 0.80213135 <a title="9-lda-11" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>12 0.80125868 <a title="9-lda-12" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>13 0.79984403 <a title="9-lda-13" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>14 0.79888624 <a title="9-lda-14" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>15 0.79783046 <a title="9-lda-15" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>16 0.79681945 <a title="9-lda-16" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>17 0.79594898 <a title="9-lda-17" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>18 0.79316455 <a title="9-lda-18" href="./nips-2001-Blind_Source_Separation_via_Multinode_Sparse_Representation.html">44 nips-2001-Blind Source Separation via Multinode Sparse Representation</a></p>
<p>19 0.79073614 <a title="9-lda-19" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>20 0.789051 <a title="9-lda-20" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
