<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-7" href="#">nips2001-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</h1>
<br/><p>Source: <a title="nips-2001-7-pdf" href="http://papers.nips.cc/paper/2039-a-dynamic-hmm-for-on-line-segmentation-of-sequential-data.pdf">pdf</a></p><p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>Reference: <a title="nips-2001-7-reference" href="../nips2001_reference/nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. [sent-7, score-0.261]
</p><p>2 In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. [sent-8, score-0.255]
</p><p>3 Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. [sent-9, score-0.114]
</p><p>4 We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i. [sent-10, score-0.529]
</p><p>5 the method is able to process incoming data in real-time. [sent-12, score-0.176]
</p><p>6 The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. [sent-13, score-0.547]
</p><p>7 The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. [sent-14, score-0.23]
</p><p>8 1  Introduction  Abrupt changes can occur in many different real-world systems like, for example, in speech, in climatological or industrial processes, in financial markets, and also in physiological signals (EEG/MEG). [sent-15, score-0.048]
</p><p>9 Methods for the analysis of time-varying dynamical systems are therefore an important issue in many application areas. [sent-16, score-0.069]
</p><p>10 In [12], we introduced the annealed competition of experts method for time series from nonlinear switching dynamics, related approaches were presented, e. [sent-17, score-0.179]
</p><p>11 First, the segmentation does not depend on the predictability of the system. [sent-22, score-0.292]
</p><p>12 Instead, we merely estimate the density distribution of the data and track its changes. [sent-23, score-0.164]
</p><p>13 This is particularly an improvement for systems where data is hard to predict, like, for example, EEG recordings [7] or financial data. [sent-24, score-0.109]
</p><p>14 An incoming data stream is processed incrementally while keeping the computational effort limited by a fixed â&euro;˘ http://www. [sent-26, score-0.443]
</p><p>15 the algorithm is able to perpetually segment and classify data streams with a fixed amount of memory and CPU resources. [sent-34, score-0.333]
</p><p>16 It is even possible to continuously monitor measured data in real-time, as long as the sampling rate is not too high. [sent-35, score-0.103]
</p><p>17 Instead, it optimizes the segmentation on-the-fly by means of dynamic programming [1], which thereby results in an automatic correction or fine-tuning of previously estimated segmentation bounds. [sent-39, score-0.584]
</p><p>18 2  The segmentation algorithm  We consider the problem of continuously segmenting a data stream on-line and simultaneously labeling the segments. [sent-40, score-0.687]
</p><p>19 The data stream is supposed to have a sequential or temporal structure as follows: it is supposed to consist of consecutive blocks of data in such a way that the data points in each block originate from the same underlying distribution. [sent-41, score-0.521]
</p><p>20 The segmentation task is to be performed in an unsupervised fashion, i. [sent-42, score-0.292]
</p><p>21 1  Using pdfs as features for segmentation  Consider Yl, Y2 , Y3, . [sent-46, score-0.716]
</p><p>22 , with Yt E Rn, an incoming data stream to be analyzed. [sent-49, score-0.273]
</p><p>23 The sequence might have already passed a pre-processing step like filtering or subsampling, as long as this can be done on-the-fly in case of an on-line scenario. [sent-50, score-0.15]
</p><p>24 The idea behind embedding is that the measured data might be a potentially non-linear projection of the systems state or phase space. [sent-53, score-0.334]
</p><p>25 In any case, an embedding in a higher-dimensional space might help to resolve structure in the data, a property which is exploited, e. [sent-54, score-0.143]
</p><p>26 After the embedding step one might perform a sub-sampling of the embedded data in order to reduce the amount of data for real-time processing. [sent-57, score-0.368]
</p><p>27 2 Next, we want to track the density distribution of the embedded data and therefore estimate the probability density function (pdf) in a sliding window of length W. [sent-58, score-0.402]
</p><p>28 We use a standard density estimator with multivariate Gaussian kernels [4] for this purpose, centered on the data points 3 in the window ~ { Xt-w }W -l w=o,  ()  1  ~l  1  Pt x = W ~ (27fa 2 )d/2 exp  (x - Xt_w)2)  (  -  2a 2  . [sent-59, score-0.165]
</p><p>29 (2)  The kernel width a is a smoothing parameter and its value is important to obtain a good representation of the underlying distribution. [sent-60, score-0.044]
</p><p>30 We propose to choose a proportional to the mean distance of each Xt to its first d nearest neighbors, averaged over a sample set {xt}. [sent-61, score-0.031]
</p><p>31 1 In our reported application we can process data at 1000 Hz (450 Hz including display) on a 1. [sent-62, score-0.061]
</p><p>32 2In that case, our further notation of time indices would refer to the subsampled data. [sent-64, score-0.072]
</p><p>33 2  Similarity of two pdfs  Once we have sampled enough data points to compute the first pdf according to eq. [sent-67, score-0.931]
</p><p>34 (2), we can compute a new pdf with each new incoming data point. [sent-68, score-0.547]
</p><p>35 3  The HMM in the off-line case  Before we can discuss the on-line variant, it is necessary to introduce the HMM and the respective off-line algorithm first. [sent-70, score-0.045]
</p><p>36 For a given a data sequence, {X'dT=l' we can obtain the corresponding sequence of pdfs {Pt(X)}tES, S = {W, . [sent-71, score-0.588]
</p><p>37 We now construct a hidden Markov model (HMM) where each of these pdfs is represented by a state s E S, with S being the set of states in the HMM. [sent-76, score-0.654]
</p><p>38 For each state s, we define a continuous observation probability distribution,  -  ( ) PPt (X) I s-~ 1  V 21f <;  exp  (  -  d(Ps(X),Pt(x))) 22 <;  '  (4)  for observing a pdf Pt(x) in state s. [sent-77, score-0.55]
</p><p>39 Next, the initial state distribution {1f s LES of the HMM is given by the uniform distribution, 1fs = liN, with N = lSI being the number of states. [sent-78, score-0.13]
</p><p>40 The HMM transition matrix, A = (PijkjES, determines each probability to switch from a state Si to a state Sj. [sent-80, score-0.323]
</p><p>41 Our aim is to find a representation of the given sequence of pdfs in terms of a sequence of a small number of representative pdfs, that we call prototypes, which moreover exhibits only a small number of prototype changes. [sent-81, score-0.875]
</p><p>42 We therefore define A in such a way that transitions to the same state are k times more likely than transitions to any of the other states, _ {  Pij -  k+~-l 1 k+N - l  ;ifi=J ;ifi-j. [sent-82, score-0.194]
</p><p>43 The well-known Viterbi algorithm [13] can now be applied to the above HMM in order to compute the optimal - i. [sent-85, score-0.18]
</p><p>44 the most likely - state sequence of prototype pdfs that might have generated the given sequence of pdfs. [sent-87, score-0.993]
</p><p>45 This state sequence represents the segmentation we are aiming at. [sent-88, score-0.525]
</p><p>46 We can compute the most likely state sequence more efficiently if we compute it in terms of costs, c = -log(p), instead of probabilities p, i. [sent-89, score-0.395]
</p><p>47 instead of computing the maximum of the likelihood function L , we compute the minimum of the cost function , -log(L), which yields the optimal state sequence as well. [sent-91, score-0.414]
</p><p>48 In addition to that, we can further simplify the computation for the special case of our particular HMM architecture, which finally results in the following algorithm: For each time step, t = w, . [sent-93, score-0.033]
</p><p>49 ,T, we compute for all S E S the cost cs(t) of the optimal state sequence from W to t, subject to the constraint that it ends in state S at  time t. [sent-96, score-0.62]
</p><p>50 We call these constrained optimal sequences c-paths and the unconstrained optimum 0* -path. [sent-97, score-0.054]
</p><p>51 The iteration can be formulated as follows, with ds,t being a short hand for d(ps(x)'pt(x)) and bs,s denoting the Kronecker delta function : Initialization, Vs E S:  Cs(W)  :=  (6)  ds ,w,  Induction, Vs E S:  cs(t) := ds,t  + min { cs(t sES  1)  + C (1- bs 's)},  for t = W  + 1, . [sent-98, score-0.079]
</p><p>52 , T,  (7)  Termination: 0* :=  (8)  min { cs(T) } . [sent-101, score-0.079]
</p><p>53 sES  The regularization constant C, which is given by C = 2C; 2 10g(k) and thus subsumes our two free HMM parameters, can be interpreted as transition cost for switching to a new state in the path. [sent-102, score-0.292]
</p><p>54 4 The optimal prototype sequence with minimal costs 0* for the complete series of pdfs, which is determined in the last step, is obtained by logging and updating the c-paths for all states s during the iteration and finally choosing the one with minimal costs according to eq. [sent-103, score-0.687]
</p><p>55 4  The on-line algorithm  In order to turn the above segmentation algorithm into an on-line algorithm, we must restrict the incremental update in eq. [sent-106, score-0.512]
</p><p>56 (7), such that it only uses pdfs (and therewith states) from past data points. [sent-107, score-0.518]
</p><p>57 We neglect at this stage that memory and CPU resources are limited. [sent-108, score-0.147]
</p><p>58 Suppose that we have already processed data up to T - 1. [sent-109, score-0.103]
</p><p>59 When a new data point YT arrives at time T, we can generate a new embedded vector XT (once we have sampled enough initial data points for the embedding), we have a new pdf pT(X) (once we have sampled enough embedded vectors Xt for the first pdf window), and thus we have given a new HMM state. [sent-110, score-1.021]
</p><p>60 We can also readily compute the distances between the new pdf and all the previous pdfs, dT,t, t < T, according to eq. [sent-111, score-0.371]
</p><p>61 A similarly simple and straightforward update of the costs, the c-paths and the optimal state sequence is only possible, however, if we neglect to consider potential c-paths that would have contained the new pdf as a prototype in previous segments. [sent-113, score-0.883]
</p><p>62 The on-line update at time T for these restricted paths, that we henceforth denote with a tilde, can be performed as follows: For T = W, we have cw(W) := o*(W) := dw,w = O. [sent-115, score-0.097]
</p><p>63 Compute the cost cT(T - 1) for the new state s For t = T - 1, compute  w, . [sent-117, score-0.257]
</p><p>64 ,  =T  at time T - 1:  0 ift=W CT(t) :=dT,t+ { min{cT(t-1) ; o*(t-1)+C}: else  (9)  and update  o*(t) := CT(t), if CT(t) < o*(t). [sent-120, score-0.097]
</p><p>65 (10) Here we use all previous optimal segmentations o*(t), so we don't need to keep the complete matrix (cs(t))S,tES and repeatedly compute the minimum 4We developed an algorithm that computes an appropriate value for the hyperparameter C from a sample set {it}. [sent-121, score-0.256]
</p><p>66 Due to the limited space we will present that algorithm in a forthcoming publication [8]. [sent-122, score-0.077]
</p><p>67 However, we must store and update the history of optimal segmentations 8* (t). [sent-124, score-0.225]
</p><p>68 Update from T - 1 to T and compute cs(T) for all states s E S obtained so far, and also get 8*(T): For s = W, . [sent-126, score-0.181]
</p><p>69 , T , compute  cs(T) := ds,T + min {cs(T - 1); 8*(T - 1) + C}  (11)  and finally get the cost of the optimal path  8* (T) := min {cs(T)} . [sent-129, score-0.384]
</p><p>70 sES  (12)  As for the off-line case, the above algorithm only shows the update equations for the costs of the C- and 8* -paths. [sent-130, score-0.231]
</p><p>71 The associated state sequences must be logged simultaneously during the computation. [sent-131, score-0.232]
</p><p>72 Note that this can be done by just storing the sequence of switching points for each path. [sent-132, score-0.219]
</p><p>73 So far we have presented the incremental version of the segmentation algorithm. [sent-134, score-0.327]
</p><p>74 This algorithm still needs an amount of memory and CPU time that is increasing with each new data point. [sent-135, score-0.209]
</p><p>75 In order to limit both resources to a fixed amount, we must remove old pdfs, i. [sent-136, score-0.279]
</p><p>76 We propose to do this by discarding all states with time indices smaller or equal to s each time the path associated with cs(T) in eq. [sent-139, score-0.329]
</p><p>77 (11) exhibits a switch back from a more recent state/pdf to the currently considered state s as a result of the min-operation in eq. [sent-140, score-0.252]
</p><p>78 In the above algorithm this can simply be done by setting W := s + 1 in that case, which also allows us to discard the corresponding old cs(T)- and 8* (t)-paths, for all s::::: sand t < s. [sent-142, score-0.233]
</p><p>79 In addition, the "if t = W" initialization clause in eq. [sent-143, score-0.066]
</p><p>80 (9) must be ignored after the first such cut and the 8* (W - I)-path must therefore still be kept to compute the else-part also for t = W now. [sent-144, score-0.143]
</p><p>81 Moreover, we do not have CT(W -1) and we therefore assume min {CT(W - 1); 8*(W - 1) + C} = 8*(W - 1) + C (in eq. [sent-145, score-0.079]
</p><p>82 The explanation for this is as follows: A switch back in eq. [sent-147, score-0.063]
</p><p>83 Vice versa, a newly obtained pdf is unlikely to properly represent the previous mode then, which justifies our above assumption about CT (W -1). [sent-149, score-0.358]
</p><p>84 The effect of the proposed cut-off strategy is that we discard paths that end in pdfs from old modes but still allow to find the optimal pdf prototype within the current segment. [sent-150, score-1.178]
</p><p>85 Cut-off conditions occur shortly after mode changes in the data and cause the removal of HMM states with pdfs from old modes. [sent-151, score-0.801]
</p><p>86 However, if no mode change takes place in the incoming data sequence, no states will be discarded. [sent-152, score-0.344]
</p><p>87 We therefore still need to set a fixed upper limit", for the number of candidate paths/pdfs that are simultaneously under consideration if we only have limited resources available. [sent-153, score-0.169]
</p><p>88 When this limit is reached because no switches are detected, we must successively discard the oldest path/pdf stored, which finally might result in choosing a suboptimal prototype for that segment however. [sent-154, score-0.491]
</p><p>89 Ultimately, a continuous discarding even enforces a change of prototypes after 2", time steps if no switching is induced by the data until then. [sent-155, score-0.41]
</p><p>90 The buffer size", should therefore be as large as possible. [sent-156, score-0.066]
</p><p>91 In any case, the buffer overflow condition can be recorded along with the segmentation, which allows us to identify such artificial switchings. [sent-157, score-0.097]
</p><p>92 5  The labeling algorithm  A labeling algorithm is required to identify segments that represent the same underlying distribution and thus have similar pdf prototypes. [sent-159, score-0.674]
</p><p>93 The labeling algorithm generates labels for the segments and assigns identical labels to segments that are similar in this respect. [sent-160, score-0.318]
</p><p>94 To this end, we propose a relatively simple on-line clustering scheme for the prototypes, since we expect the prototypes obtained from the same underlying distribution to be already well-separated from the other prototypes as a result of the segmentation algorithm. [sent-161, score-0.671]
</p><p>95 We assign a new label to a segment if the distance of its associated prototype to all preceding prototypes exceeds a certain threshold and we assign the existing label of the closest preceding prototype otherwise. [sent-162, score-0.687]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pdfs', 0.424), ('segmentation', 0.292), ('pdf', 0.29), ('cs', 0.249), ('hmm', 0.232), ('prototype', 0.186), ('ct', 0.179), ('prototypes', 0.152), ('state', 0.13), ('costs', 0.122), ('old', 0.118), ('switching', 0.116), ('incoming', 0.115), ('xt', 0.109), ('sequence', 0.103), ('pt', 0.101), ('states', 0.1), ('stream', 0.097), ('embedding', 0.096), ('ses', 0.085), ('segment', 0.081), ('compute', 0.081), ('min', 0.079), ('labeling', 0.079), ('lemm', 0.076), ('segmentations', 0.076), ('cpu', 0.076), ('discard', 0.07), ('dynamical', 0.069), ('mode', 0.068), ('embedded', 0.068), ('buffer', 0.066), ('sliding', 0.066), ('update', 0.064), ('switch', 0.063), ('segments', 0.061), ('supposed', 0.061), ('data', 0.061), ('window', 0.06), ('track', 0.059), ('exhibits', 0.059), ('neglect', 0.056), ('resources', 0.056), ('optimal', 0.054), ('viterbi', 0.053), ('incrementally', 0.053), ('fraunhofer', 0.053), ('discarding', 0.048), ('financial', 0.048), ('ps', 0.048), ('might', 0.047), ('cost', 0.046), ('algorithm', 0.045), ('path', 0.045), ('switches', 0.045), ('density', 0.044), ('underlying', 0.044), ('ends', 0.043), ('fixed', 0.043), ('sequential', 0.042), ('processed', 0.042), ('continuously', 0.042), ('sampled', 0.042), ('preceding', 0.041), ('yt', 0.041), ('hz', 0.039), ('indices', 0.039), ('simultaneously', 0.038), ('initialization', 0.036), ('paths', 0.036), ('labels', 0.036), ('amount', 0.035), ('memory', 0.035), ('vs', 0.035), ('incremental', 0.035), ('abrupt', 0.033), ('embed', 0.033), ('ise', 0.033), ('kohlmorgen', 0.033), ('logged', 0.033), ('markets', 0.033), ('originate', 0.033), ('perpetually', 0.033), ('ppt', 0.033), ('segmenting', 0.033), ('therewith', 0.033), ('enough', 0.033), ('time', 0.033), ('transitions', 0.032), ('limited', 0.032), ('berlin', 0.032), ('germany', 0.032), ('must', 0.031), ('identify', 0.031), ('variant', 0.031), ('propose', 0.031), ('limit', 0.031), ('clause', 0.03), ('shortly', 0.03), ('annealed', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="7-tfidf-1" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>2 0.19136864 <a title="7-tfidf-2" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>3 0.14097348 <a title="7-tfidf-3" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>4 0.13769585 <a title="7-tfidf-4" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>5 0.12886883 <a title="7-tfidf-5" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>6 0.11219033 <a title="7-tfidf-6" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>7 0.10543779 <a title="7-tfidf-7" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>8 0.10418443 <a title="7-tfidf-8" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>9 0.098833278 <a title="7-tfidf-9" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>10 0.077280156 <a title="7-tfidf-10" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>11 0.076807939 <a title="7-tfidf-11" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>12 0.073823199 <a title="7-tfidf-12" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>13 0.072040752 <a title="7-tfidf-13" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>14 0.069241777 <a title="7-tfidf-14" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>15 0.068584047 <a title="7-tfidf-15" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>16 0.068497926 <a title="7-tfidf-16" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>17 0.06832286 <a title="7-tfidf-17" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>18 0.068250038 <a title="7-tfidf-18" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>19 0.067996904 <a title="7-tfidf-19" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>20 0.065541536 <a title="7-tfidf-20" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.213), (1, -0.042), (2, 0.058), (3, -0.097), (4, -0.135), (5, 0.006), (6, 0.064), (7, 0.033), (8, -0.063), (9, -0.056), (10, 0.01), (11, 0.039), (12, 0.05), (13, 0.229), (14, 0.015), (15, -0.108), (16, -0.012), (17, 0.08), (18, 0.271), (19, -0.032), (20, 0.052), (21, 0.158), (22, -0.017), (23, -0.004), (24, 0.002), (25, -0.017), (26, -0.026), (27, -0.049), (28, -0.141), (29, -0.04), (30, -0.096), (31, -0.012), (32, 0.077), (33, 0.118), (34, 0.063), (35, 0.055), (36, 0.033), (37, 0.159), (38, -0.005), (39, 0.041), (40, -0.001), (41, -0.0), (42, -0.033), (43, 0.025), (44, -0.015), (45, -0.104), (46, -0.02), (47, 0.064), (48, -0.008), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94455451 <a title="7-lsi-1" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>2 0.65668178 <a title="7-lsi-2" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>3 0.63313508 <a title="7-lsi-3" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>4 0.60296935 <a title="7-lsi-4" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>5 0.58335006 <a title="7-lsi-5" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>6 0.51247615 <a title="7-lsi-6" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>7 0.50573766 <a title="7-lsi-7" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>8 0.47369424 <a title="7-lsi-8" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>9 0.41261163 <a title="7-lsi-9" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>10 0.40443102 <a title="7-lsi-10" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>11 0.38680804 <a title="7-lsi-11" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>12 0.35572952 <a title="7-lsi-12" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>13 0.34675583 <a title="7-lsi-13" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>14 0.33221772 <a title="7-lsi-14" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>15 0.32580811 <a title="7-lsi-15" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>16 0.32245162 <a title="7-lsi-16" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>17 0.30787155 <a title="7-lsi-17" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>18 0.30521774 <a title="7-lsi-18" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>19 0.30158186 <a title="7-lsi-19" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>20 0.30023423 <a title="7-lsi-20" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.038), (17, 0.062), (19, 0.033), (27, 0.119), (30, 0.071), (36, 0.011), (38, 0.015), (52, 0.186), (59, 0.04), (72, 0.052), (79, 0.082), (83, 0.042), (91, 0.182)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89599997 <a title="7-lda-1" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>2 0.78547698 <a title="7-lda-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>3 0.78194916 <a title="7-lda-3" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>4 0.77977812 <a title="7-lda-4" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>5 0.77626824 <a title="7-lda-5" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1</p><p>6 0.77305603 <a title="7-lda-6" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>7 0.77144635 <a title="7-lda-7" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>8 0.77035594 <a title="7-lda-8" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>9 0.76924753 <a title="7-lda-9" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>10 0.768713 <a title="7-lda-10" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>11 0.76826048 <a title="7-lda-11" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>12 0.76770401 <a title="7-lda-12" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>13 0.76566052 <a title="7-lda-13" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>14 0.76540458 <a title="7-lda-14" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>15 0.76535571 <a title="7-lda-15" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>16 0.76458156 <a title="7-lda-16" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>17 0.76221019 <a title="7-lda-17" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>18 0.76170588 <a title="7-lda-18" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>19 0.76015472 <a title="7-lda-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.75981289 <a title="7-lda-20" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
