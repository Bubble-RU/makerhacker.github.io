<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-146" href="#">nips2001-146</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</h1>
<br/><p>Source: <a title="nips-2001-146-pdf" href="http://papers.nips.cc/paper/1984-playing-is-believing-the-role-of-beliefs-in-multi-agent-learning.pdf">pdf</a></p><p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><p>Reference: <a title="nips-2001-146-reference" href="../nips2001_reference/nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. [sent-5, score-0.37]
</p><p>2 We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents. [sent-7, score-0.353]
</p><p>3 Game theorists have begun to examine learning models in their study of repeated games, and reinforcement learning researchers have begun to extend their singleagent learning models to the multiple-agent case. [sent-9, score-0.271]
</p><p>4 From the game theory perspective, the repeated game is a generalization of the traditional one-shot game, or matrix game. [sent-13, score-0.54]
</p><p>5 The matrix game is deﬁned as a reward matrix Ri for each player, Ri : A1 × A2 → R, where Ai is the set of actions available to player i. [sent-14, score-0.568]
</p><p>6 Purely competitive games are called zero-sum games and must satisfy R1 = −R2 . [sent-15, score-0.364]
</p><p>7 Each player simultaneously chooses to play a particular action ai ∈ Ai , or a mixed policy µi = P D(Ai ), which is a probability distribution over the possible actions, and receives reward based on the joint action taken. [sent-16, score-0.647]
</p><p>8 Some common examples of single-shot matrix games are shown in Figure 1. [sent-17, score-0.203]
</p><p>9 The traditional assumption is that each player has no prior knowledge about the other player. [sent-18, score-0.205]
</p><p>10 As is standard in the game theory literature, it is thus reasonable to assume that the opponent is fully rational and chooses actions that are in its best interest. [sent-19, score-0.974]
</p><p>11 then the game is said to be in Nash equilibrium. [sent-23, score-0.23]
</p><p>12 Once all players are playing by a Nash equilibrium, no single player has an incentive to unilaterally deviate from his equilibrium policy. [sent-24, score-0.663]
</p><p>13 Any game can be solved for its Nash equilibria using quadratic programming, and a player can choose an optimal strategy in this fashion, given prior knowledge of the game structure. [sent-25, score-0.786]
</p><p>14 If the players do not manage to coordinate on one equilibrium joint policy, then they may all end up worse off. [sent-27, score-0.362]
</p><p>15 The Hawk-Dove game shown in Figure 1(c) is a good example of this problem. [sent-28, score-0.23]
</p><p>16 The two Nash equilibria occur at (1,2) and (2,1), but if the players do not coordinate, they may end up playing a joint action (1,1) and receive 0 reward. [sent-29, score-0.377]
</p><p>17 Despite these problems, there is general agreement that Nash equilibrium is an appropriate solution concept for one-shot games. [sent-31, score-0.165]
</p><p>18 In contrast, for repeated games there are a range of different perspectives. [sent-32, score-0.241]
</p><p>19 Repeated games generalize one-shot games by assuming that the players repeat the matrix game over many time periods. [sent-33, score-0.833]
</p><p>20 Researchers in reinforcement learning view repeated games as a special case of stochastic, or Markov, games. [sent-34, score-0.309]
</p><p>21 Researchers in game theory, on the other hand, view repeated games as an extension of their theory of one-shot matrix games. [sent-35, score-0.492]
</p><p>22 The resulting frameworks are similar, but with a key difference in their treatment of game history. [sent-36, score-0.23]
</p><p>23 Reinforcement learning researchers focus their attention on choosing a single stationary policy µ that will maximize the learner’s expected rewards in all future time periods given T τ −t τ that we are in time t, maxµ Eµ R (µ) , where T may be ﬁnite or inﬁnite, τ =t γ and µ = P D(A). [sent-37, score-0.39]
</p><p>24 Littman [1] analyzes this framework for zero-sum games, proving convergence to the Nash equilibrium for his minimax-Q algorithm playing against another minimax-Q agent. [sent-39, score-0.261]
</p><p>25 Claus and Boutilier [2] examine cooperative games where R1 = R2 , and Hu and Wellman [3] focus on general-sum games. [sent-40, score-0.203]
</p><p>26 Littman [4] and Hall and Greenwald [5] further extend this approach to consider variants of Nash equilibrium for which convergence can be guaranteed. [sent-42, score-0.165]
</p><p>27 [7] propose to relax the mutual optimality requirement of Nash equilibrium by considering rational agents, which always learn to play a stationary best-response to their opponent’s strategy, even if the opponent is not playing an equilibrium strategy. [sent-44, score-1.339]
</p><p>28 The motivation is that it allows our agents to act rationally even if the opponent is not acting rationally because of physical or computational limitations. [sent-45, score-0.765]
</p><p>29 Fictitious play [8] is a similar algorithm from game theory. [sent-46, score-0.334]
</p><p>30 As alluded to in the previous section, game theorists often take a more general view of optimality in repeated games. [sent-48, score-0.343]
</p><p>31 H0 H1 H∞  B0 minimax-Q, Nash-Q  B1  Q-learning (Q0 ), (WoLF-)PHC, ﬁctitious play  Q1  B∞ Bully Godfather multiplicativeweight*  * assumes public knowledge of the opponent’s policy at each period stochastic game model, we took µi = P D(Ai ). [sent-51, score-0.518]
</p><p>32 Moreover, the agent ought to be able to form beliefs about the opponent’s strategy, and these beliefs ought to converge to the opponent’s actual strategy given sufﬁcient learning time. [sent-56, score-0.444]
</p><p>33 Let βi : H → P D(A−i ) be player i’s belief about the opponent’s strategy. [sent-57, score-0.237]
</p><p>34 Now we can deﬁne the Nash equilibrium of a repeated game in terms of our personal strategy and our beliefs about the opponent. [sent-59, score-0.626]
</p><p>35 If this holds for all players in the game, then we are guaranteed to be in Nash equilibrium. [sent-61, score-0.197]
</p><p>36 } converges to a Nash equilibrium iff the following two conditions hold: • Optimization: ∀t, µi (ht−1 ) ∈ BRi (βi (ht−1 )). [sent-67, score-0.165]
</p><p>37 We can never design an agent that will learn to both predict the opponent’s future strategy and optimize over those beliefs at the same time. [sent-72, score-0.28]
</p><p>38 Despite this fact, if we assume some extra knowledge about the opponent, we can design an algorithm that approximates the best-response stationary policy over time against any opponent. [sent-73, score-0.214]
</p><p>39 In the game theory literature, this concept is often called universal consistency. [sent-74, score-0.23]
</p><p>40 Fudenburg and Levine [8] and Freund and Schapire [10] independently show that a multiplicativeweight algorithm exhibits universal consistency from the game theory and machine learning perspectives. [sent-75, score-0.3]
</p><p>41 2 A new classiﬁcation and a new algorithm We propose a general classiﬁcation that categorizes algorithms by the cross-product of their possible strategies and their possible beliefs about the opponent’s strategy, H × B. [sent-78, score-0.174]
</p><p>42 Given more memory, the agent can formulate more complex policies, since policies are maps from histories to action distributions. [sent-80, score-0.229]
</p><p>43 H0 agents are memoryless and can only play stationary policies. [sent-81, score-0.271]
</p><p>44 At the other extreme, H∞ agents have unbounded memory and can formulate ever more complex strategies as the game is played over time. [sent-83, score-0.407]
</p><p>45 Agents that believe their opponent is memoryless are classiﬁed as B0 players, Bt players believe that the opponent bases its strategy on the previous tperiods of play, and so forth. [sent-85, score-1.661]
</p><p>46 Although not explicitly stated, most existing algorithms make assumptions and thus hold beliefs about the types of possible opponents in the world. [sent-86, score-0.235]
</p><p>47 We can think of each Hs × Bt as a different league of players, with players in each league roughly equal to one another in terms of their capabilities. [sent-87, score-0.387]
</p><p>48 Clearly some leagues contain less capable players than others. [sent-88, score-0.224]
</p><p>49 We can thus deﬁne a fair opponent as an opponent from an equal or lesser league. [sent-89, score-1.38]
</p><p>50 Within each league, we assume that players are fully rational in the sense that they can fully use their available histories to construct their future policy. [sent-92, score-0.311]
</p><p>51 If we believe that our opponent is a memoryless player, then even if we are an H∞ player, our fully rational strategy is to simply model the opponent’s stationary strategy and play our stationary best response. [sent-94, score-1.165]
</p><p>52 As discussed in the previous section, the problem with these players is that even though they have full access to the history, their fully rational strategy is stationary due to their limited belief set. [sent-100, score-0.433]
</p><p>53 A general example of a H∞ × B0 player is the policy hill climber (PHC). [sent-101, score-0.352]
</p><p>54 It maintains a policy and updates the policy based upon its history in an attempt to maximize its rewards. [sent-102, score-0.307]
</p><p>55 From state s, select action a according to the mixed policy µi (s) with some exploration. [sent-110, score-0.192]
</p><p>56 ” True to their league, PHC players play well against stationary opponents. [sent-117, score-0.367]
</p><p>57 At the opposite end of the spectrum, Littman and Stone [11] propose algorithms in H 0 ×B∞ and H1 × B∞ that are leader strategies in the sense that they choose a ﬁxed strategy and hope that their opponent will “follow” by learning a best response to that ﬁxed strategy. [sent-118, score-0.83]
</p><p>58 Their “Bully” algorithm chooses a ﬁxed memoryless stationary policy, while “Godfather” has memory of the last time period. [sent-119, score-0.152]
</p><p>59 Opponents included normal Q-learning and Q 1 players, which are similar to Q-learners except that they explicitly learn using one period of memory because they believe that their opponent is also using memory to learn. [sent-120, score-0.759]
</p><p>60 The interesting result is that “Godfather” is able to achieve non-stationary equilibria against Q 1 in the repeated prisoner’s dilemna game, with rewards for both players that are higher than the stationary Nash equilibrium rewards. [sent-121, score-0.611]
</p><p>61 Finally, Hu and Wellman’s Nash-Q and Littman’s minimax-Q are classiﬁed as H 0 × B0 players, because even though they attempt to learn the Nash equilibrium through experience, their play is ﬁxed once this equilibrium has been learned. [sent-125, score-0.434]
</p><p>62 Furthermore, they assume that the opponent also plays a ﬁxed stationary Nash equilibrium, which they hope is the other half of their own equilibrium strategy. [sent-126, score-0.881]
</p><p>63 As discussed above, most existing algorithms do not form beliefs about the opponent beyond B0 . [sent-129, score-0.804]
</p><p>64 We wish to open the door to such possibilities by designing learners that can model the opponent and use that information to achieve better rewards. [sent-131, score-0.67]
</p><p>65 Ideally we would like to design an algorithm in H∞ × B∞ that is able to win or come to an equilibrium against any fair opponent. [sent-132, score-0.245]
</p><p>66 Since many of the current algorithms are best-response players, we choose an opponent class such as PHC, which is a good example of a best-response player in H∞ × B0 . [sent-134, score-0.879]
</p><p>67 We will demonstrate that our algorithm indeed beats its PHC opponents and in fact does well against most of the existing fair opponents. [sent-135, score-0.197]
</p><p>68 In particular, we would like to model players from H ∞ × B0 . [sent-138, score-0.197]
</p><p>69 Since we are in H∞ × B∞ , it is rational for us to construct such models because we believe that the opponent is learning and adapting to us over time using its history. [sent-139, score-0.776]
</p><p>70 The idea is that we will “fool” our opponent into thinking that we are stupid by playing a decoy policy for a number of time periods and then switch to a different policy that takes advantage of their best response to our decoy policy. [sent-140, score-1.171]
</p><p>71 From a learning perspective, the idea is that we adapt much faster than the opponent; in fact, when we switch away from our decoy policy, our adjustment to the new policy is immediate. [sent-141, score-0.19]
</p><p>72 In contrast, the H∞ × B0 opponent adjusts its policy by small increments and is furthermore unable to model our changing behavior. [sent-142, score-0.777]
</p><p>73 The opponent never catches on to us because it believes that we only play stationary policies. [sent-144, score-0.82]
</p><p>74 Bowling and Veloso showed that in selfplay, a restricted version of WoLF-PHC always reaches a stationary Nash equilibrium in two-player two-action games, and that the general WoLF-PHC seems to do the same in experimental trials. [sent-146, score-0.231]
</p><p>75 Thus, in the long run, a WoLF-PHC player achieves its stationary Nash equilibrium payoff against any other PHC player. [sent-147, score-0.479]
</p><p>76 Observing action at at time t, update our history h and calculate an estimate of the −i opponent’s policy: t #(h[τ ] = a) µt (s, a) = τ =t−w ˆ−i ∀a, w where w is the window of estimation and #(h[τ ] = a) = 1 if the opponent’s action at time τ is equal to a, and 0 otherwise. [sent-150, score-0.177]
</p><p>77 otherwise  Note that we derive both the opponent’s learning rate δ and the opponent’s policy µ −i (s) ˆ from estimates using the observable history of actions. [sent-159, score-0.202]
</p><p>78 If we assume the game matrix is public information, then we can solve for the equilibrium strategy µ ∗ (s), otherwise we can ˆi run WoLF-PHC for some ﬁnite number of time periods to obtain an estimate this equilibrium strategy. [sent-160, score-0.769]
</p><p>79 The PHC-Exploiter algorithm is based upon PHC and thus exhibits the same behavior as PHC in games with a single pure Nash equilibrium. [sent-163, score-0.203]
</p><p>80 Both agents generally converge to the single pure equilibrium point. [sent-164, score-0.226]
</p><p>81 The interesting case arises in competitive games where the only equilibria require mixed strategies, as discussed by Singh et al [12] and Bowling and Veloso [6]. [sent-165, score-0.249]
</p><p>82 In the full knowledge case where we know our opponent’s policy µ 2 and learning rate δ2 at every time period, we can prove that a PHC-Exploiter learning algorithm will guarantee us unbounded reward in the long run playing games such as matching pennies. [sent-168, score-0.596]
</p><p>83 In the zero-sum game of matching pennies, where the only Nash equilibrium requires the use of mixed strategies, PHC-Exploiter is able to achieve unbounded rewards as t → ∞ against any PHC opponent given that play follows the cycle C deﬁned by the arrowed segments shown in Figure 2. [sent-171, score-1.306]
</p><p>84 Given a point (x, y) = (µ 1 (H), µ2 (H)) on the graph in Figure 2, where µi (H) is the probability by which player i plays Heads, we know that our expected reward is R1 (x, y) = −1 × [(x)(y) + (1 − x)(1 − y)] + 1 × [(1 − x)(y) + (x)(1 − y)]. [sent-175, score-0.262]
</p><p>85 Action distribution of the two agent system  Action distribution of the two agent system 1. [sent-176, score-0.216]
</p><p>86 The cyclic play is evident in our empirical results, where we play a PHC-Exploiter player 1 against a PHC player 2. [sent-189, score-0.653]
</p><p>87 Agent 1 total reward over time 8000  6000  total reward  4000  2000  0  -2000  -4000 0  20000  40000  60000  80000  100000  time period  Figure 3: Total rewards for agent 1 increase as we gain reward through each cycle. [sent-190, score-0.397]
</p><p>88 Thus, C R1 (x, y)dt = 1/3 > 0, and we have shown that we receive a payoff greater than the Nash equilibrium payoff of zero over every cycle. [sent-196, score-0.251]
</p><p>89 We used the PHC-Exploiter algorithm described above to play against several PHC variants in different iterated matrix games, including matching pennies, prisoner’s dilemna, and rock-paper-scissors. [sent-200, score-0.159]
</p><p>90 Here we give the results for the matching pennies game analyzed above, playing against WoLF-PHC. [sent-201, score-0.428]
</p><p>91 We used a window of w = 5000 time periods to estimate the opponent’s current policy µ 2 and the opponent’s  learning rate δ2 . [sent-202, score-0.238]
</p><p>92 As shown in Figure 2, the play exhibits the cyclic nature that we predicted. [sent-203, score-0.16]
</p><p>93 The two solid vertical lines indicate periods in which our PHC-Exploiter player is winning, and the dashed, roughly diagonal, lines indicate periods in which it is losing. [sent-204, score-0.341]
</p><p>94 Figure 3 plots the total reward for our PHC-Exploiter agent over time. [sent-209, score-0.165]
</p><p>95 The periods of winning and losing are very clear from this graph. [sent-210, score-0.177]
</p><p>96 Against all the existing fair opponents shown in Table 1, it achieved at least its average equilibrium payoff in the long-run. [sent-212, score-0.405]
</p><p>97 In this paper, we have presented a new classiﬁcation for multi-agent learning algorithms and suggested an algorithm that seems to dominate existing algorithms from the fair opponent leagues when playing certain games. [sent-215, score-0.959]
</p><p>98 Ideally, we would like to create an algorithm in the league H∞ × B∞ that provably dominates larger classes of fair opponents in any game. [sent-216, score-0.256]
</p><p>99 We would like to extend our framework to more general stochastic games with multiple states and multiple players. [sent-218, score-0.182]
</p><p>100 Markov games as a framework for multi-agent reinforcement learning. [sent-224, score-0.228]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('opponent', 0.65), ('nash', 0.282), ('phc', 0.244), ('game', 0.23), ('player', 0.205), ('players', 0.197), ('games', 0.182), ('equilibrium', 0.165), ('policy', 0.127), ('agent', 0.108), ('play', 0.104), ('playing', 0.096), ('league', 0.095), ('beliefs', 0.094), ('ht', 0.085), ('opponents', 0.081), ('fair', 0.08), ('strategy', 0.078), ('winning', 0.071), ('periods', 0.068), ('bowling', 0.068), ('pennies', 0.068), ('stationary', 0.066), ('agents', 0.061), ('rational', 0.06), ('repeated', 0.059), ('reward', 0.057), ('strategies', 0.056), ('godfather', 0.054), ('veloso', 0.054), ('histories', 0.054), ('history', 0.053), ('heads', 0.05), ('ai', 0.048), ('reinforcement', 0.046), ('economic', 0.043), ('equilibria', 0.043), ('payoff', 0.043), ('classi', 0.041), ('bri', 0.041), ('decoy', 0.041), ('dilemna', 0.041), ('prisoner', 0.041), ('action', 0.041), ('memoryless', 0.04), ('rewards', 0.04), ('littman', 0.038), ('losing', 0.038), ('ri', 0.037), ('existing', 0.036), ('dt', 0.036), ('period', 0.036), ('cyclic', 0.035), ('unbounded', 0.035), ('matching', 0.034), ('actions', 0.034), ('cw', 0.033), ('optimality', 0.033), ('belief', 0.032), ('cl', 0.031), ('argmaxa', 0.03), ('hu', 0.03), ('massachusetts', 0.027), ('begun', 0.027), ('bully', 0.027), ('claus', 0.027), ('ctitious', 0.027), ('fudenburg', 0.027), ('leagues', 0.027), ('multiplicativeweight', 0.027), ('nachbar', 0.027), ('nagayuki', 0.027), ('rationally', 0.027), ('wellman', 0.027), ('policies', 0.026), ('researchers', 0.025), ('memory', 0.025), ('mixed', 0.024), ('algorithms', 0.024), ('cycle', 0.024), ('payoffs', 0.024), ('ought', 0.024), ('believe', 0.023), ('mi', 0.023), ('proceeds', 0.022), ('learning', 0.022), ('theorists', 0.021), ('public', 0.021), ('multiagent', 0.021), ('cooperative', 0.021), ('michael', 0.021), ('time', 0.021), ('exhibits', 0.021), ('ideally', 0.021), ('matrix', 0.021), ('cial', 0.02), ('arti', 0.02), ('learners', 0.02), ('hill', 0.02), ('execute', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="146-tfidf-1" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><p>2 0.27222869 <a title="146-tfidf-2" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>Author: Michael L. Littman, Michael J. Kearns, Satinder P. Singh</p><p>Abstract: We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games. 1</p><p>3 0.22672704 <a title="146-tfidf-3" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>Author: Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a ﬁnite set of standard, scalar-reward learning algorithms. Suﬃcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1</p><p>4 0.12424953 <a title="146-tfidf-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>5 0.1228475 <a title="146-tfidf-5" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>Author: Carlos Guestrin, Daphne Koller, Ronald Parr</p><p>Abstract: We present a principled and efﬁcient planning algorithm for cooperative multiagent dynamic systems. A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture. We view the entire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian network (DBN). The action space of the resulting MDP is the joint action space of the entire set of agents. Our approach is based on the use of factored linear value functions as an approximation to the joint value function. This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme. We provide a simple and efﬁcient method for computing such an approximate value function by solving a single linear program, whose size is determined by the interaction between the value function structure and the DBN. We thereby avoid the exponential blowup in the state and action space. We show that our approach compares favorably with approaches based on reward sharing. We also show that our algorithm is an efﬁcient alternative to more complicated algorithms even in the single agent case.</p><p>6 0.1113672 <a title="146-tfidf-6" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>7 0.10342961 <a title="146-tfidf-7" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>8 0.097037748 <a title="146-tfidf-8" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>9 0.077859454 <a title="146-tfidf-9" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>10 0.072648786 <a title="146-tfidf-10" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>11 0.071700715 <a title="146-tfidf-11" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>12 0.069963358 <a title="146-tfidf-12" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>13 0.054642349 <a title="146-tfidf-13" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>14 0.050215065 <a title="146-tfidf-14" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>15 0.043901317 <a title="146-tfidf-15" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>16 0.039973684 <a title="146-tfidf-16" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>17 0.039914906 <a title="146-tfidf-17" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>18 0.039535135 <a title="146-tfidf-18" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>19 0.03916496 <a title="146-tfidf-19" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>20 0.037528187 <a title="146-tfidf-20" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.129), (1, -0.081), (2, 0.23), (3, 0.04), (4, 0.01), (5, 0.069), (6, -0.052), (7, -0.083), (8, 0.037), (9, 0.003), (10, 0.004), (11, -0.116), (12, -0.035), (13, -0.072), (14, -0.079), (15, -0.012), (16, -0.011), (17, 0.048), (18, -0.115), (19, 0.041), (20, -0.262), (21, 0.106), (22, 0.018), (23, 0.378), (24, 0.075), (25, 0.024), (26, -0.255), (27, -0.022), (28, -0.085), (29, -0.161), (30, -0.061), (31, -0.139), (32, 0.006), (33, 0.02), (34, 0.088), (35, -0.05), (36, -0.003), (37, -0.033), (38, -0.036), (39, 0.004), (40, 0.094), (41, -0.041), (42, 0.06), (43, -0.063), (44, -0.06), (45, -0.032), (46, 0.014), (47, -0.065), (48, -0.033), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95896977 <a title="146-lsi-1" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><p>2 0.92037386 <a title="146-lsi-2" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>Author: Michael L. Littman, Michael J. Kearns, Satinder P. Singh</p><p>Abstract: We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games. 1</p><p>3 0.67555493 <a title="146-lsi-3" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>Author: Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a ﬁnite set of standard, scalar-reward learning algorithms. Suﬃcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1</p><p>4 0.4584676 <a title="146-lsi-4" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>Author: Jeff Bilmes, Gang Ji, Marina Meila</p><p>Abstract: In this work, we introduce an information-theoretic based correction term to the likelihood ratio classiﬁcation method for multiple classes. Under certain conditions, the term is sufﬁcient for optimally correcting the difference between the true and estimated likelihood ratio, and we analyze this in the Gaussian case. We ﬁnd that the new correction term significantly improves the classiﬁcation results when tested on medium vocabulary speech recognition tasks. Moreover, the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournament-like strategies to deal with this issue. We ﬁnd that further small improvements are obtained by using an appropriate tournament. Lastly, we ﬁnd that intransitivity appears to be a good measure of classiﬁcation conﬁdence.</p><p>5 0.30524194 <a title="146-lsi-5" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><p>6 0.27376685 <a title="146-lsi-6" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>7 0.25545815 <a title="146-lsi-7" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>8 0.23079905 <a title="146-lsi-8" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>9 0.22807531 <a title="146-lsi-9" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>10 0.21696475 <a title="146-lsi-10" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>11 0.20914152 <a title="146-lsi-11" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>12 0.20680846 <a title="146-lsi-12" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>13 0.1877872 <a title="146-lsi-13" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>14 0.18579033 <a title="146-lsi-14" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>15 0.17887475 <a title="146-lsi-15" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>16 0.17089581 <a title="146-lsi-16" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>17 0.1606212 <a title="146-lsi-17" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>18 0.16028719 <a title="146-lsi-18" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>19 0.15813175 <a title="146-lsi-19" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>20 0.15507148 <a title="146-lsi-20" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.011), (14, 0.016), (17, 0.019), (19, 0.027), (27, 0.088), (30, 0.057), (38, 0.024), (59, 0.022), (67, 0.327), (72, 0.071), (79, 0.045), (83, 0.093), (91, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8044039 <a title="146-lda-1" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><p>2 0.75230873 <a title="146-lda-2" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>Author: Alberto Paccanaro, Geoffrey E. Hinton</p><p>Abstract: We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its ﬁnal goal is to be able to generalize, i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to ﬁnd compact distributed representations for variable-sized recursive data structures, such as trees and lists. 1 Linear Relational Embedding Our aim is to take a large set of facts about a domain expressed as tuples of arbitrary symbols in a simple and rigid syntactic format and to be able to infer other “common-sense” facts without having any prior knowledge about the domain. Let us imagine a situation in which we have a set of concepts and a set of relations among these concepts, and that our data consists of few instances of these relations that hold among the concepts. We want to be able to infer other instances of these relations. For example, if the concepts are the people in a certain family, the relations are kinship relations, and we are given the facts ”Alberto has-father Pietro” and ”Pietro has-brother Giovanni”, we would like to be able to infer ”Alberto has-uncle Giovanni”. Our approach is to learn appropriate distributed representations of the entities in the data, and then exploit the generalization properties of the distributed representations [2] to make the inferences. In this paper we present a method, which we have called Linear Relational Embedding (LRE), which learns a distributed representation for the concepts by embedding them in a space where the relations between concepts are linear transformations of their distributed representations. Let us consider the case in which all the relations are binary, i.e. involve two concepts. , and the problem In this case our data consists of triplets we are trying to solve is to infer missing triplets when we are given only few of them. Inferring a triplet is equivalent to being able to complete it, that is to come up with one of its elements, given the other two. Here we shall always try to complete the third element of the triplets 1 . LRE will then represent each concept in the data as a learned vector in a 2 0    £ § ¥ £  § ¥ % </p><p>3 0.64299047 <a title="146-lda-3" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>4 0.5073868 <a title="146-lda-4" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>Author: Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a ﬁnite set of standard, scalar-reward learning algorithms. Suﬃcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1</p><p>5 0.50621092 <a title="146-lda-5" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>6 0.49646714 <a title="146-lda-6" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>7 0.49278679 <a title="146-lda-7" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>8 0.47633299 <a title="146-lda-8" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>9 0.47233981 <a title="146-lda-9" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>10 0.46806625 <a title="146-lda-10" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>11 0.46731526 <a title="146-lda-11" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>12 0.46675432 <a title="146-lda-12" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>13 0.46608278 <a title="146-lda-13" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>14 0.46309227 <a title="146-lda-14" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>15 0.46280515 <a title="146-lda-15" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>16 0.45849067 <a title="146-lda-16" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>17 0.45783022 <a title="146-lda-17" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>18 0.45745718 <a title="146-lda-18" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>19 0.45718667 <a title="146-lda-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.45678759 <a title="146-lda-20" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
