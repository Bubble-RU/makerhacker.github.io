<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-146" href="#">nips2001-146</a> knowledge-graph by maker-knowledge-mining</p><h1>146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</h1>
<br/><p>Source: <a title="nips-2001-146-pdf" href="http://papers.nips.cc/paper/1984-playing-is-believing-the-role-of-beliefs-in-multi-agent-learning.pdf">pdf</a></p><p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><p>Reference: <a title="nips-2001-146-reference" href="../nips2001_reference/nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('oppon', 0.688), ('gam', 0.344), ('play', 0.304), ('nash', 0.266), ('phc', 0.229), ('equilibr', 0.18), ('policy', 0.137), ('leagu', 0.115), ('ag', 0.113), ('reward', 0.083), ('hist', 0.081), ('ht', 0.08), ('believ', 0.075), ('strategy', 0.073), ('period', 0.067), ('bowl', 0.064), ('payoff', 0.061), ('penny', 0.055), ('win', 0.054), ('strategies', 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="146-tfidf-1" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>2 0.31286275 <a title="146-tfidf-2" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>3 0.28013265 <a title="146-tfidf-3" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>4 0.14210023 <a title="146-tfidf-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>5 0.13959876 <a title="146-tfidf-5" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>6 0.12665358 <a title="146-tfidf-6" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>7 0.1066661 <a title="146-tfidf-7" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>8 0.10224156 <a title="146-tfidf-8" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>9 0.089654349 <a title="146-tfidf-9" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>10 0.089387976 <a title="146-tfidf-10" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>11 0.075741075 <a title="146-tfidf-11" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>12 0.068138346 <a title="146-tfidf-12" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>13 0.062165484 <a title="146-tfidf-13" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>14 0.056535032 <a title="146-tfidf-14" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>15 0.054093193 <a title="146-tfidf-15" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>16 0.052624345 <a title="146-tfidf-16" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>17 0.043949991 <a title="146-tfidf-17" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>18 0.043513175 <a title="146-tfidf-18" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>19 0.04295934 <a title="146-tfidf-19" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>20 0.042730607 <a title="146-tfidf-20" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.122), (1, 0.0), (2, 0.288), (3, 0.045), (4, -0.03), (5, -0.082), (6, -0.031), (7, 0.05), (8, 0.005), (9, -0.028), (10, -0.069), (11, 0.013), (12, 0.027), (13, 0.011), (14, 0.041), (15, 0.12), (16, 0.009), (17, -0.062), (18, 0.015), (19, -0.055), (20, -0.201), (21, 0.039), (22, 0.109), (23, -0.014), (24, -0.172), (25, -0.276), (26, 0.39), (27, 0.097), (28, 0.116), (29, 0.124), (30, -0.071), (31, 0.099), (32, -0.035), (33, -0.016), (34, -0.066), (35, -0.008), (36, -0.012), (37, 0.042), (38, 0.095), (39, -0.017), (40, -0.077), (41, -0.011), (42, -0.075), (43, -0.034), (44, 0.003), (45, 0.112), (46, 0.06), (47, 0.055), (48, 0.046), (49, -0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95271212 <a title="146-lsi-1" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>2 0.9151668 <a title="146-lsi-2" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>3 0.6562984 <a title="146-lsi-3" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>4 0.54037541 <a title="146-lsi-4" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>5 0.3359789 <a title="146-lsi-5" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>6 0.27775621 <a title="146-lsi-6" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>7 0.27588257 <a title="146-lsi-7" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>8 0.26592401 <a title="146-lsi-8" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>9 0.25532472 <a title="146-lsi-9" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>10 0.21747595 <a title="146-lsi-10" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>11 0.19544552 <a title="146-lsi-11" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>12 0.18234237 <a title="146-lsi-12" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>13 0.16148338 <a title="146-lsi-13" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>14 0.15240003 <a title="146-lsi-14" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>15 0.15232557 <a title="146-lsi-15" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>16 0.14882749 <a title="146-lsi-16" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>17 0.14776003 <a title="146-lsi-17" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>18 0.14562398 <a title="146-lsi-18" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>19 0.14261088 <a title="146-lsi-19" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>20 0.14092803 <a title="146-lsi-20" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.07), (16, 0.11), (31, 0.025), (50, 0.558), (63, 0.011), (77, 0.013), (91, 0.022), (92, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89931768 <a title="146-lda-1" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>2 0.89063215 <a title="146-lda-2" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>same-paper 3 0.87988704 <a title="146-lda-3" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>4 0.82729268 <a title="146-lda-4" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>5 0.74152642 <a title="146-lda-5" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>6 0.6873076 <a title="146-lda-6" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>7 0.63594687 <a title="146-lda-7" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>8 0.63354671 <a title="146-lda-8" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>9 0.62341267 <a title="146-lda-9" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>10 0.61494362 <a title="146-lda-10" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>11 0.60994357 <a title="146-lda-11" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>12 0.59975588 <a title="146-lda-12" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>13 0.56854641 <a title="146-lda-13" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>14 0.56177396 <a title="146-lda-14" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>15 0.54937547 <a title="146-lda-15" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>16 0.54735637 <a title="146-lda-16" href="./nips-2001-Optimising_Synchronisation_Times_for_Mobile_Devices.html">140 nips-2001-Optimising Synchronisation Times for Mobile Devices</a></p>
<p>17 0.54286295 <a title="146-lda-17" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>18 0.53262383 <a title="146-lda-18" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>19 0.53200501 <a title="146-lda-19" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>20 0.52542549 <a title="146-lda-20" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
