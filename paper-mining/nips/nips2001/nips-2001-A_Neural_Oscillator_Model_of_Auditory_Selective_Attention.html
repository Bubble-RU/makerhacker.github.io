<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-14" href="#">nips2001-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</h1>
<br/><p>Source: <a title="nips-2001-14-pdf" href="http://papers.nips.cc/paper/2110-a-neural-oscillator-model-of-auditory-selective-attention.pdf">pdf</a></p><p>Author: Stuart N. Wrigley, Guy J. Brown</p><p>Abstract: A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. 1</p><p>Reference: <a title="nips-2001-14-reference" href="../nips2001_reference/nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract A model of auditory grouping is described in which auditory attention plays a key role. [sent-12, score-0.626]
</p><p>2 The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. [sent-13, score-0.847]
</p><p>3 In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. [sent-15, score-0.491]
</p><p>4 Hence, the auditory system must separate an acoustic mixture in order to create a perceptual description of each sound source. [sent-17, score-0.363]
</p><p>5 Few studies have investigated the role of attention in ASA; typically, ASA is seen as a precursor to attentional mechanisms, which simply select one stream as the attentional focus. [sent-21, score-1.03]
</p><p>6 [4] investigated how attention influences auditory grouping with the use of a rapidly repeating sequence of high and low tones. [sent-24, score-0.401]
</p><p>7 It is known that high frequency separations and/or high presentation rates encourage the high tones and low tones to form separate streams, a phenomenon known as auditory streaming [2]. [sent-25, score-0.822]
</p><p>8 demonstrated that auditory streaming did not occur when listeners attended to an alternative stimulus presented simultaneously. [sent-27, score-0.437]
</p><p>9 However, when they were instructed to attend to the tone sequence, auditory streaming occurred as normal. [sent-28, score-0.479]
</p><p>10 From this, it was concluded that attention is required for stream formation and not only for stream selection. [sent-29, score-0.438]
</p><p>11 It has been proposed that attention can be divided into two different levels [9]: low-level exogenous attention which groups acoustic elements to form streams, and a higher-level endogenous mechanism which performs stream selection. [sent-30, score-0.532]
</p><p>12 The work presented here incorporates these two types of attention into a model of auditory grouping (Figure 1). [sent-34, score-0.401]
</p><p>13 Oscillators corresponding to grouped auditory elements are synchronised, and are desynchronised from oscillators encoding other groups. [sent-36, score-0.52]
</p><p>14 This theory is supported by neurobiological findings that report  ALI Correlogram Signal Cochlear Filtering  Hair cell  Cross Channel Correlation  Attentional Stream  Neural Oscillator Network  Figure 1: Schematic diagram of the model (the attentional leaky integrator is labelled ALI). [sent-37, score-0.572]
</p><p>15 Within the oscillatory correlation framework, attentional selection can be implemented by synchronising attentional activity with the stream of interest. [sent-39, score-1.09]
</p><p>16 Accordingly, the second stage of the model extracts pitch information from the simulated auditory nerve responses. [sent-47, score-0.453]
</p><p>17 This is achieved by computing the autocorrelation of the activity in each channel to form a correlogram [3]. [sent-48, score-0.438]
</p><p>18 At time t, the autocorrelation of channel i with lag τ is given by: P –1  A ( i, t , τ ) =  ∑ r ( i, t – k )r ( i, t – k – τ )w ( k )  (1)  k=0  Here, r is the auditory nerve activity. [sent-49, score-0.538]
</p><p>19 The autocorrelation for channel i is computed using a 25 ms rectangular window w (P = 200) with lag steps equal to the sampling period, up to a maximum lag of 20 ms. [sent-50, score-0.381]
</p><p>20 The correlogram may also be used to identify formant and harmonic regions due to their similar patterns of periodicity [11]. [sent-53, score-0.404]
</p><p>21 3 Neural oscillator network The network consists of 128 oscillators and is based upon the two-dimensional locally excitatory globally inhibitory oscillator network (LEGION) of Wang [10], [11]. [sent-56, score-1.042]
</p><p>22 Within  LEGION, oscillators are synchronised by placing local excitatory links between them. [sent-57, score-0.506]
</p><p>23 Additionally, a global inhibitor receives excitation from each oscillator, and inhibits every oscillator in the network. [sent-58, score-0.411]
</p><p>24 This ensures that only one block of synchronised oscillators can be active at any one time. [sent-59, score-0.446]
</p><p>25 Hence, separate blocks of synchronised oscillators - which correspond to the notion of a segment in ASA - arise through the action of local excitation and global inhibition. [sent-60, score-0.557]
</p><p>26 Finally, we introduce an attentional leaky integrator (ALI), which selects one block of oscillators to become the attentional stream (i. [sent-64, score-1.366]
</p><p>27 The input Io to oscillator i is a combination of three factors: external input Ir , network activity and global inhibition as follows: Io = I r –W z S ( z, θ z ) +  ∑ Wik S ( xk, θx )  (4)  k≠i  Here, Wik is the connection strength between oscillators i and k; xk is the activity of oscillator k. [sent-70, score-1.109]
</p><p>28 The parameter θx is a threshold above which an oscillator can affect others in the network and Wz is the weight of inhibition from the global inhibitor z. [sent-71, score-0.416]
</p><p>29 S is a squashing function which compresses oscillator activity to be within a certain range: 1 S ( n, θ ) = ------------------------------) –K ( n – θ 1+e  (5)  Here, K determines the sharpness of the sigmoidal function. [sent-73, score-0.399]
</p><p>30 These segments are encoded by a binary mask, which is unity when a channel contributes to a segment and zero otherwise. [sent-81, score-0.396]
</p><p>31 The external input (Ir) of an oscillator whose channel is a member of a segment is set to Ihigh otherwise it is set to Ilow. [sent-84, score-0.586]
</p><p>32 A segment is classed as consistent with the F0 if a majority of its corresponding correlogram channels exhibit a significant peak at the fundamental period (ratio of peak height to channel energy greater than 0. [sent-88, score-0.719]
</p><p>33 +  [n]+  Consider two segments that start at the same time; the age trackers for their constituent channels receive the same input, so the values of Bk will be the same. [sent-97, score-0.432]
</p><p>34 However, if two segments start at different times, the age trackers for the earlier segment will have already increased to a non-zero value when the second segment starts. [sent-98, score-0.509]
</p><p>35 3 Attentional leaky integrator (ALI) Each oscillator is connected to the attentional leaky integrator (ALI) by excitatory links; the strength of these connections is modulated by endogenous attention. [sent-102, score-1.19]
</p><p>36 Input to the ALI is given by:   · ali = H  ∑ S ( x k, θ x )T k – θ ALI – ali    (8)  k  θALI is a threshold above which network activity can influence the ALI. [sent-103, score-0.645]
</p><p>37 The build-up of attentional interest is therefore stimulus dependent. [sent-107, score-0.473]
</p><p>38 The attentional interest itself is modelled as a Gaussian according to the gradient model of attention [7]: A k = max A e k  k–p –---------2 2σ  (11)  Here, Ak is the normalised attentional interest at frequency channel k and maxAk is the maximum value that Ak can attain. [sent-108, score-1.348]
</p><p>39 p is the channel at which the peak of attentional interest occurs, and σ determines the width of the peak. [sent-109, score-0.722]
</p><p>40 A segment or group of segments are said to be attended to if their oscillatory activity coincides temporally with a peak in the ALI activity. [sent-110, score-0.557]
</p><p>41 Initially, the connection weights between the oscillator array and the ALI are strong: all segments feed excitation to the ALI, so all segments are attended to. [sent-111, score-0.615]
</p><p>42 During sustained activity, these weights relax toward the Ak interest vector such that strong weights exist for channels of high attentional interest and low weights exist for channels of low attentional interest. [sent-112, score-1.212]
</p><p>43 ALI activity will only coincide with activity of the channels within the attentional interest peak and any harmonically related (synchronised) activity outside the Ak peak. [sent-113, score-1.047]
</p><p>44 This behaviour allows both individual tones and harmonic complexes to be attended to using only a single Ak peak. [sent-115, score-0.482]
</p><p>45 A gray pixel indicates the presence of a segment at a particular frequency channel, which is also equivalent to the external input to the corresponding oscillator: gray signifies Ihigh (causing the oscillator to be stimulated) and white signifies Ilow (causing the oscillator to be unstimulated). [sent-132, score-0.838]
</p><p>46 Any oscillators which are temporally synchronised with the ALI are considered to be in the attentional foreground. [sent-137, score-0.814]
</p><p>47 [5] investigated the effect of a mistuned harmonic upon the pitch of a 12 component complex tone. [sent-140, score-0.557]
</p><p>48 As the degree of mistuning of the fourth harmonic increased towards 4%, the shift in the perceived pitch of the complex also increased. [sent-141, score-0.742]
</p><p>49 Apparently, the pitch of a complex tone is calculated using only those channels which belong to the corresponding stream. [sent-143, score-0.439]
</p><p>50 When the harmonic is subject to mistunings below 8%, it is grouped with the rest of the complex and so can affect the pitch percept. [sent-144, score-0.611]
</p><p>51 Mistunings of greater than 8% cause the harmonic to be segregated into a second stream, and so it is excluded from the pitch percept. [sent-145, score-0.497]
</p><p>52 5  0  0  20  40 60 Time (ms)  80  0  20  40 60 Time (ms)  80  0  20  40 60 Time (ms)  80  0 2 4 6 8 Mistuning of 4th harmonic (%)  Figure 2: A,B,C: Network response to mistuning of the fourth harmonic of a 12 harmonic complex (0%, 6% and 8% respectively). [sent-148, score-1.079]
</p><p>53 Gray areas denote the presence of a segment and black areas denote oscillators in the active phase. [sent-150, score-0.392]
</p><p>54 120 Channel  100 80 60 40 20 0  100  200  300 Time (ms)  400  500  600  Figure 3: Captor tones preceding the complex capture the fourth harmonic into a separate stream. [sent-154, score-0.547]
</p><p>55 ALI activity (top) shows that this harmonic is the focus of attention and would be ‘heard out’ . [sent-155, score-0.47]
</p><p>56 The attentional interest vector (Ak) is shown to the right of the figure. [sent-156, score-0.473]
</p><p>57 All the oscillators at frequency channels corresponding to harmonics are temporally synchronised for mistunings up to 8% (plots A and B) signifying that the harmonics belong to the same perceptual group. [sent-158, score-0.889]
</p><p>58 Mistunings beyond 8% cause the mistuned harmonic to become desychronised from the rest of the complex (plot C) - two distinct perceptual groups are now present: one containing the fourth harmonic and the other containing the remainder of the complex tone. [sent-159, score-0.881]
</p><p>59 The pitch of the complex was calculated by creating a summary correlogram (similar to that used in section 2. [sent-162, score-0.393]
</p><p>60 1 kHz were used for this summary since low frequency (resolved) harmonics are known to dominate the pitch percept [8]. [sent-165, score-0.374]
</p><p>61 also showed that the effect of mistuning was diminished when the fourth harmonic was ‘captured’ from the complex by four preceding tones at the same frequency. [sent-167, score-0.645]
</p><p>62 In this situation, no matter how small the mistuning, the harmonic is segregated from the complex and does not influence the pitch percept. [sent-168, score-0.581]
</p><p>63 Attentional interest is focused on the fourth harmonic: oscillator activity for the captor tone segments is synchronised with the ALI activity. [sent-170, score-0.972]
</p><p>64 During the 550 ms before the complex tone onset, the age tracker activities for the captor tone channels build up. [sent-171, score-0.537]
</p><p>65 When the complex tone begins, there is a significant age difference between the frequency channels stimulated by the fourth harmonic and those stimulated by the remainder of the complex. [sent-172, score-0.896]
</p><p>66 Such a difference prevents excitatory harmonicity connections from being made between the fourth harmonic and the remaining harmonics. [sent-173, score-0.552]
</p><p>67 The old-plus-new heuristic can be further demonstrated by starting the fourth harmonic before the rest of the complex. [sent-175, score-0.4]
</p><p>68 Figure 4 shows the output of the model when the fourth harmonic is subject to a 50 ms onset asynchrony. [sent-176, score-0.484]
</p><p>69 During this time, the age trackers of channels excited by the fourth harmonic increase to a significantly higher value than those of the remaining harmonics. [sent-177, score-0.661]
</p><p>70 Once again, this prevents excitatory connections being made between the fourth harmonic and the other harmonically related segments. [sent-178, score-0.552]
</p><p>71 The early harmonic is desynchronised from the rest of the complex: two streams are formed. [sent-179, score-0.398]
</p><p>72 Once this occurs, there is no longer any evidence to prevent excitatory links from being made between the fourth harmonic and the rest of the complex. [sent-181, score-0.513]
</p><p>73 Grouping by harmonicity then occurs for all segments: the complex and the early harmonic synchronise to form a single stream. [sent-182, score-0.399]
</p><p>74 2 Auditory streaming Within the framework presented here, auditory streaming is an emergent property; all events which occur over time, and are subject to attentional interest, are implicitly grouped. [sent-184, score-0.947]
</p><p>75 It is the width of the Ak peak that determines frequency separation-dependent streaming, rather than local connections between oscillators as in [10]. [sent-186, score-0.478]
</p><p>76 Figure 5 shows the effect of two different frequency separations on the ability of the network to perform auditory streaming and shows a good match to experimental findings [1], [4]. [sent-188, score-0.642]
</p><p>77 At low frequency separations, both the high and low frequency segments fall under the attentional interest peak; this allows the oscillator activities of both frequency bands to influence the ALI and hence they are considered to be in the attentional foreground. [sent-189, score-1.672]
</p><p>78 At higher frequency separations, one of the frequency bands falls outside of the attentional peak (in this example, the high frequency tones fall outside) and hence it cannot influence the ALI. [sent-190, score-1.008]
</p><p>79 Such behaviour is not seen immediately, because the attentional interest vector is subject to a build up effect as described in (9). [sent-191, score-0.514]
</p><p>80 Initially the attentional interest is maximal across all frequencies; as the leaky integrator value increases, the interest peak begins to dominate and interest in other frequencies tends toward zero. [sent-192, score-0.93]
</p><p>81 4  Discussion  A model of auditory attention has been presented which is based on previous neural oscillator work by Wang and colleagues [10], [11] but differs in two important respects. [sent-193, score-0.636]
</p><p>82 In our model, attentional interest may be consciously directed toward a particular stream, causing that stream to be selected as the attentional foreground. [sent-196, score-1.049]
</p><p>83 Few auditory models have incorporated attentional effects in a plausible manner. [sent-197, score-0.605]
</p><p>84 For example, Wang’s ‘shifting synchronisation’ theory [3] suggests that attention is directed towards a stream when its constituent oscillators reach the active phase. [sent-198, score-0.577]
</p><p>85 Additionally, Wang’s model fails to account for exogenous reorientation of attention to a sudden loud stimulus; the shifting synchronisation approach would multiplex it as normal with no attentional emphasis. [sent-200, score-0.583]
</p><p>86 By ensuring that the minimum Ak value for the attentional interest is always non-zero, it is possible to weight activity outside of the attentional interest peak and force it to influence the ALI. [sent-201, score-1.184]
</p><p>87 The time course of perception is well simulated, showing how factors such as mistuning and onset asynchrony can cause a harmonic to be segregated from a complex tone. [sent-204, score-0.537]
</p><p>88 It is interesting to note that a good match to Darwin’s pitch shift data (Figure 2D) was only found when harmonically related segments below 1. [sent-205, score-0.408]
</p><p>89 The dominance of lower (resolved) harmonics on pitch is well known [8], and our findings suggest that the correlogram does not accurately model this aspect of pitch perception. [sent-207, score-0.579]
</p><p>90 120 Channel  100 80 60 40 20 0  50  100  150  200  250  300  350  Time (ms)  Figure 4: Asynchronous onset of the fourth harmonic causes it to segregate into a separate stream. [sent-208, score-0.423]
</p><p>91 The attentional interest vector (Ak) is shown to the right of the figure. [sent-209, score-0.473]
</p><p>92 100 Channel  Channel  100 90 80  90 80  0  200  400 Time (ms)  600  0  200  400 Time (ms)  600  Figure 5: Auditory streaming at frequency separations of 5 semitones (left) and 3 semitones (right). [sent-210, score-0.437]
</p><p>93 The timescale of adaptation for the attentional interest has been reduced to aid the clarity of the figures. [sent-212, score-0.473]
</p><p>94 The simulation of two tone streaming shows how the proposed attentional mechanism and its cross-frequency spread accounts for grouping of sequential events according to their proximity in frequency. [sent-213, score-0.708]
</p><p>95 A sequence of two tones will only stream if one set of tones fall outside of the peak of attentional interest. [sent-214, score-0.938]
</p><p>96 Frequency separations for streaming to occur in the model (greater than 3 to 4 semitones) are in agreement with experimental data, as is the timescale for the build-up of the streaming effect [1]. [sent-215, score-0.415]
</p><p>97 In summary, we have proposed a physiologically plausible model in which auditory streams are encoded by a unidimensional neural oscillator network. [sent-216, score-0.614]
</p><p>98 The network creates auditory streams according to grouping factors such as harmonicity, frequency proximity and common onset, and selects one stream as the attentional foreground. [sent-217, score-1.04]
</p><p>99 Current work is concentrating on expanding the system to include binaural effects, such as inter-ear attentional competition [4]. [sent-218, score-0.38]
</p><p>100 (2001) Effects of attention and unilateral neglect on auditory stream segregation. [sent-245, score-0.495]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attentional', 0.38), ('oscillator', 0.309), ('harmonic', 0.278), ('oscillators', 0.253), ('ali', 0.245), ('auditory', 0.225), ('pitch', 0.186), ('streaming', 0.171), ('stream', 0.168), ('channel', 0.166), ('synchronised', 0.14), ('correlogram', 0.126), ('tones', 0.122), ('channels', 0.119), ('segments', 0.119), ('segment', 0.111), ('frequency', 0.109), ('attention', 0.102), ('mistuning', 0.098), ('age', 0.098), ('fourth', 0.096), ('interest', 0.093), ('activity', 0.09), ('leaky', 0.088), ('ak', 0.087), ('darwin', 0.084), ('tone', 0.083), ('peak', 0.083), ('wang', 0.078), ('excitatory', 0.075), ('grouping', 0.074), ('endogenous', 0.073), ('separations', 0.073), ('integrator', 0.072), ('oscillatory', 0.072), ('harmonically', 0.07), ('harmonicity', 0.07), ('mistunings', 0.07), ('trackers', 0.07), ('segregation', 0.061), ('ms', 0.061), ('perceptual', 0.059), ('autocorrelation', 0.056), ('streams', 0.052), ('complex', 0.051), ('onset', 0.049), ('lag', 0.049), ('exogenous', 0.049), ('asa', 0.049), ('harmonics', 0.049), ('inhibitor', 0.049), ('captor', 0.042), ('carlyon', 0.042), ('desynchronised', 0.042), ('ihigh', 0.042), ('mistuned', 0.042), ('nerve', 0.042), ('oscillations', 0.042), ('semitones', 0.042), ('attended', 0.041), ('temporally', 0.041), ('behaviour', 0.041), ('sound', 0.041), ('links', 0.038), ('acoustic', 0.038), ('segregated', 0.033), ('khz', 0.033), ('shift', 0.033), ('influence', 0.033), ('connections', 0.033), ('network', 0.032), ('outside', 0.032), ('findings', 0.032), ('period', 0.031), ('io', 0.031), ('stimulated', 0.031), ('fall', 0.031), ('summary', 0.03), ('frequencies', 0.028), ('asynchrony', 0.028), ('ilow', 0.028), ('integrators', 0.028), ('legion', 0.028), ('maxak', 0.028), ('sudden', 0.028), ('sustained', 0.028), ('unidimensional', 0.028), ('wz', 0.028), ('xmax', 0.028), ('active', 0.028), ('causing', 0.028), ('excitation', 0.027), ('rest', 0.026), ('global', 0.026), ('constituent', 0.026), ('block', 0.025), ('modelled', 0.025), ('loud', 0.024), ('cochlear', 0.024), ('wik', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="14-tfidf-1" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>Author: Stuart N. Wrigley, Guy J. Brown</p><p>Abstract: A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. 1</p><p>2 0.12666042 <a title="14-tfidf-2" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>Author: Laurent Itti, Jochen Braun, Christof Koch</p><p>Abstract: We present new simulation results , in which a computational model of interacting visual neurons simultaneously predicts the modulation of spatial vision thresholds by focal visual attention, for five dual-task human psychophysics experiments. This new study complements our previous findings that attention activates a winnertake-all competition among early visual neurons within one cortical hypercolumn. This</p><p>3 0.10176852 <a title="14-tfidf-3" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>4 0.099214382 <a title="14-tfidf-4" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>5 0.067244537 <a title="14-tfidf-5" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>Author: Brendan J. Frey, Trausti T. Kristjansson, Li Deng, Alex Acero</p><p>Abstract: A challenging, unsolved problem in the speech recognition community is recognizing speech signals that are corrupted by loud, highly nonstationary noise. One approach to noisy speech recognition is to automatically remove the noise from the cepstrum sequence before feeding it in to a clean speech recognizer. In previous work published in Eurospeech, we showed how a probability model trained on clean speech and a separate probability model trained on noise could be combined for the purpose of estimating the noisefree speech from the noisy speech. We showed how an iterative 2nd order vector Taylor series approximation could be used for probabilistic inference in this model. In many circumstances, it is not possible to obtain examples of noise without speech. Noise statistics may change significantly during an utterance, so that speechfree frames are not sufficient for estimating the noise model. In this paper, we show how the noise model can be learned even when the data contains speech. In particular, the noise model can be learned from the test utterance and then used to de noise the test utterance. The approximate inference technique is used as an approximate E step in a generalized EM algorithm that learns the parameters of the noise model from a test utterance. For both Wall Street J ournal data with added noise samples and the Aurora benchmark, we show that the new noise adaptive technique performs as well as or significantly better than the non-adaptive algorithm, without the need for a separate training set of noise examples. 1</p><p>6 0.063519128 <a title="14-tfidf-6" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>7 0.058694761 <a title="14-tfidf-7" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>8 0.0528282 <a title="14-tfidf-8" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>9 0.05166024 <a title="14-tfidf-9" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>10 0.050460193 <a title="14-tfidf-10" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>11 0.050289862 <a title="14-tfidf-11" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>12 0.047638632 <a title="14-tfidf-12" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>13 0.047620963 <a title="14-tfidf-13" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>14 0.046195574 <a title="14-tfidf-14" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>15 0.044828206 <a title="14-tfidf-15" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>16 0.042044904 <a title="14-tfidf-16" href="./nips-2001-Tempo_tracking_and_rhythm_quantization_by_sequential_Monte_Carlo.html">179 nips-2001-Tempo tracking and rhythm quantization by sequential Monte Carlo</a></p>
<p>17 0.040820573 <a title="14-tfidf-17" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>18 0.040534116 <a title="14-tfidf-18" href="./nips-2001-Estimating_the_Reliability_of_ICA_Projections.html">71 nips-2001-Estimating the Reliability of ICA Projections</a></p>
<p>19 0.03845581 <a title="14-tfidf-19" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>20 0.038313083 <a title="14-tfidf-20" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.103), (1, -0.129), (2, -0.068), (3, -0.027), (4, -0.039), (5, 0.017), (6, -0.022), (7, -0.001), (8, 0.02), (9, -0.044), (10, -0.013), (11, 0.083), (12, -0.092), (13, 0.092), (14, 0.004), (15, -0.038), (16, -0.004), (17, 0.064), (18, -0.013), (19, 0.033), (20, -0.121), (21, -0.036), (22, -0.004), (23, -0.02), (24, -0.012), (25, -0.059), (26, 0.062), (27, 0.011), (28, -0.02), (29, -0.019), (30, -0.033), (31, -0.02), (32, 0.019), (33, 0.102), (34, 0.042), (35, 0.13), (36, -0.09), (37, 0.07), (38, -0.073), (39, -0.062), (40, 0.008), (41, -0.077), (42, 0.067), (43, 0.172), (44, -0.04), (45, 0.165), (46, -0.003), (47, 0.05), (48, 0.061), (49, -0.159)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96788383 <a title="14-lsi-1" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>Author: Stuart N. Wrigley, Guy J. Brown</p><p>Abstract: A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. 1</p><p>2 0.67715359 <a title="14-lsi-2" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<p>Author: Laurent Itti, Jochen Braun, Christof Koch</p><p>Abstract: We present new simulation results , in which a computational model of interacting visual neurons simultaneously predicts the modulation of spatial vision thresholds by focal visual attention, for five dual-task human psychophysics experiments. This new study complements our previous findings that attention activates a winnertake-all competition among early visual neurons within one cortical hypercolumn. This</p><p>3 0.52583635 <a title="14-lsi-3" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>4 0.50845039 <a title="14-lsi-4" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>Author: H. Colonius, A. Diederich</p><p>Abstract: Multisensory response enhancement (MRE) is the augmentation of the response of a neuron to sensory input of one modality by simultaneous input from another modality. The maximum likelihood (ML) model presented here modifies the Bayesian model for MRE (Anastasio et al.) by incorporating a decision strategy to maximize the number of correct decisions. Thus the ML model can also deal with the important tasks of stimulus discrimination and identification in the presence of incongruent visual and auditory cues. It accounts for the inverse effectiveness observed in neurophysiological recording data, and it predicts a functional relation between uni- and bimodal levels of discriminability that is testable both in neurophysiological and behavioral experiments. 1</p><p>5 0.38804725 <a title="14-lsi-5" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>Author: B. D. Wright, Kamal Sen, William Bialek, A. J. Doupe</p><p>Abstract: In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra ﬁnches, naturalistic stimuli can be deﬁned as sounds that they encounter in a colony of conspeciﬁc birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra ﬁnches, and then analyzed the response of single neurons in the songbird central auditory area (ﬁeld L) to continuous playback of long segments from this ensemble. Following methods developed in the ﬂy visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in ﬁeld L, temporal patterns give at least % extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a signiﬁcant role.   </p><p>6 0.34939161 <a title="14-lsi-6" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>7 0.34874034 <a title="14-lsi-7" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>8 0.33935517 <a title="14-lsi-8" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>9 0.32582763 <a title="14-lsi-9" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>10 0.30697748 <a title="14-lsi-10" href="./nips-2001-A_Bayesian_Network_for_Real-Time_Musical_Accompaniment.html">6 nips-2001-A Bayesian Network for Real-Time Musical Accompaniment</a></p>
<p>11 0.30240825 <a title="14-lsi-11" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>12 0.29741049 <a title="14-lsi-12" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>13 0.296781 <a title="14-lsi-13" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>14 0.28931707 <a title="14-lsi-14" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>15 0.28442332 <a title="14-lsi-15" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>16 0.26583391 <a title="14-lsi-16" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>17 0.2653729 <a title="14-lsi-17" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>18 0.24389872 <a title="14-lsi-18" href="./nips-2001-Tempo_tracking_and_rhythm_quantization_by_sequential_Monte_Carlo.html">179 nips-2001-Tempo tracking and rhythm quantization by sequential Monte Carlo</a></p>
<p>19 0.24192052 <a title="14-lsi-19" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>20 0.23283991 <a title="14-lsi-20" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.022), (17, 0.022), (19, 0.029), (21, 0.414), (27, 0.082), (30, 0.057), (38, 0.024), (59, 0.015), (72, 0.03), (74, 0.018), (79, 0.057), (91, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83749652 <a title="14-lda-1" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>Author: Stuart N. Wrigley, Guy J. Brown</p><p>Abstract: A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone. 1</p><p>2 0.48800206 <a title="14-lda-2" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><p>3 0.39063883 <a title="14-lda-3" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>4 0.38911211 <a title="14-lda-4" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>5 0.3870151 <a title="14-lda-5" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>6 0.38671547 <a title="14-lda-6" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>7 0.38663733 <a title="14-lda-7" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>8 0.38646081 <a title="14-lda-8" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>9 0.38502347 <a title="14-lda-9" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>10 0.38489366 <a title="14-lda-10" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>11 0.38362277 <a title="14-lda-11" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>12 0.38289711 <a title="14-lda-12" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>13 0.3828482 <a title="14-lda-13" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>14 0.38227421 <a title="14-lda-14" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>15 0.3820377 <a title="14-lda-15" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>16 0.3812069 <a title="14-lda-16" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>17 0.38103378 <a title="14-lda-17" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>18 0.37953147 <a title="14-lda-18" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>19 0.3795296 <a title="14-lda-19" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>20 0.37938988 <a title="14-lda-20" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
