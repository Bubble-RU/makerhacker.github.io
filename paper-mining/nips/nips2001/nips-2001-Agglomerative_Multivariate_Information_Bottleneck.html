<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2001-Agglomerative Multivariate Information Bottleneck</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-30" href="#">nips2001-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2001-Agglomerative Multivariate Information Bottleneck</h1>
<br/><p>Source: <a title="nips-2001-30-pdf" href="http://papers.nips.cc/paper/1952-agglomerative-multivariate-information-bottleneck.pdf">pdf</a></p><p>Author: Noam Slonim, Nir Friedman, Naftali Tishby</p><p>Abstract: The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution peA, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets.</p><p>Reference: <a title="nips-2001-30-reference" href="../nips2001_reference/nips-2001-Agglomerative_Multivariate_Information_Bottleneck_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Agglomerative Multivariate Information Bottleneck  Noam Sionim Nir Friedman Naftali Tishby School of Computer Science & Engineering, Hebrew University, Jerusalem 91904, Israel {noamm, nir, tishby } @cs. [sent-1, score-0.234]
</p><p>2 il  Abstract The information bottleneck method is an unsupervised model independent data organization technique. [sent-4, score-0.491]
</p><p>3 Given a joint distribution peA, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. [sent-5, score-0.469]
</p><p>4 In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. [sent-6, score-0.89]
</p><p>5 In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. [sent-7, score-0.444]
</p><p>6 We analyze the behavior of these algorithms and apply them to several real-life datasets. [sent-8, score-0.074]
</p><p>7 1 Introduction The information bottleneck (IB) method of Tishby et al [14] is an unsupervised nonparametric data organization technique. [sent-9, score-0.629]
</p><p>8 Given a joint distribution P(A, B), this method constructs a new variable T that represents partitions of A which are (locally) maximizing the mutual information about B. [sent-10, score-0.628]
</p><p>9 In other words, the variable T induces a sufficient partition, or informative features of the variable A with respect to B. [sent-11, score-0.334]
</p><p>10 The construction of T finds a tradeoff between the information about A that we try to minimize, J(T; A), and the information about B which we try to maximize, J(T ; B). [sent-12, score-0.569]
</p><p>11 This approach is particularly useful for co-occurrence data, such as words and documents [12], where we want to capture what information one variable (e. [sent-13, score-0.288]
</p><p>12 This extension allows us to consider cases where the data partition is relevant with respect to several variables, or where we construct several systems of clusters simultaneously. [sent-20, score-0.368]
</p><p>13 In this framework, we specify the desired interactions by a pair of Bayesian networks. [sent-21, score-0.114]
</p><p>14 One network, Gin, represents which variables are compressed versions of the observed variables - each new variable compresses its parents in the network. [sent-22, score-0.518]
</p><p>15 The second network, Gout> defines the statistical relationship between these new variables and the observed variables that should be maintained. [sent-23, score-0.243]
</p><p>16 we formulated the general principle as a tradeoff between the (multi) information each network carries. [sent-25, score-0.362]
</p><p>17 On the one hand, we want to minimize the information maintained by G in and on the other to maximize the information maintained by Gout. [sent-26, score-0.431]
</p><p>18 We also provide a characterization of stationary points in this tradeoff as a set of self-consistent equations. [sent-27, score-0.306]
</p><p>19 Moreover, we prove that iterations of these equations converges to a (local) optimum. [sent-28, score-0.061]
</p><p>20 Then, we describe a deterministic annealing procedure  that constructs a solution by tracking the bifurcation of clusters as it traverses the tradeoff curve, similar to the original IB method. [sent-29, score-0.847]
</p><p>21 In this paper, we consider an alternative approach to solving multivariate IB problems which is motivated by the success of the agglomerative IB of Slonim and Tishby [11]. [sent-30, score-0.625]
</p><p>22 As shown there, a bottom-up greedy agglomeration is a simple heuristic procedure that can yield good solutions to the original IB problem. [sent-31, score-0.547]
</p><p>23 Here we extend this idea in the context of multivariate IB problems. [sent-32, score-0.264]
</p><p>24 We start by analyzing the cost of agglomeration steps within this framework. [sent-33, score-0.41]
</p><p>25 This both elucidates the criteria that guides greedy agglomeration and provides for efficient local evaluation rules for agglomeration steps. [sent-34, score-0.992]
</p><p>26 This construction results with a novel family of information theoretic agglomerative clustering algorithms, that can be specified using the graphs G in and G out. [sent-35, score-0.639]
</p><p>27 We demonstrate the performance of some of these algorithms for document and word clustering and gene expression analysis. [sent-36, score-0.319]
</p><p>28 2 Multivariate Information Bottleneck A Bayesian network structure G is a DAG that specifies interactions among variables [8]. [sent-37, score-0.284]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agglomeration', 0.371), ('ib', 0.361), ('agglomerative', 0.294), ('multivariate', 0.264), ('bottleneck', 0.261), ('tishby', 0.234), ('tradeoff', 0.225), ('partitions', 0.169), ('constructs', 0.157), ('nir', 0.147), ('friedman', 0.136), ('maintained', 0.108), ('clusters', 0.105), ('informative', 0.095), ('partition', 0.09), ('document', 0.09), ('organization', 0.088), ('variables', 0.085), ('compresses', 0.081), ('guides', 0.081), ('bifurcation', 0.081), ('dag', 0.081), ('try', 0.079), ('variable', 0.077), ('interactions', 0.075), ('noam', 0.073), ('traverses', 0.073), ('pea', 0.068), ('slonim', 0.068), ('network', 0.065), ('construction', 0.065), ('greedy', 0.065), ('unsupervised', 0.065), ('hebrew', 0.064), ('jerusalem', 0.064), ('naftali', 0.064), ('word', 0.064), ('clustering', 0.063), ('family', 0.062), ('gene', 0.061), ('multi', 0.061), ('extension', 0.06), ('compressed', 0.059), ('specifies', 0.059), ('parents', 0.056), ('extracts', 0.054), ('nonparametric', 0.054), ('annealing', 0.052), ('induces', 0.051), ('israel', 0.051), ('maximize', 0.05), ('characterization', 0.049), ('documents', 0.049), ('finds', 0.049), ('want', 0.049), ('tracking', 0.048), ('et', 0.047), ('construct', 0.047), ('theoretic', 0.046), ('joint', 0.045), ('minimize', 0.044), ('defines', 0.042), ('original', 0.042), ('method', 0.041), ('words', 0.041), ('pa', 0.041), ('algorithms', 0.041), ('bayesian', 0.041), ('recent', 0.041), ('versions', 0.04), ('extensions', 0.04), ('analyzing', 0.039), ('specify', 0.039), ('specified', 0.039), ('school', 0.038), ('criteria', 0.038), ('principled', 0.038), ('al', 0.037), ('heuristic', 0.037), ('information', 0.036), ('formulated', 0.036), ('particularly', 0.036), ('mutual', 0.036), ('represents', 0.035), ('graphs', 0.034), ('sufficient', 0.034), ('rules', 0.034), ('locally', 0.034), ('success', 0.034), ('several', 0.033), ('motivated', 0.033), ('stationary', 0.032), ('local', 0.032), ('procedure', 0.032), ('deterministic', 0.032), ('maximizing', 0.032), ('prove', 0.031), ('relationship', 0.031), ('denoted', 0.03), ('converges', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="30-tfidf-1" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>Author: Noam Slonim, Nir Friedman, Naftali Tishby</p><p>Abstract: The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution peA, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets.</p><p>2 0.13538875 <a title="30-tfidf-2" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>Author: Ran El-Yaniv, Oren Souroujon</p><p>Abstract: We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically ﬁnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiﬁcantly more accurate classiﬁcation. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the eﬀectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples. 1</p><p>3 0.082392596 <a title="30-tfidf-3" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan, Yair Weiss</p><p>Abstract: Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1</p><p>4 0.080335528 <a title="30-tfidf-4" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1</p><p>5 0.067268856 <a title="30-tfidf-5" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>Author: David M. Blei, Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1</p><p>6 0.066348843 <a title="30-tfidf-6" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>7 0.062426582 <a title="30-tfidf-7" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>8 0.056954145 <a title="30-tfidf-8" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>9 0.051548507 <a title="30-tfidf-9" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>10 0.047989532 <a title="30-tfidf-10" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>11 0.046222415 <a title="30-tfidf-11" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>12 0.045370627 <a title="30-tfidf-12" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>13 0.042890932 <a title="30-tfidf-13" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>14 0.042500626 <a title="30-tfidf-14" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>15 0.041688606 <a title="30-tfidf-15" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>16 0.040344652 <a title="30-tfidf-16" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>17 0.039470278 <a title="30-tfidf-17" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>18 0.038437497 <a title="30-tfidf-18" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>19 0.038073674 <a title="30-tfidf-19" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>20 0.037625998 <a title="30-tfidf-20" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.122), (1, -0.0), (2, 0.01), (3, -0.084), (4, 0.027), (5, -0.102), (6, -0.053), (7, -0.029), (8, -0.065), (9, -0.047), (10, 0.134), (11, -0.028), (12, -0.079), (13, 0.023), (14, -0.017), (15, -0.01), (16, -0.004), (17, -0.044), (18, -0.061), (19, -0.047), (20, 0.017), (21, 0.008), (22, 0.03), (23, -0.032), (24, 0.142), (25, 0.052), (26, 0.101), (27, 0.149), (28, 0.045), (29, -0.003), (30, -0.071), (31, -0.062), (32, -0.052), (33, 0.054), (34, -0.019), (35, 0.091), (36, -0.028), (37, -0.131), (38, 0.18), (39, 0.022), (40, 0.145), (41, -0.005), (42, 0.018), (43, -0.081), (44, -0.027), (45, -0.112), (46, -0.013), (47, -0.02), (48, 0.23), (49, 0.144)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97318912 <a title="30-lsi-1" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>Author: Noam Slonim, Nir Friedman, Naftali Tishby</p><p>Abstract: The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution peA, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets.</p><p>2 0.68464464 <a title="30-lsi-2" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>Author: Ran El-Yaniv, Oren Souroujon</p><p>Abstract: We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically ﬁnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiﬁcantly more accurate classiﬁcation. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the eﬀectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples. 1</p><p>3 0.39588621 <a title="30-lsi-3" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><p>4 0.3958534 <a title="30-lsi-4" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>Author: Wheeler Ruml</p><p>Abstract: If the promise of computational modeling is to be fully realized in higherlevel cognitive domains such as language processing, principled methods must be developed to construct the semantic representations used in such models. In this paper, we propose the use of an established formalism from mathematical psychology, additive clustering, as a means of automatically constructing binary representations for objects using only pairwise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large problems. We present a new algorithm for additive clustering, based on a novel heuristic technique for combinatorial optimization. The algorithm is simpler than previous formulations and makes fewer independence assumptions. Extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems. By making additive clustering practical, we take a signiﬁcant step toward scaling connectionist models beyond hand-coded examples.</p><p>5 0.39026821 <a title="30-lsi-5" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann</p><p>Abstract: We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure. Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances. This procedure requires identifying the exact relationship between permutations and tours. In a learning setting, the average trajectory is used as a model to construct solutions to new instances sampled from the same source. Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance. 1</p><p>6 0.38855943 <a title="30-lsi-6" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>7 0.38604403 <a title="30-lsi-7" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>8 0.37117293 <a title="30-lsi-8" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>9 0.32759622 <a title="30-lsi-9" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>10 0.31882176 <a title="30-lsi-10" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>11 0.30817744 <a title="30-lsi-11" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>12 0.28504488 <a title="30-lsi-12" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>13 0.28205639 <a title="30-lsi-13" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>14 0.27741253 <a title="30-lsi-14" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>15 0.26436511 <a title="30-lsi-15" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>16 0.24740073 <a title="30-lsi-16" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>17 0.24665609 <a title="30-lsi-17" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<p>18 0.24227244 <a title="30-lsi-18" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>19 0.2393232 <a title="30-lsi-19" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>20 0.23653859 <a title="30-lsi-20" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.024), (15, 0.378), (19, 0.015), (27, 0.062), (30, 0.081), (72, 0.07), (79, 0.032), (91, 0.228)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82545459 <a title="30-lda-1" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>Author: Noam Slonim, Nir Friedman, Naftali Tishby</p><p>Abstract: The information bottleneck method is an unsupervised model independent data organization technique. Given a joint distribution peA, B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. In a recent paper, we introduced a general principled framework for multivariate extensions of the information bottleneck method that allows us to consider multiple systems of data partitions that are inter-related. In this paper, we present a new family of simple agglomerative algorithms to construct such systems of inter-related clusters. We analyze the behavior of these algorithms and apply them to several real-life datasets.</p><p>2 0.71575177 <a title="30-lda-2" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>Author: Ji Zhu, Trevor Hastie</p><p>Abstract: The support vector machine (SVM) is known for its good performance in binary classiﬁcation, but its extension to multi-class classiﬁcation is still an on-going research issue. In this paper, we propose a new approach for classiﬁcation, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in binary classiﬁcation, but also can naturally be generalized to the multi-class case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the “support points” of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.</p><p>3 0.66796207 <a title="30-lda-3" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>Author: Sanjoy Dasgupta, Michael L. Littman, David A. McAllester</p><p>Abstract: The rule-based bootstrapping introduced by Yarowsky, and its cotraining variant by Blum and Mitchell, have met with considerable empirical success. Earlier work on the theory of co-training has been only loosely related to empirically useful co-training algorithms. Here we give a new PAC-style bound on generalization error which justiﬁes both the use of conﬁdences — partial rules and partial labeling of the unlabeled data — and the use of an agreement-based objective function as suggested by Collins and Singer. Our bounds apply to the multiclass case, i.e., where instances are to be assigned one of labels for . £ ¡ ¤¢</p><p>4 0.57607043 <a title="30-lda-4" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>Author: Ran El-Yaniv, Oren Souroujon</p><p>Abstract: We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically ﬁnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiﬁcantly more accurate classiﬁcation. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the eﬀectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples. 1</p><p>5 0.5542441 <a title="30-lda-5" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>6 0.55366367 <a title="30-lda-6" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>7 0.5518778 <a title="30-lda-7" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>8 0.54968226 <a title="30-lda-8" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>9 0.54776722 <a title="30-lda-9" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>10 0.54752082 <a title="30-lda-10" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>11 0.54108161 <a title="30-lda-11" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>12 0.54073358 <a title="30-lda-12" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>13 0.53799212 <a title="30-lda-13" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>14 0.53743494 <a title="30-lda-14" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>15 0.52883238 <a title="30-lda-15" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>16 0.52307451 <a title="30-lda-16" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>17 0.52034158 <a title="30-lda-17" href="./nips-2001-Model_Based_Population_Tracking_and_Automatic_Detection_of_Distribution_Changes.html">122 nips-2001-Model Based Population Tracking and Automatic Detection of Distribution Changes</a></p>
<p>18 0.51997244 <a title="30-lda-18" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>19 0.51975024 <a title="30-lda-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.51956534 <a title="30-lda-20" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
