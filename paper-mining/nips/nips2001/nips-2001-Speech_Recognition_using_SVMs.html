<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 nips-2001-Speech Recognition using SVMs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-172" href="#">nips2001-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 nips-2001-Speech Recognition using SVMs</h1>
<br/><p>Source: <a title="nips-2001-172-pdf" href="http://papers.nips.cc/paper/2044-speech-recognition-using-svms.pdf">pdf</a></p><p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>Reference: <a title="nips-2001-172-reference" href="../nips2001_reference/nips-2001-Speech_Recognition_using_SVMs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. [sent-11, score-0.267]
</p><p>2 The score-space defined by this mapping avoids some limitations of the Fisher score. [sent-14, score-0.205]
</p><p>3 Class-conditional generative models are directly incorporated into the definition of the score-space. [sent-15, score-0.361]
</p><p>4 The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. [sent-16, score-0.697]
</p><p>5 State-of-the-art systems use Hidden Markov Models (HMMs), either trained to maximise likelihood or discriminatively, to achieve good levels of performance. [sent-18, score-0.247]
</p><p>6 This paper examines the application of SVMs to speech recognition. [sent-22, score-0.187]
</p><p>7 Their scheme uses generative probability models of the data to define a mapping into a fixed dimension space, the Fisher score-space. [sent-33, score-0.409]
</p><p>8 This paper  examines the suitability of the Fisher kernel for classification in speech recognition and proposes an alternative, more useful, kernel. [sent-36, score-0.406]
</p><p>9 In addition some normalisation issues associated with using this kernel for speech recognition are addressed. [sent-37, score-0.522]
</p><p>10 OT) where Ot E ~D , and a set of generative probability models of the observation sequences as P = {Pk(OI(h)}, where 9 k is the vector of parameters for the kth member of the set. [sent-45, score-0.393]
</p><p>11 The observation sequence 0 can be mapped into a vector of fixed dimension [4],  i{J~ (0)  (1)  f(Âˇ) is the score-argument and is a function of the members of the set of generative models P. [sent-46, score-0.396]
</p><p>12 What are the best generative models, scorearguments and score-operators to use? [sent-50, score-0.264]
</p><p>13 2  Score-spaces  As HMMs have proved successful in speech recognition, they are a natural choice as the generative models for this task. [sent-51, score-0.462]
</p><p>14 For a two-class problem, let Pi(019 i ) represent a generative model, where i = {g, 1, 2} (g denotes the global2-class generative model, and 1 and 2 denote the class-conditional generative models for the two competing classes). [sent-54, score-0.849]
</p><p>15 Previous schemes have used the log of a single generative model, Inpi (019 i ) representing either both classes as in the original Fisher score (i = g) [4], or one of the classes (i = 1 or 2) [6]. [sent-55, score-0.497]
</p><p>16 The score-space proposed in this paper uses the log of the ratio of the two classconditional generative models, In(P1(019d / P2(019 2)) where 9 = [9{,9J] T. [sent-57, score-0.33]
</p><p>17 Thus, i{J~k(O)  (2)  i{J~(0)  (3)  The likelihood-ratio score-space can be shown to avoid some of the limitations of the likelihood score-space, and may be viewed as a generalisation of the standard generative model classifier. [sent-59, score-0.38]
</p><p>18 Having proposed forms for the generative models and score-arguments, the scoreoperators must be selected. [sent-61, score-0.321]
</p><p>19 The 1st-order derivatives of the log probability of the sequence 0 with respect to the model parameters are given below1, where the derivative operator has been defined to give column vectors, T  L ')'jk(t)S~,jkl t= l  lFor fuller details of the derivations see [2). [sent-72, score-0.451]
</p><p>20 (4)  V Wjk Inp(OIO) where  S[t ,jk]  Ijdt) is the posterior probability of component k of state j at time t. [sent-73, score-0.189]
</p><p>21 Assuming the HMM is left-to-right with no skips and assuming that a state only appears once in the HMM (i. [sent-74, score-0.151]
</p><p>22 From the definitions above, the score for an utterance is a weighted sum of scores for individual observations. [sent-77, score-0.172]
</p><p>23 If the scores for the same utterance spoken at different speaking rates were calculated, they would lie in different regions of score-space simply because of differing numbers of observations. [sent-78, score-0.179]
</p><p>24 To ease the task of the classifier in score-space, the score-space may be normalised by the number of observations, called sequence length normalisation. [sent-79, score-0.272]
</p><p>25 One method of normalisation redefines score-spaces using generative models trained to maximise a modified log likelihood function, In( 010). [sent-81, score-0.887]
</p><p>26 Consider that state j has entry time Tj and duration d j (both in numbers of observations) and output probability bj(Ot) for observation Ot [7]. [sent-82, score-0.171]
</p><p>27 However, in this paper, a simpler normalisation method is employed. [sent-85, score-0.253]
</p><p>28 The generative models are trained to maximise the standard likelihood function. [sent-86, score-0.568]
</p><p>29 Rather than define the score-space using standard state posteriors Ij(t) (the posterior probability of state j at time t), it is defined on state posteriors normalised by the total state occupancy over the utterance. [sent-87, score-0.774]
</p><p>30 The standard component posteriors 1 j k (t) are replaced in Equations 4 to 6 and 8 by their normalised form  'Yjk(t), A  . [sent-88, score-0.241]
</p><p>31 ~k  (t) _ -  Ij(t)  T  (WjkN(Ot; ILjk, ~jk) K  2:: T=l/j(T) 2:: i = l wjiN(ot; ILji' ~ji)  )  (10)  In effect, each derivative is divided by the sum of state posteriors. [sent-89, score-0.201]
</p><p>32 This is preferred to division by the total number of observations T which assumes that when the utterance length varies, the occupation of every state in the state sequence is scaled by the same ratio. [sent-90, score-0.437]
</p><p>33 The nature of the score-space affects the discriminative power of classifiers built in the score-space. [sent-92, score-0.33]
</p><p>34 For example, the likelihood score-space defined on a two-class 2Due to the sum to unity constraints, one of the weight parameters in each Gaussian mixture is discarded from the definition of the super-vector, as are the forward transitions in the left-to-right HMM with no skips. [sent-93, score-0.158]
</p><p>35 generative model is susceptible to wrap-around [7] . [sent-94, score-0.264]
</p><p>36 If an observation is generated at the peak of the first Gaussian, then the derivative relative to the mean of that Gaussian is zero because S [t ,jk] is zero (see Equation 4). [sent-97, score-0.21]
</p><p>37 However, the derivative relative to the mean of the distant second Gaussian is also zero because of a zero component posterior f jdt). [sent-98, score-0.264]
</p><p>38 A likelihood-ratio score-space defined on these two Gaussians does not suffer wraparound since the component posteriors for each Gaussian are forced to unity. [sent-103, score-0.218]
</p><p>39 For example, the zeroth-order derivative for the likelihood score-space is expected to be less useful than its counter-part in the likelihood-ratio score-space because of its greater sensitivity to acoustic conditions. [sent-107, score-0.179]
</p><p>40 Consider the simple case of true class-conditional generative models P1(OIOd and P2(OI02) with respective estimates of the same functional form P1 (0 10d and P2(010 2 ) . [sent-109, score-0.321]
</p><p>41 ]T Inpi (OIOi )  3) (11)  The output from the operator in square brackets is an infinite number of derivatives arranged as a column vector. [sent-113, score-0.162]
</p><p>42 The expressions for the two true models can be incorporated into an optimal minimum Bayes error decision A  rule as follows , where 0 priors,  AT  AT  [0 1 , 02 ]T , W = [w i, WJjT, and b encodes the class  +b wi[l, V'~1' vec(V' 91V'~1) T . [sent-115, score-0.222]
</p><p>43 ]T I n P1(OIOd + b P2(OI02) w Tiplr(o) + b  a  Inp1(OIOd -lnp2(OI02)  A  a  (12)  iplr(o) is a score in the likelihood-ratio score-space formed by an infinite number of derivatives with respect to the parameter estimates O. [sent-124, score-0.241]
</p><p>44 However, most HMMs used in speech recognition are 1st-order Markov processes but speech is a high-order or infinite-order Markov  process. [sent-128, score-0.329]
</p><p>45 Therefore, a linear decision boundary in the likelihood-ratio score-space defined on 1st-order Markov model estimates is unlikely to be sufficient for recovering the optimal decision rule due to model incorrectness. [sent-129, score-0.166]
</p><p>46 However, powerful non-linear classifiers may be trained in such a likelihood-ratio score-space to try to compensate for model incorrectness and approximate the optimal decision rule. [sent-130, score-0.27]
</p><p>47 However, an example of a 2nd-order derivative is V' J-L jk (V'~;k Inp(OIO)) , T  V' J-L;k (V'~;k Inp(OIO))  ~  -  L 'Yjk(t)"2';;k1  (13)  t= l  For simplicity the component posterior 'Yj k (t) is assumed independent of J-L j k. [sent-134, score-0.32]
</p><p>48 Failure to perform such score-space normalisation for a linear kernel in score-space results in a kernel similar to the Plain kernel [5]. [sent-140, score-0.496]
</p><p>49 Simple scaling has been found to be a reasonable approximation to full whitening and avoids inverting large matrices in [2] (though for classification of single observations rather than sequences, on a different database). [sent-142, score-0.201]
</p><p>50 This is only an acceptable normalisation for a likelihood score-space under conditions that give a zero expectation in score-space. [sent-144, score-0.403]
</p><p>51 240 utterances per letter from isolet{ 1,2,3,4} were used for training and 60 utterances per letter from isolet5 for testing. [sent-149, score-0.472]
</p><p>52 The baseline HMM system was well-trained to maximise likelihood. [sent-152, score-0.23]
</p><p>53 Each letter was modelled by a 10-emitting state left-to-right continuous density HMM with no skips, and silence by a single emitting-state HMM with no skips. [sent-153, score-0.283]
</p><p>54 Each state output distribution had the same number of Gaussian components with diagonal covariance matrices. [sent-154, score-0.147]
</p><p>55 31t is useful to note that a linear decision boundary, with zero bias, constructed in a single-dimensional likelihood-ratio score-space formed by the zeroth-order derivative operator would, under equal class priors, give the standard minimum Bayes error classifier. [sent-156, score-0.353]
</p><p>56 The baseline HMMs were used as generative models for SVM kernels. [sent-157, score-0.415]
</p><p>57 02 [9] was used to train 1vI SVM classifiers on each possible class pairing. [sent-159, score-0.232]
</p><p>58 The sequence length normalisation in Equation 10, and simple scaling for score-space normalisation, were used during training and testing. [sent-160, score-0.37]
</p><p>59 Linear kernels were used in the normalised score-space, since they gave better performance than GRBFs of variable width and polynomial kernels of degree 2 (including homogeneous, inhomogeneous, and inhomogeneous with zero-mean score-space). [sent-161, score-0.304]
</p><p>60 The abbreviations rn, v, wand t refer to the score-subspaces \7 J-L jk Inpi( OIOi), \7 veC (I;jk) Inpi(OIOi), \7Wjk Inpi(OIOi) and \7 ajj Inpi(OIOi) respectively. [sent-165, score-0.232]
</p><p>61 1 refers to the log likelihood Inpi(OIOi) and r to the log likelihood-ratio In[p2(OI02) /Pl( OIOd]. [sent-166, score-0.208]
</p><p>62 The binary SVM classification results (and, as a baseline, the binary HMM results) were combined to obtain a single classification for each utterance. [sent-167, score-0.182]
</p><p>63 This was done using a simple majority voting scheme among the full set of 1v1 binary classifiers (for tied letters, the relevant 1v1 classifiers were inspected and then, if necessary, random selection performed [2]). [sent-168, score-0.615]
</p><p>64 Table 1: Error-rates for HMM baselines and SVM score-spaces (E-set) Num compo per class per state 1 2 4  6  HMM min. [sent-169, score-0.283]
</p><p>65 6  Table 1 compares the baseline HMM and SVM classifiers as the complexity of the generative models was varied. [sent-197, score-0.608]
</p><p>66 The majority voting scheme gave the same performance as the minimum Bayes error scheme, indicating that majority voting was an acceptable multi-class scheme for the E-set experiments. [sent-200, score-0.445]
</p><p>67 For the SVMs, each likelihood-ratio score-space was defined using its competing class-conditional generative models and projected into a rnr score-space. [sent-201, score-0.403]
</p><p>68 Each likelihood (I-class) score-space was defined using only the generative model for the first of its two classes, and projected into a rnl score-space. [sent-202, score-0.475]
</p><p>69 Each likelihood (2-class) score-space was defined using a generative model for both of its classes, and projected into a rnl score-space (the original Fisher score, which is a projection into its rn score-subspace, was also tested but was found to yield slightly higher error rates). [sent-203, score-0.519]
</p><p>70 In both cases, the optimum number of components in the generative models was 2 per state, possibly reflecting the gender division within each class. [sent-207, score-0.546]
</p><p>71 However, there was an excep-  tion for generative models with 1 component per class per state (in total the models had 2 components per state since they modelled both classes). [sent-209, score-1.014]
</p><p>72 The 2 components per state did not generally reflect the gender division in the 2-class data, as first supposed, but the class division. [sent-210, score-0.362]
</p><p>73 A possible explanation is that each Gaussian component modelled a class with bi-modal distribution caused by gender differences. [sent-211, score-0.241]
</p><p>74 This task was too small to fully assess possible decorrelation in error structure between HMM and SVM classifiers [6] . [sent-214, score-0.237]
</p><p>75 Without scaling for score-space normalisation, the error-rate for the likelihood-ratio score-space defined on models with 2 components per state increased from 5. [sent-215, score-0.359]
</p><p>76 Some likelihood-ratio mr score-spaces were then augmented with 2nd-order derivatives ~ J-t jk (~~jk lnp( 018)) . [sent-218, score-0.307]
</p><p>77 The disappointing performance was probably due to the simplicity of the task, the independence assumption between component posteriors and component means, and the effect of noise with so few training scores in such large score-spaces. [sent-220, score-0.232]
</p><p>78 It is known that some dimensions of feature-space are noisy and degrade classification performance. [sent-221, score-0.185]
</p><p>79 For this reason, experiments were performed which selected subsets of the likelihood-ratio score-space and then built SVM classifiers in those score-subspaces. [sent-222, score-0.242]
</p><p>80 Again, the generative models were class-conditional HMMs with 2 components per state. [sent-225, score-0.443]
</p><p>81 The log likelihood-ratio was shown to be a powerful discriminating feature 4 â&euro;˘ Increasing the number of dimensions in score-space allowed more discriminative classifiers. [sent-226, score-0.248]
</p><p>82 There was more discrimination, or less noise, in the derivatives of the component means than the component variances. [sent-227, score-0.23]
</p><p>83 As expected in a dynamic task, the derivatives of the transitions were also useful since they contained some duration information. [sent-228, score-0.154]
</p><p>84 Table 2: Error rates for subspaces of the likelihood-ratio score-space (E-set) score-space  error rate, %  r v m mv mvt wmvtr  8. [sent-229, score-0.199]
</p><p>85 1  score-space dimensionality 1 1560 1560 3120 3140 3161  Next, subsets of the mr and wmvtr score-spaces were selected according to dimensions with highest Fisher-ratios [7] . [sent-235, score-0.263]
</p><p>86 The lowest error rates for the mr and wmvtr score-spaces were respectively 3. [sent-236, score-0.262]
</p><p>87 7% confidence levels relative to the best HMM system with 4 components per state). [sent-240, score-0.18]
</p><p>88 Generally, adding the most discriminative dimensions lowered error-rate until less discriminative dimensions were added. [sent-241, score-0.364]
</p><p>89 For most binary classifiers, the most discriminative dimension was the log likelihoodratio. [sent-242, score-0.154]
</p><p>90 As expected for the E-set, the most discriminative dimensions were dependent on initial HMM states. [sent-243, score-0.182]
</p><p>91 The HMM and SVM classifiers were run on the full alphabet. [sent-249, score-0.227]
</p><p>92 The best HMM classifier, with 4 components per state, gave 3. [sent-250, score-0.164]
</p><p>93 However, generative models with 2 components per state and a wmvtr score-space pruned to 500 dimensions by Fisher-ratios, gave a lower error rate of 2. [sent-253, score-0.827]
</p><p>94 Preliminary experiments evaluating sequence length normalisation on the full alphabet and E-set are detailed in [7]. [sent-256, score-0.441]
</p><p>95 4  Conclusions  In this work, SVMs have been successfully applied to the classification of speech data. [sent-257, score-0.232]
</p><p>96 The paper has concentrated on the nature of the score-space when handling variable length speech sequences. [sent-258, score-0.22]
</p><p>97 The standard likelihood score-space of the Fisher kernel has been extended to the likelihood-ratio score-space, and normalisation schemes introduced. [sent-259, score-0.41]
</p><p>98 The new score-space avoids some of the limitations of the Fisher score-space, and incorporates the class-conditional generative models directly into the SVM classifier. [sent-260, score-0.402]
</p><p>99 The different score-spaces have been compared on a speakerindependent isolated letter task. [sent-261, score-0.154]
</p><p>100 The likelihood-ratio score-space out-performed the likelihood score-spaces and HMMs trained to maximise likelihood. [sent-262, score-0.247]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('inpi', 0.264), ('generative', 0.264), ('normalisation', 0.253), ('hmm', 0.24), ('oioi', 0.238), ('classifiers', 0.193), ('svm', 0.185), ('oi', 0.157), ('hmms', 0.154), ('ot', 0.147), ('speech', 0.141), ('fisher', 0.14), ('maximise', 0.136), ('vec', 0.132), ('jk', 0.126), ('derivatives', 0.118), ('oio', 0.115), ('svms', 0.112), ('letter', 0.108), ('ajj', 0.106), ('oiod', 0.106), ('wmvtr', 0.106), ('normalised', 0.105), ('derivative', 0.103), ('state', 0.098), ('dimensions', 0.094), ('baseline', 0.094), ('classification', 0.091), ('discriminative', 0.088), ('defined', 0.082), ('kernel', 0.081), ('posteriors', 0.08), ('length', 0.079), ('modelled', 0.077), ('score', 0.077), ('likelihood', 0.076), ('per', 0.073), ('gender', 0.069), ('log', 0.066), ('voting', 0.064), ('mr', 0.063), ('smith', 0.062), ('inp', 0.058), ('confidence', 0.058), ('models', 0.057), ('component', 0.056), ('utterance', 0.055), ('utterances', 0.055), ('grbfs', 0.053), ('inhomogeneous', 0.053), ('iplr', 0.053), ('isolet', 0.053), ('lik', 0.053), ('rnl', 0.053), ('skips', 0.053), ('kernels', 0.052), ('majority', 0.05), ('classifier', 0.05), ('built', 0.049), ('components', 0.049), ('rates', 0.049), ('recognition', 0.047), ('isolated', 0.046), ('examines', 0.046), ('mfccs', 0.046), ('wjk', 0.046), ('dept', 0.046), ('formed', 0.046), ('scheme', 0.046), ('classes', 0.045), ('operator', 0.044), ('error', 0.044), ('decision', 0.042), ('gave', 0.042), ('acceleration', 0.042), ('msec', 0.042), ('mapping', 0.042), ('avoids', 0.041), ('incorporated', 0.04), ('limitations', 0.04), ('scores', 0.04), ('class', 0.039), ('acceptable', 0.039), ('gales', 0.039), ('sequence', 0.038), ('letters', 0.038), ('alphabet', 0.037), ('scholkopf', 0.037), ('observation', 0.037), ('duration', 0.036), ('observations', 0.035), ('trained', 0.035), ('sequences', 0.035), ('spoken', 0.035), ('poorly', 0.035), ('tied', 0.035), ('posterior', 0.035), ('zero', 0.035), ('division', 0.034), ('full', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="172-tfidf-1" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>2 0.2472095 <a title="172-tfidf-2" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>3 0.24527632 <a title="172-tfidf-3" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>4 0.18833555 <a title="172-tfidf-4" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>Author: John R. Hershey, Michael Casey</p><p>Abstract: It is well known that under noisy conditions we can hear speech much more clearly when we read the speaker's lips. This suggests the utility of audio-visual information for the task of speech enhancement. We propose a method to exploit audio-visual cues to enable speech separation under non-stationary noise and with a single microphone. We revise and extend HMM-based speech enhancement techniques, in which signal and noise models are factori ally combined, to incorporate visual lip information and employ novel signal HMMs in which the dynamics of narrow-band and wide band components are factorial. We avoid the combinatorial explosion in the factorial model by using a simple approximate inference technique to quickly estimate the clean signals in a mixture. We present a preliminary evaluation of this approach using a small-vocabulary audio-visual database, showing promising improvements in machine intelligibility for speech enhanced using audio and visual information. 1</p><p>5 0.18286876 <a title="172-tfidf-5" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>6 0.14153001 <a title="172-tfidf-6" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>7 0.13106875 <a title="172-tfidf-7" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>8 0.13012515 <a title="172-tfidf-8" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>9 0.12574734 <a title="172-tfidf-9" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>10 0.12252197 <a title="172-tfidf-10" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>11 0.12038255 <a title="172-tfidf-11" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>12 0.11952552 <a title="172-tfidf-12" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>13 0.11511179 <a title="172-tfidf-13" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>14 0.11230033 <a title="172-tfidf-14" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>15 0.10954519 <a title="172-tfidf-15" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>16 0.10418443 <a title="172-tfidf-16" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>17 0.10325201 <a title="172-tfidf-17" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>18 0.09966737 <a title="172-tfidf-18" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>19 0.092655629 <a title="172-tfidf-19" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>20 0.087601684 <a title="172-tfidf-20" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.275), (1, 0.107), (2, -0.043), (3, -0.063), (4, -0.235), (5, 0.205), (6, 0.227), (7, -0.117), (8, -0.097), (9, 0.023), (10, 0.023), (11, 0.066), (12, 0.015), (13, 0.124), (14, 0.122), (15, -0.025), (16, -0.017), (17, 0.018), (18, 0.035), (19, -0.012), (20, 0.085), (21, 0.124), (22, -0.042), (23, -0.084), (24, 0.007), (25, 0.03), (26, -0.118), (27, 0.039), (28, -0.133), (29, 0.033), (30, -0.079), (31, -0.024), (32, 0.095), (33, -0.106), (34, 0.095), (35, -0.038), (36, 0.111), (37, -0.013), (38, -0.04), (39, 0.0), (40, -0.021), (41, 0.091), (42, -0.012), (43, 0.03), (44, -0.048), (45, 0.026), (46, 0.013), (47, -0.019), (48, 0.004), (49, 0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9641645 <a title="172-lsi-1" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>2 0.78370631 <a title="172-lsi-2" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>3 0.62732494 <a title="172-lsi-3" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>4 0.62705743 <a title="172-lsi-4" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>5 0.5538941 <a title="172-lsi-5" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>6 0.55073363 <a title="172-lsi-6" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>7 0.54672033 <a title="172-lsi-7" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>8 0.48845801 <a title="172-lsi-8" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>9 0.4831709 <a title="172-lsi-9" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>10 0.48312613 <a title="172-lsi-10" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>11 0.46031535 <a title="172-lsi-11" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>12 0.45978901 <a title="172-lsi-12" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>13 0.43560299 <a title="172-lsi-13" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>14 0.41820723 <a title="172-lsi-14" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>15 0.41518 <a title="172-lsi-15" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>16 0.40570316 <a title="172-lsi-16" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>17 0.39826584 <a title="172-lsi-17" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>18 0.39182979 <a title="172-lsi-18" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>19 0.38165236 <a title="172-lsi-19" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>20 0.37447691 <a title="172-lsi-20" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.031), (17, 0.032), (19, 0.033), (20, 0.285), (27, 0.146), (30, 0.076), (38, 0.015), (59, 0.039), (72, 0.075), (79, 0.087), (83, 0.014), (91, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83661985 <a title="172-lda-1" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>2 0.8193354 <a title="172-lda-2" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>Author: O. Donchin, Reza Shadmehr</p><p>Abstract: Reaching movements require the brain to generate motor commands that rely on an internal model of the task’s dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reﬂect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we ﬁnd a surprisingly good ﬁt accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force ﬁeld, and in a random sequence of force ﬁelds where learning is not possible. Remarkably, we ﬁnd that the basis remains invariant. 1</p><p>3 0.71977717 <a title="172-lda-3" href="./nips-2001-Blind_Source_Separation_via_Multinode_Sparse_Representation.html">44 nips-2001-Blind Source Separation via Multinode Sparse Representation</a></p>
<p>Author: Michael Zibulevsky, Pavel Kisilev, Yehoshua Y. Zeevi, Barak A. Pearlmutter</p><p>Abstract: We consider a problem of blind source separation from a set of instantaneous linear mixtures, where the mixing matrix is unknown. It was discovered recently, that exploiting the sparsity of sources in an appropriate representation according to some signal dictionary, dramatically improves the quality of separation. In this work we use the property of multi scale transforms, such as wavelet or wavelet packets, to decompose signals into sets of local features with various degrees of sparsity. We use this intrinsic property for selecting the best (most sparse) subsets of features for further separation. The performance of the algorithm is verified on noise-free and noisy data. Experiments with simulated signals, musical sounds and images demonstrate significant improvement of separation quality over previously reported results. 1</p><p>4 0.7080245 <a title="172-lda-4" href="./nips-2001-%28Not%29_Bounding_the_True_Error.html">1 nips-2001-(Not) Bounding the True Error</a></p>
<p>Author: John Langford, Rich Caruana</p><p>Abstract: We present a new approach to bounding the true error rate of a continuous valued classiﬁer based upon PAC-Bayes bounds. The method ﬁrst constructs a distribution over classiﬁers by determining how sensitive each parameter in the model is to noise. The true error rate of the stochastic classiﬁer found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artiﬁcial neural networks with results of a order of magnitude improvement vs. the best deterministic neural net bounds. £ ¡ ¤¢</p><p>5 0.62923855 <a title="172-lda-5" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>Author: Roland Vollgraf, Klaus Obermayer</p><p>Abstract: We present a new method for the blind separation of sources, which do not fulfill the independence assumption. In contrast to standard methods we consider groups of neighboring samples (</p><p>6 0.60341734 <a title="172-lda-6" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>7 0.60003525 <a title="172-lda-7" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>8 0.59598857 <a title="172-lda-8" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>9 0.59198481 <a title="172-lda-9" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>10 0.59074676 <a title="172-lda-10" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>11 0.58995968 <a title="172-lda-11" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>12 0.58986676 <a title="172-lda-12" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>13 0.58940321 <a title="172-lda-13" href="./nips-2001-Estimating_the_Reliability_of_ICA_Projections.html">71 nips-2001-Estimating the Reliability of ICA Projections</a></p>
<p>14 0.58554184 <a title="172-lda-14" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>15 0.58420503 <a title="172-lda-15" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>16 0.58348513 <a title="172-lda-16" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>17 0.58218002 <a title="172-lda-17" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>18 0.58201522 <a title="172-lda-18" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>19 0.58136296 <a title="172-lda-19" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>20 0.58037126 <a title="172-lda-20" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
