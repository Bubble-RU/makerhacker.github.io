<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-66" href="#">nips2001-66</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</h1>
<br/><p>Source: <a title="nips-2001-66-pdf" href="http://papers.nips.cc/paper/2100-efficiency-versus-convergence-of-boolean-kernels-for-on-line-learning-algorithms.pdf">pdf</a></p><p>Author: Roni Khardon, Dan Roth, Rocco A. Servedio</p><p>Abstract: We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow’s behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efﬁciently computable.</p><p>Reference: <a title="nips-2001-66-reference" href="../nips2001_reference/nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. [sent-7, score-0.374]
</p><p>2 We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. [sent-8, score-0.093]
</p><p>3 We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. [sent-9, score-0.346]
</p><p>4 We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. [sent-10, score-0.352]
</p><p>5 We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. [sent-11, score-0.282]
</p><p>6 1 Introduction The Perceptron and Winnow algorithms are well known learning algorithms that make predictions using a linear function in their feature space. [sent-13, score-0.04]
</p><p>7 The system ﬁrst extracts Boolean features from examples (given as text) and then runs learning algorithms over restricted conjunctions of these basic features. [sent-16, score-0.409]
</p><p>8 There are several ways to enhance the set of features after the initial extraction. [sent-17, score-0.055]
</p><p>9 One idea is using conjunctions such as and use to expand the set of basic features these expanded higher-dimensional examples, in which each conjunction plays the role of a basic feature, for learning. [sent-18, score-0.437]
</p><p>10 This approach clearly leads to an increase in expressiveness and thus may improve performance. [sent-19, score-0.056]
</p><p>11 However, it also dramatically increases the number of features (from to if all conjunctions are used) and thus may adversely affect both the computation time and convergence rate of learning. [sent-20, score-0.356]
</p><p>12    ¡ ©¦  ¢   ¨  £¥¥¥£¡ ©§¦¦¦¤¤¢   ¨    This paper studies the computational efﬁciency and convergence of the Perceptron and Winnow algorithms over such expanded feature spaces of conjunctions. [sent-21, score-0.118]
</p><p>13 Speciﬁcally, we study the use of kernel functions to expand the feature space and thus enhance the learning abilities of Perceptron and Winnow; we refer to these enhanced algorithms as kernel Perceptron and kernel Winnow. [sent-22, score-0.469]
</p><p>14 "6 2   6 8 7 ) £¡ £  2 ¥ £ G3  54¡ 4   2   IE % 2  be a sequence of labeled examples with Theorem 1 Let and for all . [sent-27, score-0.168]
</p><p>15 Let be such that Then Perceptron makes at most mistakes on this example sequence. [sent-28, score-0.117]
</p><p>16 for all  The Winnow algorithm [4] has a very similar structure. [sent-29, score-0.025]
</p><p>17 If and the label is then for all such that the value of is set the prediction is to ; this is a promotion step. [sent-31, score-0.173]
</p><p>18 For our purposes the following mistake bound, implicit in [4], is of interest:  Theorem 2 Let the target function be a -literal monotone disjunction For any sequence of examples in labeled according to of prediction mistakes made by Winnow is at most 1. [sent-33, score-0.777]
</p><p>19 Given a sequence of labeled examples the prediction and update for each example take poly time steps. [sent-35, score-0.476]
</p><p>20 in   £ §    ¨  B  £ ¨ 9  This result is closely related to one of the main open problems in learning theory: efﬁcient learnability of disjunctions of conjunctions, or DNF (Disjunctive Normal Form) expressions. [sent-36, score-0.074]
</p><p>21 is true iff ), Theorems 1 and 3 imply that kernel Perceptron can be used to learn DNF. [sent-39, score-0.193]
</p><p>22 However this result does not preclude the efﬁcient learnability of DNF using a different class of hypotheses such as those generated by the kernel Perceptron algorithm. [sent-41, score-0.15]
</p><p>23 upper bound implied by Theorem 1 is essentially tight. [sent-42, score-0.045]
</p><p>24 We give an afﬁrmative answer, thus showing that kernel Perceptron cannot efﬁciently learn DNF:  ¨   ¥¥¥£¡ ©§£ ¤¦¤¤¢   u  Theorem 4 There is a monotone DNF over and a sequence of examples labeled according to which causes the kernel Perceptron algorithm to make mistakes. [sent-43, score-0.846]
</p><p>25 ¤ ¢ ¥¨ £¡      d £ c ra d  u  Turning to Winnow, an attractive feature of Theorem 2 is that for suitable the bound is logarithmic in the total number of features (e. [sent-44, score-0.178]
</p><p>26 Therefore, as noted by several researchers [5], if a Winnow analogue of Theorem 3 could be obtained this would imply efﬁcient learnability of DNF. [sent-47, score-0.102]
</p><p>27 c    ¦    Theorem 5 There is no polynomial time algorithm which simulates Winnow over exponentially many monotone conjunctive features for learning monotone DNF, unless every problem in #P can be solved in polynomial time. [sent-49, score-1.055]
</p><p>28 We observe that, in contrast to Theorem 5, Maass and Warmuth have shown that the Winnow algorithm can be simulated efﬁciently over exponentially many conjunctive features for learning some simple geometric concept classes [5]. [sent-50, score-0.163]
</p><p>29 While several of our results are negative, in practice one can achieve good performance by using kernel Perceptron (if is small) or the limited-conjunction kernel described in Section 2 (if is large). [sent-51, score-0.214]
</p><p>30 This is similar to common practice with polynomial kernels 2 where typically a small degree is used to aid convergence. [sent-52, score-0.119]
</p><p>31     2 Theorem 3: Kernel Perceptron with Exponentially Many Features     It is easily observed, and well known, that the hypothesis of the Perceptron algorithm is a sum of the previous examples on which prediction mistakes were made. [sent-54, score-0.3]
</p><p>32 If we let denote the label of example , then where is the set of examples on which the algorithm made a mistake. [sent-55, score-0.174]
</p><p>33 a        ¨ f § ¨ ¥© § a     B  9  £ "4¡ §    §     ¨  ¨      (§ § a     ©      ¨  For an example let denote its transformation into an enhanced feature space such as the space of all conjunctions. [sent-58, score-0.128]
</p><p>34 To run the Perceptron algorithm over the enhanced space we must predict iff where is the weight vector in the . [sent-59, score-0.157]
</p><p>35 enhanced space; from the above discussion this holds iff Denoting this holds iff . [sent-60, score-0.157]
</p><p>36 Thus we never need to construct the enhanced feature space explicitly; we need only be able to compute the kernel function efﬁciently. [sent-61, score-0.202]
</p><p>37 This is the idea behind all so-called kernel methods, which can be applied to any algorithm (such as support vector machines) whose prediction is a function of inner products of examples; see e. [sent-62, score-0.208]
</p><p>38 The result in Theorem 3 is simply obtained by presenting a kernel function capturing all conjunctions. [sent-65, score-0.107]
</p><p>39 We also describe kernels for all monotone conjunctions which allow no negative literals, and kernels capturing all (monotone) conjunctions of up to literals. [sent-66, score-1.067]
</p><p>40 t  ¨   § "  The general case: When includes all conjunctions (with positive and negative literals) must compute the number of conjunctions which are true in both and . [sent-67, score-0.578]
</p><p>41 Clearly, any literal in such a conjunction must satisfy both and and thus the corresponding bit in must have the same value. [sent-68, score-0.051]
</p><p>42 Counting all such conjunctions gives where is the number of original features that have the same value in and . [sent-69, score-0.294]
</p><p>43 %     F D  1%£   § GECBA   &%£   §  '  %   5 321   640a &£   ' % § 9 1%¤ @ £ %87  ¢ %  2 Our Boolean kernels are different than standard polynomial kernels in that all the conjunctions are weighted equally. [sent-71, score-0.429]
</p><p>44 While expressive power does not change, convergence and behavior, do. [sent-72, score-0.037]
</p><p>45 Monotone Monomials: In some applications the total number of basic features may be very large but in any one example only a small number of features take value 1. [sent-73, score-0.167]
</p><p>46 In other applications the number of features may not be known in advance (e. [sent-74, score-0.055]
</p><p>47 In these cases it may be useful to consider only monotone monomials. [sent-77, score-0.386]
</p><p>48 To express all monotone monomials we take where is the number of active features common to both and . [sent-78, score-0.654]
</p><p>49   9 7 ¤ @ %8¢  % a   &%£   §  ¢ £¡  1    5 C¥  321  A  F D  &%£   § 3¥¤G%CBA  '  A parameterized kernel: In general, one may want to trade off expressivity against number of examples and convergence time. [sent-79, score-0.128]
</p><p>50 Thus we consider a parameterized kernel which captures all conjunctions of size at most for some The number of such conjunctions that satisfy both and is . [sent-80, score-0.585]
</p><p>51 For monotone conjunctions of size at most we have . [sent-82, score-0.625]
</p><p>52 For each let be chosen by independently setting each bit to with probability 1/10. [sent-86, score-0.023]
</p><p>53 For any it is clear that a Chernoff bound implies that and thus the probability that any satisﬁes is at most Similarly, for any we have a Chernoff bound implies that and thus the probability that any with satisﬁes is at most For the value of is less than 1. [sent-87, score-0.14]
</p><p>54 Thus we have each and For any for some choice of which has we can set of the 1s to 0s, and the lemma is proved. [sent-88, score-0.075]
</p><p>55 The target DNF is very simple: it is the single conjunction While the original makes at most poly mistakes for Perceptron algorithm over the features this target function, we now show that the monotone kernel Perceptron algorithm which runs over all monotone monomials can make mistakes. [sent-89, score-1.506]
</p><p>56    1 ) ' 0 00(¨  ¨    #  % b  ¨        Recall that at the beginning of the Perceptron algorithm’s execution all coordinates of are 0. [sent-90, score-0.059]
</p><p>57 The ﬁrst example is the negative example since Perceptron incorrectly predicts 1 on this example. [sent-91, score-0.191]
</p><p>58 The resulting update causes the coefﬁcient corresponding to the empty monomial (satisﬁed by any example ) to become but all other coordinates of remain 0. [sent-92, score-0.153]
</p><p>59 The next example is the positive example For this example we have so Perceptron incorrectly predicts Since all monotone conjunctions are satisﬁed by this example the resulting update causes to become 0 and all other coordinates of to become 1. [sent-93, score-0.923]
</p><p>60 The next examples are the vectors described in Lemma 6. [sent-94, score-0.091]
</p><p>61 Since each such example has each example is negative; however as we now show the Perceptron algorithm will predict on each of these examples. [sent-95, score-0.091]
</p><p>62 a  00 22   § "  &   £¡ 4© # #     which correspond to the monomials satisﬁed by More precisely we where contains the monomials which are and for some and contains the monomials which are satisﬁed with We lower bound the two sums separately. [sent-101, score-0.728]
</p><p>63 for all implies If is monotone consistent and has labeled examples then clearly there is a monotone DNF formula consistent with which contains at most conjunctions. [sent-105, score-1.01]
</p><p>64 We consider the following problem: KERNEL WINNOW PREDICTION (KWP) of labeled examples Instance: Monotone consistent sequence with each and each unlabeled example Question: Is where is the -dimensional hypothesis vector generated by running Winnow on the example sequence ? [sent-106, score-0.319]
</p><p>65 nonempty monomials to learn monotone DNF, one In order to run Winnow over all must be able to solve KWP efﬁciently. [sent-107, score-0.656]
</p><p>66 The main result of this section is proved by showing that KWP is computationally hard for any parameter settings which yield a polynomial mistake bound for Winnow via Theorem 2. [sent-108, score-0.156]
</p><p>67 )  7   7 B¤¦¥ ¨ A 7 @¡ 8 9 ¥ ¥ 9 9 7  ¥    '  satisfying assignments in    '  ¦  7  6 a  '£ 6  §  6  £ '     §  6 ¥ £ e B ¨ &%¦£¦¥¤¦¥¤¡ ¡ % 9 ¡ DC2 %  '  High-Level Idea of the Proof  with  B C 9 Q ¨ £w ¨  2 % RVv2 % § a 2 7     4. [sent-111, score-0.038]
</p><p>68 1  ¥ ¤ &¢  MONOTONE 2-SAT (M2SAT) Instance: Monotone 2-CNF Boolean formula and each integer such that Question: Is i. [sent-112, score-0.061]
</p><p>69 all variables in and are set to 1 and all variables in are set to 0. [sent-129, score-0.11]
</p><p>70 We thus have where as type-  and monomials, monomials  monomials  as type-  We refer to monomials as type- monomials, and monomials. [sent-130, score-0.664]
</p><p>71 The example sequence is divided into four stages. [sent-131, score-0.069]
</p><p>72 For each such negative example in Stage 1 six new slack variables are used as follows: Stage 1 has repeated instances of the positive example which has and all other bits 0. [sent-154, score-0.318]
</p><p>73 These examples cause promotions which result in and hence Two other groups of similar examples (the ﬁrst with the second with ) cause and The next example in is the negative example which has for all other in and all other bits 0. [sent-155, score-0.524]
</p><p>74 For this example so Winnow makes a false positive prediction. [sent-156, score-0.126]
</p><p>75 Since has at most clauses and there are negative examples per clause, this construction can be carried out using slack variables Stage 2: Setting with for all  ,  . [sent-157, score-0.322]
</p><p>76 The ﬁrst Stage 2 example is a positive example and all other bits 0. [sent-158, score-0.134]
</p><p>77 ¦ c 7 ¦  ¦ c ¦   # ¦ ¦ © 4 c ¨ ¤ ¥ ¦  ¦    ¦ c  § 4 ra ©¨   4 c § ' ¡ 7 ! [sent-173, score-0.038]
</p><p>78 ¨ H  32§ ¡ ¨ 1 ¨   and are satisﬁed by this example have Since poly  £  monomials which contain we have the resulting promotion we have  after  Let so Stage 2 consists of repeated instances of the positive example described above. [sent-189, score-0.589]
</p><p>79 After these promotions we have Since we also have Stage 3: Setting . [sent-190, score-0.072]
</p><p>80 For the base case we have Assuming the lemma is true for we now prove it for then the desired CNF is has at most clauses. [sent-193, score-0.075]
</p><p>81 If  Since has at most then the desired CNF over each clause of we can write then  ¨  If clauses is  and  9  6  9  6    Let be an -clause monotone CNF formula over the variables in which has satisfying assignments. [sent-195, score-0.594]
</p><p>82 Similar to Stage 1, for each clause of , Stage 3 has negative examples corresponding to that clause, and as in Stage 1 slack variables in are used to ensure that Winnow makes a false positive prediction on each such negative example. [sent-196, score-0.577]
</p><p>83 Thus the examples in Stage 3 cause where Since six slack variables in are used for each negative example and there are negative examples, the slack variables are sufﬁcient for Stage 3. [sent-197, score-0.549]
</p><p>84 ¨   ¥     7 ©  c    ¦  X ¦ ¨ ¨  X # a ¥ ¦  in  ¥ ¡  Stage 4: Setting promotions on examples which have each  . [sent-204, score-0.163]
</p><p>85 #    § §   c a  ¢§  a ¥ & #   2  © ¥    ¨ ¤¥¢©§ ¥ ¦   ¨ ¤¥¢©§ ¥ ¦   ¨ ¤¥¢©§ ¥£ ¦   ¨ ¤¥¢©§ ¥ ¦  easy manipulations show that  so indeed  Finally, we observe that by construction the example sequence is monotone consistent. [sent-211, score-0.455]
</p><p>86 Since poly and contains poly examples the transformation from M2SAT to KWP is polynomial-time computable and the theorem is proved. [sent-212, score-0.604]
</p><p>87 (Theorem 7)  5 Conclusion  It is necessary to expand the feature space if linear learning algorithms are to learn expressive functions. [sent-213, score-0.068]
</p><p>88 This work explores the tradeoff between computational efﬁciency and convergence (i. [sent-214, score-0.059]
</p><p>89 Future directions include the utilization of the kernels developed here and studying convergence issues of Boolean-kernel Perceptron and Support Vector Machines in the PAC model. [sent-218, score-0.108]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('winnow', 0.518), ('monotone', 0.386), ('perceptron', 0.316), ('conjunctions', 0.239), ('dnf', 0.22), ('monomials', 0.213), ('poly', 0.199), ('stage', 0.187), ('cnf', 0.108), ('kwp', 0.108), ('kernel', 0.107), ('theorem', 0.093), ('boolean', 0.093), ('examples', 0.091), ('mistakes', 0.084), ('ef', 0.08), ('prediction', 0.076), ('lemma', 0.075), ('satis', 0.073), ('promotions', 0.072), ('clause', 0.072), ('promotion', 0.072), ('kernels', 0.071), ('slack', 0.068), ('mistake', 0.063), ('negative', 0.061), ('cause', 0.057), ('monomial', 0.057), ('variables', 0.055), ('enhanced', 0.055), ('features', 0.055), ('false', 0.054), ('iff', 0.051), ('proof', 0.049), ('polynomial', 0.048), ('clauses', 0.047), ('bound', 0.045), ('veri', 0.044), ('exponentially', 0.043), ('learnability', 0.043), ('labeled', 0.041), ('expanded', 0.041), ('feature', 0.04), ('conjunctive', 0.04), ('positive', 0.039), ('assignments', 0.038), ('ra', 0.038), ('ed', 0.037), ('convergence', 0.037), ('incorrectly', 0.036), ('khardon', 0.036), ('literals', 0.036), ('rocco', 0.036), ('roni', 0.036), ('servedio', 0.036), ('snow', 0.036), ('sequence', 0.036), ('xp', 0.036), ('coordinates', 0.035), ('imply', 0.035), ('formula', 0.034), ('ciency', 0.033), ('ciently', 0.033), ('example', 0.033), ('maass', 0.031), ('nonempty', 0.031), ('disjunctions', 0.031), ('expressiveness', 0.031), ('roth', 0.031), ('maintains', 0.03), ('bits', 0.029), ('fea', 0.029), ('truth', 0.029), ('expand', 0.028), ('predicts', 0.028), ('causes', 0.028), ('integer', 0.027), ('cba', 0.027), ('run', 0.026), ('stages', 0.026), ('conjunction', 0.026), ('label', 0.025), ('consistent', 0.025), ('algorithm', 0.025), ('exponential', 0.025), ('thus', 0.025), ('hypothesis', 0.024), ('basic', 0.024), ('threshold', 0.024), ('nsf', 0.024), ('chernoff', 0.024), ('simulates', 0.024), ('execution', 0.024), ('analogue', 0.024), ('setting', 0.023), ('receiving', 0.023), ('instance', 0.023), ('contains', 0.022), ('yx', 0.022), ('tradeoff', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="66-tfidf-1" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>Author: Roni Khardon, Dan Roth, Rocco A. Servedio</p><p>Abstract: We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow’s behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efﬁciently computable.</p><p>2 0.2121136 <a title="66-tfidf-2" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>Author: Adam Kowalczyk, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We give results about the learnability and required complexity of logical formulae to solve classiﬁcation problems. These results are obtained by linking propositional logic with kernel machines. In particular we show that decision trees and disjunctive normal forms (DNF) can be represented by the help of a special kernel, linking regularized risk to separation margin. Subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators, such as perceptron and maximal perceptron learning.</p><p>3 0.13606443 <a title="66-tfidf-3" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>Author: Michael Collins, Nigel Duffy</p><p>Abstract: We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.</p><p>4 0.12523869 <a title="66-tfidf-4" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>5 0.097430281 <a title="66-tfidf-5" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>6 0.096188717 <a title="66-tfidf-6" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>7 0.07754153 <a title="66-tfidf-7" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>8 0.073258504 <a title="66-tfidf-8" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>9 0.068065718 <a title="66-tfidf-9" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>10 0.066375196 <a title="66-tfidf-10" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>11 0.065365694 <a title="66-tfidf-11" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>12 0.065182582 <a title="66-tfidf-12" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>13 0.063349918 <a title="66-tfidf-13" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>14 0.061222378 <a title="66-tfidf-14" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>15 0.059972141 <a title="66-tfidf-15" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>16 0.05647669 <a title="66-tfidf-16" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>17 0.055275828 <a title="66-tfidf-17" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>18 0.05438906 <a title="66-tfidf-18" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>19 0.052635241 <a title="66-tfidf-19" href="./nips-2001-Pranking_with_Ranking.html">147 nips-2001-Pranking with Ranking</a></p>
<p>20 0.052314103 <a title="66-tfidf-20" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, 0.099), (2, 0.007), (3, 0.069), (4, 0.076), (5, -0.006), (6, 0.002), (7, 0.07), (8, -0.114), (9, 0.051), (10, -0.165), (11, -0.004), (12, 0.023), (13, 0.012), (14, -0.058), (15, 0.023), (16, 0.027), (17, 0.137), (18, -0.033), (19, -0.027), (20, 0.027), (21, -0.016), (22, 0.034), (23, 0.031), (24, 0.052), (25, 0.011), (26, 0.092), (27, 0.022), (28, -0.027), (29, -0.128), (30, -0.101), (31, 0.1), (32, 0.096), (33, 0.059), (34, -0.182), (35, 0.091), (36, -0.108), (37, 0.023), (38, 0.12), (39, -0.135), (40, 0.157), (41, 0.119), (42, -0.066), (43, -0.053), (44, -0.078), (45, 0.021), (46, 0.158), (47, -0.019), (48, -0.049), (49, -0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93779933 <a title="66-lsi-1" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>Author: Roni Khardon, Dan Roth, Rocco A. Servedio</p><p>Abstract: We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow’s behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efﬁciently computable.</p><p>2 0.70446509 <a title="66-lsi-2" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>Author: Adam Kowalczyk, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We give results about the learnability and required complexity of logical formulae to solve classiﬁcation problems. These results are obtained by linking propositional logic with kernel machines. In particular we show that decision trees and disjunctive normal forms (DNF) can be represented by the help of a special kernel, linking regularized risk to separation margin. Subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators, such as perceptron and maximal perceptron learning.</p><p>3 0.53331673 <a title="66-lsi-3" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>Author: Michael Collins, Nigel Duffy</p><p>Abstract: We describe the application of kernel methods to Natural Language Processing (NLP) problems. In many NLP tasks the objects being modeled are strings, trees, graphs or other discrete structures which require some mechanism to convert them into feature vectors. We describe kernels for various natural language structures, allowing rich, high dimensional representations of these structures. We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm, and we give experimental results on the ATIS corpus of parse trees.</p><p>4 0.53155708 <a title="66-lsi-4" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><p>5 0.43579215 <a title="66-lsi-5" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>6 0.4040677 <a title="66-lsi-6" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>7 0.37983865 <a title="66-lsi-7" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>8 0.37500513 <a title="66-lsi-8" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>9 0.33478794 <a title="66-lsi-9" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>10 0.3128953 <a title="66-lsi-10" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>11 0.29514036 <a title="66-lsi-11" href="./nips-2001-EM-DD%3A_An_Improved_Multiple-Instance_Learning_Technique.html">64 nips-2001-EM-DD: An Improved Multiple-Instance Learning Technique</a></p>
<p>12 0.29401404 <a title="66-lsi-12" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>13 0.28870672 <a title="66-lsi-13" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>14 0.25762758 <a title="66-lsi-14" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>15 0.25299639 <a title="66-lsi-15" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>16 0.25262752 <a title="66-lsi-16" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>17 0.23939826 <a title="66-lsi-17" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>18 0.23601075 <a title="66-lsi-18" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>19 0.23123452 <a title="66-lsi-19" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>20 0.23071997 <a title="66-lsi-20" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.05), (17, 0.029), (19, 0.031), (27, 0.104), (30, 0.052), (38, 0.021), (59, 0.021), (72, 0.052), (79, 0.029), (83, 0.037), (84, 0.012), (91, 0.451)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99380279 <a title="66-lda-1" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>Author: Michael L. Littman, Richard S. Sutton</p><p>Abstract: We show that states of a dynamical system can be usefully represented by multi-step, action-conditional predictions of future observations. State representations that are grounded in data in this way may be easier to learn, generalize better, and be less dependent on accurate prior models than, for example, POMDP state representations. Building on prior work by Jaeger and by Rivest and Schapire, in this paper we compare and contrast a linear specialization of the predictive approach with the state representations used in POMDPs and in k-order Markov models. Ours is the first specific formulation of the predictive idea that includes both stochasticity and actions (controls). We show that any system has a linear predictive state representation with number of predictions no greater than the number of states in its minimal POMDP model. In predicting or controlling a sequence of observations, the concepts of state and state estimation inevitably arise. There have been two dominant approaches. The generative-model approach, typified by research on partially observable Markov decision processes (POMDPs), hypothesizes a structure for generating observations and estimates its state and state dynamics. The history-based approach, typified by k-order Markov methods, uses simple functions of past observations as state, that is, as the immediate basis for prediction and control. (The data flow in these two approaches are diagrammed in Figure 1.) Of the two, the generative-model approach is more general. The model's internal state gives it temporally unlimited memorythe ability to remember an event that happened arbitrarily long ago--whereas a history-based approach can only remember as far back as its history extends. The bane of generative-model approaches is that they are often strongly dependent on a good model of the system's dynamics. Most uses of POMDPs, for example, assume a perfect dynamics model and attempt only to estimate state. There are algorithms for simultaneously estimating state and dynamics (e.g., Chrisman, 1992), analogous to the Baum-Welch algorithm for the uncontrolled case (Baum et al., 1970), but these are only effective at tuning parameters that are already approximately correct (e.g., Shatkay & Kaelbling, 1997). observations (and actions) 1-----1-----1..- (a) state rep'n observations (and actions) ¢E / t/' --+ 1-step delays . state rep'n (b) Figure 1: Data flow in a) POMDP and other recursive updating of state representation, and b) history-based state representation. In practice, history-based approaches are often much more effective. Here, the state representation is a relatively simple record of the stream of past actions and observations. It might record the occurrence of a specific subsequence or that one event has occurred more recently than another. Such representations are far more closely linked to the data than are POMDP representations. One way of saying this is that POMDP learning algorithms encounter many local minima and saddle points because all their states are equipotential. History-based systems immediately break symmetry, and their direct learning procedure makes them comparably simple. McCallum (1995) has shown in a number of examples that sophisticated history-based methods can be effective in large problems, and are often more practical than POMDP methods even in small ones. The predictive state representation (PSR) approach, which we develop in this paper, is like the generative-model approach in that it updates the state representation recursively, as in Figure l(a), rather than directly computing it from data. We show that this enables it to attain generality and compactness at least equal to that of the generative-model approach. However, the PSR approach is also like the history-based approach in that its representations are grounded in data. Whereas a history-based representation looks to the past and records what did happen, a PSR looks to the future and represents what will happen. In particular, a PSR is a vector of predictions for a specially selected set of action-observation sequences, called tests (after Rivest & Schapire, 1994). For example, consider the test U101U202, where U1 and U2 are specific actions and 01 and 02 are specific observations. The correct prediction for this test given the data stream up to time k is the probability of its observations occurring (in order) given that its actions are taken (in order) (i.e., Pr {Ok = 01, Ok+1 = 02 I A k = u1,A k + 1 = U2}). Each test is a kind of experiment that could be performed to tell us something about the system. If we knew the outcome of all possible tests, then we would know everything there is to know about the system. A PSR is a set of tests that is sufficient information to determine the prediction for all possible tests (a sufficient statistic). As an example of these points, consider the float/reset problem (Figure 2) consisting of a linear string of 5 states with a distinguished reset state on the far right. One action, f (float), causes the system to move uniformly at random to the right or left by one state, bounded at the two ends. The other action, r (reset), causes a jump to the reset state irrespective of the current state. The observation is always o unless the r action is taken when the system is already in the reset state, in which case the observation is 1. Thus, on an f action, the correct prediction is always 0, whereas on an r action, the correct prediction depends on how many fs there have been since the last r: for zero fS, it is 1; for one or two fS, it is 0.5; for three or four fS, it is 0.375; for five or six fs, it is 0.3125, and so on decreasing after every second f, asymptotically bottoming out at 0.2. No k-order Markov method can model this system exactly, because no limited-. .5 .5 a) float action 1,0=1 b) reset action Figure 2: Underlying dynamics of the float/reset problem for a) the float action and b) the reset action. The numbers on the arcs indicate transition probabilities. The observation is always 0 except on the reset action from the rightmost state, which produces an observation of 1. length history is a sufficient statistic. A POMDP approach can model it exactly by maintaining a belief-state representation over five or so states. A PSR, on the other hand, can exactly model the float/reset system using just two tests: rl and fOrI. Starting from the rightmost state, the correct predictions for these two tests are always two successive probabilities in the sequence given above (1, 0.5, 0.5, 0.375,...), which is always a sufficient statistic to predict the next pair in the sequence. Although this informational analysis indicates a solution is possible in principle, it would require a nonlinear updating process for the PSR. In this paper we restrict consideration to a linear special case of PSRs, for which we can guarantee that the number of tests needed does not exceed the number of states in the minimal POMDP representation (although we have not ruled out the possibility it can be considerably smaller). Of greater ultimate interest are the prospects for learning PSRs and their update functions, about which we can only speculate at this time. The difficulty of learning POMDP structures without good prior models are well known. To the extent that this difficulty is due to the indirect link between the POMDP states and the data, predictive representations may be able to do better. Jaeger (2000) introduced the idea of predictive representations as an alternative to belief states in hidden Markov models and provided a learning procedure for these models. We build on his work by treating the control case (with actions), which he did not significantly analyze. We have also been strongly influenced by the work of Rivest and Schapire (1994), who did consider tests including actions, but treated only the deterministic case, which is significantly different. They also explored construction and learning algorithms for discovering system structure. 1 Predictive State Representations We consider dynamical systems that accept actions from a discrete set A and generate observations from a discrete set O. We consider only predicting the system, not controlling it, so we do not designate an explicit reward observation. We refer to such a system as an environment. We use the term history to denote a test forming an initial stream of experience and characterize an environment by a probability distribution over all possible histories, P : {OIA}* H- [0,1], where P(Ol··· Otl a1··· at) is the probability of observations 01, ... , O£ being generated, in that order, given that actions aI, ... ,at are taken, in that order. The probability of a test t conditional on a history h is defined as P(tlh) = P(ht)/P(h). Given a set of q tests Q = {til, we define their (1 x q) prediction vector, p(h) = [P(t1Ih),P(t2Ih), ... ,P(tqlh)], as a predictive state representation (PSR) if and only if it forms a sufficient statistic for the environment, Le., if and only if P(tlh) = ft(P(h)), (1) for any test t and history h, and for some projection junction ft : [0, l]q ~ [0,1]. In this paper we focus on linear PSRs, for which the projection functions are linear, that is, for which there exist a (1 x q) projection vector mt, for every test t, such that (2) P(tlh) == ft(P(h)) =7 p(h)mf, for all histories h. Let Pi(h) denote the ith component of the prediction vector for some PSR. This can be updated recursively, given a new action-observation pair a,o, by .(h ) == P(t.lh ) == P(otil ha ) == faati(P(h)) == p(h)m'{;ati P2 ao 2 ao P(olha) faa (P(h)) p(h)mro ' (3) where the last step is specific to linear PSRs. We can now state our main result: Theorem 1 For any environment that can be represented by a finite POMDP model, there exists a linear PSR with number of tests no larger than the number of states in the minimal POMDP model. 2 Proof of Theorem 1: Constructing a PSR from a POMDP We prove Theorem 1 by showing that for any POMDP model of the environment, we can construct in polynomial time a linear PSR for that POMDP of lesser or equal complexity that produces the same probability distribution over histories as the POMDP model. We proceed in three steps. First, we review POMDP models and how they assign probabilities to tests. Next, we define an algorithm that takes an n-state POMDP model and produces a set of n or fewer tests, each of length less than or equal to n. Finally, we show that the set of tests constitute a PSR for the POMDP, that is, that there are projection vectors that, together with the tests' predictions, produce the same probability distribution over histories as the POMDP. A POMDP (Lovejoy, 1991; Kaelbling et al., 1998) is defined by a sextuple (8, A, 0, bo, T, 0). Here, 8 is a set of n underlying (hidden) states, A is a discrete set of actions, and 0 is a discrete set of observations. The (1 x n) vector bo is an initial state distribution. The set T consists of (n x n) transition matrices Ta, one for each action a, where Tlj is the probability of a transition from state i to j when action a is chosen. The set 0 consists of diagonal (n x n) observation matrices oa,o, one for each pair of observation 0 and action a, where o~'o is the probability of observation 0 when action a is selected and state i is reached. l The state representation in a POMDP (Figure l(a)) is the belief state-the (1 x n) vector of the state-occupation probabilities given the history h. It can be computed recursively given a new action a and observation 0 by b(h)Taoa,o b(hao) = b(h)Taoa,oe;' where en is the (1 x n)-vector of all Is. Finally, a POMDP defines a probability distribution over tests (and thus histories) by P(Ol ... otlhal ... at) == b(h)Ta1oal,Ol ... Taloa£,Ole~. (4) IThere are many equivalent formulations and the conversion procedure described here can be easily modified to accommodate other POMDP definitions. We now present our algorithm for constructing a PSR for a given POMDP. It uses a function u mapping tests to (1 x n) vectors defined recursively by u(c) == en and u(aot) == (Taoa,ou(t)T)T, where c represents the null test. Conceptually, the components of u(t) are the probabilities of the test t when applied from each underlying state of the POMDP; we call u(t) the outcome vector for test t. We say a test t is linearly independent of a set of tests S if its outcome vector is linearly independent of the set of outcome vectors of the tests in S. Our algorithm search is used and defined as Q -<- search(c, {}) search(t, S): for each a E A, 0 E 0 if aot is linearly independent of S then S -<- search(aot, S U {aot}) return S The algorithm maintains a set of tests and searches for new tests that are linearly independent of those already found. It is a form of depth-first search. The algorithm halts when it checks all the one-step extensions of its tests and finds none that are linearly independent. Because the set of tests Q returned by search have linearly independent outcome vectors, the cardinality of Q is bounded by n, ensuring that the algorithm halts after a polynomial number of iterations. Because each test in Q is formed by a one-step extension to some other test in Q, no test is longer than n action-observation pairs. The check for linear independence can be performed in many ways, including Gaussian elimination, implying that search terminates in polynomial time. By construction, all one-step extensions to the set of tests Q returned by search are linearly dependent on those in Q. We now show that this is true for any test. Lemma 1 The outcome vectors of the tests in Q can be linearly combined to produce the outcome vector for any test. Proof: Let U be the (n x q) matrix formed by concatenating the outcome vectors for all tests in Q. Since, for all combinations of a and 0, the columns of Taoa,ou are linearly dependent on the columns of U, we can write Taoa,ou == UW T for some q x q matrix of weights W. If t is a test that is linearly dependent on Q, then anyone-step extension of t, aot, is linearly dependent on Q. This is because we can write the outcome vector for t as u(t) == (UwT)T for some (1 x q) weight vector w and the outcome vector for aot as u(aot) == (Taoa,ou(t)T)T == (Taoa,oUwT)T == (UWTwT)T. Thus, aot is linearly dependent on Q. Now, note that all one-step tests are linearly dependent on Q by the structure of the search algorithm. Using the previous paragraph as an inductive argument, this implies that all tests are linearly dependent on Q. 0 Returning to the float/reset example POMDP, search begins with by enumerating the 4 extensions to the null test (fO, fl, rO, and rl). Of these, only fa and rO are are linearly independent. Of the extensions of these, fOrO is the only one that is linearly independent of the other two. The remaining two tests added to Q by search are fOfOrO and fOfOfOrO. No extensions of the 5 tests in Q are linearly independent of the 5 tests in Q, so the procedure halts. We now show that the set of tests Q constitute a PSR for the POMDP by constructing projection vectors that, together with the tests' predictions, produce the same probability distribution over histories as the POMDP. For each combination of a and 0, define a q x q matrix Mao == (U+Taoa,ou)T and a 1 x q vector mao == (U+Taoa,oe;;J T , where U is the matrix of outcome vectors defined in the previous section and U+ is its pseudoinverse2 • The ith row of Mao is maoti. The probability distribution on histories implied by these projection vectors is p(h )m~101 alOl p(h)M~ol M~_10l_1 m~Ol b(h)UU+r a1 oa 1,01 U ... U+T al-10 al-1,Ol-1 UU+Taloal,ol b(h)T a1 0 a1,01 ... ral-l0al-t,ol-lTaloal,Ole~, Le., it is the same as that of the POMDP, as in Equation 4. Here, the last step uses the fact that UU+v T == v T for v T linearly dependent on the columns of U. This holds by construction of U in the previous section. This completes the proof of Theorem 1. Completing the float/reset example, consider the Mf,o matrix found by the process defined in this section. It derives predictions for each test in Q after taking action f. Most of these are quite simple because the tests are so similar: the new prediction for rO is exactly the old prediction for fOrO, for example. The only non trivial test is fOfOfOrO. Its outcome can be computed from 0.250 p(rOlh) - 0.0625 p(fOrOlh) + 0.750 p(fOfOrOlh). This example illustrates that the projection vectors need not contain only positive entries. 3 Conclusion We have introduced a predictive state representation for dynamical systems that is grounded in actions and observations and shown that, even in its linear form, it is at least as general and compact as POMDPs. In essence, we have established PSRs as a non-inferior alternative to POMDPs, and suggested that they might have important advantages, while leaving demonstration of those advantages to future work. We conclude by summarizing the potential advantages (to be explored in future work): Learnability. The k-order Markov model is similar to PSRs in that it is entirely based on actions and observations. Such models can be learned trivially from data by counting-it is an open question whether something similar can be done with a PSR. Jaeger (2000) showed how to learn such a model in the uncontrolled setting, but the situation is more complex in the multiple action case since outcomes are conditioned on behavior, violating some required independence assumptions. Compactness. We have shown that there exist linear PSRs no more complex that the minimal POMDP for an environment, but in some cases the minimal linear PSR seems to be much smaller. For example, a POMDP extension of factored MDPs explored by Singh and Cohn (1998) would be cross-products of separate POMDPs and have linear PSRs that increase linearly with the number and size of the component POMDPs, whereas their minimal POMDP representation would grow as the size 2If U = A~BT is the singular value decomposition of U, then B:E+ AT is the pseudoinverse. The pseudoinverse of the diagonal matrix }J replaces each non-zero element with its reciprocal. e; of the state space, Le., exponential in the number of component POMDPs. This (apparent) advantage stems from the PSR's combinatorial or factored structure. As a vector of state variables, capable of taking on diverse values, a PSR may be inherently more powerful than the distribution over discrete states (the belief state) of a POMDP. We have already seen that general PSRs can be more compact than POMDPs; they are also capable of efficiently capturing environments in the diversity representation used by Rivest and Schapire (1994), which is known to provide an extremely compact representation for some environments. Generalization. There are reasons to think that state variables that are themselves predictions may be particularly useful in learning to make other predictions. With so many things to predict, we have in effect a set or sequence of learning problems, all due to the same environment. In many such cases the solutions to earlier problems have been shown to provide features that generalize particularly well to subsequent problems (e.g., Baxter, 2000; Thrun & Pratt, 1998). Powerful, extensible representations. PSRs that predict tests could be generalized to predict the outcomes of multi-step options (e.g., Sutton et al., 1999). In this case, particularly, they would constitute a powerful language for representing the state of complex environments. AcknowledgIllents: We thank Peter Dayan, Lawrence Saul, Fernando Pereira and Rob Schapire for many helpful discussions of these and related ideas. References Baum, L. E., Petrie, T., Soules, G., & Weiss, N. (1970). A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. Annals of Mathematical Statistics, 41, 164-171. Baxter, J. (2000). A model of inductive bias learning. Journal of Artificial Intelligence Research, 12, 149-198. Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. Proceedings of the Tenth National Conference on Artificial Intelligence (pp. 183-188). San Jose, California: AAAI Press. Jaeger, H. (2000). Observable operator models for discrete stochastic time series. Neural Computation, 12, 1371-1398. Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in ' partially observable stochastic domains. Artificial Intelligence, 101, 99-134. Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observable Markov decision processes. Annals of Operations Research, 28, 47-65. McCallum, A. K. (1995). Reinforcement learning with selective perception and hidden state. Doctoral diss.ertation, Department of Computer Science, University of Rochester. Rivest, R. L., & Schapire, R. E. (1994). Diversity-based inference of finite automata. Journal of the ACM, 41, 555-589. Shatkay, H., & Kaelbling, L. P. (1997). Learning topological maps with weak local odometric information~ Proceedings of Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-91) (pp. 920-929). Singh, S., & Cohn, D. (1998). How to dynamically merge Markov decision processes. Advances in Neural and Information Processing Systems 10 (pp. 1057-1063). Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 181-211. Thrun, S., & Pratt, L. (Eds.). (1998). Learning to learn. Kluwer Academic Publishers.</p><p>2 0.99110055 <a title="66-lda-2" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>Author: David M. Blei, Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1</p><p>3 0.99104726 <a title="66-lda-3" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>4 0.98968965 <a title="66-lda-4" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>Author: Michael C. Mozer, Michael D. Colagrosso, David E. Huber</p><p>Abstract: We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conﬂict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality—that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission—we obtain a good ﬁt to overall RT and error data, as well as trial-by-trial variations in performance. Consider the following scenario: While driving, you approach an intersection at which the trafﬁc light has already turned yellow, signaling that it is about to turn red. You also notice that a car is approaching you rapidly from behind, with no indication of slowing. Should you stop or speed through the intersection? The decision is difﬁcult due to the presence of two conﬂicting signals. Such response conﬂict can be produced in a psychological laboratory as well. For example, Stroop (1935) asked individuals to name the color of ink on which a word is printed. When the words are color names incongruous with the ink color— e.g., “blue” printed in red—reaction times are slower and error rates are higher. We are interested in the control mechanisms underlying performance of high-conﬂict tasks. Conﬂict requires individuals to monitor and adjust their behavior, possibly responding more slowly if errors are too frequent. In this paper, we model a speeded discrimination paradigm in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). The stimuli are letters of the alphabet, A–Z, presented in rapid succession. In a choice task, individuals are asked to press one response key if the letter is an X or another response key for any letter other than X (as a shorthand, we will refer to non-X stimuli as Y). In a go/no-go task, individuals are asked to press a response key when X is presented and to make no response otherwise. We address both tasks because they elicit slightly different decision-making behavior. In both tasks, Jones and Braver (2001) manipulated the relative frequency of the X and Y stimuli; the ratio of presentation frequency was either 17:83, 50:50, or 83:17. Response conﬂict arises when the two stimulus classes are unbalanced in frequency, resulting in more errors and slower reaction times. For example, when X’s are frequent but Y is presented, individuals are predisposed toward producing the X response, and this predisposition must be overcome by the perceptual evidence from the Y. Jones and Braver (2001) also performed an fMRI study of this task and found that anterior cingulate cortex (ACC) becomes activated in situations involving response conﬂict. Specifically, when one stimulus occurs infrequently relative to the other, event-related fMRI response in the ACC is greater for the low frequency stimulus. Jones and Braver also extended a neural network model of Botvinick, Braver, Barch, Carter, and Cohen (2001) to account for human performance in the two discrimination tasks. The heart of the model is a mechanism that monitors conﬂict—the posited role of the ACC—and adjusts response biases accordingly. In this paper, we develop a parsimonious alternative account of the role of the ACC and of how control processes modulate behavior when response conﬂict arises. 1 A RATIONAL ANALYSIS Our account is based on a rational analysis of human cognition, which views cognitive processes as being optimized with respect to certain task-related goals, and being adaptive to the structure of the environment (Anderson, 1990). We make three assumptions of rationality: (1) perceptual inference is optimal but is subject to rate limitations on information transmission, (2) response class prior probabilities are accurately estimated, and (3) the goal of individuals is to minimize a cost that depends both on error rate and reaction time. The heart of our account is an existing probabilistic model that explains a variety of facilitation effects that arise from long-term repetition priming (Colagrosso, in preparation; Mozer, Colagrosso, & Huber, 2000), and more broadly, that addresses changes in the nature of information transmission in neocortex due to experience. We give a brief overview of this model; the details are not essential for the present work. The model posits that neocortex can be characterized by a collection of informationprocessing pathways, and any act of cognition involves coordination among pathways. To model a simple discrimination task, we might suppose a perceptual pathway to map the visual input to a semantic representation, and a response pathway to map the semantic representation to a response. The choice and go/no-go tasks described earlier share a perceptual pathway, but require different response pathways. The model is framed in terms of probability theory: pathway inputs and outputs are random variables and microinference in a pathway is carried out by Bayesian belief revision.   To elaborate, consider a pathway whose input at time is a discrete random variable, denoted , which can assume values corresponding to alternative input states. Similarly, the output of the pathway at time is a discrete random variable, denoted , which can assume values . For example, the input to the perceptual pathway in the discrimination task is one of visual patterns corresponding to the letters of the alphabet, and the output is one of letter identities. (This model is highly abstract: the visual patterns are enumerated, but the actual pixel patterns are not explicitly represented in the model. Nonetheless, the similarity structure among inputs can be captured, but we skip a discussion of this issue because it is irrelevant for the current work.) To present a particular input alternative, , to the model for time steps, we clamp for . The model computes a probability distribution over given , i.e., P . ¡ # 4 0 ©2' &  0 ' ! 1)(</p><p>5 0.98960787 <a title="66-lda-5" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>6 0.98781341 <a title="66-lda-6" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>7 0.97758162 <a title="66-lda-7" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>same-paper 8 0.97213268 <a title="66-lda-8" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>9 0.9337281 <a title="66-lda-9" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>10 0.91308868 <a title="66-lda-10" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>11 0.89097261 <a title="66-lda-11" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>12 0.88799775 <a title="66-lda-12" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>13 0.88400656 <a title="66-lda-13" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>14 0.88340938 <a title="66-lda-14" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>15 0.88154936 <a title="66-lda-15" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>16 0.87952769 <a title="66-lda-16" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>17 0.87668866 <a title="66-lda-17" href="./nips-2001-Information-Geometric_Decomposition_in_Spike_Analysis.html">96 nips-2001-Information-Geometric Decomposition in Spike Analysis</a></p>
<p>18 0.87476438 <a title="66-lda-18" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>19 0.86657703 <a title="66-lda-19" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>20 0.86171389 <a title="66-lda-20" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
