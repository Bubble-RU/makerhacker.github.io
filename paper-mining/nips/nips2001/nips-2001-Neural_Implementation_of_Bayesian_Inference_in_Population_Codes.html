<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-131" href="#">nips2001-131</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</h1>
<br/><p>Source: <a title="nips-2001-131-pdf" href="http://papers.nips.cc/paper/1988-neural-implementation-of-bayesian-inference-in-population-codes.pdf">pdf</a></p><p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>Reference: <a title="nips-2001-131-reference" href="../nips2001_reference/nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 for Mathematic Neuroscience, RIKEN Brain Science Institute, JAPAN  Abstract This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. [sent-2, score-1.149]
</p><p>2 We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. [sent-3, score-1.252]
</p><p>3 1  Introduction  Information in the brain is not processed by a single neuron, but rather by a population of them. [sent-4, score-0.228]
</p><p>4 It is conceivable that population coding has advantage of being robust to the fluctuation in a single neuron's activity. [sent-6, score-0.255]
</p><p>5 However, people argue that population coding may have other computationally desirable properties. [sent-7, score-0.215]
</p><p>6 One such property is to provide a framework for encoding complex objects by using basis functions [1]. [sent-8, score-0.048]
</p><p>7 It is reasonable to think that similar strategies are used in the brain under the support of population codes. [sent-11, score-0.201]
</p><p>8 However, to confirm this idea, a general suspicion has to be clarified: can the brain perform such complex statistic inference? [sent-12, score-0.049]
</p><p>9 They show that Maximum Likelihood (ML) Inference, which is usually thought to be complex, can be implemented by a biologically plausible recurrent network using the idea of line attractor. [sent-14, score-0.374]
</p><p>10 ML is a special case of Bayesian inference when the stimulus is (or assumed to be) uniformly distributed. [sent-15, score-0.266]
</p><p>11 In case there is prior knowledge on the stimulus distribution, Maximum a Posteriori (MAP) Estimate has better performance. [sent-16, score-0.354]
</p><p>12 has successfully applied MAP for reconstructing the rat position in a maze from the activity of hippocampal place cells [6]. [sent-18, score-0.201]
</p><p>13 In their method, the prior knowledge is the rat's position in the previous time step, which restricts the variability of rat's position in the current step under the continuity constraint. [sent-19, score-0.323]
</p><p>14 It turns out that MAP has a much better performance than other decoding methods, and overcomes the inefficiency of ML when information is not sufficient (when the rat stops running). [sent-20, score-0.484]
</p><p>15 So far, in the literature MAP has been mainly studied as a mathematic tool for reconstructing data, though its potential neural implementation was pointed out by [1 ,6]. [sent-22, score-0.117]
</p><p>16 In the present study, we will firmly show how to implement MAP in a biologic way. [sent-23, score-0.057]
</p><p>17 The same kind of recurrent network for achieving ML is used [4,5]. [sent-24, score-0.246]
</p><p>18 In the first step when there is no prior knowledge of the stimulus, the network implements ML. [sent-26, score-0.448]
</p><p>19 Its estimation is subsequently used to form the prior distribution of stimulus for consecutive decoding, which we assume is a Gaussian function with the mean value being the estimation. [sent-27, score-0.454]
</p><p>20 It turns out that this prior knowledge can be naturally conveyed by the change in the recurrent interactions according to the Hebbian learning rule. [sent-28, score-0.379]
</p><p>21 In the second step, with the changed interactions, the network implements MAP. [sent-30, score-0.197]
</p><p>22 The decoding accuracy of MAP and the optimal form of Gaussian prior are also analyzed in this paper. [sent-31, score-0.575]
</p><p>23 2  MAP in Population Codes  Let us consider a standard population coding paradigm. [sent-32, score-0.215]
</p><p>24 Here ri is the response of the ith neuron, which is given by  (1) where fi(X) is the tuning function and  fi  is a random noise. [sent-35, score-0.073]
</p><p>25 The encoding process of a population code is specified by the conditional probability Q(rlx) (i. [sent-36, score-0.231]
</p><p>26 The decoding is to infer the value of x from the observed r. [sent-39, score-0.398]
</p><p>27 We consider a general Bayesian inference in a population code, which estimates the stimulus by maximizing a log posterior distribution , In P(xlr) , i. [sent-40, score-0.418]
</p><p>28 , argmaxx argmaxx  In P(xlr) , InP(rlx) + InP(x),  (2)  where P(rlx) is the likelihood function. [sent-42, score-0.202]
</p><p>29 It can be equal to or different from the real encoding model Q(rlx) , depending on the available information of the encoding process [7]. [sent-43, score-0.096]
</p><p>30 P(x) is the distribution of x , representing the prior knowledge. [sent-44, score-0.093]
</p><p>31 When the distribution of x is, or is assumed to be (when there is no prior knowledge) uniform, MAP is equivalent to ML. [sent-46, score-0.093]
</p><p>32 MAP could be used in the information processing of the brain in several occasions. [sent-47, score-0.049]
</p><p>33 Let us consider the following scenario: a stimulus is decoded in multiple steps. [sent-48, score-0.226]
</p><p>34 This happens when the same stimulus is presented through multiple steps, or during a single presentation, neural signals are sampled many times. [sent-49, score-0.226]
</p><p>35 In both cases, the brain successively gains a rough estimation of the stimulus in each step decoding, which can serve to be the prior knowledge when further decoding is concerned. [sent-50, score-0.927]
</p><p>36 Experiencing slightly different stimuli in consecutive steps as studied in [6], or more generally, stimulus slowly changes with time (multiple-step diagram is a discreted approximation), is a similar scenario. [sent-52, score-0.327]
</p><p>37 For simplicity, we only consider that stimulus is unchanged in the present study. [sent-53, score-0.2]
</p><p>38 Denote Xt a particular estimation of the stimulus in the tth step, and 0; the corresponding variance. [sent-57, score-0.295]
</p><p>39 The prior distribution of x in the t + lth step is assumed to be a Gaussian with the mean value X"~ i. [sent-58, score-0.212]
</p><p>40 ,J2irTt where the parameter Tt reflects the estimator's confidence on value will be calculated later. [sent-61, score-0.028]
</p><p>41 (3)  xt,  whose optimal  The posterior distribution of x in the t + lth step is given by  P(  I )=  xr  P(rlx)P(xlxt) P(r) ,  (4)  and the solution of MAP is obtained by solving \7 In P(Xt+1 Ir)  \7lnP(rlxt+l) - (Xt+l-Xt)/T;,  O. [sent-62, score-0.144]
</p><p>42 (5)  We calculate the decoding accuracies iteratively. [sent-63, score-0.433]
</p><p>43 In the first step decoding, since there is no prior knowledge on x, ML is used, whose decoding accuracy is known to be [7] 02- Â«\7lnP(rlx))2> (6) 1 - < -\7\7lnP(rlx) >2' where the bracket < . [sent-64, score-0.73]
</p><p>44 This includes the cases when neural responses are independent, weakly correlated, uniformly correlated, correlated with strength proportional to firing rate (multiplicative correlation), or the fluctuation in neural responses are sufficiently small. [sent-67, score-0.229]
</p><p>45 In other strong correlation cases, ML is proved to be non-Fisherian, i. [sent-68, score-0.054]
</p><p>46 e, its decoding error satisfies a Cauchy type of distribution with variance diverging. [sent-69, score-0.452]
</p><p>47 Decoding accuracy can no longer be quantified by variance in such situations (for details, please refer to [8]) . [sent-70, score-0.112]
</p><p>48 Now come to calculate the decoding error in the second step. [sent-71, score-0.398]
</p><p>49 (7)  The random variable Xl can be decomposed as Xl = x + f1, where f1 is a random number satisfying Gaussian distribution of zero mean and variance Oi. [sent-75, score-0.031]
</p><p>50 By using the notation of f1, we have A  X2  -x =  \7lnP(rlx)+fdTf \7\7lnP(rlx)'  l/T; -  (8)  For the correlation cases considered in the present study (i. [sent-76, score-0.069]
</p><p>51 Obviously R satisfies the Gaussian distribution of zero mean and variance =  (10)  0I. [sent-79, score-0.054]
</p><p>52 By using the notations 0: and R, we get  X2-  X  o:R+fl = ---  (11)  1+0:  whose variance is calculated to be  (12) Since (1 + 0: 2)/(1 + 0:)2 ::::: 1 holds for any positive 0:, the decoding accuracy in the second step is always improved. [sent-80, score-0.631]
</p><p>53 - \7\71n P(rlx)  When a faithful model is used , -\7\71nQ(rlx) is the Fisher information. [sent-96, score-0.037]
</p><p>54 (14)  Tl  hence  Following the same procedure, it can be proved that the optimal decoding accuracy in the tth step is 0; = tOI when the width of Gaussian prior being Tl = tTl. [sent-99, score-0.725]
</p><p>55 It is interesting to see that the above multiple decoding procedure, when the optimal values of Tt are used, achieves the same decoding accuracy as a one-step ML by using all N x t signals. [sent-100, score-0.906]
</p><p>56 However, the multiple decoding is not a trivial replacement of one-step ML, and has many advantages. [sent-102, score-0.424]
</p><p>57 One of them is to save memory, considering that only N signals and the value of previous estimation are stored in each step. [sent-103, score-0.051]
</p><p>58 Moreover, if a slowly changing stimulus is concerned, the multiple decoding outperforms one-step ML for the balance between adaptation and memory. [sent-104, score-0.676]
</p><p>59 3  Network Implementation of MAP  In this section, we investigate how to implement MAP by a recurrent network. [sent-106, score-0.188]
</p><p>60 The network we consider is a fully connected one-dimensional homogeneous neural field, in which c denotes the position coordinate, i. [sent-109, score-0.162]
</p><p>61 The tuning function of the neuron with preferred stimulus c is  f c(x) = _1_ exp-( c- x)2/ 2a 2 . [sent-112, score-0.357]
</p><p>62 A faithful model is used in both steps decoding, i. [sent-114, score-0.037]
</p><p>63 For the above model setting, the solution of ML in the first step is calculated to be  J  rc! [sent-118, score-0.125]
</p><p>64 e(x)de,  Xl = argmaxx  where the condition  J J;(x)de =  (17)  const has been used. [sent-119, score-0.141]
</p><p>65 The solution of MAP in the second step is  X2  = argmaxx  J  rc! [sent-120, score-0.176]
</p><p>66 (18) has one more term corresponding to the contribution of prior distribution. [sent-124, score-0.093]
</p><p>67 Now come to the study of using a recurrent network to realize eqs. [sent-125, score-0.268]
</p><p>68 Let Ue denote the (average) internal state of neuron at e, and W e,e' the recurrent connection weights from neurons at e to those at e'. [sent-129, score-0.282]
</p><p>69 The dynamics of neural excitation is governed by  dUe dt where  = -Ue +  J  We ,e' 0 e, de '  + Ie,  U; oe = ----;;-=--=1 + f. [sent-130, score-0.284]
</p><p>70 LJU;de  (19)  (20)  is the activity of neurons at e and Ie is the external input arriving at e. [sent-132, score-0.132]
</p><p>71 The recurrent interactions are chosen to be W c,c' - exp-(e-e')2/ 2a 2, -  (21)  which ensures that when there is no external input (Ie = 0), the network is neutrally stable on line attractor, 'r:/z,  (22)  where the parameter D is constant and can be determined easily. [sent-133, score-0.425]
</p><p>72 Note that the line attractor has the same shape as the tuning function. [sent-134, score-0.184]
</p><p>73 This is crucial, which allows the network perform template-matching by using the tuning function , being as same as ML and MAP. [sent-135, score-0.165]
</p><p>74 When a sufficiently small input Ie is added, the network is no longer neutrally stable on the line attractor. [sent-136, score-0.245]
</p><p>75 It can be proved that the steady state of the network has approximately the same shape as eq. [sent-137, score-0.192]
</p><p>76 ), whereas, its steady position on the line attractor (i. [sent-139, score-0.227]
</p><p>77 , the network estimation) is determined by maximizing the overlap between Ie and Oe(Z)  [4,9]. [sent-141, score-0.164]
</p><p>78 Thus, if Ie = ere in the first step1, where e is a sufficiently small number, the network estimation is given by 21 = argmaxz  -------------  J  reOe(z)de,  (23)  lConsider an instant input, triggering the network to be initially at Oe(t = 0) = r e, as used in [5] , has the same result . [sent-142, score-0.412]
</p><p>79 To implement MAP in the second step, it is critical to identify a neural mechanism which can 'transmit' the prior knowledge obtained in the first step to the second one. [sent-146, score-0.308]
</p><p>80 After the first step decoding, the recurrent interaction changes a small amount according to the Hebbian rule, whose new value is  (24) where TJ is a small positive number representing the Hebbian learning rate, and Oe(,2d is the neuron activity in the first step. [sent-148, score-0.374]
</p><p>81 With the new recurrent interactions, the net input from other neurons to the one at c is calculated to be  J  We,e Oe dc' l  l  J  We,e Oe dc' +TJOe(,2d l  l  J  Oe/(zd Oe,dc' , (25)  where 1/ is a small constant. [sent-149, score-0.226]
</p><p>82 These factors ensures the approximation, Oe/ (zd Oe,dc' :=;:j const to be good enough. [sent-151, score-0.04]
</p><p>83 (25) in (19), we see that the network dynamics in the second step, when compared with the first one, is in effect to modify the input Ie to be I~ = â¬(re + AOc(zd), where A is a constant and can be determined easily. [sent-153, score-0.159]
</p><p>84 Thus, the network estimation in the second step is determined by maximizing the overlap between I~ and Oc(z), which gives  Z2  = argmaxz  J  rcOc(z)dc + A  J  Oe(zdO e(z )dc. [sent-154, score-0.34]
</p><p>85 Let us see the contribution of the second one, which can be transformed to  J  =  Bexp-CZI-Z)2/4a2,  :=;:j  Oe(zd Oc(z)dc  -B(z - zd 2 /4a 2  + terms  not on z,  (27)  where B is a constant. [sent-156, score-0.151]
</p><p>86 (I8) and (27), we see that the second term plays the same role as the prior knowledge in MAP. [sent-159, score-0.154]
</p><p>87 I) , which was done with 101 neurons uniformly distributed in the region [-3,3] and the true stimulus being at O. [sent-163, score-0.293]
</p><p>88 It shows that the estimation of the network agrees well with MAP. [sent-164, score-0.166]
</p><p>89 Table 1: Comparing the decoding accuracies of the network and MAP with different values of a (the corresponding values of T[ and A are adjusted. [sent-165, score-0.548]
</p><p>90 4  Conclusion and Discussion  In summary we have investigated how to implement MAP by using a biologically plausible recurrent network. [sent-171, score-0.273]
</p><p>91 In the first step when there is no prior knowledge, the network implements ML, whose estimation is subsequently used to form the prior distribution of stimulus for consecutive decoding. [sent-173, score-0.841]
</p><p>92 Line attractor and Hebbian learning are two critical elements to implement MAP. [sent-175, score-0.148]
</p><p>93 The former enables the network to do template-matching by using the tuning function, being as same as ML and MAP. [sent-176, score-0.165]
</p><p>94 The latter provides a mechanism that conveys the prior knowledge obtained from the first step to the second one. [sent-177, score-0.251]
</p><p>95 Though the results in this paper may quantitatively depend on the formulation of the models , it is reasonable to believe that they are qualitatively true, as both Hebbian learning and line attractor are biologically plausible. [sent-178, score-0.189]
</p><p>96 Line attractor comes from the translation invariance of network interactions, and has been shown to be involved in several neural computations [10-12]. [sent-179, score-0.206]
</p><p>97 We expect that the essential idea of Bayesian inference of utilizing previous knowledge for successive decoding is used in the information processing of the brain. [sent-180, score-0.499]
</p><p>98 We also analyzed the decoding accuracy of MAP in a population code and the optimal form of Gaussian prior. [sent-181, score-0.665]
</p><p>99 In the present study, stimulus is kept to be fixed during consecutive decodings. [sent-182, score-0.275]
</p><p>100 A generalization to the case when stimulus slowly changes over time is straightforward. [sent-183, score-0.252]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rlx', 0.578), ('decoding', 0.398), ('oe', 0.262), ('ml', 0.201), ('stimulus', 0.2), ('map', 0.162), ('population', 0.152), ('zd', 0.151), ('recurrent', 0.131), ('hebbian', 0.128), ('network', 0.115), ('argmaxx', 0.101), ('pouget', 0.101), ('prior', 0.093), ('attractor', 0.091), ('neuron', 0.084), ('implements', 0.082), ('consecutive', 0.075), ('step', 0.075), ('ie', 0.072), ('neurons', 0.067), ('rat', 0.064), ('coding', 0.063), ('interactions', 0.061), ('knowledge', 0.061), ('accuracy', 0.059), ('implement', 0.057), ('tl', 0.057), ('dc', 0.056), ('biologically', 0.055), ('slowly', 0.052), ('estimation', 0.051), ('wu', 0.05), ('argmaxz', 0.05), ('neutrally', 0.05), ('reconstructing', 0.05), ('xlr', 0.05), ('tuning', 0.05), ('brain', 0.049), ('encoding', 0.048), ('xt', 0.048), ('position', 0.047), ('steady', 0.046), ('correlated', 0.046), ('mathematic', 0.044), ('lth', 0.044), ('tth', 0.044), ('line', 0.043), ('activity', 0.04), ('inference', 0.04), ('const', 0.04), ('fluctuation', 0.04), ('notations', 0.04), ('posteriori', 0.038), ('inp', 0.037), ('oc', 0.037), ('faithful', 0.037), ('sufficiently', 0.037), ('amari', 0.036), ('subsequently', 0.035), ('accuracies', 0.035), ('rc', 0.033), ('conveyed', 0.033), ('proved', 0.031), ('variance', 0.031), ('code', 0.031), ('plausible', 0.03), ('zhang', 0.028), ('calculated', 0.028), ('responses', 0.028), ('bayesian', 0.028), ('processed', 0.027), ('tt', 0.027), ('uniformly', 0.026), ('xl', 0.026), ('multiple', 0.026), ('maximizing', 0.026), ('external', 0.025), ('optimal', 0.025), ('cases', 0.024), ('gaussian', 0.024), ('implementation', 0.023), ('efficient', 0.023), ('fi', 0.023), ('paradigm', 0.023), ('preferred', 0.023), ('satisfies', 0.023), ('correlation', 0.023), ('overlap', 0.023), ('codes', 0.023), ('study', 0.022), ('first', 0.022), ('dynamics', 0.022), ('triggering', 0.022), ('latham', 0.022), ('quantified', 0.022), ('nakahara', 0.022), ('inefficiency', 0.022), ('bracket', 0.022), ('investigates', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="131-tfidf-1" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>2 0.19952647 <a title="131-tfidf-2" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difï¬cult to obtain the true âbeliefâ by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>3 0.18738823 <a title="131-tfidf-3" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>Author: Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari</p><p>Abstract: We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1. 1</p><p>4 0.11955666 <a title="131-tfidf-4" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efï¬ciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>5 0.11809638 <a title="131-tfidf-5" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BÄÂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÃË . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>6 0.10766589 <a title="131-tfidf-6" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>7 0.10032495 <a title="131-tfidf-7" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>8 0.098349966 <a title="131-tfidf-8" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>9 0.096127957 <a title="131-tfidf-9" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>10 0.089277633 <a title="131-tfidf-10" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>11 0.086396277 <a title="131-tfidf-11" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>12 0.085191064 <a title="131-tfidf-12" href="./nips-2001-Boosting_and_Maximum_Likelihood_for_Exponential_Models.html">45 nips-2001-Boosting and Maximum Likelihood for Exponential Models</a></p>
<p>13 0.082997099 <a title="131-tfidf-13" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>14 0.072255701 <a title="131-tfidf-14" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>15 0.070524707 <a title="131-tfidf-15" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>16 0.064044029 <a title="131-tfidf-16" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>17 0.062791415 <a title="131-tfidf-17" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>18 0.062127922 <a title="131-tfidf-18" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>19 0.060000297 <a title="131-tfidf-19" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>20 0.058615215 <a title="131-tfidf-20" href="./nips-2001-Modeling_the_Modulatory_Effect_of_Attention_on_Human_Spatial_Vision.html">124 nips-2001-Modeling the Modulatory Effect of Attention on Human Spatial Vision</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, -0.19), (2, -0.092), (3, 0.004), (4, 0.051), (5, -0.135), (6, 0.099), (7, -0.134), (8, 0.106), (9, 0.127), (10, -0.139), (11, 0.039), (12, -0.08), (13, 0.008), (14, 0.051), (15, -0.064), (16, 0.201), (17, -0.049), (18, 0.058), (19, 0.043), (20, -0.069), (21, 0.152), (22, 0.129), (23, 0.013), (24, 0.022), (25, 0.046), (26, 0.151), (27, 0.001), (28, 0.094), (29, 0.071), (30, 0.022), (31, -0.114), (32, 0.03), (33, -0.101), (34, 0.008), (35, 0.11), (36, 0.097), (37, -0.07), (38, 0.053), (39, -0.041), (40, -0.077), (41, 0.038), (42, -0.039), (43, -0.046), (44, -0.008), (45, -0.117), (46, 0.057), (47, -0.029), (48, 0.022), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95630038 <a title="131-lsi-1" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>2 0.70084947 <a title="131-lsi-2" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>Author: Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari</p><p>Abstract: We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1. 1</p><p>3 0.69997185 <a title="131-lsi-3" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difï¬cult to obtain the true âbeliefâ by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>4 0.64667964 <a title="131-lsi-4" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>Author: Maoz Shamir, Haim Sompolinsky</p><p>Abstract: Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efï¬ciency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.</p><p>5 0.43933922 <a title="131-lsi-5" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<p>Author: Gal Chechik, Amir Globerson, M. J. Anderson, E. D. Young, Israel Nelken, Naftali Tishby</p><p>Abstract: The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis. 1</p><p>6 0.43378466 <a title="131-lsi-6" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>7 0.39177263 <a title="131-lsi-7" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>8 0.38135475 <a title="131-lsi-8" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>9 0.36591968 <a title="131-lsi-9" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>10 0.34934488 <a title="131-lsi-10" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>11 0.34227967 <a title="131-lsi-11" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>12 0.34098819 <a title="131-lsi-12" href="./nips-2001-Spike_timing_and_the_coding_of_naturalistic_sounds_in_a_central_auditory_area_of_songbirds.html">174 nips-2001-Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds</a></p>
<p>13 0.33637065 <a title="131-lsi-13" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>14 0.33380878 <a title="131-lsi-14" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>15 0.3316946 <a title="131-lsi-15" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>16 0.32641685 <a title="131-lsi-16" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>17 0.31966561 <a title="131-lsi-17" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>18 0.31664333 <a title="131-lsi-18" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>19 0.29535696 <a title="131-lsi-19" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>20 0.28470823 <a title="131-lsi-20" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.232), (14, 0.05), (17, 0.026), (19, 0.033), (27, 0.146), (30, 0.084), (38, 0.052), (59, 0.039), (72, 0.065), (79, 0.041), (91, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86034155 <a title="131-lda-1" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>Author: Benjamin Blankertz, Gabriel Curio, Klaus-Robert MÃ¼ller</p><p>Abstract: Driven by the progress in the ï¬eld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming ï¬nger movements in a natural keyboard typing condition and predicts their laterality. This can be done on average 100â230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiï¬cation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiï¬ers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable regularization properties for dealing with high noise cases (inter-trial variablity).</p><p>same-paper 2 0.84081429 <a title="131-lda-2" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>Author: Si Wu, Shun-ichi Amari</p><p>Abstract: This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate), and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning. 1</p><p>3 0.70716667 <a title="131-lda-3" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>4 0.69645494 <a title="131-lda-4" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>Author: MÃ¡rio Figueiredo</p><p>Abstract: In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classiï¬cation. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparsenesscontrolling hyper-parameters.</p><p>5 0.69484526 <a title="131-lda-5" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>6 0.69022268 <a title="131-lda-6" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>7 0.69019091 <a title="131-lda-7" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>8 0.68976301 <a title="131-lda-8" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>9 0.68974751 <a title="131-lda-9" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>10 0.68859953 <a title="131-lda-10" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>11 0.68726867 <a title="131-lda-11" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>12 0.68719721 <a title="131-lda-12" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>13 0.6863395 <a title="131-lda-13" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>14 0.6854893 <a title="131-lda-14" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>15 0.68466437 <a title="131-lda-15" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>16 0.68362963 <a title="131-lda-16" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>17 0.68275791 <a title="131-lda-17" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>18 0.68187755 <a title="131-lda-18" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>19 0.68095636 <a title="131-lda-19" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>20 0.68068397 <a title="131-lda-20" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
