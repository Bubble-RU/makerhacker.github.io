<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-117" href="#">nips2001-117</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</h1>
<br/><p>Source: <a title="nips-2001-117-pdf" href="http://papers.nips.cc/paper/2009-mime-mutual-information-minimization-and-entropy-maximization-for-bayesian-belief-propagation.pdf">pdf</a></p><p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>Reference: <a title="nips-2001-117-reference" href="../nips2001_reference/nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. [sent-7, score-0.075]
</p><p>2 demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. [sent-9, score-0.421]
</p><p>3 Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. [sent-10, score-0.32]
</p><p>4 In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). [sent-11, score-0.396]
</p><p>5 Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. [sent-12, score-0.433]
</p><p>6 For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. [sent-13, score-0.284]
</p><p>7 1  Introduction  In graphical models, Bayesian belief propagation (BBP) algorithms often (but not always) yield reasonable estimates of the marginal probabilities at each node [6]. [sent-15, score-0.126]
</p><p>8 [7] demonstrated an intriguing connection between BBP and certain inference methods based in statistical physics. [sent-17, score-0.017]
</p><p>9 Essentially, they demonstrated that traditional BBP algorithms can be shown to arise from approximations  of the extrema of the Bethe and Kikuchi free energies. [sent-18, score-0.138]
</p><p>10 Next, Yuille [8] derived new double-loop algorithms which are guaranteed to minimize the Bethe and Kikuchi energy functions while continuing to have close ties to the original BBP algorithms. [sent-19, score-0.199]
</p><p>11 Yuille’s approach relies on a certain decomposition of the Bethe and Kikuchi free energies. [sent-20, score-0.121]
</p><p>12 In the present work, we begin with a new principle—pairwise mutual information minimization and marginal entropy maximization (MIME)—and derive a new energy function which is shown to be equivalent to the Bethe free energy. [sent-21, score-0.374]
</p><p>13 After demonstrating this connection, we derive a family of free energies closely related to the MIME principle which also shown to be equivalent, when constraint satisfaction is exact, to the Bethe free energy. [sent-22, score-0.422]
</p><p>14 For each member in this family of energy functions , we derive a new algorithm that is guaranteed to converge to a local minimum. [sent-23, score-0.247]
</p><p>15 Moreover, the resulting form of the algorithm is very simple despite the somewhat unwieldy nature of the algebraic development. [sent-24, score-0.063]
</p><p>16 Preliminary comparisons of the new algorithm with BBP were carried out on spin glass-like problems and indicate that the new algorithm is convergent when BBP is not. [sent-25, score-0.033]
</p><p>17 2  Bethe free energy and the MIME principle  In this section, we show that the Bethe free energy can be interpreted as pairwise mutual information minimization and marginal entropy maximization. [sent-27, score-0.629]
</p><p>18 Link functions ψij > 0 are available relational data between nodes i and j. [sent-29, score-0.017]
</p><p>19 The singleton function ψi is also available at each node i. [sent-30, score-0.032]
</p><p>20 The double summation ij:i>j is carried out only over the nodes that are connected. [sent-31, score-0.027]
</p><p>21 The Lagrange parameters {λij , γij } are needed in the Bethe free energy (1) to satisfy the following constraints relating the joint probabilities {pij } with the marginals {pi }: pij (xi , xj ) = pj (xj ), xi  pij (xi , xj ) = pi (xi ), and  pij (xi , xj ) = 1. [sent-32, score-3.887]
</p><p>22 (2)  xi ,xj  xj  The pairwise mutual information is deﬁned as M Iij =  pij (xi , xj ) log xi ,xj  pij (xi , xj ) pi (xi )pj (xj )  (3)  The mutual information is minimized when the joint probability pij (xi , xj ) = pi (xi )pj (xj ) or equivalently when nodes i and j are independent. [sent-33, score-4.899]
</p><p>23 When nodes i and j are connected via a non-separable link ψij (xi , xj ) they will not be independent. [sent-34, score-0.484]
</p><p>24 Statement of the MIME principle: Maximize the marginal entropy and minimize the pairwise mutual information using the available marginal and pairwise link function expectations while satisfying the joint probability constraints. [sent-36, score-0.231]
</p><p>25 (4)  In the above free energy, we minimize the pairwise mutual information and maximize the marginal entropies. [sent-38, score-0.218]
</p><p>26 The singleton and pairwise link functions are additional information which do not allow the system to reach its “natural” equilibrium—a uniform i. [sent-39, score-0.082]
</p><p>27 The Lagrange parameters enforce the constraints between the pairwise and marginal probabilities. [sent-43, score-0.089]
</p><p>28 These constraints are the same as in the Bethe free energy (1). [sent-44, score-0.252]
</p><p>29 Note that the Lagrange parameter terms vanish if the constraints in (2) are exactly satisﬁed. [sent-45, score-0.035]
</p><p>30 This is an important point when considering equivalences between diﬀerent energy functions. [sent-46, score-0.132]
</p><p>31 Lemma 1 Provided the constraints in (2) are exactly satisﬁed, the MIME free energy in (4) is equivalent to the Bethe free energy in (1). [sent-47, score-0.498]
</p><p>32 ni i  (5)  xi  We have shown that a marginal entropy term emerges from the mutual information term in (4) when constraint satisfaction is exact. [sent-49, score-0.804]
</p><p>33 Collecting the marginal entropy terms together and rearranging the MIME free energy in (4), we get the Bethe free energy in (1). [sent-50, score-0.523]
</p><p>34 3  A family of decompositions of the Bethe free energy  Recall that the Bethe free energy and the energy function resulting from application of the MIME principle were shown to be equivalent. [sent-51, score-0.708]
</p><p>35 However, the MIME energy function is merely one particular decomposition of the Bethe free energy. [sent-52, score-0.244]
</p><p>36 The main motivation for considering alternative decompositions is for algorithmic reasons. [sent-54, score-0.045]
</p><p>37 We believe that certain decompositions may be more eﬀective than others. [sent-55, score-0.054]
</p><p>38 This belief is based on our previous experience with closely related deterministic annealing algorithms [3, 2]. [sent-56, score-0.056]
</p><p>39 In this section, we derive a family of free energies that are equivalent to the Bethe free energy provided constraint satisfaction is exact. [sent-57, score-0.556]
</p><p>40 The family of free energies is inspired by and closely related to the MIME free energy in (4). [sent-58, score-0.403]
</p><p>41 Lemma 2 The following family of energy functions indexed by the free parameters δ > 0 and {ξi } is equivalent to the original Bethe free energy (1) provided the  constraints in (2) are exactly satisﬁed and the parameters q and r are set to {q i = (1 − δ)ni } and {ri = 1 − ni ξi } respectively. [sent-59, score-0.595]
</p><p>42 (6)  In (6), the ﬁrst term is no longer the pairwise mutual information as in (4). [sent-61, score-0.072]
</p><p>43 And unlike (4), pi (xi ) no longer appears in the pairwise mutual information-like term. [sent-62, score-0.229]
</p><p>44 Proof: We selectively substitute xi pij (xi , xj ) = pj (xj ) and pi (xi ) to show the equivalence. [sent-63, score-1.734]
</p><p>45 First pij (xi , xj )]δ [  pij (xi , xj ) log[ ij:i>j xi ,xj  xj  pij (xi , xj )]δ = δ  ξ  xj  i ξ  pij (xi , xj ) log ψi i (xi )ψj j (xj ) = ij:i>j xi ,xj  pij (xi , xj ) =  pi (xi ) log pi (xi ),  ni  xi  xj  pi (xi ) log ψi (xi ). [sent-64, score-8.437]
</p><p>46 n i ξi i  (7)  xj  Substituting the identities in (7) into (6), we see that the free energies are algebraically equivalent. [sent-65, score-0.598]
</p><p>47 4  A family of algorithms for belief propagation  We now derive descent algorithms for the family of energy functions in (6). [sent-66, score-0.324]
</p><p>48 All the algorithms are guaranteed to converge to a local minimum of (6) under mild assumptions regarding the number of ﬁxed points. [sent-67, score-0.052]
</p><p>49 For each member in the family of energy functions, there is a corresponding descent algorithm. [sent-68, score-0.196]
</p><p>50 Since the form of the free energy in (6) is complex and precludes easy minimization, we use algebraic (Legendre) transformations [1] to simplify the optimization. [sent-69, score-0.266]
</p><p>51 ρi (xi )  (8)  We now apply the above algebraic transforms. [sent-71, score-0.03]
</p><p>52 (9)  xi ,xj  We continue to keep the parameters {qi } and {ri } in (9). [sent-73, score-0.567]
</p><p>53 However, from Lemma 2, we know that the equivalence of (9) to the Bethe free energy is predicated upon appropriate setting of these parameters. [sent-74, score-0.236]
</p><p>54 The algebraically transformed energy function in (9) is separately convex w. [sent-77, score-0.159]
</p><p>55 Since the overall energy function is not convex w. [sent-84, score-0.146]
</p><p>56 all the variables, we pursue an alternating algorithm strategy similar to the double loop algorithm in Yuille [8]. [sent-87, score-0.095]
</p><p>57 the variables {σij , ρi } and the variables {pij , pi }. [sent-91, score-0.157]
</p><p>58 The linear constraints in (2) are enforced when minimizing w. [sent-92, score-0.016]
</p><p>59 t the latter and do not aﬀect the convergence properties of the algorithm since the energy function w. [sent-94, score-0.141]
</p><p>60 pij (xi , xj ), σji (xi ) =  σij (xj ) =  (10)  xj  xi  The ﬁxpoints of {pij , pi } are evaluated next. [sent-104, score-2.12]
</p><p>61 ξ  ξ δ δ = σji (xi )σij (xj )ψij (xi , xj )ψi i (xi )ψj j (xj )e−λij (xj )−λji (xi )−γij −1  pij (xi , xj )  r = ρqi (xi )ψi i (xi )e i  pi (xi )  k  λki (xi )−1  (11)  . [sent-109, score-1.561]
</p><p>62 Consider a Lagrange parameter update sequence where the Lagrange parameter currently being updated is tagged as “new” with the rest designated as “old. [sent-111, score-0.06]
</p><p>63 ” We can then rewrite the Lagrange parameter updates using “old” and “new” values. [sent-112, score-0.019]
</p><p>64 Please note that each Lagrange parameter update corresponds to one of the constraints in (2). [sent-113, score-0.057]
</p><p>65 It can be shown that the iterative update of the Lagrange parameters is guaranteed to converge to the unique solution of (2) [8]. [sent-114, score-0.052]
</p><p>66 While rewriting (12), we multiply the old left and right sides with e−2λji (xi ) . [sent-115, score-0.043]
</p><p>67 new  e2λji  (xi )−2λold (xi ) ji  xj  =  ξ −λold (xj )−λold (xi )−γ old −1 ξ δ δ ji ij σji (xi )σij (xj )ψij (xi ,xj )ψi i (xi )ψj j (xj )e ij q  r  ρi i (xi )ψi i (xi )e  k  λold (xi )−1 ki  . [sent-116, score-1.214]
</p><p>68 (13)  Using (11), we relate each Lagrange parameter update with an update of p ij (xi , xj ) and pi (xi ). [sent-117, score-0.932]
</p><p>69 The update equations (16) can be seen to satisfy the ﬁrst constraint in (2). [sent-121, score-0.047]
</p><p>70 Similar update equations can be derived for the other constraints in (2). [sent-122, score-0.048]
</p><p>71 For each Lagrange parameter update, an equivalent, simultaneous probability (joint and marginal) update can be derived similar to (16). [sent-123, score-0.051]
</p><p>72 The overall family of algorithms can be summarized as shown in the pseudocode. [sent-124, score-0.053]
</p><p>73 Despite the unwieldy algebraic development preceding it, the algorithm is very simple and straightforward. [sent-125, score-0.054]
</p><p>74 pi (xi ) pij (xi ,xj )  pij (xi , xj ) ← pij (xi , xj )  xj  pi (xi ) ←  pi (xi )  xj  pij (xi , xj )  Simultaneously update pij (xi , xj ) and pj (xj ) below. [sent-130, score-5.753]
</p><p>75 pij (xi , xj ) ← pij (xi , xj )  pj (xj ) pij (xi ,xj )  xi  pj (xj ) ←  pj (xj )  xi  pij (xi , xj )  Normalize pij (xi , xj ). [sent-131, score-5.606]
</p><p>76 pij (xi , xj ) ←  pij (xi ,xj ) pij (xi ,xj )  xi ,xj  End B End A In the above family of algorithms, the MIME algorithm corresponds to free parameter settings δ = 1 and ξi = 0 which in turn lead to parameter settings qi = 0 and ri = 1. [sent-132, score-2.774]
</p><p>77 The Yuille [8] double loop algorithm corresponds to the free parameter settings δ = 0 and ξi = 0 which in turn leads to parameter settings qi = ni and ri = 1. [sent-133, score-0.368]
</p><p>78 A crucial point is that the energy function for every valid parameter setting is equivalent to the Bethe free energy provided constraint satisfaction is exact. [sent-134, score-0.532]
</p><p>79 The inner loop constraint satisfaction threshold parameter cthr setting is very important in this regard. [sent-135, score-0.271]
</p><p>80 We are obviously not restricted to the MIME parameter settings. [sent-136, score-0.019]
</p><p>81 At this early stage of exploration of the inter-relationships between Bayesian belief propagation and inference methods based in statistical physics [7], it is premature to speculate regarding the “best” parameter settings for δ and {ξi }. [sent-137, score-0.1]
</p><p>82 Most likely, the eﬀectiveness of the algorithms will vary depending on the problem setting which enters into the formulation via the link functions {ψij } and the singleton functions {ψi }. [sent-138, score-0.063]
</p><p>83 5  Results  We implemented the family of algorithms in C++ and conducted tests on locally connected 50 node graphs and binary state variables. [sent-139, score-0.068]
</p><p>84 The ψi (xi ) and ψij (xi , xj ) are of the form e±hi and e±hij where hi and hij are drawn from uniform distributions (in the interval [−1, 1]). [sent-140, score-0.481]
</p><p>85 Provided the constraint satisfaction theshold parameter cthr was set low enough, the algorithm (for δ = 1 and other parameter settings as described in Figure 1) exhibited monotonic convergence. [sent-141, score-0.233]
</p><p>86 Figure 2 shows the number of inner loop iterations corresponding to diﬀerent settings of the constraint satisfaction threshold parameter. [sent-142, score-0.248]
</p><p>87 We also implemented the BBP algorithm and empirically observed that it often did not converge for these graphs. [sent-143, score-0.022]
</p><p>88 6  Conclusion  We began with the MIME principle and showed the equivalence of the MIMEbased free energy to the Bethe free energy assuming constraint satisfaction to be exact. [sent-146, score-0.615]
</p><p>89 Then, we derived new decompositions of the Bethe free energy inspired by the MIME principle, and driven by our belief that certain decompositions may be more eﬀective than others. [sent-147, score-0.375]
</p><p>90 We then derived a convergent algorithm for each member in the family of MIME-based decompositions. [sent-148, score-0.085]
</p><p>91 While the MIME-based algorithms derived here use closed-form solutions in the constraint satisfaction inner loop, it may turn out that the inner loop is better handled using preconditioned gradient-based descent algorithms. [sent-150, score-0.265]
</p><p>92 And it is important to explore the inter-relationships between the convergent MIME-based descent algorithms and other recent related approaches with interesting convergence properties [4, 5]. [sent-151, score-0.041]
</p><p>93 Self annealing and self annihilation: unifying deterministic annealing and relaxation labeling. [sent-159, score-0.036]
</p><p>94 Correctness of local probability propagation in graphical models with loops. [sent-181, score-0.044]
</p><p>95 Bethe free energy, Kikuchi approximations and belief propagation algorithms. [sent-189, score-0.161]
</p><p>96 A double loop algorithm to minimize the Bethe and Kikuchi free energies. [sent-195, score-0.199]
</p><p>97 5  1500  0  500  1000  iteration  iteration  (b)  1500  iteration  (a)  (c)  Figure 1: MIME energy versus outer loop iteration: 50 node, local topology, δ = 1. [sent-204, score-0.273]
</p><p>98 Constraint satisfaction threshold parameter cthr was set to (a) 10−8 (b) 10−4 (c) 10−2 7  20  2  18  1. [sent-205, score-0.164]
</p><p>99 8  12  10  8  5  total # of inner loop iterations  14  total # of inner loop iterations  total # of inner loop iterations  16  4  3  6  1. [sent-207, score-0.276]
</p><p>100 1  0  500  1000 outer loop iteration index  (a)  1500  1  0  500  1000 outer loop iteration index  (b)  1500  1  0  500  1000  1500  outer loop iteration index  (c)  Figure 2: Inner loop iterations versus outer loop: 50 node, local topology, δ = 1. [sent-214, score-0.412]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xi', 0.559), ('pij', 0.488), ('xj', 0.458), ('ij', 0.254), ('mime', 0.16), ('pi', 0.157), ('bethe', 0.139), ('energy', 0.132), ('free', 0.104), ('satisfaction', 0.099), ('ji', 0.094), ('pold', 0.076), ('pj', 0.072), ('loop', 0.059), ('bbp', 0.053), ('log', 0.049), ('yuille', 0.048), ('decompositions', 0.045), ('old', 0.043), ('family', 0.04), ('pairwise', 0.04), ('ni', 0.038), ('cthr', 0.038), ('lagrange', 0.035), ('marginal', 0.033), ('mutual', 0.032), ('algebraic', 0.03), ('belief', 0.03), ('qi', 0.03), ('outer', 0.028), ('propagation', 0.027), ('pnew', 0.026), ('constraint', 0.025), ('ri', 0.024), ('kikuchi', 0.024), ('settings', 0.024), ('inner', 0.023), ('energies', 0.023), ('update', 0.022), ('principle', 0.019), ('parameter', 0.019), ('entropy', 0.018), ('double', 0.018), ('guaranteed', 0.017), ('link', 0.017), ('ki', 0.017), ('singleton', 0.017), ('constraints', 0.016), ('minimization', 0.015), ('fequiv', 0.015), ('hij', 0.015), ('unwieldy', 0.015), ('xpoints', 0.015), ('node', 0.015), ('convergent', 0.015), ('iteration', 0.015), ('convex', 0.014), ('algorithms', 0.013), ('converge', 0.013), ('yedidia', 0.013), ('mentions', 0.013), ('extrema', 0.013), ('algebraically', 0.013), ('anand', 0.013), ('rangarajan', 0.013), ('begin', 0.013), ('descent', 0.013), ('annealing', 0.013), ('ectiveness', 0.012), ('legendre', 0.012), ('member', 0.011), ('provided', 0.011), ('lemma', 0.011), ('iterations', 0.01), ('equivalent', 0.01), ('ties', 0.01), ('derived', 0.01), ('self', 0.01), ('satis', 0.009), ('ective', 0.009), ('local', 0.009), ('joint', 0.009), ('nodes', 0.009), ('despite', 0.009), ('certain', 0.009), ('minimize', 0.009), ('topology', 0.009), ('maximization', 0.009), ('preliminary', 0.009), ('algorithm', 0.009), ('bayesian', 0.008), ('hi', 0.008), ('continue', 0.008), ('graphical', 0.008), ('threshold', 0.008), ('functions', 0.008), ('decomposition', 0.008), ('derive', 0.008), ('demonstrated', 0.008), ('min', 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="117-tfidf-1" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>2 0.17446809 <a title="117-tfidf-2" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>Author: John Shawe-Taylor, Nello Cristianini, Jaz S. Kandola</p><p>Abstract: We consider the problem of measuring the eigenvalues of a randomly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results. 1</p><p>3 0.141543 <a title="117-tfidf-3" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>Author: Florence D'alché-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><p>4 0.13037042 <a title="117-tfidf-4" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>5 0.11210443 <a title="117-tfidf-5" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1</p><p>6 0.096211821 <a title="117-tfidf-6" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>7 0.094596662 <a title="117-tfidf-7" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>8 0.087852143 <a title="117-tfidf-8" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>9 0.071370021 <a title="117-tfidf-9" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>10 0.070427619 <a title="117-tfidf-10" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>11 0.070093654 <a title="117-tfidf-11" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>12 0.057970777 <a title="117-tfidf-12" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>13 0.055813555 <a title="117-tfidf-13" href="./nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</a></p>
<p>14 0.054835241 <a title="117-tfidf-14" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>15 0.054106861 <a title="117-tfidf-15" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>16 0.053920414 <a title="117-tfidf-16" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>17 0.053406056 <a title="117-tfidf-17" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>18 0.047811907 <a title="117-tfidf-18" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>19 0.047164369 <a title="117-tfidf-19" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>20 0.04584495 <a title="117-tfidf-20" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.11), (1, 0.051), (2, 0.009), (3, -0.099), (4, 0.111), (5, -0.074), (6, 0.029), (7, -0.118), (8, 0.03), (9, 0.077), (10, 0.071), (11, 0.1), (12, 0.054), (13, 0.09), (14, 0.154), (15, -0.022), (16, -0.094), (17, -0.127), (18, 0.061), (19, 0.041), (20, -0.088), (21, -0.094), (22, -0.224), (23, 0.082), (24, -0.13), (25, -0.247), (26, -0.135), (27, -0.049), (28, 0.097), (29, 0.038), (30, -0.187), (31, 0.032), (32, -0.212), (33, -0.047), (34, -0.103), (35, 0.055), (36, -0.141), (37, -0.02), (38, 0.084), (39, 0.002), (40, -0.071), (41, 0.149), (42, 0.075), (43, -0.079), (44, -0.049), (45, -0.014), (46, 0.02), (47, 0.035), (48, 0.053), (49, -0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99766099 <a title="117-lsi-1" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>2 0.5276196 <a title="117-lsi-2" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>Author: John Shawe-Taylor, Nello Cristianini, Jaz S. Kandola</p><p>Abstract: We consider the problem of measuring the eigenvalues of a randomly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results. 1</p><p>3 0.48418376 <a title="117-lsi-3" href="./nips-2001-The_Concave-Convex_Procedure_%28CCCP%29.html">180 nips-2001-The Concave-Convex Procedure (CCCP)</a></p>
<p>Author: Alan L. Yuille, Anand Rangarajan</p><p>Abstract: We introduce the Concave-Convex procedure (CCCP) which constructs discrete time iterative dynamical systems which are guaranteed to monotonically decrease global optimization/energy functions. It can be applied to (almost) any optimization problem and many existing algorithms can be interpreted in terms of CCCP. In particular, we prove relationships to some applications of Legendre transform techniques. We then illustrate CCCP by applications to Potts models, linear assignment, EM algorithms, and Generalized Iterative Scaling (GIS). CCCP can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms. 1</p><p>4 0.48225492 <a title="117-lsi-4" href="./nips-2001-Semi-supervised_MarginBoost.html">167 nips-2001-Semi-supervised MarginBoost</a></p>
<p>Author: Florence D'alché-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><p>5 0.42229223 <a title="117-lsi-5" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>6 0.34861788 <a title="117-lsi-6" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>7 0.3451626 <a title="117-lsi-7" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>8 0.33678085 <a title="117-lsi-8" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>9 0.32983145 <a title="117-lsi-9" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>10 0.30966276 <a title="117-lsi-10" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>11 0.30795312 <a title="117-lsi-11" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>12 0.2964341 <a title="117-lsi-12" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>13 0.26738417 <a title="117-lsi-13" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>14 0.25338709 <a title="117-lsi-14" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>15 0.2510432 <a title="117-lsi-15" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>16 0.23438995 <a title="117-lsi-16" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>17 0.23388778 <a title="117-lsi-17" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>18 0.22409809 <a title="117-lsi-18" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>19 0.20206091 <a title="117-lsi-19" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>20 0.16658573 <a title="117-lsi-20" href="./nips-2001-Group_Redundancy_Measures_Reveal_Redundancy_Reduction_in_the_Auditory_Pathway.html">87 nips-2001-Group Redundancy Measures Reveal Redundancy Reduction in the Auditory Pathway</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.023), (19, 0.016), (27, 0.598), (30, 0.029), (59, 0.018), (72, 0.039), (79, 0.054), (91, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99539369 <a title="117-lda-1" href="./nips-2001-MIME%3A_Mutual_Information_Minimization_and_Entropy_Maximization_for_Bayesian_Belief_Propagation.html">117 nips-2001-MIME: Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation</a></p>
<p>Author: Anand Rangarajan, Alan L. Yuille</p><p>Abstract: Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation ﬁxed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development. 1</p><p>2 0.98235059 <a title="117-lda-2" href="./nips-2001-Scaling_Laws_and_Local_Minima_in_Hebbian_ICA.html">165 nips-2001-Scaling Laws and Local Minima in Hebbian ICA</a></p>
<p>Author: Magnus Rattray, Gleb Basalyga</p><p>Abstract: We study the dynamics of a Hebbian ICA algorithm extracting a single non-Gaussian component from a high-dimensional Gaussian background. For both on-line and batch learning we ﬁnd that a surprisingly large number of examples are required to avoid trapping in a sub-optimal state close to the initial conditions. To extract a skewed signal at least examples are required for -dimensional data and examples are required to extract a symmetrical signal with non-zero kurtosis. § ¡ ©£¢  £ §¥ ¡ ¨¦¤£¢</p><p>3 0.98147267 <a title="117-lda-3" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>Author: Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We investigate a learning algorithm for the classiﬁcation of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classiﬁers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm—its guarantee of monotonic improvement, and its absence of tuning parameters—with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classiﬁcation than comparably sized models trained by EM. 1</p><p>4 0.97900975 <a title="117-lda-4" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We compare discriminative and generative learning as typified by logistic regression and naive Bayes. We show, contrary to a widelyheld belief that discriminative classifiers are almost always to be preferred, that there can often be two distinct regimes of performance as the training set size is increased, one in which each algorithm does better. This stems from the observation- which is borne out in repeated experiments- that while discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster. 1</p><p>5 0.97684127 <a title="117-lda-5" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>Author: Mikhail Belkin, Partha Niyogi</p><p>Abstract: Drawing on the correspondence between the graph Laplacian, the Laplace-Beltrami op erator on a manifold , and the connections to the heat equation , we propose a geometrically motivated algorithm for constructing a representation for data sampled from a low dimensional manifold embedded in a higher dimensional space. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality preserving properties and a natural connection to clustering. Several applications are considered. In many areas of artificial intelligence, information retrieval and data mining, one is often confronted with intrinsically low dimensional data lying in a very high dimensional space. For example, gray scale n x n images of a fixed object taken with a moving camera yield data points in rn: n2 . However , the intrinsic dimensionality of the space of all images of t he same object is the number of degrees of freedom of the camera - in fact the space has the natural structure of a manifold embedded in rn: n2 . While there is a large body of work on dimensionality reduction in general, most existing approaches do not explicitly take into account the structure of the manifold on which the data may possibly reside. Recently, there has been some interest (Tenenbaum et aI, 2000 ; Roweis and Saul, 2000) in the problem of developing low dimensional representations of data in this particular context. In this paper , we present a new algorithm and an accompanying framework of analysis for geometrically motivated dimensionality reduction. The core algorithm is very simple, has a few local computations and one sparse eigenvalu e problem. The solution reflects th e intrinsic geom etric structure of the manifold. Th e justification comes from the role of the Laplacian op erator in providing an optimal emb edding. Th e Laplacian of the graph obtained from the data points may be viewed as an approximation to the Laplace-Beltrami operator defined on the manifold. The emb edding maps for the data come from approximations to a natural map that is defined on the entire manifold. The framework of analysis presented here makes this connection explicit. While this connection is known to geometers and specialists in sp ectral graph theory (for example , see [1, 2]) to the best of our knowledge we do not know of any application to data representation yet. The connection of the Laplacian to the heat kernel enables us to choose the weights of the graph in a principled manner. The locality preserving character of the Laplacian Eigenmap algorithm makes it relatively insensitive to outliers and noise. A byproduct of this is that the algorithm implicitly emphasizes the natural clusters in the data. Connections to spectral clustering algorithms developed in learning and computer vision (see Shi and Malik , 1997) become very clear. Following the discussion of Roweis and Saul (2000) , and Tenenbaum et al (2000), we note that the biological perceptual apparatus is confronted with high dimensional stimuli from which it must recover low dimensional structure. One might argue that if the approach to recovering such low-dimensional structure is inherently local , then a natural clustering will emerge and thus might serve as the basis for the development of categories in biological perception. 1 The Algorithm Given k points Xl , ... , Xk in ]]{ I, we construct a weighted graph with k nodes, one for each point , and the set of edges connecting neighboring points to each other. 1. Step 1. [Constru cting th e Graph] We put an edge between nodes i and j if Xi and Xj are</p><p>6 0.97477221 <a title="117-lda-6" href="./nips-2001-A_theory_of_neural_integration_in_the_head-direction_system.html">23 nips-2001-A theory of neural integration in the head-direction system</a></p>
<p>7 0.96161699 <a title="117-lda-7" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>8 0.87797976 <a title="117-lda-8" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>9 0.84783524 <a title="117-lda-9" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>10 0.8153649 <a title="117-lda-10" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>11 0.75968385 <a title="117-lda-11" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>12 0.75404716 <a title="117-lda-12" href="./nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</a></p>
<p>13 0.74958646 <a title="117-lda-13" href="./nips-2001-Multi_Dimensional_ICA_to_Separate_Correlated_Sources.html">127 nips-2001-Multi Dimensional ICA to Separate Correlated Sources</a></p>
<p>14 0.74904281 <a title="117-lda-14" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>15 0.74848211 <a title="117-lda-15" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>16 0.73795199 <a title="117-lda-16" href="./nips-2001-Learning_from_Infinite_Data_in_Finite_Time.html">114 nips-2001-Learning from Infinite Data in Finite Time</a></p>
<p>17 0.735282 <a title="117-lda-17" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>18 0.73238438 <a title="117-lda-18" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>19 0.73182851 <a title="117-lda-19" href="./nips-2001-Products_of_Gaussians.html">154 nips-2001-Products of Gaussians</a></p>
<p>20 0.72907549 <a title="117-lda-20" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
