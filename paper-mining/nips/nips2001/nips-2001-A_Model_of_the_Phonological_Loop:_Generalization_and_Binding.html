<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-12" href="#">nips2001-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</h1>
<br/><p>Source: <a title="nips-2001-12-pdf" href="http://papers.nips.cc/paper/2025-a-model-of-the-phonological-loop-generalization-and-binding.pdf">pdf</a></p><p>Author: Randall C. O'Reilly, R. Soto</p><p>Abstract: We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training. 1</p><p>Reference: <a title="nips-2001-12-reference" href="../nips2001_reference/nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i. [sent-4, score-1.059]
</p><p>2 The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. [sent-7, score-1.188]
</p><p>3 Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. [sent-8, score-0.314]
</p><p>4 To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. [sent-9, score-0.582]
</p><p>5 To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training. [sent-10, score-0.573]
</p><p>6 1  Introduction  Sequential binding is a version of the binding problem requiring that the identity of an item and its position within a sequence be bound. [sent-11, score-0.791]
</p><p>7 It has been suggested that the brain may have developed a specialized system for this form of binding in the domain of phonological sequences, in the form of the phonological loop (Baddeley, 1986; Baddeley, Gathercole, & Papagno, 1998; Burgess & Hitch, 1999). [sent-15, score-1.412]
</p><p>8 The phonological loop is generally conceived of as a system that can quickly encode a sequence of phonemes and then repeat this sequence back repeatedly. [sent-16, score-0.986]
</p><p>9 Standard estimates place the capacity of this loop at about 2. [sent-17, score-0.168]
</p><p>10 5 seconds of "inner speech," and it is widely regarded as depending on the prefrontal cortex (e. [sent-18, score-0.288]
</p><p>11 We have developed a model of the phonological loop based on our existing framework for understanding how the prefrontal cortex and basal ganglia interact to support  activation-based working memory (Frank, Loughry, & O'Reilly, 2001). [sent-21, score-1.414]
</p><p>12 This model performs binding by using different neural substrates for the different sequential positions of phonemes. [sent-22, score-0.496]
</p><p>13 This is a viable solution for a small, closed-class set of items like phonemes. [sent-23, score-0.143]
</p><p>14 However, through the combinatorial power of language, these phonological sequences can represent a huge number of distinct combinations of concepts. [sent-24, score-0.522]
</p><p>15 Therefore, this basic maintenance mechanism can be leveraged in many different circumstances to bind information needed for immediate use (e. [sent-25, score-0.234]
</p><p>16 A good example of this form of transient, phonologically-dependent binding comes from a task studied by Miyake and Soto (in preparation). [sent-28, score-0.364]
</p><p>17 In this task, participants saw sequentially-presented colored letters one at a time on a computer display, and had to respond to targets of a red X or a green Y, but not to any other color-letter combination (e. [sent-29, score-0.119]
</p><p>18 , green X's and red Y's, which were also presented). [sent-31, score-0.08]
</p><p>19 After an initial series of trials with this set of targets, the targets were switched to be a green X and a red Y. [sent-32, score-0.08]
</p><p>20 Thus, the task clearly requires binding of color and letter information, and updating of these bindings after the switch condition. [sent-33, score-0.429]
</p><p>21 Miyake and Soto (in preparation) found that if they simply had participants repeat the word "the" over and over during the task (i. [sent-34, score-0.072]
</p><p>22 Miyake and Soto (in preparation) interpret this pattern of results as showing that the phonological loop supports the binding of stimulus features (e. [sent-38, score-0.94]
</p><p>23 , participants repeatedly say to themselves "red X, green y' . [sent-40, score-0.078]
</p><p>24 " , which is supported by debriefing reports), and that the use of this phonological system for unrelated information during articulatory suppression leads to the observed performance deficits. [sent-42, score-0.534]
</p><p>25 This form of phonological binding can be contrasted with other forms of binding that can be used in other situations and subserved by other brain areas besides the prefrontal cortex. [sent-43, score-1.286]
</p><p>26 This form of binding takes place within the basic representations in the network that are shaped by gradual learning processes and provides a long-lasting (nontransient) form of binding. [sent-47, score-0.442]
</p><p>27 However, this form of binding cannot rapidly encode novel bindings required for specific tasks - the phonological loop mechanism can thus complement the basic cortical mechanism by providing flexible, transient bindings on an ad-hoc basis. [sent-49, score-1.542]
</p><p>28 • Hippocampal episodic conjunctive binding: Many theories of hippocampal function converge on the idea that it binds together individual elements of an experience into a unitary representation, which can for example be later recalled from partial cues (see O'Reilly & Rudy, 2001 for a review). [sent-50, score-0.181]
</p><p>29 These hippocampal conjunctive representations are higher-order and more spe-  cific than the lower-order coarse-coded cortical conjunctive representations (i. [sent-51, score-0.43]
</p><p>30 , a hippocampal conjunction encodes the combination of many feature elements, while a cortical conjunction encodes relatively few). [sent-53, score-0.19]
</p><p>31 Thus, the hippocampus can be seen as a specialized system for doing long-term binding of specific episodes, complementing the more generalized conjunctive binding performed by the cortex. [sent-54, score-0.865]
</p><p>32 Importantly, the hippocampus can also encode these conjunctions rapidly, and therefore it shares some of the same functionality as the phonological loop mechanism (i. [sent-55, score-0.942]
</p><p>33 Thus, it is likely that the hippocampus and the prefrontal-mediated working memory system (including the phonological loop) are partially redundant with each other, and work together in many tasks (Cohen & O'Reilly, 1996). [sent-58, score-0.728]
</p><p>34 2  Prefrontal Cortex and Basal Ganglia in Working Memory  Our model of the phonological loop takes advantage of recent work showing how the prefrontal cortex and basal ganglia can interact to support activation-based working memory (Frank et al. [sent-59, score-1.414]
</p><p>35 The critical principles behind this work are as follows: • Prefrontal cortex (PFC) is specialized relative to the posterior cortex for robust and rapidly updatable maintenance of information in an active state (i. [sent-61, score-0.496]
</p><p>36 • Robust maintenance and rapid updating are in fundamental conflict, and require a dynamic gating mechanism that can switch between these two modes of operation (O'Reilly et al. [sent-65, score-0.365]
</p><p>37 • The basal ganglia (BG) can provide this dynamic gating mechanism via modulatory, dis inhibitory connectivity with the PFC. [sent-68, score-0.567]
</p><p>38 Furthermore, this BG-based gating mechanism provides selectivity, such that separate regions of the PFC can be independently updated or allowed to perform robust maintenance. [sent-69, score-0.231]
</p><p>39 A possible anatomical substrate for these separably updatable PFC regions are the stripe structures identified by Levitt, Lewis, Yoshioka, and Lund (1993). [sent-70, score-0.169]
</p><p>40 • Active maintenance in the PFC is implemented via a combination of recurrent excitatory connections and intracellular excitatory ionic conductances. [sent-71, score-0.208]
</p><p>41 This allows the PFC units to generally reflect the current inputs, except when these units have their intracellular maintenance currents activated, which causes them to reflect previously maintained information. [sent-72, score-0.433]
</p><p>42 3  Phonological Loop Model  The above mechanisms motivated our modeling of the phonological loop as follows (see Figure 1) . [sent-75, score-0.609]
</p><p>43 First, separate PFC stripes are used to encode each step in the sequence. [sent-76, score-0.155]
</p><p>44 Thus, binding of phoneme identity and sequential order occurs in this model by using distinct neural substrates to represent the sequential information. [sent-77, score-0.621]
</p><p>45 This is entirely feasible because each stripe can represent all of the possible phonemes, given that they represent a closed class of items. [sent-78, score-0.131]
</p><p>46 Second, the storage of a  Figure 1: Phonological loop model. [sent-79, score-0.168]
</p><p>47 Ten different input symbols are possible at each time step (one unit out of ten activated in the Input layer) . [sent-80, score-0.165]
</p><p>48 A sequence is encoded in one pass by presenting the Input together with the sequential location in the Time input layer for each step in the sequence. [sent-81, score-0.296]
</p><p>49 Thus, the first stripe must learn to encode the first input, etc. [sent-83, score-0.237]
</p><p>50 Immediately after encoding, the network is then trained to produce the correct output in response to the time input, without any Input activation (the activation state shown is the network correctly recalling the third item in a sequence). [sent-84, score-0.271]
</p><p>51 The hidden layer must therefore learn to decode the context representations for this recall phase. [sent-85, score-0.242]
</p><p>52 new sequence involves the basal ganglia gating mechanism triggering updates of the different PFC stripes in the appropriate order. [sent-87, score-0.67]
</p><p>53 We assume this can be learned over experience, and we are currently working on developing powerful learning mechanisms for adapting the basal ganglia gating mechanism in this way. [sent-88, score-0.667]
</p><p>54 This kind of gating control would also likely require some kind of temporal/sequential input that indicates the location within the sequence - such information might come from the cerebellum (e. [sent-89, score-0.25]
</p><p>55 For the temporal signal indicating location within the sequence, we simply activated a different individual time unit for each point in the sequence (the Time input layer in Figure 1). [sent-93, score-0.313]
</p><p>56 This signal was then used by a simulated gating mechanism (implemented in script code in the simulator) to update the corresponding stripe in prefrontal cortex. [sent-94, score-0.578]
</p><p>57 Specifically, the stripe context layers had to learn to encode and maintain the current input value properly, and the Hidden layer had to be able to decode the context layer information as a function of the time input value. [sent-96, score-0.685]
</p><p>58 0  100  200 300 800 Number of Training Events  Figure 2: Generalization results for the phonological loop model as a function of number training patterns. [sent-111, score-0.609]
</p><p>59 Generalization is over 90% correct with training on less than 20% of the possible input patterns. [sent-112, score-0.065]
</p><p>60 Sequences (of length 3 for our initial work) were presented by sequentially activating an input "phoneme" and a corresponding sequential location input (in the Time input layer) . [sent-116, score-0.278]
</p><p>61 For example, the network could get Time = 0, Input = 2, then Time = 1, Input = 7, then Time = 2, Input = 3 to encode the sequence 2,7,3. [sent-118, score-0.203]
</p><p>62 During this encoding phase, the network was trained to activate the current Input on the Output layer, and the simulated gating function simply activated the intracellular maintenance currents for the units in the stripe in the Context (PFC) layer that corresponded to the Time input (i. [sent-119, score-0.898]
</p><p>63 Then, the network was trained to recall this sequence, during which t ime no Input activation was present. [sent-122, score-0.098]
</p><p>64 The network received the sequence of Time inputs (0,1,2), and was trained to produce the corresponding Output for that location in the sequence (e. [sent-123, score-0.151]
</p><p>65 The PFC context layers just maintained their activation states based on the intracellular ion currents activated during encoding (and recurrent activation) - once the network has been trained, the active PFC state represents the entire sequence. [sent-126, score-0.485]
</p><p>66 With 10 input phonemes and sequences of length t hree, there were 1,000 different sequences possible (we allowed phonemes to repeat). [sent-129, score-0.553]
</p><p>67 The generalization results are shown in Figure 2, which clearly shows that the network learned these sequences in a systematic manner and could transfer its training knowledge to novel sequences. [sent-131, score-0.205]
</p><p>68 Interestingly, there appears to be a critical transition between 100 and 200 training sequences - 100 sequences corresponds to each item within each slot being presented roughly 10 times, which appears to provide sufficient statistical information regarding the independence of individual slots. [sent-132, score-0.237]
</p><p>69 Figure 3: Hidden unit representations (values are weights into a hidden unit from all other layers). [sent-133, score-0.132]
</p><p>70 Unit in a) encodes the conjunction of a subset of input/output items at time 2. [sent-134, score-0.188]
</p><p>71 (b) encodes a different subset of items at time 2. [sent-135, score-0.188]
</p><p>72 (d) has no selectivity in the input, but does project to the output and likely participates in recall of items at time step 3. [sent-137, score-0.143]
</p><p>73 3  Analysis of Representations  To understand how the hidden units encode and retrieve information in the maintained context layer in a systematic fashion that supports the good generalization observed, we examined the patterns of learned weights. [sent-139, score-0.401]
</p><p>74 Here, we see evidence of coarse-coded representations that encode a subset of items in either one time point in the sequence or a couple of time points. [sent-141, score-0.371]
</p><p>75 These types of representations are consistent with our other work showing how these kinds of representations can support good generalization (O'Reilly & Busby, 2002). [sent-143, score-0.177]
</p><p>76 4  Discussion  We have presented a model of sequential encoding of phonemes, based on independently-motivated computational and biological considerations, focused on the neural substrates of the prefrontal cortex and basal ganglia (Frank et al. [sent-144, score-0.832]
</p><p>77 Viewed in more abstract, functional terms, however , our model is just another in a long line of computational models of how people might encode sequential order information. [sent-146, score-0.189]
</p><p>78 There are two classic models: (a) associative chaining, where the acti-  vation of a given item triggers the activation of the next item via associative links, and (b) item-position association models where items are associated with their sequential positions and recalled from position cues (e. [sent-147, score-0.542]
</p><p>79 The basic associative chaining model has been decisively ruled out based on error patterns (Henson, Norris, Page, & Baddeley, 1996), but modified versions of it may avoid these problems (e. [sent-150, score-0.095]
</p><p>80 Probably the most accomplished current model, Burgess and Hitch (1999), is a version of the itemposition association model with a competitive queuing mechanism where the most active item is output first and is then suppressed to allow other items to be output. [sent-153, score-0.355]
</p><p>81 Compared to these existing models, our model is unique in not requiring fast associational links to encode items within the sequence. [sent-154, score-0.249]
</p><p>82 For example, the Burgess and Hitch (1999) model uses rapid weight changes to associate items with a context representation that functions much like the time input in our model. [sent-155, score-0.258]
</p><p>83 In contrast, items are maintained strictly via persistent activation in our model , and the basalganglia based gating mechanism provides a means of encoding items into separate neural slots that implicitly represent sequential order. [sent-156, score-0.753]
</p><p>84 Thus, the time inputs act independently on the basal ganglia, which then operates generically on whatever phoneme information is presently activated in the auditory input, obviating the need for specific item-context links. [sent-157, score-0.267]
</p><p>85 The clear benefit of not requiring associationallinks is that it makes the model much more flexible and capable of generalization to novel sequences as we have demonstrated here (see O'Reilly & Munakata, 2000 for extended discussion of this general issue). [sent-158, score-0.208]
</p><p>86 Thus, we believe our model is uniquely well suited for explaining the role of the phonological loop in rapid binding of novel task information. [sent-159, score-1.013]
</p><p>87 Rodolfo Soto died tragically at a relatively young age during the preparation of this manuscript - this work is dedicated to his memory. [sent-164, score-0.071]
</p><p>88 Memory for serial order: A network model of the phonological loop and its timing. [sent-180, score-0.707]
</p><p>89 A computational approach to prefrontal cortex, cognitive control, and schizophrenia: Recent developments and current challenges. [sent-189, score-0.183]
</p><p>90 A preliminary theory of the interactions between prefrontal cortex and hippocampus that contribute to planning and prospective memory. [sent-196, score-0.401]
</p><p>91 Interactions between the frontal cortex and basal ganglia in working memory: A computational model. [sent-212, score-0.541]
</p><p>92 Unclaimed memory: Error patterns rule out chaining models of immediate serial recall. [sent-225, score-0.111]
</p><p>93 Order and position in primary memory for letter strings. [sent-236, score-0.081]
</p><p>94 Topography of pyramidal neuron intrinsic connections in macaque monkey prefrontal cortex (areas 9 & 46). [sent-246, score-0.288]
</p><p>95 The role of the phonological loop in executive control. [sent-270, score-0.609]
</p><p>96 ) , Mod els of working m emory: M echanisms of active maintenance and executiv e control. [sent-287, score-0.271]
</p><p>97 Three forms of binding and their neural substrates: Alternatives to temporal synchrony. [sent-304, score-0.331]
</p><p>98 Conjunctive representations in learning and memory: Principles of cortical and hippocampal function . [sent-320, score-0.168]
</p><p>99 The neural correlates of the verbal component of working memory. [sent-329, score-0.156]
</p><p>100 Context-sensitive coding, associative memory, and serial order in (speech) behavior. [sent-334, score-0.094]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('phonological', 0.441), ('binding', 0.331), ('pfc', 0.226), ('prefrontal', 0.183), ('ganglia', 0.179), ('soto', 0.169), ('loop', 0.168), ('phonemes', 0.163), ('basal', 0.157), ('items', 0.143), ('maintenance', 0.134), ('gating', 0.131), ('stripe', 0.131), ('cohen', 0.118), ('baddeley', 0.113), ('hitch', 0.113), ('miyake', 0.113), ('encode', 0.106), ('cortex', 0.105), ('mechanism', 0.1), ('working', 0.1), ('burgess', 0.098), ('conjunctive', 0.097), ('layer', 0.094), ('sequential', 0.083), ('busby', 0.082), ('substrates', 0.082), ('sequences', 0.081), ('memory', 0.081), ('hippocampus', 0.075), ('item', 0.075), ('intracellular', 0.074), ('preparation', 0.071), ('activated', 0.068), ('representations', 0.068), ('frank', 0.065), ('bindings', 0.065), ('input', 0.065), ('currents', 0.06), ('braver', 0.06), ('psychological', 0.056), ('articulatory', 0.056), ('chaining', 0.056), ('verbal', 0.056), ('serial', 0.055), ('units', 0.055), ('maintained', 0.055), ('activation', 0.055), ('sequence', 0.054), ('conjunctions', 0.052), ('hippocampal', 0.051), ('context', 0.05), ('munakata', 0.049), ('stripes', 0.049), ('cortical', 0.049), ('rapidly', 0.046), ('boulder', 0.046), ('flexible', 0.046), ('encodes', 0.045), ('encoding', 0.043), ('network', 0.043), ('phoneme', 0.042), ('red', 0.041), ('generalization', 0.041), ('novel', 0.04), ('participants', 0.039), ('green', 0.039), ('associative', 0.039), ('gathercole', 0.038), ('henson', 0.038), ('ivry', 0.038), ('levitt', 0.038), ('lewandowsky', 0.038), ('loughry', 0.038), ('murdock', 0.038), ('norris', 0.038), ('papagno', 0.038), ('paulesu', 0.038), ('prospective', 0.038), ('rodolfo', 0.038), ('rudy', 0.038), ('updatable', 0.038), ('active', 0.037), ('suppression', 0.037), ('task', 0.033), ('estes', 0.033), ('eview', 0.033), ('frackowiak', 0.033), ('frith', 0.033), ('lund', 0.033), ('recalled', 0.033), ('script', 0.033), ('ucb', 0.033), ('wickelgren', 0.033), ('yoshioka', 0.033), ('unit', 0.032), ('psychology', 0.031), ('specialized', 0.031), ('tasks', 0.031), ('decode', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="12-tfidf-1" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>Author: Randall C. O'Reilly, R. Soto</p><p>Abstract: We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training. 1</p><p>2 0.2729204 <a title="12-tfidf-2" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>3 0.11456999 <a title="12-tfidf-3" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>4 0.072506726 <a title="12-tfidf-4" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>Author: David Jacobs, Bas Rokers, Archisman Rudra, Zili Liu</p><p>Abstract: Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word fragments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the ﬂexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word fragments, in a manner similar to models of visual perceptual completion.</p><p>5 0.067014568 <a title="12-tfidf-5" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>Author: Michael C. Mozer, Michael D. Colagrosso, David E. Huber</p><p>Abstract: We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conﬂict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality—that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission—we obtain a good ﬁt to overall RT and error data, as well as trial-by-trial variations in performance. Consider the following scenario: While driving, you approach an intersection at which the trafﬁc light has already turned yellow, signaling that it is about to turn red. You also notice that a car is approaching you rapidly from behind, with no indication of slowing. Should you stop or speed through the intersection? The decision is difﬁcult due to the presence of two conﬂicting signals. Such response conﬂict can be produced in a psychological laboratory as well. For example, Stroop (1935) asked individuals to name the color of ink on which a word is printed. When the words are color names incongruous with the ink color— e.g., “blue” printed in red—reaction times are slower and error rates are higher. We are interested in the control mechanisms underlying performance of high-conﬂict tasks. Conﬂict requires individuals to monitor and adjust their behavior, possibly responding more slowly if errors are too frequent. In this paper, we model a speeded discrimination paradigm in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). The stimuli are letters of the alphabet, A–Z, presented in rapid succession. In a choice task, individuals are asked to press one response key if the letter is an X or another response key for any letter other than X (as a shorthand, we will refer to non-X stimuli as Y). In a go/no-go task, individuals are asked to press a response key when X is presented and to make no response otherwise. We address both tasks because they elicit slightly different decision-making behavior. In both tasks, Jones and Braver (2001) manipulated the relative frequency of the X and Y stimuli; the ratio of presentation frequency was either 17:83, 50:50, or 83:17. Response conﬂict arises when the two stimulus classes are unbalanced in frequency, resulting in more errors and slower reaction times. For example, when X’s are frequent but Y is presented, individuals are predisposed toward producing the X response, and this predisposition must be overcome by the perceptual evidence from the Y. Jones and Braver (2001) also performed an fMRI study of this task and found that anterior cingulate cortex (ACC) becomes activated in situations involving response conﬂict. Specifically, when one stimulus occurs infrequently relative to the other, event-related fMRI response in the ACC is greater for the low frequency stimulus. Jones and Braver also extended a neural network model of Botvinick, Braver, Barch, Carter, and Cohen (2001) to account for human performance in the two discrimination tasks. The heart of the model is a mechanism that monitors conﬂict—the posited role of the ACC—and adjusts response biases accordingly. In this paper, we develop a parsimonious alternative account of the role of the ACC and of how control processes modulate behavior when response conﬂict arises. 1 A RATIONAL ANALYSIS Our account is based on a rational analysis of human cognition, which views cognitive processes as being optimized with respect to certain task-related goals, and being adaptive to the structure of the environment (Anderson, 1990). We make three assumptions of rationality: (1) perceptual inference is optimal but is subject to rate limitations on information transmission, (2) response class prior probabilities are accurately estimated, and (3) the goal of individuals is to minimize a cost that depends both on error rate and reaction time. The heart of our account is an existing probabilistic model that explains a variety of facilitation effects that arise from long-term repetition priming (Colagrosso, in preparation; Mozer, Colagrosso, & Huber, 2000), and more broadly, that addresses changes in the nature of information transmission in neocortex due to experience. We give a brief overview of this model; the details are not essential for the present work. The model posits that neocortex can be characterized by a collection of informationprocessing pathways, and any act of cognition involves coordination among pathways. To model a simple discrimination task, we might suppose a perceptual pathway to map the visual input to a semantic representation, and a response pathway to map the semantic representation to a response. The choice and go/no-go tasks described earlier share a perceptual pathway, but require different response pathways. The model is framed in terms of probability theory: pathway inputs and outputs are random variables and microinference in a pathway is carried out by Bayesian belief revision.   To elaborate, consider a pathway whose input at time is a discrete random variable, denoted , which can assume values corresponding to alternative input states. Similarly, the output of the pathway at time is a discrete random variable, denoted , which can assume values . For example, the input to the perceptual pathway in the discrimination task is one of visual patterns corresponding to the letters of the alphabet, and the output is one of letter identities. (This model is highly abstract: the visual patterns are enumerated, but the actual pixel patterns are not explicitly represented in the model. Nonetheless, the similarity structure among inputs can be captured, but we skip a discussion of this issue because it is irrelevant for the current work.) To present a particular input alternative, , to the model for time steps, we clamp for . The model computes a probability distribution over given , i.e., P . ¡ # 4 0 ©2' &  0 ' ! 1)(</p><p>6 0.065838419 <a title="12-tfidf-6" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>7 0.06468457 <a title="12-tfidf-7" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>8 0.063258909 <a title="12-tfidf-8" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>9 0.061762758 <a title="12-tfidf-9" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>10 0.061210502 <a title="12-tfidf-10" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>11 0.060874995 <a title="12-tfidf-11" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>12 0.060429431 <a title="12-tfidf-12" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>13 0.056328282 <a title="12-tfidf-13" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>14 0.056046981 <a title="12-tfidf-14" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>15 0.052772924 <a title="12-tfidf-15" href="./nips-2001-Bayesian_Predictive_Profiles_With_Applications_to_Retail_Transaction_Data.html">41 nips-2001-Bayesian Predictive Profiles With Applications to Retail Transaction Data</a></p>
<p>16 0.04873484 <a title="12-tfidf-16" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>17 0.048651613 <a title="12-tfidf-17" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>18 0.048349518 <a title="12-tfidf-18" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>19 0.048277352 <a title="12-tfidf-19" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>20 0.048166011 <a title="12-tfidf-20" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.135), (1, -0.13), (2, -0.053), (3, 0.009), (4, -0.099), (5, 0.007), (6, -0.103), (7, -0.006), (8, -0.099), (9, 0.03), (10, -0.065), (11, 0.105), (12, 0.016), (13, 0.01), (14, 0.035), (15, 0.156), (16, 0.129), (17, -0.061), (18, 0.093), (19, -0.015), (20, 0.121), (21, -0.055), (22, -0.071), (23, 0.288), (24, -0.041), (25, 0.087), (26, 0.06), (27, 0.16), (28, 0.093), (29, -0.089), (30, -0.068), (31, -0.018), (32, -0.085), (33, 0.113), (34, -0.108), (35, 0.124), (36, 0.04), (37, 0.092), (38, -0.099), (39, 0.165), (40, 0.125), (41, 0.021), (42, -0.179), (43, 0.101), (44, -0.022), (45, 0.028), (46, 0.103), (47, -0.015), (48, -0.086), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96367824 <a title="12-lsi-1" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>Author: Randall C. O'Reilly, R. Soto</p><p>Abstract: We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training. 1</p><p>2 0.78345281 <a title="12-lsi-2" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>3 0.45679337 <a title="12-lsi-3" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>4 0.41848248 <a title="12-lsi-4" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>Author: Bram Bakker</p><p>Abstract: This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task. 1</p><p>5 0.36328724 <a title="12-lsi-5" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>Author: Michael C. Mozer, Michael D. Colagrosso, David E. Huber</p><p>Abstract: We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conﬂict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality—that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission—we obtain a good ﬁt to overall RT and error data, as well as trial-by-trial variations in performance. Consider the following scenario: While driving, you approach an intersection at which the trafﬁc light has already turned yellow, signaling that it is about to turn red. You also notice that a car is approaching you rapidly from behind, with no indication of slowing. Should you stop or speed through the intersection? The decision is difﬁcult due to the presence of two conﬂicting signals. Such response conﬂict can be produced in a psychological laboratory as well. For example, Stroop (1935) asked individuals to name the color of ink on which a word is printed. When the words are color names incongruous with the ink color— e.g., “blue” printed in red—reaction times are slower and error rates are higher. We are interested in the control mechanisms underlying performance of high-conﬂict tasks. Conﬂict requires individuals to monitor and adjust their behavior, possibly responding more slowly if errors are too frequent. In this paper, we model a speeded discrimination paradigm in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). The stimuli are letters of the alphabet, A–Z, presented in rapid succession. In a choice task, individuals are asked to press one response key if the letter is an X or another response key for any letter other than X (as a shorthand, we will refer to non-X stimuli as Y). In a go/no-go task, individuals are asked to press a response key when X is presented and to make no response otherwise. We address both tasks because they elicit slightly different decision-making behavior. In both tasks, Jones and Braver (2001) manipulated the relative frequency of the X and Y stimuli; the ratio of presentation frequency was either 17:83, 50:50, or 83:17. Response conﬂict arises when the two stimulus classes are unbalanced in frequency, resulting in more errors and slower reaction times. For example, when X’s are frequent but Y is presented, individuals are predisposed toward producing the X response, and this predisposition must be overcome by the perceptual evidence from the Y. Jones and Braver (2001) also performed an fMRI study of this task and found that anterior cingulate cortex (ACC) becomes activated in situations involving response conﬂict. Specifically, when one stimulus occurs infrequently relative to the other, event-related fMRI response in the ACC is greater for the low frequency stimulus. Jones and Braver also extended a neural network model of Botvinick, Braver, Barch, Carter, and Cohen (2001) to account for human performance in the two discrimination tasks. The heart of the model is a mechanism that monitors conﬂict—the posited role of the ACC—and adjusts response biases accordingly. In this paper, we develop a parsimonious alternative account of the role of the ACC and of how control processes modulate behavior when response conﬂict arises. 1 A RATIONAL ANALYSIS Our account is based on a rational analysis of human cognition, which views cognitive processes as being optimized with respect to certain task-related goals, and being adaptive to the structure of the environment (Anderson, 1990). We make three assumptions of rationality: (1) perceptual inference is optimal but is subject to rate limitations on information transmission, (2) response class prior probabilities are accurately estimated, and (3) the goal of individuals is to minimize a cost that depends both on error rate and reaction time. The heart of our account is an existing probabilistic model that explains a variety of facilitation effects that arise from long-term repetition priming (Colagrosso, in preparation; Mozer, Colagrosso, & Huber, 2000), and more broadly, that addresses changes in the nature of information transmission in neocortex due to experience. We give a brief overview of this model; the details are not essential for the present work. The model posits that neocortex can be characterized by a collection of informationprocessing pathways, and any act of cognition involves coordination among pathways. To model a simple discrimination task, we might suppose a perceptual pathway to map the visual input to a semantic representation, and a response pathway to map the semantic representation to a response. The choice and go/no-go tasks described earlier share a perceptual pathway, but require different response pathways. The model is framed in terms of probability theory: pathway inputs and outputs are random variables and microinference in a pathway is carried out by Bayesian belief revision.   To elaborate, consider a pathway whose input at time is a discrete random variable, denoted , which can assume values corresponding to alternative input states. Similarly, the output of the pathway at time is a discrete random variable, denoted , which can assume values . For example, the input to the perceptual pathway in the discrimination task is one of visual patterns corresponding to the letters of the alphabet, and the output is one of letter identities. (This model is highly abstract: the visual patterns are enumerated, but the actual pixel patterns are not explicitly represented in the model. Nonetheless, the similarity structure among inputs can be captured, but we skip a discussion of this issue because it is irrelevant for the current work.) To present a particular input alternative, , to the model for time steps, we clamp for . The model computes a probability distribution over given , i.e., P . ¡ # 4 0 ©2' &  0 ' ! 1)(</p><p>6 0.29773498 <a title="12-lsi-6" href="./nips-2001-Fragment_Completion_in_Humans_and_Machines.html">78 nips-2001-Fragment Completion in Humans and Machines</a></p>
<p>7 0.28791893 <a title="12-lsi-7" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>8 0.28005421 <a title="12-lsi-8" href="./nips-2001-3_state_neurons_for_contextual_processing.html">2 nips-2001-3 state neurons for contextual processing</a></p>
<p>9 0.27952108 <a title="12-lsi-9" href="./nips-2001-Orientational_and_Geometric_Determinants_of_Place_and_Head-direction.html">142 nips-2001-Orientational and Geometric Determinants of Place and Head-direction</a></p>
<p>10 0.27919549 <a title="12-lsi-10" href="./nips-2001-A_Bayesian_Model_Predicts_Human_Parse_Preference_and_Reading_Times_in_Sentence_Processing.html">5 nips-2001-A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing</a></p>
<p>11 0.27367973 <a title="12-lsi-11" href="./nips-2001-Constructing_Distributed_Representations_Using_Additive_Clustering.html">53 nips-2001-Constructing Distributed Representations Using Additive Clustering</a></p>
<p>12 0.25967759 <a title="12-lsi-12" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>13 0.2549035 <a title="12-lsi-13" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>14 0.25322428 <a title="12-lsi-14" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>15 0.2471568 <a title="12-lsi-15" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>16 0.24430181 <a title="12-lsi-16" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>17 0.23635015 <a title="12-lsi-17" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>18 0.23443609 <a title="12-lsi-18" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>19 0.2273341 <a title="12-lsi-19" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>20 0.22584218 <a title="12-lsi-20" href="./nips-2001-Learning_Hierarchical_Structures_with_Linear_Relational_Embedding.html">110 nips-2001-Learning Hierarchical Structures with Linear Relational Embedding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.01), (17, 0.02), (19, 0.024), (27, 0.076), (30, 0.073), (38, 0.122), (56, 0.329), (59, 0.033), (72, 0.028), (79, 0.068), (91, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81906354 <a title="12-lda-1" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>Author: Randall C. O'Reilly, R. Soto</p><p>Abstract: We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training. 1</p><p>2 0.53044641 <a title="12-lda-2" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>Author: Julian Eggert, Berthold BĂ¤uml</p><p>Abstract: Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate- and- Fire type neurons , these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate- and- Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent. 1 Introduction and motivation The goal of the population dynamics approach is to model the time course of the collective activity of entire populations of functionally and dynamically similar neurons in a compact way, using a higher descriptionallevel than that of single neurons and spikes. The usual observable at the level of neuronal populations is the populationaveraged instantaneous firing rate A(t), with A(t)6.t being the number of neurons in the population that release a spike in an interval [t, t+6.t). Population dynamics are formulated in such a way, that they match quantitatively the time course of a given A(t), either gained experimentally or by microscopical, detailed simulation. At least three main reasons can be formulated which underline the importance of the population dynamics approach for computational neuroscience. First, it enables the simulation of extensive networks involving a massive number of neurons and connections, which is typically the case when dealing with biologically realistic functional models that go beyond the single neuron level. Second, it increases the analytical understanding of large-scale neuronal dynamics , opening the way towards better control and predictive capabilities when dealing with large networks. Third, it enables a systematic embedding of the numerous neuronal models operating at different descriptional scales into a generalized theoretic framework, explaining the relationships, dependencies and derivations of the respective models. Early efforts on population dynamics approaches date back as early as 1972, to the work of Wilson and Cowan [8] and Knight [4], which laid the basis for all current population-averaged graded-response models (see e.g. [6] for modeling work using these models). More recently, population-based approaches for spiking neurons were developed, mainly by Gerstner [3, 2] and Knight [5]. In our own previous work [1], we have developed a theoretical framework which enables to systematize and simulate a wide range of models for population-based dynamics. It was shown that the equations of the framework produce results that agree quantitatively well with detailed simulations using spiking neurons, so that they can be used for realistic simulations involving networks with large numbers of spiking neurons. Nevertheless, for neuronal populations composed of Integrate-and-Fire (I&F;) neurons, this framework was only correct in an approximation. In this paper, we derive the exact population dynamics formulation for I&F; neurons. This is achieved by reducing the I&F; population dynamics to a point process and by taking advantage of the particular properties of I&F; neurons. 2 2.1 Background: Integrate-and-Fire dynamics Differential form We start with the standard Integrate- and- Fire (I&F;) model in form of the wellknown differential equation [7] (1) which describes the dynamics of the membrane potential Vi of a neuron i that is modeled as a single compartment with RC circuit characteristics. The membrane relaxation time is in this case T = RC with R being the membrane resistance and C the membrane capacitance. The resting potential v R est is the stationary potential that is approached in the no-input case. The input arriving from other neurons is described in form of a current ji. In addition to eq. (1), which describes the integrate part of the I&F; model, the neuronal dynamics are completed by a nonlinear step. Every time the membrane potential Vi reaches a fixed threshold () from below, Vi is lowered by a fixed amount Ll > 0, and from the new value of the membrane potential integration according to eq. (1) starts again. if Vi(t) = () (from below) . (2) At the same time, it is said that the release of a spike occurred (i.e., the neuron fired), and the time ti = t of this singular event is stored. Here ti indicates the time of the most recent spike. Storing all the last firing times , we gain the sequence of spikes {t{} (spike ordering index j, neuronal index i). 2.2 Integral form Now we look at the single neuron in a neuronal compound. We assume that the input current contribution ji from presynaptic spiking neurons can be described using the presynaptic spike times tf, a response-function ~ and a connection weight WÂˇ . ',J ji(t) = Wi ,j ~(t - tf) (3) l: l: j f Integrating the I&F; equation (1) beginning at the last spiking time tT, which determines the initial condition by Vi(ti) = vi(ti - 0) - 6., where vi(ti - 0) is the membrane potential just before the neuron spikes, we get 1 Vi(t) = v Rest + fj(t - t:) + l: Wi ,j l: a(t - t:; t - tf) , j - Vi(t:)) e- S / T (4) f with the refractory function fj(s) = - (v Rest (5) and the alpha-function r ds</p><p>3 0.5148347 <a title="12-lda-3" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>Author: Lance R. Williams, John W. Zweck</p><p>Abstract: We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the input to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well deﬁned function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern.</p><p>4 0.5033083 <a title="12-lda-4" href="./nips-2001-Generalizable_Relational_Binding_from_Coarse-coded_Distributed_Representations.html">80 nips-2001-Generalizable Relational Binding from Coarse-coded Distributed Representations</a></p>
<p>Author: Randall C. O'Reilly, R. S. Busby</p><p>Abstract: We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efﬁciency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly speciﬁc and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.</p><p>5 0.49084312 <a title="12-lda-5" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>Author: Gregor Wenning, Klaus Obermayer</p><p>Abstract: Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model. 1</p><p>6 0.48742646 <a title="12-lda-6" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>7 0.47717527 <a title="12-lda-7" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>8 0.47356939 <a title="12-lda-8" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>9 0.47192156 <a title="12-lda-9" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>10 0.47158039 <a title="12-lda-10" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>11 0.46824273 <a title="12-lda-11" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>12 0.46652436 <a title="12-lda-12" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>13 0.46572787 <a title="12-lda-13" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>14 0.46442711 <a title="12-lda-14" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>15 0.46384037 <a title="12-lda-15" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>16 0.46244347 <a title="12-lda-16" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>17 0.46140695 <a title="12-lda-17" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>18 0.45878118 <a title="12-lda-18" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>19 0.45539287 <a title="12-lda-19" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>20 0.45326167 <a title="12-lda-20" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
