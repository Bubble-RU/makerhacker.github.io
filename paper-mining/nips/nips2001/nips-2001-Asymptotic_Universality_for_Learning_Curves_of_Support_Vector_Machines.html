<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-38" href="#">nips2001-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2001-38-pdf" href="http://papers.nips.cc/paper/2123-asymptotic-universality-for-learning-curves-of-support-vector-machines.pdf">pdf</a></p><p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>Reference: <a title="nips-2001-38-reference" href="../nips2001_reference/nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). [sent-9, score-0.109]
</p><p>2 We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. [sent-10, score-1.038]
</p><p>3 Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. [sent-11, score-0.514]
</p><p>4 The price one has to pay for the flexibility of these models is the need to choose the proper model complexity for a given task, i. [sent-13, score-0.136]
</p><p>5 the system architecture which gives good generalization ability for novel data. [sent-15, score-0.15]
</p><p>6 A common argument in computational learning theory suggests that it is dangerous to utilize the full flexibility of the SVM to learn the training data perfectly when these contain an amount of noise. [sent-18, score-0.204]
</p><p>7 Rence, modifications of SVM training [2] that allow for training errors were suggested to be necessary for realistic noisy scenarios. [sent-21, score-0.293]
</p><p>8 Using methods of Statistical Physics, we show that asymptotically, the SVM achieves optimal generalization ability for noisy data already for zero training error. [sent-24, score-0.352]
</p><p>9 Moreover, the asymptotic rate of decay of the generalization error is universal, i. [sent-25, score-0.373]
</p><p>10 As is well known, SVMs classify inputs y using a nonlinear mapping into a feature vector w(y) which is an element of a Hilbert space. [sent-29, score-0.148]
</p><p>11 Based on a training set of m inputs xl' and their desired classifications 71' , SVMs construct the maximum margin hyperplane P in the feature space. [sent-30, score-0.411]
</p><p>12 P can be expressed as a linear combination of the feature vectors w(xl'), and to classify an input y, that is to decide on which side of P the image W(y) lies, one basically has to evaluate inner products W(xl') . [sent-31, score-0.257]
</p><p>13 For carefully chosen mappings wand Hilbert spaces, inner products w(x) . [sent-33, score-0.109]
</p><p>14 w(y) can be evaluated efficiently using a kernel function k(x, y) = w(x) . [sent-34, score-0.257]
</p><p>15 In this manner it becomes computationally feasible to use very high and even infinite dimensional feature vectors. [sent-36, score-0.202]
</p><p>16 This raises the intriguing question whether the use of a very high dimensional feature space may typically be helpful. [sent-37, score-0.136]
</p><p>17 They suggest (as one might perhaps expect) that it is rather important to match the complexity of the kernel to the target rule. [sent-39, score-0.475]
</p><p>18 The analysis in [4] considers the case of N-dimensional inputs with binary components and assumes that the target rule giving the correct classification 7 of an input x is obtained as the sign of a function t(x) which is polynomial in the input components and of degree L. [sent-40, score-0.985]
</p><p>19 The SVM uses a kernel which is a polynomial of the inner product x . [sent-41, score-0.613]
</p><p>20 y in input space of degree K ;::: L, and the feature space dimension is thus O(N K ). [sent-42, score-0.254]
</p><p>21 In this scenario it is shown, under mild regularity condition on the kernel and for large N, that the SVM generalizes well when the number of training examples m is on the order of N L . [sent-43, score-0.503]
</p><p>22 So the scale of the learning curve is determined by the complexity of the target rule and not by the kernel. [sent-44, score-0.421]
</p><p>23 However, considering the rate with which the generalization error approaches zero one finds the optimal N L 1m decay only when K is equal to L and the convergence is substantially slower when K > L. [sent-45, score-0.412]
</p><p>24 So it is important to match the complexity of the kernel to the target rule and using a large value of K is only justified if L is assumed large and if one can use on the order of N L examples for training. [sent-46, score-0.619]
</p><p>25 In this paper we show that the situation is very different when one considers the arguably more realistic scenario of a target rule corrupted by noise. [sent-47, score-0.552]
</p><p>26 Now one can no longer use K = L since no separating hyperplane P will exist when m is sufficiently large compared to N L. [sent-48, score-0.107]
</p><p>27 However when K > L, this plane will exist and we will show that it achieves optimal generalization performance in the limit that N L 1m is small. [sent-49, score-0.258]
</p><p>28 Remarkably, the asymptotic rate of decay of the generalization error is independent of the kernel in this case and a general characterization of the asymptote in terms of properties of the target rule is possible. [sent-50, score-1.131]
</p><p>29 In a second step we show that under mild regularity conditions these findings also hold when k(x, y) is an arbitrary function of x . [sent-51, score-0.196]
</p><p>30 y or when the kernel is a function of the Euclidean distance Ix - YI. [sent-52, score-0.257]
</p><p>31 The latter type of kernels is widely used in practical applications of SVMs. [sent-53, score-0.172]
</p><p>32 2  Learning with Noise: Polynomial Kernels  We begin by assuming a polynomial kernel k(x, y) = J(x· y) where J(z) = Lf= Ck zk is of degree K. [sent-54, score-0.602]
</p><p>33 r and the degree of xp is Ipi = Lf:l Pi·  The kernel can then be described by features wp(x) = JCiPTxp since k(x,y) = Lp wp(x)wp(y), where the summation runs over all multi-indices of degree up to K. [sent-59, score-0.571]
</p><p>34 To assure that the features are real, we assume that the coefficients Ck in the kernel are nonnegative. [sent-60, score-0.387]
</p><p>35 A hyperplane in feature space is parameterized by a weight vector w with components w p, and if 0 < TI'W . [sent-61, score-0.257]
</p><p>36 W(xl'), a point (xl', TI') of the training set lies on the correct side of the plane. [sent-62, score-0.092]
</p><p>37 To express that the plane P has maximal distance to the points of the training set, we choose an arbitrary positive stability parameter /'i, and require that the weight vector w* of P minimize w . [sent-63, score-0.092]
</p><p>38 In this approach, one computes a partition function which serves as a generating function for important averages such as the generalization error. [sent-71, score-0.229]
</p><p>39 One considers the partition function  z=  f dwe- ~f3w. [sent-73, score-0.145]
</p><p>40 Properties of w * can be computed from In Z and taking the limit f3 -t 00. [sent-75, score-0.108]
</p><p>41 To model the training data, we assume that the random and independent input components have zero mean and variance liN . [sent-76, score-0.27]
</p><p>42 For the target rule we assume that its deterministic part is given by the polynomial t(x) = Lp JCiPTBpxp with real parameters Bp. [sent-79, score-0.526]
</p><p>43 The magnitude of the contribution of each degree k to the value of t(x) is measured by the quantities 1 Tk = Ck Nk  '"' Bp 2 ~  (2)  p,lpl =k where Nk = (N+; - I) is the number of terms in the sum. [sent-80, score-0.106]
</p><p>44 The degree of t(x) is L and lower than K, so TL > 0 and TL+l = . [sent-81, score-0.106]
</p><p>45 Here we consider a nondeterministic rule and the output label is obtained using a random variable Tu E {-1, 1} parameterized by a scalar u. [sent-89, score-0.144]
</p><p>46 In general, we will assume that the noise does not systematically corrupt the deterministic component, formally 1  (3)  1> Prob(Tu = sgn(u)) >"2 for all u. [sent-91, score-0.169]
</p><p>47 So sgn( t( x)) is the best possible prediction of the output label of x, and the minimal achievable generalization error is fmin = (8( -t(X)Tt(x)))x. [sent-92, score-0.418]
</p><p>48 In the limit of many input dimensions N, a central limit argument yields that for a typical target rule fmin = 2(8( -u)0(u))u , where u is zero mean and unit variance Gaussian. [sent-93, score-0.87]
</p><p>49 1) over random drawings of training data for large N in terms of t he order parameters  Q r  (((W. [sent-98, score-0.092]
</p><p>50 For the large N limit, a scaling of the training set size m must be specified, for which we make  t he generic Ansatz m = aNt, where I = 1, . [sent-102, score-0.092]
</p><p>51 Focusing on the limit of large j3, where the density on the weight vectors converges to a delta peak and q -+ Q, we introduce the rescaled order parameter X = j3( Q - q) / St, with t  St = i (1) -  L  Ci . [sent-106, score-0.108]
</p><p>52 (5)  i=O  Note that this scaling with St is only possible since the degree K of the kernel i(x, y) is greater than I, and thus St ¥- O. [sent-107, score-0.363]
</p><p>53 Finally, we obtain an expression for it = lim,B--+oo limN --+00 «In Z)) St / (j3Nt ), where the double brackets denote averaging over all training sets of size m . [sent-108, score-0.092]
</p><p>54 So the higher order components of w* are small, (W;)2 « 1 for Ipi > I, although these components playa crucial role in ensuring that a hyperplane separating the training points exists even for large a. [sent-112, score-0.333]
</p><p>55 (6) is the stationary value of r which determines the generalization error of the SVM via fg = (0(-u)8(ru + ~v))u,v, and in particular fg = fmin for r = 1. [sent-114, score-0.469]
</p><p>56 4  Universal Asymptotics  We now specialize to the case that l equals L, the degree of the polynomial t(x) in the target rule. [sent-116, score-0.446]
</p><p>57 00,  one may show that  Equation (7) shows that optimal generalization performance is achieved on this scale in the limit of large a. [sent-120, score-0.258]
</p><p>58 Remarkably, as long as K > L, the asymptote is invariant to the choice of the kernel since A(q) and B(q) are defined solely in terms of the target rule. [sent-121, score-0.615]
</p><p>59 3  Extension to other Kernels  Our next goal is to understand cases where the kernel is a general function of the inner product or of the distance between the vectors. [sent-122, score-0.425]
</p><p>60 We still assume that the target rule is of finite complexity, i. [sent-123, score-0.354]
</p><p>61 defined by a polynomial and corrupted by noise and that the number of examples is polynomial in N. [sent-125, score-0.597]
</p><p>62 Remarkably, the more general kernels then reduce to the polynomial case in the thermodynamic limit. [sent-126, score-0.465]
</p><p>63 w* = 2:;=1 AM and for a polynomial kernel we thus obtain a bound on 2:;=1AM since w* . [sent-130, score-0.445]
</p><p>64 We first consider kernels ¢(x· y), with a general continuous function ¢ of the inner product, and assume that ¢ can be approximated by a polynomial J in the sense that ¢(1) = J(l) and ¢(z) - J(z) = O(ZK) for z --+ O. [sent-132, score-0.469]
</p><p>65 Now, with a probability approaching 1 with increasing N, the magnitude of xM·xl/ is smaller than, say, N- 1/3 for all different indices {t and v as long as m is polynomial in N. [sent-133, score-0.188]
</p><p>66 (8), for large N the functions ¢(z) and J(z) will only be evaluated in a small region around zero and at z = 1 when used as kernels of a SVM trained on m = aNL examples. [sent-135, score-0.218]
</p><p>67 (6,7) can be used to calculate the generalization error for ¢ by setting ttl = ¢(l) (O)/l! [sent-138, score-0.294]
</p><p>68 Note that results in [4] assure that ttl ::::: 0 if the kernel ¢( X· y) is positive definite for all input dimensions N. [sent-143, score-0.58]
</p><p>69 Further, the same reduction to the polynomial case will hold in many instances where ¢ is not analytical but just sufficiently smooth close to O. [sent-144, score-0.235]
</p><p>70 1  RBF Kernels  We next turn to radial basis function (RBF) kernels where k( x, y) depends only on the Euclidean distance between two inputs, k(x,y) =  (lx - YI2). [sent-146, score-0.172]
</p><p>71 For binary input components (Xi = ±N- 1 / 2 ) this is just the inner product case since  (lx Y12) =  (2 - 2x· y). [sent-147, score-0.3]
</p><p>72 Gaussian input components, the fluctuations of Ixl around its mean value 1 have the same magnitude as x . [sent-150, score-0.116]
</p><p>73 y even for large N, and an equivalence with inner product kernels is not evident. [sent-151, score-0.34]
</p><p>74 Our starting point is the observation that any kernel  (lx - Y12) which is positive definite for all input dimensions N is a positive mixture of Gaussians [6]. [sent-152, score-0.418]
</p><p>75 For the special case of a single Gaussian one easily obtains features 'IT p by rewriting x1  (lx - Y12) = e-lx-v I2/2 = e- 1 2/2ex've-lvI2 /2. [sent-154, score-0.092]
</p><p>76 Expanding the kernel e X ' v into polynomial features, yields the features 'IT p(x) = e- 1x12 /2x pl for  (lx _ YI2). [sent-155, score-0.494]
</p><p>77 But, for a generic weight vector w in feature space,  M  w· 'IT(x) = ~Wp'ITp(x) = e-~lxI2 ~wp  M  (9)  is of order 1, and thus for large N the fluctuations of Ixl can be neglected. [sent-156, score-0.134]
</p><p>78 This line of argument can be extended to the case that the kernel is a finite mixture of Gaussians,  (z) = L~=l aie-'Y7z /2 with positive coefficients ai. [sent-157, score-0.357]
</p><p>79 Applying the reasoning for a single Gaussian to each term in the sum, one obtains a doubly indexed feature vector with components 'lTp,i(X) = e-'Y7IxI2/2(ai/';lpl/lpll)1/2xp. [sent-158, score-0.263]
</p><p>80 It is then straightforward to adapt the calculation of the partition function (Eq. [sent-159, score-0.126]
</p><p>81 16) to the doubly indexed features, showing that the kernel  (lx - Y12) yields the same generalization behavior as the inner product kernel   (2 - 2x . [sent-160, score-0.902]
</p><p>82 an infinite mixture of Gaussians, even if it would be involved to prove that the limit of many Gaussians commutes with the large N limit. [sent-164, score-0.174]
</p><p>83 4  Simulations  To illustrate the general results we first consider a scenario where a linear target rule, corrupted by additive Gaussian noise, is learned using different transcendental RBF kernels (Fig. [sent-165, score-0.625]
</p><p>84 (7) predicts that the asymptote of the generalization error does not depend on the kernel, remarkably, the dependence on the kernel is very weak for all values of a. [sent-168, score-0.632]
</p><p>85 In contrast, the generalization error depends substantially on the nature of the noise process. [sent-169, score-0.34]
</p><p>86 Figure 2 shows the finding for a quadratic rule with additive noise for the cases that the noise is Gaussian and binary. [sent-170, score-0.473]
</p><p>87 In the Gaussian case a 1/a decay of Eg to Emin is found, whereas for binary noise the decay is exponential in a. [sent-171, score-0.347]
</p><p>88 5  Summary  The general characterization of learning curves obtained in this paper demonstrates that support vector machines with high order or even transcendental kernels have definitive advantages when the training data is noisy. [sent-173, score-0.497]
</p><p>89 (6) show that maximizing the margin is an essential ingredient of the  approach: If one just chooses any hyperplane which classifies the training data correctly, the scale of the learning curve is not determined by the target rule and far more examples are needed to achieve good generalization. [sent-175, score-0.66]
</p><p>90 Since we have considered hard margins, in contrast to t he generalization error, the training error is zero, and we find no convergence between the two quantities. [sent-177, score-0.305]
</p><p>91 But close to optimal generalization is achieved since maximizing the margin biases the SVM to explain as much as possible of the data in terms of a low order polynomial. [sent-178, score-0.256]
</p><p>92 While the Statistical Physics approach used in this paper is only exactly valid in the thermodynamic limit, the numerical simulations indicate that the theory is already a good approximation for a realistic number of input dimensions. [sent-179, score-0.267]
</p><p>93 1 - trllin -  o  -  -  -  -  -  10  5  20  15  a=P/N Figure 1: Linear target rule corrupted by additive Gaussian noise rJ ((rJ) = 0, \rJ 2 ) = 1/16) and learned using different kernels. [sent-225, score-0.592]
</p><p>94 The curves are the theoretical prediction; symbols show simulation results for N = 600 with Gaussian inputs and error bars are approximately the size of the symbols. [sent-226, score-0.194]
</p><p>95 (B) Wiener kernel given by the non analytic function  (z) = e - e. [sent-228, score-0.308]
</p><p>96 (C) Gaussian kernel with k = 1/20, the influence of the parameter change on t he learning curve is minimal. [sent-233, score-0.316]
</p><p>97 5 vanishing training error cannot be achieved and the SVM is undefined. [sent-236, score-0.155]
</p><p>98 1 -  -E~in-  -  o w-______ o 2  -  -  -  -  -  -  -  -  -  -  -  ~______~_ _ _ _ _~_____ _ w  4  6  8  a = P/N2  Figure 2: A noisy quadratic rule (Tl = 0, T2 = 1) learned using the Gaussian kernel with k = 1/20. [sent-239, score-0.465]
</p><p>99 20 so that the value of fmin is the same for the two noise processes. [sent-246, score-0.289]
</p><p>100 The inset shows that f9 decays as l/a for Gaussian noise, whereas an exponential decay is found in the binary case. [sent-247, score-0.11]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernel', 0.257), ('polynomial', 0.188), ('kernels', 0.172), ('asymptote', 0.162), ('fmin', 0.162), ('sgn', 0.162), ('lx', 0.16), ('svm', 0.156), ('target', 0.152), ('generalization', 0.15), ('rule', 0.144), ('wp', 0.128), ('noise', 0.127), ('xl', 0.113), ('decay', 0.11), ('inner', 0.109), ('limit', 0.108), ('hyperplane', 0.107), ('degree', 0.106), ('thermodynamic', 0.105), ('svms', 0.103), ('rj', 0.098), ('corrupted', 0.094), ('remarkably', 0.094), ('training', 0.092), ('tu', 0.089), ('tt', 0.088), ('feature', 0.083), ('st', 0.082), ('physics', 0.082), ('anl', 0.081), ('assure', 0.081), ('assures', 0.081), ('asymptotics', 0.081), ('transcendental', 0.081), ('ttl', 0.081), ('wurzburg', 0.081), ('ck', 0.08), ('partition', 0.079), ('additive', 0.075), ('tk', 0.074), ('gaussian', 0.071), ('universal', 0.071), ('mechanics', 0.071), ('ixl', 0.07), ('dietrich', 0.07), ('doubly', 0.07), ('flexibility', 0.07), ('nk', 0.07), ('tl', 0.068), ('components', 0.067), ('complexity', 0.066), ('hilbert', 0.066), ('infinite', 0.066), ('considers', 0.066), ('curves', 0.066), ('inputs', 0.065), ('input', 0.065), ('ipi', 0.064), ('ru', 0.064), ('margin', 0.064), ('noisy', 0.064), ('error', 0.063), ('rbf', 0.061), ('curve', 0.059), ('product', 0.059), ('gaussians', 0.058), ('finite', 0.058), ('regularity', 0.056), ('qa', 0.056), ('lf', 0.056), ('xp', 0.053), ('definite', 0.053), ('dimensional', 0.053), ('simulations', 0.052), ('scenario', 0.051), ('analytic', 0.051), ('fluctuations', 0.051), ('zk', 0.051), ('asymptotic', 0.05), ('opper', 0.049), ('features', 0.049), ('hold', 0.047), ('fg', 0.047), ('mild', 0.047), ('calculation', 0.047), ('zero', 0.046), ('findings', 0.046), ('realistic', 0.045), ('pi', 0.045), ('invariant', 0.044), ('dimensions', 0.043), ('characterization', 0.043), ('finds', 0.043), ('obtains', 0.043), ('prediction', 0.043), ('machines', 0.043), ('argument', 0.042), ('deterministic', 0.042), ('maximizing', 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="38-tfidf-1" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>2 0.20284101 <a title="38-tfidf-2" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>3 0.1976894 <a title="38-tfidf-3" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>4 0.18793483 <a title="38-tfidf-4" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>5 0.18647076 <a title="38-tfidf-5" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>6 0.16666235 <a title="38-tfidf-6" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>7 0.15252838 <a title="38-tfidf-7" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>8 0.14930472 <a title="38-tfidf-8" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>9 0.1483357 <a title="38-tfidf-9" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>10 0.14309511 <a title="38-tfidf-10" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>11 0.14192936 <a title="38-tfidf-11" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>12 0.14075121 <a title="38-tfidf-12" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>13 0.1334165 <a title="38-tfidf-13" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>14 0.13106875 <a title="38-tfidf-14" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>15 0.12854536 <a title="38-tfidf-15" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>16 0.12287199 <a title="38-tfidf-16" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>17 0.1227888 <a title="38-tfidf-17" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>18 0.12050248 <a title="38-tfidf-18" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>19 0.11589137 <a title="38-tfidf-19" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>20 0.11567052 <a title="38-tfidf-20" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.363), (1, 0.161), (2, -0.074), (3, -0.038), (4, 0.074), (5, 0.165), (6, 0.121), (7, -0.032), (8, -0.06), (9, 0.104), (10, -0.078), (11, 0.036), (12, 0.101), (13, -0.137), (14, 0.091), (15, 0.182), (16, 0.037), (17, 0.08), (18, -0.041), (19, -0.002), (20, -0.055), (21, -0.083), (22, -0.002), (23, -0.006), (24, 0.091), (25, 0.018), (26, 0.106), (27, 0.023), (28, -0.044), (29, 0.077), (30, 0.029), (31, -0.037), (32, 0.03), (33, 0.063), (34, 0.043), (35, 0.007), (36, -0.025), (37, -0.017), (38, 0.07), (39, -0.0), (40, 0.021), (41, 0.065), (42, -0.057), (43, -0.024), (44, 0.039), (45, -0.051), (46, 0.008), (47, 0.011), (48, -0.013), (49, -0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97446066 <a title="38-lsi-1" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>2 0.70948726 <a title="38-lsi-2" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Bernhard Schćž&scaron;lkopf</p><p>Abstract: The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice. 1</p><p>3 0.69876564 <a title="38-lsi-3" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>4 0.67552942 <a title="38-lsi-4" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>Author: Dörthe Malzahn, Manfred Opper</p><p>Abstract: We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.</p><p>5 0.66802055 <a title="38-lsi-5" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>Author: Koji Tsuda, Motoaki Kawanabe, Gunnar Rätsch, Sören Sonnenburg, Klaus-Robert Müller</p><p>Abstract: Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called</p><p>6 0.6450268 <a title="38-lsi-6" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>7 0.63267154 <a title="38-lsi-7" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>8 0.61199707 <a title="38-lsi-8" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>9 0.60103178 <a title="38-lsi-9" href="./nips-2001-Gaussian_Process_Regression_with_Mismatched_Models.html">79 nips-2001-Gaussian Process Regression with Mismatched Models</a></p>
<p>10 0.60024309 <a title="38-lsi-10" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>11 0.58316493 <a title="38-lsi-11" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>12 0.57684016 <a title="38-lsi-12" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>13 0.5766995 <a title="38-lsi-13" href="./nips-2001-Fast_Parameter_Estimation_Using_Green%27s_Functions.html">76 nips-2001-Fast Parameter Estimation Using Green's Functions</a></p>
<p>14 0.55874789 <a title="38-lsi-14" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>15 0.5546912 <a title="38-lsi-15" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>16 0.55108571 <a title="38-lsi-16" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>17 0.53877103 <a title="38-lsi-17" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>18 0.53290862 <a title="38-lsi-18" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>19 0.52655506 <a title="38-lsi-19" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>20 0.51740021 <a title="38-lsi-20" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.32), (17, 0.025), (19, 0.023), (27, 0.121), (30, 0.052), (36, 0.012), (38, 0.02), (59, 0.029), (72, 0.142), (78, 0.01), (79, 0.045), (83, 0.015), (91, 0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95777589 <a title="38-lda-1" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>2 0.95119804 <a title="38-lda-2" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>3 0.94580543 <a title="38-lda-3" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>same-paper 4 0.9272086 <a title="38-lda-4" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>5 0.89893055 <a title="38-lda-5" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>6 0.75498199 <a title="38-lda-6" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>7 0.74720001 <a title="38-lda-7" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>8 0.74518371 <a title="38-lda-8" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>9 0.73604435 <a title="38-lda-9" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>10 0.73153609 <a title="38-lda-10" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>11 0.73108435 <a title="38-lda-11" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>12 0.72976071 <a title="38-lda-12" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>13 0.72557056 <a title="38-lda-13" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>14 0.72249222 <a title="38-lda-14" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>15 0.71247572 <a title="38-lda-15" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>16 0.70254815 <a title="38-lda-16" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>17 0.69727063 <a title="38-lda-17" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>18 0.69429433 <a title="38-lda-18" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>19 0.69330269 <a title="38-lda-19" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>20 0.69007921 <a title="38-lda-20" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
