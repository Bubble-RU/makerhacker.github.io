<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 nips-2001-Spectral Kernel Methods for Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-170" href="#">nips2001-170</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>170 nips-2001-Spectral Kernel Methods for Clustering</h1>
<br/><p>Source: <a title="nips-2001-170-pdf" href="http://papers.nips.cc/paper/2002-spectral-kernel-methods-for-clustering.pdf">pdf</a></p><p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>Reference: <a title="nips-2001-170-reference" href="../nips2001_reference/nips-2001-Spectral_Kernel_Methods_for_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. [sent-5, score-0.382]
</p><p>2 All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. [sent-6, score-0.308]
</p><p>3 We use two different but related cost functions, the Alignment and the 'cut cost'. [sent-7, score-0.16]
</p><p>4 The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. [sent-8, score-0.167]
</p><p>5 Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. [sent-9, score-0.226]
</p><p>6 We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. [sent-10, score-0.282]
</p><p>7 We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. [sent-11, score-0.31]
</p><p>8 A general algorithm can be selected for the appropriate task before being mapped onto a particular application through the choice of a problem specific kernel function. [sent-14, score-0.302]
</p><p>9 The kernel based method works by mapping data to a high dimensional feature space implicitly defined by the choice of the kernel function. [sent-15, score-0.721]
</p><p>10 The kernel function computes the inner product of the images of two inputs in the feature space. [sent-16, score-0.376]
</p><p>11 From a practitioners viewpoint this function can also be regarded as a similarity measure and hence provides a natural way of incorporating domain knowledge about the problem into the bias of the system. [sent-17, score-0.143]
</p><p>12 One important learning problem is that of dividing the data into classes according to a cost function together with their relative positions in the feature space. [sent-18, score-0.27]
</p><p>13 We can think of this as clustering in the kernel defined feature space, or non-linear clustering in the input space. [sent-19, score-0.61]
</p><p>14 They both assume that a kernel has been chosen and the kernel matrix constructed. [sent-21, score-0.711]
</p><p>15 The methods then make use of the matrix's eigenvectors, or of the eigenvectors of the closely related Laplacian matrix, in order to infer a label assignment that approximately optimizes one of two cost functions . [sent-22, score-0.451]
</p><p>16 See also [4] for use of spectral decompositions of the kernel matrix. [sent-23, score-0.364]
</p><p>17 2  Two partition cost measures  All the information needed to specify a clustering of a set of data is contained in the matrix Mij = (cluster(xi) == cluster(xj)), where (A == B) E {-I, +1}. [sent-25, score-0.451]
</p><p>18 After a clustering is specified, one can measure its cost in many ways. [sent-26, score-0.318]
</p><p>19 We propose here two cost functions that are easy to compute and lead to efficient algorithms. [sent-27, score-0.16]
</p><p>20 Typically one would expect points with similar labels to be clustered and the clusters to be separated. [sent-29, score-0.175]
</p><p>21 This can be detected in two ways: either by measuring the amount of label-clustering or by measuring the correlation between such variables. [sent-30, score-0.147]
</p><p>22 In the first case, we need to measure how points of the same class are close to each other and distant from points of different classes. [sent-31, score-0.189]
</p><p>23 In the second case, kernels can be regarded as oracles predicting whether two points are in the same class. [sent-32, score-0.123]
</p><p>24 The 'true' oracle is the one that knows the true matrix M. [sent-33, score-0.107]
</p><p>25 A measure of quality can be obtained by measuring the Pearson correlation coefficient between the kernel matrix K and the true M . [sent-34, score-0.575]
</p><p>26 Both approaches lead to the same quantity, known as the alignment [3]. [sent-35, score-0.551]
</p><p>27 Definition 1 Alignment The (empirical) alignment of a kernel kl with a kernel k2 with respect to the sample S is the quantity  . [sent-38, score-1.25]
</p><p>28 1(S,k1 ,k2) =  (K 1 ,K2)F , yi(K1 ,K1 )F(K2,K2)F  where Ki is the kernel matrix for the sample S using kernel k i . [sent-41, score-0.711]
</p><p>29 If we consider k2 = yy', where y is the vector of { -1, + I} labels for the sample, then with a slight abuse of notation  A A (Sk)= , ,y  V  /  (K,yy')F (K,yY')F. [sent-43, score-0.141]
</p><p>30 (") = mllKllF ' smce yy,yy F = (K, K) F (YY' , yy') F  2  m  Another measure of separation between classes is the average separation between two points in different classes, again normalised by the matrix norm. [sent-44, score-0.239]
</p><p>31 The cut cost of a clustering is defined as  k(Xi XJ)  L. [sent-46, score-0.447]
</p><p>32 This quantity is motivated by a graph theoretic concept. [sent-51, score-0.137]
</p><p>33 If we consider the Kernel matrix as the adjacency matrix of a fully connected weighted graph whose nodes are the data points, the cost of partitioning a graph is given by the total weight of the edges that one needs to cut or remove, and is exactly the numerator of the 'cut cost'. [sent-52, score-0.683]
</p><p>34 Notice also the relation between alignment and cutcost:  . [sent-53, score-0.551]
</p><p>35 Among other appealing properties of the alignment, is that this quantity is sharply concentrated around  its mean, as proven in the companion paper [3]. [sent-62, score-0.155]
</p><p>36 This shows that the expected alignment can be reliably estimated from its empirical estimate A. [sent-63, score-0.551]
</p><p>37 As the cut cost can be expressed as the difference of two alignments  C(S,k,y) = O. [sent-65, score-0.293]
</p><p>38 3  Optimising the cost with spectral techniques  In this section we will introduce and test two related methods for clustering, as well as their extensions to transduction. [sent-68, score-0.222]
</p><p>39 The general problem we want to solve is to assign class-labels to datapoints so as to maximize one of the two cost functions given above. [sent-69, score-0.16]
</p><p>40 The difference between the approaches is in the two approximation algorithms developed for the different cost functions. [sent-71, score-0.193]
</p><p>41 The approximation algorithms are obtained by relaxing the discrete problems of optimising over all possible labellings of a dataset to closely related continuous problems solved by eigenvalue decompositions. [sent-72, score-0.44]
</p><p>42 See [5] for use of eigenvectors in partitioning sparse matrices. [sent-73, score-0.196]
</p><p>43 After solving the relaxed problem, we can obtain an approximate discrete solution by choosing a suitable threshold to the entries in the vector y and applying the sign function. [sent-77, score-0.391]
</p><p>44 One can now transform the vector v into a vector in {-I, +l}m by choosing the threshold 8 that gives maximum alignment of y = sign(vrnaX - 8). [sent-84, score-0.799]
</p><p>45 One can hence estimate the quality of a dichotomy by comparing its value with the upper bound. [sent-89, score-0.126]
</p><p>46 The absolute alignment tells us how specialized a kernel is on a given dataset: the higher this quantity, the more committed to a specific dichotomy. [sent-90, score-0.853]
</p><p>47 The first eigenvector can be calculated in many ways, for example the Lanczos procedure, which is already effective for large datasets. [sent-91, score-0.295]
</p><p>48 Search engines like Google are based on estimating the first eigenvector of a matrix with dimensionality more than 10 9 , so for very large datasets there are approximation techniques. [sent-92, score-0.437]
</p><p>49 We preprocessed the data by normalising the input vectors in the kernel defined feature space and then centering them by shifting the origin (of the feature space) to their centre of gravity. [sent-94, score-0.488]
</p><p>50 This can be achieved by the following transformation of the kernel matrix, K +--- K - m - 1jg' - m - 1gj' + m - 2 j'KjJ, where j is the all ones vector, J the all ones matrix and 9 the vector of row sums of K. [sent-95, score-0.53]
</p><p>51 Eigenva lue Number  (a) (b) Figure 1: (a) Plot of alignment of the different eigenvectors with the labels ordered by increasing eigenvalue. [sent-96, score-0.761]
</p><p>52 1(S, k, y) for y = sign(v maX - (}i ) (bottom curve), and the accuracy of y (middle curve) against threshold number i. [sent-101, score-0.233]
</p><p>53 The first experiment applied the unsupervised technique to the Breast Cancer data with a linear kernel. [sent-102, score-0.119]
</p><p>54 Figure l(a) shows the alignmment of the different eigenvectors with the labels. [sent-103, score-0.135]
</p><p>55 The highest alignment is shown by the last eigenvector corresponding to the largest eigenvalue. [sent-104, score-0.811]
</p><p>56 For each value (}i of the threshold Figure l(b) shows the upper bound of . [sent-105, score-0.189]
</p><p>57 1(S, k, y) for y = sign( v max - (}i) (bottom curve), and t he accuracy of y (middle curve). [sent-109, score-0.173]
</p><p>58 Notice that where actual alignment and upper bound on alignment get closest, we have confidence that we have partitioned our data well, and in fact the accuracy is also maximized. [sent-110, score-1.273]
</p><p>59 Notice also that the choice of the threshold corresponds to maintaining the correct proportion between positives and negatives. [sent-111, score-0.177]
</p><p>60 This suggests another possible t hreshold selection strategy, based on the availability of enough labeled points to give a good estimate of the proportion of positive points in the dataset. [sent-112, score-0.143]
</p><p>61 It is a measure of how naturally t he data separates that t his procedure is able to optimise the split with an accuracy of approximately 97. [sent-115, score-0.292]
</p><p>62 29% by choosing the threshold that maximises the alignment (threshold number 435) but without making any use of the labels. [sent-116, score-0.758]
</p><p>63 In Figure 2a we present the same results for the Gaussian kernel (u = 6). [sent-117, score-0.302]
</p><p>64 In this case the accuracy obtained by optimising the alignment (threshold number 316) of t he resulting dichotomy is less impressive being only about 79. [sent-118, score-0.863]
</p><p>65 Here the accuracy of the split that optimises the alignment (threshold number 158) is approximately  (a)  (b)  Figure 2: Plot for Breast Cancer data (Gaussian kernel) (a) and Ionosphere data (linear kernel) (b) of Amax/ilKIIF (straight line), . [sent-121, score-0.761]
</p><p>66 4(S, k, y) for y = sign(v maX - ()i) (bottom curve), and the accuracy of y (middle curve) against threshold number i. [sent-122, score-0.233]
</p><p>67 We can also use the overall approach to adapt the kernel to the data. [sent-125, score-0.302]
</p><p>68 For example we can choose the kernel parameters so as to optimize Amax/IIKIIF. [sent-126, score-0.302]
</p><p>69 Then find the first eigenvector, choose a threshold to maximise the alignment and output the corresponding y. [sent-127, score-0.813]
</p><p>70 The cost to the alignment of changing a label Yi is 2 Lj Yjk(Xi' xj)/IIKIIF , so that if a point is isolated from the others, or if it is equally close to the two different classes, then changing its label will have only a very small effect. [sent-128, score-0.951]
</p><p>71 On the other hand, labels in strongly clustered points clearly contribute to the overall cost and changing their label will alter the alignment significantly. [sent-129, score-1.006]
</p><p>72 We consider embedding the set into the real line, so as to satisfy a clustering criterion. [sent-133, score-0.114]
</p><p>73 The resulting Kernel matrix should appear as a block diagonal matrix. [sent-134, score-0.137]
</p><p>74 In those cases, the eigenvectors of the Laplacian have been used, and the approach is called the Fiedler ordering. [sent-136, score-0.135]
</p><p>75 Although the Fiedler ordering could be used here as well, we present here a variation based on the simple kernel matrix. [sent-137, score-0.302]
</p><p>76 It is maximized when points with high similarity have the same sign and high absolute value, and when points with different sign have low similarity. [sent-140, score-0.292]
</p><p>77 The choice of coordinates v that optimizes this cost is the first eigenvector, and hence by sorting the data according to the value of their entry in this eigenvector one can hope to find a good permutation, that renders the kernel matrix block diagonal. [sent-141, score-1.083]
</p><p>78 Figure 3 shows the results of this heuristic applied to the Breast cancer dataset. [sent-142, score-0.157]
</p><p>79 The grey level indicates the size of the kernel entry. [sent-143, score-0.302]
</p><p>80 ( ) "'"' Kij ="2 ~ Kij - Y' Ky ="2 Y, Ly, 1",", 1 We can express this quantity as ~ ydy; i,j where L is the Laplacian matrix, defined as L = D-K, where D = diag(d l , . [sent-149, score-0.104]
</p><p>81 One would like to find y E {-l,+l}m so as to minimize the cut cost subject to the division being even, but this problem is NP-hard. [sent-153, score-0.326]
</p><p>82 Following the same strategy as with the alignment we can impose a slightly looser constraint on y, y E Rm, '拢i yt = m, l:i Yi = O. [sent-154, score-0.587]
</p><p>83 Since, zero is an eigenvalue of L with eigenvector j, the all ones vector, the problem is equivalent to finding the eigenvector of the smallest non-zero eigenvalue . [sent-156, score-0.727]
</p><p>84 So the eigenvector corresponding to the eigenvalue . [sent-167, score-0.342]
</p><p>85 One can now threshold the entries of the eigenvector in order to obtain a vector with -1 and + 1 entries. [sent-172, score-0.487]
</p><p>86 We applied the procedure to the Breast cancer data with both linear and Gaussian kernels. [sent-174, score-0.194]
</p><p>87 Now using the cut cost to select the best threshold for the linear kernel sets it at 378 with an accuracy of 67. [sent-176, score-0.828]
</p><p>88 86%, significantly worse than the results obtained by optimising the alignment. [sent-177, score-0.164]
</p><p>89 With the Gaussian kernel, on the other hand, the method selects threshold 312 with an accuracy of 80. [sent-178, score-0.233]
</p><p>90 31 %, a slight improvement over the results obtained with this kernel by optimising the alignment. [sent-179, score-0.497]
</p><p>91 The idea that some labelled data might improve performance comes from observing Figure 4b, where the selection based on the cut-cost is clearly suboptimal. [sent-183, score-0.094]
</p><p>92 By incorporating some label information, it is hoped that we can obtain an improved threshold selection. [sent-184, score-0.259]
</p><p>93 0050 '---------c:c------c=--=--~-_=_-_=_-~  (a) (b) Figure 4: Plot for Breast Cancer data using (a) Linear kernel) and (b) Gaussian kernel of C(S,k,y) - ,X/(21IKIIF) (dashed curves), for y = sign(v maX - ()i) and the error of y (solid curve) against threshold number i. [sent-185, score-0.483]
</p><p>94 Let z be the vector containing the known labels and 0 elsewhere. [sent-186, score-0.11]
</p><p>95 We now use the original matrix K to generate the eigenvector, but the matrix K P when measuring the cut-cost of the classifications generated by different thresholds. [sent-188, score-0.272]
</p><p>96 67%) for the Breast cancer data with Gaussian kernel, a marked improvement over the 80. [sent-191, score-0.194]
</p><p>97 4  Conclusions  The paper has considered two partition costs the first derived from the so-called alignment of a kernel to a label vector, and the second from the cut-cost of a label vector for a given kernel matrix. [sent-193, score-1.428]
</p><p>98 The two quantities are both optimised by the same labelling, but give rise to different approximation algorithms when the discrete constraint is removed from the labelling vector. [sent-194, score-0.134]
</p><p>99 It was shown how these relaxed problems are solved exactly using spectral t echniques, hence leading to two distinct approximation algorithms through a post-processing phase that re-discretises the vector to create a labelling that is chosen to optimise the given criterion. [sent-195, score-0.422]
</p><p>100 Experiments are presented showing the performance of both of these clustering techniques with some very striking results. [sent-196, score-0.114]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignment', 0.551), ('kernel', 0.302), ('eigenvector', 0.26), ('optimising', 0.164), ('cost', 0.16), ('cancer', 0.157), ('threshold', 0.144), ('eigenvectors', 0.135), ('cut', 0.133), ('breast', 0.133), ('clustering', 0.114), ('matrix', 0.107), ('labelling', 0.101), ('laplacian', 0.094), ('sign', 0.091), ('accuracy', 0.089), ('label', 0.085), ('max', 0.084), ('eigenvalue', 0.082), ('jaz', 0.08), ('labels', 0.075), ('optimise', 0.075), ('yy', 0.075), ('nello', 0.07), ('curve', 0.069), ('dirn', 0.067), ('fiedler', 0.067), ('xj', 0.066), ('plot', 0.064), ('quantity', 0.064), ('ly', 0.064), ('spectral', 0.062), ('partitioning', 0.061), ('cristianini', 0.061), ('dichotomy', 0.059), ('companion', 0.059), ('yi', 0.058), ('measuring', 0.058), ('xi', 0.058), ('labelled', 0.057), ('points', 0.055), ('definition', 0.052), ('ye', 0.05), ('relaxing', 0.05), ('maximise', 0.05), ('ionosphere', 0.05), ('kij', 0.05), ('straight', 0.049), ('entries', 0.048), ('unsupervised', 0.047), ('split', 0.047), ('sorting', 0.047), ('min', 0.046), ('bound', 0.045), ('clustered', 0.045), ('measure', 0.044), ('solved', 0.043), ('ones', 0.043), ('gram', 0.041), ('feature', 0.04), ('defined', 0.04), ('middle', 0.039), ('graph', 0.039), ('rm', 0.039), ('relaxed', 0.039), ('john', 0.038), ('optimizes', 0.038), ('data', 0.037), ('line', 0.036), ('yt', 0.036), ('dataset', 0.035), ('first', 0.035), ('vector', 0.035), ('changing', 0.035), ('datasets', 0.035), ('regarded', 0.035), ('inner', 0.034), ('choosing', 0.034), ('theoretic', 0.034), ('notice', 0.034), ('hence', 0.034), ('algorithms', 0.033), ('kernels', 0.033), ('closely', 0.033), ('classes', 0.033), ('partition', 0.033), ('proportion', 0.033), ('permutation', 0.033), ('find', 0.033), ('quality', 0.033), ('concentrated', 0.032), ('kl', 0.031), ('correlation', 0.031), ('slight', 0.031), ('bottom', 0.03), ('block', 0.03), ('incorporating', 0.03), ('ferent', 0.029), ('maximises', 0.029), ('normalising', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="170-tfidf-1" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>2 0.65167493 <a title="170-tfidf-2" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>3 0.22625421 <a title="170-tfidf-3" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>4 0.19826205 <a title="170-tfidf-4" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>Author: Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, Horst D. Simon</p><p>Abstract: The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function. 1</p><p>5 0.19749486 <a title="170-tfidf-5" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>6 0.194032 <a title="170-tfidf-6" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>7 0.18333249 <a title="170-tfidf-7" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>8 0.15552862 <a title="170-tfidf-8" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>9 0.1424897 <a title="170-tfidf-9" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>10 0.14075121 <a title="170-tfidf-10" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>11 0.13282965 <a title="170-tfidf-11" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>12 0.12148366 <a title="170-tfidf-12" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>13 0.11755806 <a title="170-tfidf-13" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>14 0.10318471 <a title="170-tfidf-14" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>15 0.10268387 <a title="170-tfidf-15" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>16 0.092787504 <a title="170-tfidf-16" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>17 0.090078749 <a title="170-tfidf-17" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>18 0.086552076 <a title="170-tfidf-18" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>19 0.084899433 <a title="170-tfidf-19" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>20 0.082765169 <a title="170-tfidf-20" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.299), (1, 0.214), (2, -0.074), (3, -0.338), (4, 0.309), (5, 0.198), (6, -0.031), (7, 0.068), (8, -0.095), (9, 0.224), (10, -0.074), (11, 0.032), (12, -0.06), (13, 0.117), (14, -0.244), (15, -0.046), (16, 0.004), (17, -0.081), (18, 0.033), (19, 0.02), (20, 0.004), (21, 0.086), (22, -0.216), (23, 0.067), (24, 0.053), (25, -0.039), (26, 0.029), (27, -0.04), (28, 0.117), (29, 0.099), (30, 0.242), (31, -0.069), (32, -0.053), (33, 0.101), (34, 0.068), (35, -0.096), (36, 0.024), (37, 0.037), (38, -0.042), (39, 0.052), (40, 0.011), (41, -0.143), (42, 0.032), (43, -0.031), (44, -0.053), (45, 0.007), (46, 0.09), (47, -0.029), (48, 0.043), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95843822 <a title="170-lsi-1" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>2 0.9233073 <a title="170-lsi-2" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>3 0.55577487 <a title="170-lsi-3" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><p>4 0.49437886 <a title="170-lsi-4" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>Author: Matthias Seeger</p><p>Abstract: We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant. 1</p><p>5 0.47076884 <a title="170-lsi-5" href="./nips-2001-On_the_Concentration_of_Spectral_Properties.html">136 nips-2001-On the Concentration of Spectral Properties</a></p>
<p>Author: John Shawe-Taylor, Nello Cristianini, Jaz S. Kandola</p><p>Abstract: We consider the problem of measuring the eigenvalues of a randomly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results. 1</p><p>6 0.43989125 <a title="170-lsi-6" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>7 0.4378112 <a title="170-lsi-7" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>8 0.42051864 <a title="170-lsi-8" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>9 0.39645639 <a title="170-lsi-9" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>10 0.35516137 <a title="170-lsi-10" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>11 0.33646789 <a title="170-lsi-11" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>12 0.32540801 <a title="170-lsi-12" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>13 0.32085896 <a title="170-lsi-13" href="./nips-2001-Kernel_Feature_Spaces_and_Nonlinear_Blind_Souce_Separation.html">103 nips-2001-Kernel Feature Spaces and Nonlinear Blind Souce Separation</a></p>
<p>14 0.31717917 <a title="170-lsi-14" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>15 0.31717896 <a title="170-lsi-15" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>16 0.29766664 <a title="170-lsi-16" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>17 0.29713747 <a title="170-lsi-17" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>18 0.29223081 <a title="170-lsi-18" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>19 0.28651217 <a title="170-lsi-19" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<p>20 0.27654344 <a title="170-lsi-20" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.412), (17, 0.073), (19, 0.019), (20, 0.017), (27, 0.118), (30, 0.05), (59, 0.036), (72, 0.065), (79, 0.024), (91, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96501648 <a title="170-lda-1" href="./nips-2001-Citcuits_for_VLSI_Implementation_of_Temporally_Asymmetric_Hebbian_Learning.html">49 nips-2001-Citcuits for VLSI Implementation of Temporally Asymmetric Hebbian Learning</a></p>
<p>Author: A. Bofill, D. P. Thompson, Alan F. Murray</p><p>Abstract: Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a O.6J.lm CMOS process are given. 1</p><p>same-paper 2 0.91737896 <a title="170-lda-2" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>3 0.90634537 <a title="170-lda-3" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>Author: Jyrki Kivinen, Alex J. Smola, Robert C. Williamson</p><p>Abstract: We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efﬁcient and leads to simple algorithms. In particular we derive update equations for classiﬁcation, regression, and novelty detection. The inclusion of the -trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the -trick only applies to the -insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber’s robust loss.     ¡</p><p>4 0.83202088 <a title="170-lda-4" href="./nips-2001-On_Kernel-Target_Alignment.html">134 nips-2001-On Kernel-Target Alignment</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, André Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><p>5 0.82194555 <a title="170-lda-5" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>6 0.65099138 <a title="170-lda-6" href="./nips-2001-On_the_Convergence_of_Leveraging.html">137 nips-2001-On the Convergence of Leveraging</a></p>
<p>7 0.62570554 <a title="170-lda-7" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>8 0.61841273 <a title="170-lda-8" href="./nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</a></p>
<p>9 0.61099499 <a title="170-lda-9" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>10 0.6089977 <a title="170-lda-10" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>11 0.60791498 <a title="170-lda-11" href="./nips-2001-Learning_Spike-Based_Correlations_and_Conditional_Probabilities_in_Silicon.html">112 nips-2001-Learning Spike-Based Correlations and Conditional Probabilities in Silicon</a></p>
<p>12 0.59880006 <a title="170-lda-12" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>13 0.59440082 <a title="170-lda-13" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>14 0.59103632 <a title="170-lda-14" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>15 0.58828479 <a title="170-lda-15" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>16 0.58784699 <a title="170-lda-16" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>17 0.57952034 <a title="170-lda-17" href="./nips-2001-Stochastic_Mixed-Signal_VLSI_Architecture_for_High-Dimensional_Kernel_Machines.html">176 nips-2001-Stochastic Mixed-Signal VLSI Architecture for High-Dimensional Kernel Machines</a></p>
<p>18 0.57750285 <a title="170-lda-18" href="./nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</a></p>
<p>19 0.57411319 <a title="170-lda-19" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>20 0.57364136 <a title="170-lda-20" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
