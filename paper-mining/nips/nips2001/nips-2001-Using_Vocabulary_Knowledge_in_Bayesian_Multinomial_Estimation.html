<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-194" href="#">nips2001-194</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</h1>
<br/><p>Source: <a title="nips-2001-194-pdf" href="http://papers.nips.cc/paper/2063-using-vocabulary-knowledge-in-bayesian-multinomial-estimation.pdf">pdf</a></p><p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><p>Reference: <a title="nips-2001-194-reference" href="../nips2001_reference/nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. [sent-5, score-0.399]
</p><p>2 Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. [sent-6, score-0.765]
</p><p>3 We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. [sent-7, score-0.149]
</p><p>4 We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. [sent-8, score-0.542]
</p><p>5 1  Introduction  Sparse multinomial distributions arise in many statistical domains, including natural language processing and graphical models. [sent-9, score-0.396]
</p><p>6 Consequently, a number of approaches to parameter estimation for sparse multinomial distributions have been suggested [3]. [sent-10, score-0.558]
</p><p>7 These approaches tend to be domain-independent: they make little use of prior knowledge about a specific domain. [sent-11, score-0.18]
</p><p>8 In many domains where multinomial distributions are estimated there is often at least weak prior knowledge about' the potential structure of distributions, such as a set of hypotheses about restricted vocabularies from which the symbols might be generated. [sent-12, score-1.261]
</p><p>9 Such knowledge can be solicited from experts or obtained from unlabeled data. [sent-13, score-0.236]
</p><p>10 We present a method for Bayesian_parameter estimation in sparse discrete domains that exploits this weak form of prior knowledge to improve estimates over knowledge-free approaches. [sent-14, score-0.388]
</p><p>11 1  Bayesian parameter estimation for multinomial distributions  Following the presentation in [4], we consider a language ~ containing L distinct symbols. [sent-16, score-0.551]
</p><p>12 A multinomial distribution is specified by a parameter vector f) == (Ol, . [sent-17, score-0.304]
</p><p>13 The task of multinomial estimation is to take a data set D and produce a'vector f) that results in a good approximation to the distribution that produced D. [sent-25, score-0.359]
</p><p>14 x N drawn from the distribution to be estimated, which can be summarized by the statistics N i specifying the number of times the ith symbol occurs in the data. [sent-29, score-0.052]
</p><p>15 D also determines the set ~o  of symbols that occur in the data. [sent-30, score-0.088]
</p><p>16 Stated in this way, multinomial estimation involves predicting the next observation based on the data. [sent-31, score-0.359]
</p><p>17 The Bayesian estimate for this probability is given by  PL(xN+lID)  =  I  p(XN+1IB)P(BID)dB  where P(X N + 1 10) follows from the multinomial distribution corresponding to O. [sent-33, score-0.295]
</p><p>18 The posterior probability P(OID) can be obtained via Bayes rule L  P(OID) oc P(DIO)P(O) == P(8}  II ONi i==l  where P(O) is the prior probability of a given O. [sent-34, score-0.162]
</p><p>19 Laplace used this method with a uniform prior over 0 to give the famous "law of succession" [6J. [sent-35, score-0.071]
</p><p>20 A more general approach is to assume a Dirichlet prior over (), which is conjugate to the multinomial distribution and gives  N i +LCY. [sent-36, score-0.338]
</p><p>21 5 is the Jeffreys-Perks law or Expected Likelihood Estimation [2] [5J [9J, while using arbitrary O! [sent-45, score-0.074]
</p><p>22 2  EstiIllating sparse Illultinomial distributions  Several authors have extended the Bayesian approach to sparse multinomial distributions, in which only a restricted vocabulary of symbols are used, by maintaining uncertainty over these vocabularies. [sent-49, score-0.971]
</p><p>23 Friedman and Singer consider the vocabulary V ~ :E to be a random variable, allowing them to write  p. [sent-52, score-0.325]
</p><p>24 Ivlaking use of weak prior knowiedge  Friedman and Singer assume a prior that gives equal probability to all vocabularies of a given cardinality. [sent-61, score-0.709]
</p><p>25 However, many real-world tasks provide limited knowledge about the structure of distributions that we can build into our methods for parameter estimation. [sent-62, score-0.214]
</p><p>26 In the context of sparse multinomial estimation, one instance of such knowledge the importance of specific vocabularies. [sent-63, score-0.407]
</p><p>27 For example, in predicting the next character in a file, our predictions could be facilitated by considering the fact that most files either use a vocabulary consisting of ASCII printing characters (such as text files), or all possible characters (suc~ as object files). [sent-64, score-1.001]
</p><p>28 This kind of structural knowledge about a domain is typically easier to solicit from experts than accurate distributional information, and forms a valuable informational resource. [sent-65, score-0.225]
</p><p>29 If we have this kind of prior knowledge, we can restrict our attention to a subset of the 2L possible vocabularies. [sent-66, score-0.099]
</p><p>30 fu particular, we can specify a set of vocabularies V which we consider as hypotheses for the vocabulary used in producing D, where the elements of V are specified by our knowledge of the domain. [sent-67, score-0.932]
</p><p>31 This stands as a compromise between Friedman and Singer's approach, in which V consists of all vocabularies, and traditional Bayesian parameter estimation as represented by Equation 1, in which V consists of only the vocabulary containing all words. [sent-68, score-0.48]
</p><p>32 The intuition behind this approach is that it attempts to classify the target distribution as using one of a known set of vocabularies, where the vocabularies are obtained either from experts or from unlabeled data. [sent-71, score-0.604]
</p><p>33 Applying standard Bayesian multinomial estimation within this vocabulary gives enough flexibility for the method to capture a range of distributions, while making use of our weak prior knowledge. [sent-72, score-0.799]
</p><p>34 1  An illustration: Text compression  Text compression is an effective test of methods for multinomial estimation. [sent-74, score-0.533]
</p><p>35 Adaptive coding can be performed by specifying a method for calculating a distribution over the probability of the next byte in a file based upon the preceding bytes [1]. [sent-75, score-0.394]
</p><p>36 The extent to which the file is compressed depends upon the quality of these predictions. [sent-76, score-0.252]
</p><p>37 To illustrate the utility of including prior knowledge, we follow Ristad in using the Calgary text compression corpus [1]. [sent-77, score-0.398]
</p><p>38 The files include Bib'IEXsource (bib), formatted English text (book*, paper*), geological data (geo), newsgroup articles (news), object files (obj*), a bit-mapped picture (pic), programs in three different languages (prog*) and a terminal transcript (trans). [sent-79, score-0.785]
</p><p>39 The task was to estimate the distribution from which characters in the file were drawn based upon the first N characters and thus predict the N + 1st character. [sent-80, score-0.534]
</p><p>40 Performance was measured in terms of the length of the resulting file, where the contribution of the N + 1st character to the length is log2 P(XN+lID). [sent-81, score-0.044]
</p><p>41 The results are expressed as the number of bytes required to encode the file relative to the empirical entropy NH(Ni/N) as assessed by Ristad [10]. [sent-82, score-0.27]
</p><p>42 P v is the restricted vocabulary model outlined above, with V consisting of just two hypotheses: one corresponding to binary files, containing all 256 characters, and one consisting of a 107 character vocabulary representing formatted English. [sent-84, score-0.844]
</p><p>43 The latter vocabulary was estimated from 5MB of English text, C code, Bib'IEXsource, and newsgroup data from outside the Calgary corpus. [sent-85, score-0.46]
</p><p>44 P y outperformed the other methods on all files based upon English text, bar bookl, and all files using all 256 symbols l . [sent-89, score-0.55]
</p><p>45 The high performance followed from rapid classification of these files as using the appropriate vocabulary in V. [sent-90, score-0.556]
</p><p>46 When the vocabulary included all symbols Py performed as PJ, which gave the best predictions for these files. [sent-91, score-0.443]
</p><p>47 1 A number of excellent techniques for· text compression exist that outperform all of those presented here. [sent-92, score-0.263]
</p><p>48 We have not included these techniques for comparison because our interest is in using text compression as a means of assessing estimation procedures, rather than as an end in itself. [sent-93, score-0.355]
</p><p>49 We thus consider only methods for multinomial estimation as our comparison. [sent-94, score-0.359]
</p><p>50 2  Maintaining uncertainty in vocabularies  The results for book1 illustrate a weakness of the approach outlined above. [sent-97, score-0.61]
</p><p>51 The file length for P y is higher than those for PF and PR , despite the fact that the file uses a text-based vocabulary. [sent-98, score-0.416]
</p><p>52 This file contains two characters that were not encountered in the data used to construct V. [sent-99, score-0.377]
</p><p>53 These characters caused P y to default to the unrestricted vocabulary of all 256 characters. [sent-100, score-0.466]
</p><p>54 This behavior results from the assumption that the candidate vocabularies in V are completely accurate. [sent-102, score-0.529]
</p><p>55 Since in many cases the knowledge that informs the vocabularies in V may be imperfect, it is desirable to allow for uncertainty in vocabularies. [sent-103, score-0.629]
</p><p>56 When V is determined by the judgments of domain experts, ty is the probability that an unmentioned word actually belongs to a particular vocabulary. [sent-105, score-0.174]
</p><p>57 While it may not be the most efficient use of such data, the V E V can also be estimated from some form of unlabeled data. [sent-106, score-0.075]
</p><p>58 Friedman and Singer explicitly calculate the probability that an unseen word is in V based upon a dataset: from the second condition of Equation 5, we find that we should set ty == L_1IYI (1- C(D, L)). [sent-108, score-0.225]
</p><p>59 3  Bayesian parameter estimation in natural language  Statistical natural language processing often uses sparse multinomial distributions over large vocabularies of words. [sent-110, score-1.139]
</p><p>60 By specifying a basis set of vocabularies, we can perform parameter estimation by classifying distributions according to their vocabulary. [sent-112, score-0.252]
</p><p>61 This dataset is commonly used in testing text classification algorithms (eg. [sent-114, score-0.214]
</p><p>62 Ten newsgroups were used to estimate a set of vocabularies V with corresponding ty. [sent-116, score-0.725]
</p><p>63 These vocabularies were used in estimating multinomial distributions on these newsgroups and ten others. [sent-117, score-1.18]
</p><p>64 The articl~s in each of the 20 newsgroups were then divided into three sets. [sent-119, score-0.23]
</p><p>65 The first 500 articles from ten newsgroups were used to estimate the candidate vocabularies V and uncertainty parameters ty. [sent-120, score-1.019]
</p><p>66 Articles 501700 for all 20 newsgroups were used as training data for multinomial estimation. [sent-121, score-0.497]
</p><p>67 Following [8], a dictionary was built up by running over the 13,000 articles resulting from this division, and all words that occurred only once were mapped to an "unknown" word. [sent-123, score-0.24]
</p><p>68 As before, the restricted vocabulary method (Py), Friedman and Singer's method (PF ), and Ristad's (PR ), Laplace's (PL ) and the Jeffreys-Perks (PJ ) laws were ap-  alt. [sent-125, score-0.403]
</p><p>69 hardware  100  10000 Number of words  50000  talk. [sent-161, score-0.069]
</p><p>70 graphics  ~~~ 100  10000  F~gure 1: Cross-entropy of predictions on newsgroup data as a function of the logarithm of the number of words. [sent-164, score-0.136]
</p><p>71 mideast and those to its right) are for the newsgroups with unknown vocabularies. [sent-168, score-0.23]
</p><p>72 The bottom ten are for those that contributed vocabularies to V, trained and tested on novel data. [sent-169, score-0.649]
</p><p>73 mideast indicates the point at which Pv defaults to the full vocabulary, as the number of unseen words makes this vocabulary more likely. [sent-173, score-0.469]
</p><p>74 'V featured one vocabulary that contained all words in the dictionary, and ten vocabularies each corresponding to the words used in the first 500 articles of one of the newsgroups designated for this purpose. [sent-178, score-1.393]
</p><p>75 Testing for each newsgroup consisted of taking words from the 200 articles assigned for training purposes, estimating a. [sent-180, score-0.329]
</p><p>76 The cross-entropy is H{Q; P) == Ei Qi log2 Pi, where Q is the true distribution and P is the distribution produced by the estimation method. [sent-182, score-0.092]
</p><p>77 Q was given by the maximum likelihood estimate formed from the word frequencies in all 200 articles assigned for testing purposes. [sent-183, score-0.205]
</p><p>78 As expected, P y consistently outperformed the other methods on the newsgroups that contributed to V. [sent-191, score-0.301]
</p><p>79 However, performance on novel newsgroups was also greatly improved. [sent-192, score-0.267]
</p><p>80 As can be seen in Figure 2, the novel newsgroups were classified to appropriate vocabularies - for example  all words  rec. [sent-193, score-0.858]
</p><p>81 graphics  o  10000 Number of words  Figure 2: Classification of newsgroup vocabularies. [sent-221, score-0.175]
</p><p>82 The lines illustrate the vocabulary which had maximum posterior probability for each of the ten test newsgroups after exposure to differing numbers of words. [sent-222, score-0.722]
</p><p>83 The vocabularies in V are listed along the left hand side of the axis, and the lines are identified with newsgroups by the labels on the right hand side. [sent-223, score-0.753]
</p><p>84 The proportion of word types occurring in the test data but  not the vocabulary to which the novel newsgroups were classified ranged between 30. [sent-233, score-0.66]
</p><p>85 This illustrates that even approximate knowledge can facilitate predictions: the basis set of vocabularies allowed the high frequency words in the data to be modelled effectively, without excess mass being attributed to the low frequency novel word tokens. [sent-237, score-0.762]
</p><p>86 mideast illustrates the same defaulting behavior that was shown for text classification: when the data become more probable under the vocabulary containing all words than under a restricted vocabulary the method defaults to the Jeffreys-Perks law. [sent-240, score-0.96]
</p><p>87 This guarantees that the method will tend to perform no worse than P J when unseen words are encountered in sufficient proportions. [sent-241, score-0.13]
</p><p>88 4  Discussion  Bayesian approaches to parameter estimation for sparse multinomial distributions have employed the notion of a restricted vocabulary from which symbols are produced. [sent-243, score-1.014]
</p><p>89 In many domains where such distributions are estimated; there is often at  least limited knowledge about the structure of these vocabularies. [sent-244, score-0.218]
</p><p>90 By considering just the vocabularies suggested by such knowledge, together with some uncertainty concerning those vocabularies, we can achieve very good estimates of distributions in these domains. [sent-245, score-0.621]
</p><p>91 We have presented a Bayesian approach that employs limited prior knowledge, and shown that it outperforms a range of approaches to multinomial estimation for both text compression and a task involving natural language. [sent-246, score-0.75]
</p><p>92 While our applications in this paper estimated approximate vocabularies from data, the real promise of this approach lies with domain knowledge solicited from experts. [sent-247, score-0.677]
</p><p>93 Experts are typically better at providing qualitative structural information than quantitative distributional information, and our approach provides a way of using this information in parameter estimation. [sent-248, score-0.066]
</p><p>94 For example, in the context of parameter estimation for graphical models to be used in medical diagnosis, distinguishing classes of symptoms might be informative in determining the parameters governing their relationship to diseases. [sent-249, score-0.129]
</p><p>95 This form of knowledge is naturally translated into a set of vocabularies to be considered for each such distribution. [sent-250, score-0.574]
</p><p>96 More complex applications to natural language lllay also be possible, such as using syntactic information in estimating probabilities for n-gram models. [sent-251, score-0.091]
</p><p>97 The approach we have presented in this paper provides a simple way to allow this kind of limited domain knowledge to be useful in Bayesian parameter estimation. [sent-252, score-0.197]
</p><p>98 An empirical study of smoothing techniques for language modeling. [sent-274, score-0.058]
</p><p>99 An invariant form for the prior probability in estimation problems. [sent-283, score-0.191]
</p><p>100 Text classification fro'in labeled and unlabeled documents using EM. [sent-301, score-0.087]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vocabularies', 0.495), ('vocabulary', 0.325), ('multinomial', 0.267), ('newsgroups', 0.23), ('file', 0.208), ('files', 0.19), ('ristad', 0.144), ('ko', 0.141), ('characters', 0.141), ('compression', 0.133), ('text', 0.13), ('articles', 0.121), ('friedman', 0.117), ('singer', 0.114), ('newsgroup', 0.106), ('ild', 0.104), ('estimation', 0.092), ('symbols', 0.088), ('pf', 0.087), ('ten', 0.084), ('ilv', 0.083), ('ty', 0.079), ('knowledge', 0.079), ('pj', 0.076), ('pl', 0.076), ('xn', 0.074), ('law', 0.074), ('calgary', 0.072), ('distributions', 0.071), ('prior', 0.071), ('words', 0.069), ('experts', 0.063), ('bayesian', 0.063), ('bytes', 0.062), ('div', 0.062), ('sparse', 0.061), ('language', 0.058), ('laplace', 0.058), ('pv', 0.057), ('uncertainty', 0.055), ('specifying', 0.052), ('dirichlet', 0.052), ('pr', 0.051), ('dictionary', 0.05), ('bib', 0.048), ('formatted', 0.048), ('geo', 0.048), ('misc', 0.048), ('sclcrypt', 0.048), ('sclmed', 0.048), ('sclspace', 0.048), ('solicited', 0.048), ('vid', 0.048), ('unlabeled', 0.046), ('weak', 0.044), ('upon', 0.044), ('character', 0.044), ('english', 0.044), ('restricted', 0.043), ('testing', 0.043), ('lid', 0.042), ('trans', 0.042), ('oid', 0.042), ('actuaries', 0.042), ('nh', 0.042), ('pic', 0.042), ('defaults', 0.042), ('word', 0.041), ('classification', 0.041), ('domains', 0.041), ('facilitate', 0.041), ('ka', 0.038), ('outperformed', 0.038), ('py', 0.038), ('corpus', 0.037), ('parameter', 0.037), ('novel', 0.037), ('ni', 0.036), ('oc', 0.035), ('laws', 0.035), ('candidate', 0.034), ('estimating', 0.033), ('hypotheses', 0.033), ('outlined', 0.033), ('unseen', 0.033), ('contributed', 0.033), ('predictions', 0.03), ('approaches', 0.03), ('distributional', 0.029), ('estimated', 0.029), ('otherwise', 0.029), ('kind', 0.028), ('probability', 0.028), ('encountered', 0.028), ('lines', 0.028), ('classified', 0.027), ('limited', 0.027), ('illustrate', 0.027), ('containing', 0.026), ('domain', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="194-tfidf-1" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><p>2 0.12771149 <a title="194-tfidf-2" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>Author: David M. Blei, Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1</p><p>3 0.082752548 <a title="194-tfidf-3" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>Author: Ilya Nemenman, F. Shafee, William Bialek</p><p>Abstract: We study properties of popular near–uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam–style phase space argument expands the priors into their inﬁnite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions. Learning a probability distribution from examples is one of the basic problems in data analysis. Common practical approaches introduce a family of parametric models, leading to questions about model selection. In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting “phase space volume” automatically discriminates against models with larger numbers of parameters—hence the description of these volume terms as Occam factors [1, 2]. As we move from ﬁnite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum ﬁeld theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3]. Further, at least under some conditions the relevant degree of smoothness can be determined self–consistently from the data, so that we approach something like a model independent method for learning a distribution [4]. The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space. Here the probability distribution is just a list of numbers {q i }, i = 1, 2, · · · , K, where K is the number of bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to believe that any q i and qj should be similar. The task is to learn this distribution from a set of examples, which we can describe as the number of times ni each possibility is observed in a set of N = K ni i=1 samples. This problem arises in the context of language, where the index i might label words or phrases, so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about similarity are consistent with the constraints of a metric space. Similarly, in bioinformatics the index i might label n–mers of the the DNA or amino acid sequence, and although most work in the ﬁeld is based on metrics for sequence comparison one might like an alternative approach that does not rest on such assumptions. In the analysis of neural responses, once we ﬁx our time resolution the response becomes a set of discrete “words,” and estimates of the information content in the response are de- termined by the probability distribution on this discrete space. What all of these examples have in common is that we often need to draw some conclusions with data sets that are not in the asymptotic limit N K. Thus, while we might use a large corpus to sample the distribution of words in English by brute force (reaching N K with K the size of the vocabulary), we can hardly do the same for three or four word phrases. In models described by continuous functions, the inﬁnite number of “possibilities” can never be overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities may play this role. If we have a joint distribution of two variables, the analog of a smooth distribution would be one which does not have too much mutual information between these variables. Even more simply, we might say that smooth distributions have large entropy. While the idea of “maximum entropy inference” is common [5], the interplay between constraints on the entropy and the volume in the space of models seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly sensible, more or less uniform priors on the space of discrete probability distributions correspond to disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that reliable inference outside the asymptotic regime N K requires a more uniform prior on the entropy, and we offer one way of doing this. While many distributions are consistent with the data when N ≤ K, we provide empirical evidence that this ﬂattening of the entropic prior allows us to make surprisingly reliable statements about the entropy itself in this regime. At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform priors on the space of distributions. The natural “uniform” prior is given by Pu ({qi }) = 1 δ Zu K 1− K qi i=1 , Zu = A dq1 dq2 · · · dqK δ 1− qi (1) i=1 where the delta function imposes the normalization, Zu is the total volume in the space of models, and the integration domain A is such that each qi varies in the range [0, 1]. Note that, because of the normalization constraint, an individual q i chosen from this distribution in fact is not uniformly distributed—this is also an example of phase space effects, since in choosing one qi we constrain all the other {qj=i }. What we mean by uniformity is that all distributions that obey the normalization constraint are equally likely a priori. Inference with this uniform prior is straightforward. If our examples come independently from {qi }, then we calculate the probability of the model {qi } with the usual Bayes rule: 1 K P ({qi }|{ni }) = P ({ni }|{qi })Pu ({qi }) , P ({ni }|{qi }) = (qi )ni . Pu ({ni }) i=1 (2) If we want the best estimate of the probability qi in the least squares sense, then we should compute the conditional mean, and this can be done exactly, so that [6, 7] qi = ni + 1 . N +K (3) Thus we can think of inference with this uniform prior as setting probabilities equal to the observed frequencies, but with an “extra count” in every bin. This sensible procedure was ﬁrst introduced by Laplace [8]. It has the desirable property that events which have not been observed are not automatically assigned probability zero. 1 If the data are unordered, extra combinatorial factors have to be included in P ({ni }|{qi }). However, these cancel immediately in later expressions. A natural generalization of these ideas is to consider priors that have a power–law dependence on the probabilities, the so called Dirichlet family of priors: K Pβ ({qi }) = 1 δ 1− qi Z(β) i=1 K β−1 qi , (4) i=1 It is interesting to see what typical distributions from these priors look like. Even though different qi ’s are not independent random variables due to the normalizing δ–function, generation of random distributions is still easy: one can show that if q i ’s are generated successively (starting from i = 1 and proceeding up to i = K) from the Beta–distribution j</p><p>4 0.073450893 <a title="194-tfidf-4" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>Author: Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, Horst D. Simon</p><p>Abstract: The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function. 1</p><p>5 0.068426199 <a title="194-tfidf-5" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>Author: Michiro Negishi, Stephen J. Hanson</p><p>Abstract: It has been known that people, after being exposed to sentences generated by an artificial grammar, acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modified grammar. We show that a second order recurrent neural network is able to transfer grammatical knowledge from one language (generated by a Finite State Machine) to another language which differ both in vocabularies and syntax. Representation of the grammatical knowledge in the network is analyzed using linear discriminant analysis. 1</p><p>6 0.066753112 <a title="194-tfidf-6" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>7 0.06553109 <a title="194-tfidf-7" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>8 0.063430078 <a title="194-tfidf-8" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>9 0.056721389 <a title="194-tfidf-9" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>10 0.052941907 <a title="194-tfidf-10" href="./nips-2001-Bayesian_Predictive_Profiles_With_Applications_to_Retail_Transaction_Data.html">41 nips-2001-Bayesian Predictive Profiles With Applications to Retail Transaction Data</a></p>
<p>11 0.051128369 <a title="194-tfidf-11" href="./nips-2001-Grammatical_Bigrams.html">86 nips-2001-Grammatical Bigrams</a></p>
<p>12 0.050987739 <a title="194-tfidf-12" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>13 0.049934179 <a title="194-tfidf-13" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>14 0.049634121 <a title="194-tfidf-14" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>15 0.048630066 <a title="194-tfidf-15" href="./nips-2001-PAC_Generalization_Bounds_for_Co-training.html">143 nips-2001-PAC Generalization Bounds for Co-training</a></p>
<p>16 0.047907751 <a title="194-tfidf-16" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>17 0.04790296 <a title="194-tfidf-17" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>18 0.047621313 <a title="194-tfidf-18" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>19 0.047502387 <a title="194-tfidf-19" href="./nips-2001-Algorithmic_Luckiness.html">31 nips-2001-Algorithmic Luckiness</a></p>
<p>20 0.045683265 <a title="194-tfidf-20" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.142), (1, 0.003), (2, -0.013), (3, -0.05), (4, -0.065), (5, -0.105), (6, -0.022), (7, 0.024), (8, -0.098), (9, -0.032), (10, 0.071), (11, -0.008), (12, -0.085), (13, -0.1), (14, -0.01), (15, -0.072), (16, 0.053), (17, -0.042), (18, -0.005), (19, -0.009), (20, -0.052), (21, 0.079), (22, -0.035), (23, -0.005), (24, 0.093), (25, -0.066), (26, -0.105), (27, 0.012), (28, 0.131), (29, 0.06), (30, 0.02), (31, -0.009), (32, -0.015), (33, -0.047), (34, -0.022), (35, 0.079), (36, -0.03), (37, -0.003), (38, 0.209), (39, 0.026), (40, 0.128), (41, -0.079), (42, -0.074), (43, 0.172), (44, -0.016), (45, 0.011), (46, -0.161), (47, -0.109), (48, -0.03), (49, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95355213 <a title="194-lsi-1" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><p>2 0.70047593 <a title="194-lsi-2" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>Author: David M. Blei, Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1</p><p>3 0.47969863 <a title="194-lsi-3" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>Author: Jeff Bilmes, Gang Ji, Marina Meila</p><p>Abstract: In this work, we introduce an information-theoretic based correction term to the likelihood ratio classiﬁcation method for multiple classes. Under certain conditions, the term is sufﬁcient for optimally correcting the difference between the true and estimated likelihood ratio, and we analyze this in the Gaussian case. We ﬁnd that the new correction term significantly improves the classiﬁcation results when tested on medium vocabulary speech recognition tasks. Moreover, the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournament-like strategies to deal with this issue. We ﬁnd that further small improvements are obtained by using an appropriate tournament. Lastly, we ﬁnd that intransitivity appears to be a good measure of classiﬁcation conﬁdence.</p><p>4 0.45094746 <a title="194-lsi-4" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>5 0.4283821 <a title="194-lsi-5" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>Author: Ilya Nemenman, F. Shafee, William Bialek</p><p>Abstract: We study properties of popular near–uniform (Dirichlet) priors for learning undersampled probability distributions on discrete nonmetric spaces and show that they lead to disastrous results. However, an Occam–style phase space argument expands the priors into their inﬁnite mixture and resolves most of the observed problems. This leads to a surprisingly good estimator of entropies of discrete distributions. Learning a probability distribution from examples is one of the basic problems in data analysis. Common practical approaches introduce a family of parametric models, leading to questions about model selection. In Bayesian inference, computing the total probability of the data arising from a model involves an integration over parameter space, and the resulting “phase space volume” automatically discriminates against models with larger numbers of parameters—hence the description of these volume terms as Occam factors [1, 2]. As we move from ﬁnite parameterizations to models that are described by smooth functions, the integrals over parameter space become functional integrals and methods from quantum ﬁeld theory allow us to do these integrals asymptotically; again the volume in model space consistent with the data is larger for models that are smoother and hence less complex [3]. Further, at least under some conditions the relevant degree of smoothness can be determined self–consistently from the data, so that we approach something like a model independent method for learning a distribution [4]. The results emphasizing the importance of phase space factors in learning prompt us to look back at a seemingly much simpler problem, namely learning a distribution on a discrete, nonmetric space. Here the probability distribution is just a list of numbers {q i }, i = 1, 2, · · · , K, where K is the number of bins or possibilities. We do not assume any metric on the space, so that a priori there is no reason to believe that any q i and qj should be similar. The task is to learn this distribution from a set of examples, which we can describe as the number of times ni each possibility is observed in a set of N = K ni i=1 samples. This problem arises in the context of language, where the index i might label words or phrases, so that there is no natural way to place a metric on the space, nor is it even clear that our intuitions about similarity are consistent with the constraints of a metric space. Similarly, in bioinformatics the index i might label n–mers of the the DNA or amino acid sequence, and although most work in the ﬁeld is based on metrics for sequence comparison one might like an alternative approach that does not rest on such assumptions. In the analysis of neural responses, once we ﬁx our time resolution the response becomes a set of discrete “words,” and estimates of the information content in the response are de- termined by the probability distribution on this discrete space. What all of these examples have in common is that we often need to draw some conclusions with data sets that are not in the asymptotic limit N K. Thus, while we might use a large corpus to sample the distribution of words in English by brute force (reaching N K with K the size of the vocabulary), we can hardly do the same for three or four word phrases. In models described by continuous functions, the inﬁnite number of “possibilities” can never be overwhelmed by examples; one is saved by the notion of smoothness. Is there some nonmetric analog of this notion that we can apply in the discrete case? Our intuition is that information theoretic quantities may play this role. If we have a joint distribution of two variables, the analog of a smooth distribution would be one which does not have too much mutual information between these variables. Even more simply, we might say that smooth distributions have large entropy. While the idea of “maximum entropy inference” is common [5], the interplay between constraints on the entropy and the volume in the space of models seems not to have been considered. As we shall explain, phase space factors alone imply that seemingly sensible, more or less uniform priors on the space of discrete probability distributions correspond to disastrously singular prior hypotheses about the entropy of the underlying distribution. We argue that reliable inference outside the asymptotic regime N K requires a more uniform prior on the entropy, and we offer one way of doing this. While many distributions are consistent with the data when N ≤ K, we provide empirical evidence that this ﬂattening of the entropic prior allows us to make surprisingly reliable statements about the entropy itself in this regime. At the risk of being pedantic, we state very explicitly what we mean by uniform or nearly uniform priors on the space of distributions. The natural “uniform” prior is given by Pu ({qi }) = 1 δ Zu K 1− K qi i=1 , Zu = A dq1 dq2 · · · dqK δ 1− qi (1) i=1 where the delta function imposes the normalization, Zu is the total volume in the space of models, and the integration domain A is such that each qi varies in the range [0, 1]. Note that, because of the normalization constraint, an individual q i chosen from this distribution in fact is not uniformly distributed—this is also an example of phase space effects, since in choosing one qi we constrain all the other {qj=i }. What we mean by uniformity is that all distributions that obey the normalization constraint are equally likely a priori. Inference with this uniform prior is straightforward. If our examples come independently from {qi }, then we calculate the probability of the model {qi } with the usual Bayes rule: 1 K P ({qi }|{ni }) = P ({ni }|{qi })Pu ({qi }) , P ({ni }|{qi }) = (qi )ni . Pu ({ni }) i=1 (2) If we want the best estimate of the probability qi in the least squares sense, then we should compute the conditional mean, and this can be done exactly, so that [6, 7] qi = ni + 1 . N +K (3) Thus we can think of inference with this uniform prior as setting probabilities equal to the observed frequencies, but with an “extra count” in every bin. This sensible procedure was ﬁrst introduced by Laplace [8]. It has the desirable property that events which have not been observed are not automatically assigned probability zero. 1 If the data are unordered, extra combinatorial factors have to be included in P ({ni }|{qi }). However, these cancel immediately in later expressions. A natural generalization of these ideas is to consider priors that have a power–law dependence on the probabilities, the so called Dirichlet family of priors: K Pβ ({qi }) = 1 δ 1− qi Z(β) i=1 K β−1 qi , (4) i=1 It is interesting to see what typical distributions from these priors look like. Even though different qi ’s are not independent random variables due to the normalizing δ–function, generation of random distributions is still easy: one can show that if q i ’s are generated successively (starting from i = 1 and proceeding up to i = K) from the Beta–distribution j</p><p>6 0.42114165 <a title="194-lsi-6" href="./nips-2001-Grammatical_Bigrams.html">86 nips-2001-Grammatical Bigrams</a></p>
<p>7 0.41338688 <a title="194-lsi-7" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>8 0.39538541 <a title="194-lsi-8" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>9 0.36740568 <a title="194-lsi-9" href="./nips-2001-The_g_Factor%3A_Relating_Distributions_on_Features_to_Distributions_on_Images.html">189 nips-2001-The g Factor: Relating Distributions on Features to Distributions on Images</a></p>
<p>10 0.36196429 <a title="194-lsi-10" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>11 0.33052427 <a title="194-lsi-11" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>12 0.32800058 <a title="194-lsi-12" href="./nips-2001-Distribution_of_Mutual_Information.html">61 nips-2001-Distribution of Mutual Information</a></p>
<p>13 0.32149443 <a title="194-lsi-13" href="./nips-2001-Bayesian_Predictive_Profiles_With_Applications_to_Retail_Transaction_Data.html">41 nips-2001-Bayesian Predictive Profiles With Applications to Retail Transaction Data</a></p>
<p>14 0.31239456 <a title="194-lsi-14" href="./nips-2001-Analysis_of_Sparse_Bayesian_Learning.html">35 nips-2001-Analysis of Sparse Bayesian Learning</a></p>
<p>15 0.30896708 <a title="194-lsi-15" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>16 0.30178693 <a title="194-lsi-16" href="./nips-2001-Minimax_Probability_Machine.html">120 nips-2001-Minimax Probability Machine</a></p>
<p>17 0.29085097 <a title="194-lsi-17" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>18 0.2863645 <a title="194-lsi-18" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>19 0.28254417 <a title="194-lsi-19" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>20 0.28206784 <a title="194-lsi-20" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.286), (14, 0.017), (17, 0.043), (19, 0.034), (27, 0.097), (30, 0.069), (36, 0.019), (38, 0.016), (59, 0.043), (72, 0.065), (79, 0.067), (83, 0.011), (91, 0.153)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81933743 <a title="194-lda-1" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><p>2 0.59786642 <a title="194-lda-2" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>3 0.59729648 <a title="194-lda-3" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>4 0.59547883 <a title="194-lda-4" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>5 0.59473181 <a title="194-lda-5" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>Author: Hilbert J. Kappen, Wim Wiegerinck</p><p>Abstract: The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems. 1</p><p>6 0.59119892 <a title="194-lda-6" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>7 0.59114635 <a title="194-lda-7" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>8 0.58990479 <a title="194-lda-8" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>9 0.58869684 <a title="194-lda-9" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>10 0.58865905 <a title="194-lda-10" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>11 0.58818233 <a title="194-lda-11" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>12 0.58712196 <a title="194-lda-12" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>13 0.58655709 <a title="194-lda-13" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>14 0.58518922 <a title="194-lda-14" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>15 0.58471513 <a title="194-lda-15" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>16 0.58468115 <a title="194-lda-16" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>17 0.58406544 <a title="194-lda-17" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>18 0.58343279 <a title="194-lda-18" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>19 0.5817672 <a title="194-lda-19" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>20 0.58142245 <a title="194-lda-20" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
