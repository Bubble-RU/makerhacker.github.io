<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2001-Model-Free Least-Squares Policy Iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-121" href="#">nips2001-121</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2001-Model-Free Least-Squares Policy Iteration</h1>
<br/><p>Source: <a title="nips-2001-121-pdf" href="http://papers.nips.cc/paper/2134-model-free-least-squares-policy-iteration.pdf">pdf</a></p><p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><p>Reference: <a title="nips-2001-121-reference" href="../nips2001_reference/nips-2001-Model-Free_Least-Squares_Policy_Iteration_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.541), ('lspi', 0.459), ('lsq', 0.382), ('lstd', 0.325), ('bicyc', 0.282), ('it', 0.09), ('reward', 0.088), ('sampl', 0.079), ('mdp', 0.076), ('vh', 0.07), ('episod', 0.067), ('gw', 0.066), ('discount', 0.065), ('ru', 0.061), ('alst', 0.057), ('randl', 0.057), ('reinforc', 0.057), ('reus', 0.051), ('crash', 0.05), ('duk', 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="121-tfidf-1" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>2 0.36768925 <a title="121-tfidf-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>3 0.32636106 <a title="121-tfidf-3" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>4 0.30035898 <a title="121-tfidf-4" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>5 0.24037428 <a title="121-tfidf-5" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>6 0.23349494 <a title="121-tfidf-6" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>7 0.20399487 <a title="121-tfidf-7" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>8 0.19618158 <a title="121-tfidf-8" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>9 0.17935392 <a title="121-tfidf-9" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>10 0.16391592 <a title="121-tfidf-10" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>11 0.12305754 <a title="121-tfidf-11" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>12 0.10224156 <a title="121-tfidf-12" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>13 0.093519256 <a title="121-tfidf-13" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>14 0.085427068 <a title="121-tfidf-14" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>15 0.084029809 <a title="121-tfidf-15" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>16 0.083816759 <a title="121-tfidf-16" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>17 0.062069163 <a title="121-tfidf-17" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>18 0.058927268 <a title="121-tfidf-18" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>19 0.049593098 <a title="121-tfidf-19" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>20 0.045617521 <a title="121-tfidf-20" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.183), (1, 0.044), (2, 0.502), (3, 0.106), (4, -0.074), (5, -0.155), (6, 0.007), (7, 0.006), (8, 0.013), (9, 0.002), (10, -0.048), (11, 0.04), (12, 0.045), (13, -0.004), (14, -0.028), (15, 0.075), (16, 0.006), (17, -0.024), (18, -0.0), (19, 0.032), (20, 0.06), (21, 0.042), (22, 0.007), (23, 0.003), (24, 0.029), (25, 0.041), (26, -0.09), (27, -0.052), (28, 0.02), (29, -0.022), (30, -0.032), (31, -0.008), (32, 0.006), (33, 0.045), (34, -0.019), (35, -0.041), (36, 0.035), (37, 0.025), (38, -0.037), (39, 0.006), (40, -0.062), (41, 0.023), (42, -0.03), (43, -0.044), (44, 0.085), (45, -0.089), (46, 0.033), (47, -0.047), (48, 0.021), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93404132 <a title="121-lsi-1" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>2 0.84819025 <a title="121-lsi-2" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>3 0.83674985 <a title="121-lsi-3" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>4 0.76416504 <a title="121-lsi-4" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>5 0.76392692 <a title="121-lsi-5" href="./nips-2001-Stabilizing_Value_Function_Approximation_with_the_BFBP_Algorithm.html">175 nips-2001-Stabilizing Value Function Approximation with the BFBP Algorithm</a></p>
<p>6 0.75513625 <a title="121-lsi-6" href="./nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">40 nips-2001-Batch Value Function Approximation via Support Vectors</a></p>
<p>7 0.75205243 <a title="121-lsi-7" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>8 0.68788093 <a title="121-lsi-8" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>9 0.68353897 <a title="121-lsi-9" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>10 0.61500907 <a title="121-lsi-10" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>11 0.56323314 <a title="121-lsi-11" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>12 0.53973806 <a title="121-lsi-12" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>13 0.461907 <a title="121-lsi-13" href="./nips-2001-Switch_Packet_Arbitration_via_Queue-Learning.html">177 nips-2001-Switch Packet Arbitration via Queue-Learning</a></p>
<p>14 0.40263423 <a title="121-lsi-14" href="./nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</a></p>
<p>15 0.36727396 <a title="121-lsi-15" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>16 0.35390553 <a title="121-lsi-16" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>17 0.34554604 <a title="121-lsi-17" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>18 0.32266024 <a title="121-lsi-18" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>19 0.26552254 <a title="121-lsi-19" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>20 0.26329252 <a title="121-lsi-20" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (7, 0.273), (14, 0.122), (16, 0.163), (31, 0.034), (50, 0.084), (63, 0.021), (68, 0.019), (77, 0.038), (79, 0.016), (91, 0.042), (92, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73379314 <a title="121-lda-1" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>2 0.64134234 <a title="121-lda-2" href="./nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</a></p>
<p>3 0.63951772 <a title="121-lda-3" href="./nips-2001-Approximate_Dynamic_Programming_via_Linear_Programming.html">36 nips-2001-Approximate Dynamic Programming via Linear Programming</a></p>
<p>4 0.63529325 <a title="121-lda-4" href="./nips-2001-Direct_value-approximation_for_factored_MDPs.html">59 nips-2001-Direct value-approximation for factored MDPs</a></p>
<p>5 0.63437337 <a title="121-lda-5" href="./nips-2001-Predictive_Representations_of_State.html">148 nips-2001-Predictive Representations of State</a></p>
<p>6 0.63099736 <a title="121-lda-6" href="./nips-2001-Rao-Blackwellised_Particle_Filtering_via_Data_Augmentation.html">156 nips-2001-Rao-Blackwellised Particle Filtering via Data Augmentation</a></p>
<p>7 0.62540531 <a title="121-lda-7" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>8 0.61934894 <a title="121-lda-8" href="./nips-2001-Efficient_Resources_Allocation_for_Markov_Decision_Processes.html">67 nips-2001-Efficient Resources Allocation for Markov Decision Processes</a></p>
<p>9 0.61886907 <a title="121-lda-9" href="./nips-2001-Multiagent_Planning_with_Factored_MDPs.html">128 nips-2001-Multiagent Planning with Factored MDPs</a></p>
<p>10 0.61587322 <a title="121-lda-10" href="./nips-2001-Risk_Sensitive_Particle_Filters.html">163 nips-2001-Risk Sensitive Particle Filters</a></p>
<p>11 0.60817444 <a title="121-lda-11" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>12 0.60212159 <a title="121-lda-12" href="./nips-2001-Variance_Reduction_Techniques_for_Gradient_Estimates_in_Reinforcement_Learning.html">195 nips-2001-Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning</a></p>
<p>13 0.60089135 <a title="121-lda-13" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>14 0.59999675 <a title="121-lda-14" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>15 0.59593177 <a title="121-lda-15" href="./nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</a></p>
<p>16 0.59553897 <a title="121-lda-16" href="./nips-2001-An_Efficient%2C_Exact_Algorithm_for_Solving_Tree-Structured_Graphical_Games.html">32 nips-2001-An Efficient, Exact Algorithm for Solving Tree-Structured Graphical Games</a></p>
<p>17 0.59508425 <a title="121-lda-17" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>18 0.58453739 <a title="121-lda-18" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>19 0.58393723 <a title="121-lda-19" href="./nips-2001-Tree-based_reparameterization_for_approximate_inference_on_loopy_graphs.html">192 nips-2001-Tree-based reparameterization for approximate inference on loopy graphs</a></p>
<p>20 0.58331162 <a title="121-lda-20" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
