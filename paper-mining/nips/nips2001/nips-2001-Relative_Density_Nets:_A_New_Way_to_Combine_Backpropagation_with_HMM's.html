<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-162" href="#">nips2001-162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</h1>
<br/><p>Source: <a title="nips-2001-162-pdf" href="http://papers.nips.cc/paper/2137-relative-density-nets-a-new-way-to-combine-backpropagation-with-hmms.pdf">pdf</a></p><p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>Reference: <a title="nips-2001-162-reference" href="../nips2001_reference/nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. [sent-8, score-0.48]
</p><p>2 This leads us to consider substituting other density models. [sent-9, score-0.089]
</p><p>3 We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. [sent-10, score-0.227]
</p><p>4 Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. [sent-11, score-0.152]
</p><p>5 1  Introduction  A standard way of performing classification using a generative model is to divide the training cases into their respective classes and t hen train a set of class conditional models. [sent-12, score-0.229]
</p><p>6 This unsupervised approach to classification is appealing for two reasons. [sent-13, score-0.109]
</p><p>7 It is possible to reduce overfitting, because t he model learns the class-conditional input densities P(xlc) rather t han the input -conditional class probabilities P(clx). [sent-14, score-0.126]
</p><p>8 Also, provided that the model density is a good match to the underlying data density then the decision provided by a probabilistic model is Bayes optimal. [sent-15, score-0.178]
</p><p>9 The problem with this unsupervised approach to using probabilistic models for classification is that, for reasons of computational efficiency and analytical convenience, very simple generative models are typically used and the optimality of the procedure no longer holds. [sent-16, score-0.173]
</p><p>10 For this reason it is usually advantageous to train a classifier discriminatively. [sent-17, score-0.074]
</p><p>11 In this paper we will look specifically at the problem of learning HMM 's for classifying speech sequences. [sent-18, score-0.086]
</p><p>12 It is an application area where the assumption that the HMM is the correct generative model for the data is inaccurate and discriminative methods of training have been successful. [sent-19, score-0.09]
</p><p>13 The first section will give an overview of current methods of discriminatively training HMM classifiers. [sent-20, score-0.092]
</p><p>14 We will then introduce a new type of multi-layer backpropagation network which takes better advantage of the HMM 's for discrimination. [sent-21, score-0.138]
</p><p>15 Finally, we present some simulations comparing the two methods. [sent-22, score-0.021]
</p><p>16 1' S 9 1  c1  1  1  1  ="  [tn] [tn][t n] HMM 's  \V Sequence  Figure 1: An Alphanet with one HMM per class. [sent-23, score-0.031]
</p><p>17 Each computes a score for the sequence and this feeds into a softmax output layer. [sent-24, score-0.207]
</p><p>18 2  Alphanets and Discriminative Learning  The unsupervised way of using an HMM for classifying a collection of sequences is to use the Baum-Welch algorithm [1] to fit one HMM per class. [sent-25, score-0.141]
</p><p>19 Then new sequences are classified by computing the probability of a sequence under each model and assigning it to the one with the highest probability. [sent-26, score-0.086]
</p><p>20 Speech recognition is one of the commonest applications of HMM 's, but unfortunately an HMM is a poor model of the speech production process. [sent-27, score-0.088]
</p><p>21 For this reason speech researchers have looked at the possibility of improving the performance of an HMM classifier by using information from negative examples - examples drawn from classes other than the one which the HMM was meant to model. [sent-28, score-0.136]
</p><p>22 One way of doing this is to compute the mutual information between the class label and the data under the HMM density, and maximize that objective function [2]. [sent-29, score-0.068]
</p><p>23 It was later shown that this procedure could be viewed as a type of neural network (see Figure 1) in which the inputs to the network are the log-probability scores C(Xl:TIH) of the sequence under hidden Markov model H [3]. [sent-30, score-0.329]
</p><p>24 In such a model there is one HMM per class, and the output is a softmax non-linearity:  (1) Training this model by maximizing the log probability of correct classification leads to a classifier which will perform better than an equivalent HMM model trained solely in a unsupervised manner. [sent-31, score-0.413]
</p><p>25 Such an architecture has been termed an "AIphanet" because it may be implemented as a recurrent neural network which mimics the forward pass of the forward-backward algorithm. [sent-32, score-0.209]
</p><p>26 Given a mixture of two Gaussians where we know the component priors P(9) and the component densities P(xl9) then the posterior probability that Gaussian, 90 , generated an observation x , is a logistic function whose argument is the negative log-odds of the two classes [4] . [sent-34, score-0.184]
</p><p>27 This can clearly be seen by rearranging lThe results of the forward pass are the probabilities of the hidden states conditioned on the past observations, or "alphas" in standard HMM terminology. [sent-35, score-0.225]
</p><p>28 4  A New Kind of Discriminative Net  This view of a feedforward network suggests variations in which other kinds of density models are used in place of Gaussians in the input space. [sent-37, score-0.235]
</p><p>29 In particular, instead of performing pairwise comparisons between Gaussians, the units in the first hidden layer can perform pairwise comparisons between the densities of an input sequence under M different HMM's. [sent-38, score-0.672]
</p><p>30 For a given sequence the log-probability of a sequence under each HMM is computed and the difference in log-probability is used as input to the logistic hidden unit. [sent-39, score-0.322]
</p><p>31 2 This is equivalent to computing the posterior responsibilities of a mixture of two HMM's with equal prior probabilities. [sent-40, score-0.058]
</p><p>32 In order to maximally leverage the information captured by the HMM's we use (~) hidden units so that all possible pairs are included. [sent-41, score-0.22]
</p><p>33 The output of a hidden unit h is given by  (7) where we have used (mn) as an index over the set, (~) , of all unordered pairs of the HMM's. [sent-42, score-0.279]
</p><p>34 The results of this hidden layer computation are then combined using a fully connected layer of free weights, W, and finally passed through a soft max function to make the final decision. [sent-43, score-0.454]
</p><p>35 Density Comparator Units  Figure 2: A multi-layer density net with HMM's in the input layer. [sent-45, score-0.152]
</p><p>36 The hidden layer units perform all pairwise comparisons between the HMM 's. [sent-46, score-0.461]
</p><p>37 where we have used u(·) as shorthand for the logistic function, and Pk is the value of the kth output unit. [sent-47, score-0.158]
</p><p>38 Because each unit in the hidden layer takes as input the difference in log-probability of two HMM 's, this can be thought of as a fixed layer of weights connecting each hidden unit to a pair of HMM's with weights of ±l. [sent-49, score-0.764]
</p><p>39 In contrast to the Alphanet , which allocates one HMM to model each class, this network does not require a one-to-one alignment between models and classes and it gets maximum discriminative benefit from the HMM's by comparing all pairs. [sent-50, score-0.227]
</p><p>40 Another benefit of this architecture is that it allows us to use more HMM's than there are classes. [sent-51, score-0.141]
</p><p>41 The unsupervised approach to training HMM classifiers is problematic because it depends on the assumption that a single HMM is a good model of the data and, in the case of speech, this is a poor assumption. [sent-52, score-0.06]
</p><p>42 Training the classifier discriminatively alleviated this drawback and the multi-layer classifier goes even further in this direction by allowing many HMM's to be used to learn the decision boundaries between the classes. [sent-53, score-0.199]
</p><p>43 The intuition here is that many small HMM's can be a far more efficient way to characterize sequences than one big HMM. [sent-54, score-0.076]
</p><p>44 When many small HMM's cooperate to generate sequences, the mutual information between different parts of generated sequences scales linearly with the number of HMM's and only logarithmically with the number of hidden nodes in each HMM [5]. [sent-55, score-0.254]
</p><p>45 5  Derivative Updates for a Relative Density Network  The learning algorithm for an RDN is just the backpropagation algorithm applied to the network architecture as defined in equations 7,8 and 9. [sent-56, score-0.23]
</p><p>46 The output layer is a distribution over class memberships of data point Xl:T, and this is parameterized as a softmax function. [sent-57, score-0.359]
</p><p>47 We minimize the cross-entropy loss function: K  f =  2: tk logpk  (10)  k= l  where Pk is the value of the kth output unit and tk is an indicator variable which is equal to 1 if k is the true class. [sent-58, score-0.266]
</p><p>48 This derivative can be chained with the the derivatives backpropagated from the output to the hidden layer. [sent-60, score-0.432]
</p><p>49 For the final step of the backpropagation procedure we need the derivative of the log-likelihood of each HMM with respect to its parameters. [sent-61, score-0.124]
</p><p>50 In the experiments we use HMM 's with a single, axis-aligned, Gaussian output density per state. [sent-62, score-0.186]
</p><p>51 We use the following notation for the parameters:  • • • • •  A: aij is the transition probability from state i to state j II: 7ri is the initial state prior f. [sent-63, score-0.117]
</p><p>52 /,i: mean vector for state i Vi: vector of variances for state i 1-l: set of HMM parameters {A , II, f. [sent-64, score-0.048]
</p><p>53 /" v}  We also use the variable St to represent the state of the HMM at time t. [sent-65, score-0.024]
</p><p>54 We make use of the property of all latent variable density models that the derivative of the log-likelihood is equal to the expected derivative of the joint log-likelihood under the posterior distribution. [sent-66, score-0.252]
</p><p>55 For an HMM this means that:  O£(Xl:TI1-l) '" 0 o1-l i = ~ P(Sl:Tlxl:T' 1-l) o1-l i log P(Xl:T' Sl:TI1-l)  (14)  Sl:T  The joint likelihood of an HMM is: (logP(Xl:T ' Sl:TI1-l)) = T  L(b81 ,i)log 7ri  + LL(b "jb 8  8  ,_1 ,i)log aij  +  i,j  t=2  ~ ~(b8" i) [-~ ~IOgVi'd ~ ~(Xt'd -  f. [sent-67, score-0.078]
</p><p>56 /,i,d) 2 /Vi,d]  -  + canst  (15)  where (-) denotes expectations under the posterior distribution and (b 8 , ,i) and (b 8 , ,jb8 '_1 ,i) are the expected state occupancies and transitions under this distribution. [sent-68, score-0.081]
</p><p>57 All the necessary expectations are computed by the forward backward algorithm. [sent-69, score-0.05]
</p><p>58 We could take derivatives with respect to this functional directly, but that would require doing constrained gradient descent on the probInstead, we reparameterize the model using a abilities and the variances. [sent-70, score-0.055]
</p><p>59 softmax basis for probability vectors and an exponential basis for the variance parameters. [sent-71, score-0.1]
</p><p>60 _  a' J -  exp(e;; ») (e (a» ) ,  2:  JI  exp  1JI  . [sent-74, score-0.04]
</p><p>61 li ,d)2/Vi ,d -  IJ  (19)  t= l  When chained with the error signal backpropagated from the output, these derivatives give us the direction in which to move the parameters of each HMM in order to increase the log probability of the correct classification of the sequence. [sent-83, score-0.262]
</p><p>62 6  Experiments  To evaluate the relative merits of the RDN, we compared it against an Alphanet on a speaker identification task. [sent-84, score-0.108]
</p><p>63 It consisted of 12 speakers uttering phrases consisting of 6 different sequences of connected digits recorded multiple times (48) over the course of 12 recording sessions. [sent-86, score-0.093]
</p><p>64 The log magnitude filter response was then used as the feature vector for the HMM's. [sent-89, score-0.033]
</p><p>65 This pre-processing reduced the data dimensionality while retaining its spectral structure. [sent-90, score-0.021]
</p><p>66 While mel-cepstral coefficients are typically recommended for use with axis-aligned Gaussians, they destroy the spectral structure of the data, and we would like to allow for the possibility that of the many HMM's some of them will specialize on particular sub-bands of the frequency domain. [sent-91, score-0.021]
</p><p>67 They can do this by treating the variance as a measure of the importance of a particular frequency band - using large variances for unimportant bands, and small ones for bands to which they pay particular attention. [sent-92, score-0.033]
</p><p>68 We compared the RDN with an Alphanet and three other models which were implemented as controls. [sent-93, score-0.021]
</p><p>69 The first of these was a network with a similar architecture to the RDN (as shown in figure 2), except that instead of fixed connections of ±1, the hidden units have a set of adaptable weights to all M of the HMM's. [sent-94, score-0.462]
</p><p>70 We refer to this network as a comparative density net (CDN). [sent-95, score-0.194]
</p><p>71 A second control experiment used an architecture similar to a CDN without the hidden layer, i. [sent-96, score-0.267]
</p><p>72 there is a single layer of adaptable weights directly connecting the HMM's with the softmax output units. [sent-98, score-0.414]
</p><p>73 The CDN-l differs from the Alphanet in that each softmax output unit has adaptable connections to the HMM's and we can vary the number of HMM's, whereas the Alphanet has just one HMM per class directly connected to each softmax output unit. [sent-100, score-0.514]
</p><p>74 Finally, we implemented a version of a network similar to an Alphanet, but using a mixture of Gaussians as the input density model. [sent-101, score-0.203]
</p><p>75 The point of this comparison was to see if the HMM actually achieves a benefit from modelling the temporal aspects of the speaker recognition task. [sent-102, score-0.134]
</p><p>76 In each experiment an RDN constructed out of a set of, M, 4-state HMM's was compared to the four other networks all matched to have the same number of free parameters, except for the MoGnet. [sent-103, score-0.042]
</p><p>77 In the case of the MoGnet, we used the same number of Gaussian mixture models as HMM's in the Alphanet, each with the same number of hidden states. [sent-104, score-0.197]
</p><p>78 6  B RDN  Alphanet  CDN-1  Architecture  MeG net  Architecture  CDN  CDN-1  8  0. [sent-131, score-0.038]
</p><p>79 For the Alphanet and MoGnet we varied the number of states in the HMM's and the Gaussian mixtures, respectively. [sent-135, score-0.053]
</p><p>80 For the CDN model we used the same number of 4-state HMM's as the RDN and varied the number of units in the hidden layer of the network. [sent-136, score-0.392]
</p><p>81 Since the CDN-1 network has no hidden units, we used the same number of HMM's as the RDN and varied the number of states in the HMM. [sent-137, score-0.274]
</p><p>82 All the models were trained using 90 iterations of a conjugate gradient optimization procedure [6] . [sent-139, score-0.042]
</p><p>83 7  Results  The boxplot in figure 3 shows the results of the classification performance on the 10 runs in each of the 4 experiments. [sent-140, score-0.07]
</p><p>84 This indicates that given a classification network with a fixed number of parameters, there is an advantage to using many small HMM 's and using all the pairwise information about an observed sequence, as opposed to using a network with a single large HMM per class. [sent-144, score-0.284]
</p><p>85 In the third experiment involving the MoGnet we see that its performance is comparable to that of the Alphanet. [sent-145, score-0.021]
</p><p>86 This suggests that the HMM's ability to model the temporal structure of the data is not really necessary for the speaker classification task as we have set it Up. [sent-146, score-0.127]
</p><p>87 3 Nevertheless, the performance of both the Alphanet and 3If we had done text-dependent speaker identification, instead of multiple digit phrases  the MoGnet is less than the RDN. [sent-147, score-0.085]
</p><p>88 Unfortunately the CDN and CDN-l networks perform much worse than we expected. [sent-148, score-0.041]
</p><p>89 While we expected these models to perform similarly to the RDN, it seems that the optimization procedure takes much longer with these models. [sent-149, score-0.041]
</p><p>90 This is probably because the small initial weights from the HMM's to the next layer severely attenuate the backpropagated error derivatives that are used to train the HMM's. [sent-150, score-0.298]
</p><p>91 As a result the CDN networks do not converge properly in the time allowed. [sent-151, score-0.021]
</p><p>92 8  Conclusions  We have introduced relative density networks, and shown that this method of discriminatively learning many small density models in place of a single density model per class has benefits in classification performance. [sent-152, score-0.513]
</p><p>93 In addition, there may be a small speed benefit to using many smaller HMM 's compared to a few big ones. [sent-153, score-0.08]
</p><p>94 Computing the probability of a sequence under an HMM is order O(TK 2 ), where T is the length of the sequence and K is the number of hidden states in the network. [sent-154, score-0.257]
</p><p>95 However, this is somewhat counterbalanced by the quadratic growth in the size of the hidden layer as M increases. [sent-156, score-0.294]
</p><p>96 Mercer, "Maximum mutual information of hidden Markov model parameters for speech recognition," in Proceeding of the IEEE International Conference on Acoustics, Speech and Signal Processing, pp. [sent-175, score-0.249]
</p><p>97 Bridle, "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters," in Advances in Neural Information Processing Systems (D. [sent-178, score-0.084]
</p><p>98 Hinton, "Products of hidden Markov models," in Proceedings of Artificial Intelligence and Statistics 2001 (T. [sent-193, score-0.154]
</p><p>99 Matlab conjugate gradient code available from http ://www . [sent-202, score-0.021]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmm', 0.664), ('alphanet', 0.338), ('rdn', 0.338), ('cdn', 0.203), ('hidden', 0.154), ('layer', 0.14), ('mn', 0.105), ('softmax', 0.1), ('architecture', 0.092), ('mognet', 0.09), ('density', 0.089), ('pk', 0.085), ('discriminatively', 0.071), ('backpropagation', 0.071), ('xl', 0.071), ('classification', 0.07), ('network', 0.067), ('qo', 0.067), ('units', 0.066), ('output', 0.066), ('logistic', 0.061), ('speech', 0.06), ('oak', 0.059), ('adaptable', 0.059), ('backpropagated', 0.059), ('speaker', 0.057), ('gaussians', 0.056), ('derivatives', 0.055), ('tk', 0.055), ('classifier', 0.054), ('derivative', 0.053), ('benefit', 0.049), ('pairwise', 0.049), ('discriminative', 0.047), ('bim', 0.045), ('chained', 0.045), ('magnet', 0.045), ('sequences', 0.045), ('aij', 0.045), ('densities', 0.043), ('sl', 0.043), ('sequence', 0.041), ('exp', 0.04), ('unsupervised', 0.039), ('bridle', 0.039), ('unit', 0.039), ('net', 0.038), ('toronto', 0.037), ('brown', 0.036), ('gj', 0.036), ('posterior', 0.036), ('mutual', 0.035), ('class', 0.033), ('bands', 0.033), ('feedforward', 0.033), ('log', 0.033), ('varied', 0.032), ('comparisons', 0.032), ('hinton', 0.031), ('identification', 0.031), ('ow', 0.031), ('kth', 0.031), ('big', 0.031), ('per', 0.031), ('forward', 0.029), ('tn', 0.028), ('phrases', 0.028), ('bin', 0.028), ('recognition', 0.028), ('classifying', 0.026), ('markov', 0.026), ('input', 0.025), ('connecting', 0.025), ('state', 0.024), ('weights', 0.024), ('generative', 0.022), ('classes', 0.022), ('mixture', 0.022), ('networks', 0.021), ('states', 0.021), ('expectations', 0.021), ('pass', 0.021), ('xt', 0.021), ('conjugate', 0.021), ('spectral', 0.021), ('models', 0.021), ('experiment', 0.021), ('performing', 0.021), ('comparing', 0.021), ('training', 0.021), ('indicator', 0.02), ('train', 0.02), ('perform', 0.02), ('relative', 0.02), ('connected', 0.02), ('memberships', 0.02), ('unordered', 0.02), ('alleviated', 0.02), ('hen', 0.02), ('cooperate', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="162-tfidf-1" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>2 0.2472095 <a title="162-tfidf-2" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>3 0.19998382 <a title="162-tfidf-3" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>4 0.19136864 <a title="162-tfidf-4" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>5 0.17025758 <a title="162-tfidf-5" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>6 0.15803149 <a title="162-tfidf-6" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>7 0.14128305 <a title="162-tfidf-7" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>8 0.084090821 <a title="162-tfidf-8" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>9 0.08294867 <a title="162-tfidf-9" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>10 0.080686942 <a title="162-tfidf-10" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>11 0.079603337 <a title="162-tfidf-11" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>12 0.078636147 <a title="162-tfidf-12" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>13 0.078291059 <a title="162-tfidf-13" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>14 0.070848592 <a title="162-tfidf-14" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>15 0.069599375 <a title="162-tfidf-15" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>16 0.068217427 <a title="162-tfidf-16" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<p>17 0.067924581 <a title="162-tfidf-17" href="./nips-2001-Multiplicative_Updates_for_Classification_by_Mixture_Models.html">129 nips-2001-Multiplicative Updates for Classification by Mixture Models</a></p>
<p>18 0.066977777 <a title="162-tfidf-18" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>19 0.06657017 <a title="162-tfidf-19" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>20 0.066213645 <a title="162-tfidf-20" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.195), (1, 0.004), (2, -0.012), (3, -0.084), (4, -0.285), (5, 0.077), (6, 0.198), (7, -0.068), (8, -0.112), (9, -0.046), (10, 0.007), (11, 0.119), (12, 0.061), (13, 0.161), (14, 0.006), (15, -0.037), (16, -0.056), (17, -0.029), (18, 0.2), (19, -0.018), (20, 0.114), (21, 0.197), (22, -0.042), (23, -0.015), (24, -0.031), (25, 0.125), (26, -0.11), (27, 0.007), (28, -0.215), (29, 0.017), (30, -0.077), (31, -0.102), (32, -0.018), (33, -0.053), (34, 0.015), (35, -0.014), (36, -0.011), (37, -0.073), (38, -0.036), (39, 0.069), (40, 0.041), (41, 0.024), (42, 0.026), (43, -0.068), (44, -0.078), (45, -0.03), (46, 0.053), (47, 0.003), (48, -0.015), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97105312 <a title="162-lsi-1" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>2 0.73071432 <a title="162-lsi-2" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>3 0.68874109 <a title="162-lsi-3" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><p>4 0.68263555 <a title="162-lsi-4" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>5 0.65071517 <a title="162-lsi-5" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>6 0.4632605 <a title="162-lsi-6" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>7 0.43650442 <a title="162-lsi-7" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>8 0.416347 <a title="162-lsi-8" href="./nips-2001-On_Discriminative_vs._Generative_Classifiers%3A_A_comparison_of_logistic_regression_and_naive_Bayes.html">133 nips-2001-On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes</a></p>
<p>9 0.4133943 <a title="162-lsi-9" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>10 0.35386258 <a title="162-lsi-10" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>11 0.33827364 <a title="162-lsi-11" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>12 0.33035752 <a title="162-lsi-12" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>13 0.3209087 <a title="162-lsi-13" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>14 0.31253785 <a title="162-lsi-14" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>15 0.30259266 <a title="162-lsi-15" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>16 0.3021909 <a title="162-lsi-16" href="./nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</a></p>
<p>17 0.28348815 <a title="162-lsi-17" href="./nips-2001-Geometrical_Singularities_in_the_Neuromanifold_of_Multilayer_Perceptrons.html">83 nips-2001-Geometrical Singularities in the Neuromanifold of Multilayer Perceptrons</a></p>
<p>18 0.27455398 <a title="162-lsi-18" href="./nips-2001-Tempo_tracking_and_rhythm_quantization_by_sequential_Monte_Carlo.html">179 nips-2001-Tempo tracking and rhythm quantization by sequential Monte Carlo</a></p>
<p>19 0.26840016 <a title="162-lsi-19" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>20 0.26233724 <a title="162-lsi-20" href="./nips-2001-Fast%2C_Large-Scale_Transformation-Invariant_Clustering.html">75 nips-2001-Fast, Large-Scale Transformation-Invariant Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.025), (17, 0.026), (18, 0.252), (19, 0.04), (27, 0.121), (30, 0.109), (38, 0.023), (59, 0.026), (72, 0.059), (79, 0.053), (83, 0.026), (88, 0.011), (91, 0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85761851 <a title="162-lda-1" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>Author: Antonino Casile, Michele Rucci</p><p>Abstract: Neural activity appears to be a crucial component for shaping the receptive ﬁelds of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V1 neurons are reﬁned by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the reﬁnement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and ﬁxational eye movements, such as microsaccades, tremor and ocular drift. The speciﬁc patterns of activity required for a quantitatively accurate development of simple cell receptive ﬁelds with segregated ON and OFF subregions were observed during ﬁxational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual ﬁxation in the reﬁnement of orientation selectivity.</p><p>same-paper 2 0.81040621 <a title="162-lda-2" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><p>3 0.65009534 <a title="162-lda-3" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>4 0.64993322 <a title="162-lda-4" href="./nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">102 nips-2001-KLD-Sampling: Adaptive Particle Filters</a></p>
<p>Author: Dieter Fox</p><p>Abstract: Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique.</p><p>5 0.64973038 <a title="162-lda-5" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>Author: Paul Viola, Michael Jones</p><p>Abstract: This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed (e.g. face detection or database retrieval). In such domains a cascade of simple classiﬁers each trained to achieve high detection rates and modest false positive rates can yield a ﬁnal detector with many desirable features: including high detection rates, very low false positive rates, and fast performance. Achieving extremely high detection rates, rather than low error, is not a task typically addressed by machine learning algorithms. We propose a new variant of AdaBoost as a mechanism for training the simple classiﬁers used in the cascade. Experimental results in the domain of face detection show the training algorithm yields signiﬁcant improvements in performance over conventional AdaBoost. The ﬁnal face detection system can process 15 frames per second, achieves over 90% detection, and a false positive rate of 1 in a 1,000,000.</p><p>6 0.64837062 <a title="162-lda-6" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>7 0.64721787 <a title="162-lda-7" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>8 0.64546734 <a title="162-lda-8" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<p>9 0.64462692 <a title="162-lda-9" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>10 0.64388418 <a title="162-lda-10" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>11 0.6436168 <a title="162-lda-11" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>12 0.64361334 <a title="162-lda-12" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>13 0.64285135 <a title="162-lda-13" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>14 0.64252472 <a title="162-lda-14" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>15 0.64183116 <a title="162-lda-15" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>16 0.64182246 <a title="162-lda-16" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>17 0.63853037 <a title="162-lda-17" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>18 0.63797402 <a title="162-lda-18" href="./nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">121 nips-2001-Model-Free Least-Squares Policy Iteration</a></p>
<p>19 0.63699013 <a title="162-lda-19" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>20 0.63682926 <a title="162-lda-20" href="./nips-2001-The_Method_of_Quantum_Clustering.html">185 nips-2001-The Method of Quantum Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
