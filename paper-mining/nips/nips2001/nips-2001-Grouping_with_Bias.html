<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 nips-2001-Grouping with Bias</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-89" href="#">nips2001-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 nips-2001-Grouping with Bias</h1>
<br/><p>Source: <a title="nips-2001-89-pdf" href="http://papers.nips.cc/paper/2013-grouping-with-bias.pdf">pdf</a></p><p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>Reference: <a title="nips-2001-89-reference" href="../nips2001_reference/nips-2001-Grouping_with_Bias_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pixel', 0.35), ('cut', 0.341), ('seg', 0.325), ('group', 0.298), ('ncut', 0.261), ('partit', 0.187), ('nod', 0.148), ('xt', 0.139), ('bia', 0.137), ('foreground', 0.136), ('strips', 0.13), ('eigenvect', 0.121), ('graph', 0.12), ('im', 0.114), ('affin', 0.108), ('constraint', 0.106), ('criter', 0.093), ('vertex', 0.086), ('neighbo', 0.086), ('px', 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="89-tfidf-1" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>2 0.21269868 <a title="89-tfidf-2" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>Author: Chris Stauffer, Erik Miller, Kinh Tieu</p><p>Abstract: Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. 1</p><p>3 0.19236736 <a title="89-tfidf-3" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>4 0.17987353 <a title="89-tfidf-4" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>Author: Nello Cristianini, John Shawe-Taylor, Jaz S. Kandola</p><p>Abstract: In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results. 1</p><p>5 0.1768934 <a title="89-tfidf-5" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>Author: Andrew Y. Ng, Michael I. Jordan, Yair Weiss</p><p>Abstract: Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1</p><p>6 0.17152409 <a title="89-tfidf-6" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>7 0.15072 <a title="89-tfidf-7" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>8 0.14788988 <a title="89-tfidf-8" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>9 0.1372344 <a title="89-tfidf-9" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>10 0.13405325 <a title="89-tfidf-10" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>11 0.12927146 <a title="89-tfidf-11" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>12 0.12205985 <a title="89-tfidf-12" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>13 0.12087741 <a title="89-tfidf-13" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>14 0.11551032 <a title="89-tfidf-14" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>15 0.11288634 <a title="89-tfidf-15" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>16 0.10855908 <a title="89-tfidf-16" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>17 0.10640454 <a title="89-tfidf-17" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>18 0.10143829 <a title="89-tfidf-18" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>19 0.09735734 <a title="89-tfidf-19" href="./nips-2001-Face_Recognition_Using_Kernel_Methods.html">74 nips-2001-Face Recognition Using Kernel Methods</a></p>
<p>20 0.094308615 <a title="89-tfidf-20" href="./nips-2001-Thin_Junction_Trees.html">190 nips-2001-Thin Junction Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.274), (1, 0.013), (2, -0.053), (3, 0.115), (4, 0.229), (5, -0.001), (6, -0.107), (7, -0.121), (8, 0.115), (9, -0.022), (10, -0.045), (11, -0.034), (12, -0.251), (13, -0.165), (14, -0.086), (15, 0.029), (16, -0.0), (17, 0.107), (18, 0.042), (19, -0.145), (20, 0.038), (21, 0.054), (22, -0.134), (23, -0.017), (24, 0.035), (25, -0.112), (26, -0.001), (27, 0.009), (28, -0.006), (29, 0.036), (30, -0.181), (31, -0.08), (32, -0.125), (33, -0.043), (34, 0.068), (35, -0.011), (36, 0.036), (37, -0.119), (38, -0.087), (39, -0.019), (40, -0.022), (41, 0.103), (42, -0.0), (43, -0.095), (44, 0.095), (45, 0.013), (46, -0.1), (47, 0.018), (48, -0.061), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96979862 <a title="89-lsi-1" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>2 0.61143136 <a title="89-lsi-2" href="./nips-2001-Transform-invariant_Image_Decomposition_with_Similarity_Templates.html">191 nips-2001-Transform-invariant Image Decomposition with Similarity Templates</a></p>
<p>Author: Chris Stauffer, Erik Miller, Kinh Tieu</p><p>Abstract: Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-speciﬁc image set enabling a decomposition of that object class into component regions. 1</p><p>3 0.59286481 <a title="89-lsi-3" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><p>4 0.54827845 <a title="89-lsi-4" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>5 0.51634771 <a title="89-lsi-5" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>6 0.50804317 <a title="89-lsi-6" href="./nips-2001-Laplacian_Eigenmaps_and_Spectral_Techniques_for_Embedding_and_Clustering.html">106 nips-2001-Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</a></p>
<p>7 0.47336751 <a title="89-lsi-7" href="./nips-2001-A_Neural_Oscillator_Model_of_Auditory_Selective_Attention.html">14 nips-2001-A Neural Oscillator Model of Auditory Selective Attention</a></p>
<p>8 0.47147262 <a title="89-lsi-8" href="./nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</a></p>
<p>9 0.45015272 <a title="89-lsi-9" href="./nips-2001-The_Fidelity_of_Local_Ordinal_Encoding.html">182 nips-2001-The Fidelity of Local Ordinal Encoding</a></p>
<p>10 0.43806085 <a title="89-lsi-10" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>11 0.39826936 <a title="89-lsi-11" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>12 0.39284062 <a title="89-lsi-12" href="./nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</a></p>
<p>13 0.37426162 <a title="89-lsi-13" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>14 0.36916849 <a title="89-lsi-14" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>15 0.35685927 <a title="89-lsi-15" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>16 0.35619527 <a title="89-lsi-16" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>17 0.35278875 <a title="89-lsi-17" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<p>18 0.35240912 <a title="89-lsi-18" href="./nips-2001-The_Noisy_Euclidean_Traveling_Salesman_Problem_and_Learning.html">186 nips-2001-The Noisy Euclidean Traveling Salesman Problem and Learning</a></p>
<p>19 0.32581416 <a title="89-lsi-19" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>20 0.3237417 <a title="89-lsi-20" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.037), (16, 0.18), (31, 0.068), (36, 0.021), (50, 0.078), (63, 0.032), (77, 0.066), (79, 0.034), (81, 0.033), (91, 0.073), (92, 0.096), (94, 0.187)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82208323 <a title="89-lda-1" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>Author: Stella X. Yu, Jianbo Shi</p><p>Abstract: With the optimization of pattern discrimination as a goal, graph partitioning approaches often lack the capability to integrate prior knowledge to guide grouping. In this paper, we consider priors from unitary generative models, partially labeled data and spatial attention. These priors are modelled as constraints in the solution space. By imposing uniformity condition on the constraints, we restrict the feasible space to one of smooth solutions. A subspace projection method is developed to solve this constrained eigenproblema We demonstrate that simple priors can greatly improve image segmentation results. 1</p><p>2 0.82029063 <a title="89-lda-2" href="./nips-2001-Information_Geometrical_Framework_for_Analyzing_Belief_Propagation_Decoder.html">98 nips-2001-Information Geometrical Framework for Analyzing Belief Propagation Decoder</a></p>
<p>Author: Shiro Ikeda, Toshiyuki Tanaka, Shun-ichi Amari</p><p>Abstract: The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difﬁcult to obtain the true “belief” by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.</p><p>3 0.76492989 <a title="89-lda-3" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>4 0.75809938 <a title="89-lda-4" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>Author: Bram Bakker</p><p>Abstract: This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task. 1</p><p>5 0.75660276 <a title="89-lda-5" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>Author: Xiaohui Xie, Martin A. Giese</p><p>Abstract: Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network’s nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by speciﬁc spatiotemporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.</p><p>6 0.75619787 <a title="89-lda-6" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>7 0.75528795 <a title="89-lda-7" href="./nips-2001-Means%2C_Correlations_and_Bounds.html">119 nips-2001-Means, Correlations and Bounds</a></p>
<p>8 0.7514019 <a title="89-lda-8" href="./nips-2001-Exact_differential_equation_population_dynamics_for_integrate-and-fire_neurons.html">72 nips-2001-Exact differential equation population dynamics for integrate-and-fire neurons</a></p>
<p>9 0.75136125 <a title="89-lda-9" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>10 0.75077486 <a title="89-lda-10" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>11 0.74962997 <a title="89-lda-11" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>12 0.74961036 <a title="89-lda-12" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>13 0.74852562 <a title="89-lda-13" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>14 0.74773246 <a title="89-lda-14" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>15 0.74726939 <a title="89-lda-15" href="./nips-2001-Cobot%3A_A_Social_Reinforcement_Learning_Agent.html">51 nips-2001-Cobot: A Social Reinforcement Learning Agent</a></p>
<p>16 0.74721831 <a title="89-lda-16" href="./nips-2001-Unsupervised_Learning_of_Human_Motion_Models.html">193 nips-2001-Unsupervised Learning of Human Motion Models</a></p>
<p>17 0.74686849 <a title="89-lda-17" href="./nips-2001-Activity_Driven_Adaptive_Stochastic_Resonance.html">27 nips-2001-Activity Driven Adaptive Stochastic Resonance</a></p>
<p>18 0.74540877 <a title="89-lda-18" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>19 0.74477029 <a title="89-lda-19" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>20 0.74454147 <a title="89-lda-20" href="./nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
