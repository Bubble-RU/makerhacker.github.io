<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-20" href="#">nips2001-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</h1>
<br/><p>Source: <a title="nips-2001-20-pdf" href="http://papers.nips.cc/paper/1951-a-sequence-kernel-and-its-application-to-speaker-recognition.pdf">pdf</a></p><p>Author: William M. Campbell</p><p>Abstract: A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expansion kernel reduces classiﬁer model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error training. Training using standard support vector machine methodology gives accuracy that signiﬁcantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task.</p><p>Reference: <a title="nips-2001-20-reference" href="../nips2001_reference/nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('speak', 0.489), ('eer', 0.29), ('mse', 0.242), ('enrol', 0.203), ('frr', 0.203), ('impost', 0.203), ('nap', 0.203), ('ver', 0.189), ('sess', 0.172), ('speech', 0.164), ('scor', 0.162), ('cat', 0.155), ('kernel', 0.154), ('polynom', 0.142), ('qv', 0.126), ('gv', 0.115), ('svm', 0.112), ('recognit', 0.11), ('campbel', 0.107), ('pool', 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="20-tfidf-1" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>Author: William M. Campbell</p><p>Abstract: A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expansion kernel reduces classiﬁer model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error training. Training using standard support vector machine methodology gives accuracy that signiﬁcantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task.</p><p>2 0.27155697 <a title="20-tfidf-2" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>3 0.25009638 <a title="20-tfidf-3" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>Author: John R. Hershey, Michael Casey</p><p>Abstract: It is well known that under noisy conditions we can hear speech much more clearly when we read the speaker's lips. This suggests the utility of audio-visual information for the task of speech enhancement. We propose a method to exploit audio-visual cues to enable speech separation under non-stationary noise and with a single microphone. We revise and extend HMM-based speech enhancement techniques, in which signal and noise models are factori ally combined, to incorporate visual lip information and employ novel signal HMMs in which the dynamics of narrow-band and wide band components are factorial. We avoid the combinatorial explosion in the factorial model by using a simple approximate inference technique to quickly estimate the clean signals in a mixture. We present a preliminary evaluation of this approach using a small-vocabulary audio-visual database, showing promising improvements in machine intelligibility for speech enhanced using audio and visual information. 1</p><p>4 0.18888868 <a title="20-tfidf-4" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>5 0.15730184 <a title="20-tfidf-5" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><p>6 0.13728675 <a title="20-tfidf-6" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>7 0.13244386 <a title="20-tfidf-7" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>8 0.11990844 <a title="20-tfidf-8" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>9 0.11929571 <a title="20-tfidf-9" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>10 0.11915641 <a title="20-tfidf-10" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>11 0.11601435 <a title="20-tfidf-11" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>12 0.11575279 <a title="20-tfidf-12" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>13 0.10923786 <a title="20-tfidf-13" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>14 0.10431883 <a title="20-tfidf-14" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>15 0.10354453 <a title="20-tfidf-15" href="./nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</a></p>
<p>16 0.098891832 <a title="20-tfidf-16" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>17 0.09779802 <a title="20-tfidf-17" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>18 0.096176691 <a title="20-tfidf-18" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>19 0.0953779 <a title="20-tfidf-19" href="./nips-2001-Online_Learning_with_Kernels.html">139 nips-2001-Online Learning with Kernels</a></p>
<p>20 0.091997534 <a title="20-tfidf-20" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.245), (1, 0.111), (2, -0.127), (3, -0.16), (4, -0.163), (5, -0.188), (6, 0.05), (7, 0.144), (8, 0.022), (9, 0.044), (10, 0.002), (11, -0.089), (12, 0.126), (13, -0.155), (14, 0.066), (15, 0.012), (16, 0.059), (17, -0.006), (18, -0.034), (19, 0.049), (20, 0.031), (21, 0.012), (22, -0.006), (23, 0.076), (24, -0.048), (25, -0.052), (26, -0.002), (27, 0.018), (28, 0.078), (29, -0.028), (30, -0.009), (31, -0.065), (32, -0.014), (33, 0.004), (34, 0.117), (35, 0.008), (36, 0.035), (37, 0.089), (38, -0.109), (39, 0.063), (40, 0.006), (41, 0.109), (42, 0.003), (43, -0.026), (44, 0.091), (45, -0.078), (46, 0.064), (47, 0.067), (48, 0.033), (49, -0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92617589 <a title="20-lsi-1" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>Author: William M. Campbell</p><p>Abstract: A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expansion kernel reduces classiﬁer model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error training. Training using standard support vector machine methodology gives accuracy that signiﬁcantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task.</p><p>2 0.80332869 <a title="20-lsi-2" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>Author: Hiroshi Shimodaira, Ken-ichi Noma, Mitsuru Nakai, Shigeki Sagayama</p><p>Abstract: A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classiﬁcation algorithms can be employed without further modiﬁcations. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs). 1</p><p>3 0.67747843 <a title="20-lsi-3" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>Author: N. Smith, Mark Gales</p><p>Abstract: An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood. 1</p><p>4 0.62945062 <a title="20-lsi-4" href="./nips-2001-Speech_Recognition_with_Missing_Data_using_Recurrent_Neural_Nets.html">173 nips-2001-Speech Recognition with Missing Data using Recurrent Neural Nets</a></p>
<p>Author: S. Parveen, P. Green</p><p>Abstract: In the ‘missing data’ approach to improving the robustness of automatic speech recognition to added noise, an initial process identiﬁes spectraltemporal regions which are dominated by the speech source. The remaining regions are considered to be ‘missing’. In this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks. In contrast to methods based on Hidden Markov Models, RNNs allow us to make use of long-term time constraints and to make the problems of classiﬁcation with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task.</p><p>5 0.62370557 <a title="20-lsi-5" href="./nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</a></p>
<p>Author: Ji Zhu, Trevor Hastie</p><p>Abstract: The support vector machine (SVM) is known for its good performance in binary classiﬁcation, but its extension to multi-class classiﬁcation is still an on-going research issue. In this paper, we propose a new approach for classiﬁcation, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in binary classiﬁcation, but also can naturally be generalized to the multi-class case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the “support points” of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.</p><p>6 0.61935955 <a title="20-lsi-6" href="./nips-2001-Audio-Visual_Sound_Separation_Via_Hidden_Markov_Models.html">39 nips-2001-Audio-Visual Sound Separation Via Hidden Markov Models</a></p>
<p>7 0.55371743 <a title="20-lsi-7" href="./nips-2001-ALGONQUIN_-_Learning_Dynamic_Noise_Models_From_Noisy_Speech_for_Robust_Speech_Recognition.html">4 nips-2001-ALGONQUIN - Learning Dynamic Noise Models From Noisy Speech for Robust Speech Recognition</a></p>
<p>8 0.51685506 <a title="20-lsi-8" href="./nips-2001-Adaptive_Sparseness_Using_Jeffreys_Prior.html">29 nips-2001-Adaptive Sparseness Using Jeffreys Prior</a></p>
<p>9 0.51450777 <a title="20-lsi-9" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>10 0.50992411 <a title="20-lsi-10" href="./nips-2001-Sequential_Noise_Compensation_by_Sequential_Monte_Carlo_Method.html">168 nips-2001-Sequential Noise Compensation by Sequential Monte Carlo Method</a></p>
<p>11 0.50421518 <a title="20-lsi-11" href="./nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</a></p>
<p>12 0.47623643 <a title="20-lsi-12" href="./nips-2001-Incorporating_Invariances_in_Non-Linear_Support_Vector_Machines.html">92 nips-2001-Incorporating Invariances in Non-Linear Support Vector Machines</a></p>
<p>13 0.47443274 <a title="20-lsi-13" href="./nips-2001-Reducing_multiclass_to_binary_by_coupling_probability_estimates.html">159 nips-2001-Reducing multiclass to binary by coupling probability estimates</a></p>
<p>14 0.47027516 <a title="20-lsi-14" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<p>15 0.4689765 <a title="20-lsi-15" href="./nips-2001-Kernel_Machines_and_Boolean_Functions.html">105 nips-2001-Kernel Machines and Boolean Functions</a></p>
<p>16 0.43468401 <a title="20-lsi-16" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>17 0.43052322 <a title="20-lsi-17" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>18 0.42195889 <a title="20-lsi-18" href="./nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</a></p>
<p>19 0.42060408 <a title="20-lsi-19" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>20 0.41966039 <a title="20-lsi-20" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.014), (16, 0.224), (22, 0.343), (31, 0.057), (50, 0.058), (63, 0.048), (77, 0.031), (79, 0.015), (81, 0.011), (91, 0.034), (92, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76542675 <a title="20-lda-1" href="./nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</a></p>
<p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><p>2 0.75422871 <a title="20-lda-2" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>Author: Shih-Chii Liu, Jörg Kramer, Giacomo Indiveri, Tobi Delbrück, Rodney J. Douglas</p><p>Abstract: We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-ﬁre neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be conﬁgured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.</p><p>same-paper 3 0.74627054 <a title="20-lda-3" href="./nips-2001-A_Sequence_Kernel_and_its_Application_to_Speaker_Recognition.html">20 nips-2001-A Sequence Kernel and its Application to Speaker Recognition</a></p>
<p>Author: William M. Campbell</p><p>Abstract: A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expansion kernel reduces classiﬁer model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error training. Training using standard support vector machine methodology gives accuracy that signiﬁcantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task.</p><p>4 0.62574548 <a title="20-lda-4" href="./nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</a></p>
<p>Author: Michael Collins, S. Dasgupta, Robert E. Schapire</p><p>Abstract: Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.</p><p>5 0.58874881 <a title="20-lda-5" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classiﬁcation with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classiﬁcation. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classiﬁcation problems.</p><p>6 0.58793557 <a title="20-lda-6" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>7 0.58786976 <a title="20-lda-7" href="./nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</a></p>
<p>8 0.58785981 <a title="20-lda-8" href="./nips-2001-Global_Coordination_of_Local_Linear_Models.html">84 nips-2001-Global Coordination of Local Linear Models</a></p>
<p>9 0.58643758 <a title="20-lda-9" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>10 0.58538944 <a title="20-lda-10" href="./nips-2001-A_New_Discriminative_Kernel_From_Probabilistic_Models.html">15 nips-2001-A New Discriminative Kernel From Probabilistic Models</a></p>
<p>11 0.58506304 <a title="20-lda-11" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>12 0.58430666 <a title="20-lda-12" href="./nips-2001-Dynamic_Time-Alignment_Kernel_in_Support_Vector_Machine.html">63 nips-2001-Dynamic Time-Alignment Kernel in Support Vector Machine</a></p>
<p>13 0.5842886 <a title="20-lda-13" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>14 0.58404183 <a title="20-lda-14" href="./nips-2001-Speech_Recognition_using_SVMs.html">172 nips-2001-Speech Recognition using SVMs</a></p>
<p>15 0.58394259 <a title="20-lda-15" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<p>16 0.58384627 <a title="20-lda-16" href="./nips-2001-A_General_Greedy_Approximation_Algorithm_with_Applications.html">8 nips-2001-A General Greedy Approximation Algorithm with Applications</a></p>
<p>17 0.5824635 <a title="20-lda-17" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>18 0.58242238 <a title="20-lda-18" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>19 0.58219177 <a title="20-lda-19" href="./nips-2001-Active_Learning_in_the_Drug_Discovery_Process.html">25 nips-2001-Active Learning in the Drug Discovery Process</a></p>
<p>20 0.58195472 <a title="20-lda-20" href="./nips-2001-Spectral_Kernel_Methods_for_Clustering.html">170 nips-2001-Spectral Kernel Methods for Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
