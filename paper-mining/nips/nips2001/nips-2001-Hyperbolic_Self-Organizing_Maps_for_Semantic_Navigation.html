<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-90" href="#">nips2001-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</h1>
<br/><p>Source: <a title="nips-2001-90-pdf" href="http://papers.nips.cc/paper/2029-hyperbolic-self-organizing-maps-for-semantic-navigation.pdf">pdf</a></p><p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>Reference: <a title="nips-2001-90-reference" href="../nips2001_reference/nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. [sent-5, score-0.155]
</p><p>2 We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. [sent-6, score-0.658]
</p><p>3 The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. [sent-7, score-0.665]
</p><p>4 We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods. [sent-8, score-0.262]
</p><p>5 So far, the overwhelming majority of SOM approaches have taken it for granted to use a ﬂat space as their data model and, motivated by its convenience for visualization, have favored the (suitably discretized) euclidean plane as their chief “canvas” for the generated mappings. [sent-10, score-0.17]
</p><p>6 However, even if our thinking is deeply entrenched with euclidean space, an obvious limiting factor is the rather restricted neighborhood that “ﬁts” around a point on a euclidean 2D surface. [sent-11, score-0.172]
</p><p>7 They are characterized by uniform negative curvature, resulting in a geometry such that the size of a neighborhood around a point increases exponentially with its radius . [sent-13, score-0.155]
</p><p>8 Consequently, we suggest to use hyperbolic spaces also in conjunction with the SOM. [sent-15, score-0.52]
</p><p>9 The lattice structure of the resulting hyperbolic SOMs (HSOMs) is based on a tesselation of the hyperbolic space (in 2D or 3D) and the lattice neighborhood reﬂects the hyperbolic distance metric that is responsible for the non-intuitive properties of hyperbolic spaces. [sent-16, score-2.464]
</p><p>10 After a brief introduction to the construction of hyperbolic spaces we describe several computer experiments that indicate that the HSOM offers new interesting perspectives in the ﬁeld of text-mining. [sent-17, score-0.52]
</p><p>11 [7]), and the relationships for the area and the circumference of a circle of radius are given by     ¡     )# ¨ 0! [sent-21, score-0.133]
</p><p>12 It is this property that was observed in [3, 4] to make hyperbolic spaces extremely useful for accommodating hierarchical structures. [sent-24, score-0.52]
</p><p>13 ) lattice into “ﬂat space” in order to be able to inspect the generated maps. [sent-27, score-0.135]
</p><p>14 , distance preserving) embedding of the hyperbolic plane into a “ﬂat” space, we may use a Minkowski space [8]. [sent-31, score-0.669]
</p><p>15 Under this embedding, the hyperbolic plane about the -axis. [sent-36, score-0.565]
</p><p>16 The N Klein model is obtained by projecting the points C onto the plane along rays passing of through the origin (see Fig. [sent-39, score-0.173]
</p><p>17 S The Poincar´ Model results if we add two fure ther steps: ﬁrst a perpendicular projection of Figure 1: Construction steps underlying the Klein Model onto the (“northern”) surface Klein and Poincar´ -models of the space H2 e of the unit sphere centered at the origin (e. [sent-45, score-0.17]
</p><p>18 , ), and then a stereographic projection of the “northern” hemisphere onto the unit circle about the origin in the ground plane (point ). [sent-47, score-0.261]
</p><p>19 The neighborhood of the H2 origin is mapped almost faithfully (up to a linear shrinkage factor of 2), while more distant regions become increasingly “squeezed”. [sent-52, score-0.217]
</p><p>20 , (sufﬁciently small) shapes painted onto H2 are not deformed, only their size shrinks with increasing distance from the origin. [sent-55, score-0.106]
</p><p>21 By translating the original H2, the ﬁsh-eye-fovea can be moved to any other part of H2, allowing to selectively zoom-in on interesting portions of a map painted on H2 while still keeping a coarser view of its surrounding context. [sent-56, score-0.111]
</p><p>22 2 Tesselations of the Hyperbolic Plane To complete the set-up for a hyperbolic SOM we still need an equivalent of a regular grid in the hyperbolic plane. [sent-58, score-0.979]
</p><p>23 For the hyperbolic plane there exist an inﬁnite number of tesselations with congruent polygons such that each grid point is surrounded by the same number of neighbors [9, 10]. [sent-59, score-0.713]
</p><p>24 2 shows two example tesselations (for the minimal value of and for ), using the Poincar´ model for their visualization. [sent-61, score-0.123]
</p><p>25 While these tesselations e appear non-uniform, this is only due to the ﬁsh-eye effect of the Poincar´ projection. [sent-62, score-0.123]
</p><p>26 In the e original H2, each tesselation triangle has the same size. [sent-63, score-0.153]
</p><p>27 ¡ ¢¢      s A¢   a  One way to generate these tesselations algorithmically is by repeated application of a suitable set of generators of their symmetry group to a (suitably sized, cf. [sent-64, score-0.123]
</p><p>28 Figure 2: Regular triangle tesselations of the hyperbolic plane, projected into the unit disk using the Poincar´ mapping. [sent-67, score-0.71]
</p><p>29 The left tesselation shows the case where the minimal number ( e ) of equilateral triangles meet at each vertex, the right ﬁgure was constructed with . [sent-68, score-0.123]
</p><p>30 We organize the nodes of a lattice as described above in “rings” around an origin node. [sent-71, score-0.25]
</p><p>31 The numbers of nodes of such a lattice grows very rapidly (asymptotically exponentially) with the chosen lattice radius (its number of rings). [sent-72, score-0.418]
</p><p>32 Each lattice node carries a prototype vector from some -dimensional feature space (if we wish to make any non-standard assumptions about the metric structure of this space, we would build this into the distance metric that is used for determining the best-match node). [sent-74, score-0.377]
</p><p>33 #   $   4 &BX;¨¢  © "   #    BC 5   §¨¢    ©   ¦ &#¥£$    ¡    repeatedly determining the winner node and adjusting all nodes lattice neighborhood around according to the familiar rule  in a radial     (4) with . [sent-79, score-0.421]
</p><p>34 However, since we now work on a hyperbolic lattice, we have to determine both the neighborhood and the (squared) node distance according to the natural metric that is inherited by the hyperbolic lattice. [sent-80, score-1.207]
</p><p>35 # $  4      The simplest way to do this is to keep with each node a complex number to identify its position in the Poincar´ model. [sent-81, score-0.117]
</p><p>36 The node distance is then given (using the Poincar´ model, e e see e. [sent-82, score-0.155]
</p><p>37 arctanh  (5)  " ¢4  #  $ £ ¡  The neighborhood can be deﬁned as the subset of nodes within a certain graph distance (which is chosen as a small multiple of the neighborhood radius ) around . [sent-92, score-0.308]
</p><p>38 A major example of the use of the SOM for text mining is the WEBSOM project [2]. [sent-94, score-0.181]
</p><p>39 1 Text Categorization In order to apply the HSOM to natural text categorization, i. [sent-96, score-0.155]
</p><p>40 the assignment of natural language documents to a number of predeﬁned categories, we follow the widely used vector-space-model of Information Retrieval (IR). [sent-98, score-0.233]
</p><p>41 For each document we construct a fea, where the components are determined by the frequency of which term ture vector occurs in that document. [sent-99, score-0.13]
</p><p>42 The HSOM can be utilized for text categorization in the following manner. [sent-103, score-0.262]
</p><p>43 During the second step, the training set is mapped onto the HSOM lattice. [sent-105, score-0.164]
</p><p>44 To this end, for each training example its best match node is determined such that     (7)  F @ 4 D B $ E! [sent-106, score-0.147]
</p><p>45 @4  @ # A4   (  where denotes the feature vector of document , as described above. [sent-118, score-0.103]
</p><p>46 After all examples have been presented to the net, each node is labelled with the union of all categories that belonged to the documents that were mapped to this node. [sent-119, score-0.563]
</p><p>47 A new, unknown text is then classiﬁed into the union of categories which are associated with its winner node selected in the HSOM. [sent-120, score-0.384]
</p><p>48 The classiﬁcation effectiveness is commonly measured in terms of precision and recall [16], which can be estimated as where and are the numbers of documents correctly classiﬁed, and and are the correctly not classiﬁed to category , respectively. [sent-132, score-0.292]
</p><p>49 ¢ ¢ ) g $ ¤ ¢ ¥©£§¦¢¨¤ ¤¡ £¡ ¢ )  ) ¡  )    ) ¡ g )     )      ) ¡         )  $  ¤  ¨ ¦ ¢ ©§¤ ¡ ¤ ¢ ¡  For each node and each category a conﬁdence value is determined. [sent-134, score-0.176]
</p><p>50 It describes the number of training documents belonging to class which were mapped to node . [sent-135, score-0.477]
</p><p>51 When retrieving documents from a given category , we compare for each node its associated against a threshold . [sent-136, score-0.409]
</p><p>52 Documents from nodes with become then included into the retrieval set. [sent-137, score-0.096]
</p><p>53 For nodes which contain a set of documents , the order of the , where . [sent-138, score-0.302]
</p><p>54  u # @ 4 (  #   $ # @ 4 (    6      In this way the number of retrieved documents can be controlled and we obtain the precision-recall-diagrams as shown in Fig. [sent-142, score-0.258]
</p><p>55 In order to compare the HSOM’s performance for text categorization, we also evaluated a -nearest neighbor ( -NN) classiﬁer with our training set. [sent-144, score-0.185]
</p><p>56 The conﬁdence level of a -NN classiﬁer to assign document to class is  "  )  "  "  @ 4  4 # @ 4$ @ 4   6  " @ 8 64 20 (& 975% 31 )'% # $ 0# @ 4 $ @ 4    6  % A@) ) $ ¢ # @ 4  ¡ -NN  (8)  )# @ 4 # ¡   is the set of documents for which is maximum. [sent-146, score-0.336]
</p><p>57 We have compared a HSOM with rings and a tesselation with neighbors (summing up to 1306 nodes) to a spherical standard euclidean SOM as described in [11] with approx. [sent-151, score-0.253]
</p><p>58 This is due to the fact, that the nodes in H2 cover a much broader space and therefore offer more freedom to map smaller portions of the original dataspace with less distortions as compared to euclidean space. [sent-155, score-0.229]
</p><p>59 ¢    ¤ A¢ g  D E  "  "  "  As the -NN results suggest, other state-of-the-art techniques like support vector machines will probably lead to better numerical categorization results than the HSOM. [sent-156, score-0.107]
</p><p>60 69  Figure 3: Precision-recall curves for the three most frequent categories earn, acq and money-fx. [sent-208, score-0.157]
</p><p>61 2 Text Mining & Semantic Navigation A major advantage of the HSOM is its remarkable capability to map high-dimensional similarity relationships to a low-dimensional space which can be more easily handled and interpreted by the human observer. [sent-210, score-0.134]
</p><p>62 This feature and the particular “ﬁsh-eye” capability motivates our approach to visualize whole text collections with the HSOM. [sent-211, score-0.248]
</p><p>63 It can be regarded as an interface capturing the semantic structure of a text database and provides a way to guide the users attention. [sent-212, score-0.247]
</p><p>64 In preliminary experiments we have labelled the nodes with glyphs corresponding to the categories of the documents mapped to that node. [sent-213, score-0.55]
</p><p>65 Note, that the major amount of data gets mapped to the outermost region, where the nodes of the HSOM make use of the large space offered by the hyperbolic geometry. [sent-216, score-0.672]
</p><p>66 During the unsupervised training process, the document’s categories were not presented to the HSOM. [sent-217, score-0.116]
</p><p>67 The two most prominent are the earn and acquisition region of the map, reﬂecting the large proportion of these categories in the Reuters-21578 collection. [sent-219, score-0.174]
</p><p>68 Note, that categories which are semantically similar are located beside each other, as can be seen in the corn, wheat, grain the interest, money-fx or the crude, ship area of the map. [sent-220, score-0.258]
</p><p>69 Additional to the category (glyph type) and the number of training documents per node (glyph size), the number of test documents mapped to each node is shown as the height of the symbol above the ground plane. [sent-221, score-0.918]
</p><p>70 In this way the HSOM can be used as a novelty detector in chronological document streams. [sent-222, score-0.103]
</p><p>71 For the Reuters-21578 dataset, a particular node strikes out. [sent-223, score-0.117]
</p><p>72 It corresponds to the small glyph tagged with the “ship” label in Fig. [sent-224, score-0.137]
</p><p>73 Only a few documents from the training collection are mapped to that node as shown by it’s relatively small glyph size. [sent-226, score-0.583]
</p><p>74 A closer inspection reveals, that the vast majority (35 of 40) of the test documents describe an incident where an Iranian oil rig was attacked in the gulf. [sent-230, score-0.29]
</p><p>75   The next example illustrates that the HSOM can provide more information about an unknown text than just it’s category. [sent-232, score-0.155]
</p><p>76 For this experiment we have taken movie reviews from the rec. [sent-233, score-0.132]
</p><p>77 Since all the reviews describe a certain movie, we retrieved their associated genres from the Internet Movie Database (http://www. [sent-237, score-0.092]
</p><p>78 The training set contained 8923 ran-  money−fx  ship  trade  corn wheat  grain  interest acq  crude  earn  ¡ ¢¤    Figure 4: The left ﬁgure shows a central view of the Reuters data. [sent-240, score-0.485]
</p><p>79 We used a HSOM with rings and a tesselation with neighbors. [sent-241, score-0.179]
</p><p>80 Ten different glyphs were used to visualize the ten most frequent categories. [sent-242, score-0.126]
</p><p>81 The glyph sizes and the -values (height above ground plane) reﬂect the number of training and test documents mapped to the corresponding node, respectively. [sent-244, score-0.498]
</p><p>82 ¥  £ ¤¤ £  domly selected reviews (without their genre information) from ﬁlms released before 2000. [sent-245, score-0.129]
</p><p>83 We then presented the system with ﬁve reviews from the ﬁlm “Atlantis”, a Disney cartoon released in 2001. [sent-246, score-0.098]
</p><p>84 The HSOM correctly classiﬁed all of the ﬁve texts as reviews for an animation movie. [sent-247, score-0.169]
</p><p>85 5 the projection of the ﬁve new documents onto the map with the previously acquired text collection is shown. [sent-249, score-0.537]
</p><p>86 5 it can be seen that all of the “Atlantis” reviews where mapped to a node in immediate vicinity of documents describing other Disney animation movies. [sent-253, score-0.57]
</p><p>87 This example motivates the approach of “semantic navigation” to rapidly visualize the linkage between unknown documents and previously acquired semantic concepts. [sent-254, score-0.472]
</p><p>88 In both ﬁgures, glyph size and -value indicate the number of texts related to the animation genre mapped to the corresponding node. [sent-256, score-0.336]
</p><p>89 Nodes exceeding a certain threshold were labelled with the title corresponding to the most frequently occuring movie mapped to that node. [sent-257, score-0.192]
</p><p>90 The underlined label in the right ﬁgure indicates the position of the node to which ﬁve new documents were mapped to. [sent-258, score-0.447]
</p><p>91 5 Conclusion Efﬁcient navigation in “Sematic Space” requires to address two challenges: (i) how to create a low dimensional display of semantic relationship of documents, and (ii) how to obtain these relationships by automated text categorization. [sent-259, score-0.378]
</p><p>92 The HSOM is able to exploit the peculiar geometric properties of hyperbolic space to successfully compress complex semantic relationships between text documents. [sent-261, score-0.83]
</p><p>93 Additionally, the use of hyperbolic lattice topology for the arrangement of the HSOM nodes offers new and attractive features for interactive “semantic navigation”. [sent-262, score-0.679]
</p><p>94 Large document databases can be inspected at a glance while the HSOM provides additional information which was captured during a previous training step, allowing e. [sent-263, score-0.133]
</p><p>95 to rapidly visualize relationships between new documents and previously acquired collections. [sent-265, score-0.401]
</p><p>96 Future work will address more sophisticated visualization strategies based on the new approach, as well as the exploration of other text representations which might take advantage of hyperbolic space properties. [sent-266, score-0.698]
</p><p>97 Laying out and visualizing large trees using a hyperbolic space. [sent-276, score-0.475]
</p><p>98 Text categorization and semantic browsing with self-organizing maps on non-euclidean spaces. [sent-323, score-0.246]
</p><p>99 Text categorization with support vector machines: learning with many relevant features. [sent-333, score-0.107]
</p><p>100 An improved boosting algorithm and its application to automated text categorization. [sent-346, score-0.186]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hsom', 0.511), ('hyperbolic', 0.475), ('documents', 0.233), ('som', 0.23), ('poincar', 0.176), ('text', 0.155), ('lattice', 0.135), ('tesselation', 0.123), ('tesselations', 0.123), ('node', 0.117), ('categorization', 0.107), ('glyph', 0.106), ('document', 0.103), ('mapped', 0.097), ('semantic', 0.092), ('plane', 0.09), ('earn', 0.088), ('ship', 0.088), ('categories', 0.086), ('neighborhood', 0.074), ('acq', 0.071), ('klein', 0.07), ('nodes', 0.069), ('reviews', 0.067), ('movie', 0.065), ('visualize', 0.065), ('category', 0.059), ('rings', 0.056), ('animation', 0.056), ('bielefeld', 0.056), ('disk', 0.056), ('map', 0.054), ('radius', 0.053), ('atlantis', 0.053), ('corn', 0.053), ('grain', 0.053), ('wheat', 0.053), ('navigation', 0.051), ('euclidean', 0.049), ('relationships', 0.049), ('crude', 0.049), ('classi', 0.049), ('maps', 0.047), ('kohonen', 0.046), ('texts', 0.046), ('origin', 0.046), ('spaces', 0.045), ('helge', 0.042), ('ve', 0.039), ('distance', 0.038), ('onto', 0.037), ('visualization', 0.037), ('embedding', 0.035), ('disney', 0.035), ('glyphs', 0.035), ('hsoms', 0.035), ('isometric', 0.035), ('kaski', 0.035), ('mulan', 0.035), ('northern', 0.035), ('ontrup', 0.035), ('tarzan', 0.035), ('ground', 0.032), ('space', 0.031), ('released', 0.031), ('automated', 0.031), ('painted', 0.031), ('reuters', 0.031), ('circumference', 0.031), ('genre', 0.031), ('incident', 0.031), ('semantically', 0.031), ('tagged', 0.031), ('training', 0.03), ('labelled', 0.03), ('triangle', 0.03), ('projection', 0.03), ('regular', 0.029), ('geometry', 0.028), ('acquired', 0.028), ('money', 0.028), ('peculiar', 0.028), ('motivates', 0.028), ('neuroinformatics', 0.028), ('metric', 0.028), ('frequency', 0.027), ('retrieval', 0.027), ('ten', 0.026), ('mining', 0.026), ('inspection', 0.026), ('winner', 0.026), ('portions', 0.026), ('riemannian', 0.026), ('arcs', 0.026), ('fx', 0.026), ('unit', 0.026), ('rapidly', 0.026), ('neighbors', 0.025), ('retrieved', 0.025), ('suitably', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="90-tfidf-1" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>2 0.11700086 <a title="90-tfidf-2" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>Author: David M. Blei, Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1</p><p>3 0.084119968 <a title="90-tfidf-3" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>4 0.059473373 <a title="90-tfidf-4" href="./nips-2001-Active_Information_Retrieval.html">24 nips-2001-Active Information Retrieval</a></p>
<p>Author: Tommi Jaakkola, Hava T. Siegelmann</p><p>Abstract: In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration. 1</p><p>5 0.05851499 <a title="90-tfidf-5" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classiﬁcation with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classiﬁcation. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classiﬁcation problems.</p><p>6 0.057344757 <a title="90-tfidf-6" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>7 0.055841818 <a title="90-tfidf-7" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>8 0.050891355 <a title="90-tfidf-8" href="./nips-2001-An_Efficient_Clustering_Algorithm_Using_Stochastic_Association_Model_and_Its_Implementation_Using_Nanostructures.html">33 nips-2001-An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures</a></p>
<p>9 0.050230388 <a title="90-tfidf-9" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>10 0.050093397 <a title="90-tfidf-10" href="./nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">60 nips-2001-Discriminative Direction for Kernel Classifiers</a></p>
<p>11 0.045778729 <a title="90-tfidf-11" href="./nips-2001-Categorization_by_Learning_and_Combining_Object_Parts.html">46 nips-2001-Categorization by Learning and Combining Object Parts</a></p>
<p>12 0.044105675 <a title="90-tfidf-12" href="./nips-2001-A_kernel_method_for_multi-labelled_classification.html">22 nips-2001-A kernel method for multi-labelled classification</a></p>
<p>13 0.041544031 <a title="90-tfidf-13" href="./nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">171 nips-2001-Spectral Relaxation for K-means Clustering</a></p>
<p>14 0.041069508 <a title="90-tfidf-14" href="./nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</a></p>
<p>15 0.040883545 <a title="90-tfidf-15" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>16 0.040061474 <a title="90-tfidf-16" href="./nips-2001-Fast_and_Robust_Classification_using_Asymmetric_AdaBoost_and_a_Detector_Cascade.html">77 nips-2001-Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade</a></p>
<p>17 0.039592303 <a title="90-tfidf-17" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>18 0.039496247 <a title="90-tfidf-18" href="./nips-2001-Classifying_Single_Trial_EEG%3A_Towards_Brain_Computer_Interfacing.html">50 nips-2001-Classifying Single Trial EEG: Towards Brain Computer Interfacing</a></p>
<p>19 0.038605992 <a title="90-tfidf-19" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>20 0.036477458 <a title="90-tfidf-20" href="./nips-2001-Sampling_Techniques_for_Kernel_Methods.html">164 nips-2001-Sampling Techniques for Kernel Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.124), (1, 0.02), (2, -0.028), (3, -0.004), (4, -0.017), (5, -0.06), (6, -0.103), (7, -0.013), (8, -0.053), (9, -0.024), (10, 0.087), (11, -0.083), (12, -0.051), (13, -0.004), (14, 0.008), (15, 0.012), (16, 0.003), (17, 0.023), (18, 0.011), (19, 0.024), (20, -0.034), (21, 0.058), (22, 0.064), (23, 0.019), (24, -0.084), (25, 0.106), (26, 0.029), (27, 0.066), (28, 0.125), (29, 0.106), (30, 0.004), (31, -0.038), (32, -0.027), (33, 0.026), (34, -0.033), (35, -0.113), (36, -0.139), (37, -0.058), (38, 0.229), (39, -0.186), (40, -0.031), (41, -0.134), (42, -0.088), (43, 0.106), (44, 0.044), (45, -0.061), (46, -0.093), (47, 0.115), (48, -0.032), (49, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95659822 <a title="90-lsi-1" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>2 0.59083098 <a title="90-lsi-2" href="./nips-2001-Small-World_Phenomena_and_the_Dynamics_of_Information.html">169 nips-2001-Small-World Phenomena and the Dynamics of Information</a></p>
<p>Author: Jon M. Kleinberg</p><p>Abstract: unkown-abstract</p><p>3 0.57417268 <a title="90-lsi-3" href="./nips-2001-Latent_Dirichlet_Allocation.html">107 nips-2001-Latent Dirichlet Allocation</a></p>
<p>Author: David M. Blei, Andrew Y. Ng, Michael I. Jordan</p><p>Abstract: We propose a generative model for text and other collections of discrete data that generalizes or improves on several previous models including naive Bayes/unigram, mixture of unigrams [6], and Hofmann's aspect model , also known as probabilistic latent semantic indexing (pLSI) [3]. In the context of text modeling, our model posits that each document is generated as a mixture of topics, where the continuous-valued mixture proportions are distributed as a latent Dirichlet random variable. Inference and learning are carried out efficiently via variational algorithms. We present empirical results on applications of this model to problems in text modeling, collaborative filtering, and text classification. 1</p><p>4 0.43100193 <a title="90-lsi-4" href="./nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</a></p>
<p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><p>5 0.3997122 <a title="90-lsi-5" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>Author: Ran El-Yaniv, Oren Souroujon</p><p>Abstract: We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically ﬁnd that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a signiﬁcantly more accurate classiﬁcation. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the eﬀectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples. 1</p><p>6 0.3339836 <a title="90-lsi-6" href="./nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">144 nips-2001-Partially labeled classification with Markov random walks</a></p>
<p>7 0.32026124 <a title="90-lsi-7" href="./nips-2001-The_Intelligent_surfer%3A_Probabilistic_Combination_of_Link_and_Content_Information_in_PageRank.html">184 nips-2001-The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank</a></p>
<p>8 0.31900647 <a title="90-lsi-8" href="./nips-2001-Agglomerative_Multivariate_Information_Bottleneck.html">30 nips-2001-Agglomerative Multivariate Information Bottleneck</a></p>
<p>9 0.3133108 <a title="90-lsi-9" href="./nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">62 nips-2001-Duality, Geometry, and Support Vector Regression</a></p>
<p>10 0.31153166 <a title="90-lsi-10" href="./nips-2001-Probabilistic_Abstraction_Hierarchies.html">149 nips-2001-Probabilistic Abstraction Hierarchies</a></p>
<p>11 0.29269633 <a title="90-lsi-11" href="./nips-2001-Active_Portfolio-Management_based_on_Error_Correction_Neural_Networks.html">26 nips-2001-Active Portfolio-Management based on Error Correction Neural Networks</a></p>
<p>12 0.28991368 <a title="90-lsi-12" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>13 0.28779677 <a title="90-lsi-13" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>14 0.28583264 <a title="90-lsi-14" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>15 0.26918468 <a title="90-lsi-15" href="./nips-2001-Intransitive_Likelihood-Ratio_Classifiers.html">99 nips-2001-Intransitive Likelihood-Ratio Classifiers</a></p>
<p>16 0.26763678 <a title="90-lsi-16" href="./nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</a></p>
<p>17 0.25175023 <a title="90-lsi-17" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>18 0.24810682 <a title="90-lsi-18" href="./nips-2001-Causal_Categorization_with_Bayes_Nets.html">47 nips-2001-Causal Categorization with Bayes Nets</a></p>
<p>19 0.23965335 <a title="90-lsi-19" href="./nips-2001-Incremental_A%2A.html">93 nips-2001-Incremental A*</a></p>
<p>20 0.2229498 <a title="90-lsi-20" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.045), (17, 0.024), (19, 0.036), (27, 0.103), (30, 0.052), (38, 0.022), (59, 0.036), (68, 0.333), (70, 0.011), (72, 0.049), (79, 0.042), (83, 0.018), (91, 0.15)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.823493 <a title="90-lda-1" href="./nips-2001-Hyperbolic_Self-Organizing_Maps_for_Semantic_Navigation.html">90 nips-2001-Hyperbolic Self-Organizing Maps for Semantic Navigation</a></p>
<p>Author: Jorg Ontrup, Helge Ritter</p><p>Abstract: We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a “hyperbolic SOM” (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.</p><p>2 0.80043304 <a title="90-lda-2" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>Author: Daniel Yarlett, Michael Ramscar</p><p>Abstract: In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning – a linear and a noisy-OR model – based on information contained in conceptual dependency networks. Empirical data is acquired in a study and the ﬁt of the models compared to it. We conclude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other parametric approaches in the future.</p><p>3 0.77937144 <a title="90-lda-3" href="./nips-2001-Matching_Free_Trees_with_Replicator_Equations.html">118 nips-2001-Matching Free Trees with Replicator Equations</a></p>
<p>Author: Marcello Pelillo</p><p>Abstract: Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.</p><p>4 0.52381051 <a title="90-lda-4" href="./nips-2001-A_Dynamic_HMM_for_On-line_Segmentation_of_Sequential_Data.html">7 nips-2001-A Dynamic HMM for On-line Segmentation of Sequential Data</a></p>
<p>Author: Jens Kohlmorgen, Steven Lemm</p><p>Abstract: We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system. 1</p><p>5 0.52307957 <a title="90-lda-5" href="./nips-2001-A_Natural_Policy_Gradient.html">13 nips-2001-A Natural Policy Gradient</a></p>
<p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><p>6 0.52036852 <a title="90-lda-6" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>7 0.51905203 <a title="90-lda-7" href="./nips-2001-Novel_iteration_schemes_for_the_Cluster_Variation_Method.html">132 nips-2001-Novel iteration schemes for the Cluster Variation Method</a></p>
<p>8 0.51849163 <a title="90-lda-8" href="./nips-2001-Grouping_with_Bias.html">89 nips-2001-Grouping with Bias</a></p>
<p>9 0.51775146 <a title="90-lda-9" href="./nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</a></p>
<p>10 0.51507658 <a title="90-lda-10" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>11 0.51503015 <a title="90-lda-11" href="./nips-2001-On_Spectral_Clustering%3A_Analysis_and_an_algorithm.html">135 nips-2001-On Spectral Clustering: Analysis and an algorithm</a></p>
<p>12 0.51489621 <a title="90-lda-12" href="./nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</a></p>
<p>13 0.5140183 <a title="90-lda-13" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>14 0.51384896 <a title="90-lda-14" href="./nips-2001-Neural_Implementation_of_Bayesian_Inference_in_Population_Codes.html">131 nips-2001-Neural Implementation of Bayesian Inference in Population Codes</a></p>
<p>15 0.51326233 <a title="90-lda-15" href="./nips-2001-Iterative_Double_Clustering_for_Unsupervised_and_Semi-Supervised_Learning.html">100 nips-2001-Iterative Double Clustering for Unsupervised and Semi-Supervised Learning</a></p>
<p>16 0.51324713 <a title="90-lda-16" href="./nips-2001-Covariance_Kernels_from_Bayesian_Generative_Models.html">58 nips-2001-Covariance Kernels from Bayesian Generative Models</a></p>
<p>17 0.51274455 <a title="90-lda-17" href="./nips-2001-Correlation_Codes_in_Neuronal_Populations.html">57 nips-2001-Correlation Codes in Neuronal Populations</a></p>
<p>18 0.51258928 <a title="90-lda-18" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>19 0.51258683 <a title="90-lda-19" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>20 0.51169878 <a title="90-lda-20" href="./nips-2001-Convolution_Kernels_for_Natural_Language.html">56 nips-2001-Convolution Kernels for Natural Language</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
