<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 nips-2001-ACh, Uncertainty, and Cortical Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-3" href="#">nips2001-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 nips-2001-ACh, Uncertainty, and Cortical Inference</h1>
<br/><p>Source: <a title="nips-2001-3-pdf" href="http://papers.nips.cc/paper/2103-ach-uncertainty-and-cortical-inference.pdf">pdf</a></p><p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, inﬂuenced by input stimuli, and top-down inference, inﬂuenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.</p><p>Reference: <a title="nips-2001-3-reference" href="../nips2001_reference/nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ach', 0.659), ('yw', 0.316), ('cholinerg', 0.212), ('inf', 0.178), ('va', 0.158), ('neuromod', 0.15), ('acetylcholin', 0.141), ('context', 0.133), ('uncertainty', 0.124), ('hasselmo', 0.115), ('hc', 0.107), ('day', 0.103), ('cort', 0.1), ('postery', 0.1), ('plast', 0.086), ('exact', 0.083), ('uent', 0.077), ('pur', 0.073), ('lay', 0.071), ('holland', 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="3-tfidf-1" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, inﬂuenced by input stimuli, and top-down inference, inﬂuenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.</p><p>2 0.12088229 <a title="3-tfidf-2" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>Author: Antonio Torralba</p><p>Abstract: The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. fu this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes. 1</p><p>3 0.11456966 <a title="3-tfidf-3" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>Author: Peter Dayan</p><p>Abstract: The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine, 12,13 into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.</p><p>4 0.099106029 <a title="3-tfidf-4" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>5 0.082803622 <a title="3-tfidf-5" href="./nips-2001-Bayesian_time_series_classification.html">43 nips-2001-Bayesian time series classification</a></p>
<p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: This paper proposes an approach to classiﬁcation of adjacent segments of a time series as being either of classes. We use a hierarchical model that consists of a feature extraction stage and a generative classiﬁer which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classiﬁer is implemented as hidden Markov model with Gaussian and Multinomial observation distributions deﬁned on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classiﬁcations will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classiﬁcation of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a signiﬁcant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.</p><p>6 0.078596368 <a title="3-tfidf-6" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>7 0.077908158 <a title="3-tfidf-7" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>8 0.071604826 <a title="3-tfidf-8" href="./nips-2001-A_Variational_Approach_to_Learning_Curves.html">21 nips-2001-A Variational Approach to Learning Curves</a></p>
<p>9 0.068420552 <a title="3-tfidf-9" href="./nips-2001-The_Unified_Propagation_and_Scaling_Algorithm.html">188 nips-2001-The Unified Propagation and Scaling Algorithm</a></p>
<p>10 0.066735789 <a title="3-tfidf-10" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>11 0.065314837 <a title="3-tfidf-11" href="./nips-2001-Characterizing_Neural_Gain_Control_using_Spike-triggered_Covariance.html">48 nips-2001-Characterizing Neural Gain Control using Spike-triggered Covariance</a></p>
<p>12 0.065217458 <a title="3-tfidf-12" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>13 0.061894104 <a title="3-tfidf-13" href="./nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</a></p>
<p>14 0.06163485 <a title="3-tfidf-14" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>15 0.060748342 <a title="3-tfidf-15" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>16 0.059669979 <a title="3-tfidf-16" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>17 0.059668366 <a title="3-tfidf-17" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>18 0.059103213 <a title="3-tfidf-18" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>19 0.058892071 <a title="3-tfidf-19" href="./nips-2001-Infinite_Mixtures_of_Gaussian_Process_Experts.html">95 nips-2001-Infinite Mixtures of Gaussian Process Experts</a></p>
<p>20 0.057821512 <a title="3-tfidf-20" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2001_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, -0.075), (2, 0.024), (3, -0.09), (4, 0.096), (5, -0.031), (6, 0.005), (7, 0.028), (8, 0.003), (9, -0.041), (10, 0.068), (11, -0.019), (12, -0.058), (13, 0.03), (14, 0.031), (15, -0.041), (16, -0.046), (17, -0.012), (18, 0.075), (19, 0.012), (20, -0.023), (21, -0.077), (22, 0.026), (23, 0.03), (24, 0.047), (25, 0.085), (26, 0.063), (27, 0.009), (28, -0.015), (29, -0.066), (30, 0.032), (31, 0.095), (32, 0.062), (33, 0.085), (34, -0.021), (35, 0.011), (36, -0.085), (37, 0.034), (38, 0.188), (39, 0.119), (40, 0.121), (41, -0.064), (42, 0.049), (43, -0.004), (44, 0.03), (45, 0.05), (46, -0.041), (47, -0.033), (48, 0.151), (49, -0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91959 <a title="3-lsi-1" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, inﬂuenced by input stimuli, and top-down inference, inﬂuenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.</p><p>2 0.57226956 <a title="3-lsi-2" href="./nips-2001-A_Rational_Analysis_of_Cognitive_Control_in_a_Speeded_Discrimination_Task.html">18 nips-2001-A Rational Analysis of Cognitive Control in a Speeded Discrimination Task</a></p>
<p>Author: Michael C. Mozer, Michael D. Colagrosso, David E. Huber</p><p>Abstract: We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conﬂict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality—that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission—we obtain a good ﬁt to overall RT and error data, as well as trial-by-trial variations in performance. Consider the following scenario: While driving, you approach an intersection at which the trafﬁc light has already turned yellow, signaling that it is about to turn red. You also notice that a car is approaching you rapidly from behind, with no indication of slowing. Should you stop or speed through the intersection? The decision is difﬁcult due to the presence of two conﬂicting signals. Such response conﬂict can be produced in a psychological laboratory as well. For example, Stroop (1935) asked individuals to name the color of ink on which a word is printed. When the words are color names incongruous with the ink color— e.g., “blue” printed in red—reaction times are slower and error rates are higher. We are interested in the control mechanisms underlying performance of high-conﬂict tasks. Conﬂict requires individuals to monitor and adjust their behavior, possibly responding more slowly if errors are too frequent. In this paper, we model a speeded discrimination paradigm in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). The stimuli are letters of the alphabet, A–Z, presented in rapid succession. In a choice task, individuals are asked to press one response key if the letter is an X or another response key for any letter other than X (as a shorthand, we will refer to non-X stimuli as Y). In a go/no-go task, individuals are asked to press a response key when X is presented and to make no response otherwise. We address both tasks because they elicit slightly different decision-making behavior. In both tasks, Jones and Braver (2001) manipulated the relative frequency of the X and Y stimuli; the ratio of presentation frequency was either 17:83, 50:50, or 83:17. Response conﬂict arises when the two stimulus classes are unbalanced in frequency, resulting in more errors and slower reaction times. For example, when X’s are frequent but Y is presented, individuals are predisposed toward producing the X response, and this predisposition must be overcome by the perceptual evidence from the Y. Jones and Braver (2001) also performed an fMRI study of this task and found that anterior cingulate cortex (ACC) becomes activated in situations involving response conﬂict. Specifically, when one stimulus occurs infrequently relative to the other, event-related fMRI response in the ACC is greater for the low frequency stimulus. Jones and Braver also extended a neural network model of Botvinick, Braver, Barch, Carter, and Cohen (2001) to account for human performance in the two discrimination tasks. The heart of the model is a mechanism that monitors conﬂict—the posited role of the ACC—and adjusts response biases accordingly. In this paper, we develop a parsimonious alternative account of the role of the ACC and of how control processes modulate behavior when response conﬂict arises. 1 A RATIONAL ANALYSIS Our account is based on a rational analysis of human cognition, which views cognitive processes as being optimized with respect to certain task-related goals, and being adaptive to the structure of the environment (Anderson, 1990). We make three assumptions of rationality: (1) perceptual inference is optimal but is subject to rate limitations on information transmission, (2) response class prior probabilities are accurately estimated, and (3) the goal of individuals is to minimize a cost that depends both on error rate and reaction time. The heart of our account is an existing probabilistic model that explains a variety of facilitation effects that arise from long-term repetition priming (Colagrosso, in preparation; Mozer, Colagrosso, & Huber, 2000), and more broadly, that addresses changes in the nature of information transmission in neocortex due to experience. We give a brief overview of this model; the details are not essential for the present work. The model posits that neocortex can be characterized by a collection of informationprocessing pathways, and any act of cognition involves coordination among pathways. To model a simple discrimination task, we might suppose a perceptual pathway to map the visual input to a semantic representation, and a response pathway to map the semantic representation to a response. The choice and go/no-go tasks described earlier share a perceptual pathway, but require different response pathways. The model is framed in terms of probability theory: pathway inputs and outputs are random variables and microinference in a pathway is carried out by Bayesian belief revision.   To elaborate, consider a pathway whose input at time is a discrete random variable, denoted , which can assume values corresponding to alternative input states. Similarly, the output of the pathway at time is a discrete random variable, denoted , which can assume values . For example, the input to the perceptual pathway in the discrimination task is one of visual patterns corresponding to the letters of the alphabet, and the output is one of letter identities. (This model is highly abstract: the visual patterns are enumerated, but the actual pixel patterns are not explicitly represented in the model. Nonetheless, the similarity structure among inputs can be captured, but we skip a discussion of this issue because it is irrelevant for the current work.) To present a particular input alternative, , to the model for time steps, we clamp for . The model computes a probability distribution over given , i.e., P . ¡ # 4 0 ©2' &  0 ' ! 1)(</p><p>3 0.56978041 <a title="3-lsi-3" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>Author: Peter Dayan</p><p>Abstract: The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine, 12,13 into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.</p><p>4 0.56320536 <a title="3-lsi-4" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>Author: Aaron C. Courville, David S. Touretzky</p><p>Abstract: The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1 , 2, 3], which other models are unable to explain. 1</p><p>5 0.52181309 <a title="3-lsi-5" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><p>6 0.49927989 <a title="3-lsi-6" href="./nips-2001-Learning_Body_Pose_via_Specialized_Maps.html">108 nips-2001-Learning Body Pose via Specialized Maps</a></p>
<p>7 0.48444948 <a title="3-lsi-7" href="./nips-2001-A_Quantitative_Model_of_Counterfactual_Reasoning.html">17 nips-2001-A Quantitative Model of Counterfactual Reasoning</a></p>
<p>8 0.47546527 <a title="3-lsi-8" href="./nips-2001-Entropy_and_Inference%2C_Revisited.html">68 nips-2001-Entropy and Inference, Revisited</a></p>
<p>9 0.4537037 <a title="3-lsi-9" href="./nips-2001-Reinforcement_Learning_and_Time_Perception_--_a_Model_of_Animal_Experiments.html">160 nips-2001-Reinforcement Learning and Time Perception -- a Model of Animal Experiments</a></p>
<p>10 0.43991891 <a title="3-lsi-10" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>11 0.43634969 <a title="3-lsi-11" href="./nips-2001-A_Maximum-Likelihood_Approach_to_Modeling_Multisensory_Enhancement.html">11 nips-2001-A Maximum-Likelihood Approach to Modeling Multisensory Enhancement</a></p>
<p>12 0.43418834 <a title="3-lsi-12" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<p>13 0.42709613 <a title="3-lsi-13" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>14 0.41491142 <a title="3-lsi-14" href="./nips-2001-Product_Analysis%3A_Learning_to_Model_Observations_as_Products_of_Hidden_Variables.html">153 nips-2001-Product Analysis: Learning to Model Observations as Products of Hidden Variables</a></p>
<p>15 0.41169021 <a title="3-lsi-15" href="./nips-2001-A_Rotation_and_Translation_Invariant_Discrete_Saliency_Network.html">19 nips-2001-A Rotation and Translation Invariant Discrete Saliency Network</a></p>
<p>16 0.40662515 <a title="3-lsi-16" href="./nips-2001-A_Model_of_the_Phonological_Loop%3A_Generalization_and_Binding.html">12 nips-2001-A Model of the Phonological Loop: Generalization and Binding</a></p>
<p>17 0.39986858 <a title="3-lsi-17" href="./nips-2001-Improvisation_and_Learning.html">91 nips-2001-Improvisation and Learning</a></p>
<p>18 0.39835614 <a title="3-lsi-18" href="./nips-2001-Probabilistic_principles_in_unsupervised_learning_of_visual_structure%3A_human_data_and_a_model.html">151 nips-2001-Probabilistic principles in unsupervised learning of visual structure: human data and a model</a></p>
<p>19 0.39613253 <a title="3-lsi-19" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>20 0.38890475 <a title="3-lsi-20" href="./nips-2001-Very_loopy_belief_propagation_for_unwrapping_phase_images.html">196 nips-2001-Very loopy belief propagation for unwrapping phase images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2001_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.026), (16, 0.132), (26, 0.015), (31, 0.101), (36, 0.022), (50, 0.06), (63, 0.022), (65, 0.024), (77, 0.03), (79, 0.014), (81, 0.012), (91, 0.058), (92, 0.062), (93, 0.341)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.75330961 <a title="3-lda-1" href="./nips-2001-The_Emergence_of_Multiple_Movement_Units_in_the_Presence_of_Noise_and_Feedback_Delay.html">181 nips-2001-The Emergence of Multiple Movement Units in the Presence of Noise and Feedback Delay</a></p>
<p>Author: Michael Kositsky, Andrew G. Barto</p><p>Abstract: Tangential hand velocity proﬁles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efﬁciently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simpliﬁed dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simpliﬁed model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm’s end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm’s end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.</p><p>same-paper 2 0.72809762 <a title="3-lda-2" href="./nips-2001-ACh%2C_Uncertainty%2C_and_Cortical_Inference.html">3 nips-2001-ACh, Uncertainty, and Cortical Inference</a></p>
<p>Author: Peter Dayan, Angela J. Yu</p><p>Abstract: Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, inﬂuenced by input stimuli, and top-down inference, inﬂuenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.</p><p>3 0.66088307 <a title="3-lda-3" href="./nips-2001-Linking_Motor_Learning_to_Function_Approximation%3A_Learning_in_an_Unlearnable_Force_Field.html">116 nips-2001-Linking Motor Learning to Function Approximation: Learning in an Unlearnable Force Field</a></p>
<p>Author: O. Donchin, Reza Shadmehr</p><p>Abstract: Reaching movements require the brain to generate motor commands that rely on an internal model of the task’s dynamics. Here we consider the errors that subjects make early in their reaching trajectories to various targets as they learn an internal model. Using a framework from function approximation, we argue that the sequence of errors should reﬂect the process of gradient descent. If so, then the sequence of errors should obey hidden state transitions of a simple dynamical system. Fitting the system to human data, we ﬁnd a surprisingly good ﬁt accounting for 98% of the variance. This allows us to draw tentative conclusions about the basis elements used by the brain in transforming sensory space to motor commands. To test the robustness of the results, we estimate the shape of the basis elements under two conditions: in a traditional learning paradigm with a consistent force ﬁeld, and in a random sequence of force ﬁelds where learning is not possible. Remarkably, we ﬁnd that the basis remains invariant. 1</p><p>4 0.5256241 <a title="3-lda-4" href="./nips-2001-Motivated_Reinforcement_Learning.html">126 nips-2001-Motivated Reinforcement Learning</a></p>
<p>Author: Peter Dayan</p><p>Abstract: The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine, 12,13 into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.</p><p>5 0.52019006 <a title="3-lda-5" href="./nips-2001-Effective_Size_of_Receptive_Fields_of_Inferior_Temporal_Visual_Cortex_Neurons_in_Natural_Scenes.html">65 nips-2001-Effective Size of Receptive Fields of Inferior Temporal Visual Cortex Neurons in Natural Scenes</a></p>
<p>Author: Thomas P. Trappenberg, Edmund T. Rolls, Simon M. Stringer</p><p>Abstract: Inferior temporal cortex (IT) neurons have large receptive ﬁelds when a single effective object stimulus is shown against a blank background, but have much smaller receptive ﬁelds when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magniﬁcation factor than the peripheral visual ﬁeld. Furthermore, we show that top-down object bias can increase the receptive ﬁeld size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive ﬁeld size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.</p><p>6 0.50755411 <a title="3-lda-6" href="./nips-2001-Self-regulation_Mechanism_of_Temporally_Asymmetric_Hebbian_Plasticity.html">166 nips-2001-Self-regulation Mechanism of Temporally Asymmetric Hebbian Plasticity</a></p>
<p>7 0.50117904 <a title="3-lda-7" href="./nips-2001-Generating_velocity_tuning_by_asymmetric_recurrent_connections.html">82 nips-2001-Generating velocity tuning by asymmetric recurrent connections</a></p>
<p>8 0.49819863 <a title="3-lda-8" href="./nips-2001-Probabilistic_Inference_of_Hand_Motion_from_Neural_Activity_in_Motor_Cortex.html">150 nips-2001-Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex</a></p>
<p>9 0.49172288 <a title="3-lda-9" href="./nips-2001-Eye_movements_and_the_maturation_of_cortical_orientation_selectivity.html">73 nips-2001-Eye movements and the maturation of cortical orientation selectivity</a></p>
<p>10 0.48966941 <a title="3-lda-10" href="./nips-2001-Contextual_Modulation_of_Target_Saliency.html">54 nips-2001-Contextual Modulation of Target Saliency</a></p>
<p>11 0.48777661 <a title="3-lda-11" href="./nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</a></p>
<p>12 0.48680356 <a title="3-lda-12" href="./nips-2001-Modularity_in_the_motor_system%3A_decomposition_of_muscle_patterns_as_combinations_of_time-varying_synergies.html">125 nips-2001-Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies</a></p>
<p>13 0.4847995 <a title="3-lda-13" href="./nips-2001-Why_Neuronal_Dynamics_Should_Control_Synaptic_Learning_Rules.html">197 nips-2001-Why Neuronal Dynamics Should Control Synaptic Learning Rules</a></p>
<p>14 0.4818804 <a title="3-lda-14" href="./nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">115 nips-2001-Linear-time inference in Hierarchical HMMs</a></p>
<p>15 0.48109409 <a title="3-lda-15" href="./nips-2001-Associative_memory_in_realistic_neuronal_networks.html">37 nips-2001-Associative memory in realistic neuronal networks</a></p>
<p>16 0.47865832 <a title="3-lda-16" href="./nips-2001-Modeling_Temporal_Structure_in_Classical_Conditioning.html">123 nips-2001-Modeling Temporal Structure in Classical Conditioning</a></p>
<p>17 0.4737803 <a title="3-lda-17" href="./nips-2001-Grammar_Transfer_in_a_Second_Order_Recurrent_Neural_Network.html">85 nips-2001-Grammar Transfer in a Second Order Recurrent Neural Network</a></p>
<p>18 0.473039 <a title="3-lda-18" href="./nips-2001-Orientation-Selective_aVLSI_Spiking_Neurons.html">141 nips-2001-Orientation-Selective aVLSI Spiking Neurons</a></p>
<p>19 0.47118437 <a title="3-lda-19" href="./nips-2001-The_Infinite_Hidden_Markov_Model.html">183 nips-2001-The Infinite Hidden Markov Model</a></p>
<p>20 0.47042775 <a title="3-lda-20" href="./nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">161 nips-2001-Reinforcement Learning with Long Short-Term Memory</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
