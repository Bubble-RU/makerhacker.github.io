<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-200" href="#">nips2011-200</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</h1>
<br/><p>Source: <a title="nips-2011-200-pdf" href="http://papers.nips.cc/paper/4401-on-the-analysis-of-multi-channel-neural-spike-data.pdf">pdf</a></p><p>Author: Bo Chen, David E. Carlson, Lawrence Carin</p><p>Abstract: Nonparametric Bayesian methods are developed for analysis of multi-channel spike-train data, with the feature learning and spike sorting performed jointly. The feature learning and sorting are performed simultaneously across all channels. Dictionary learning is implemented via the beta-Bernoulli process, with spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP), with these two models coupled. The dHDP is augmented to eliminate refractoryperiod violations, it allows the “appearance” and “disappearance” of neurons over time, and it models smooth variation in the spike statistics. 1</p><p>Reference: <a title="nips-2011-200-reference" href="../nips2011_reference/nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Nonparametric Bayesian methods are developed for analysis of multi-channel spike-train data, with the feature learning and spike sorting performed jointly. [sent-3, score-0.874]
</p><p>2 The feature learning and sorting are performed simultaneously across all channels. [sent-4, score-0.472]
</p><p>3 Dictionary learning is implemented via the beta-Bernoulli process, with spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP), with these two models coupled. [sent-5, score-0.958]
</p><p>4 The dHDP is augmented to eliminate refractoryperiod violations, it allows the “appearance” and “disappearance” of neurons over time, and it models smooth variation in the spike statistics. [sent-6, score-0.552]
</p><p>5 1  Introduction  The analysis of action potentials (“spikes”) from neural-recording devices is a problem of longstanding interest (see [21, 1, 16, 22, 8, 4, 6] and the references therein). [sent-7, score-0.045]
</p><p>6 In such research one is typically interested in clustering (sorting) the spikes, with the goal of linking a given cluster to a particular neuron. [sent-8, score-0.146]
</p><p>7 Such technology is of interest for brain-machine interfaces and for gaining insight into the properties of neural circuits [14]. [sent-9, score-0.073]
</p><p>8 In such research one typically (i) ﬁlters the raw sensor readings, (ii) performs thresholding to “detect” the spikes, (iii) maps each detected spike to a feature vector, and (iv) then clusters the feature vectors [12]. [sent-10, score-0.78]
</p><p>9 Principal component analysis (PCA) is a popular choice [12] for feature mapping. [sent-11, score-0.049]
</p><p>10 Many of the early methods for spike sorting were based on classical clustering techniques [12] (e. [sent-14, score-0.844]
</p><p>11 However, [5] assumed that the spike features were provided via PCA in the ﬁrst two or three principal components (PCs). [sent-18, score-0.559]
</p><p>12 In [6] feature learning and spike sorting were performed jointly via a mixture of factor analyzers (MFA) formulation. [sent-19, score-0.987]
</p><p>13 There has been an increasing interest in developing neural devices with C > 1 recording channels, each of which produces a separate electrical recording of neural activity. [sent-21, score-0.319]
</p><p>14 (a) Ground truth; (b) K-means clustering on the ﬁrst 2  principal components; (c) GMM clustering with the ﬁrst 2 principal components; (d) proposed method. [sent-24, score-0.234]
</p><p>15 We label using arrows examples K-means and the GMM miss, and that the proposed method properly sort. [sent-25, score-0.034]
</p><p>16 has been performed on a single channel, or when multiple channels are present each is typically analyzed in isolation. [sent-26, score-0.226]
</p><p>17 In [5] C = 4 channels were considered, but it was assumed that a spike occurred at the same time (or nearly same time) across all channels, and the features from the four channels were concatenated, effectively reducing this again to a single-channel analysis. [sent-27, score-0.863]
</p><p>18 When C 1, the assumption that a given neuron is observed simultaneously on all channels is typically inappropriate, and in fact the diversity of neuron sensing across the device is desired, to enhance functionality [18]. [sent-28, score-0.593]
</p><p>19 This paper addresses the multi-channel neural-recording problem, under conditions for which concatenation may be inappropriate; the proposed model generalizes the DP formulation of [5], with a hierarchical DP (HDP) formulation [20]. [sent-29, score-0.043]
</p><p>20 In this formulation statistical strength is shared across the channels, without assuming that a given neuron is simultaneously viewed across all channels. [sent-30, score-0.285]
</p><p>21 Further, the model generalizes the HDP, via a dynamic HDP (dHDP) [17] to allow the “appearance”/“disappearance” of neurons, while also allowing smooth changes in the statistics of the neurons. [sent-31, score-0.09]
</p><p>22 Further, we explicitly account for refractory times, as in [5]. [sent-32, score-0.043]
</p><p>23 We also perform joint feature learning and clustering, using a mixture of factor analyzers construction as in [6], but we do so in a fully Bayesian, multi-channel setting (additionally, [6] did not account for time-varying statistics). [sent-33, score-0.166]
</p><p>24 The learned factor loadings are found to be similar to wavelets, but they are matched to the properties of neuron spikes; this is in contrast to previous feature extraction on spikes [11] based on orthogonal wavelets, that are not necessarily matched to neuron properties. [sent-34, score-0.785]
</p><p>25 To give a preview of the results, providing a sense of the importance of feature learning (relative to mapping data into PCA features learned ofﬂine), in Figure 1 we show a comparison of clustering results on the ﬁrst channel of d533101 data from hc-1 [7]. [sent-35, score-0.301]
</p><p>26 For all cases in Figure 1 the data are depicted in the ﬁrst two PCs for visualization, but the proposed method in (d) learns the number of features and their composition, while simultaneously performing clustering. [sent-36, score-0.029]
</p><p>27 The results in (b) and (c) correspond respectively to widely employed K-means and GMM analysis, based on using two PCs (in these cases the analysis are employed in PCA space, as have been many more-advanced approaches [5]). [sent-37, score-0.078]
</p><p>28 From Figures 1 (b) and (c), we observe that both K-means and GMM work well, but due to the constrained feature space they incorrectly classify some spikes (marked by arrows). [sent-38, score-0.334]
</p><p>29 However, the proposed model, shown in Figure 1(d), which incorporates dictionary learning with spike sorting, infers an appropriate feature space (not shown) and more effectively clusters the neurons. [sent-39, score-0.887]
</p><p>30 1  Model Construction Dictionary learning  We initially assume that spike detection has been performed on all channels. [sent-42, score-0.523]
</p><p>31 , C} is a vector xn 2 RD , deﬁned by D time samples for each spike, centered at the peak of the detected signal; there are Nc spikes on channel c. [sent-49, score-0.604]
</p><p>32 (c)  Data from spike n on channel c, xn , is represented in terms of a dictionary D 2 RD⇥K , where K is an upper bound on the number of needed dictionary elements (columns of D), and the model 2  (c)  infers the subset of dictionary elements needed to represent the data. [sent-50, score-1.467]
</p><p>33 Each xn is represented as x(c) = D⇤(c) s(c) + ✏(c) n n n (c)  (c)  (1)  (c)  where ⇤(c) = diag( 1 b1 , 2 b2 , . [sent-51, score-0.039]
</p><p>34 Deﬁning dk as the kth column of D, and letting ID represent the D ⇥ D identity matrix, the priors on the model parameters are dk ⇠ N (0,  1 ID ) , D  (c)  (c) k  ⇠ T N + (0,  c  1  ),  ✏(c) ⇠ N (0, ⌃c 1 ) n  (2)  (c)  where ⌃c = diag(⌘1 , . [sent-58, score-0.101]
</p><p>35 Gamma priors (detailed when presenting results) are placed on c and on each of the ele(c) (c) ments of (⌘1 , . [sent-62, score-0.039]
</p><p>36 For the binary vector b we impose the prior bk ⇠ Bernoulli(⇡k ), with ⇡k ⇠ Beta(a/K, b(K 1)/K), implying that the number of non-zero components of b is drawn Binomial(K, a/(a + b(K 1))); this corresponds to Poisson(a/b) in the limit K ! [sent-66, score-0.087]
</p><p>37 (c)  This model imposes that each xn is drawn from a linear subspace, deﬁned by the columns of D with corresponding non-zero components in b; the same linear subspace is shared across all channels (c) c 2 {1, . [sent-69, score-0.295]
</p><p>38 However, the strength with which a column of D contributes toward xn depends on the channel c, as deﬁned by ⇤(c) . [sent-73, score-0.221]
</p><p>39 Concerning ⇤(c) , rather than explicitly imposing a sparse (c) diagonal via b, we may also draw k ⇠ T N + (0, ck1 ), with shrinkage priors employed on the ck (i. [sent-74, score-0.147]
</p><p>40 , with the ck drawn from a gamma prior that favors large ck ; which encourages many of the diagonal elements of ⇤(c) to be small, but typically not exactly zero). [sent-76, score-0.162]
</p><p>41 In tests, the model performed similarly when shrinkage priors were used on ⇤(c) relative to explicit imposition of sparseness via b; all results below are based on the latter construction. [sent-77, score-0.118]
</p><p>42 2  Multi-Channel Dynamic hierarchical Dirichlet process (c)  We sort the spikes on the channels by clustering the {sn }, and in this sense feature design (learning {D⇤(c) }) and sorting are performed simultaneously. [sent-79, score-0.975]
</p><p>43 We ﬁrst discuss how this may be performed via a hierarchical Dirichlet process (HDP) construction [20], and then extend this via a dynamic (c) HDP (dHDP) [17] considering multiple channels. [sent-80, score-0.216]
</p><p>44 In an HDP construction, the {sn } are modeled as being drawn (c) s(c) ⇠ f (✓n ) , n  (c) ✓n ⇠ G(c) ,  G(c) ⇠ DP(↵c G) ,  G ⇠ DP(↵0 G0 ) (3) P1 ⇤ where a Q from, for example, DP(↵0 G0 ) may be constructed [19] as G = i=1 ⇡i ✓i , where draw ⇤ ⇤ ⇤ ⇡i = Vi h <. [sent-81, score-0.028]
</p><p>45 5ms) to an intracellular spike, we assume that the spike detected in the extracellular recording corresponds to the known neuron’s spikes. [sent-82, score-0.926]
</p><p>46 This allows us to know partial ground truth, and allows us to test on methods compared to the known information. [sent-83, score-0.029]
</p><p>47 For the accuracy analysis, we determine one cluster that corresponds to the known neuron. [sent-84, score-0.076]
</p><p>48 Then we consider a spike to be correctly sorted if it is a known spike and is in the known cluster or if it is an unknown spike in the unknown cluster. [sent-85, score-1.492]
</p><p>49 This data consists of a 4-channel extracellular recordings and 1-channel intracellular recording. [sent-87, score-0.283]
</p><p>50 We used 2491 detected spikes and 786 of those spikes came from the known neuron. [sent-88, score-0.668]
</p><p>51 The results show that learning the feature space instead of using the top 2 PCA components increases sorting accuracy. [sent-90, score-0.391]
</p><p>52 This phenomenon can be seen in Figure 1, where it is impossible to accurately resolve the clusters in the space based on the 2 principal components, through either K-means or GMM. [sent-91, score-0.159]
</p><p>53 Thus, by jointly learning the suitable feature space and clustering, we are able to separate the unknown and known neurons clusters more accurately. [sent-92, score-0.269]
</p><p>54 In the HDP model the advantage is clear in the global accuracy as we achieve 89. [sent-93, score-0.042]
</p><p>55 In addition to learning the appropriate feature space, HDP-DL and DP-DL can infer the appropriate number of clusters, allowing the data to deﬁne the number of neurons. [sent-96, score-0.049]
</p><p>56 The posterior distribution on the number of global clusters and number of factors (dictionary elements) used is shown in Figure 3(a) and 3(b), along with the most used elements of the learned dictionary in Figure 3(c). [sent-97, score-0.414]
</p><p>57 The dictionary elements show shapes similar to both neuron spikes in Figure 3(d) and wavelets. [sent-98, score-0.719]
</p><p>58 Next we used the d561102 data from hc-1, which consists of 4 extracellular recording and 1 intracellular recording. [sent-100, score-0.356]
</p><p>59 To do spike detection we high-pass ﬁltered the data from 300 Hz and detected spikes when the voltage level passed a positive or negative threshold, as in [2]. [sent-101, score-0.855]
</p><p>60 We choose this data the known neuron displays dynamic properties by showing periods of activity and inactivity. [sent-102, score-0.295]
</p><p>61 The intracellular recording in Figure 4(a) shows the known neuron is active for only a brief section of the recorded signal, and is then inactive for the rest of the signal. [sent-103, score-0.454]
</p><p>62 The nonstationarity passes along to the extracellular spike train and the detect spikes. [sent-104, score-0.591]
</p><p>63 We used the ﬁrst 930 detected spikes, which included 202 spikes from the known cluster. [sent-105, score-0.383]
</p><p>64 In order to model the dynamic properties, we binned the data into 31 subgroups of 30 spikes to use with our multichannel dynamic HDP. [sent-106, score-0.559]
</p><p>65 (a) approximate posterior probability on the number of global clusters (across all channels); (b) approximate posterior distribution on the number of dictionary elements; (c) six most used dictionary elements; (d) examples of typical spikes from the data. [sent-118, score-0.861]
</p><p>66 The model adapts to the nonstationary spike dynamics by learning the parameters to model (c) dynamic properties at block 11 (w11 ⇡ 1, indicating that the dHDP has detected a change in the characteristics of the spikes), where the known neuron goes inactive. [sent-164, score-0.946]
</p><p>67 Thus, the model is more likely to draw new local clusters at this point, reﬂecting the nonstationary data. [sent-165, score-0.182]
</p><p>68 Additionally, in Figure 4(c) the global cluster usage shows a dramatic change at time block 11, where a cluster in the model goes inactive at the same time the known neuron is inactive. [sent-166, score-0.531]
</p><p>69 Because the dynamic model can map these dynamic properties, the results improve while using this model. [sent-167, score-0.18]
</p><p>70 Additionally, we obtain a global accuracy (across all channels) of 82. [sent-168, score-0.042]
</p><p>71 66% using the HDP-DL and an global accuracy of 84. [sent-169, score-0.042]
</p><p>72 1 The probability of introducing a new component for the 11th block  0. [sent-173, score-0.035]
</p><p>73 2 0  10  20 Block Index  30  (d)  Figure 4: Results of the multichannel dHDP on d561102. [sent-177, score-0.094]
</p><p>74 (a) ﬁrst 40 seconds of the intracellular recording  of d561102; (b) local cluster usage by each spike in the d561102 data in channel 4; (c) global cluster usage at (c) different time blocks for the data d561102; (d) sharing weight wj at each time blocks in the fourth channel. [sent-178, score-1.185]
</p><p>75 The spike in 11 occurs when the known neuron goes inactive. [sent-179, score-0.681]
</p><p>76 5  Conclusions  We have presented a new method for performing multi-channel spike sorting, in which the underlying features (dictionary elements) and sorting are performed jointly, while also allowing timeevolving variation in the spike statistics. [sent-180, score-1.297]
</p><p>77 The model adaptively learns dictionary elements of a wavelet-like nature (but not orthogonal), with characteristics like the shape of the spikes. [sent-181, score-0.289]
</p><p>78 Kalman ﬁlter mixture model for spike sorting of non-stationary data. [sent-196, score-0.804]
</p><p>79 Probabilistic inference of arm motion from neural activity in motor cortex. [sent-212, score-0.095]
</p><p>80 Intracellular feautures predicted by extracellular recordings in the hippocampus in vivo. [sent-240, score-0.153]
</p><p>81 A review of methods for spike sorting: the detection and classiﬁcation of neural action potentials. [sent-284, score-0.502]
</p><p>82 Brain-machine interfaces to restore motor function and probe neural circuits. [sent-297, score-0.107]
</p><p>83 Retrospective Markov Chain Monte Carlo methods for Dirichlet process hierarchiacal models. [sent-303, score-0.028]
</p><p>84 Improved spike-sorting by modeling ﬁring statistics and burst-dependent spike amplitude attenuation: A Markov Chain Monte Carlo approach. [sent-310, score-0.472]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.472), ('sorting', 0.302), ('spikes', 0.285), ('dictionary', 0.211), ('channel', 0.182), ('hdp', 0.181), ('channels', 0.175), ('neuron', 0.174), ('pcs', 0.166), ('dhdp', 0.166), ('gmm', 0.151), ('intracellular', 0.13), ('dirichlet', 0.128), ('pc', 0.127), ('extracellular', 0.119), ('clusters', 0.112), ('disappearance', 0.111), ('recording', 0.107), ('detected', 0.098), ('multichannel', 0.094), ('dynamic', 0.09), ('dp', 0.085), ('neurons', 0.08), ('cluster', 0.076), ('wavelet', 0.075), ('clustering', 0.07), ('kfm', 0.063), ('analyzers', 0.055), ('calabrese', 0.055), ('gorur', 0.055), ('pca', 0.052), ('appearance', 0.051), ('neuroscience', 0.051), ('nc', 0.051), ('spiky', 0.051), ('performed', 0.051), ('usage', 0.05), ('elements', 0.049), ('feature', 0.049), ('iv', 0.049), ('wood', 0.048), ('violations', 0.048), ('principal', 0.047), ('bk', 0.047), ('bienenstock', 0.045), ('devices', 0.045), ('interfaces', 0.043), ('refractory', 0.043), ('wavelets', 0.043), ('inactive', 0.043), ('infers', 0.043), ('hierarchical', 0.043), ('global', 0.042), ('nonstationary', 0.042), ('ck', 0.041), ('across', 0.041), ('neurophysiology', 0.04), ('components', 0.04), ('xn', 0.039), ('priors', 0.039), ('inappropriate', 0.039), ('orthogonal', 0.039), ('employed', 0.039), ('additionally', 0.038), ('block', 0.035), ('goes', 0.035), ('gao', 0.035), ('motor', 0.034), ('recordings', 0.034), ('arrows', 0.034), ('kalman', 0.032), ('construction', 0.032), ('matched', 0.032), ('gamma', 0.031), ('dk', 0.031), ('activity', 0.031), ('neural', 0.03), ('mixture', 0.03), ('truth', 0.03), ('id', 0.029), ('nature', 0.029), ('simultaneously', 0.029), ('ground', 0.029), ('sn', 0.028), ('jointly', 0.028), ('draw', 0.028), ('csicsvari', 0.028), ('daubechies', 0.028), ('henze', 0.028), ('pouzat', 0.028), ('proximate', 0.028), ('shoham', 0.028), ('braincomputer', 0.028), ('serruya', 0.028), ('shaikhouni', 0.028), ('improperly', 0.028), ('hierarchiacal', 0.028), ('papaspiliopoulos', 0.028), ('carlson', 0.028), ('imposition', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999881 <a title="200-tfidf-1" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>Author: Bo Chen, David E. Carlson, Lawrence Carin</p><p>Abstract: Nonparametric Bayesian methods are developed for analysis of multi-channel spike-train data, with the feature learning and spike sorting performed jointly. The feature learning and sorting are performed simultaneously across all channels. Dictionary learning is implemented via the beta-Bernoulli process, with spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP), with these two models coupled. The dHDP is augmented to eliminate refractoryperiod violations, it allows the “appearance” and “disappearance” of neurons over time, and it models smooth variation in the spike statistics. 1</p><p>2 0.38343063 <a title="200-tfidf-2" href="./nips-2011-A_blind_sparse_deconvolution_method_for_neural_spike_identification.html">13 nips-2011-A blind sparse deconvolution method for neural spike identification</a></p>
<p>Author: Chaitanya Ekanadham, Daniel Tranchina, Eero P. Simoncelli</p><p>Abstract: We consider the problem of estimating neural spikes from extracellular voltage recordings. Most current methods are based on clustering, which requires substantial human supervision and systematically mishandles temporally overlapping spikes. We formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform. Joint maximum-a-posteriori (MAP) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefﬁcients are sparse. We develop a block-coordinate descent procedure to approximate the MAP solution, based on our recently developed continuous basis pursuit method. We validate our method on simulated data as well as real data for which ground truth is available via simultaneous intracellular recordings. In both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering overlapping spikes. The method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors. 1</p><p>3 0.25542811 <a title="200-tfidf-3" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>4 0.20429656 <a title="200-tfidf-4" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>Author: Konrad Koerding, Ian Stevenson</p><p>Abstract: Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difﬁcult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modiﬁcation functions from pairs of spike trains. Then, using recursive point-process adaptive ﬁltering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modiﬁcation, we show that the true modiﬁcation function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data. 1</p><p>5 0.17511886 <a title="200-tfidf-5" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>Author: Mijung Park, Greg Horwitz, Jonathan W. Pillow</p><p>Abstract: A sizeable literature has focused on the problem of estimating a low-dimensional feature space for a neuron’s stimulus sensitivity. However, comparatively little work has addressed the problem of estimating the nonlinear function from feature space to spike rate. Here, we use a Gaussian process (GP) prior over the inﬁnitedimensional space of nonlinear functions to obtain Bayesian estimates of the “nonlinearity” in the linear-nonlinear-Poisson (LNP) encoding model. This approach offers increased ﬂexibility, robustness, and computational tractability compared to traditional methods (e.g., parametric forms, histograms, cubic splines). We then develop a framework for optimal experimental design under the GP-Poisson model using uncertainty sampling. This involves adaptively selecting stimuli according to an information-theoretic criterion, with the goal of characterizing the nonlinearity with as little experimental data as possible. Our framework relies on a method for rapidly updating hyperparameters under a Gaussian approximation to the posterior. We apply these methods to neural data from a color-tuned simple cell in macaque V1, characterizing its nonlinear response function in the 3D space of cone contrasts. We ﬁnd that it combines cone inputs in a highly nonlinear manner. With simulated experiments, we show that optimal design substantially reduces the amount of data required to estimate these nonlinear combination rules. 1</p><p>6 0.16075684 <a title="200-tfidf-6" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>7 0.1472888 <a title="200-tfidf-7" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>8 0.13438781 <a title="200-tfidf-8" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>9 0.13246587 <a title="200-tfidf-9" href="./nips-2011-From_Stochastic_Nonlinear_Integrate-and-Fire_to_Generalized_Linear_Models.html">99 nips-2011-From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models</a></p>
<p>10 0.1309703 <a title="200-tfidf-10" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>11 0.13082366 <a title="200-tfidf-11" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>12 0.11926091 <a title="200-tfidf-12" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>13 0.11255695 <a title="200-tfidf-13" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>14 0.11184762 <a title="200-tfidf-14" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>15 0.10453863 <a title="200-tfidf-15" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>16 0.10166419 <a title="200-tfidf-16" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>17 0.10036051 <a title="200-tfidf-17" href="./nips-2011-Energetically_Optimal_Action_Potentials.html">87 nips-2011-Energetically Optimal Action Potentials</a></p>
<p>18 0.099810138 <a title="200-tfidf-18" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>19 0.099735662 <a title="200-tfidf-19" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>20 0.098236807 <a title="200-tfidf-20" href="./nips-2011-Estimating_time-varying_input_signals_and_ion_channel_states_from_a_single_voltage_trace_of_a_neuron.html">89 nips-2011-Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.168), (2, 0.312), (3, -0.049), (4, 0.077), (5, 0.006), (6, -0.045), (7, 0.042), (8, -0.008), (9, 0.093), (10, 0.105), (11, 0.254), (12, -0.066), (13, -0.029), (14, -0.095), (15, -0.062), (16, -0.033), (17, -0.008), (18, 0.01), (19, -0.006), (20, 0.2), (21, 0.028), (22, 0.026), (23, -0.108), (24, -0.031), (25, -0.022), (26, 0.032), (27, 0.125), (28, -0.151), (29, 0.161), (30, 0.057), (31, -0.035), (32, -0.034), (33, 0.157), (34, 0.066), (35, 0.069), (36, -0.044), (37, -0.084), (38, 0.046), (39, 0.064), (40, 0.019), (41, -0.063), (42, -0.014), (43, -0.019), (44, 0.073), (45, 0.024), (46, 0.053), (47, -0.048), (48, -0.008), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97073448 <a title="200-lsi-1" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>Author: Bo Chen, David E. Carlson, Lawrence Carin</p><p>Abstract: Nonparametric Bayesian methods are developed for analysis of multi-channel spike-train data, with the feature learning and spike sorting performed jointly. The feature learning and sorting are performed simultaneously across all channels. Dictionary learning is implemented via the beta-Bernoulli process, with spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP), with these two models coupled. The dHDP is augmented to eliminate refractoryperiod violations, it allows the “appearance” and “disappearance” of neurons over time, and it models smooth variation in the spike statistics. 1</p><p>2 0.91423327 <a title="200-lsi-2" href="./nips-2011-A_blind_sparse_deconvolution_method_for_neural_spike_identification.html">13 nips-2011-A blind sparse deconvolution method for neural spike identification</a></p>
<p>Author: Chaitanya Ekanadham, Daniel Tranchina, Eero P. Simoncelli</p><p>Abstract: We consider the problem of estimating neural spikes from extracellular voltage recordings. Most current methods are based on clustering, which requires substantial human supervision and systematically mishandles temporally overlapping spikes. We formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform. Joint maximum-a-posteriori (MAP) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefﬁcients are sparse. We develop a block-coordinate descent procedure to approximate the MAP solution, based on our recently developed continuous basis pursuit method. We validate our method on simulated data as well as real data for which ground truth is available via simultaneous intracellular recordings. In both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering overlapping spikes. The method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors. 1</p><p>3 0.68879604 <a title="200-lsi-3" href="./nips-2011-From_Stochastic_Nonlinear_Integrate-and-Fire_to_Generalized_Linear_Models.html">99 nips-2011-From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models</a></p>
<p>Author: Skander Mensi, Richard Naud, Wulfram Gerstner</p><p>Abstract: Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-theart models from both schools of thought. First we ﬁnd the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-andFire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the ﬁring probability given a deterministic membrane potential. We ﬁnd a mathematical expression for this link-function and test the ability of the GLM to predict the ﬁring probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we ﬁnd that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models. 1 Motivation When it comes to modeling the intrinsic variability in simple neuron models, we can distinguish two traditional approaches. One approach is inspired by the stochastic Leaky Integrate-and-Fire (LIF) hypothesis of Stein (1967) [1], where a noise term is added to the system of differential equations implementing the leaky integration to a threshold. There are multiple versions of such a stochastic LIF [2]. How the noise affects the ﬁring probability is also a function of the parameters of the neuron model. Therefore, it is important to take into account the reﬁnements of simple neuron models in terms of subthreshold resonance [3, 4], spike-triggered adaptation [5, 6] and non-linear spike 1 initiation [7, 5]. All these improvements are encompassed by the Adaptive Exponential Integrateand-Fire model (AdEx [8, 9]). The other approach is to start with some deterministic dynamics for the the state of the neuron (for instance the instantaneous distance from the membrane potential to the threshold) and link the probability intensity of emitting a spike with a non-linear function of the state variable. Under some conditions, this type of model is part of a greater class of statistical models called Generalized Linear Models (GLM [10]). As a single neuron model, the Spike Response Model (SRM) with escape noise is a GLM in which the state variable is explicitly the distance between a deterministic voltage and the threshold. The original SRM could account for subthreshold resonance, refractory effects and spike-frequency adaptation [11]. Mathematically similar models were developed independently in the study of the visual system [12] where spike-frequency adaptation has also been modeled [13]. Recently, this approach has retained increased attention since the probabilistic framework can be linked with the Bayesian theory of neural systems [14] and because Bayesian inference can be applied to the population of neurons [15]. In this paper, we investigate the similarity and differences between the state-of-the-art GLM and the stochastic AdEx. The motivation behind this work is to relate the traditional threshold neuron models to Bayesian theory. Our results extend the work of Plesser and Gerstner (2000) [16] since we include the non-linearity for spike initiation and spike-frequency adaptation. We also provide relationships between the parameters of the AdEx and the equivalent GLM. These precise relationships can be used to relate analog implementations of threshold models [17] to the probabilistic models used in the Bayesian approach. The paper is organized as follows: We ﬁrst describe the expressions relating the SRM state-variable to the parameters of the AdEx (Sect. 3.1) in the subthreshold regime. Then, we use numerical methods to ﬁnd the non-linear link-function that models the ﬁring probability (Sect. 3.2). We ﬁnd a functional form for the SRM link-function that best describes the ﬁring probability of a stochastic AdEx. We then compare the performance of this link-function with the often used exponential or linear-rectiﬁer link-functions (also called half-wave linear rectiﬁer) in terms of predicting the ﬁring probability of an AdEx under complex stimulus (Sect. 3.3). We ﬁnd that the exponential linkfunction yields almost perfect prediction. Finally, we explore the relations between the statistic of the noise and the sharpness of the non-linearity for spike initiation with the parameters of the SRM. 2 Presentation of the Models In this section we present the general formula for the stochastic AdEx model (Sect. 2.1) and the SRM (Sect 2.2). 2.1 The Stochastic Adaptive Exponential Integrate-and-Fire Model The voltage dynamics of the stochastic AdEx is given by: V −Θ ˙ τm V = El − V + ∆T exp − Rw + RI + R (1) ∆T τw w = a(V − El ) − w ˙ (2) where τm is the membrane time constant, El the reverse potential, R the membrane resistance, Θ is the threshold, ∆T is the shape factor and I(t) the input current which is chosen to be an Ornstein−Θ Uhlenbeck process with correlation time-constant of 5 ms. The exponential term ∆T exp( V∆T ) is a non-linear function responsible for the emission of spikes and is a diffusive white noise with standard deviation σ (i.e. ∼ N (0, σ)). Note that the diffusive white-noise does not imply white noise ﬂuctuations of the voltage V (t), the probability distribution of V (t) will depend on ∆T and Θ. The second variable, w, describes the subthreshold as well as the spike-triggered adaptation both ˆ parametrized by the coupling strength a and the time constant τw . Each time tj the voltage goes to inﬁnity, we assumed that a spike is emitted. Then the voltage is reset to a ﬁxed value Vr and w is increased by a constant value b. 2.2 The Generalized Linear Model In the SRM, The voltage V (t) is given by the convolution of the injected current I(t) with the membrane ﬁlter κ(t) plus the additional kernel η(t) that acts after each spikes (here we split the 2 spike-triggered kernel in two η(t) = ηv (t) + ηw (t) for reasons that will become clear later): V (t) = ˆ ˆ ηv (t − tj ) + ηw (t − tj ) El + [κ ∗ I](t) + (3) ˆ {tj } ˆ Then at each time tj a spike is emitted which results in a change of voltage described by η(t) = ηv (t) + ηw (t). Given the deterministic voltage, (Eq. 3) a spike is emitted according to the ﬁring intensity λ(V ): λ(t) = f (V (t)) (4) where f (·) is an arbitrary function called the link-function. Then the ﬁring behavior of the SRM depends on the choice of the link-function and its parameters. The most common link-function used to model single neuron activities are the linear-rectiﬁer and the exponential function. 3 Mapping In order to map the stochastic AdEx to the SRM we follow a two-step procedure. First we derive the ﬁlter κ(t) and the kernels ηv (t) and ηw (t) analytically as a function of AdEx parameters. Second, we derive the link-function of the SRM from the stochastic spike emission of the AdEx. Figure 1: Mapping of the subthreshold dynamics of an AdEx to an equivalent SRM. A. Membrane ﬁlter κ(t) for three different sets of parameters of the AdEx leading to over-damped, critically damped and under-damped cases (upper, middle and lower panel, respectively). B. Spike-Triggered η(t) (black), ηv (t) (light gray) and ηw (gray) for the three cases. C. Example of voltage trace produced when an AdEx is stimulated with a step of colored noise (black). The corresponding voltage from a SRM stimulated with the same current and where we forced the spikes to match those of the AdEx (red). D. Error in the subthreshold voltage (VAdEx − VGLM ) as a function of the mean voltage of the AdEx, for the three different cases: over-, critically and under-damped (light gray, gray and black, respectively) with ∆T = 1 mV. Red line represents the voltage threshold Θ. E. Root Mean Square Error (RMSE) ratio for the three cases with ∆T = 1 mV. The RMSE ratio is the RMSE between the deterministic VSRM and the stochastic VAdEx divided by the RMSE between repetitions of the stochastic AdEx voltage. The error bar shows a single standard deviation as the RMSE ratio is averaged accross multiple value of σ. 3.1 Subthreshold voltage dynamics We start by assuming that the non-linearity for spike initiation does not affect the mean subthreshold voltage of the stochastic AdEx (see Figure 1 D). This assumption is motivated by the small ∆T 3 observed in in-vitro recordings (from 0.5 to 2 mV [8, 9]) which suggest that the subthreshold dynamics are mainly linear except very close to Θ. Also, we expect that the non-linear link-function will capture some of the dynamics due to the non-linearity for spike initiation. Thus it is possible to rewrite the deterministic subthreshold part of the AdEx (Eq. 1-2 without and without ∆T exp((V − Θ)/∆T )) using matrices: ˙ x = Ax (5) with x = V w and A = − τ1 m a τw − gl1m τ − τ1 w (6) In this form, the dynamics of the deterministic AdEx voltage is a damped oscillator with a driving force. Depending on the eigenvalues of A the system could be over-damped, critically damped or under-damped. The ﬁlter κ(t) of the GLM is given by the impulse response of the system of coupled differential equations of the AdEx, described by Eq. 5 and 6. In other words, one has to derive the response of the system when stimulating with a Dirac-delta function. The type of damping gives three different qualitative shapes of the kernel κ(t), which are summarized in Table 3.1 and Figure 1 A. Since the three different ﬁlters also affect the nature of the stochastic voltage ﬂuctuations, we will keep the distinction between over-damped, critically damped and under-damped scenarios throughout the paper. This means that our approach is valid for at least 3 types of diffusive voltage-noise (i.e. the white noise in Eq. 1 ﬁltered by 3 different membrane ﬁlters κ(t)). To complete the description of the deterministic voltage, we need an expression for the spiketriggered kernels. The voltage reset at each spike brings a spike-triggered jump in voltage of magˆ nitude ∆ = Vr − V (t). This perturbation is superposed to the current ﬂuctuations due to I(t) and can be mediated by a Delta-diract pulse of current. Thus we can write the voltage reset kernel by: ηv (t) = ∆ ∆ [δ ∗ κ] (t) = κ(t) κ(0) κ(0) (7) where δ(t) is the Dirac-delta function. The shape of this kernel depends on κ(t) and can be computed from Table 3.1 (see Figure 1 B). Finally, the AdEx mediates spike-frequency adaptation by the jump of the second variables w. From Eq. 2 we can see that this produces a current wspike (t) = b exp (−t/τw ) that can cumulate over subsequent spikes. The effect of this current on voltage is then given by the convolution of wspike (t) with the membrane ﬁlter κ(t). Thus in the SRM framework the spike-frequency adaptation is taken into account by: ηw (t) = [wspike ∗ κ](t) (8) Again the precise form of ηw (t) depends on κ(t) and can be computed from Table 3.1 (see Figure 1 B). At this point, we would like to verify our assumption that the non-linearity for spike emission can be neglected. Fig. 1 C and D shows that the error between the voltage from Eq. 3 and the voltage from the stochastic AdEx is generally small. Moreover, we see that the main contribution to the voltage prediction error is due to the mismatch close to the spikes. However the non-linearity for spike initiation may change the probability distribution of the voltage ﬂuctuations, which in turn inﬂuences the probability of spiking. This will inﬂuence the choice of the link-function, as we will see in the next section. 3.2 Spike Generation Using κ(t), ηv (t) and ηw (t), we must relate the spiking probability of the stochastic AdEx as a function of its deterministic voltage. According to [2] the probability of spiking in time bin dt given the deterministic voltage V (t) is given by: p(V ) = prob{spike in [t, t + dt]} = 1 − exp (−f (V (t))dt) (9) where f (·) gives the ﬁring intensity as a function of the deterministic V (t) (Eq. 3). Thus to extract the link-function f we have to compute the probability of spiking given V (t) for our SRM. To do so we apply the method proposed by Jolivet et al. (2004) [18], where the probability of spiking is simply given by the distribution of the deterministic voltage estimated at the spike times divided by the distribution of the SRM voltage when there is no spike (see ﬁgure 2 A). One can numerically compute these two quantities for our models using N repetitions of the same stimulus. 4 Table 1: Analytical expressions for the membrane ﬁlter κ(t) in terms of the parameters of the AdEx for over-, critically-, and under-damped cases. Membrane Filter: κ(t) over-damped if: (τm + τw )2 > 4τm τw (gl +a) gl κ(t) = k1 eλ1 t + k2 eλ2 t λ1 = 1 2τm τw (−(τm + τw ) + critically-damped if: (τm + τw )2 = 4τm τw (gl +a) gl κ(t) = (αt + β)eλt λ= under-damped if: (τm + τw )2 < 4τm τw (gl +a) gl κ(t) = (k1 cos (ωt) + k2 sin (ωt)) eλt −(τm +τw ) 2τm τw λ= −(τm +τw ) 2τm τw (τm + τw )2 − 4 τm τw (gl + a) gl λ2 = 1 2τm τw (−(τm + τw ) − α= τm −τw 2Cτm τw ω= τw −τm 2τm τw 2 − a g l τm τw (τm + τw )2 − 4 τm τw (gl + a) gl k1 = −(1+(τm λ2 )) Cτm (λ1 −λ2 ) k2 = 1+(τm λ1 ) Cτm (λ1 −λ2 ) β= 1 C k1 = k2 = 1 C −(1+τm λ) Cωτm The standard deviation σ of the noise and the parameter ∆T of the AdEx non-linearity may affect the shape of the link-function. We thus extract p(V ) for different σ and ∆T (Fig. 2 B). Then using visual heuristics and previous knowledge about the potential analytical expression of the link-funtion, we try to ﬁnd a simple analytical function that captures p(V ) for a large range of combinations of σ and ∆T . We observed that the log(− log(p)) is close to linear in most studied conditions Fig. 2 B suggesting the following two distributions of p(V ): V − VT (10) p(V ) = 1 − exp − exp ∆V V − VT p(V ) = exp − exp − (11) ∆V Once we have p(V ), we can use Eq. 4 to obtain the equivalent SRM link-function, which leads to: −1 f (V ) = log (1 − p(V )) (12) dt Then the two potential link-functions of the SRM can be derived from Eq. 10 and Eq. 11 (respectively): V − VT f (V ) = λ0 exp (13) ∆V V − VT (14) f (V ) = −λ0 log 1 − exp − exp − ∆V 1 with λ0 = dt , VT the threshold of the SRM and ∆V the sharpness of the link-function (i.e. the parameters that governs the degree of the stochasticity). Note that the exact value of λ0 has no importance since it is redundant with VT . Eq. 13 is the standard exponential link-function, but we call Eq. 14 the log-exp-exp link-function. 3.3 Prediction The next point is to evaluate the ﬁt quality of each link-function. To do this, we ﬁrst estimate the parameters VT and ∆V of the GLM link-function that maximize the likelihood of observing a spike 5 Figure 2: SRM link-function. A. Histogram of the SRM voltage at the AdEx ﬁring times (red) and at non-ﬁring times (gray). The ratio of the two distributions gives p(V ) (Eq. 9, dashed lines). Inset, zoom to see the voltage histogram evaluated at the ﬁring time (red). B. log(− log(p)) as a function of the SRM voltage for three different noise levels σ = 0.07, 0.14, 0.18 nA (pale gray, gray, black dots, respectively) and ∆T = 1 mV. The line is a linear ﬁt corresponding to the log-exp-exp linkfunction and the dashed line corresponds to a ﬁt with the exponential link-function. C. Same data and labeling scheme as B, but plotting f (V ) according to Eq. 12. The lines are produced with Eq. 14 with parameters ﬁtted as described in B. and the dashed lines are produced with Eq. 13. Inset, same plot but on a semi-log(y) axis. train generated with an AdEx. Second we look at the predictive power of the resulting SRM in terms of Peri-Stimulus Time Histogram (PSTH). In other words we ask how close the spike trains generated with a GLM are from the spike train generated with a stochastic AdEx when both models are stimulated with the same input current. For any GLM with link-function f (V ) ≡ f (t|I, θ) and parameters θ regulating the shape of κ(t), ˆ ηv (t) and ηw (t), the Negative Log-Likelihood (NLL) of observing a spike-train {t} is given by:   NLL = − log(f (t|I, θ)) − f (t|I, θ) (15) t ˆ t It has been shown that the negative log-likelihood is convex in the parameters if f is convex and logconcave [19]. It is easy to show that a linear-rectiﬁer link-function, the exponential link-function and the log-exp-exp link-function all satisfy these conditions. This allows efﬁcient estimation of ˆ ˆ the optimal parameters VT and ∆V using a simple gradient descent. One can thus estimate from a single AdEx spike train the optimal parameters of a given link-function, which is more efﬁcient than the method used in Sect. 3.2. The minimal NLL resulting from the gradient descent gives an estimation of the ﬁt quality. A better estimate of the ﬁt quality is given by the distance between the PSTHs in response to stimuli not used for parameter ﬁtting . Let ν1 (t) be the PSTH of the AdEx, and ν2 (t) be the PSTH of the ﬁtted SRM, 6 Figure 3: PSTH prediction. A. Injected current. B. Voltage traces produced by an AdEx (black) and the equivalent SRM (red), when stimulated with the current in A. C. Raster plot for 20 realizations of AdEx (black tick marks) and equivalent SRM (red tick marks). D. PSTH of the AdEx (black) and the SRM (red) obtained by averaging 10,000 repetitions. E. Optimal log-likelihood for the three cases of the AdEx, using three different link-functions, a linear-rectiﬁer (light gray), an exponential link-function (gray) and the link-function deﬁned by Eq. 14 (dark gray), these values are obtained by averaging over 40 different combinations σ and ∆T (see Fig. 4). Error bars are one standard deviation, the stars denote a signiﬁcant difference, two-sample t-test with α = 0.01. F. same as E. but for Md (Eq. 16). then we use Md ∈ [0, 1] as a measure of match: Md = 2 2 (ν1 (t) − ν2 (t)) dt ν1 (t)2 dt + ν2 (t)2 dt (16) Md = 1 means that it is impossible to differentiate the SRM from the AdEx in terms of their PSTHs, whereas a Md of 0 means that the two PSTHs are completely different. Thus Md is a normalized similarity measure between two PSTHs. In practice, Md is estimated from the smoothed (boxcar average of 1 ms half-width) averaged spike train of 1 000 repetitions for each models. We use both the NLL and Md to quantify the ﬁt quality for each of the three damping cases and each of the three link-functions. Figure 3 shows the match between the stochastic AdEx used as a reference and the derived GLM when both are stimulated with the same input current (Fig. 3 A). The resulting voltage traces are almost identical (Fig. 3 B) and both models predict almost the same spike trains and so the same PSTHs (Fig. 3 C and D). More quantitalively, we see on Fig. 3 E and F, that the linear-rectiﬁer ﬁts signiﬁcantly worse than both the exponential and log-exp-exp link-functions, both in terms of NLL and of Md . The exponential link-function performs as well as the log-exp-exp link-function, with a spike train similarity measure Md being almost 1 for both. Finally the likelihood-based method described above gives us the opportunity to look at the relationship between the AdEx parameters σ and ∆T that governs its spike emission and the parameters VT and ∆V of the link-function (Fig. 4). We observe that an increase of the noise level produces a ﬂatter link-function (greater ∆V ) while an increase in ∆T also produces an increase in ∆V and VT (note that Fig. 4 shows ∆V and VT for the exponential link-function only, but equivalent results are obtained with the log-exp-exp link-function). 4 Discussion In Sect. 3.3 we have shown that it is possible to predict with almost perfect accuracy the PSTH of a stochastic AdEx model using an appropriate set of parameters in the SRM. Moreover, since 7 Figure 4: Inﬂuence of the AdEx parameters on the parameters of the exponential link-function. A. VT as a function of ∆T and σ. B. ∆V as a function of ∆T and σ. the subthreshold voltage of the AdEx also gives a good match with the deterministic voltage of the SRM, we expect that the AdEx and the SRM will not differ in higher moments of the spike train probability distributions beyond the PSTH. We therefore conclude that diffusive noise models of the type of Eq. 1-2 are equivalent to GLM of the type of Eq. 3-4. Once combined with similar results on other types of stochastic LIF (e.g. correlated noise), we could bridge the gap between the literature on GLM and the literature on diffusive noise models. Another noteworthy observation pertains to the nature of the link-function. The link-function has been hypothesized to be a linear-rectiﬁer, an exponential, a sigmoidal or a Gaussian [16]. We have observed that for the AdEx the link-function follows Eq. 14 that we called the log-exp-exp linkfunction. Although the link-function is log-exp-exp for most of the AdEx parameters, the exponential link-function gives an equivalently good prediction of the PSTH. This can be explained by the fact that the difference between log-exp-exp and exponential link-functions happens mainly at low voltage (i.e. far from the threshold), where the probability of emitting a spike is so low (Figure 2 C, until -50 mv). Therefore, even if the exponential link-function overestimates the ﬁring probability at these low voltages it rarely produces extra spikes. At voltages closer to the threshold, where most of the spikes are emitted, the two link-functions behave almost identically and hence produce the same PSTH. The Gaussian link-function can be seen as lying in-between the exponential link-function and the log-exp-exp link-function in Fig. 2. This means that the work of Plesser and Gerstner (2000) [16] is in agreement with the results presented here. The importance of the time-derivative of the ˙ voltage stressed by Plesser and Gerstner (leading to a two-dimensional link-function f (V, V )) was not studied here to remain consistent with the typical usage of GLM in neural systems [14]. Finally we restricted our study to exponential non-linearity for spike initiation and do not consider other cases such as the Quadratic Integrate-and-ﬁre (QIF, [5]) or other polynomial functional shapes. We overlooked these cases for two reasons. First, there are many evidences that the non-linearity in neurons (estimated from in-vitro recordings of Pyramidal neurons) is well approximated by a single exponential [9]. Second, the exponential non-linearity of the AdEx only affects the subthreshold voltage at high voltage (close to threshold) and thus can be neglected to derive the ﬁlters κ(t) and η(t). Polynomial non-linearities on the other hand affect a larger range of the subthreshold voltage so that it would be difﬁcult to justify the linearization of subthreshold dynamics essential to the method presented here. References [1] R. B. Stein, “Some models of neuronal variability,” Biophys J, vol. 7, no. 1, pp. 37–68, 1967. [2] W. Gerstner and W. Kistler, Spiking neuron models. Cambridge University Press New York, 2002. [3] E. Izhikevich, “Resonate-and-ﬁre neurons,” Neural Networks, vol. 14, no. 883-894, 2001. [4] M. J. E. Richardson, N. Brunel, and V. Hakim, “From subthreshold to ﬁring-rate resonance,” Journal of Neurophysiology, vol. 89, pp. 2538–2554, 2003. 8 [5] E. Izhikevich, “Simple model of spiking neurons,” IEEE Transactions on Neural Networks, vol. 14, pp. 1569–1572, 2003. [6] S. Mensi, R. Naud, M. Avermann, C. C. H. Petersen, and W. Gerstner, “Parameter extraction and classiﬁcation of three neuron types reveals two different adaptation mechanisms,” Under review. [7] N. Fourcaud-Trocme, D. Hansel, C. V. Vreeswijk, and N. Brunel, “How spike generation mechanisms determine the neuronal response to ﬂuctuating inputs,” Journal of Neuroscience, vol. 23, no. 37, pp. 11 628–11 640, 2003. [8] R. Brette and W. Gerstner, “Adaptive exponential integrate-and-ﬁre model as an effective description of neuronal activity,” Journal of Neurophysiology, vol. 94, pp. 3637–3642, 2005. [9] L. Badel, W. Gerstner, and M. Richardson, “Dependence of the spike-triggered average voltage on membrane response properties,” Neurocomputing, vol. 69, pp. 1062–1065, 2007. [10] P. McCullagh and J. A. Nelder, Generalized linear models, 2nd ed. Chapman & Hall/CRC, 1998, vol. 37. [11] W. Gerstner, J. van Hemmen, and J. Cowan, “What matters in neuronal locking?” Neural computation, vol. 8, pp. 1653–1676, 1996. [12] D. Hubel and T. Wiesel, “Receptive ﬁelds and functional architecture of monkey striate cortex,” Journal of Physiology, vol. 195, pp. 215–243, 1968. [13] J. Pillow, L. Paninski, V. Uzzell, E. Simoncelli, and E. Chichilnisky, “Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model,” Journal of Neuroscience, vol. 25, no. 47, pp. 11 003–11 013, 2005. [14] K. Doya, S. Ishii, A. Pouget, and R. P. N. Rao, Bayesian brain: Probabilistic approaches to neural coding. The MIT Press, 2007. [15] S. Gerwinn, J. H. Macke, M. Seeger, and M. Bethge, “Bayesian inference for spiking neuron models with a sparsity prior,” in Advances in Neural Information Processing Systems, 2007. [16] H. Plesser and W. Gerstner, “Noise in integrate-and-ﬁre neurons: From stochastic input to escape rates,” Neural Computation, vol. 12, pp. 367–384, 2000. [17] J. Schemmel, J. Fieres, and K. Meier, “Wafer-scale integration of analog neural networks,” in Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, june 2008, pp. 431 –438. [18] R. Jolivet, T. Lewis, and W. Gerstner, “Generalized integrate-and-ﬁre models of neuronal activity approximate spike trains of a detailed model to a high degree of accuracy,” Journal of Neurophysiology, vol. 92, pp. 959–976, 2004. [19] L. Paninski, “Maximum likelihood estimation of cascade point-process neural encoding models,” Network: Computation in Neural Systems, vol. 15, pp. 243–262, 2004. 9</p><p>4 0.68494797 <a title="200-lsi-4" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>Author: Konrad Koerding, Ian Stevenson</p><p>Abstract: Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difﬁcult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modiﬁcation functions from pairs of spike trains. Then, using recursive point-process adaptive ﬁltering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modiﬁcation, we show that the true modiﬁcation function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data. 1</p><p>5 0.55595112 <a title="200-lsi-5" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>6 0.52250844 <a title="200-lsi-6" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>7 0.50567323 <a title="200-lsi-7" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>8 0.49203989 <a title="200-lsi-8" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>9 0.48640206 <a title="200-lsi-9" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>10 0.48165107 <a title="200-lsi-10" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>11 0.45775837 <a title="200-lsi-11" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>12 0.40264085 <a title="200-lsi-12" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>13 0.36710998 <a title="200-lsi-13" href="./nips-2011-Bayesian_Spike-Triggered_Covariance_Analysis.html">44 nips-2011-Bayesian Spike-Triggered Covariance Analysis</a></p>
<p>14 0.36120558 <a title="200-lsi-14" href="./nips-2011-Estimating_time-varying_input_signals_and_ion_channel_states_from_a_single_voltage_trace_of_a_neuron.html">89 nips-2011-Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron</a></p>
<p>15 0.36101246 <a title="200-lsi-15" href="./nips-2011-Signal_Estimation_Under_Random_Time-Warpings_and_Nonlinear_Signal_Alignment.html">253 nips-2011-Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment</a></p>
<p>16 0.35544127 <a title="200-lsi-16" href="./nips-2011-Energetically_Optimal_Action_Potentials.html">87 nips-2011-Energetically Optimal Action Potentials</a></p>
<p>17 0.35079437 <a title="200-lsi-17" href="./nips-2011-Emergence_of_Multiplication_in_a_Biophysical_Model_of_a_Wide-Field_Visual_Neuron_for_Computing_Object_Approaches%3A_Dynamics%2C_Peaks%2C_%26_Fits.html">85 nips-2011-Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, & Fits</a></p>
<p>18 0.34676832 <a title="200-lsi-18" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>19 0.33821443 <a title="200-lsi-19" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>20 0.32957828 <a title="200-lsi-20" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (4, 0.028), (20, 0.016), (26, 0.015), (31, 0.064), (33, 0.036), (43, 0.054), (45, 0.091), (57, 0.026), (74, 0.094), (83, 0.107), (84, 0.331), (99, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81510246 <a title="200-lda-1" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>Author: Bo Chen, David E. Carlson, Lawrence Carin</p><p>Abstract: Nonparametric Bayesian methods are developed for analysis of multi-channel spike-train data, with the feature learning and spike sorting performed jointly. The feature learning and sorting are performed simultaneously across all channels. Dictionary learning is implemented via the beta-Bernoulli process, with spike sorting performed via the dynamic hierarchical Dirichlet process (dHDP), with these two models coupled. The dHDP is augmented to eliminate refractoryperiod violations, it allows the “appearance” and “disappearance” of neurons over time, and it models smooth variation in the spike statistics. 1</p><p>2 0.74088937 <a title="200-lda-2" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>Author: Carsten Rother, Martin Kiefel, Lumin Zhang, Bernhard Schölkopf, Peter V. Gehler</p><p>Abstract: We address the challenging task of decoupling material properties from lighting properties given a single image. In the last two decades virtually all works have concentrated on exploiting edge information to address this problem. We take a different route by introducing a new prior on reﬂectance, that models reﬂectance values as being drawn from a sparse set of basis colors. This results in a Random Field model with global, latent variables (basis colors) and pixel-accurate output reﬂectance values. We show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information. Finally, we are able to improve on state-of-the-art results by integrating edge information into our model. We believe that our new approach is an excellent starting point for future developments in this ﬁeld. 1</p><p>3 0.71851599 <a title="200-lda-3" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that ﬁt the empirical data distribution. We instantiate this similarity measure with the Gammacompound-Laplace distribution, and show signiﬁcant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost. 1</p><p>4 0.6925202 <a title="200-lda-4" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>Author: Florian Stimberg, Manfred Opper, Guido Sanguinetti, Andreas Ruttor</p><p>Abstract: We consider the problem of Bayesian inference for continuous-time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times. We propose exact inference and sampling methodologies for two speciﬁc cases where the discontinuous dynamics is given by a Poisson process and a two-state Markovian switch. We test the methodology on simulated data, and apply it to two real data sets in ﬁnance and systems biology. Our experimental results show that the approach leads to valid inferences and non-trivial insights. 1</p><p>5 0.53421724 <a title="200-lda-5" href="./nips-2011-A_blind_sparse_deconvolution_method_for_neural_spike_identification.html">13 nips-2011-A blind sparse deconvolution method for neural spike identification</a></p>
<p>Author: Chaitanya Ekanadham, Daniel Tranchina, Eero P. Simoncelli</p><p>Abstract: We consider the problem of estimating neural spikes from extracellular voltage recordings. Most current methods are based on clustering, which requires substantial human supervision and systematically mishandles temporally overlapping spikes. We formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform. Joint maximum-a-posteriori (MAP) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefﬁcients are sparse. We develop a block-coordinate descent procedure to approximate the MAP solution, based on our recently developed continuous basis pursuit method. We validate our method on simulated data as well as real data for which ground truth is available via simultaneous intracellular recordings. In both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering overlapping spikes. The method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors. 1</p><p>6 0.52031797 <a title="200-lda-6" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>7 0.51731431 <a title="200-lda-7" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>8 0.50689799 <a title="200-lda-8" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>9 0.50001913 <a title="200-lda-9" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>10 0.49899226 <a title="200-lda-10" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>11 0.49727073 <a title="200-lda-11" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>12 0.4913618 <a title="200-lda-12" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>13 0.49055025 <a title="200-lda-13" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>14 0.48445565 <a title="200-lda-14" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>15 0.47923586 <a title="200-lda-15" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>16 0.47289845 <a title="200-lda-16" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>17 0.47270158 <a title="200-lda-17" href="./nips-2011-Demixed_Principal_Component_Analysis.html">68 nips-2011-Demixed Principal Component Analysis</a></p>
<p>18 0.47239396 <a title="200-lda-18" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>19 0.47217685 <a title="200-lda-19" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>20 0.47163165 <a title="200-lda-20" href="./nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
