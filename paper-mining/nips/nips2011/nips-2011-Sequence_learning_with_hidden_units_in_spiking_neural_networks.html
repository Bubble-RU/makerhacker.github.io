<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 nips-2011-Sequence learning with hidden units in spiking neural networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-249" href="#">nips2011-249</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>249 nips-2011-Sequence learning with hidden units in spiking neural networks</h1>
<br/><p>Source: <a title="nips-2011-249-pdf" href="http://papers.nips.cc/paper/4383-sequence-learning-with-hidden-units-in-spiking-neural-networks.pdf">pdf</a></p><p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>Reference: <a title="nips-2011-249-reference" href="../nips2011_reference/nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sequence learning with hidden units in spiking neural networks  Johanni Brea, Walter Senn and Jean-Pascal Pﬁster Department of Physiology University of Bern B¨ hlplatz 5 u CH-3012 Bern, Switzerland {brea, senn, pfister}@pyl. [sent-1, score-0.521]
</p><p>2 ch  Abstract We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. [sent-3, score-0.788]
</p><p>3 Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. [sent-4, score-1.92]
</p><p>4 We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. [sent-5, score-1.06]
</p><p>5 Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited. [sent-6, score-0.705]
</p><p>6 1 Introduction Learning to produce temporal sequences is a general problem that the brain needs to solve. [sent-7, score-0.185]
</p><p>7 Early attempts to model sequence learning used a simple asymmetric Hebbian learning rule [10, 20, 6] and succeeded to store sequences of random patterns, but perform poorly as soon as there are temporal correlations between the patterns [3]. [sent-9, score-0.511]
</p><p>8 Other studies [14] included a reservoir of hidden neurons but assumed weights towards the hidden neurons to be ﬁxed. [sent-11, score-1.957]
</p><p>9 Here we start by deﬁning a stochastic neuronal dynamics - that can be arbitrarily complicated (e. [sent-13, score-0.159]
</p><p>10 This stochastic dynamics deﬁnes the overall probability distribution which is parametrized by the synaptic weights. [sent-16, score-0.181]
</p><p>11 The goal of learning is to adapt synaptic weights such that the model distribution approximates as good as possible the target distribution of temporal sequences. [sent-17, score-0.274]
</p><p>12 This can be seen as the extension of the maximum likelihood approach of Barber [2] where we add stochastic hidden neurons with plastic weights. [sent-18, score-0.849]
</p><p>13 1  A  B  ht−1  ht  stochastic hidden neurons  ht−1  ht  vt−1  vt  stochastic visible neurons  vt−1  vt  Figure 1: Graphical representation of the conditional dependencies of the joint distribution over visible and hidden sequences. [sent-20, score-2.984]
</p><p>14 A Graphical model used for the derivation of the learning rule in section 2 and the example in section 4. [sent-21, score-0.173]
</p><p>15 B Markovian model used in the example with binary neurons in section 3. [sent-22, score-0.49]
</p><p>16 The resulting learning rule is local (but modulated by a global factor), causal and biologically relevant in the sense that it shares important features with Spike-Timing Dependent Plasticity (STDP). [sent-23, score-0.252]
</p><p>17 We also derive an online version of the learning rule and show numerically that it performs almost equally well as the exact batch learning rule. [sent-24, score-0.277]
</p><p>18 2 Learning a distribution of sequences Let us consider temporal sequences v = {vt,i |t = 0 . [sent-25, score-0.361]
</p><p>19 Nv } of Nv visible neurons over the interval [0, T ]. [sent-31, score-0.865]
</p><p>20 from a target distribution P ∗ (v) that must be learned by a model which consists of Nv visible neurons and Nh hidden neurons. [sent-46, score-1.261]
</p><p>21 The model distribution over those visible sequences is denoted by Pθ (v) = h Pθ (v, h) where θ denotes the model parameters, h = {ht,i |t = 0 . [sent-47, score-0.551]
</p><p>22 Nh } the hidden temporal sequence and Pθ (v, h) the joint distribution over the visible and the hidden sequences. [sent-53, score-1.153]
</p><p>23 (2), it is possible to approximate it by sampling the visible sequences v from the target distribution P ∗ (v) and the hidden sequences from the posterior distribution Pθ (h|v) given the visible ones. [sent-60, score-1.493]
</p><p>24 Indeed, at a time t the posterior distribution over ht does not only depend on the past visible activity but also on the future visible activity, since it is conditioned on the whole visible activity v0:T from time step 0 to T . [sent-62, score-1.633]
</p><p>25 We exploit that in all neuronal network models of interest, neuronal ﬁring at any time point is conditionally independent given the past activity of the network. [sent-69, score-0.287]
</p><p>26 Using the chain rule this means that we can write the joint distribution Pθ (v, h) (see Fig. [sent-70, score-0.207]
</p><p>27 The sampling can be accomplished by clamping the visible neurons to a target sequence v and let the hidden dynamics run, i. [sent-72, score-1.374]
</p><p>28 at time t, ht is sampled from Pθ (ht |v0:t−1 h0:t−1 ). [sent-74, score-0.218]
</p><p>29 (3), the posterior distribution Pθ (h|v) can be written as Pθ (h|v) =  Rθ (v|h)Qθ (h|v) , Pθ (v)  (4)  where the marginal distribution over the visible sequences v can be also expressed as Pθ (v) = Rθ (v|h) Qθ (h|v) . [sent-76, score-0.585]
</p><p>30 Note that in the absence of hidden neurons, this factor γθ (v, h) is equal to one and the maximum likelihood learning rule [2, 18] is recovered. [sent-82, score-0.498]
</p><p>31 N do h ∼ Qθ (h|v) α(v) ← α(v) + Rθ (v|h) ∂ log Pθ (v,h) ∂θ Pθ (v) ← Pθ (v) + N −1 Rθ (v|h) end for α(v) θ ← θ + η Pθ (v) end while return θ  3  B  C  G  10 20 30  10 20 30 time step  D  10 20 30 time step  E  10 20 30 time step  perform ance  unit number  A  1. [sent-89, score-0.24]
</p><p>32 A The target distribution contained only this training pattern for 30 visible neurons and 30 time steps. [sent-96, score-1.09]
</p><p>33 B-F, H-J Overlay of 20 recalls after learning with 15 000 training pattern presentations, B with only visible neurons and a simple asymmetric Hebb rule (see main text) C only visible neurons and learning rule Eq. [sent-97, score-2.251]
</p><p>34 (5) D static weights towards 30 hidden neurons (Reservoir Computing) E learning rule Eq. [sent-98, score-1.239]
</p><p>35 G Learning curves for the training pattern in A for only visible neurons (black line), static weights towards hidden (blue line), online learning approximation (purple line) exact learning rule (red line). [sent-101, score-1.799]
</p><p>36 The performance was measured in one minus average Hamming distance per neuron per time step (see main text). [sent-102, score-0.191]
</p><p>37 I Recall with a network of 30 visible and 10 hidden neurons without learning the weights towards hidden neurons. [sent-104, score-1.706]
</p><p>38 J Recall after training the same network with learning rule Eq. [sent-105, score-0.252]
</p><p>39 3 Binary neurons In order to illustrate the learning rule given by Eq. [sent-107, score-0.663]
</p><p>40 Let x denote the activity of the visible and hidden neurons, i. [sent-109, score-0.776]
</p><p>41 For the distribution over the initial conditions Pθ (v0 ) and Pθ (h0 ) we choose delta distributions such that v0 is equal to the ﬁrst state of the training sequence and h0 is an arbitrary but ﬁxed vector of binary values. [sent-123, score-0.185]
</p><p>42 If we assume that the weights wij are the only adaptable parameters in this model, 4  A  B 1. [sent-124, score-0.294]
</p><p>43 5  20  40  60  80  100  20  40  60  80  100  seq u en ce len gth  number of hidden units  Figure 3: Adding trainable hidden neurons leads to much better recall performance than having static hidden neurons or no hidden neurons at all. [sent-136, score-2.985]
</p><p>44 A Comparison of the performance after 20000 learning cycles between static (blue curve) and dynamic weights (red curve) towards hidden neurons for a network with 30 visible and different numbers of hidden neurons in a training task with a uncorrelated random pattern of length 60 time steps. [sent-137, score-2.497]
</p><p>45 For B we generated random, uncorrelated sequences of different length and compared the performance after 20000 learning cycles for only visible neurons (black curve), static weights towards hidden (blue curve) and dynamic weights towards hidden (red curve). [sent-138, score-2.134]
</p><p>46 (3) and (6) we ﬁnd ∂ log Pw (x) β = ∂wij 2  T  (xt,i − xt,i  Pθ (xt,i |xt−1 ) )xt−1,j  ,  (8)  t=1  where xt,i Pθ (xt,i |xt−1 ) = g(ut,i )δt − (1 − g(ut,i )δt) and the indices i and j run over all visible and hidden neurons. [sent-141, score-0.733]
</p><p>47 2) where the distribution over sequences is a delta distribution P ∗ (v) = δ(v − v ∗ ) around a single pattern v ∗ (Fig. [sent-144, score-0.324]
</p><p>48 a non-Markovian pattern, thus making it a dift=0 T ∗ ∗ ﬁcult pattern to learn with a simple asymmetric Hebb rule ∆wij ∝ t=0 vt+1,i vt,j (Fig. [sent-147, score-0.295]
</p><p>49 The performance was measured ∗ by one minus the Hamming distance per visible neuron and time step 1−(T Nv )−1 t,i |vt,i −vt,i |/2 between target pattern and recall pattern averaged over 100 runs. [sent-150, score-0.792]
</p><p>50 Adding hidden neurons without learning the weights towards hidden neurons is similar to the idea used in the framework of Reservoir Computing (for a review see [13]): the visible states feed a ﬁxed reservoir of neurons that returns a non-linear transformation of the input. [sent-151, score-2.847]
</p><p>51 Only the readout from hidden to visible neurons and in our case the recurrent connections in the visible layer are trained. [sent-152, score-1.614]
</p><p>52 To assure a sensible distribution of weights towards hidden units, we used the weights that were obtained after learning with Eq. [sent-153, score-0.604]
</p><p>53 Obviously, without training the reservoir the performance is always worse compared to a system with an equal number of hidden neurons but dynamic weights (Fig. [sent-155, score-1.11]
</p><p>54 With only a few hidden neurons our rule is also capable to learn patterns where the visible neurons are silent during a few time-steps. [sent-157, score-1.9]
</p><p>55 After learning the weights towards 10 hidden neurons with learning rule Eq. [sent-160, score-1.153]
</p><p>56 2) or static weights towards hidden neurons the time gap was not learned (see Fig. [sent-164, score-1.124]
</p><p>57 5  w arbitrary units  0 40  20 0 20 t post t pre ms  40  Figure 4: The learning rule Eq. [sent-166, score-0.206]
</p><p>58 (11) is compatible with Spike-Timing Dependent Plasticity (STDP): the weight gets potentiated if a presynaptic spike is followed by a postsynaptic spike and depressed otherwise. [sent-167, score-0.406]
</p><p>59 3 we used again delta target distributions P ∗ (v) = δ(v − v ∗ ) with random uncorrelated patterns v ∗ of different length. [sent-170, score-0.168]
</p><p>60 For a pattern of length 2Nv = 60 only Nv /2 = 15 trainable hidden neurons are sufﬁcient to reach perfect recall (see Fig. [sent-172, score-0.978]
</p><p>61 This is in clear contrast to the case of static hidden weights. [sent-174, score-0.411]
</p><p>62 Again the static weights were obtained by reshufﬂing those that we obtained after learning with Eq. [sent-175, score-0.166]
</p><p>63 3B compares the capacity of our learning rule with Nh = Nv = 30 hidden neurons to the case of no hidden neuron or static weights towards hidden neurons. [sent-178, score-2.017]
</p><p>64 Without learning the weights towards hidden neurons the performance drops to almost chance level for sequences of 45 or more time steps, whereas with our learning rule this decrease of performance occurs only at sequences of 100 or more time steps. [sent-179, score-1.505]
</p><p>65 4 Limit to Continuous Time Starting from the neurons in the last section we show that in the limit to continuous time we can implement the sequence learning task with stochastic spiking neurons [7]. [sent-180, score-1.239]
</p><p>66 First note that the state of a neuron at time t in the model described in the previous section is fully deﬁned by ut,i := j wij xt−1,j (see Eq. [sent-181, score-0.342]
</p><p>67 The weighted sum j wij xt−1,j is the response of neuron i to the spikes of its presynaptic neurons and its own spikes. [sent-183, score-0.985]
</p><p>68 In a more realistic model the postsynaptic neuron feels the inﬂuence of presynaptic spikes through a perturbation of the membrane potential on the order of a few milliseconds, which in the limit to continuous time clearly cannot be modeled by a one-time step response. [sent-185, score-0.486]
</p><p>69 (6) by ∞  ut,i =  ∞  κs xt−s,i + s=1  j=i =:xκ t,i  ǫs xt−s,j ,  wij  (10)  s=1 =:xǫ t,j  where xt−s,i ∈ {0, 1}. [sent-187, score-0.214]
</p><p>70 The kernel ǫ models the time-course of the response to a presynaptic spike and κ the refractoriness. [sent-188, score-0.262]
</p><p>71 (9) we note that we can scale Rw (v|h) without changing the learning rule Eq. [sent-191, score-0.173]
</p><p>72 We use the scaling Rw (v|h) → Rw (v|h) := (g0 δt)−Sv Rw (v|h), where Sv denotes the total number of spikes T Nv in the visible sequence v, i. [sent-193, score-0.47]
</p><p>73 With neuron i’s response to past spiking activity ui (t) = xκ (t) + j=i wij xǫ (t) and the escape rate function ρi (t) = g0 exp (βui (t)) we j i recovered the deﬁning equations of a simpliﬁed stochastic spike response model [7]. [sent-200, score-0.827]
</p><p>74 4 we display the weight change after forcing two neurons to ﬁre with a ﬁxed time lag. [sent-202, score-0.524]
</p><p>75 Our learning rule is consistent with STDP in the sense that a presynaptic spike followed by a postsynaptic spike leads to potentiation and to depression otherwise. [sent-204, score-0.647]
</p><p>76 5 Approximate online version Without hidden neurons the learning rule found by using Eq. [sent-206, score-1.053]
</p><p>77 (11) is straightforward to implement in an online way where the parameters are updated at every moment in time according to wij ∝ ˙ (xi (t) − ρi (t))xǫ (t) instead of waiting with the update until a training batch ﬁnished. [sent-207, score-0.405]
</p><p>78 Finding j an online version of the learning algorithm for networks with hidden neurons turns out to be a challenge, since we need to know the whole sequences v and h in order to evaluate the importance factor Rθ (v|h)/ Rθ (v|h′ ) Qθ (h′ |v) . [sent-208, score-1.095]
</p><p>79 Here we propose to use in each time step an approximation of the importance factor based on the network dynamics during the preceding period of typical sequence length and multiply it by the low-pass ﬁltered change of parameters. [sent-209, score-0.259]
</p><p>80 To ﬁnd an online estimate of Rθ (v, h′ ) Qθ (h′ |v) we assume that a training pattern v ∼ P ∗ (v) is presented a few times in a row and after time N T , with N ∈ N, N ≫ 1, a new training pattern is picked from the training distribution. [sent-216, score-0.392]
</p><p>81 Under this assumption we can replace the average over 7  hidden sequences by a low-pass ﬁlter of r with time constant N T , see Eq. [sent-217, score-0.501]
</p><p>82 during the time interval [0, τ ), with τ on the order of the kernel time constant τm - the hidden activity h(s) is drawn from a given distribution P (h(s)). [sent-221, score-0.503]
</p><p>83 2A) in section 3, the performance of the online rule is close to the one of the batch rule (Fig. [sent-225, score-0.45]
</p><p>84 6 Discussion Learning long and temporally correlated sequences with neural networks is a difﬁcult task. [sent-227, score-0.239]
</p><p>85 In this paper we suggested a statistical model with hidden neurons and derived a learning rule that leads to optimal recall of the learned sequences given the neuronal dynamics. [sent-228, score-1.243]
</p><p>86 The learning rule is derived by minimizing the Kullback-Leibler divergence from training distribution to model distribution with a variant of the EM-algorithm, where we use importance sampling to draw hidden sequences given the visible training sequence. [sent-229, score-1.296]
</p><p>87 Choosing an appropriate distribution in the importance sampling step we are able to circumvent inference which usually makes the training of non-Markovian models hard. [sent-230, score-0.197]
</p><p>88 We showed that it is ready to be implemented with biologically realistic neurons and that an approximate online version exists. [sent-232, score-0.634]
</p><p>89 Our approach follows the ideas outlined in [2], where sequence learning was considered with visible neurons. [sent-233, score-0.426]
</p><p>90 Here we extended this model by adding stochastic hidden neurons that help to perform well with sequences of linearly depend states - including non-Markovian sequences - or long sequences. [sent-234, score-1.158]
</p><p>91 As in [18] we look at the limit of continuous time and ﬁnd that the learning rule is consistent with Spike-Timing Dependent Plasticity. [sent-235, score-0.244]
</p><p>92 In contrast to Reservoir Computing [13] we train the weights towards hidden neurons which clearly helps to improve performance. [sent-236, score-0.98]
</p><p>93 Our learning rule does not need a “wake” and a “sleep” phase as we know it from Boltzmann machines [1, 8]. [sent-237, score-0.198]
</p><p>94 Viewed in a different light our learning algorithm has a nice interpretation: as in reinforcement learning, the hidden neurons explore different sequences, where each trial leads to a global reward signal that modulates the weight change. [sent-238, score-0.815]
</p><p>95 However, in contrast to common reinforcement learning the reward is not provided by an external teacher but depends solely on the internal dynamics and the visible neurons do not explore but are clamped to the training sequence. [sent-239, score-0.985]
</p><p>96 To make our model even more biologically relevant, future work should aim for a biological implementation of the global importance factor that depends on the spike timing and the membrane potential of all the visible neurons (see Eq. [sent-240, score-1.123]
</p><p>97 Phase diagram and storage capacity of sequence processing u neural networks. [sent-279, score-0.188]
</p><p>98 Matching storage and recall: hippocampal spike timingdependent plasticity and phase response curves. [sent-313, score-0.319]
</p><p>99 Efﬁcient methods for sampling spike trains in networks of coupled neurons. [sent-340, score-0.175]
</p><p>100 A tutorial on hidden Markov models and selected applications in speech recognition. [sent-352, score-0.325]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('neurons', 0.49), ('visible', 0.375), ('hidden', 0.325), ('wij', 0.214), ('nv', 0.199), ('ht', 0.184), ('rule', 0.173), ('reservoir', 0.162), ('sequences', 0.142), ('rw', 0.14), ('eij', 0.133), ('spike', 0.119), ('presynaptic', 0.107), ('spiking', 0.103), ('neuron', 0.094), ('static', 0.086), ('towards', 0.085), ('weights', 0.08), ('activity', 0.076), ('pw', 0.074), ('storage', 0.07), ('xt', 0.07), ('plasticity', 0.069), ('pattern', 0.067), ('vt', 0.067), ('dynamics', 0.067), ('online', 0.065), ('nh', 0.064), ('postsynaptic', 0.061), ('markovian', 0.061), ('neuronal', 0.058), ('stdp', 0.056), ('asymmetric', 0.055), ('recall', 0.055), ('training', 0.053), ('boltzmann', 0.052), ('sequence', 0.051), ('recurrent', 0.049), ('biologically', 0.049), ('sv', 0.048), ('patterns', 0.047), ('delta', 0.047), ('brea', 0.046), ('reshuf', 0.046), ('senn', 0.046), ('synaptic', 0.046), ('importance', 0.046), ('membrane', 0.044), ('spikes', 0.044), ('temporal', 0.043), ('ui', 0.041), ('trainable', 0.041), ('exp', 0.039), ('batch', 0.039), ('ring', 0.038), ('temporally', 0.037), ('snf', 0.037), ('hebb', 0.037), ('barber', 0.037), ('target', 0.037), ('limit', 0.037), ('em', 0.037), ('uncorrelated', 0.037), ('response', 0.036), ('depression', 0.035), ('step', 0.035), ('past', 0.035), ('time', 0.034), ('dependent', 0.034), ('stochastic', 0.034), ('capacity', 0.034), ('curve', 0.034), ('distribution', 0.034), ('potentiation', 0.033), ('neural', 0.033), ('units', 0.033), ('log', 0.033), ('refractory', 0.032), ('bern', 0.032), ('ster', 0.032), ('divergence', 0.032), ('hinton', 0.03), ('realistic', 0.03), ('modulated', 0.03), ('calculating', 0.029), ('sampling', 0.029), ('swiss', 0.029), ('mod', 0.029), ('minus', 0.028), ('contrastive', 0.028), ('networks', 0.027), ('ti', 0.027), ('network', 0.026), ('states', 0.025), ('phase', 0.025), ('cycles', 0.024), ('gap', 0.024), ('populations', 0.023), ('converged', 0.023), ('hamming', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="249-tfidf-1" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>2 0.37883991 <a title="249-tfidf-2" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>3 0.27169827 <a title="249-tfidf-3" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>Author: Balazs B. Ujfalussy, Máté Lengyel</p><p>Abstract: Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment ﬂuctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity. 1</p><p>4 0.20947585 <a title="249-tfidf-4" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>Author: Konrad Koerding, Ian Stevenson</p><p>Abstract: Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difﬁcult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modiﬁcation functions from pairs of spike trains. Then, using recursive point-process adaptive ﬁltering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modiﬁcation, we show that the true modiﬁcation function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data. 1</p><p>5 0.18423958 <a title="249-tfidf-5" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>Author: Matthew D. Zeiler, Graham W. Taylor, Leonid Sigal, Iain Matthews, Rob Fergus</p><p>Abstract: We present a type of Temporal Restricted Boltzmann Machine that deﬁnes a probability distribution over an output sequence conditional on an input sequence. It shares the desirable properties of RBMs: efﬁcient exact inference, an exponentially more expressive latent state than HMMs, and the ability to model nonlinear structure and dynamics. We apply our model to a challenging real-world graphics problem: facial expression transfer. Our results demonstrate improved performance over several baselines modeling high-dimensional 2D and 3D data. 1</p><p>6 0.18360548 <a title="249-tfidf-6" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>7 0.18321568 <a title="249-tfidf-7" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>8 0.1633033 <a title="249-tfidf-8" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>9 0.14776637 <a title="249-tfidf-9" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>10 0.1472888 <a title="249-tfidf-10" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>11 0.14478052 <a title="249-tfidf-11" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>12 0.14222632 <a title="249-tfidf-12" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>13 0.1311473 <a title="249-tfidf-13" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>14 0.12822972 <a title="249-tfidf-14" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>15 0.12573835 <a title="249-tfidf-15" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>16 0.11844693 <a title="249-tfidf-16" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>17 0.11683421 <a title="249-tfidf-17" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>18 0.10442755 <a title="249-tfidf-18" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>19 0.10397787 <a title="249-tfidf-19" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>20 0.10342714 <a title="249-tfidf-20" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.23), (1, 0.096), (2, 0.394), (3, -0.034), (4, 0.138), (5, 0.012), (6, -0.074), (7, -0.09), (8, -0.056), (9, -0.134), (10, 0.085), (11, -0.015), (12, 0.001), (13, 0.038), (14, -0.038), (15, -0.088), (16, -0.059), (17, 0.069), (18, -0.037), (19, 0.118), (20, 0.046), (21, 0.111), (22, -0.07), (23, 0.052), (24, -0.043), (25, -0.094), (26, 0.072), (27, -0.17), (28, 0.157), (29, 0.039), (30, -0.017), (31, -0.016), (32, 0.02), (33, -0.069), (34, 0.045), (35, -0.006), (36, 0.053), (37, -0.018), (38, -0.013), (39, -0.048), (40, 0.001), (41, -0.068), (42, 0.01), (43, 0.018), (44, -0.079), (45, 0.068), (46, -0.03), (47, -0.03), (48, -0.016), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97183347 <a title="249-lsi-1" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>2 0.82336354 <a title="249-lsi-2" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>Author: Balazs B. Ujfalussy, Máté Lengyel</p><p>Abstract: Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment ﬂuctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity. 1</p><p>3 0.79701018 <a title="249-lsi-3" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>4 0.73569804 <a title="249-lsi-4" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>Author: Konrad Koerding, Ian Stevenson</p><p>Abstract: Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difﬁcult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modiﬁcation functions from pairs of spike trains. Then, using recursive point-process adaptive ﬁltering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modiﬁcation, we show that the true modiﬁcation function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data. 1</p><p>5 0.71214658 <a title="249-lsi-5" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>Author: Cristina Savin, Peter Dayan, Máté Lengyel</p><p>Abstract: Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This ﬁnding provides a new window onto actively contentious psychological and neural aspects of recognition memory. 1</p><p>6 0.70563471 <a title="249-lsi-6" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>7 0.64639825 <a title="249-lsi-7" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>8 0.6353026 <a title="249-lsi-8" href="./nips-2011-Emergence_of_Multiplication_in_a_Biophysical_Model_of_a_Wide-Field_Visual_Neuron_for_Computing_Object_Approaches%3A_Dynamics%2C_Peaks%2C_%26_Fits.html">85 nips-2011-Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, & Fits</a></p>
<p>9 0.62644476 <a title="249-lsi-9" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>10 0.5988853 <a title="249-lsi-10" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>11 0.59385496 <a title="249-lsi-11" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>12 0.58844239 <a title="249-lsi-12" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>13 0.5343926 <a title="249-lsi-13" href="./nips-2011-From_Stochastic_Nonlinear_Integrate-and-Fire_to_Generalized_Linear_Models.html">99 nips-2011-From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models</a></p>
<p>14 0.51449531 <a title="249-lsi-14" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>15 0.49504733 <a title="249-lsi-15" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>16 0.47141284 <a title="249-lsi-16" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>17 0.46174002 <a title="249-lsi-17" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>18 0.46046242 <a title="249-lsi-18" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>19 0.44424313 <a title="249-lsi-19" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>20 0.41009924 <a title="249-lsi-20" href="./nips-2011-Estimating_time-varying_input_signals_and_ion_channel_states_from_a_single_voltage_trace_of_a_neuron.html">89 nips-2011-Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.014), (4, 0.029), (20, 0.022), (26, 0.025), (31, 0.22), (33, 0.012), (43, 0.046), (45, 0.079), (57, 0.037), (74, 0.05), (82, 0.104), (83, 0.158), (84, 0.011), (99, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94008803 <a title="249-lda-1" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>2 0.86909056 <a title="249-lda-2" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>Author: Balazs B. Ujfalussy, Máté Lengyel</p><p>Abstract: Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment ﬂuctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity. 1</p><p>3 0.85063416 <a title="249-lda-3" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>Author: Konrad Koerding, Ian Stevenson</p><p>Abstract: Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difﬁcult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modiﬁcation functions from pairs of spike trains. Then, using recursive point-process adaptive ﬁltering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modiﬁcation, we show that the true modiﬁcation function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data. 1</p><p>4 0.84879363 <a title="249-lda-4" href="./nips-2011-A_blind_sparse_deconvolution_method_for_neural_spike_identification.html">13 nips-2011-A blind sparse deconvolution method for neural spike identification</a></p>
<p>Author: Chaitanya Ekanadham, Daniel Tranchina, Eero P. Simoncelli</p><p>Abstract: We consider the problem of estimating neural spikes from extracellular voltage recordings. Most current methods are based on clustering, which requires substantial human supervision and systematically mishandles temporally overlapping spikes. We formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform. Joint maximum-a-posteriori (MAP) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefﬁcients are sparse. We develop a block-coordinate descent procedure to approximate the MAP solution, based on our recently developed continuous basis pursuit method. We validate our method on simulated data as well as real data for which ground truth is available via simultaneous intracellular recordings. In both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering overlapping spikes. The method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors. 1</p><p>5 0.84398627 <a title="249-lda-5" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>Author: Biljana Petreska, Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: Simultaneous recordings of many neurons embedded within a recurrentlyconnected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function. In principle, these dynamics might be identiﬁed by purely unsupervised, statistical means. Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model— in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process—is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements. The regimes are identiﬁed without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial. The HSLDS model also performs better than recent comparable models in predicting the ﬁring rate of an isolated neuron based on the ﬁring rates of others, suggesting that it captures more of the “shared variance” of the data. Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reﬂect its computational role. 1</p><p>6 0.83695292 <a title="249-lda-6" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>7 0.83483028 <a title="249-lda-7" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>8 0.82587713 <a title="249-lda-8" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>9 0.81794012 <a title="249-lda-9" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>10 0.81423694 <a title="249-lda-10" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>11 0.81290406 <a title="249-lda-11" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>12 0.81178129 <a title="249-lda-12" href="./nips-2011-Automated_Refinement_of_Bayes_Networks%27_Parameters_based_on_Test_Ordering_Constraints.html">40 nips-2011-Automated Refinement of Bayes Networks' Parameters based on Test Ordering Constraints</a></p>
<p>13 0.80793399 <a title="249-lda-13" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>14 0.8066172 <a title="249-lda-14" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>15 0.8053776 <a title="249-lda-15" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>16 0.80531025 <a title="249-lda-16" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>17 0.80415249 <a title="249-lda-17" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>18 0.80352598 <a title="249-lda-18" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>19 0.80062813 <a title="249-lda-19" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>20 0.8001827 <a title="249-lda-20" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
