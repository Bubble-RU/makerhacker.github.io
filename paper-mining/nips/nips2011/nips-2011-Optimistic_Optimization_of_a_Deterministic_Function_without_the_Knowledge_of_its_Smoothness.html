<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 nips-2011-Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-208" href="#">nips2011-208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>208 nips-2011-Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness</h1>
<br/><p>Source: <a title="nips-2011-208-pdf" href="http://papers.nips.cc/paper/4304-optimistic-optimization-of-a-deterministic-function-without-the-knowledge-of-its-smoothness.pdf">pdf</a></p><p>Author: Rémi Munos</p><p>Abstract: We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.</p><p>Reference: <a title="nips-2011-208-reference" href="../nips2011_reference/nips-2011-Optimistic_Optimization_of_a_Deterministic_Function_without_the_Knowledge_of_its_Smoothness_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fr  Abstract We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. [sent-3, score-0.246]
</p><p>2 The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. [sent-4, score-0.116]
</p><p>3 We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. [sent-5, score-0.266]
</p><p>4 We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted. [sent-8, score-0.133]
</p><p>5 The performance of the algorithm is evaluated by the loss rn = sup f (x) − f (x(n)). [sent-19, score-0.171]
</p><p>6 The loss criterion (1) is closer to the simple regret deﬁned in the bandit setting [BMS09, ABM10]. [sent-22, score-0.179]
</p><p>7 The approach followed here can be seen as an optimistic sampling strategy where, at each round, we explore the space where the function could be the largest, given the knowledge of previous evaluations. [sent-24, score-0.24]
</p><p>8 ) this literature is to considerably weaken the Lipschitz assumption usually made and consider only a locally one-sided Lipschitz assumption around the maximum of f . [sent-29, score-0.125]
</p><p>9 The case of Lipschitz (or relaxed) assumption in a metric spaces is considered in [Kle04, AOS07] and more recently in [KSU08, BMSS08, BMSS11], and in the case of unknown Lipschitz constant, see [BSY11, Sli11] (where they assume a bound on the Hessian or another related parameter). [sent-32, score-0.119]
</p><p>10 Compared to this literature, our contribution is the design and analysis of two algorithms: (1) A ﬁrst algorithm, Deterministic Optimistic Optimization (DOO), that requires the knowledge of the semimetric ℓ for which f is locally smooth around its maximum. [sent-33, score-0.234]
</p><p>11 A loss bound is provided (in terms of the near-optimality dimension of f under ℓ) in a more general setting that previously considered. [sent-34, score-0.13]
</p><p>12 2 Assumptions about the hierarchical partition and the function Our optimization algorithms will be implemented by resorting to a hierarchical partitioning of the space X , which is given to the algorithms. [sent-37, score-0.112]
</p><p>13 This partitioning may be represented by a K-ary tree structure where each cell Xh,i corresponds to a node (h, i) of the tree (indexed by its depth h and index i), and such that each node (h, i) possesses K children nodes {(h + 1, ik )}1≤k≤K . [sent-39, score-0.79]
</p><p>14 In addition, the cells of the children {Xh+1,ik , 1 ≤ k ≤ K} form a partition of the parent’s cell Xh,i . [sent-40, score-0.145]
</p><p>15 (2) This condition guarantees that f does not decrease too fast around (at least) one global optimum x∗ (this is a sort of a locally one-sided Lipschitz assumption). [sent-60, score-0.139]
</p><p>16 There exists a decreasing sequence δ(h) > 0, such that for any depth h ≥ 0, for any cell Xh,i of depth h, we have supx∈Xh,i ℓ(xh,i , x) ≤ δ(h). [sent-63, score-0.578]
</p><p>17 There exists ν > 0 such that for any depth h ≥ 0, any cell Xh,i contains a ℓ-ball of radius νδ(h) centered in xh,i . [sent-65, score-0.387]
</p><p>18 Expand this node: add to Tt the K children of (h, j) end for Return x(n) = arg max(h,i)∈Tn f (xh,i )  Figure 1: Deterministic optimistic optimization (DOO) algorithm. [sent-73, score-0.273]
</p><p>19 selecting at each round t a leaf of the current tree Tt to expand. [sent-74, score-0.173]
</p><p>20 Expanding a leaf means adding its K children to the current tree (this corresponds to splitting the cell Xh,j into K sub-cells). [sent-75, score-0.246]
</p><p>21 We write Lt the leaves of Tt (set of nodes whose children are not in Tt ), which are the set of nodes that can be expanded at round t. [sent-77, score-0.433]
</p><p>22 This algorithm is called optimistic because it expands at each round a cell that may contain the optimum of f , based on the information about (i) the previously observed evaluations of f , and (ii) the knowledge of the local smoothness property (2) of f (since ℓ is known). [sent-78, score-0.573]
</p><p>23 The algorithm computes def the b-values bh,j = f (xh,j ) + δ(h) of all nodes (h, j) of the current tree Tt and select the leaf with highest b-value to expand next. [sent-79, score-0.29]
</p><p>24 As a consequence, a node (h, i) such that f (xh,i ) + δ(h) < f ∗ will never be expanded (since at any time t, the b-value of such a node will be dominated by the b-value of the leaf containing x∗ ). [sent-85, score-0.452]
</p><p>25 We def deduce that DOO only expands nodes of the set I = ∪h≥0 Ih , where def  Ih = {nodes (h, i) such that f (xh,i ) + δ(h) ≥ f ∗ }. [sent-86, score-0.309]
</p><p>26 The near-optimality dimension is the smallest d ≥ 0 such that there exists C > 0 such that for any ε > 0, the maximal number of disjoint ℓ-balls of radius νε and center in Xε is less than Cε−d . [sent-91, score-0.137]
</p><p>27 From Assumption 4, each cell (h, i) contains a ball of radius νδ(h) centered in xh,i , thus if |Ih | = |{xh,i ∈ Xδ(h) }| exceeded Cδ(h)−d , this would mean that there exists more than Cδ(h)−d disjoint ℓ-balls of radius νδ(h) with center in Xδ(h) , which contradicts the deﬁnition of d. [sent-97, score-0.199]
</p><p>28 Let us write h(n) the smallest integer h such that C of DOO is bounded as rn ≤ δ(h(n)). [sent-100, score-0.117]
</p><p>29 Let (hmax , j) be the deepest node that has been expanded by the algorithm up to round n. [sent-103, score-0.393]
</p><p>30 We known that DOO only expands nodes in the set I. [sent-104, score-0.112]
</p><p>31 Now, among all node expansion strategies of the set of expandable nodes I, the uniform strategy is the one which minimizes the depth of the resulting tree. [sent-105, score-0.45]
</p><p>32 From the deﬁnition of h(n) and from Lemma 1, we have h(n)−1 |Il | l=0  h(n)−1 l=0  ≤C  δ(l)−d < n,  thus the maximum depth of the uniform strategy is at least h(n), and we deduce that hmax ≥ h(n). [sent-106, score-0.928]
</p><p>33 Now since node (hmax , j) has been expanded, we have that (hmax , j) ∈ I, thus f (x(n)) ≥ f (xhmax ,j ) ≥ f ∗ − δ(hmax ) ≥ f ∗ − δ(h(n)). [sent-107, score-0.125]
</p><p>34 This bound is in terms of the number of expanded nodes n. [sent-109, score-0.224]
</p><p>35 Now, let us make the bound more explicit when the diameter δ(h) of the cells decreases exponentially fast with their depth (this case is rather general as illustrated in the examples described next, as well as in the discussion in [BMSS11]). [sent-111, score-0.387]
</p><p>36 Now, of f is d > 0, then the loss decreases polynomially fast: rn ≤ c d 1 − γ d if d = 0, then the loss decreases exponentially fast: rn ≤ cγ (n/C)−1 . [sent-115, score-0.443]
</p><p>37 From Theorem 1, whenever d > 0 we have n ≤ C  thus γ  n ≥ cC 1/d −1/d  −dh(n)  d −1/d  h(n) l=0  δ(l)−d = c C γ  d  1 − γ , from which we deduce that rn ≤ δ(h(n)) ≤ cγ  γ C n . [sent-117, score-0.216]
</p><p>38 Now, if d = 0 then n ≤ C the loss is bounded as rn ≤ δ(h(n)) = cγ (n/C)−1 . [sent-118, score-0.171]
</p><p>39 Expanding a node means splitting the corresponding square in 2D squares of half length. [sent-122, score-0.149]
</p><p>40 The optimum of f is x∗ = 0 and f satisﬁes the local smoothness property (2). [sent-126, score-0.132]
</p><p>41 1/α D For any ε > 0, Xε is the L∞ -ball of radius ε1/α centered in 0, which can be packed by ε1/β ε L∞ -balls of diameter ε (since a L∞ -balls of diameter ε is a ℓ-ball of diameter ε1/β ). [sent-128, score-0.27]
</p><p>42 From Corollary 1 we deduce 1 αβ that (i) when α > β, then d > 0 and in this case, rn = O n− D α−β . [sent-130, score-0.192]
</p><p>43 And (ii) when α = β, then d = 0 and the loss decreases exponentially fast: rn ≤ 21−n . [sent-131, score-0.223]
</p><p>44 For any ε > 0, Xε is included in a ball of radius (ε/c2 )1/α centered in x∗ , 1/α D ℓ-balls of diameter ε. [sent-147, score-0.122]
</p><p>45 We deduce that the behavior of the algorithm depends on our knowledge of the local smoothness (i. [sent-152, score-0.233]
</p><p>46 Indeed, if this smoothness information is available, then one should deﬁned the semi-metric ℓ (which impacts the algorithm through the deﬁnition of δ(h)) to match this smoothness (i. [sent-155, score-0.178]
</p><p>47 by choosing 1 αβ β ≤ α) and suffer a loss rn = O n− D α−β , rather than overestimating it (β > α) since in this case, (2) may not hold anymore and there is a risk that the algorithm converges to a local optimum (thus suffering a constant loss). [sent-160, score-0.24]
</p><p>48 4 Comparison with previous works Optimistic planning: The deterministic planning problem described in [HM08] considers an optimistic approach for selecting the ﬁrst action of a sequence x that maximizes the sum of discounted rewards. [sent-162, score-0.275]
</p><p>49 Their algorithm is very close to DOO: it expands a node of the tree (ﬁnite sequence of actions) with highest upper-bound on the possible value. [sent-169, score-0.265]
</p><p>50 Corollary 1 implies directly that the loss log 1/γ bound is rn = O(n− log κ ) which is the result reported in [HM08]. [sent-172, score-0.19]
</p><p>51 Those works consider the case of noisy evaluations of the function (X -armed bandit setting), which is assumed to be weakly Lipschitz (slightly stronger than our Assumption 2). [sent-174, score-0.108]
</p><p>52 4 When the semi-metric ℓ is unknown We now consider the setting where Assumptions 1-4 hold for some semi-metric ℓ, but the semimetric ℓ is unknown. [sent-178, score-0.127]
</p><p>53 The hierarchical partitioning of the space is still given, but since ℓ is unknown, one cannot use the diameter δ(h) of the cells to design upper-bounds, like in DOO. [sent-179, score-0.14]
</p><p>54 The question we wish to address is: If ℓ is unknown, is it possible to implement an optimistic algorithm with performance guarantees? [sent-180, score-0.187]
</p><p>55 5  The maximum depth function t → hmax (t) is a parameter of the algorithm. [sent-182, score-0.793]
</p><p>56 1 The SOO algorithm The idea is to expand at each round simultaneously all the leaves (h, j) for which there exists a semi-metric ℓ such that the corresponding upper-bound f (xh,j ) + supx∈Xh,j ℓ(xh,j , x) would be the highest. [sent-189, score-0.185]
</p><p>57 This is implemented by expanding at each round at most a leaf per depth, and a leaf is expanded only if it has the largest value among all leaves of same or lower depths. [sent-190, score-0.387]
</p><p>58 The SOO algorithm takes as parameter a function t → hmax (t) which forces the tree to a maximal depth of hmax (t) after t node expansions. [sent-192, score-1.567]
</p><p>59 At time t, let us write h∗ the depth of the deepest expanded node in the branch containing x∗ (an t optimal branch). [sent-196, score-0.574]
</p><p>60 Let (h∗ + 1, i∗ ) be an optimal node of depth h∗ + 1 (i. [sent-197, score-0.354]
</p><p>61 t t t Since this node has not been expanded yet, any node (h∗ +1, i) of depth h∗ +1 that is later expanded, t t before (h∗ + 1, i∗ ) is expanded, is δ(h∗ + 1)-optimal. [sent-200, score-0.63]
</p><p>62 We deduce that once an optimal node of depth h is expanded, it takes at most |Ih+1 | node t expansions at depth h + 1 before the optimal node of depth h + 1 is expanded. [sent-202, score-1.179]
</p><p>63 For any depth 0 ≤ h ≤ hmax (t), whenever t ≥ (|I0 | + |I1 | + · · · + |Ih |)hmax (t), we have h∗ ≥ h. [sent-205, score-0.817]
</p><p>64 Assume that the proposition t is true for all 0 ≤ h ≤ h0 with h0 < hmax (t). [sent-209, score-0.564]
</p><p>65 In this t t t latter case, consider the nodes of depth h0 + 1 that are expanded. [sent-214, score-0.283]
</p><p>66 We have seen that as long as the optimal node of depth h0 + 1 is not expanded, any node of depth h0 + 1 that is expanded must be δ(h0 + 1)-optimal, i. [sent-215, score-0.859]
</p><p>67 Since there are |Ih0 +1 | of them, after |Ih0 +1 |hmax (t) node expansions, the optimal one must be expanded, thus h∗ ≥ h0 + 1. [sent-218, score-0.125]
</p><p>68 (3)  Then the loss is bounded as rn ≤ δ min(h(n), hmax (n) + 1) . [sent-221, score-0.735]
</p><p>69 From Lemma 1 and the deﬁnition of h(n) we have h(n)−1  hmax (n) l=0  h(n)−1  |Il | ≤ Chmax (n)  δ(l)−d < n, l=0  thus from Lemma 2, when h(n) − 1 ≤ hmax (n) we have h∗ ≥ h(n) − 1. [sent-223, score-1.128]
</p><p>70 Now in the case n h(n) − 1 > hmax (n), since the SOO algorithm does not expand nodes beyond depth hmax (n), we have h∗ = hmax (n). [sent-224, score-2.026]
</p><p>71 n n  Let (h, j) be the deepest node in Tn that has been expanded by the algorithm up to round n. [sent-226, score-0.393]
</p><p>72 Now, from the deﬁnition of the algorithm, we only expand a node when its value is larger n than the value of all the leaves of equal or lower depths. [sent-228, score-0.216]
</p><p>73 Thus, since the node (h, j) has been expanded, its value is at least as high as that of the optimal node (h∗ + 1, i∗ ) of depth h∗ + 1 (which n n has not been expanded, by deﬁnition of h∗ ). [sent-229, score-0.479]
</p><p>74 Thus n f (x(n)) ≥ f (xh,j ) ≥ f (xh∗ +1,i∗ ) ≥ f ∗ − δ(h∗ + 1) ≥ f ∗ − δ(min(h(n), hmax (n) + 1)). [sent-230, score-0.564]
</p><p>75 The main difference is that the maximal depth hmax (n) appears both as a multiplicative factor in the deﬁnition of h(n) in (3) and as a threshold in the loss bound (4). [sent-233, score-0.913]
</p><p>76 Those two appearances of hmax (n) deﬁnes a tradeoff between deep (large hmax ) versus broad (small hmax ) types of exploration. [sent-234, score-1.692]
</p><p>77 Let the depth function hmax (t) = tε , for some ε > 0 arbitrarily small. [sent-239, score-0.793]
</p><p>78 Then, for n large enough (as a function of ε) the loss of SOO is bounded as: rn ≤ c  C 1 − γd  d+1 d  1/d  n−  1−ε d  . [sent-240, score-0.171]
</p><p>79 Let the depth function hmax (t) = bounded as: √ rn ≤ cγ n min(1/C,1)−1 . [sent-242, score-0.892]
</p><p>80 From Theorem 1, when d > 0 we have h(n)  n ≤ Chmax (n)  δ(l)−d = cChmax (n) l=0  γ −d(h(n)+1) − 1 γ −d − 1  1−ε  thus for the choice hmax (n) = n , we deduce γ −dh(n) ≥ ncC 1 − γ d . [sent-245, score-0.657]
</p><p>81 Thus h(n) is logarithmic in n and for n large enough (as a function of ε), h(n) ≤ hmax (n) + 1, thus ε  rn ≤ δ min(h(n), hmax (n) + 1) = δ(h(n)) ≤ cγ h(n) ≤ c h(n)  d+1 d  C 1 − γd  1/d  n−  1−ε d  . [sent-246, score-1.227]
</p><p>82 Now, if d = 0 then n ≤ Chmax (n) l=0 δ(l)−d = Chmax (n)(h(n) + 1), thus for the choice √ hmax (n) = n we deduce that the loss decreases as: rn ≤ δ min(h(n), hmax (n) + 1) ≤ cγ  √ n min(1/C,1)−1  . [sent-247, score-1.421]
</p><p>83 The maximal depth function hmax (t) is still a parameter of the algorithm, which somehow inﬂuences the behavior of the algorithm (deep versus broad exploration of the tree). [sent-249, score-0.848]
</p><p>84 In particular, we can think of problems for which there exists a semimetric ℓ such that the corresponding near-optimality dimension d is 0. [sent-255, score-0.168]
</p><p>85 We have seen that DOO with the metric ℓ(x, y) = x − y β ∞ ∞ 1 αβ provides a polynomial loss rn = O n− D α−β whenever β < α, and an exponential loss rn ≤ 21−n when β = α. [sent-259, score-0.415]
</p><p>86 √ Now consider the SOO algorithm with the maximum depth function hmax (t) = t. [sent-261, score-0.793]
</p><p>87 We deduce that the loss of SOO is rn ≤ 2(1− n)α . [sent-265, score-0.264]
</p><p>88 Note that a uniform grid provides the loss n−α/D , which is polynomially decreasing only (and subject to the curse of dimensionality). [sent-267, score-0.14]
</p><p>89 The fact that SOO is not as good as DOO optimally ﬁtted √ comes from the truncation of SOO at a maximal depth hmax (n) = n (whereas DOO optimally ﬁtted would explore the tree up to a depth linear in n). [sent-269, score-1.107]
</p><p>90 Example 2: The same conclusion holds for Example 2, where we consider a function f deﬁned on [0, 1]D that is locally equivalent to x−x∗ α , for some unknown α > 0 (see the precise assumptions in Section 3. [sent-270, score-0.116]
</p><p>91 We have seen that DOO using ℓ(x, y) = c x − y β with β < α has a loss 1 αβ rn = O n− D α−β , and when α = β, then d = 0 and the loss is rn = O(2−α(n/C−1) ). [sent-272, score-0.342]
</p><p>92 √ Now by using SOO (which does not require the knowledge of α) with hmax (t) = t we deduce the √ stretched-exponential loss rn = O(2− nα/C ) (by using ℓ(x, y) = x − y α in the analysis, which gives δ(h) = 2−hα and d = 0). [sent-273, score-0.86]
</p><p>93 It uses an optimistic splitting technique similar to ours where at each round, it expands the set of nodes that have the highest upper-bound (as deﬁned in DOO) for at least some value of L. [sent-276, score-0.349]
</p><p>94 Our approach generalizes DIRECT and we are able to derive ﬁnite-time loss bounds in a much broader setting where the function is only locally smooth and the space is semi-metric. [sent-278, score-0.148]
</p><p>95 We are not aware of other ﬁnite-time analysis of global optimization algorithms that do not require the knowledge of the smoothness of the function. [sent-279, score-0.197]
</p><p>96 5 Conclusions We presented two algorithms: DOO requires the knowledge of the semi-metric ℓ under which the function f is locally smooth (according to Assumption 2). [sent-280, score-0.108]
</p><p>97 We reported ﬁnite-time loss bounds using the near-optimality dimension d, which relates the local smoothness of f around its maximum and the quantity of near-optimal states, measured by the semi-metric ℓ. [sent-284, score-0.244]
</p><p>98 We provided illustrative examples of the performance of SOO in Euclidean spaces where the local smoothness of f is unknown. [sent-285, score-0.108]
</p><p>99 ℓ and whose corresponding near-optimal dimension is d = 0 (in order to have a stretchedexponentially decreasing loss), and (iii) extending the SOO algorithm to stochastic X -armed bandits (optimization of a noisy function) when the smoothness of f is unknown. [sent-289, score-0.199]
</p><p>100 Gaussian process optimization in the bandit setting: No regret and experimental design. [sent-440, score-0.143]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmax', 0.564), ('doo', 0.439), ('soo', 0.371), ('depth', 0.229), ('optimistic', 0.187), ('expanded', 0.151), ('node', 0.125), ('semimetric', 0.101), ('rn', 0.099), ('deduce', 0.093), ('ih', 0.093), ('lipschitz', 0.092), ('smoothness', 0.089), ('tt', 0.086), ('chmax', 0.084), ('bandit', 0.075), ('loss', 0.072), ('round', 0.066), ('cell', 0.065), ('bubeck', 0.062), ('expands', 0.058), ('xh', 0.058), ('diameter', 0.057), ('tree', 0.056), ('nodes', 0.054), ('munos', 0.052), ('def', 0.052), ('expand', 0.051), ('leaf', 0.051), ('deepest', 0.051), ('locally', 0.05), ('children', 0.05), ('metric', 0.049), ('deterministic', 0.045), ('zooming', 0.045), ('dordrecht', 0.045), ('bandits', 0.044), ('lt', 0.044), ('planning', 0.043), ('diameters', 0.041), ('radius', 0.041), ('corollary', 0.041), ('leaves', 0.04), ('assumptions', 0.04), ('global', 0.04), ('supx', 0.039), ('dimension', 0.039), ('stoltz', 0.038), ('kluwer', 0.038), ('optimization', 0.036), ('vmax', 0.035), ('szepesv', 0.034), ('nearoptimality', 0.034), ('packed', 0.034), ('boston', 0.033), ('evaluations', 0.033), ('knowledge', 0.032), ('regret', 0.032), ('cells', 0.03), ('academic', 0.03), ('partitioning', 0.03), ('hoo', 0.03), ('decreases', 0.029), ('remark', 0.029), ('maximal', 0.029), ('expanding', 0.028), ('publishers', 0.028), ('exists', 0.028), ('decreasing', 0.027), ('highest', 0.026), ('exploration', 0.026), ('smooth', 0.026), ('anymore', 0.026), ('unknown', 0.026), ('assumption', 0.025), ('tn', 0.025), ('around', 0.025), ('expansions', 0.024), ('budget', 0.024), ('splitting', 0.024), ('whenever', 0.024), ('centered', 0.024), ('optimum', 0.024), ('exponentially', 0.023), ('tted', 0.023), ('hyper', 0.023), ('constants', 0.023), ('hierarchical', 0.023), ('il', 0.021), ('strategy', 0.021), ('uniform', 0.021), ('london', 0.021), ('partitions', 0.021), ('polynomially', 0.02), ('local', 0.019), ('root', 0.019), ('bound', 0.019), ('reinforcement', 0.019), ('dh', 0.019), ('write', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="208-tfidf-1" href="./nips-2011-Optimistic_Optimization_of_a_Deterministic_Function_without_the_Knowledge_of_its_Smoothness.html">208 nips-2011-Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness</a></p>
<p>Author: Rémi Munos</p><p>Abstract: We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.</p><p>2 0.10696988 <a title="208-tfidf-2" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>Author: Jia Deng, Sanjeev Satheesh, Alexander C. Berg, Fei Li</p><p>Abstract: We present a novel approach to efﬁciently learn a label tree for large scale classiﬁcation with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classiﬁers for each node in the tree. This approach also allows ﬁne grained control over the efﬁciency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classiﬁcation with 10184 classes and 9 million images. We demonstrate signiﬁcant improvements in test accuracy and efﬁciency with less training time and more balanced trees compared to the previous state of the art by Bengio et al. 1</p><p>3 0.10336888 <a title="208-tfidf-3" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>Author: Joel Z. Leibo, Jim Mutch, Tomaso Poggio</p><p>Abstract: Many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes. Recent electrophysiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpointspeciﬁc cells projecting to downstream viewpoint-invariant identity-speciﬁc cells [1]. A separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-speciﬁc. In particular, the 2D images evoked by a face undergoing a 3D rotation are not produced by the same image transformation (2D) that would produce the images evoked by an object of another class undergoing the same 3D rotation. However, within the class of faces, knowledge of the image transformation evoked by 3D rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint. We show, through computational simulations, that an architecture which applies this method of gaining invariance to class-speciﬁc transformations is effective when restricted to faces and fails spectacularly when applied to other object classes. We argue here that in order to accomplish viewpoint-invariant face identiﬁcation from a single example view, visual cortex must separate the circuitry involved in discounting 3D rotations of faces from the generic circuitry involved in processing other objects. The resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network. 1</p><p>4 0.098343641 <a title="208-tfidf-4" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>Author: Aleksandrs Slivkins</p><p>Abstract: The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation. In this setting, an online algorithm has a ﬁxed set of alternatives (“arms”), and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood, a lot of recent work has focused on multi-armed bandits with (inﬁnitely) many arms, where one needs to assume extra structure in order to make the problem tractable. In particular, in the Lipschitz MAB problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs. In this paper we consider the more realistic scenario in which the metric space is implicit – it is deﬁned by the available structure but not revealed to the algorithm directly. Speciﬁcally, we assume that an algorithm is given a tree-based classiﬁcation of arms. For any given problem instance such a classiﬁcation implicitly deﬁnes a similarity metric space, but the numerical similarity information is not available to the algorithm. We provide an algorithm for this setting, whose performance guarantees (almost) match the best known guarantees for the corresponding instance of the Lipschitz MAB problem. 1</p><p>5 0.097706676 <a title="208-tfidf-5" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>Author: Odalric-ambrym Maillard, Daniil Ryabko, Rémi Munos</p><p>Abstract: The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a ﬁnite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T 2/3 where T is the horizon time. 1</p><p>6 0.077503242 <a title="208-tfidf-6" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>7 0.0752469 <a title="208-tfidf-7" href="./nips-2011-Reconstructing_Patterns_of_Information_Diffusion_from_Incomplete_Observations.html">234 nips-2011-Reconstructing Patterns of Information Diffusion from Incomplete Observations</a></p>
<p>8 0.069711141 <a title="208-tfidf-8" href="./nips-2011-Projection_onto_A_Nonnegative_Max-Heap.html">226 nips-2011-Projection onto A Nonnegative Max-Heap</a></p>
<p>9 0.06375657 <a title="208-tfidf-9" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>10 0.060693946 <a title="208-tfidf-10" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>11 0.054425281 <a title="208-tfidf-11" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>12 0.053940535 <a title="208-tfidf-12" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>13 0.05293306 <a title="208-tfidf-13" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>14 0.052797295 <a title="208-tfidf-14" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>15 0.051802833 <a title="208-tfidf-15" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>16 0.051387951 <a title="208-tfidf-16" href="./nips-2011-Distributed_Delayed_Stochastic_Optimization.html">72 nips-2011-Distributed Delayed Stochastic Optimization</a></p>
<p>17 0.049601141 <a title="208-tfidf-17" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>18 0.049148496 <a title="208-tfidf-18" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>19 0.048977342 <a title="208-tfidf-19" href="./nips-2011-Finite_Time_Analysis_of_Stratified_Sampling_for_Monte_Carlo.html">97 nips-2011-Finite Time Analysis of Stratified Sampling for Monte Carlo</a></p>
<p>20 0.048537403 <a title="208-tfidf-20" href="./nips-2011-Hierarchical_Topic_Modeling_for_Analysis_of_Time-Evolving_Personal_Choices.html">115 nips-2011-Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, -0.084), (2, -0.043), (3, 0.022), (4, 0.053), (5, -0.026), (6, -0.015), (7, 0.011), (8, 0.02), (9, -0.098), (10, -0.072), (11, 0.054), (12, -0.132), (13, 0.037), (14, -0.006), (15, 0.031), (16, -0.035), (17, -0.066), (18, -0.019), (19, 0.001), (20, 0.041), (21, -0.086), (22, -0.053), (23, -0.002), (24, -0.103), (25, 0.028), (26, -0.032), (27, -0.001), (28, -0.015), (29, -0.055), (30, 0.032), (31, 0.073), (32, 0.004), (33, 0.003), (34, 0.072), (35, -0.01), (36, 0.012), (37, 0.062), (38, -0.017), (39, -0.008), (40, -0.02), (41, 0.021), (42, -0.023), (43, -0.022), (44, -0.005), (45, -0.063), (46, 0.092), (47, -0.107), (48, 0.003), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92238092 <a title="208-lsi-1" href="./nips-2011-Optimistic_Optimization_of_a_Deterministic_Function_without_the_Knowledge_of_its_Smoothness.html">208 nips-2011-Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness</a></p>
<p>Author: Rémi Munos</p><p>Abstract: We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.</p><p>2 0.8426615 <a title="208-lsi-2" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>Author: Aleksandrs Slivkins</p><p>Abstract: The multi-armed bandit (MAB) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation. In this setting, an online algorithm has a ﬁxed set of alternatives (“arms”), and in each round it selects one arm and then observes the corresponding reward. While the case of small number of arms is by now well-understood, a lot of recent work has focused on multi-armed bandits with (inﬁnitely) many arms, where one needs to assume extra structure in order to make the problem tractable. In particular, in the Lipschitz MAB problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs. In this paper we consider the more realistic scenario in which the metric space is implicit – it is deﬁned by the available structure but not revealed to the algorithm directly. Speciﬁcally, we assume that an algorithm is given a tree-based classiﬁcation of arms. For any given problem instance such a classiﬁcation implicitly deﬁnes a similarity metric space, but the numerical similarity information is not available to the algorithm. We provide an algorithm for this setting, whose performance guarantees (almost) match the best known guarantees for the corresponding instance of the Lipschitz MAB problem. 1</p><p>3 0.74250901 <a title="208-lsi-3" href="./nips-2011-Reconstructing_Patterns_of_Information_Diffusion_from_Incomplete_Observations.html">234 nips-2011-Reconstructing Patterns of Information Diffusion from Incomplete Observations</a></p>
<p>Author: Flavio Chierichetti, David Liben-nowell, Jon M. Kleinberg</p><p>Abstract: Motivated by the spread of on-line information in general and on-line petitions in particular, recent research has raised the following combinatorial estimation problem. There is a tree T that we cannot observe directly (representing the structure along which the information has spread), and certain nodes randomly decide to make their copy of the information public. In the case of a petition, the list of names on each public copy of the petition also reveals a path leading back to the root of the tree. What can we conclude about the properties of the tree we observe from these revealed paths, and can we use the structure of the observed tree to estimate the size of the full unobserved tree T ? Here we provide the ﬁrst algorithm for this size estimation task, together with provable guarantees on its performance. We also establish structural properties of the observed tree, providing the ﬁrst rigorous explanation for some of the unusual structural phenomena present in the spread of real chain-letter petitions on the Internet. 1</p><p>4 0.66547906 <a title="208-lsi-4" href="./nips-2011-Projection_onto_A_Nonnegative_Max-Heap.html">226 nips-2011-Projection onto A Nonnegative Max-Heap</a></p>
<p>Author: Jun Liu, Liang Sun, Jieping Ye</p><p>Abstract: We consider the problem of computing the Euclidean projection of a vector of length p onto a non-negative max-heap—an ordered tree where the values of the nodes are all nonnegative and the value of any parent node is no less than the value(s) of its child node(s). This Euclidean projection plays a building block role in the optimization problem with a non-negative maxheap constraint. Such a constraint is desirable when the features follow an ordered tree structure, that is, a given feature is selected for the given regression/classiﬁcation task only if its parent node is selected. In this paper, we show that such Euclidean projection problem admits an analytical solution and we develop a top-down algorithm where the key operation is to ﬁnd the so-called maximal root-tree of the subtree rooted at each node. A naive approach for ﬁnding the maximal root-tree is to enumerate all the possible root-trees, which, however, does not scale well. We reveal several important properties of the maximal root-tree, based on which we design a bottom-up algorithm with merge for eﬃciently ﬁnding the maximal roottree. The proposed algorithm has a (worst-case) linear time complexity for a sequential list, and O(p2 ) for a general tree. We report simulation results showing the eﬀectiveness of the max-heap for regression with an ordered tree structure. Empirical results show that the proposed algorithm has an expected linear time complexity for many special cases including a sequential list, a full binary tree, and a tree with depth 1. 1</p><p>5 0.64785576 <a title="208-lsi-5" href="./nips-2011-See_the_Tree_Through_the_Lines%3A_The_Shazoo_Algorithm.html">242 nips-2011-See the Tree Through the Lines: The Shazoo Algorithm</a></p>
<p>Author: Fabio Vitale, Nicolò Cesa-bianchi, Claudio Gentile, Giovanni Zappella</p><p>Abstract: Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains. Since graph sparsiﬁcation via spanning trees retains enough information while making the task much easier, trees are an important special case of this problem. Although it is known how to predict the nodes of an unweighted tree in a nearly optimal way, in the weighted case a fully satisfactory algorithm is not available yet. We ﬁll this hole and introduce an efﬁcient node predictor, S HAZOO, which is nearly optimal on any weighted tree. Moreover, we show that S HAZOO can be viewed as a common nontrivial generalization of both previous approaches for unweighted trees and weighted lines. Experiments on real-world datasets conﬁrm that S HAZOO performs well in that it fully exploits the structure of the input tree, and gets very close to (and sometimes better than) less scalable energy minimization methods. 1</p><p>6 0.54528505 <a title="208-lsi-6" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>7 0.51456165 <a title="208-lsi-7" href="./nips-2011-Spectral_Methods_for_Learning_Multivariate_Latent_Tree_Structure.html">267 nips-2011-Spectral Methods for Learning Multivariate Latent Tree Structure</a></p>
<p>8 0.47690165 <a title="208-lsi-8" href="./nips-2011-Finite_Time_Analysis_of_Stratified_Sampling_for_Monte_Carlo.html">97 nips-2011-Finite Time Analysis of Stratified Sampling for Monte Carlo</a></p>
<p>9 0.46069935 <a title="208-lsi-9" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>10 0.45490539 <a title="208-lsi-10" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>11 0.44846502 <a title="208-lsi-11" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>12 0.44645169 <a title="208-lsi-12" href="./nips-2011-Multi-Bandit_Best_Arm_Identification.html">175 nips-2011-Multi-Bandit Best Arm Identification</a></p>
<p>13 0.43102759 <a title="208-lsi-13" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>14 0.43077534 <a title="208-lsi-14" href="./nips-2011-Hierarchical_Topic_Modeling_for_Analysis_of_Time-Evolving_Personal_Choices.html">115 nips-2011-Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices</a></p>
<p>15 0.422658 <a title="208-lsi-15" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>16 0.41671792 <a title="208-lsi-16" href="./nips-2011-Active_Learning_Ranking_from_Pairwise_Preferences_with_Almost_Optimal_Query_Complexity.html">20 nips-2011-Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity</a></p>
<p>17 0.41406596 <a title="208-lsi-17" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>18 0.40684515 <a title="208-lsi-18" href="./nips-2011-A_Global_Structural_EM_Algorithm_for_a_Model_of_Cancer_Progression.html">6 nips-2011-A Global Structural EM Algorithm for a Model of Cancer Progression</a></p>
<p>19 0.39778915 <a title="208-lsi-19" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>20 0.38056496 <a title="208-lsi-20" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.067), (4, 0.04), (20, 0.07), (26, 0.031), (31, 0.058), (33, 0.025), (40, 0.014), (43, 0.08), (45, 0.115), (57, 0.068), (62, 0.202), (74, 0.022), (79, 0.035), (83, 0.016), (99, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80902255 <a title="208-lda-1" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>Author: Fahad S. Khan, Joost Weijer, Andrew D. Bagdanov, Maria Vanrell</p><p>Abstract: We describe a novel technique for feature combination in the bag-of-words model of image classiﬁcation. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classiﬁcation problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to ﬁnd discriminative combinations of cues and the resulting vocabulary of portmanteau1 words is compact, has the cue binding property, and supports individual weighting of cues in the ﬁnal image representation. State-of-theart results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, signiﬁcantly more complex approaches to multi-cue image representation. 1</p><p>same-paper 2 0.80047429 <a title="208-lda-2" href="./nips-2011-Optimistic_Optimization_of_a_Deterministic_Function_without_the_Knowledge_of_its_Smoothness.html">208 nips-2011-Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness</a></p>
<p>Author: Rémi Munos</p><p>Abstract: We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.</p><p>3 0.77813208 <a title="208-lda-3" href="./nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</a></p>
<p>Author: Zhouchen Lin, Risheng Liu, Zhixun Su</p><p>Abstract: Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efﬁciently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve lowrank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n3 ) of the original ADM based method to O(rn2 ), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms. 1</p><p>4 0.7506147 <a title="208-lda-4" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<p>Author: Gilles Blanchard, Gyemin Lee, Clayton Scott</p><p>Abstract: We consider the problem of assigning class labels to an unlabeled test data set, given several labeled training data sets drawn from similar distributions. This problem arises in several applications where data distributions ﬂuctuate because of biological, technical, or other sources of variation. We develop a distributionfree, kernel-based approach to the problem. This approach involves identifying an appropriate reproducing kernel Hilbert space and optimizing a regularized empirical risk over the space. We present generalization error analysis, describe universal kernels, and establish universal consistency of the proposed methodology. Experimental results on ﬂow cytometry data are presented. 1</p><p>5 0.67067152 <a title="208-lda-5" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>Author: Jia Deng, Sanjeev Satheesh, Alexander C. Berg, Fei Li</p><p>Abstract: We present a novel approach to efﬁciently learn a label tree for large scale classiﬁcation with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classiﬁers for each node in the tree. This approach also allows ﬁne grained control over the efﬁciency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classiﬁcation with 10184 classes and 9 million images. We demonstrate signiﬁcant improvements in test accuracy and efﬁciency with less training time and more balanced trees compared to the previous state of the art by Bengio et al. 1</p><p>6 0.65445346 <a title="208-lda-6" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>7 0.65097946 <a title="208-lda-7" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>8 0.65087688 <a title="208-lda-8" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>9 0.64818901 <a title="208-lda-9" href="./nips-2011-On_fast_approximate_submodular_minimization.html">199 nips-2011-On fast approximate submodular minimization</a></p>
<p>10 0.64636761 <a title="208-lda-10" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>11 0.64611411 <a title="208-lda-11" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>12 0.64534181 <a title="208-lda-12" href="./nips-2011-Finite_Time_Analysis_of_Stratified_Sampling_for_Monte_Carlo.html">97 nips-2011-Finite Time Analysis of Stratified Sampling for Monte Carlo</a></p>
<p>13 0.64443457 <a title="208-lda-13" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>14 0.64433664 <a title="208-lda-14" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>15 0.64366883 <a title="208-lda-15" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>16 0.6421572 <a title="208-lda-16" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>17 0.64192331 <a title="208-lda-17" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>18 0.64112937 <a title="208-lda-18" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>19 0.63701898 <a title="208-lda-19" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>20 0.63696587 <a title="208-lda-20" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
