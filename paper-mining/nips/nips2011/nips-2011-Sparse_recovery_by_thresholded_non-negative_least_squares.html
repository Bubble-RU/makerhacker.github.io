<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 nips-2011-Sparse recovery by thresholded non-negative least squares</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-265" href="#">nips2011-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 nips-2011-Sparse recovery by thresholded non-negative least squares</h1>
<br/><p>Source: <a title="nips-2011-265-pdf" href="http://papers.nips.cc/paper/4231-sparse-recovery-by-thresholded-non-negative-least-squares.pdf">pdf</a></p><p>Author: Martin Slawski, Matthias Hein</p><p>Abstract: Non-negative data are commonly encountered in numerous ﬁelds, making nonnegative least squares regression (NNLS) a frequently used tool. At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern high-dimensional linear models. Even in this setting − unlike ﬁrst intuition may suggest − we show that for a broad class of designs, NNLS is resistant to overﬁtting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming 1 regularization. Since NNLS also circumvents the delicate choice of a regularization parameter, our ﬁndings suggest that NNLS may be the method of choice. 1</p><p>Reference: <a title="nips-2011-265-reference" href="../nips2011_reference/nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sparse recovery by thresholded non-negative least squares  Martin Slawski and Matthias Hein Department of Computer Science Saarland University Campus E 1. [sent-1, score-0.46]
</p><p>2 de  Abstract Non-negative data are commonly encountered in numerous ﬁelds, making nonnegative least squares regression (NNLS) a frequently used tool. [sent-4, score-0.196]
</p><p>3 Even in this setting − unlike ﬁrst intuition may suggest − we show that for a broad class of designs, NNLS is resistant to overﬁtting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming 1 regularization. [sent-7, score-0.231]
</p><p>4 1  Introduction  Consider the linear regression model  y = Xβ ∗ + ε, (1) where y is a vector of observations, X ∈ Rn×p a design matrix, ε a vector of noise and β ∗ a vector of coefﬁcients to be estimated. [sent-9, score-0.059]
</p><p>5 p = O(n) or even p n, in which case one cannot hope to recover the target β ∗ if it does not satisfy one of various kinds of sparsity constraints, the simplest being that ∗ β ∗ is supported on S = {j : βj = 0}, |S| = s < n. [sent-12, score-0.059]
</p><p>6 Non-negativity constraints emerge in numerous deconvolution and unmixing problems in diverse ﬁelds such as acoustics [1], astronomical imaging [2], computer vision [3], genomics [4], proteomics [5] and spectroscopy [6]; see [7] for a survey. [sent-19, score-0.145]
</p><p>7 Sparse recovery of non-negative signals in a noiseless setting (ε = 0) has been studied in a series of recent papers [8, 9, 10, 11]. [sent-20, score-0.132]
</p><p>8 One important ﬁnding of this body of work is that non-negativity constraints alone may sufﬁce for sparse recovery, without the need to employ sparsity-promoting 1 -regularization as usually. [sent-21, score-0.09]
</p><p>9 More speciﬁcally, we study non-negative least squares (NNLS) 1 2 min y − Xβ 2 (2) β 0 n with minimizer β and its counterpart after hard thresholding β(λ), βj (λ) =  βj , 0,  βj > λ, otherwise, j = 1, . [sent-23, score-0.441]
</p><p>10 On the other hand, the rather restrictive ’irrepresentable condition’ on the design is essentially necessary in order to infer the support S from the sparsity pattern of the lasso [15, 16]. [sent-28, score-0.323]
</p><p>11 [17, 18, 19], suggest to apply hard thresholding to the lasso solution to achieve support recovery. [sent-34, score-0.333]
</p><p>12 In light of this, thresholding a non-negative least squares solution, provided it is close to the target w. [sent-35, score-0.266]
</p><p>13 the ∞ -norm, is more attractive for at least two reasons: ﬁrst, there is no need to carefully tune the amount of 1 -regularization prior to thresholding; second, one may hope to detect relatively small non-zero coefﬁcients whose recovery is negatively affected by the bias of 1 -regularization. [sent-38, score-0.173]
</p><p>14 We ﬁrst prove a bound on the mean square prediction error of the NNLS estimator, demonstrating that it may be resistant to overﬁtting. [sent-40, score-0.05]
</p><p>15 Section 3 contains our main results on sparse recovery with noise. [sent-41, score-0.181]
</p><p>16 Experiments providing strong support of our theoretical ﬁndings are presented in Section 4. [sent-42, score-0.072]
</p><p>17 For a matrix A ∈ Rn×m , AJ denotes the matrix one obtains by extracting the columns corresponding to J. [sent-46, score-0.134]
</p><p>18 The matrix AJK is the sub-matrix of A by extracting rows in J and columns in K. [sent-51, score-0.067]
</p><p>19 The matrix X is assumed to be non-random and scaled s. [sent-59, score-0.065]
</p><p>20 zero-mean sub-Gaussian entries with parameter σ > 0, cf. [sent-65, score-0.05]
</p><p>21 2  Prediction error and uniqueness of the solution  In the following, the quantity of interest is the mean squared prediction error (MSE)  1 n  Xβ ∗ −X β  2 2. [sent-67, score-0.063]
</p><p>22 It is well-known that the MSE of ordinary least squares (OLS) as well as that of ridge regression in general does not vanish unless p/n → 0. [sent-69, score-0.235]
</p><p>23 To make this clear, let a design matrix X be given and set X = [X − X] by concatenating X and −X columnwise. [sent-72, score-0.097]
</p><p>24 The non-negativity constraint is then vacuous in the sense that X β = X β ols , where β ols is any OLS solution. [sent-73, score-0.15]
</p><p>25 However, non-negativity constraints on β can be strong when coupled with the following 1 condition imposed on the Gram matrix Σ = n X X. [sent-74, score-0.103]
</p><p>26 We call a design self-regularizing with universal constant κ ∈ (0, 1] if β Σβ ≥ κ(1 β)2 ∀β  0. [sent-76, score-0.059]
</p><p>27 (4)  The term ’self-regularizing’ refers to the fact that the quadratic form in Σ restricted to the nonnegative orthant acts like a regularizer arising from the design itself. [sent-77, score-0.117]
</p><p>28 all entries of the Gram matrix are at least κ0 , then (4) holds with κ = κ0 . [sent-80, score-0.129]
</p><p>29 , BB such that min1≤b≤B n XBb XBb κ0 , then B  min β Σβ ≥ β 0  b=1  1 βBb XBb XBb βBb ≥ κ0 n  B  (1 βBb )2 ≥ b=1  κ0 (1 β)2 . [sent-87, score-0.088]
</p><p>30 Then, with probability no less than 1 - 2/p, the NNLS estimator obeys 1 Xβ ∗ − X β n  2 2  ≤  8σ κ  2 log p ∗ β n  1  +  8σ 2 log p . [sent-92, score-0.128]
</p><p>31 It is important to note that exact sparsity of β ∗ is not needed for Theorem 1 to hold. [sent-94, score-0.059]
</p><p>32 The rate is the same as for the lasso if no further assumptions on the design are made, a result that is essentially obtained in the pioneering work [20]. [sent-95, score-0.216]
</p><p>33 φ1  φ15  y' y w B1  B2  B3  B4  B5  Figure 2: A polyhedral cone in R3 and its intersection with the simplex (right). [sent-96, score-0.087]
</p><p>34 The point y is contained in a face (bold) with normal vector w, whereas y is not. [sent-97, score-0.086]
</p><p>35 Denote by C = XRp the polyhedral cone generated + by the columns {Xj }p of X, which are henceforth assumed to be in general position in Rn . [sent-102, score-0.155]
</p><p>36 As j=1 visualized in Figure 2, sparse recovery by non-negativity constraints can be analyzed by studying the |F | face lattice of C [9, 10, 11]. [sent-103, score-0.285]
</p><p>37 , p}, we say that XF R+ is a face of C if there exists a separating hyperplane with normal vector w passing through the origin such that Xj , w > 0, j ∈ / F , Xj , w = 0, j ∈ F . [sent-107, score-0.205]
</p><p>38 Sparse recovery in a noiseless setting (ε = 0) can then be characterized concisely by the following statement which can essentially be found in prior work [9, 10, 11, 21]. [sent-108, score-0.132]
</p><p>39 If XS Rs is a face of C and the + columns of X are in general position in Rn , then the constrained linear system Xβ = y sb. [sent-111, score-0.127]
</p><p>40 By deﬁnition, since XS Rs is a face of C, there exists a w ∈ Rn s. [sent-115, score-0.087]
</p><p>41 Given Theorem 1 and Proposition 1, we turn to uniqueness in the noisy case. [sent-124, score-0.088]
</p><p>42 Using general position of the columns of X, Proposition 1 implies that β is unique. [sent-131, score-0.064]
</p><p>43 2 3  3  Sparse recovery in the presence of noise  Proposition 1 states that support recovery requires XS Rs to be a face of XRp , which is equivalent + + to the existence of a hyperplane separating XS Rs from the rest of C. [sent-134, score-0.493]
</p><p>44 For the noisy case, mere + separation is not enough − a quantiﬁcation is needed, which is provided by the following two incoherence constants that are of central importance for our main result. [sent-135, score-0.108]
</p><p>45 Both are speciﬁc to NNLS and have not been used previously in the literature on sparse recovery. [sent-136, score-0.049]
</p><p>46 , p}, the separating hyperplane constant is deﬁned as τ (S) = max τ τ,w  1 1 sb. [sent-141, score-0.118]
</p><p>47 √ XS w = 0, √ XS c w τ 1, w n n 1 duality √ XS θ − XS c λ 2 , = min n θ∈Rs , λ∈T p−s−1  2  ≤ 1,  (6) (7)  where T m−1 = {v ∈ Rm : v 0, 1 v = 1} denotes the simplex in Rm , i. [sent-143, score-0.111]
</p><p>48 We denote by ΠS and Π⊥ the orthogonal projections on the subspace spanned by {Xj }j∈S and its S orthogonal complement, respectively, and set Z = Π⊥ XS c . [sent-146, score-0.115]
</p><p>49 One can equivalently express (7) as S 1 2 τ (S) = min λ Z Zλ. [sent-147, score-0.088]
</p><p>50 , p} and Z = Π⊥ XS c , ω(S) is deﬁned as S ω(S) =  min  min  ∅=F ⊆{1,. [sent-154, score-0.176]
</p><p>51 (9)  In the supplement, we show that i) ω(S) > 0 ⇔ τ (S) > 0 ⇔ XS Rs is a face of C, and ii) + 1 ω(S) ≤ 1, with equality if {Xj }j∈S and {Xj }j∈S c are orthogonal and n XS c XS c is entry-wise non1 negative. [sent-158, score-0.109]
</p><p>52 Denoting the entries of Σ = n X X by σjk , 1 ≤ j, k ≤ p, our main result additionally involves the constants ∗ µ(S) = maxj∈S maxk∈S c |σjk |, µ+ (S) = maxj∈S k∈S c |σjk |, βmin (S) = minj∈S βj , −1 K(S) = maxv: v ∞ =1 ΣSS v ∞ , φmin (S) = minv: v 2 =1 ΣSS v 2 . [sent-159, score-0.091]
</p><p>53 Consider the thresholded NNLS estimator β(λ) deﬁned in (3) with support S(λ). [sent-161, score-0.27]
</p><p>54 (i) If λ >  2 log p n  2σ τ 2 (S) b  and  βmin (S) > λ, λ = λ(1 + K(S)µ(S)) + (ii) or if λ >  2 log p n  2σ ω (S) b  ∞  2 log p , n  and  βmin (S) > λ, λ = λ(1 + K(S)µ+ (S)) + then β(λ) − β ∗  2σ {φmin (S)}1/2  2σ {φmin (S)}1/2  2 log p , n  ≤ λ and S(λ) = S with probability no less than 1 − 10/p. [sent-162, score-0.192]
</p><p>55 The concept of a separating functional as in (6) is also used to show support recovery for the lasso [15, 16] as well as for orthogonal matching pursuit [22, 23]. [sent-164, score-0.487]
</p><p>56 β is a minimizer of (2) if and only if there exists F ⊆ {1, . [sent-169, score-0.07]
</p><p>57 n j  The next lemma is crucial, since it permits us to decouple βS from βS c . [sent-173, score-0.064]
</p><p>58 Consider the two non-negative least squares problems (P 1) : min β (P 1) 0  1 ⊥ Π (ε − XS c β (P 1) ) n S  2 2  (P 2) :  min β (P 2) 0  1 ΠS y − XS β (P 2) − ΠS XS c β (P 1) n  2 2  with minimizers β (P 1) of (P 1) and β (P 2) of (P 2), respectively. [sent-175, score-0.314]
</p><p>59 If β (P 2) 0, then setting βS = (P 2) (P 1) c = β β and βS yields a minimizer β of the non-negative least squares problem (2). [sent-176, score-0.184]
</p><p>60 can be lower bounded via 1 ξ − Z β (P 1) n  2 2  (β (P 1) )  1 ξ n  ≤  2 2  ⇒ (β (P 1) )  1 Z Z β (P 1) ≥ n  1 Z Zλ n  min λ p−s−1  λ∈T  β (P 1)  2 1  = τ 2 (S) β (P 1) 2 . [sent-188, score-0.088]
</p><p>61 Using the closed form expression for the ordinary least squares estimator, one obtains 1 1 ∗ ∗ ¯ β (P 2) = Σ−1 XS (XS βS + ΠS ε − ΠS XS c β (P 1) ) = βS + Σ−1 XS ε − Σ−1 ΣSS c β (P 1) . [sent-201, score-0.235]
</p><p>62 It follows that the two events {M ≤ 2σ 2 log p } and {M ≤ {φmin2σ 1/2 2 log p } both hold with probability n n (S)} no less than 1 − 10/p, cf. [sent-207, score-0.096]
</p><p>63 Subsequent thresholding with the respective choices made for λ yields the assertion. [sent-211, score-0.128]
</p><p>64 2 In the sequel, we apply Theorem 2 to speciﬁc classes of designs commonly studied in the literature, for which thresholded NNLS achieves an ∞ -error of the optimal order O( log(p)/n). [sent-212, score-0.315]
</p><p>65 Let the entries of the Gram matrix Σ be given by σjk = ρ|j−k| , 1 ≤ j, k ≤ p, 0 ≤ ρ < 1, so that the {Xj }p form a Markov random ﬁeld in which Xj is conditionally j=1 independent of {Xk }k∈{j−1,j,j+1} given {Xj−1 , Xj+1 }, cf. [sent-215, score-0.088]
</p><p>66 The conditional independence / structure implies that all entries of Z Z are non-negative, such that, using the deﬁnition of ω(S), ω(S) ≥  min  1 Zj =1 n ∞  min  1≤j≤p−s v 0, v  Zv =  min 1≤j≤(p−s)  1 1 (Z Z)jj + n n  min{(Z Z)jk , 0}, k=j 2  2ρ 1 the sum on the r. [sent-217, score-0.314]
</p><p>67 For the remaining constants in (10), one can show that ΣSS is a band matrix of bandwidth no more than 3 for all choices of S such that φmin (S) and K(S) are uniformly lower and upper bounded, respectively, by constants depending on ρ only. [sent-221, score-0.12]
</p><p>68 For any S, one computes that the matrix n Z Z is of the same regular structure with diagonal entries all equal to 1 − δ and off-diagonal entries all equal to ρ − δ, where δ = ρ2 s/(1 + (s − 1)ρ). [sent-226, score-0.138]
</p><p>69 Therefore, using (8), the separating hyperplane constant (7) can be computed in closed form: τ 2 (S) =  (1 − ρ)ρ 1−ρ + = O(s−1 ). [sent-227, score-0.118]
</p><p>70 2 log p as in part (ii) of Theorem 2 and combining the strong Choosing the threshold λ = ω2σ b (S) n 1 -bound (17) on the off-support coefﬁcients with a slight modiﬁcation of the bound (14) together with φmin (S) = 1 − ρ yields again the desired optimal bound of the form (15). [sent-230, score-0.1]
</p><p>71 So far, the design matrix X has been assumed to be ﬁxed. [sent-232, score-0.124]
</p><p>72 As shown above, the incoherence constant τ 2 (S), which gives rise to a strong bound on βS c 1 , scales favourably and can be computed in closed form. [sent-244, score-0.066]
</p><p>73 For random designs from Ens+ , one additionally has to take into account the deviation between Σ and Σ∗ . [sent-245, score-0.125]
</p><p>74 Then there exists constants c, c1 , c2 , c3 , C, C > 0 such that for all n ≥ C log(p)s2 ,  τ 2 (S) ≥ cs−1 − C  log(p)/n  with probability no less than 1 − 3/p − exp(−c1 n) − 2 exp(−c2 log p) − exp(−c3 log1/2 (p)s). [sent-255, score-0.113]
</p><p>75 from a Gaussian distribution whose covariance matrix has the power decay structure of Example 1 with parameter ρ = 0. [sent-265, score-0.181]
</p><p>76 the population Gram matrix Σ∗ has equicorrelation structure with ρ = 3/4. [sent-269, score-0.088]
</p><p>77 In the ﬁrst part, the parameter b is kept ﬁxed while the aspect ratio p/n of X and the fraction of sparsity s/n vary. [sent-275, score-0.059]
</p><p>78 The grid used for b is chosen speciﬁc to the designs, calibrated such that the sparse recovery problems are sufﬁciently challenging. [sent-285, score-0.181]
</p><p>79 For the design from Ens+ , p/n ∈ {2, 3, 5, 10}, whereas for power decay p/n ∈ {1. [sent-286, score-0.202]
</p><p>80 Across these runs, we compare the probability of ’success’ of thresholded NNLS (tNNLS), non-negative lasso (NN 1 ), thresholded non-negative lasso (tNN 1 ) and orthogonal matching pursuit (OMP, [22, 23]). [sent-292, score-0.78]
</p><p>81 For OMP, we check whether the support S is recovered in the ﬁrst s steps. [sent-301, score-0.073]
</p><p>82 Note that, when comparing tNNLS and tNN 1 , the lasso is given an advantage, since we optimize over a range of solutions. [sent-302, score-0.157]
</p><p>83 6  b  Figure 3: Comparison of thresholded NNLS (red) and thresholded non-negative lasso (blue) for the experiments with constant s/n, while b (abscissa) and p/n (symbols) vary. [sent-342, score-0.537]
</p><p>84 3  s/n  s/n  power decay, w/o thresholding  power decay, non−negative lasso vs. [sent-375, score-0.385]
</p><p>85 3  s/n  Figure 4: Top: Comparison of thresholded NNLS (red) and the thresholded non-negative lasso (blue) for the experiments with constant b, while s/n (abscissa) and p/n (symbols) vary. [sent-407, score-0.537]
</p><p>86 Bottom left: Non-negative lasso without thresholding (blue) and orthogonal matching pursuit (magenta). [sent-408, score-0.371]
</p><p>87 Bottom right: Thresholded non-negative lasso (blue) and thresholded ordinary lasso (green). [sent-409, score-0.572]
</p><p>88 15 for power decay as displayed in the bottom left panel of Figure 4. [sent-412, score-0.169]
</p><p>89 This is in accordance with the literature where thresholding is proposed as remedy [17, 18, 19]. [sent-414, score-0.128]
</p><p>90 Yet, for a wide range of conﬁgurations, tNNLS visibly outperforms tNN 1 , a notable exception being power decay with larger values for p/n. [sent-415, score-0.143]
</p><p>91 This is in contrast to the design from Ens+ , where even p/n = 10 can be handled. [sent-416, score-0.059]
</p><p>92 To deal with higher levels of sparsity, thresholding seems to be inevitable. [sent-419, score-0.128]
</p><p>93 Thresholding the biased solution obtained by 1 -regularization requires a proper choice of the regularization parameter and is likely to be inferior to thresholded NNLS with regard to the detection of small signals. [sent-420, score-0.216]
</p><p>94 The experimental results provide strong support for the central message of the paper: even in high-dimensional, noisy settings, non-negativity constraints can be unexpectedly powerful when interacting with ’self-regularizing ’properties of the design. [sent-421, score-0.138]
</p><p>95 A split Bregman method for non-negative sparsity penalized least squares with applications to hyperspectral demixing. [sent-442, score-0.197]
</p><p>96 In NIPS workshop on practical applications of sparse modelling, 2010. [sent-453, score-0.049]
</p><p>97 On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations. [sent-470, score-0.219]
</p><p>98 A unique nonnegative solution to an undetermined system: from vectors to matrices. [sent-486, score-0.058]
</p><p>99 Sharp thresholds for noisy and high-dimensional recovery of sparsity using 1 constrained quadratic programming (Lasso). [sent-508, score-0.216]
</p><p>100 Sparse nonnegative solution of underdetermined linear equations by linear programming. [sent-531, score-0.107]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nnls', 0.521), ('xs', 0.43), ('ss', 0.237), ('thresholded', 0.19), ('ens', 0.169), ('lasso', 0.157), ('recovery', 0.132), ('thresholding', 0.128), ('designs', 0.125), ('zf', 0.124), ('xj', 0.12), ('gram', 0.111), ('tnn', 0.099), ('tnnls', 0.099), ('xbb', 0.099), ('squares', 0.097), ('decay', 0.093), ('rs', 0.089), ('min', 0.088), ('ols', 0.075), ('xrp', 0.074), ('jk', 0.074), ('ordinary', 0.068), ('bb', 0.068), ('separating', 0.064), ('uniqueness', 0.063), ('face', 0.063), ('deconvolution', 0.06), ('success', 0.06), ('design', 0.059), ('sparsity', 0.059), ('nonnegative', 0.058), ('nn', 0.055), ('maxj', 0.055), ('omp', 0.054), ('hyperplane', 0.054), ('power', 0.05), ('mse', 0.05), ('entries', 0.05), ('equicorrelation', 0.05), ('resistant', 0.05), ('slawski', 0.05), ('minj', 0.049), ('underdetermined', 0.049), ('sparse', 0.049), ('ii', 0.048), ('log', 0.048), ('support', 0.048), ('jj', 0.047), ('minimizer', 0.046), ('orthogonal', 0.046), ('symbols', 0.045), ('relegated', 0.044), ('astronomical', 0.044), ('irrepresentable', 0.044), ('abscissa', 0.044), ('incoherence', 0.042), ('constraints', 0.041), ('counterpart', 0.041), ('constants', 0.041), ('least', 0.041), ('pursuit', 0.04), ('zj', 0.04), ('xij', 0.039), ('matrix', 0.038), ('polyhedral', 0.038), ('kkt', 0.038), ('donoho', 0.038), ('theorem', 0.036), ('lemma', 0.036), ('position', 0.035), ('johnstone', 0.034), ('controlling', 0.034), ('estimator', 0.032), ('proposition', 0.031), ('columns', 0.029), ('obtains', 0.029), ('vanish', 0.029), ('threshold', 0.028), ('moderate', 0.028), ('permits', 0.028), ('rn', 0.028), ('assumed', 0.027), ('regard', 0.026), ('cone', 0.026), ('bottom', 0.026), ('cients', 0.026), ('noisy', 0.025), ('aj', 0.025), ('check', 0.025), ('coef', 0.025), ('exists', 0.024), ('strong', 0.024), ('blue', 0.024), ('uj', 0.024), ('annals', 0.023), ('spanned', 0.023), ('simplex', 0.023), ('contained', 0.023), ('sign', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="265-tfidf-1" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>Author: Martin Slawski, Matthias Hein</p><p>Abstract: Non-negative data are commonly encountered in numerous ﬁelds, making nonnegative least squares regression (NNLS) a frequently used tool. At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern high-dimensional linear models. Even in this setting − unlike ﬁrst intuition may suggest − we show that for a broad class of designs, NNLS is resistant to overﬁtting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming 1 regularization. Since NNLS also circumvents the delicate choice of a regularization parameter, our ﬁndings suggest that NNLS may be the method of choice. 1</p><p>2 0.20975918 <a title="265-tfidf-2" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>Author: Le Song, Eric P. Xing, Ankur P. Parikh</p><p>Abstract: Latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision, bioinformatics and natural language processing problems. However, existing models are largely restricted to discrete and Gaussian variables due to computational constraints; furthermore, algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search. We present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-Gaussian variables. Our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efﬁcient inference. Experiments on simulated and real data show the advantage of our proposed approach. 1</p><p>3 0.17065649 <a title="265-tfidf-3" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><p>4 0.16669071 <a title="265-tfidf-4" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><p>5 0.15690976 <a title="265-tfidf-5" href="./nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</a></p>
<p>Author: Jiarong Jiang, Piyush Rai, Hal Daume</p><p>Abstract: We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this. The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework. We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efﬁcacy of our proposed algorithm. 1</p><p>6 0.11497479 <a title="265-tfidf-6" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>7 0.099430293 <a title="265-tfidf-7" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>8 0.096126862 <a title="265-tfidf-8" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>9 0.091440171 <a title="265-tfidf-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.087556496 <a title="265-tfidf-10" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>11 0.084884144 <a title="265-tfidf-11" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>12 0.082278609 <a title="265-tfidf-12" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>13 0.074073493 <a title="265-tfidf-13" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>14 0.071427196 <a title="265-tfidf-14" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>15 0.070714623 <a title="265-tfidf-15" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>16 0.066702552 <a title="265-tfidf-16" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>17 0.066239521 <a title="265-tfidf-17" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>18 0.066045523 <a title="265-tfidf-18" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>19 0.066013142 <a title="265-tfidf-19" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>20 0.064652756 <a title="265-tfidf-20" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, 0.011), (2, -0.055), (3, -0.157), (4, -0.083), (5, 0.039), (6, 0.003), (7, 0.004), (8, 0.107), (9, -0.06), (10, 0.029), (11, -0.016), (12, -0.105), (13, 0.164), (14, 0.023), (15, -0.051), (16, 0.121), (17, -0.134), (18, 0.021), (19, 0.009), (20, -0.037), (21, 0.052), (22, -0.033), (23, -0.041), (24, 0.042), (25, -0.032), (26, -0.018), (27, 0.078), (28, 0.062), (29, 0.073), (30, 0.082), (31, 0.042), (32, 0.1), (33, -0.086), (34, 0.091), (35, 0.106), (36, 0.056), (37, -0.02), (38, 0.095), (39, -0.088), (40, -0.002), (41, 0.046), (42, -0.115), (43, 0.064), (44, 0.003), (45, -0.011), (46, 0.065), (47, 0.148), (48, 0.035), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92226869 <a title="265-lsi-1" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>Author: Martin Slawski, Matthias Hein</p><p>Abstract: Non-negative data are commonly encountered in numerous ﬁelds, making nonnegative least squares regression (NNLS) a frequently used tool. At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern high-dimensional linear models. Even in this setting − unlike ﬁrst intuition may suggest − we show that for a broad class of designs, NNLS is resistant to overﬁtting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming 1 regularization. Since NNLS also circumvents the delicate choice of a regularization parameter, our ﬁndings suggest that NNLS may be the method of choice. 1</p><p>2 0.69850981 <a title="265-lsi-2" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><p>3 0.61864388 <a title="265-lsi-3" href="./nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</a></p>
<p>Author: Jiarong Jiang, Piyush Rai, Hal Daume</p><p>Abstract: We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this. The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework. We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efﬁcacy of our proposed algorithm. 1</p><p>4 0.60118467 <a title="265-lsi-4" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><p>5 0.58548158 <a title="265-lsi-5" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>6 0.52321988 <a title="265-lsi-6" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>7 0.51400757 <a title="265-lsi-7" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>8 0.48209155 <a title="265-lsi-8" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>9 0.47049734 <a title="265-lsi-9" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>10 0.46944314 <a title="265-lsi-10" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>11 0.4667092 <a title="265-lsi-11" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>12 0.44920874 <a title="265-lsi-12" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>13 0.4470332 <a title="265-lsi-13" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>14 0.43723673 <a title="265-lsi-14" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>15 0.41744247 <a title="265-lsi-15" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>16 0.41607317 <a title="265-lsi-16" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>17 0.40422839 <a title="265-lsi-17" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>18 0.40144706 <a title="265-lsi-18" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>19 0.39130631 <a title="265-lsi-19" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>20 0.38443908 <a title="265-lsi-20" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.057), (4, 0.048), (20, 0.038), (26, 0.043), (31, 0.05), (33, 0.011), (43, 0.091), (45, 0.123), (48, 0.012), (57, 0.07), (65, 0.027), (74, 0.075), (83, 0.033), (98, 0.208), (99, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80482626 <a title="265-lda-1" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>Author: Martin Slawski, Matthias Hein</p><p>Abstract: Non-negative data are commonly encountered in numerous ﬁelds, making nonnegative least squares regression (NNLS) a frequently used tool. At least relative to its simplicity, it often performs rather well in practice. Serious doubts about its usefulness arise for modern high-dimensional linear models. Even in this setting − unlike ﬁrst intuition may suggest − we show that for a broad class of designs, NNLS is resistant to overﬁtting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming 1 regularization. Since NNLS also circumvents the delicate choice of a regularization parameter, our ﬁndings suggest that NNLS may be the method of choice. 1</p><p>2 0.6771881 <a title="265-lda-2" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>3 0.67491269 <a title="265-lda-3" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>Author: Kevin G. Jamieson, Robert Nowak</p><p>Abstract: This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects). In general, the ranking of n objects can be identiﬁed by standard sorting methods using n log2 n pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. Speciﬁcally, we assume that the objects can be embedded into a d-dimensional Euclidean space and that the rankings reﬂect their relative distances from a common reference point in Rd . We show that under this assumption the number of possible rankings grows like n2d and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than d log n adaptively selected pairwise comparisons, on average. If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis. 1</p><p>4 0.67324185 <a title="265-lda-4" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>Author: Arthur D. Szlam, Karol Gregor, Yann L. Cun</p><p>Abstract: This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modiﬁcations of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efﬁcient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-speciﬁed. 1</p><p>5 0.6722374 <a title="265-lda-5" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><p>6 0.67058355 <a title="265-lda-6" href="./nips-2011-On_fast_approximate_submodular_minimization.html">199 nips-2011-On fast approximate submodular minimization</a></p>
<p>7 0.66991895 <a title="265-lda-7" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>8 0.66979647 <a title="265-lda-8" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>9 0.66949826 <a title="265-lda-9" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>10 0.66888845 <a title="265-lda-10" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>11 0.66769058 <a title="265-lda-11" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>12 0.66664129 <a title="265-lda-12" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>13 0.66640121 <a title="265-lda-13" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>14 0.66566491 <a title="265-lda-14" href="./nips-2011-Projection_onto_A_Nonnegative_Max-Heap.html">226 nips-2011-Projection onto A Nonnegative Max-Heap</a></p>
<p>15 0.66496557 <a title="265-lda-15" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>16 0.66450256 <a title="265-lda-16" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>17 0.66429478 <a title="265-lda-17" href="./nips-2011-Shaping_Level_Sets_with_Submodular_Functions.html">251 nips-2011-Shaping Level Sets with Submodular Functions</a></p>
<p>18 0.66384566 <a title="265-lda-18" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>19 0.6634059 <a title="265-lda-19" href="./nips-2011-Optimistic_Optimization_of_a_Deterministic_Function_without_the_Knowledge_of_its_Smoothness.html">208 nips-2011-Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness</a></p>
<p>20 0.66335452 <a title="265-lda-20" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
