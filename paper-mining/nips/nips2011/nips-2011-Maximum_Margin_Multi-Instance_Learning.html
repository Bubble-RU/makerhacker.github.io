<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2011-Maximum Margin Multi-Instance Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-168" href="#">nips2011-168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 nips-2011-Maximum Margin Multi-Instance Learning</h1>
<br/><p>Source: <a title="nips-2011-168-pdf" href="http://papers.nips.cc/paper/4294-maximum-margin-multi-instance-learning.pdf">pdf</a></p><p>Author: Hua Wang, Heng Huang, Farhad Kamangar, Feiping Nie, Chris H. Ding</p><p>Abstract: Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reﬂect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class speciﬁc distance metrics and the locally adaptive signiﬁcance coefﬁcients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.</p><p>Reference: <a title="nips-2011-168-reference" href="../nips2011_reference/nips-2011-Maximum_Margin_Multi-Instance_Learning_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bag', 0.374), ('dist', 0.325), ('scs', 0.292), ('ipn', 0.27), ('mk', 0.263), ('sk', 0.223), ('wk', 0.2), ('im', 0.169), ('hors', 0.161), ('met', 0.161), ('mimlboost', 0.157), ('mimlsvm', 0.157), ('mil', 0.157), ('person', 0.141), ('sj', 0.139), ('sc', 0.125), ('query', 0.104), ('inst', 0.103), ('xi', 0.102), ('yik', 0.102)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="168-tfidf-1" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>Author: Hua Wang, Heng Huang, Farhad Kamangar, Feiping Nie, Chris H. Ding</p><p>Abstract: Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reﬂect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class speciﬁc distance metrics and the locally adaptive signiﬁcance coefﬁcients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.</p><p>2 0.19505386 <a title="168-tfidf-2" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>Author: Dan Zhang, Yan Liu, Luo Si, Jian Zhang, Richard D. Lawrence</p><p>Abstract: Most existing Multiple-Instance Learning (MIL) algorithms assume data instances and/or data bags are independently and identically distributed. But there often exists rich additional dependency/structure information between instances/bags within many applications of MIL. Ignoring this structure information limits the performance of existing MIL algorithms. This paper explores the research problem as multiple instance learning on structured data (MILSD) and formulates a novel framework that considers additional structure information. In particular, an effective and efﬁcient optimization algorithm has been proposed to solve the original non-convex optimization problem by using a combination of ConcaveConvex Constraint Programming (CCCP) method and an adapted Cutting Plane method, which deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data. Our method has the nice convergence property, with speciﬁed precision on each set of constraints. Experimental results on three different applications, i.e., webpage classiﬁcation, market targeting, and protein fold identiﬁcation, clearly demonstrate the advantages of the proposed method over state-of-the-art methods. 1</p><p>3 0.16592582 <a title="168-tfidf-3" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>Author: Vicente Ordonez, Girish Kulkarni, Tamara L. Berg</p><p>Abstract: We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset – performing a huge number of Flickr queries and then ﬁltering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning. 1</p><p>4 0.15977427 <a title="168-tfidf-4" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>5 0.15562707 <a title="168-tfidf-5" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that ﬁt the empirical data distribution. We instantiate this similarity measure with the Gammacompound-Laplace distribution, and show signiﬁcant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost. 1</p><p>6 0.15041137 <a title="168-tfidf-6" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>7 0.13888967 <a title="168-tfidf-7" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>8 0.12214992 <a title="168-tfidf-8" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>9 0.11738778 <a title="168-tfidf-9" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>10 0.11572725 <a title="168-tfidf-10" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>11 0.11454498 <a title="168-tfidf-11" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>12 0.10702945 <a title="168-tfidf-12" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>13 0.10375948 <a title="168-tfidf-13" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>14 0.10072419 <a title="168-tfidf-14" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>15 0.099549964 <a title="168-tfidf-15" href="./nips-2011-Multi-Bandit_Best_Arm_Identification.html">175 nips-2011-Multi-Bandit Best Arm Identification</a></p>
<p>16 0.099249303 <a title="168-tfidf-16" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>17 0.097491771 <a title="168-tfidf-17" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>18 0.097003244 <a title="168-tfidf-18" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>19 0.09542352 <a title="168-tfidf-19" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>20 0.094503984 <a title="168-tfidf-20" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.216), (1, -0.035), (2, 0.162), (3, -0.092), (4, -0.122), (5, 0.019), (6, -0.006), (7, -0.042), (8, -0.012), (9, 0.123), (10, 0.041), (11, -0.096), (12, 0.079), (13, 0.026), (14, -0.034), (15, -0.018), (16, -0.003), (17, -0.014), (18, 0.059), (19, 0.124), (20, -0.113), (21, -0.027), (22, -0.052), (23, 0.044), (24, -0.139), (25, -0.001), (26, -0.146), (27, 0.057), (28, -0.005), (29, -0.045), (30, -0.121), (31, -0.01), (32, -0.033), (33, -0.009), (34, -0.1), (35, 0.085), (36, 0.113), (37, 0.04), (38, -0.139), (39, -0.019), (40, -0.065), (41, -0.051), (42, -0.063), (43, 0.12), (44, -0.061), (45, -0.069), (46, -0.066), (47, 0.075), (48, -0.057), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94935387 <a title="168-lsi-1" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>Author: Hua Wang, Heng Huang, Farhad Kamangar, Feiping Nie, Chris H. Ding</p><p>Abstract: Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reﬂect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class speciﬁc distance metrics and the locally adaptive signiﬁcance coefﬁcients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.</p><p>2 0.74030823 <a title="168-lsi-2" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that ﬁt the empirical data distribution. We instantiate this similarity measure with the Gammacompound-Laplace distribution, and show signiﬁcant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost. 1</p><p>3 0.72998643 <a title="168-lsi-3" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efﬁcient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>4 0.67897737 <a title="168-lsi-4" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>5 0.64332139 <a title="168-lsi-5" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>Author: Dan Zhang, Yan Liu, Luo Si, Jian Zhang, Richard D. Lawrence</p><p>Abstract: Most existing Multiple-Instance Learning (MIL) algorithms assume data instances and/or data bags are independently and identically distributed. But there often exists rich additional dependency/structure information between instances/bags within many applications of MIL. Ignoring this structure information limits the performance of existing MIL algorithms. This paper explores the research problem as multiple instance learning on structured data (MILSD) and formulates a novel framework that considers additional structure information. In particular, an effective and efﬁcient optimization algorithm has been proposed to solve the original non-convex optimization problem by using a combination of ConcaveConvex Constraint Programming (CCCP) method and an adapted Cutting Plane method, which deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data. Our method has the nice convergence property, with speciﬁed precision on each set of constraints. Experimental results on three different applications, i.e., webpage classiﬁcation, market targeting, and protein fold identiﬁcation, clearly demonstrate the advantages of the proposed method over state-of-the-art methods. 1</p><p>6 0.6356163 <a title="168-lsi-6" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>7 0.6165117 <a title="168-lsi-7" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>8 0.614654 <a title="168-lsi-8" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>9 0.59765416 <a title="168-lsi-9" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>10 0.56102979 <a title="168-lsi-10" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>11 0.51634109 <a title="168-lsi-11" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>12 0.50931603 <a title="168-lsi-12" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>13 0.50619572 <a title="168-lsi-13" href="./nips-2011-Testing_a_Bayesian_Measure_of_Representativeness_Using_a_Large_Image_Database.html">280 nips-2011-Testing a Bayesian Measure of Representativeness Using a Large Image Database</a></p>
<p>14 0.49009058 <a title="168-lsi-14" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>15 0.47823101 <a title="168-lsi-15" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>16 0.47366041 <a title="168-lsi-16" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>17 0.47132969 <a title="168-lsi-17" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>18 0.46303284 <a title="168-lsi-18" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>19 0.46185142 <a title="168-lsi-19" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>20 0.46038654 <a title="168-lsi-20" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.043), (22, 0.043), (32, 0.283), (36, 0.135), (55, 0.151), (65, 0.056), (68, 0.179)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78219914 <a title="168-lda-1" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>Author: Xinghua Lou, Fred A. Hamprecht</p><p>Abstract: We study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology. Reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research. Existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search. We propose a structured learning approach that allows to learn optimum parameters automatically from a training set. This allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences. 1</p><p>same-paper 2 0.74304688 <a title="168-lda-2" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>Author: Hua Wang, Heng Huang, Farhad Kamangar, Feiping Nie, Chris H. Ding</p><p>Abstract: Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reﬂect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class speciﬁc distance metrics and the locally adaptive signiﬁcance coefﬁcients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.</p><p>3 0.70728314 <a title="168-lda-3" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>4 0.7051301 <a title="168-lda-4" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>Author: Stéphan J. Clémençcon</p><p>Abstract: Many clustering techniques aim at optimizing empirical criteria that are of the form of a U -statistic of degree two. Given a measure of dissimilarity between pairs of observations, the goal is to minimize the within cluster point scatter over a class of partitions of the feature space. It is the purpose of this paper to deﬁne a general statistical framework, relying on the theory of U -processes, for studying the performance of such clustering methods. In this setup, under adequate assumptions on the complexity of the subsets forming the partition candidates, the √ excess of clustering risk is proved to be of the order OP (1/ n). Based on recent results related to the tail behavior of degenerate U -processes, it is also shown how to establish tighter rate bounds. Model selection issues, related to the number of clusters forming the data partition in particular, are also considered. 1</p><p>5 0.70020688 <a title="168-lda-5" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>Author: Shilin Ding, Grace Wahba, Xiaojin Zhu</p><p>Abstract: In discrete undirected graphical models, the conditional independence of node labels Y is speciﬁed by the graph structure. We study the case where there is another input random vector X (e.g. observed features) such that the distribution P (Y | X) is determined by functions of X that characterize the (higher-order) interactions among the Y ’s. The main contribution of this paper is to learn the graph structure and the functions conditioned on X at the same time. We prove that discrete undirected graphical models with feature X are equivalent to multivariate discrete models. The reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure. The functional spaces can be ﬂexibly determined by kernels. Additionally, we impose a Structure Lasso (SLasso) penalty on groups of functions to learn the graph structure. These groups with overlaps are designed to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure. 1</p><p>6 0.6982708 <a title="168-lda-6" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>7 0.69604552 <a title="168-lda-7" href="./nips-2011-Multi-Bandit_Best_Arm_Identification.html">175 nips-2011-Multi-Bandit Best Arm Identification</a></p>
<p>8 0.69444972 <a title="168-lda-8" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>9 0.69427025 <a title="168-lda-9" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>10 0.69340897 <a title="168-lda-10" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>11 0.69172287 <a title="168-lda-11" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>12 0.68289101 <a title="168-lda-12" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>13 0.68107212 <a title="168-lda-13" href="./nips-2011-Higher-Order_Correlation_Clustering_for_Image_Segmentation.html">119 nips-2011-Higher-Order Correlation Clustering for Image Segmentation</a></p>
<p>14 0.67869598 <a title="168-lda-14" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>15 0.67817008 <a title="168-lda-15" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>16 0.67781246 <a title="168-lda-16" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>17 0.67552114 <a title="168-lda-17" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>18 0.6741361 <a title="168-lda-18" href="./nips-2011-Data_Skeletonization_via_Reeb_Graphs.html">67 nips-2011-Data Skeletonization via Reeb Graphs</a></p>
<p>19 0.67279905 <a title="168-lda-19" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>20 0.67170638 <a title="168-lda-20" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
