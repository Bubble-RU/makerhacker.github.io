<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-57" href="#">nips2011-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</h1>
<br/><p>Source: <a title="nips-2011-57-pdf" href="http://papers.nips.cc/paper/4333-comparative-analysis-of-viterbi-training-and-maximum-likelihood-estimation-for-hmms.pdf">pdf</a></p><p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>Reference: <a title="nips-2011-57-reference" href="../nips2011_reference/nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. [sent-3, score-0.14]
</p><p>2 While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. [sent-4, score-0.388]
</p><p>3 We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. [sent-5, score-0.344]
</p><p>4 Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. [sent-8, score-0.187]
</p><p>5 For the parameter estimation problem, the prevailing method is maximum likelihood (ML) estimation, which ﬁnds the parameters by maximizing the likelihood of the observed data. [sent-13, score-0.347]
</p><p>6 Instead of maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. [sent-16, score-0.388]
</p><p>7 Maximizing VT objective function is hard [8], so in practice it is implemented via an EM-style iterations between calculating the MAP sequence and adjusting the model parameters based on the sequence statistics. [sent-17, score-0.28]
</p><p>8 We develop an analytical approach based on generating functions for examining the asymptotic properties of both approaches. [sent-29, score-0.15]
</p><p>9 Previously, a similar approach was used for calculating entropy rate of a hidden Markov process [1]. [sent-30, score-0.333]
</p><p>10 Furthermore, we demonstrate the approach on a particular class of HMM with one unambiguous symbol and obtain a closed–form solution to the estimation problem. [sent-33, score-0.247]
</p><p>11 We ﬁnd that for the considered model VT is a better option if the ML objective is degenerate (i. [sent-37, score-0.15]
</p><p>12 Namely, not only VT recovers the identiﬁable parameters but it also provides a simple (in the sense that non-identiﬁable parameters are set to zero) and optimal (in the sense of the MAP performance) solution. [sent-40, score-0.13]
</p><p>13 In addition, we show that the VT algorithm for this model converges faster than the conventional EM approach. [sent-42, score-0.133]
</p><p>14 Whenever the ML objective is not degenerate, VT leads generally to inferior results that, nevertheless, may be partially correct in the sense of recovering certain (not all) parameters. [sent-43, score-0.136]
</p><p>15 } be a discrete-time, stationary, Markov process with conditional probability Pr[Sk+l = sk |Sk−1+l = sk−1 ] = p(sk |sk−1 ),  (1)  where l is an integer. [sent-47, score-0.212]
</p><p>16 Each realization sk of the random variable Sk takes values 1, . [sent-48, score-0.216]
</p><p>17 We assume L that S is mixing: it has a unique stationary distribution pst (s), r=1 p(s|r)pst (r) = pst (s), that is established from any initial probability in the long time limit. [sent-52, score-0.246]
</p><p>18 , M , be noisy observations of Si : the (timeinvariant) conditional probability of observing Xi = xi given the realization Si = si of the Markov process is π(xk |sk ). [sent-55, score-0.246]
</p><p>19 Ts1 s0 (x1 ) pst (s0 ),  (2)  where the L × L transfer-matrix T (x) with matrix elements Tsi si−1 (x) is deﬁned as Tsi si−1 (x) = π(x|si ) p(si |si−1 ). [sent-65, score-0.123]
</p><p>20 The probabilities for X can be represented as follows: P (x) =  ss  [T(x)]ss pst (s ), T(x) ≡ T (xN )T (xN −1 ) . [sent-71, score-0.247]
</p><p>21 1  Parameter Estimation Maximum Likelihood Estimation  The unknown parameters of an HMM are the transition probabilities p(s|s ) of the Markov process and the observation probabilities π(x|s); see (2). [sent-76, score-0.385]
</p><p>22 This is standardly done via the maximum-likelihood approach: one starts with some ˆ ˆ trial values p(s|s ), π (x|s) of the parameters and calculates the (log)-likelihood ln P (x), where P ˆ ˆ means the probability (4) calculated at the trial values of the parameters. [sent-78, score-0.718]
</p><p>23 Next, one maximizes ˆ ln P (x) over p(s|s ) and π (x|s) for the given observed sequence x (in practice this is done via the ˆ ˆ Baum-Welch algorithm [20, 9]). [sent-79, score-0.364]
</p><p>24 Provided that the length N of the observed sequence is long, and recaling that X is mixing (due to the analogous feature of S) we get probability-one convergence (law of large numbers) [9]: ˆ ln P (x) →  y  ˆ P (y) ln P (y),  (5)  where the average is taken over the true probability P (. [sent-81, score-0.726]
</p><p>25 Since the relative ˆ ˆ entropy is non-negative, x P (x) ln[P (x)/P (x)] ≥ 0, the global maximum of x P (x) ln P (x) as a function of p(s|s ) and π (x|s) is reached for p(s|s ) = p(s|s ) and π (x|s) = π(x|s). [sent-85, score-0.33]
</p><p>26 2  Viterbi Training  An alternative approach to the parameter learning employs the maximal a posteriori (MAP) estimation and proceeds as follows: Instead of maximizing the likelihood of observed data (5) one tries to ˆ maximize the probability of the most likely sequence [20, 9]. [sent-88, score-0.418]
</p><p>27 Given the joint probability P (s, x) at trial values of parameters, and given the observed sequence x, one estimates the generating statesequence s via maximizing the a posteriori probability ˆ ˆ ˆ P (s|x) = P (s, x)/P (x)  (6)  ˆ ˆ over s. [sent-89, score-0.443]
</p><p>28 Since P (x) does not depend on s, one can maximize ln P (s, x). [sent-90, score-0.322]
</p><p>29 If the number of obserˆ vations is sufﬁciently large N → ∞, one can substitute maxs ln P (s, x) by its average over P (. [sent-91, score-0.312]
</p><p>30 ) [see (5)] and instead maximize (over model parameters) x  ˆ P (x) maxs ln P (s, x). [sent-94, score-0.366]
</p><p>31 (7)  To relate (7) to the free energy concept (see e. [sent-95, score-0.201]
</p><p>32 As a function of s (and for a ﬁxed x), ρβ→∞ (s|x) concentrates on ˆ ˆ (s, x): those s that maximize ln P ρβ→∞ (s|x) → ˆ  1 N  j  δ[s, s[j] (x)],  (9)  where δ(s, s ) is the Kronecker delta, s[j] (x) are equivalent outcomes of the maximization, and N is the number of such outcomes. [sent-98, score-0.322]
</p><p>33 8 and 10 refer to, respectively, the Gibbs distribution and free energy of a physical system with Hamiltonian H = − ln P (s, x) coupled to a thermal bath at inverse temperature β = 1/T [2, 4]. [sent-101, score-0.504]
</p><p>34 It is then clear that ML and Viterbi Training correspond to minimizing the free energy Eq. [sent-102, score-0.248]
</p><p>35 Note that β 2 ∂β F = − x P (x) s ρβ (s|x) ln ρβ (s|x) ≥ 0, which yields F1 ≤ F∞ . [sent-104, score-0.268]
</p><p>36 The resulting estimates of the parameters are now employed as new trial parameters and the previous step is repeated. [sent-108, score-0.303]
</p><p>37 3  For our purposes, this procedure can be understood as calculating certain statistics of the hidden sequence averaged over the Gibbs distribution Eqs. [sent-110, score-0.277]
</p><p>38 Indeed, let us introduce fγ (s) ≡ PN eβγ i=1 δ(si+1 ,a)δ(si ,b) and deﬁne ˆ βFβ (γ) ≡ − P (x) ln P β (s, x)fγ (s). [sent-112, score-0.268]
</p><p>39 (11) x  s  Then, for instance, the (iterative) Viterbi estimate of the transition probabilities are given as follows: P (Sk+1 = a, Sk = b) = −∂γ [F∞ (γ)]|γ→0 . [sent-113, score-0.153]
</p><p>40 For a large number of multipliers the behavior of such products is governed by the multiplicative law of large numbers. [sent-116, score-0.111]
</p><p>41 We now recall its formulation from [10]: for N → ∞ and x generated by the mixing process X there is a probability-one convergence: 1 1 ln ||T(x)|| → P (y) ln λ[T(y)], (13) y N N where ||. [sent-117, score-0.627]
</p><p>42 Altogether, we calculate (5) via its probability-one limit 1 1 ˆ ˆ P (x) ln P (x) → λ[T(x)] ln λ[T(x)]. [sent-124, score-0.536]
</p><p>43 (14) x x N N Note that the multiplicative law of large numbers is normally formulated for the maximal singular value. [sent-125, score-0.152]
</p><p>44 Recalling that Λ(0) = 1 and taking n → 0 in 0 = d 1 dn ξ( Λ(n) , n), we get from (16) 1 ∂n ξ(1, 0) ˆ λ[T(x)] ln λ[T(x)] = . [sent-131, score-0.312]
</p><p>45 4  2  p1 q1  1  q2  1  2  r2  p2 r1  3  Figure 1: The hidden Markov process (21–22) for = 0. [sent-134, score-0.206]
</p><p>46 1  Hidden Markov Model with One Unambiguous Symbol Deﬁnition  Given a L-state Markov process S, the observed process X has two states 1 and 2; see Fig. [sent-138, score-0.131]
</p><p>47 All internal states besides one are observed as 2, while the internal state 1 produces, respectively, 1 and 2 with probabilities 1 − and . [sent-140, score-0.202]
</p><p>48 The simplest example of such HMM exists already for L = 2; see [12] for analytical features of entropy for this case. [sent-144, score-0.154]
</p><p>49 The transition matrix (1) of a general L = 3 Markov process reads P ≡ { p(s|s ) }3 =1 = s,s  p0 p1 p2  q1 q0 q2  r1 r2 r0  p0 q0 r0  ,  =  1 − p1 − p2 1 − r1 − r2 1 − r1 − r2  (21)  where, e. [sent-146, score-0.175]
</p><p>50 For = 0 we get from (22) the simplest example of an aggregated HMM, where several Markov states are mapped into one observed state. [sent-154, score-0.133]
</p><p>51 5  We get from (23, 19): ξ(1, n) = (1 − τ n τ ) 1 − ˆ  ∞ k=0  ˆk tn tk ,  ∞ ˆ k=0 tk ln[tk ] . [sent-168, score-0.446]
</p><p>52 ∞ k=0 (k + 1)tk  ∂n ξ(1, 0) = ∂z ξ(1, 0)  (26) (27)  Note that for = 0, tk are return probabilities to the state 1 of the L-state Markov process. [sent-169, score-0.256]
</p><p>53 For ∞ > 0 this interpretation does not hold, but tk still has a meaning of probability as k=0 tk = 1. [sent-170, score-0.342]
</p><p>54 Turning to equations (19, 27) for the free energy, we note that as a function of trial values it depends on the following 2L parameters: ˆ (ˆ1 , . [sent-171, score-0.271]
</p><p>55 τ ˆ ˆ  (28)  As a function of the true values, the free energy depends on the same 2L parameters (28) [without hats], though concrete dependencies are different. [sent-178, score-0.266]
</p><p>56 For the studied class of HMM there are at most L(L − 1) + 1 unknown parameters: L(L − 1) transition probabilities of the unobserved Markov chain, and one parameter coming from observations. [sent-179, score-0.194]
</p><p>57 We checked numerically that the Jacobian of the transformation from the unknown parameters to the parameters (28) has rank 2L − 1. [sent-180, score-0.209]
</p><p>58 For L > 2 the number of effective independent parameters that affect the free energy is smaller than the number of parameters. [sent-182, score-0.313]
</p><p>59 (30–32) with obvious changes si → si for every symbol si hold for tk , τk and ψk . [sent-188, score-0.731]
</p><p>60 (27) and (30–32) imply  ∞ k=0 (k  + 1)tk =  µ 1−τ 2 ,  µ ≡ 1 − τ 2 + t2 + (1 − t0 )(1 + τ 2 ) > 0, (34) µF1 ˆ ˆ ˆ − = t1 ln t1 + t2 ln t2 + (1 − τ 2 )t0 ln t0 + (1 − t0 )τ 2 ln τ 2 . [sent-192, score-1.072]
</p><p>61 ˆ (35) N ˆ ˆ ˆ The free energy F1 depends on three independent parameters t0 , t1 , t2 [recall (33)]. [sent-193, score-0.266]
</p><p>62 Hence, minimizˆi = ti (i = 0, 1, 2), but we do not obtain a deﬁnite solution for the unknown parameing F1 we get t ters: any four numbers p1 , p2 , q1 , r1 satisfying three equations t0 = 1 − p1 − p2 , t1 = p1 q1 + p2 r1 , ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ t2 = p1 r1 (1 − q1 ) + p2 q1 (1 − r1 ), minimize F1 . [sent-194, score-0.167]
</p><p>63 Hence, as seen from p k=0 q k=1 r k=1 βF (20), the free energy at β > 0 is recovered from (35) by equating its LHS to − Nβ and by taking in 6  ˆ ˆ1 ˆβ ˆβ ˆ2 ˆβ ˆβ ˆ1 ˆβ ˆ2 ˆβ ˆ ˆβ ˆβ ˆ its RHS: t0 → pβ , τ 2 → q2 r2 , t1 → pβ q1 + pβ r1 , t2 → pβ r1 q2 + pβ q1 r2 . [sent-197, score-0.201]
</p><p>64 The zero-temperature ˆ0 ˆ free energy reads from (35) µF∞ ˆ − = (1 − τ 2 )t0 ln t0 + (1 − t0 )τ 2 ln τ 2 + t1 ln max[ˆ1 q1 , p2 r1 ] ˆ p ˆ ˆ ˆ N + t2 ln max[ˆ2 q1 r2 , p1 r1 q2 ]. [sent-198, score-1.339]
</p><p>65 p ˆˆ ˆ ˆ ˆ (36) We now minimize F∞ over the trial parameters p1 , p2 , q1 , r1 . [sent-199, score-0.238]
</p><p>66 Minimizing F∞ over the trial parameters produces four distinct solutions: {ˆi }4 = {ˆ1 = 0, p2 = 0, q1 = 0, r1 = 0}. [sent-204, score-0.275]
</p><p>67 σ i=1 p ˆ ˆ ˆ  (37)  ˆ For each of these four solutions: ti = ti (i = 0, 1, 2) and F1 = F∞ . [sent-205, score-0.164]
</p><p>68 The easiest way to get these ˆ results is to minimize F∞ under conditions ti = ti (for i = 0, 1, 2), obtain F1 = F∞ and then to conclude that due to the inequality F1 ≤ F∞ the conditional minimization led to the global minimization. [sent-206, score-0.208]
</p><p>69 The logics of (37) is that the unambiguous state tends to get detached from the ambiguous ones, since the probabilities nullifying in (37) refer to transitions from or to the unambiguous state. [sent-207, score-0.498]
</p><p>70 Note that although minimizing either F∞ and F1 produces correct values of the independent variables t0 , t1 , t2 , in the present situation minimizing F∞ is preferable, because it leads to the four-fold degenerate set of solutions (37) instead of the continuously degenerate set. [sent-208, score-0.487]
</p><p>71 For instance, if the solution with p1 = 0 is chosen we get for other parameters ˆ t2 t1 p 2 = 1 − t0 , q 1 = ˆ ˆ , r1 = ˆ . [sent-209, score-0.109]
</p><p>72 (38) 1 − t0 − t1 1 − t0 Furthermore, a more elaborate analysis reveals that for each ﬁxed set of correct parameters only one among the four solutions Eq. [sent-210, score-0.155]
</p><p>73 Finally, we note that minimizing F∞ allows one to get the correct values t0 , t1 , t2 of the independent ˆ ˆ ˆ variables t0 , t1 and t2 only if their number is less than the number of unknown parameters. [sent-214, score-0.168]
</p><p>74 This is not a drawback, since once the number of unknown parameters is sufﬁciently small [less than four for the present case (29)] their exact values are obtained by minimizing F1 . [sent-215, score-0.153]
</p><p>75 These effective parameters are ˆ ˆ ˆ recovered, because they do not depend on the known parameter r1 = r1 . [sent-221, score-0.112]
</p><p>76 In contrast, if p1 = p1 (or p2 = p2 ) is known exactly, there are three local ˆ ˆ minima again—ˆ2 = 0, q1 = 0, r1 = 0—but now none of effective parameters is equal to its true p ˆ ˆ ˆ value: ti = ti (i = 0, 1, 2). [sent-224, score-0.32]
</p><p>77 ˆ ˆ ˆ  The EM approach amounts to  starting with some trial values p1 , p2 , q1 , r1 and using p1 , p2 , q1 , r1 as new trial parameters (and ˆ ˆ ˆ ˆ so on). [sent-229, score-0.411]
</p><p>78 We see from (39–41) that the algorithm converges just in one step: (39–41) are equal to the parameters given by one of four solutions (37)—which one among the solutions (37) is selected depends on the on initial trial parameters in (39–41)—recovering the correct effective parameters (30–32); e. [sent-230, score-0.601]
</p><p>79 Further research should show whether our situation is similar: the VT works just in one step for this exactly solvable HMM model that belongs to a class of models, where VT generally performs faster than ML. [sent-238, score-0.156]
</p><p>80 We conclude this section by noting that the solvable case (29) is generic: its key results extend to the general situation deﬁned above (21). [sent-239, score-0.118]
</p><p>81 In particular, the minimization of F∞ nulliﬁes as many trial parameters as necessary to express the remaining parameters via independent effective parameters t0 , t1 , . [sent-241, score-0.415]
</p><p>82 Hence for L = 3 and = 0 two such trial parameters are nulliﬁed; cf. [sent-245, score-0.238]
</p><p>83 If the true error probability = 0, the trial value ˆ is among the nulliﬁed parameters. [sent-247, score-0.173]
</p><p>84 Again, there is a discrete degeneracy in solutions provided by minimizing F∞ . [sent-248, score-0.15]
</p><p>85 7  Summary  We presented a method for analyzing two basic techniques for parameter estimation in HMMs, and illustrated it on a speciﬁc class of HMMs with one unambiguous symbol. [sent-249, score-0.203]
</p><p>86 This is a rare occasion, because characteristics of HMM such as likelihood or entropy are notoriously difﬁcult to calculate explicitly [1]. [sent-251, score-0.126]
</p><p>87 An important feature of the example considered here is that the set of unknown parameters is not completely identiﬁable in the maximum likelihood sense [7, 14]. [sent-252, score-0.17]
</p><p>88 One of our main result is that in contrast to the ML approach that produces continuously degenerate solutions, VT results in ﬁnitely degenerate solution that is sparse, i. [sent-257, score-0.263]
</p><p>89 , some [non-identiﬁable] parameters are set to zero, and, furthermore, converges faster. [sent-259, score-0.107]
</p><p>90 For instance, imposing sparsity on conventional EM-type learning has been shown to produce better results part of speech tagging applications [25]. [sent-261, score-0.158]
</p><p>91 Whereas [25] had to impose sparsity via an additional penalty term in the objective function, in our case sparsity is a natural outcome of maximizing the likelihood of the best sequence. [sent-262, score-0.176]
</p><p>92 The fact that VT provides simpler and more deﬁnite solutions—among all choices of the parameters compatible with the observed data—can be viewed as a type of the Occam’s razor for the parameter learning. [sent-264, score-0.221]
</p><p>93 Note ﬁnally that statistical mechanics intuition behind these results is that the aposteriori likelihood is (negative) zero-temperature free energy of a certain physical system. [sent-265, score-0.356]
</p><p>94 Minimizing this free energy makes physical sense: this is the premise of the second law of thermodynamics that ensures relaxation towards a more equilibrium state. [sent-266, score-0.397]
</p><p>95 In that zero-temperature equilibrium state certain types of motion are frozen, which means nullifying the corresponding transition probabilities. [sent-267, score-0.164]
</p><p>96 Marcus, Analyticity of entropy rate of hidden Markov chains, IEEE Trans. [sent-337, score-0.227]
</p><p>97 Rabiner, The segmental k-means algorithm for estimating parameters of hidden Markov models, IEEE Trans. [sent-360, score-0.322]
</p><p>98 Ephraim, Maximum likelihood hidden Markov modeling using a dominant sequence of states, IEEE Transactions on Signal Processing, vol. [sent-369, score-0.276]
</p><p>99 Qin, Restoration of single-channel currents using the segmental k-means method based on hidden Markov modeling, Biophys J 86, 14881501 (2004). [sent-374, score-0.257]
</p><p>100 Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proc. [sent-377, score-0.224]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vt', 0.327), ('viterbi', 0.285), ('ln', 0.268), ('hmm', 0.23), ('trial', 0.173), ('tk', 0.171), ('sk', 0.171), ('hidden', 0.165), ('markov', 0.162), ('ml', 0.161), ('si', 0.16), ('unambiguous', 0.154), ('occam', 0.134), ('pst', 0.123), ('razor', 0.107), ('tsi', 0.107), ('energy', 0.103), ('free', 0.098), ('hmms', 0.097), ('degenerate', 0.094), ('allahverdyan', 0.092), ('galstyan', 0.092), ('nulli', 0.092), ('segmental', 0.092), ('probabilities', 0.085), ('ti', 0.082), ('solvable', 0.078), ('em', 0.077), ('law', 0.077), ('baum', 0.069), ('transition', 0.068), ('grammars', 0.066), ('reads', 0.066), ('parameters', 0.065), ('calculating', 0.065), ('likelihood', 0.064), ('entropy', 0.062), ('benedi', 0.061), ('ephraim', 0.061), ('merhav', 0.061), ('nullifying', 0.061), ('tab', 0.061), ('yerevan', 0.061), ('tn', 0.06), ('generating', 0.06), ('speech', 0.059), ('posteriori', 0.058), ('mechanics', 0.056), ('objective', 0.056), ('maximizing', 0.056), ('maximize', 0.054), ('solutions', 0.054), ('conventional', 0.053), ('analytical', 0.052), ('mixing', 0.05), ('chains', 0.049), ('sanchez', 0.049), ('degeneracy', 0.049), ('rabiner', 0.049), ('thermodynamics', 0.049), ('estimation', 0.049), ('observed', 0.049), ('sequence', 0.047), ('minimizing', 0.047), ('effective', 0.047), ('tagging', 0.046), ('realization', 0.045), ('recovering', 0.044), ('symbol', 0.044), ('get', 0.044), ('maxs', 0.044), ('comparative', 0.044), ('minima', 0.044), ('converges', 0.042), ('maximal', 0.041), ('process', 0.041), ('unknown', 0.041), ('simplest', 0.04), ('situation', 0.04), ('maximization', 0.04), ('identi', 0.039), ('zm', 0.039), ('calculates', 0.039), ('ss', 0.039), ('continuously', 0.038), ('asymptotic', 0.038), ('faster', 0.038), ('realizations', 0.038), ('checked', 0.038), ('produces', 0.037), ('acl', 0.036), ('correct', 0.036), ('obvious', 0.036), ('physical', 0.035), ('equilibrium', 0.035), ('eigenvalue', 0.034), ('multiplicative', 0.034), ('map', 0.034), ('biology', 0.034), ('internal', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="57-tfidf-1" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>2 0.17738266 <a title="57-tfidf-2" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>3 0.1575152 <a title="57-tfidf-3" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>Author: Yoshinobu Kawahara, Takashi Washio</p><p>Abstract: In this paper, we propose the ﬁrst exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning. In fact, this generalizes any set-function optimization. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.</p><p>4 0.13952011 <a title="57-tfidf-4" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>Author: Matthew D. Zeiler, Graham W. Taylor, Leonid Sigal, Iain Matthews, Rob Fergus</p><p>Abstract: We present a type of Temporal Restricted Boltzmann Machine that deﬁnes a probability distribution over an output sequence conditional on an input sequence. It shares the desirable properties of RBMs: efﬁcient exact inference, an exponentially more expressive latent state than HMMs, and the ability to model nonlinear structure and dynamics. We apply our model to a challenging real-world graphics problem: facial expression transfer. Our results demonstrate improved performance over several baselines modeling high-dimensional 2D and 3D data. 1</p><p>5 0.13393596 <a title="57-tfidf-5" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>Author: Yevgeny Seldin, Peter Auer, John S. Shawe-taylor, Ronald Ortner, François Laviolette</p><p>Abstract: We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The p scaling of our regret bound with the number of states (contexts) N goes as N I⇢t (S; A), where I⇢t (S; A) is the mutual information between states and actions (the side information) used by the algorithm at round t. If the algorithm p uses all the side information, the regret bound scales as N ln K, where K is the number of actions (arms). However, if the side information I⇢t (S; A) is not fully used, the regret bound is signiﬁcantly tighter. In the extreme case, when I⇢t (S; A) = 0, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round. 1</p><p>6 0.11844693 <a title="57-tfidf-6" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>7 0.10229164 <a title="57-tfidf-7" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>8 0.10001283 <a title="57-tfidf-8" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>9 0.099843934 <a title="57-tfidf-9" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>10 0.095641248 <a title="57-tfidf-10" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>11 0.091600202 <a title="57-tfidf-11" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>12 0.090202771 <a title="57-tfidf-12" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>13 0.087665267 <a title="57-tfidf-13" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>14 0.087207615 <a title="57-tfidf-14" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>15 0.082577504 <a title="57-tfidf-15" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>16 0.082180843 <a title="57-tfidf-16" href="./nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</a></p>
<p>17 0.080220304 <a title="57-tfidf-17" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>18 0.079142503 <a title="57-tfidf-18" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>19 0.07813599 <a title="57-tfidf-19" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>20 0.076594435 <a title="57-tfidf-20" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, -0.052), (2, 0.048), (3, -0.027), (4, -0.012), (5, -0.051), (6, -0.054), (7, -0.017), (8, 0.002), (9, -0.061), (10, -0.03), (11, -0.164), (12, 0.084), (13, -0.051), (14, -0.066), (15, -0.138), (16, -0.062), (17, -0.061), (18, 0.035), (19, 0.114), (20, 0.093), (21, 0.04), (22, 0.057), (23, 0.125), (24, 0.011), (25, 0.133), (26, 0.028), (27, -0.022), (28, 0.077), (29, -0.131), (30, -0.272), (31, 0.122), (32, 0.036), (33, 0.019), (34, 0.012), (35, -0.03), (36, -0.033), (37, -0.078), (38, 0.029), (39, 0.024), (40, 0.095), (41, -0.16), (42, -0.048), (43, 0.094), (44, -0.034), (45, 0.069), (46, -0.07), (47, -0.034), (48, 0.023), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96329546 <a title="57-lsi-1" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>2 0.5828613 <a title="57-lsi-2" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>3 0.57531029 <a title="57-lsi-3" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>Author: Dong Dai, Tong Zhang</p><p>Abstract: This paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression. It is known that if the models are mis-speciﬁed, model averaging is superior to model selection. Speciﬁcally, let n be the sample size, then the worst case regret of the former decays at the rate of O(1/n) while the worst √ case regret of the latter decays at the rate of O(1/ n). In the literature, the most important and widely studied model averaging method that achieves the optimal O(1/n) average regret is the exponential weighted model averaging (EWMA) algorithm. However this method suffers from several limitations. The purpose of this paper is to present a new greedy model averaging procedure that improves EWMA. We prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples. 1</p><p>4 0.54627335 <a title="57-lsi-4" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>Author: Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman</p><p>Abstract: We propose a novel Adaptive Markov Chain Monte Carlo algorithm to compute the partition function. In particular, we show how to accelerate a ﬂat histogram sampling technique by signiﬁcantly reducing the number of “null moves” in the chain, while maintaining asymptotic convergence properties. Our experiments show that our method converges quickly to highly accurate solutions on a range of benchmark instances, outperforming other state-of-the-art methods such as IJGP, TRW, and Gibbs sampling both in run-time and accuracy. We also show how obtaining a so-called density of states distribution allows for efﬁcient weight learning in Markov Logic theories. 1</p><p>5 0.52681321 <a title="57-lsi-5" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>Author: Ardavan Saeedi, Alexandre Bouchard-côté</p><p>Abstract: We introduce the Gamma-Exponential Process (GEP), a prior over a large family of continuous time stochastic processes. A hierarchical version of this prior (HGEP; the Hierarchical GEP) yields a useful model for analyzing complex time series. Models based on HGEPs display many attractive properties: conjugacy, exchangeability and closed-form predictive distribution for the waiting times, and exact Gibbs updates for the time scale parameters. After establishing these properties, we show how posterior inference can be carried efﬁciently using Particle MCMC methods [1]. This yields a MCMC algorithm that can resample entire sequences atomically while avoiding the complications of introducing slice and stick auxiliary variables of the beam sampler [2]. We applied our model to the problem of estimating the disease progression in multiple sclerosis [3], and to RNA evolutionary modeling [4]. In both domains, we found that our model outperformed the standard rate matrix estimation approach. 1</p><p>6 0.52619815 <a title="57-lsi-6" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>7 0.5115031 <a title="57-lsi-7" href="./nips-2011-Quasi-Newton_Methods_for_Markov_Chain_Monte_Carlo.html">228 nips-2011-Quasi-Newton Methods for Markov Chain Monte Carlo</a></p>
<p>8 0.48285881 <a title="57-lsi-8" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>9 0.47874686 <a title="57-lsi-9" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>10 0.46713117 <a title="57-lsi-10" href="./nips-2011-A_Global_Structural_EM_Algorithm_for_a_Model_of_Cancer_Progression.html">6 nips-2011-A Global Structural EM Algorithm for a Model of Cancer Progression</a></p>
<p>11 0.46453622 <a title="57-lsi-11" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>12 0.46428227 <a title="57-lsi-12" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>13 0.46054724 <a title="57-lsi-13" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>14 0.45268533 <a title="57-lsi-14" href="./nips-2011-Modelling_Genetic_Variations_using_Fragmentation-Coagulation_Processes.html">173 nips-2011-Modelling Genetic Variations using Fragmentation-Coagulation Processes</a></p>
<p>15 0.45264387 <a title="57-lsi-15" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>16 0.43125409 <a title="57-lsi-16" href="./nips-2011-Simultaneous_Sampling_and_Multi-Structure_Fitting_with_Adaptive_Reversible_Jump_MCMC.html">255 nips-2011-Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC</a></p>
<p>17 0.42159688 <a title="57-lsi-17" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>18 0.41727972 <a title="57-lsi-18" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>19 0.40862834 <a title="57-lsi-19" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>20 0.40769732 <a title="57-lsi-20" href="./nips-2011-Collective_Graphical_Models.html">55 nips-2011-Collective Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.029), (4, 0.043), (20, 0.054), (26, 0.03), (31, 0.142), (33, 0.027), (43, 0.064), (45, 0.078), (57, 0.052), (74, 0.091), (83, 0.047), (84, 0.015), (86, 0.209), (99, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89484459 <a title="57-lda-1" href="./nips-2011-Autonomous_Learning_of_Action_Models_for_Planning.html">41 nips-2011-Autonomous Learning of Action Models for Planning</a></p>
<p>Author: Neville Mehta, Prasad Tadepalli, Alan Fern</p><p>Abstract: This paper introduces two new frameworks for learning action models for planning. In the mistake-bounded planning framework, the learner has access to a planner for the given model representation, a simulator, and a planning problem generator, and aims to learn a model with at most a polynomial number of faulty plans. In the planned exploration framework, the learner does not have access to a problem generator and must instead design its own problems, plan for them, and converge with at most a polynomial number of planning attempts. The paper reduces learning in these frameworks to concept learning with one-sided error and provides algorithms for successful learning in both frameworks. A speciﬁc family of hypothesis spaces is shown to be efﬁciently learnable in both the frameworks. 1</p><p>same-paper 2 0.83415771 <a title="57-lda-2" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>3 0.77928978 <a title="57-lda-3" href="./nips-2011-Distributed_Delayed_Stochastic_Optimization.html">72 nips-2011-Distributed Delayed Stochastic Optimization</a></p>
<p>Author: Alekh Agarwal, John C. Duchi</p><p>Abstract: We analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information. The main application of our results is to the development of distributed minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to asynchrony. Our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible. In application to distributed optimization, we show n-node architectures whose optimization error in stochastic problems—in √ spite of asynchronous delays—scales asymptotically as O(1/ nT ), which is known to be optimal even in the absence of delays. 1</p><p>4 0.68840724 <a title="57-lda-4" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>Author: Biljana Petreska, Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: Simultaneous recordings of many neurons embedded within a recurrentlyconnected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function. In principle, these dynamics might be identiﬁed by purely unsupervised, statistical means. Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model— in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process—is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements. The regimes are identiﬁed without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial. The HSLDS model also performs better than recent comparable models in predicting the ﬁring rate of an isolated neuron based on the ﬁring rates of others, suggesting that it captures more of the “shared variance” of the data. Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reﬂect its computational role. 1</p><p>5 0.68404758 <a title="57-lda-5" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>Author: Michael L. Wick, Andrew McCallum</p><p>Abstract: Traditional approaches to probabilistic inference such as loopy belief propagation and Gibbs sampling typically compute marginals for all the unobserved variables in a graphical model. However, in many real-world applications the user’s interests are focused on a subset of the variables, speciﬁed by a query. In this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten. In this paper we propose a query-speciﬁc approach to MCMC that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efﬁciency. Surprisingly there has been almost no previous work on query-aware MCMC. We demonstrate the success of our approach with positive experimental results on a wide range of graphical models. 1</p><p>6 0.68118608 <a title="57-lda-6" href="./nips-2011-Demixed_Principal_Component_Analysis.html">68 nips-2011-Demixed Principal Component Analysis</a></p>
<p>7 0.67935085 <a title="57-lda-7" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>8 0.6791451 <a title="57-lda-8" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>9 0.67746812 <a title="57-lda-9" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>10 0.67600119 <a title="57-lda-10" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>11 0.67588717 <a title="57-lda-11" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>12 0.67587388 <a title="57-lda-12" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>13 0.67564213 <a title="57-lda-13" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<p>14 0.67553926 <a title="57-lda-14" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>15 0.6741128 <a title="57-lda-15" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>16 0.67334455 <a title="57-lda-16" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>17 0.67192835 <a title="57-lda-17" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>18 0.67011648 <a title="57-lda-18" href="./nips-2011-Collective_Graphical_Models.html">55 nips-2011-Collective Graphical Models</a></p>
<p>19 0.66863948 <a title="57-lda-19" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>20 0.66852146 <a title="57-lda-20" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
