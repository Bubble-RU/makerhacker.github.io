<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>215 nips-2011-Policy Gradient Coagent Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-215" href="#">nips2011-215</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>215 nips-2011-Policy Gradient Coagent Networks</h1>
<br/><p>Source: <a title="nips-2011-215-pdf" href="http://papers.nips.cc/paper/4449-policy-gradient-coagent-networks.pdf">pdf</a></p><p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>Reference: <a title="nips-2011-215-reference" href="../nips2011_reference/nips-2011-Policy_Gradient_Coagent_Networks_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.473), ('vcn', 0.43), ('ai', 0.272), ('coag', 0.248), ('jm', 0.246), ('comdp', 0.208), ('vac', 0.192), ('grady', 0.171), ('st', 0.158), ('modul', 0.154), ('td', 0.142), ('ncn', 0.134), ('vm', 0.133), ('reward', 0.124), ('vanill', 0.117), ('nac', 0.108), ('rt', 0.1), ('broadcast', 0.096), ('pgcn', 0.089), ('mod', 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="215-tfidf-1" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>2 0.42434558 <a title="215-tfidf-2" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>3 0.25200549 <a title="215-tfidf-3" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>4 0.25173911 <a title="215-tfidf-4" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>5 0.22961318 <a title="215-tfidf-5" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>6 0.21104252 <a title="215-tfidf-6" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>7 0.21084604 <a title="215-tfidf-7" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>8 0.20224795 <a title="215-tfidf-8" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>9 0.19551557 <a title="215-tfidf-9" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>10 0.17213137 <a title="215-tfidf-10" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>11 0.17169917 <a title="215-tfidf-11" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>12 0.15814409 <a title="215-tfidf-12" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>13 0.15622802 <a title="215-tfidf-13" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>14 0.14327395 <a title="215-tfidf-14" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>15 0.14180587 <a title="215-tfidf-15" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>16 0.13083474 <a title="215-tfidf-16" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>17 0.1275429 <a title="215-tfidf-17" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>18 0.11735562 <a title="215-tfidf-18" href="./nips-2011-Modelling_Genetic_Variations_using_Fragmentation-Coagulation_Processes.html">173 nips-2011-Modelling Genetic Variations using Fragmentation-Coagulation Processes</a></p>
<p>19 0.10003074 <a title="215-tfidf-19" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>20 0.098968707 <a title="215-tfidf-20" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, -0.127), (2, -0.266), (3, -0.36), (4, 0.28), (5, -0.045), (6, -0.052), (7, -0.064), (8, 0.11), (9, -0.09), (10, 0.008), (11, -0.056), (12, 0.026), (13, 0.015), (14, 0.028), (15, 0.052), (16, -0.014), (17, 0.059), (18, 0.046), (19, 0.09), (20, -0.023), (21, -0.012), (22, -0.09), (23, -0.086), (24, 0.044), (25, 0.021), (26, -0.058), (27, 0.025), (28, 0.001), (29, -0.041), (30, -0.018), (31, -0.028), (32, 0.002), (33, -0.018), (34, 0.091), (35, 0.074), (36, -0.031), (37, -0.015), (38, -0.037), (39, -0.054), (40, -0.062), (41, 0.042), (42, 0.031), (43, -0.074), (44, 0.025), (45, 0.017), (46, -0.041), (47, -0.02), (48, -0.019), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96060091 <a title="215-lsi-1" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>2 0.89903706 <a title="215-lsi-2" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>3 0.86154288 <a title="215-lsi-3" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>4 0.8192066 <a title="215-lsi-4" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>5 0.78706753 <a title="215-lsi-5" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>6 0.75812602 <a title="215-lsi-6" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>7 0.75223511 <a title="215-lsi-7" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>8 0.73880255 <a title="215-lsi-8" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>9 0.66582656 <a title="215-lsi-9" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>10 0.61076307 <a title="215-lsi-10" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>11 0.61017424 <a title="215-lsi-11" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>12 0.53697819 <a title="215-lsi-12" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>13 0.52652264 <a title="215-lsi-13" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>14 0.51923412 <a title="215-lsi-14" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>15 0.50726086 <a title="215-lsi-15" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>16 0.49507526 <a title="215-lsi-16" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>17 0.47414142 <a title="215-lsi-17" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>18 0.45296228 <a title="215-lsi-18" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>19 0.44916439 <a title="215-lsi-19" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>20 0.41601577 <a title="215-lsi-20" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.036), (22, 0.015), (36, 0.017), (55, 0.658), (65, 0.049), (68, 0.1), (79, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98694009 <a title="215-lda-1" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>2 0.97979343 <a title="215-lda-2" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>3 0.9732638 <a title="215-lda-3" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>4 0.96846038 <a title="215-lda-4" href="./nips-2011-Complexity_of_Inference_in_Latent_Dirichlet_Allocation.html">58 nips-2011-Complexity of Inference in Latent Dirichlet Allocation</a></p>
<p>same-paper 5 0.96197617 <a title="215-lda-5" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>6 0.94879329 <a title="215-lda-6" href="./nips-2011-An_Application_of_Tree-Structured_Expectation_Propagation_for_Channel_Decoding.html">31 nips-2011-An Application of Tree-Structured Expectation Propagation for Channel Decoding</a></p>
<p>7 0.93616861 <a title="215-lda-7" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>8 0.92783403 <a title="215-lda-8" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>9 0.9251554 <a title="215-lda-9" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>10 0.90999293 <a title="215-lda-10" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>11 0.90781182 <a title="215-lda-11" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>12 0.89866018 <a title="215-lda-12" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>13 0.88889194 <a title="215-lda-13" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>14 0.88854432 <a title="215-lda-14" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>15 0.88439155 <a title="215-lda-15" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>16 0.87624627 <a title="215-lda-16" href="./nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</a></p>
<p>17 0.87096953 <a title="215-lda-17" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>18 0.86667144 <a title="215-lda-18" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>19 0.86482924 <a title="215-lda-19" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>20 0.86103189 <a title="215-lda-20" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
