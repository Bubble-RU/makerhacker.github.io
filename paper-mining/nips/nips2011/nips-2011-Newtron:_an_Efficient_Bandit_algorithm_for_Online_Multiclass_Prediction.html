<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-185" href="#">nips2011-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</h1>
<br/><p>Source: <a title="nips-2011-185-pdf" href="http://papers.nips.cc/paper/4245-newtron-an-efficient-bandit-algorithm-for-online-multiclass-prediction.pdf">pdf</a></p><p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>Reference: <a title="nips-2011-185-reference" href="../nips2011_reference/nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. [sent-6, score-0.381]
</p><p>2 We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. [sent-7, score-0.267]
</p><p>3 We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . [sent-8, score-0.267]
</p><p>4 For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. [sent-9, score-0.287]
</p><p>5 Our algorithm is based on a novel application of the online Newton method [HAK07]. [sent-10, score-0.068]
</p><p>6 ) we only obtain limited feedback about the true label of the input (e. [sent-14, score-0.078]
</p><p>7 , in recommender systems, we only get feedback on the recommended items). [sent-16, score-0.065]
</p><p>8 Several such problems can be cast as online, bandit versions of multiclass prediction problems1 . [sent-17, score-0.214]
</p><p>9 In each round, the learner receives an input x in some high dimensional feature space (the “context”), and produces an action in response, and obtains an associated reward. [sent-19, score-0.064]
</p><p>10 The goal is to minimize regret with respect to a reference class of policies specifying actions for each context. [sent-20, score-0.25]
</p><p>11 In this paper, we consider the special case of multiclass prediction, which is a fundamental problem in this area introduced by Kakade et al [KSST08]. [sent-21, score-0.115]
</p><p>12 Here, a learner obtains a feature vector, which is associated with an unknown label y which can take one of k values. [sent-22, score-0.102]
</p><p>13 Then the learner produces a prediction of the label, y . [sent-23, score-0.071]
</p><p>14 The goal is to design an efﬁcient algorithm that minimizes regret with respect to a natural reference class of policies: linear predictors. [sent-25, score-0.266]
</p><p>15 Kakade et al [KSST08] gave an efﬁcient algorithm, dubbed BANDITRON. [sent-26, score-0.067]
</p><p>16 Their algorithm attains regret of O(T 2/3 ) for a natural multiclass hinge loss, and they ask the question whether a better regret bound is possible. [sent-27, score-0.664]
</p><p>17 While the EXP4 √ algorithm [ACBFS03], applied to this setting, has an O( T log T ) regret bound, it is highly inefﬁcient, requiring O(T n/2 ) time per iteration,√ where n is the dimension of the feature space. [sent-28, score-0.305]
</p><p>18 Ideally, one would like to match or improve the O( T log T ) regret bound of the EXP4 algorithm with an efﬁcient algorithm (for a suitable loss function). [sent-29, score-0.405]
</p><p>19 In COLT 2009, Abernethy and Rakhlin [AR09] formulated the open question precisely as minimizing regret for a suitable loss function in the fully 1  For the basic bandit classiﬁcation problem see [DHK07, RTB07, DH06, FKM05, AK08, MB04, AHR08]. [sent-31, score-0.461]
</p><p>20 In this paper we address this question and design a novel algorithm for the fully adversarial setting with its expected regret measured with respect to log-loss function deﬁned in [AR09], which is parameterized by a scalar α. [sent-36, score-0.358]
</p><p>21 When α is a constant independent of T , we get a much stronger guarantee than required by the open problem: the regret is bounded by O(log T ). [sent-37, score-0.304]
</p><p>22 In fact, the regret √ is bounded by O( T ) even for α = Θ(log T ). [sent-38, score-0.267]
</p><p>23 Our regret bound for larger values of α increases smoothly to a maximum of O(T 2/3 ), matching that of BANDITRON in the worst case. [sent-39, score-0.288]
</p><p>24 The algorithm is efﬁcient to implement, and it is based on the online Newton method introduced in [HAK07]; hence we call the new algorithm N EWTRON. [sent-40, score-0.084]
</p><p>25 We implement the algorithm (and a faster variant, PN EWTRON) and test it on the same data sets used by Kakade et al [KSST08]. [sent-41, score-0.058]
</p><p>26 For any Rn , let 1, 0 denote the all 1s and all 0s vectors respectively, and let I denote the identity matrix in Rn×n . [sent-48, score-0.054]
</p><p>27 For a matrix W, we denote by W the Frobenius norm of W, which is also the usual 2 norm of the vector form of W, and so the notation is consistent. [sent-64, score-0.09]
</p><p>28 For two square symmetric matrices W, V of like order, denote by W V the fact that W − V is positive semideﬁnite, i. [sent-69, score-0.055]
</p><p>29 A useful fact of the Kronecker product is the following: if W, V are symmetric matrices such that W V, and if U is a positive semideﬁnite symmetric matrix, then W ⊗U V ⊗U. [sent-72, score-0.06]
</p><p>30 , T , we are presented a feature vector xt ∈ X , where X ⊆ Rn , and x ≤ R for all x ∈ X . [sent-79, score-0.156]
</p><p>31 We are required to produce a prediction, yt ∈ [k], as the label ˆ of xt . [sent-82, score-0.423]
</p><p>32 In response, we obtain only 1 bit of information: whether yt = yt or not. [sent-83, score-0.458]
</p><p>33 In particular, when ˆ yt = yt , the identity of yt remains unknown (although one label, yt , is ruled out). [sent-84, score-0.916]
</p><p>34 ˆ ˆ The learner’s hypothesis class is parameterized by matrices W ∈ Rk×n with W ≤ D, for some speciﬁed constant D. [sent-85, score-0.054]
</p><p>35 , Wk , the prediction associated with W for xt is yt = arg max Wi · xt . [sent-90, score-0.592]
</p><p>36 ˆ i∈[k]  While ideally we would like to minimize the 0 − 1 loss suffered by the learner, for computational reasons it is preferable to consider convex loss functions. [sent-91, score-0.108]
</p><p>37 A natural choice used in Kakade et al [KSST08] is the multi-class hinge loss: (W, (xt , yt )) = max [1 − Wyt · xt + Wi · xt ]+ . [sent-92, score-0.604]
</p><p>38 T  T  (Wt , (xt , yt )) − min  Regret :=  W ∈K  t=1  (W , (xt , yt )). [sent-96, score-0.458]
</p><p>39 t=1  A different loss function was proposed in an open problem by Abernethy and Rakhlin in COLT 2009 [AR09]. [sent-97, score-0.066]
</p><p>40 We choose a constant α which parameterizes the loss function. [sent-99, score-0.063]
</p><p>41 Suppose we make our prediction yt by sampling from p. [sent-102, score-0.261]
</p><p>42 ˆ A natural loss function for this scheme is log-loss deﬁned as follows: (W, (x, y)) = −  exp(αWy · x) j exp(αWk · x)  1 1 log(py ) = − log α α  1 log j exp(αWj · x) . [sent-103, score-0.124]
</p><p>43 As α becomes large, this log-loss function has the property that when the prediction given by W for x is correct, it is very close to zero, and when the prediction is incorrect, it is roughly proportional to the margin of the incorrect prediction over the correct one. [sent-105, score-0.112]
</p><p>44 = −Wy · x +  The algorithm and its analysis depend upon the the gradient and Hessian of the loss function w. [sent-106, score-0.062]
</p><p>45 The following lemma derives these quantities (proof in full version). [sent-110, score-0.071]
</p><p>46 Then we have (W, (x, y)) = (p − ey ) ⊗ x and  2  (W, (x, y)) = α(diag(p) − pp ) ⊗ xx . [sent-114, score-0.092]
</p><p>47 4: Let pt = P(Wt , xt ), and set pt = (1 − γ) · pt + γ 1. [sent-129, score-0.582]
</p><p>48 ˆ 7: if yt = yt then ˆ 1 t (y 8: Deﬁne ˜ t := 1−p(yt )t ) · k 1 − eyt ⊗ xt and κt := pt (yt ). [sent-135, score-0.899]
</p><p>49 pt 9: else p (ˆt ) y 1 10: Deﬁne ˜ t := pt (ˆt ) · eyt − k 1 ⊗ xt and κt := 1. [sent-136, score-0.583]
</p><p>50 ˆ t y 11: end if 12: Deﬁne the cost function 1 ft (W) := ˜ t · (W − Wt ) + κt β( ˜ t · (W − Wt ))2 . [sent-137, score-0.124]
</p><p>51 (3) Wt+1 := arg min ft (W) + W∈K 2D τ =1 14: end for  2. [sent-139, score-0.123]
</p><p>52 This algorithm is an online version of the Newton step algorithm in ofﬂine optimization. [sent-141, score-0.084]
</p><p>53 The following lemma speciﬁes the algorithm, specialized to our setting, and gives its regret bound. [sent-142, score-0.321]
</p><p>54 Then the algorithm that, in round t, plays t−1  ft (w)  wt := arg min  w∈K  τ =1  2  has regret bounded by O( nb log( DraT )). [sent-146, score-0.725]
</p><p>55 a b  3  The N EWTRON algorithm  Our algorithm for bandit multiclass learning algorithm, dubbed N EWTRON, is shown as Algorithm 1 above. [sent-147, score-0.239]
</p><p>56 In each iteration, we randomly choose a label from the distribution speciﬁed by the current weight matrix on the current example mixed with the uniform distribution over labels speciﬁed by an exploration parameter γ. [sent-148, score-0.081]
</p><p>57 The parameter γ (which is similar to the exploration parameter used in the EXP3 algorithm of [ACBFS03]) is eventually tuned based on the value of the parameter α in the loss function (see Corollary 5). [sent-149, score-0.081]
</p><p>58 We then use the observed feedback to construct a quadratic loss function (which is strongly convex) that lower bounds the true loss function in expectation (see Lemma 7) and thus allows us to bound the regret. [sent-150, score-0.191]
</p><p>59 Furthermore, we also choose a parameter κt , which is an adjustment factor for the strongly convexity of the quadratic loss function ensuring that its expectation lower bounds the true loss function. [sent-152, score-0.113]
</p><p>60 Finally, we compute the new loss matrix using a Follow-The-Regularized-Leader strategy, by minimizing the sum of all quadratic loss functions so far with 2 regularization. [sent-153, score-0.137]
</p><p>61 As described in [HAK07], this convex program can be solved in quadratic time, plus a projection on K in the norm induced by the Hessian. [sent-154, score-0.055]
</p><p>62 To simplify notation, deﬁne the function t : K → R as t (W) = (W, (xt , yt )). [sent-156, score-0.229]
</p><p>63 With this notation, we can state our main theorem giving the regret bound: 1 Theorem 4. [sent-161, score-0.25]
</p><p>64 Given α, there is a setting of γ so that the regret of N EWTRON is bounded by min c  exp(4αRD) log(T ), α  6cRDT 2/3 ,  where the constant c = O(k 3 n) is independent of α. [sent-166, score-0.284]
</p><p>65 Our main result as given in Corollary 5 which entails logarithmic regret for constant α, contains a constant which depends exponentially on α. [sent-169, score-0.284]
</p><p>66 1 Note that even when α grows with T , as long as α ≤ 8RD log(T ), the regret can be bounded as √ O(cRD T ), thus solving the open problem of [KSST08, AR09] for log-loss functions with this range of α. [sent-171, score-0.287]
</p><p>67 We can say something even stronger - our results provide a “safety net” - no matter what the value of α is, the regret of our algorithm is never worse than O(T 2/3 ), matching the bound of the BAN DITRON algorithm (although the latter holds for the multiclass hinge loss). [sent-172, score-0.414]
</p><p>68 ) The optimization (3) is essentially running the algorithm from Lemma 3 on 1 K with the cost functions ft (W), with additional nk initial ﬁctitious cost functions 2D (Eil · W)2 for i ∈ [n] and l ∈ [k]. [sent-176, score-0.16]
</p><p>69 While technically these ﬁctitious cost functions are not necessary to prove our regret bound, we include them since this seems to give better experimental performance and only adds a constant to the regret. [sent-178, score-0.287]
</p><p>70 We now apply the regret bound of Lemma 3 by estimating the parameters r, a, b. [sent-179, score-0.288]
</p><p>71 ν Hence, the regret bound of Lemma 3 implies that for any W ∈ K, T  ft (Wt ) − ft (W ) = O  kn νβ  log T . [sent-181, score-0.569]
</p><p>72 t=1  Note that the bound above excludes the ﬁctitious cost functions since they only add a constant additive term to the regret, which is absorbed by the O(log T ) term. [sent-182, score-0.075]
</p><p>73 Similarly, we have also suppressed additive constants arising from the log( DraT ) term in the regret bound of Lemma 3. [sent-183, score-0.288]
</p><p>74 b Taking expectation on both sides of the above bound with respect to the randomness in the algorithm, and using the speciﬁcation (2) of ft (W) we get 1 2 E ˜ t · (Wt − W ) − κt β( ˜ t · (Wt − W )) 2 5  = O  kn νβ  log T . [sent-184, score-0.215]
</p><p>75 T  E[ (Wt )] − (W ) = O  kn νβ  log T +  γ log(k) T α  . [sent-187, score-0.073]
</p><p>76 The next lemma shows that in each round, the expected regret of the inner FTAL algorithm with ft cost functions is larger than the regret of N EWTRON. [sent-194, score-0.711]
</p><p>77 We show that Et [ ˜ t ] = (p − eyt ) ⊗ xt , ˜ ˜ which by Lemma 1 equals t (Wt ). [sent-199, score-0.299]
</p><p>78 Next, we show that Et [κt t t ] = Ht ⊗ xt xt for some matrix Ht s. [sent-200, score-0.336]
</p><p>79 By upper bounding Ht , we then show (using Lemma 2) that for any Ψ ∈ K we have 2 βHt ⊗ xt xt . [sent-203, score-0.312]
</p><p>80  1 − pt (yt ) · E[ ˜ t ] = pt (yt ) · pt (yt ) t  1 1 − eyt k  + y=yt  (7)   pt (y) 1  ⊗ xt pt (y) · · eyt − 1 ˆ pt (y) k  = (pt − eyt ) ⊗ xt . [sent-208, score-1.451]
</p><p>81 E[κt ˜ t ˜ t ] = pt (yt ) · κt t  pt (y) ·  + y=yt  1 − pt (yt ) pt (yt ) pt (y) pt (y)  2  ·  2  · ey −  =: Ht ⊗ xt xt ,  1 1 − eyt k 1 1 k  1 1 − eyt k  1 ey − 1  ⊗ xt xt k (10)  6  where Ht is the matrix in the brackets above. [sent-211, score-1.884]
</p><p>82 largest eigenvalue) of Ht is bounded as: 1 2 2 1 1 Ht 2 ≤ k 1 − eyt + pt (y) ey − k 1 ≤ 10, (1 − γ)2 y=yt  for γ ≤  1 2. [sent-216, score-0.351]
</p><p>83 Now, for any Ψ ∈ K, by Lemma 2, for the speciﬁed value of β we have αδ 2 Ht ⊗ xt xt . [sent-217, score-0.312]
</p><p>84 Finally, we have 1 1 (W − Wt ) [ηHt ⊗ xt xt ](W − Wt ) ≤ η Ht ⊗ xt xt 2 W − Wt 2 ≤ 20ηR2 D2 , (13) 2 2 since W − Wt ≤ 2D. [sent-219, score-0.624]
</p><p>85 4  Experiments  While the theoretical regret bound for N EWTRON is O(log T ) when α = O(1), the provable constant in O(·) notation is quite large, leading one to question the practical performance of the algorithm. [sent-221, score-0.321]
</p><p>86 PN EWTRON does not have the same regret guarantees of N EWTRON however. [sent-241, score-0.25]
</p><p>87 To derive PN EWTRON, we can restate N EWTRON equivalently as (see [HAK07]): Wt = arg min (W − Wt ) At (W − Wt ) W∈K  t−1 ˜ ˜ and bt = t−1 (1 − κτ β ˜ τ · Wτ ) ˜ τ . [sent-242, score-0.055]
</p><p>88 7  1 where Wt = −A−1 bt , for At = D I + t t−1 bt = τ =1 (1 − κτ β ˜ τ · Wτ ) ˜ τ . [sent-246, score-0.072]
</p><p>89 For the S YN S EP data set, PN EWTRON very rapidly converges to the lowest possible error rate due to setting the exploration parameter γ = 0. [sent-279, score-0.05]
</p><p>90 For the R EUTERS 4 data set, both BANDITRON and PN EWTRON decrease the error rate at roughly same pace; however PN EWTRON still obtains better performance consistently by a few percentage points. [sent-292, score-0.056]
</p><p>91 4  error rate  error rate  Error rate  10  −1  10  −0. [sent-303, score-0.078]
</p><p>92 Is it possible to obtain similar regret guarantees for a linear time algorithm? [sent-319, score-0.25]
</p><p>93 Our regret bound has an exponentially large constant, which depends on the loss functions parameters. [sent-320, score-0.334]
</p><p>94 Does there exist an algorithm with similar regret guarantees but better constants? [sent-321, score-0.266]
</p><p>95 Competing in the dark: An efﬁcient algorithm for bandit linear optimization. [sent-330, score-0.125]
</p><p>96 An efﬁcient bandit algorithm for T -regret in online multiclass prediction? [sent-340, score-0.25]
</p><p>97 Robbing the bandit: less regret in online geometric optimization against an adaptive adversary. [sent-347, score-0.302]
</p><p>98 Online convex optimization in the bandit setting: gradient descent without a gradient. [sent-356, score-0.125]
</p><p>99 Online geometric optimization in the bandit setting against an adaptive adversary. [sent-377, score-0.109]
</p><p>100 Closing the gap between bandit and full-information online optimization: High-probability regret bound. [sent-380, score-0.411]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ewtron', 0.685), ('wt', 0.294), ('banditron', 0.261), ('regret', 0.25), ('yt', 0.229), ('pn', 0.158), ('xt', 0.156), ('eyt', 0.143), ('pt', 0.142), ('ht', 0.115), ('bandit', 0.109), ('ft', 0.104), ('euters', 0.098), ('ep', 0.089), ('yn', 0.08), ('multiclass', 0.073), ('lemma', 0.071), ('ctitious', 0.065), ('pnewtron', 0.065), ('online', 0.052), ('ftal', 0.049), ('rkn', 0.049), ('ey', 0.049), ('loss', 0.046), ('abernethy', 0.043), ('feedback', 0.04), ('adversarial', 0.039), ('log', 0.039), ('learner', 0.039), ('bound', 0.038), ('label', 0.038), ('kakade', 0.038), ('bt', 0.036), ('kn', 0.034), ('rakhlin', 0.034), ('vt', 0.034), ('rn', 0.033), ('ditron', 0.033), ('drat', 0.033), ('eil', 0.033), ('synnonsep', 0.033), ('prediction', 0.032), ('wk', 0.029), ('satyen', 0.029), ('ban', 0.029), ('wy', 0.029), ('varsha', 0.029), ('elad', 0.028), ('colt', 0.028), ('hazan', 0.026), ('brendan', 0.026), ('kronecker', 0.026), ('rk', 0.026), ('obtains', 0.025), ('round', 0.025), ('recommender', 0.025), ('dubbed', 0.025), ('wi', 0.024), ('matrix', 0.024), ('diag', 0.023), ('newton', 0.023), ('al', 0.023), ('alexander', 0.023), ('xx', 0.022), ('corollary', 0.022), ('dani', 0.022), ('ambuj', 0.022), ('quadratic', 0.021), ('hinge', 0.021), ('pp', 0.021), ('semide', 0.021), ('open', 0.02), ('matrices', 0.02), ('crammer', 0.02), ('cost', 0.02), ('symmetric', 0.02), ('fully', 0.02), ('sham', 0.02), ('et', 0.019), ('arg', 0.019), ('exploration', 0.019), ('soda', 0.019), ('proof', 0.019), ('norm', 0.018), ('jacob', 0.018), ('israel', 0.018), ('bounded', 0.017), ('bandits', 0.017), ('constant', 0.017), ('chose', 0.017), ('parameterized', 0.017), ('question', 0.016), ('convex', 0.016), ('exp', 0.016), ('rate', 0.016), ('algorithm', 0.016), ('incorrect', 0.016), ('denote', 0.015), ('error', 0.015), ('usual', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="185-tfidf-1" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>2 0.25689733 <a title="185-tfidf-2" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>Author: Nicolò Cesa-bianchi, Ohad Shamir</p><p>Abstract: Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely diﬀerent approach, which combines “random playout” and randomized rounding of loss subgradients. As an application of our approach, we provide the ﬁrst computationally eﬃcient online algorithm for collaborative ﬁltering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning. 1</p><p>3 0.25271094 <a title="185-tfidf-3" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>4 0.23865575 <a title="185-tfidf-4" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>Author: Yasin Abbasi-yadkori, Csaba Szepesvári, David Tax</p><p>Abstract: We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller conﬁdence sets. For their construction we use a novel tail inequality for vector-valued martingales. 1</p><p>5 0.23281616 <a title="185-tfidf-5" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>Author: Dan Garber, Elad Hazan</p><p>Abstract: In recent years semideﬁnite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the ﬁrst sublinear time approximation algorithm for semideﬁnite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric. 1</p><p>6 0.20954147 <a title="185-tfidf-6" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>7 0.17687637 <a title="185-tfidf-7" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>8 0.14245763 <a title="185-tfidf-8" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>9 0.14078633 <a title="185-tfidf-9" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>10 0.14066131 <a title="185-tfidf-10" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>11 0.13348398 <a title="185-tfidf-11" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>12 0.1152861 <a title="185-tfidf-12" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>13 0.11407811 <a title="185-tfidf-13" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>14 0.10979495 <a title="185-tfidf-14" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>15 0.10407932 <a title="185-tfidf-15" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>16 0.1004485 <a title="185-tfidf-16" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>17 0.10013611 <a title="185-tfidf-17" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>18 0.096871249 <a title="185-tfidf-18" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>19 0.09294983 <a title="185-tfidf-19" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>20 0.090963863 <a title="185-tfidf-20" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.188), (1, -0.319), (2, -0.032), (3, -0.07), (4, 0.293), (5, 0.009), (6, 0.097), (7, 0.022), (8, 0.029), (9, -0.016), (10, 0.143), (11, -0.009), (12, 0.112), (13, 0.012), (14, -0.059), (15, 0.028), (16, 0.012), (17, 0.044), (18, 0.01), (19, -0.035), (20, -0.01), (21, -0.013), (22, 0.059), (23, 0.012), (24, 0.002), (25, -0.012), (26, 0.03), (27, -0.05), (28, -0.059), (29, -0.016), (30, 0.018), (31, 0.065), (32, -0.034), (33, 0.011), (34, -0.015), (35, 0.008), (36, -0.02), (37, 0.019), (38, 0.047), (39, -0.051), (40, -0.007), (41, -0.002), (42, 0.094), (43, -0.048), (44, -0.019), (45, 0.005), (46, -0.016), (47, -0.046), (48, 0.008), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95615226 <a title="185-lsi-1" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>2 0.8482216 <a title="185-lsi-2" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>Author: Nicolò Cesa-bianchi, Ohad Shamir</p><p>Abstract: Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely diﬀerent approach, which combines “random playout” and randomized rounding of loss subgradients. As an application of our approach, we provide the ﬁrst computationally eﬃcient online algorithm for collaborative ﬁltering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning. 1</p><p>3 0.82971734 <a title="185-lsi-3" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>4 0.80495209 <a title="185-lsi-4" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>Author: Wouter M. Koolen, Wojciech Kotlowski, Manfred K. Warmuth</p><p>Abstract: We extend the classical problem of predicting a sequence of outcomes from a ﬁnite alphabet to the matrix domain. In this extension, the alphabet of n outcomes is replaced by the set of all dyads, i.e. outer products uu where u is a vector in Rn of unit length. Whereas in the classical case the goal is to learn (i.e. sequentially predict as well as) the best multinomial distribution, in the matrix case we desire to learn the density matrix that best explains the observed sequence of dyads. We show how popular online algorithms for learning a multinomial distribution can be extended to learn density matrices. Intuitively, learning the n2 parameters of a density matrix is much harder than learning the n parameters of a multinomial distribution. Completely surprisingly, we prove that the worst-case regrets of certain classical algorithms and their matrix generalizations are identical. The reason is that the worst-case sequence of dyads share a common eigensystem, i.e. the worst case regret is achieved in the classical case. So these matrix algorithms learn the eigenvectors without any regret. 1</p><p>5 0.75329649 <a title="185-lsi-5" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>Author: Dan Garber, Elad Hazan</p><p>Abstract: In recent years semideﬁnite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the ﬁrst sublinear time approximation algorithm for semideﬁnite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric. 1</p><p>6 0.73553938 <a title="185-lsi-6" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>7 0.73330003 <a title="185-lsi-7" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>8 0.61130899 <a title="185-lsi-8" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>9 0.59623712 <a title="185-lsi-9" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>10 0.54181278 <a title="185-lsi-10" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>11 0.53890026 <a title="185-lsi-11" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>12 0.53063536 <a title="185-lsi-12" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>13 0.50979108 <a title="185-lsi-13" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>14 0.48051229 <a title="185-lsi-14" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>15 0.47880596 <a title="185-lsi-15" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>16 0.47112015 <a title="185-lsi-16" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>17 0.46791801 <a title="185-lsi-17" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>18 0.45131293 <a title="185-lsi-18" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>19 0.43068066 <a title="185-lsi-19" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>20 0.41341844 <a title="185-lsi-20" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.046), (4, 0.096), (20, 0.032), (26, 0.037), (31, 0.058), (33, 0.014), (43, 0.064), (45, 0.125), (57, 0.024), (65, 0.012), (66, 0.202), (74, 0.062), (79, 0.02), (83, 0.067), (99, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82696456 <a title="185-lda-1" href="./nips-2011-Learning_Probabilistic_Non-Linear_Latent_Variable_Models_for_Tracking_Complex_Activities.html">148 nips-2011-Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities</a></p>
<p>Author: Angela Yao, Juergen Gall, Luc V. Gool, Raquel Urtasun</p><p>Abstract: A common approach for handling the complexity and inherent ambiguities of 3D human pose estimation is to use pose priors learned from training data. Existing approaches however, are either too simplistic (linear), too complex to learn, or can only learn latent spaces from “simple data”, i.e., single activities such as walking or running. In this paper, we present an efﬁcient stochastic gradient descent algorithm that is able to learn probabilistic non-linear latent spaces composed of multiple activities. Furthermore, we derive an incremental algorithm for the online setting which can update the latent space without extensive relearning. We demonstrate the effectiveness of our approach on the task of monocular and multi-view tracking and show that our approach outperforms the state-of-the-art. 1</p><p>2 0.81988162 <a title="185-lda-2" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>Author: Andre S. Barreto, Doina Precup, Joelle Pineau</p><p>Abstract: Kernel-based reinforcement-learning (KBRL) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees. However, the size of the approximator grows with the number of transitions, which makes the approach impractical for large problems. In this paper we introduce a novel algorithm to improve the scalability of KBRL. We resort to a special decomposition of a transition matrix, called stochastic factorization, to ﬁx the size of the approximator while at the same time incorporating all the information contained in the data. The resulting algorithm, kernel-based stochastic factorization (KBSF), is much faster but still converges to a unique solution. We derive a theoretical upper bound for the distance between the value functions computed by KBRL and KBSF. The effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems, including a difﬁcult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains. We empirically demonstrate that the proposed approach is able to compress the information contained in KBRL’s model. Also, on the tasks studied, KBSF outperforms two of the most prominent reinforcement-learning algorithms, namely least-squares policy iteration and ﬁtted Q-iteration. 1</p><p>same-paper 3 0.80054867 <a title="185-lda-3" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>4 0.75188661 <a title="185-lda-4" href="./nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">178 nips-2011-Multiclass Boosting: Theory and Algorithms</a></p>
<p>Author: Mohammad J. Saberian, Nuno Vasconcelos</p><p>Abstract: The problem of multi-class boosting is considered. A new framework, based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets. 1</p><p>5 0.74642915 <a title="185-lda-5" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>Author: Neil D. Lawrence, Michalis K. Titsias, Andreas Damianou</p><p>Abstract: High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences. 1</p><p>6 0.68215948 <a title="185-lda-6" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>7 0.67995656 <a title="185-lda-7" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>8 0.67923671 <a title="185-lda-8" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>9 0.67801833 <a title="185-lda-9" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>10 0.67774439 <a title="185-lda-10" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>11 0.67676824 <a title="185-lda-11" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>12 0.67554206 <a title="185-lda-12" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>13 0.67520165 <a title="185-lda-13" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>14 0.6728887 <a title="185-lda-14" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>15 0.67120981 <a title="185-lda-15" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>16 0.67009503 <a title="185-lda-16" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>17 0.6694417 <a title="185-lda-17" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>18 0.66765094 <a title="185-lda-18" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>19 0.66748279 <a title="185-lda-19" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>20 0.66494548 <a title="185-lda-20" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
