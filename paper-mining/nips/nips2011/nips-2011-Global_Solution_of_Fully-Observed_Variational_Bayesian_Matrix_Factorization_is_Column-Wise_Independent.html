<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-107" href="#">nips2011-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</h1>
<br/><p>Source: <a title="nips-2011-107-pdf" href="http://papers.nips.cc/paper/4344-global-solution-of-fully-observed-variational-bayesian-matrix-factorization-is-column-wise-independent.pdf">pdf</a></p><p>Author: Shinichi Nakajima, Masashi Sugiyama, S. D. Babacan</p><p>Abstract: Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. 1</p><p>Reference: <a title="nips-2011-107-reference" href="../nips2011_reference/nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. [sent-8, score-0.288]
</p><p>2 A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. [sent-9, score-0.242]
</p><p>3 However, it was not clear how restrictive the column-wise independence assumption is. [sent-10, score-0.124]
</p><p>4 In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. [sent-11, score-0.374]
</p><p>5 A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. [sent-12, score-0.398]
</p><p>6 We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. [sent-13, score-0.373]
</p><p>7 1  Introduction  The goal of matrix factorization (MF) is to approximate an observed matrix by a low-rank one. [sent-14, score-0.102]
</p><p>8 In this paper, we consider fully-observed MF where the observed matrix has no missing entry1 . [sent-15, score-0.067]
</p><p>9 This formulation includes classical multivariate analysis techniques based on singular-value decomposition such as principal component analysis (PCA) [9] and canonical correlation analysis [10]. [sent-16, score-0.041]
</p><p>10 In the framework of probabilistic MF [20, 17, 19], posterior distributions of factorized matrices are considered. [sent-17, score-0.138]
</p><p>11 Since exact inference is computationally intractable, the Laplace approximation [3], the Markov chain Monte Carlo sampling [3, 18], and the variational Bayesian (VB) approximation [4, 13, 16, 15] were used for approximate inference in practice. [sent-18, score-0.116]
</p><p>12 In the original VBMF [4, 13], factored matrices are assumed to be matrix-wise independent, and a local optimal solution is computed by an iterative algorithm. [sent-20, score-0.208]
</p><p>13 A simpliﬁed variant of VBMF (simpleVBMF) was also proposed [16], which assumes a stronger constraint that the factored matrices 1 This excludes the collaborative ﬁltering setup, which is aimed at imputing missing entries of an observed matrix [12, 7]. [sent-21, score-0.163]
</p><p>14 A notable advantage of simpleVBMF is that the global optimal solution can be computed analytically in a computationally very efﬁcient way [15]. [sent-23, score-0.179]
</p><p>15 Intuitively, it is suspected that simpleVBMF only possesses weaker approximation ability due to its stronger column-wise independence assumption. [sent-24, score-0.174]
</p><p>16 Nevertheless, the inﬂuence of the stronger column-wise independence assumption was not elucidated beyond this empirical evaluation. [sent-27, score-0.174]
</p><p>17 The main contribution of this paper is to theoretically show that the column-wise independence assumption does not degrade the performance. [sent-28, score-0.146]
</p><p>18 More speciﬁcally, we prove that a global optimal solution of the original VBMF is actually column-wise independent. [sent-29, score-0.126]
</p><p>19 Thus, a global optimal solution of the original VBMF can be obtained by the analytic-form solution of simpleVBMF—no computationally-expensive iterative algorithm is necessary. [sent-30, score-0.289]
</p><p>20 We show the usefulness of the analyticform solution through experiments on probabilistic PCA. [sent-31, score-0.132]
</p><p>21 2  Formulation  In this section, we ﬁrst formulate the problem of probabilistic MF, and then introduce the VB approximation and its simpliﬁed variant. [sent-32, score-0.066]
</p><p>22 We assume that the prior covariance matrices CA and CB are diagonal and positive deﬁnite, i. [sent-36, score-0.054]
</p><p>23 Throughout the paper, we denote a column vector of a matrix by a bold smaller letter, and a row vector by a bold smaller letter with a tilde, namely, ⊤  A = (a1 , . [sent-51, score-0.09]
</p><p>24 Variational Bayesian Approximation  The Bayes posterior is written as p(A, B|Y ) =  p(Y |A, B)p(A)p(B) , Z(Y )  (3)  where Z(Y ) = 〈p(Y |A, B)〉p(A)p(B) is the marginal likelihood. [sent-65, score-0.058]
</p><p>25 Since the Bayes posterior (3) is computationally intractable, the VB approximation was proposed [4, 13, 16, 15]. [sent-67, score-0.079]
</p><p>26 The following functional with respect to r is called the free energy: F (r|Y ) =  log  r(A, B) p(Y |A, B)p(A)p(B)  = r(A,B)  2  log  r(A, B) p(A, B|Y )  − log Z(Y ). [sent-69, score-0.153]
</p><p>27 Therefore, minimizing the free energy (4) amounts to ﬁnding the distribution closest to the Bayes posterior in the sense of the KL distance. [sent-71, score-0.244]
</p><p>28 In the VB approximation, the free energy (4) is minimized over some restricted function space. [sent-72, score-0.235]
</p><p>29 A standard constraint for the MF model is matrix-wise independence [4, 13], i. [sent-73, score-0.145]
</p><p>30 (5)  This constraint breaks off the entanglement between the parameter matrices A and B, and leads to a computationally-tractable iterative algorithm. [sent-76, score-0.144]
</p><p>31 Using the variational method, we can show that, under the constraint (5), the VB posterior minimizing the free energy (4) is written as M  L  NH (am ; am , ΣA )  rVB (A, B) = m=1  NH (bl ; bl , ΣB ), l=1  where the parameters satisfy A = a1 , . [sent-77, score-0.387]
</p><p>32 (6) and (7) until convergence gives a local minimum of the free energy (4). [sent-86, score-0.208]
</p><p>33 When the noise variance σ 2 is unknown, it can also be estimated based on the free energy minimization. [sent-87, score-0.273]
</p><p>34 The update rule for σ 2 is given by 2  σ =  ∥Y ∥2 − tr(2Y ⊤ B A⊤ ) + tr (A⊤ A + M ΣA )(B ⊤ B + LΣB ) Fro LM  . [sent-88, score-0.045]
</p><p>35 (8)  Furthermore, in the empirical Bayesian scenario, the hyperparameters CA and CB are also estimated from data. [sent-89, score-0.063]
</p><p>36 In this scenario, CA and CB are updated in each iteration by the following formulas: c2 h = ∥ah ∥2 /M + (ΣA )hh , a 2. [sent-90, score-0.037]
</p><p>37 b  (9)  SimpleVB Approximation  A simpliﬁed variant, called the simpleVB approximation, assumes column-wise independence of each matrix [16, 15], i. [sent-92, score-0.154]
</p><p>38 simpleVB rah (ah ) h=1  (10)  h=1  This constraint restricts the covariances ΣA and ΣB to be diagonal, and thus necessary memory storage and computational cost are substantially reduced [16]. [sent-95, score-0.038]
</p><p>39 (11) and (12) until convergence, we can obtain a local minimum of the free energy. [sent-99, score-0.102]
</p><p>40 (8) and (9) are similarly applied if the noise variance σ 2 is unknown and in the empirical Bayesian scenario, respectively. [sent-101, score-0.061]
</p><p>41 A recent study has derived the analytic solution for simpleVB when the observed matrix has no missing entry [15]. [sent-102, score-0.374]
</p><p>42 This work made simpleVB more attractive, because it did not only provide substantial reduction of computation costs, but also guaranteed the global optimality of the solution. [sent-103, score-0.058]
</p><p>43 However, it was not clear how restrictive the column-wise independence assumption is, beyond its experimental success [14]. [sent-104, score-0.141]
</p><p>44 In the next section, we theoretically show that the column-wise independence assumption is actually harmless. [sent-105, score-0.124]
</p><p>45 3  Analytic Solution of VBMF under Matrix-wise Independence  Under the matrix-wise independence constraint (5), the free energy (4) can be written as F = 〈log r(A) + log r(B) − log p(Y |A, B)p(A)p(B)〉r(A)r(B) 2  =  LM M |CA | L |CB | ∥Y ∥ log σ 2 + log + log + + const. [sent-106, score-0.46]
</p><p>46 2 2 |ΣA | 2 |ΣB | 2σ 2 1 −1 −1 + tr CA A⊤ A + M ΣA + CB B ⊤ B + LΣB 2 +σ −2 −2A⊤ Y ⊤ B + A⊤ A + M ΣA  B ⊤ B + LΣB  . [sent-107, score-0.045]
</p><p>47 Below, we show that a global solution of ΣA and ΣB is diagonal. [sent-111, score-0.126]
</p><p>48 , cah cbh > cah′ cbh′ for any pair h < h′ ), the global solution is unique and diagonal. [sent-114, score-0.472]
</p><p>49 On the other hand, when CA CB is degenerate, the global solutions are not unique because arbitrary rotation in the degenerate subspace is possible without changing the free energy. [sent-115, score-0.178]
</p><p>50 Theorem 1 Diagonal ΣA and ΣB minimize the free energy (13). [sent-117, score-0.208]
</p><p>51 The basic idea of our proof is that, since minimizing the free energy (13) with respect to A, B, ΣA , and ΣB is too complicated, we focus on a restricted space written in a particular form that includes the optimal solution. [sent-118, score-0.23]
</p><p>52 Then, as a function of Ω, the free energy (13) can be simpliﬁed as 1 1/2 1/2 −1 −1 ∗ F (Ω) = tr CA CB ΩCA B ∗⊤ B ∗ + LΣB CA Ω ⊤ + const. [sent-124, score-0.253]
</p><p>53 Then, as a function of Ω ′ , the free energy (13) can be expressed as 1 1/2 1/2 −1 −1 ∗ F (Ω ′ ) = tr CA CB Ω ′ CB A∗⊤ A∗ + M ΣA CB Ω ′⊤ + const. [sent-130, score-0.253]
</p><p>54 Note, however, that Theorem 1 does not necessarily hold when the observed matrix has missing entries. [sent-135, score-0.067]
</p><p>55 Theorem 1 implies that the stronger column-wise independence constraint (10) does not degrade approximation accuracy, and the VB solution under matrix-wise independence (5) essentially agrees with the simpleVB solution. [sent-136, score-0.435]
</p><p>56 Consequently, we can obtain a global analytic solution for VB, by combining Theorem 1 above with Theorem 1 in [15]: Corollary 1 Let γh (≥ 0) be the h-th largest singular value of Y , and let ω ah and ω bh be the associated right and left singular vectors: L  γh ω bh ω ⊤h . [sent-137, score-1.303]
</p><p>57 Let γh =  (L + M )σ 2 σ4 + 2 2 + 2 2cah cbh  (L + M )σ 2 σ4 + 2 2 2 2cah cbh  2  − LM σ 4 . [sent-139, score-0.346]
</p><p>58 (15)  Then, the global VB solution under matrix-wise independence (5) can be expressed as H VB VB γh ω bh ω ⊤h , where γh = a  U VB ≡ 〈BA⊤ 〉rVB (A,B) = B A⊤ = h=1  γh 0  if γh > γh , otherwise. [sent-140, score-0.599]
</p><p>59 Theorem 1 holds also in the empirical Bayesian scenario, where the hyperparameters (CA , CB ) are also estimated from observation. [sent-141, score-0.063]
</p><p>60 Accordingly, the empirical VB solution also agrees with the empirical simpleVB solution, whose analytic-form is given in Corollary 5 in [15]. [sent-142, score-0.134]
</p><p>61 Thus, we obtain the global analytic solution for empirical VB: Corollary 2 The global empirical VB solution under matrix-wise independence (5) is given by H EVB EVB γh ω bh ω ⊤h , where γh = a  U EVB = h=1  γh ˘ VB 0  if γh > γ h and ∆h ≤ 0, otherwise. [sent-143, score-0.983]
</p><p>62 Here, √ √ γ h = ( L + M )σ, c2 = ˘h  1 2LM  2 γh − (L + M )σ 2 +  (16) 2  2 (γh − (L + M )σ 2 ) − 4LM σ 4 ,  γh VB 1 γh VB γ + 1 + L log ˘ γ + 1 + 2 −2γh γh + LM c2 , ˘ ˘ VB ˘h M σ2 h Lσ 2 h σ ˘ and γh is the VB solution for cah cbh = ch . [sent-144, score-0.431]
</p><p>63 ˘ VB ∆h = M log  5  (17) (18)  When we calculate the empirical VB solution, we ﬁrst check if γh > γ h holds. [sent-145, score-0.037]
</p><p>64 When the noise variance σ 2 is unknown, it is optimized by a naive 1-dimensional search to minimize the free energy [15]. [sent-151, score-0.249]
</p><p>65 To evaluate the free energy (13), we need the covariances ΣA and ΣB , which neither Corollary 1 nor Corollary 2 provides. [sent-152, score-0.225]
</p><p>66 h  Note that the ratio cah /cbh is arbitrary in empirical VB, so we can ﬁx it to, e. [sent-154, score-0.193]
</p><p>67 4  Experimental Results  In this section, we ﬁrst introduce probabilistic PCA as a probabilistic MF model. [sent-157, score-0.092]
</p><p>68 Then, we show experimental results on artiﬁcial and benchmark datasets, which illustrate practical advantages of using our analytic solution. [sent-158, score-0.252]
</p><p>69 1  Probabilistic PCA  In probabilistic PCA [20], the observation y ∈ RL is assumed to be driven by a latent vector a ∈ RH in the following form: y = Ba + ε. [sent-160, score-0.046]
</p><p>70 Then, the probabilistic PCA model is written as Eqs. [sent-169, score-0.068]
</p><p>71 2  Experiment on Artiﬁcial Data  We compare the iterative algorithm and the analytic solution in the empirical VB scenario with unknown noise variance, i. [sent-174, score-0.45]
</p><p>72 , the hyperparameters (CA , CB ) and the noise variance σ 2 are also 6  80  2. [sent-176, score-0.06]
</p><p>73 6  5 0  50  100 150 Iteration  200  250  0 0  (a) Free energy  50  100 150 Iteration  200  (b) Computation time  250  0 0  50  100 150 Iteration  200  250  (c) Estimated rank  Figure 2: Experimental results for Artiﬁcial2 dataset (L = 70, M = 300, and H ∗ = 40). [sent-185, score-0.14]
</p><p>74 Figure 1 shows the free energy, the computation time, and the estimated rank over iterations for an artiﬁcial (Artiﬁcial1) dataset with L = 100, M = 300, and H ∗ = 20. [sent-190, score-0.16]
</p><p>75 We randomly created true ∗ ∗ matrices A∗ ∈ RM ×H and B ∗ ∈ RL×H so that each entry of A∗ and B ∗ follows N1 (0, 1). [sent-191, score-0.065]
</p><p>76 An observed matrix Y was created by adding a noise subject to N1 (0, 1) to each entry of B ∗ A∗⊤ . [sent-192, score-0.092]
</p><p>77 The iterative algorithm consists of the update rules (6)–(9). [sent-193, score-0.095]
</p><p>78 Initial values were set in the following way: A and B are randomly created so that each entry follows N1 (0, 1). [sent-194, score-0.037]
</p><p>79 We ran the iterative algorithm 10 times, starting from different initial points, and each trial is plotted by a solid line in Figure 1. [sent-197, score-0.116]
</p><p>80 The analytic solution consists of applying Corollary 2 combined with a naive 1-dimensional search for noise variance σ 2 estimation [15]. [sent-198, score-0.327]
</p><p>81 The analytic solution is plotted by the dashed line. [sent-199, score-0.286]
</p><p>82 We see that the analytic solution estimates the true rank H = H ∗ = 20 immediately (∼ 0. [sent-200, score-0.32]
</p><p>83 1 sec on average over 10 trials), while the iterative algorithm does not converge in 60 sec. [sent-201, score-0.14]
</p><p>84 In this case, all the 10 trials of the iterative algorithm are trapped at local minima. [sent-203, score-0.095]
</p><p>85 We empirically observed a tendency that the iterative algorithm suffers from the local minima problem when H ∗ is large (close to H). [sent-204, score-0.111]
</p><p>86 We also conducted experiments on various benchmark datasets, and found that the iterative algorithm typically converges slowly, and sometimes suffers from the local minima problem, while our analytic-form gives the global solution immediately. [sent-207, score-0.254]
</p><p>87 5  200  20  100  50  100 150 Iteration  200  0 0  250  (a) Free energy  10 50  100 150 Iteration  200  0 0  250  (b) Computation time  50  100 150 Iteration  200  250  (c) Estimated rank  Figure 3: Experimental results for the Sat dataset (L = 36, M = 6435). [sent-211, score-0.14]
</p><p>88 5  0 0  (a) Free energy  50  100 150 Iteration  200  (b) Computation time  250  0 0  50  100 150 Iteration  200  250  (c) Estimated rank  Figure 4: Experimental results for the Spectf dataset (L = 44, M = 267). [sent-214, score-0.14]
</p><p>89 5  Conclusion and Discussion  In this paper, we have analyzed the fully-observed variational Bayesian matrix factorization (VBMF) under matrix-wise independence. [sent-215, score-0.125]
</p><p>90 We have shown that the VB solution under matrix-wise independence essentially agrees with the simpliﬁed VB (simpleVB) solution under column-wise independence. [sent-216, score-0.286]
</p><p>91 As a consequence, we can obtain the global VB solution under matrix-wise independence analytically in a computationally very efﬁcient way. [sent-217, score-0.303]
</p><p>92 With correlated priors, the posterior is no longer uncorrelated and thus it is not straightforward to obtain a global solution analytically. [sent-219, score-0.18]
</p><p>93 Nevertheless, there exists a situation where an analytic solution can be easily obtained: Suppose there exists an H × H ′ ′ non-singular matrix T such that both of CA = T CA T ⊤ and CB = (T −1 )⊤ CB T −1 are diagonal. [sent-220, score-0.316]
</p><p>94 We can show that the free energy (13) is invariant under the following transformation for any T : A → AT ⊤ ,  ΣA → T ΣA T ⊤ ,  CA → T CA T ⊤ ,  B → BT −1 ,  ΣB → (T −1 )T ΣB T −1 ,  CB → (T −1 )⊤ CB T −1 . [sent-221, score-0.208]
</p><p>95 Accordingly, the following procedure gives the global solution analytically: the analytic solution ′ ′ given the diagonal (CA , CB ) is ﬁrst computed, and the above transformation is then applied. [sent-222, score-0.438]
</p><p>96 We have demonstrated the usefulness of our analytic solution in probabilistic PCA. [sent-223, score-0.35]
</p><p>97 Analyzing such a situation, as well as missing value imputation and tensor factorization [11, 6, 8, 21] is our important future work. [sent-231, score-0.095]
</p><p>98 Analysis of a complex of statistical variables into principal components. [sent-289, score-0.041]
</p><p>99 On Bayesian PCA: Automatic dimensionality selection and analytic solution. [sent-326, score-0.218]
</p><p>100 Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. [sent-364, score-0.118]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vb', 0.509), ('cb', 0.399), ('bh', 0.349), ('ah', 0.261), ('analytic', 0.218), ('ca', 0.193), ('cah', 0.173), ('cbh', 0.173), ('simplevb', 0.173), ('vbmf', 0.138), ('independence', 0.124), ('lm', 0.117), ('energy', 0.106), ('free', 0.102), ('iterative', 0.095), ('simplevbmf', 0.086), ('ih', 0.083), ('evb', 0.069), ('fro', 0.069), ('mf', 0.068), ('solution', 0.068), ('corollary', 0.065), ('global', 0.058), ('rl', 0.056), ('variational', 0.053), ('babacan', 0.052), ('rvb', 0.052), ('bl', 0.047), ('probabilistic', 0.046), ('pca', 0.046), ('sugiyama', 0.046), ('tr', 0.045), ('sec', 0.045), ('bayesian', 0.044), ('il', 0.043), ('factorization', 0.042), ('nakajima', 0.042), ('principal', 0.041), ('missing', 0.037), ('iteration', 0.037), ('posterior', 0.036), ('nl', 0.036), ('nh', 0.036), ('derin', 0.035), ('rsimplevb', 0.035), ('spectf', 0.035), ('ba', 0.034), ('rank', 0.034), ('tokyo', 0.031), ('psychometrika', 0.03), ('matrix', 0.03), ('stronger', 0.03), ('analytically', 0.03), ('factorized', 0.028), ('matrices', 0.028), ('minimized', 0.027), ('masashi', 0.026), ('agrees', 0.026), ('diagonal', 0.026), ('noise', 0.025), ('estimated', 0.024), ('scenario', 0.024), ('hh', 0.024), ('arti', 0.023), ('computationally', 0.023), ('rb', 0.023), ('written', 0.022), ('letter', 0.022), ('degrade', 0.022), ('simpli', 0.022), ('ads', 0.021), ('japan', 0.021), ('constraint', 0.021), ('trial', 0.021), ('bayes', 0.021), ('ra', 0.021), ('im', 0.021), ('entry', 0.021), ('approximation', 0.02), ('nm', 0.02), ('empirical', 0.02), ('hyperparameters', 0.019), ('accordingly', 0.019), ('bold', 0.019), ('uncorrelated', 0.018), ('usefulness', 0.018), ('degenerate', 0.018), ('factored', 0.017), ('experimental', 0.017), ('covariances', 0.017), ('repository', 0.017), ('benchmark', 0.017), ('log', 0.017), ('uci', 0.016), ('minima', 0.016), ('tensor', 0.016), ('created', 0.016), ('roweis', 0.016), ('variance', 0.016), ('salakhutdinov', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="107-tfidf-1" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. D. Babacan</p><p>Abstract: Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. 1</p><p>2 0.20100044 <a title="107-tfidf-2" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>Author: Samory Kpotufe</p><p>Abstract: Many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data. These regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic dimension. In particular our rates are local to a query x and depend only on the way masses of balls centered at x vary with radius. Furthermore, we show a simple way to choose k = k(x) locally at any x so as to nearly achieve the minimax rate at x in terms of the unknown intrinsic dimension in the vicinity of x. We also establish that the minimax rate does not depend on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure. 1</p><p>3 0.11142058 <a title="107-tfidf-3" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>Author: Yevgeny Seldin, Peter Auer, John S. Shawe-taylor, Ronald Ortner, François Laviolette</p><p>Abstract: We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The p scaling of our regret bound with the number of states (contexts) N goes as N I⇢t (S; A), where I⇢t (S; A) is the mutual information between states and actions (the side information) used by the algorithm at round t. If the algorithm p uses all the side information, the regret bound scales as N ln K, where K is the number of actions (arms). However, if the side information I⇢t (S; A) is not fully used, the regret bound is signiﬁcantly tighter. In the extreme case, when I⇢t (S; A) = 0, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round. 1</p><p>4 0.080468975 <a title="107-tfidf-4" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>5 0.069548741 <a title="107-tfidf-5" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>Author: David A. Knowles, Tom Minka</p><p>Abstract: Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability. 1</p><p>6 0.065888211 <a title="107-tfidf-6" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>7 0.059747785 <a title="107-tfidf-7" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>8 0.058026191 <a title="107-tfidf-8" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>9 0.054207556 <a title="107-tfidf-9" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>10 0.052247528 <a title="107-tfidf-10" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>11 0.051345434 <a title="107-tfidf-11" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>12 0.045089401 <a title="107-tfidf-12" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>13 0.044701837 <a title="107-tfidf-13" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>14 0.043769669 <a title="107-tfidf-14" href="./nips-2011-Simultaneous_Sampling_and_Multi-Structure_Fitting_with_Adaptive_Reversible_Jump_MCMC.html">255 nips-2011-Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC</a></p>
<p>15 0.042471401 <a title="107-tfidf-15" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>16 0.041380286 <a title="107-tfidf-16" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>17 0.04128189 <a title="107-tfidf-17" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>18 0.040718209 <a title="107-tfidf-18" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>19 0.040451135 <a title="107-tfidf-19" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>20 0.039481886 <a title="107-tfidf-20" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, -0.014), (2, 0.018), (3, -0.064), (4, -0.033), (5, -0.026), (6, 0.039), (7, -0.017), (8, 0.055), (9, 0.049), (10, -0.018), (11, -0.036), (12, -0.006), (13, -0.053), (14, 0.007), (15, -0.064), (16, -0.063), (17, -0.057), (18, -0.015), (19, 0.024), (20, -0.025), (21, -0.003), (22, 0.074), (23, 0.072), (24, 0.049), (25, 0.087), (26, 0.051), (27, 0.008), (28, -0.046), (29, -0.006), (30, -0.071), (31, -0.025), (32, -0.062), (33, -0.047), (34, 0.142), (35, -0.015), (36, 0.014), (37, 0.109), (38, -0.06), (39, 0.064), (40, -0.03), (41, -0.017), (42, -0.077), (43, -0.138), (44, 0.113), (45, -0.068), (46, 0.003), (47, 0.031), (48, -0.038), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91153467 <a title="107-lsi-1" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. D. Babacan</p><p>Abstract: Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. 1</p><p>2 0.64727867 <a title="107-lsi-2" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>Author: Samory Kpotufe</p><p>Abstract: Many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data. These regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic dimension. In particular our rates are local to a query x and depend only on the way masses of balls centered at x vary with radius. Furthermore, we show a simple way to choose k = k(x) locally at any x so as to nearly achieve the minimax rate at x in terms of the unknown intrinsic dimension in the vicinity of x. We also establish that the minimax rate does not depend on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure. 1</p><p>3 0.58206958 <a title="107-lsi-3" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>4 0.54024881 <a title="107-lsi-4" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>Author: David A. Knowles, Tom Minka</p><p>Abstract: Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability. 1</p><p>5 0.4944776 <a title="107-lsi-5" href="./nips-2011-Demixed_Principal_Component_Analysis.html">68 nips-2011-Demixed Principal Component Analysis</a></p>
<p>Author: Wieland Brendel, Ranulfo Romo, Christian K. Machens</p><p>Abstract: In many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters. In electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difﬁcult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret. Here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model, and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response. 1</p><p>6 0.4334569 <a title="107-lsi-6" href="./nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">260 nips-2011-Sparse Features for PCA-Like Linear Regression</a></p>
<p>7 0.42624405 <a title="107-lsi-7" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>8 0.42530346 <a title="107-lsi-8" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>9 0.42093146 <a title="107-lsi-9" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>10 0.40682024 <a title="107-lsi-10" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>11 0.38319927 <a title="107-lsi-11" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>12 0.38267568 <a title="107-lsi-12" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>13 0.38035342 <a title="107-lsi-13" href="./nips-2011-Differentially_Private_M-Estimators.html">69 nips-2011-Differentially Private M-Estimators</a></p>
<p>14 0.37766033 <a title="107-lsi-14" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>15 0.37567744 <a title="107-lsi-15" href="./nips-2011-On_the_Completeness_of_First-Order_Knowledge_Compilation_for_Lifted_Probabilistic_Inference.html">201 nips-2011-On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference</a></p>
<p>16 0.36702374 <a title="107-lsi-16" href="./nips-2011-A_Denoising_View_of_Matrix_Completion.html">5 nips-2011-A Denoising View of Matrix Completion</a></p>
<p>17 0.35114595 <a title="107-lsi-17" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>18 0.3455095 <a title="107-lsi-18" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>19 0.3434414 <a title="107-lsi-19" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>20 0.33903435 <a title="107-lsi-20" href="./nips-2011-Simultaneous_Sampling_and_Multi-Structure_Fitting_with_Adaptive_Reversible_Jump_MCMC.html">255 nips-2011-Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.041), (4, 0.032), (20, 0.027), (26, 0.028), (31, 0.084), (33, 0.01), (40, 0.327), (43, 0.061), (45, 0.119), (57, 0.034), (74, 0.046), (83, 0.044), (99, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78219998 <a title="107-lda-1" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. D. Babacan</p><p>Abstract: Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. 1</p><p>2 0.6410225 <a title="107-lda-2" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>3 0.6281268 <a title="107-lda-3" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>Author: Shie Mannor, Ohad Shamir</p><p>Abstract: We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known “experts” setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds. 1</p><p>4 0.58983982 <a title="107-lda-4" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>Author: Kamil A. Wnuk, Stefano Soatto</p><p>Abstract: We propose a robust ﬁltering approach based on semi-supervised and multiple instance learning (MIL). We assume that the posterior density would be unimodal if not for the eﬀect of outliers that we do not wish to explicitly model. Therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior. Our approach can be thought of as a combination of standard ﬁnite-dimensional ﬁltering (Extended Kalman Filter, or Unscented Filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements. We show how both the state (regression) and the inlier set (classiﬁcation) can be estimated iteratively and causally by processing only the current measurement. We illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set). 1</p><p>5 0.55295128 <a title="107-lda-5" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>Author: Nobuyuki Morioka, Shin'ichi Satoh</p><p>Abstract: Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, 1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents “Generalized Lasso based Approximation of Sparse coding” (GLAS). By representing the distribution of sparse coefﬁcients with slice transform, we ﬁt a piece-wise linear mapping function with the generalized lasso. We also propose an efﬁcient post-reﬁnement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to 1 regularized sparse coding, yet achieves a signiﬁcant speed up demonstrating its effectiveness for large-scale visual recognition problems. 1</p><p>6 0.49173051 <a title="107-lda-6" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>7 0.48732248 <a title="107-lda-7" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>8 0.48684347 <a title="107-lda-8" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>9 0.48665559 <a title="107-lda-9" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>10 0.48637998 <a title="107-lda-10" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>11 0.48614058 <a title="107-lda-11" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>12 0.48543182 <a title="107-lda-12" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>13 0.48521176 <a title="107-lda-13" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>14 0.48341566 <a title="107-lda-14" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>15 0.48286444 <a title="107-lda-15" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>16 0.48285207 <a title="107-lda-16" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>17 0.48234409 <a title="107-lda-17" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>18 0.48219189 <a title="107-lda-18" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>19 0.48155954 <a title="107-lda-19" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>20 0.48104295 <a title="107-lda-20" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
