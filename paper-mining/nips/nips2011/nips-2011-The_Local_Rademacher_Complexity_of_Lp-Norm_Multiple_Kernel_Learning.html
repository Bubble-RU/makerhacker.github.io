<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-286" href="#">nips2011-286</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2011-286-pdf" href="http://papers.nips.cc/paper/4259-the-local-rademacher-complexity-of-lp-norm-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><p>Reference: <a title="nips-2011-286-reference" href="../nips2011_reference/nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. [sent-5, score-0.569]
</p><p>2 Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. [sent-6, score-0.111]
</p><p>3 We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. [sent-7, score-0.261]
</p><p>4 Nevertheless, after more than a decade of research it still remains an unsolved problem to ﬁnd the best abstraction or kernel for a problem at hand. [sent-9, score-0.131]
</p><p>5 Most frequently, the kernel is selected from a candidate set according to its generalization performance on a validation set. [sent-10, score-0.131]
</p><p>6 Clearly, the performance of such an algorithm is limited by the best kernel in the set. [sent-11, score-0.131]
</p><p>7 Unfortunately, in the current state of research, there is little hope that in the near future a machine will be able to automatically ﬁnd—or even engineer—the best kernel for a particular problem at hand [25]. [sent-12, score-0.131]
</p><p>8 However, by restricting to a less general problem, can we hope to achieve the automatic kernel selection? [sent-13, score-0.131]
</p><p>9 [18] it was shown that learning a support vector machine (SVM) [9] and a convex kernel combination at the same time is computationally feasible. [sent-15, score-0.131]
</p><p>10 , [10]) revealed further negative evidence and peaked in the provocative question “Can learning kernels help performance? [sent-22, score-0.096]
</p><p>11 ” posed by Corinna Cortes in an invited talk at ICML 2009 [5]. [sent-23, score-0.058]
</p><p>12 A ﬁrst step towards a model of kernel learning ∗ Marius Kloft is also with Friedrich Miescher Laboratory, Max Planck Society, T¨ bingen. [sent-25, score-0.131]
</p><p>13 33−norm MKL SVM  C  H  P  Z  S  V  L1  L4 L14 L30 SW1SW2  MKL experiment in terms of accuracy (L EFT) and kernel weights output  by MKL (R IGHT). [sent-47, score-0.131]
</p><p>14 that is useful for practical applications was made in [7, 13, 14]: by imposing an q -norm penalty (q > 1) rather than an 1 -norm one on the kernel combination coefﬁcients. [sent-48, score-0.151]
</p><p>15 This q -norm MKL is an empirical minimization algorithm that operates on the multi-kernel class consisting of functions f : x → w, φk (x) with w k ≤ D, where φk is the kernel mapping into the reproducing kernel Hilbert space (RKHS) Hk with kernel k and norm . [sent-49, score-0.445]
</p><p>16 k , while the kernel k itself ranges over the M  set of possible kernels k = m=1 θm km θ q ≤ 1, θ ≥ 0 . [sent-50, score-0.274]
</p><p>17 A conceptual milestone going back to the work of [1] and [20] is that this multi-kernel class can equivalently be represented as a block-norm regularized linear class in the product RKHS: Hp,D,M = fw : x → w, φ(x) w = (w(1) , . [sent-51, score-0.541]
</p><p>18 In Figure 1, we show exemplary results of an p -norm MKL experiment, achieved on the protein fold prediction dataset used in [4] (see supplementary material A for experimental details). [sent-55, score-0.056]
</p><p>19 We ﬁrst observe that, as expected, p -norm MKL enforces strong sparsity in the coefﬁcients θm when p = 1 and no sparsity at all otherwise (but various degrees of soft sparsity for intermediate p). [sent-56, score-0.188]
</p><p>20 Crucially, the performance (as measured by the test error) is not monotonic as a function of p; p = 1 (sparse MKL) yields the same performance as the regular SVM using a uniform kernel combination, but optimal performance is attained for some intermediate value of p—namely, p = 1. [sent-57, score-0.223]
</p><p>21 Clearly, the complexity of (1) will be greater than one that is based on a single kernel only. [sent-60, score-0.174]
</p><p>22 To this end, the main aim of this paper is to analyze the sample complexity of the hypothesis class (1). [sent-62, score-0.064]
</p><p>23 In the present work, we base our main analysis on the theory of local Rademacher complexities, which allows to derive improved and more precise rates of convergence that cover the whole range of p ∈ [1, ∞]. [sent-64, score-0.057]
</p><p>24 • A lower bound is shown that beside absolute constants matches the upper bounds, showing that our results are tight. [sent-67, score-0.112]
</p><p>25 • The generalization performance of p -norm MKL as guaranteed by the excess risk bound is studied for varying values of p, shedding light on the appropriateness of a small/large p in various learning scenarios. [sent-68, score-0.218]
</p><p>26 Furthermore, we also present a simpler, more general proof of the global Rademacher bound shown in [8] (at the expense of a slightly worse constant). [sent-69, score-0.156]
</p><p>27 A comparison of the rates obtained with local and global Rademacher analysis is carried out in Section 3. [sent-70, score-0.088]
</p><p>28 We denote the (normalized) kernel matrices corresponding to k and km by K and Km , respectively, i. [sent-73, score-0.197]
</p><p>29 We work with operators in Hilbert spaces and will use instead of the usual vector/matrix notation φ(x)φ(x) the tensor notation φ(x) ⊗ φ(x) ∈ HS(H), which is a Hilbert-Schmidt operator H → H deﬁned as (φ(x) ⊗ φ(x))u = φ(x), u φ(x). [sent-90, score-0.048]
</p><p>30 We denote by J = Eφ(x) ⊗ φ(x) and Jm = Eφm (x) ⊗ φm (x) the uncentered covariance operators corresponding to variables φ(x) and φm (x), respectively; it holds that 2 2 tr(J) = E φ(x) 2 and tr(Jm ) = E φm (x) 2 . [sent-92, score-0.095]
</p><p>31 Global Rademacher Complexities We ﬁrst review global Rademacher complexities (GRC) in multiple kernel learning. [sent-93, score-0.203]
</p><p>32 The global Rademacher n 1 complexity is deﬁned as R(Hp ) = E supfw ∈Hp w, n i=1 σi φ(xi ) , where (σi )1≤i≤n is an i. [sent-101, score-0.124]
</p><p>33 This bound is tight and improves a series of loose results that were given for p = 1 in the past (see [8] and references therein). [sent-110, score-0.085]
</p><p>34 In fact, the above result can be extended to the whole range of p ∈ [1, ∞] (in the supplementary material we present a quite simple proof using c = 1): Proposition 1 (G LOBAL R ADEMACHER COMPLEXITY BOUND ). [sent-111, score-0.075]
</p><p>35 For any p ≥ 1 the empirical version of global Rademacher complexity of the multi-kernel class Hp can be bounded as R(Hp ) ≤ min D t∈[p,∞]  t∗ n  1 tr(Km ) n  M m=1  t∗ 2  . [sent-112, score-0.131]
</p><p>36 Interestingly, the above GRC bound is not monotonic in p and thus the minimum is not always attained for t := p. [sent-113, score-0.13]
</p><p>37 We deﬁne the local Rademacher comn 1 2 plexity (LRC) of Hp as Rr (Hp ) = E supfw ∈Hp :P fw ≤r w, n i=1 σi φ(xi ) , where P fw := 2 E(fw (φ(x)))2 . [sent-121, score-1.082]
</p><p>38 We will also use the following assumption in the bounds for the case p ∈ [1, 2]: Assumption (U) (no-correlation). [sent-123, score-0.042]
</p><p>39 For example, if X = RM , the above means that the input variable x ∈ X has independent coordinates, and the kernels k1 , . [sent-129, score-0.077]
</p><p>40 To state the bounds, note that covariance operators enjoy ∞ discrete eigenvalue-eigenvector decompositions J = Eφ(x) ⊗ φ(x) = j=1 λj uj ⊗ uj and ∞  (m)  (m)  (m)  (m)  Jm = Ex(m) ⊗ x(m) = j=1 λj uj ⊗ uj , where (uj )j≥1 and (uj )j≥1 form orthonormal bases of H and Hm , respectively. [sent-134, score-0.761]
</p><p>41 Assume that the kernels are uniformly bounded ( k ∞ ≤ B < ∞) and that Assumption (U) holds. [sent-136, score-0.077]
</p><p>42 The local Rademacher complexity of the multi-kernel class Hp can be bounded for any p ∈ [1, 2] as √ 1 ∞ M 16 BeDM t∗ t∗ 1− t2 2 t∗ 2 λ(m) ∗ min rM , ceD + . [sent-137, score-0.134]
</p><p>43 Rr (Hp ) ≤ min j ∗ n n t∈[p,2] m=1 t j=1 2  3  Theorem 3 (L OCAL R ADEMACHER COMPLEXITY BOUND , p ∈ [2, ∞] ). [sent-138, score-0.036]
</p><p>44 For any p ∈ [2, ∞], 2 n  Rr (Hp ) ≤ min  t∈[p,∞]  ∞ 2  min(r, D2 M t∗ −1 λj ). [sent-139, score-0.036]
</p><p>45 j=1  It is interesting to compare the above bounds for the special case p = 2 with the ones of Bartlett et al. [sent-140, score-0.042]
</p><p>46 The main term of the bound of Theorem 3 (taking t = p = 2) is then essentially determined by (m) 1/2 M ∞ 1 . [sent-142, score-0.085]
</p><p>47 It is also interesting to study the case p = 1: by using t = (log(M ))∗ in Theorem 2, we obtain the ∞  M  (m)  √  1/2  3 2  3 2 2 bound Rr (H1 ) ≤ 16 + Be D log(M ) , for j=1 min rM, e D (log M ) λj n n m=1 ∞ 2 all M ≥ e . [sent-144, score-0.121]
</p><p>48 the proof of Theorem 3 is straightforward and shown in the supplementary material C. [sent-146, score-0.075]
</p><p>49 In order to exploit the no-correlation assumption, we will work in large parts of the proof with the centered class Hp = fw w 2,p ≤ D , wherein fw : x → w, φ(x) , and φ(x) := φ(x) − Eφ(x). [sent-151, score-1.11]
</p><p>50 (2)  1 p∗  p∗ 2  m=1  M  Jensen  Eφm (x), Eφm (x)  =  m=1  2  p∗ 2  E φm (x), φm (x)  1 p∗  M  =  tr(Jm )  m=1  m=1  p∗ 2  (3)  so that we can express the complexity of the centered class in terms of the uncentered one as follows: n n 1 1 Rr (Hp ) ≤ E sup w, σi φ(xi ) + E sup w, σi Eφ(x) . [sent-153, score-0.269]
</p><p>51 n i=1 n i=1 fw ∈Hp , fw ∈Hp , 2 P fw ≤r  2 P fw ≤r  2 2 Concerning the ﬁrst term of the above upper bound, using (2) we have P fw ≤ P fw , and thus  E sup  w,  fw ∈Hp , 2 P fw ≤r  1 n  n  σi φ(xi ) ≤ E sup  w,  fw ∈Hp , 2 P fw ≤r  i=1  1 n  n  σi φ(xi ) = Rr (Hp ). [sent-154, score-5.129]
</p><p>52 i=1  Now to bound the second term, we write n √ 1 E sup w, σi Eφ(x) ≤ n sup w, Eφ(x) . [sent-155, score-0.197]
</p><p>53 n i=1 fw ∈Hp , fw ∈Hp , 2 P fw ≤r  2 P fw ≤r  Now observe that we have w, Eφ(x)  H¨ lder o  ≤  (3)  w  2,p  Eφ(x)  2,p∗  ≤ w  2,p  tr(Jm )  M m=1  p∗ 2  2 P fw . [sent-156, score-2.528]
</p><p>54 (4)  as well as w, Eφ(x) = Efw (x) ≤  2  This shows that there is no loss in working with the centered class instead of the uncentered one. [sent-158, score-0.114]
</p><p>55 First we note that, since the (centered) covariance operator Eφm (x) ⊗ φm (x) is also a self-adjoint Hilbert-Schmidt operator on Hm , there (m) (m) (m) (m) ∞ exists an eigendecomposition Eφm (x) ⊗ φm (x) = j=1 λj uj ⊗ uj , wherein (uj )j≥1 is an orthogonal basis of Hm . [sent-161, score-0.423]
</p><p>56 As a consequence, for all j and m, M 2 P fw  1 n  E  wm , φm (x) m=1  n  2  (m)  σi φm (xi ), uj i=1  (m)  =  2  (m)  λj  w m , uj  m=1 j=1  n  1 (m) 1 u , n j n  =  ∞  M  2  E(fw (x))2 = E  =  (5) (m)  (m)  E φm (xi ) ⊗ φm (xi ) uj  λj . [sent-163, score-1.048]
</p><p>57 We can express the LRC in terms of the eigendecompositon as follows Rr (Hp ) = E  sup  1 n  w,  2 fw ∈Hp :P fw ≤r  M  C. [sent-168, score-1.054]
</p><p>58 2 in the supplementary material) to further bound the right  ≤  p∗  M m=1  n  σi φm (xi ), uj p∗ 2  (m) 2  n i=1  1 j>hm n  E  (m)  n i=1  1 j>hm n  term in the above expression as E  φm (xi ), uj  1 p∗  (m)  uj  M m=1 2,p∗  . [sent-175, score-0.662]
</p><p>59 Note that for p ≥ 2 it holds that  ∗  p /2 ≤ 1, and thus it sufﬁces to employ Jensen’s inequality once again to move the expectation ∗ operator inside the inner term. [sent-176, score-0.066]
</p><p>60 Thus 5  by the subadditivity of the root function hm  n M m=1  r  ≤  M m=1  r  Rr (Hp ) ≤  hm  n  BM p∗ + n  M (m)  λj  m=1  j=hm +1  ∞  ep∗ 2 D2 n  +  ∞  2  ep∗ 2 n  +D  √  M (m)  λj  + p∗ 2  m=1  j=hm +1  p∗ 2 1  BeDM p∗ p∗ . [sent-180, score-0.779]
</p><p>61 Thus the following preliminary bound  ∞  4ep∗ 2 D2 n  are 1/2  M m=1  hm is nonzero) so that in any case we get n− 2 M m=1  hm  all  √  M (m) λj  + m=1  j=hm +1  p∗ 2  1  BeDM p∗ p∗ , n  (8)  for all nonnegative integers hm ≥ 0. [sent-183, score-1.28]
</p><p>62 (11)  Since the above holds for all nonnegative integers hm , the result follows, completing the proof. [sent-187, score-0.471]
</p><p>63 1  Lower and Excess Risk Bounds  To investigate the tightness of the presented upper bounds on the LRC of Hp , we consider the case where φ1 (x), . [sent-189, score-0.069]
</p><p>64 This shows that the upper bounds of the previous section are tight. [sent-208, score-0.069]
</p><p>65 As an application of our results to prediction problems such as classiﬁcation or regression, we also n 1 ˆ bound the excess loss of empirical minimization, f := argminf n i=1 l(f (xi ), yi ), w. [sent-209, score-0.21]
</p><p>66 to a loss ˆ function l: P (lf − lf ∗ ) := E l(f (x), y) − E l(f ∗ (x), y), where f ∗ := argminf E l(f (x), y) . [sent-212, score-0.171]
</p><p>67 [2] to show the following excess risk bound under the assump(m) tion of algebraically decreasing eigenvalues of the kernel matrices, i. [sent-214, score-0.349]
</p><p>68 ∃d > 0, α > 1, ∀m : λj ≤ −α dj (proof shown in the supplementary material E): (m)  Theorem 5. [sent-216, score-0.056]
</p><p>69 Let l be a Lipschitz continuous loss with constant L and assume there is a positive constant F such that ∀f ∈ F : P (f − f ∗ )2 ≤ F P (lf − lf ∗ ). [sent-218, score-0.138]
</p><p>70 Then for all z > 0 with probability at least 1 − e−z the excess loss of the multi-kernel class Hp can be bounded for p ∈ [1, 2] as P (lf − lf ∗ ) ≤ min ˆ  186  t∈[p,2]  3−α dD2 L2 t∗ 2 1−α  1 1+α  α−1  2  F α+1 M 1+ 1+α  1 t∗  −1  α  n− 1+α  √ 1 1 47 BDLM t∗ t∗ (22BDLM t∗ + 27F )z + + . [sent-219, score-0.287]
</p><p>71 n n 1  1  We see from the above bound that convergence can be almost as slow as O p∗ M p∗ n− 2 (if α ≈ 1 is small ) and almost as fast as O n−1 (if α is large). [sent-220, score-0.085]
</p><p>72 3  Interpretation of Bounds  In this section, we discuss the rates of Theorem 5 obtained by local analysis bounds, that is ∀t ∈ [p, 2] :  P (lf − lf ∗ ) = O ˆ  t∗ D  2 1+α  2  M 1+ 1+α  1 t∗  −1  α  n− 1+α . [sent-221, score-0.195]
</p><p>73 (13)  On the other hand, the global Rademacher complexity directly leads to a bound of the form [8] 1 1 (14) ∀t ∈ [p, 2] : P (lf − lf ∗ ) = O t∗ DM t∗ n− 2 . [sent-222, score-0.297]
</p><p>74 Clearly, the rate obtained through local analysis is better in n since α > 1. [sent-224, score-0.054]
</p><p>75 Regarding the rate in the number of kernels M and the radius D, a straightforward calculation shows that the local analysis improves √ 1 over the global one whenever M p /D = O( n) . [sent-225, score-0.162]
</p><p>76 Second, if p ≤ (log M )∗ , the best choice in (13) and (14) is t = (log M )∗ so that 1  1  (15) P (lf − lf ∗ ) ≤ O min M n−1 , min t∗ DM t∗ n− 2 ˆ t∈[p,2] √ M and the phase transition occurs for D log M = O( n). [sent-229, score-0.228]
</p><p>77 This situation is to be compared to the sharp analysis of the optimal convergence rate of convex aggregation of M functions obtained by [27] in the framework of squared error loss regression, which is shown to be 1/2  1 M O min M , n log √n . [sent-231, score-0.097]
</p><p>78 This corresponds to the setting studied here with D = 1, p = 1 n and α → ∞, and we see that our bound recovers (up to log factors) in this case this sharp bound and the related phase transition phenomenon. [sent-232, score-0.21]
</p><p>79 (5), Assumption (U)—a similar assumption was also used in [23]—can be relaxed to a more general, RIP-like assumption as used in [16]; this comes at the expense of an additional factor in the bounds (details omitted here). [sent-234, score-0.063]
</p><p>80 As a practical application of the presented bounds, we analyze the impact of the norm-parameter p on the accuracy of p -norm MKL in various learning scenarios, showing why an intermediate p often turns out to be optimal in practical applications. [sent-236, score-0.087]
</p><p>81 As indicated in the introduction, there is empirical evidence that the performance of p -norm MKL crucially depends on the choice of the norm parameter p (for example, cf. [sent-237, score-0.05]
</p><p>82 Figure 1 7  60 50 40  bound  30 20  bound  40 45 50 55 60 65  110 90  bound  60  70  80  w*  w*  w*  1. [sent-238, score-0.255]
</p><p>83 5  Figure 2: Illustration of the three analyzed learning scenarios (T OP) differing in their soft sparsity of the  Bayes hypothesis w∗ (parametrized by β) and corresponding values of the bound factor νt as a function of p (B OTTOM). [sent-257, score-0.162]
</p><p>84 A soft sparse (L EFT), a intermediate non-sparse (C ENTER), and an almost uniform w∗ (R IGHT). [sent-258, score-0.092]
</p><p>85 To start with, ﬁrst note that the choice of p only affects the excess risk bound in the factor (cf. [sent-261, score-0.218]
</p><p>86 Plugging in this value for Dp , the bound factor νp becomes νp := w∗  2 1+α  p  2  2  min t∗ 1+α M 1+ 1+α  1 t∗  −1  . [sent-270, score-0.121]
</p><p>87 Note that the soft sparsity of w∗ is increased from the left hand to the right hand side. [sent-275, score-0.077]
</p><p>88 2, while for the intermediate case (C ENTER) p = 1. [sent-277, score-0.047]
</p><p>89 This means that if the true Bayes hypothesis has an intermediately dense representation (which is frequently encountered in practical applications), our bound gives the strongest generalization guarantees to p -norm MKL using an intermediate choice of p. [sent-281, score-0.173]
</p><p>90 4  Conclusion  We derived a sharp upper bound on the local Rademacher complexity of p -norm multiple kernel learning. [sent-282, score-0.342]
</p><p>91 We also proved a lower bound that matches the upper one and shows that our result is tight. [sent-283, score-0.112]
</p><p>92 Using the local Rademacher complexity bound, we derived an excess risk bound that attains α the fast rate of O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. [sent-284, score-0.383]
</p><p>93 In a practical case study, we found that the optimal value of that bound depends on the true Bayesoptimal kernel weights. [sent-285, score-0.236]
</p><p>94 If the true weights exhibit soft sparsity but are not strongly sparse, then the generalization bound is minimized for an intermediate p. [sent-286, score-0.209]
</p><p>95 This is not only intuitive but also supports empirical studies showing that sparse MKL (p = 1) rarely works in practice, while some intermediate choice of p can improve performance. [sent-287, score-0.047]
</p><p>96 Multiple kernel learning, conic duality, and the SMO algorithm. [sent-303, score-0.131]
</p><p>97 Let the kernel ﬁgure it out: Principled learning of pre-processing for kernel classiﬁers. [sent-373, score-0.262]
</p><p>98 The best constant in the rosenthal inequality for nonnegative random variables. [sent-378, score-0.104]
</p><p>99 Local Rademacher complexities and oracle inequalities in risk minimization. [sent-412, score-0.082]
</p><p>100 Minimax-optimal rates for sparse additive models over kernel classes via convex programming. [sent-472, score-0.154]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fw', 0.499), ('hp', 0.456), ('hm', 0.375), ('mkl', 0.298), ('uj', 0.183), ('rademacher', 0.169), ('rr', 0.141), ('lf', 0.138), ('kernel', 0.131), ('jm', 0.106), ('excess', 0.092), ('bound', 0.085), ('efw', 0.083), ('kloft', 0.081), ('kernels', 0.077), ('tep', 0.073), ('bedm', 0.067), ('lrc', 0.067), ('km', 0.066), ('cortes', 0.057), ('sup', 0.056), ('tr', 0.054), ('centered', 0.053), ('ep', 0.053), ('ademacher', 0.05), ('eft', 0.05), ('marius', 0.05), ('supfw', 0.05), ('intermediate', 0.047), ('soft', 0.045), ('ight', 0.044), ('rosenthal', 0.044), ('complexity', 0.043), ('bounds', 0.042), ('complexities', 0.041), ('risk', 0.041), ('rm', 0.04), ('uncentered', 0.04), ('lanckriet', 0.04), ('nonnegative', 0.039), ('xi', 0.039), ('sonnenburg', 0.038), ('jensen', 0.037), ('min', 0.036), ('local', 0.034), ('bartlett', 0.034), ('argminf', 0.033), ('elating', 0.033), ('grc', 0.033), ('invited', 0.033), ('ocal', 0.033), ('ounding', 0.033), ('ller', 0.033), ('lder', 0.033), ('sch', 0.032), ('sparsity', 0.032), ('norm', 0.031), ('hs', 0.031), ('integers', 0.031), ('global', 0.031), ('mohri', 0.03), ('subadditivity', 0.029), ('aq', 0.029), ('spec', 0.029), ('operators', 0.029), ('hilbert', 0.028), ('material', 0.028), ('theorem', 0.028), ('supplementary', 0.028), ('svm', 0.027), ('upper', 0.027), ('blanchard', 0.027), ('brefeld', 0.027), ('holds', 0.026), ('talk', 0.025), ('spectra', 0.025), ('gilles', 0.025), ('eigenvalue', 0.025), ('attained', 0.024), ('decay', 0.023), ('tsch', 0.023), ('rates', 0.023), ('dp', 0.023), ('sharp', 0.022), ('monotonic', 0.021), ('bm', 0.021), ('class', 0.021), ('frequently', 0.021), ('expense', 0.021), ('inequality', 0.021), ('rate', 0.02), ('practical', 0.02), ('wherein', 0.019), ('proof', 0.019), ('operator', 0.019), ('aggregation', 0.019), ('rkhs', 0.019), ('evidence', 0.019), ('phase', 0.018), ('enter', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="286-tfidf-1" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><p>2 0.32804143 <a title="286-tfidf-2" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>Author: Taiji Suzuki</p><p>Abstract: In this paper, we give a new generalization error bound of Multiple Kernel Learning (MKL) for a general class of regularizations. Our main target in this paper is dense type regularizations including ℓp -MKL that imposes ℓp -mixed-norm regularization instead of ℓ1 -mixed-norm regularization. According to the recent numerical experiments, the sparse regularization does not necessarily show a good performance compared with dense type regularizations. Motivated by this fact, this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary mixed-norm-type regularizations in a unifying manner. As a by-product of our general result, we show a fast learning rate of ℓp -MKL that is tightest among existing bounds. We also show that our general learning rate achieves the minimax lower bound. Finally, we show that, when the complexities of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type regularization shows better learning rate compared with sparse ℓ1 regularization. 1</p><p>3 0.2284397 <a title="286-tfidf-3" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>4 0.11776754 <a title="286-tfidf-4" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>Author: Iasonas Kokkinos</p><p>Abstract: In this work we use Branch-and-Bound (BB) to efﬁciently detect objects with deformable part models. Instead of evaluating the classiﬁer score exhaustively over image locations and scales, we use BB to focus on promising image locations. The core problem is to compute bounds that accommodate part deformations; for this we adapt the Dual Trees data structure [7] to our problem. We evaluate our approach using Mixture-of-Deformable Part Models [4]. We obtain exactly the same results but are 10-20 times faster on average. We also develop a multiple-object detection variation of the system, where hypotheses for 20 categories are inserted in a common priority queue. For the problem of ﬁnding the strongest category in an image this results in a 100-fold speedup.</p><p>5 0.10455104 <a title="286-tfidf-5" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>Author: Jun Wang, Huyen T. Do, Adam Woznica, Alexandros Kalousis</p><p>Abstract: Metric learning has become a very active research ﬁeld. The most popular representative–Mahalanobis metric learning–can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist. However, the problem then becomes ﬁnding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predeﬁned kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly, and despite the extensive work on multiple kernel learning for SVMs, there has been no work in the area of metric learning with multiple kernel learning. In this paper we ﬁll this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection. 1</p><p>6 0.084526099 <a title="286-tfidf-6" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>7 0.063565947 <a title="286-tfidf-7" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>8 0.063459143 <a title="286-tfidf-8" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>9 0.062291425 <a title="286-tfidf-9" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>10 0.061237551 <a title="286-tfidf-10" href="./nips-2011-An_Application_of_Tree-Structured_Expectation_Propagation_for_Channel_Decoding.html">31 nips-2011-An Application of Tree-Structured Expectation Propagation for Channel Decoding</a></p>
<p>11 0.056812644 <a title="286-tfidf-11" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>12 0.055410855 <a title="286-tfidf-12" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>13 0.054051738 <a title="286-tfidf-13" href="./nips-2011-Optimal_learning_rates_for_least_squares_SVMs_using_Gaussian_kernels.html">207 nips-2011-Optimal learning rates for least squares SVMs using Gaussian kernels</a></p>
<p>14 0.053585127 <a title="286-tfidf-14" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>15 0.05329347 <a title="286-tfidf-15" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>16 0.048453379 <a title="286-tfidf-16" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>17 0.048153669 <a title="286-tfidf-17" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>18 0.048132118 <a title="286-tfidf-18" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>19 0.044653263 <a title="286-tfidf-19" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>20 0.044302151 <a title="286-tfidf-20" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, -0.034), (2, -0.05), (3, -0.109), (4, -0.045), (5, 0.067), (6, 0.047), (7, -0.081), (8, 0.02), (9, 0.051), (10, -0.162), (11, 0.16), (12, 0.125), (13, 0.158), (14, -0.105), (15, 0.015), (16, 0.097), (17, 0.018), (18, -0.017), (19, 0.229), (20, -0.03), (21, -0.205), (22, -0.119), (23, -0.007), (24, 0.05), (25, -0.03), (26, -0.047), (27, -0.01), (28, 0.183), (29, 0.027), (30, -0.164), (31, -0.175), (32, -0.059), (33, 0.072), (34, -0.067), (35, 0.076), (36, -0.113), (37, 0.038), (38, 0.007), (39, 0.044), (40, -0.047), (41, 0.079), (42, 0.006), (43, -0.004), (44, 0.079), (45, -0.02), (46, 0.019), (47, 0.028), (48, 0.097), (49, -0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94531029 <a title="286-lsi-1" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><p>2 0.88941371 <a title="286-lsi-2" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>Author: Taiji Suzuki</p><p>Abstract: In this paper, we give a new generalization error bound of Multiple Kernel Learning (MKL) for a general class of regularizations. Our main target in this paper is dense type regularizations including ℓp -MKL that imposes ℓp -mixed-norm regularization instead of ℓ1 -mixed-norm regularization. According to the recent numerical experiments, the sparse regularization does not necessarily show a good performance compared with dense type regularizations. Motivated by this fact, this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary mixed-norm-type regularizations in a unifying manner. As a by-product of our general result, we show a fast learning rate of ℓp -MKL that is tightest among existing bounds. We also show that our general learning rate achieves the minimax lower bound. Finally, we show that, when the complexities of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type regularization shows better learning rate compared with sparse ℓ1 regularization. 1</p><p>3 0.69598824 <a title="286-lsi-3" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>4 0.42857859 <a title="286-lsi-4" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>Author: Jun Wang, Huyen T. Do, Adam Woznica, Alexandros Kalousis</p><p>Abstract: Metric learning has become a very active research ﬁeld. The most popular representative–Mahalanobis metric learning–can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist. However, the problem then becomes ﬁnding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predeﬁned kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly, and despite the extensive work on multiple kernel learning for SVMs, there has been no work in the area of metric learning with multiple kernel learning. In this paper we ﬁll this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection. 1</p><p>5 0.42529973 <a title="286-lsi-5" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<p>Author: Kenji Fukumizu, Gert R. Lanckriet, Bharath K. Sriperumbudur</p><p>Abstract: The goal of this paper is to investigate the advantages and disadvantages of learning in Banach spaces over Hilbert spaces. While many works have been carried out in generalizing Hilbert methods to Banach spaces, in this paper, we consider the simple problem of learning a Parzen window classiﬁer in a reproducing kernel Banach space (RKBS)—which is closely related to the notion of embedding probability measures into an RKBS—in order to carefully understand its pros and cons over the Hilbert space classiﬁer. We show that while this generalization yields richer distance measures on probabilities compared to its Hilbert space counterpart, it however suffers from serious computational drawback limiting its practical applicability, which therefore demonstrates the need for developing efﬁcient learning algorithms in Banach spaces.</p><p>6 0.39549583 <a title="286-lsi-6" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>7 0.36935413 <a title="286-lsi-7" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>8 0.33564201 <a title="286-lsi-8" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>9 0.31352693 <a title="286-lsi-9" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>10 0.31205451 <a title="286-lsi-10" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>11 0.27855071 <a title="286-lsi-11" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>12 0.2766338 <a title="286-lsi-12" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>13 0.27504674 <a title="286-lsi-13" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>14 0.25261921 <a title="286-lsi-14" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>15 0.25252241 <a title="286-lsi-15" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>16 0.24588923 <a title="286-lsi-16" href="./nips-2011-Optimal_learning_rates_for_least_squares_SVMs_using_Gaussian_kernels.html">207 nips-2011-Optimal learning rates for least squares SVMs using Gaussian kernels</a></p>
<p>17 0.23909184 <a title="286-lsi-17" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>18 0.23824403 <a title="286-lsi-18" href="./nips-2011-The_Fast_Convergence_of_Boosting.html">282 nips-2011-The Fast Convergence of Boosting</a></p>
<p>19 0.23061235 <a title="286-lsi-19" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>20 0.22613801 <a title="286-lsi-20" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (4, 0.047), (17, 0.287), (20, 0.039), (26, 0.042), (31, 0.063), (33, 0.015), (43, 0.086), (45, 0.113), (48, 0.024), (57, 0.024), (65, 0.062), (74, 0.036), (83, 0.025), (99, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79384774 <a title="286-lda-1" href="./nips-2011-Advice_Refinement_in_Knowledge-Based_SVMs.html">27 nips-2011-Advice Refinement in Knowledge-Based SVMs</a></p>
<p>Author: Gautam Kunapuli, Richard Maclin, Jude W. Shavlik</p><p>Abstract: Knowledge-based support vector machines (KBSVMs) incorporate advice from domain experts, which can improve generalization signiﬁcantly. A major limitation that has not been fully addressed occurs when the expert advice is imperfect, which can lead to poorer models. We propose a model that extends KBSVMs and is able to not only learn from data and advice, but also simultaneously improves the advice. The proposed approach is particularly effective for knowledge discovery in domains with few labeled examples. The proposed model contains bilinear constraints, and is solved using two iterative approaches: successive linear programming and a constrained concave-convex approach. Experimental results demonstrate that these algorithms yield useful reﬁnements to expert advice, as well as improve the performance of the learning algorithm overall.</p><p>same-paper 2 0.75348407 <a title="286-lda-2" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><p>3 0.67389423 <a title="286-lda-3" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>Author: Arthur D. Szlam, Karol Gregor, Yann L. Cun</p><p>Abstract: This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modiﬁcations of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efﬁcient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-speciﬁed. 1</p><p>4 0.5405398 <a title="286-lda-4" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>5 0.53127521 <a title="286-lda-5" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>6 0.53018838 <a title="286-lda-6" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>7 0.52363783 <a title="286-lda-7" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>8 0.5205344 <a title="286-lda-8" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>9 0.5187574 <a title="286-lda-9" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>10 0.51829642 <a title="286-lda-10" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>11 0.5178104 <a title="286-lda-11" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>12 0.5173642 <a title="286-lda-12" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>13 0.51714158 <a title="286-lda-13" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<p>14 0.51697624 <a title="286-lda-14" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>15 0.51599509 <a title="286-lda-15" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>16 0.51567721 <a title="286-lda-16" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>17 0.51513082 <a title="286-lda-17" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>18 0.51434124 <a title="286-lda-18" href="./nips-2011-On_fast_approximate_submodular_minimization.html">199 nips-2011-On fast approximate submodular minimization</a></p>
<p>19 0.51339281 <a title="286-lda-19" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>20 0.51337802 <a title="286-lda-20" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
