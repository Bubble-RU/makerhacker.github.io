<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-238" href="#">nips2011-238</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</h1>
<br/><p>Source: <a title="nips-2011-238-pdf" href="http://papers.nips.cc/paper/4254-relative-density-ratio-estimation-for-robust-distribution-comparison.pdf">pdf</a></p><p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>Reference: <a title="nips-2011-238-reference" href="../nips2011_reference/nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. [sent-17, score-0.383]
</p><p>2 In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. [sent-18, score-0.191]
</p><p>3 Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. [sent-19, score-0.174]
</p><p>4 Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. [sent-20, score-0.836]
</p><p>5 , outlier detection [1, 2], two-sample homogeneity test [3, 4], and transfer learning [5, 6]. [sent-25, score-0.439]
</p><p>6 A standard approach to comparing probability densities p(x) and p (x) would be to estimate a divergence from p(x) to p (x), such as the Kullback-Leibler (KL) divergence [7]: KL[p(x), p (x)] := Ep(x) [log r(x)] , r(x) := p(x)/p (x), where Ep(x) denotes the expectation over p(x). [sent-26, score-0.755]
</p><p>7 A naive way to estimate the KL divergence is to separately approximate the densities p(x) and p (x) from data and plug the estimated densities in the above deﬁnition. [sent-27, score-0.436]
</p><p>8 However, since density estimation is known to be a hard task [8], this approach does not work well unless a good parametric model is available. [sent-28, score-0.114]
</p><p>9 Recently, a divergence estimation approach which directly approximates the density-ratio r(x) without going through separate approximation of densities p(x) and p (x) has been proposed [9, 10]. [sent-29, score-0.479]
</p><p>10 Such density-ratio approximation methods were proved to achieve the optimal non-parametric convergence rate in the mini-max sense. [sent-30, score-0.087]
</p><p>11 However, the KL divergence estimation via density-ratio approximation is computationally rather expensive due to the non-linearity introduced by the ‘log’ term. [sent-31, score-0.412]
</p><p>12 To cope with this problem, another divergence called the Pearson (PE) divergence [11] is useful. [sent-32, score-0.716]
</p><p>13 The PE divergence is deﬁned as PE[p(x), p (x)] := 1 Ep (x) (r(x) − 1)2 . [sent-33, score-0.358]
</p><p>14 2 1  The PE divergence is a squared-loss variant of the KL divergence, and they both belong to the class of the Ali-Silvey-Csisz´ r divergences (which is also known as the f -divergences, see [12, 13]). [sent-34, score-0.396]
</p><p>15 The practical usefulness of the uLSIF-based PE divergence estimator was demonstrated in various applications such as outlier detection [2], twosample homogeneity test [4], and dimensionality reduction [15]. [sent-39, score-0.919]
</p><p>16 In this paper, we ﬁrst establish the non-parametric convergence rate of the uLSIF-based PE divergence estimator, which elucidates its superior theoretical properties. [sent-40, score-0.416]
</p><p>17 This implies that, in the region where the denominator density p (x) takes small values, the density-ratio r(x) = p(x)/p (x) tends to take large values and therefore the overall convergence speed becomes slow. [sent-42, score-0.156]
</p><p>18 This makes the paradigm of divergence estimation based on density-ratio approximation unreliable. [sent-46, score-0.412]
</p><p>19 In order to overcome this fundamental problem, we propose an alternative approach to distribution comparison called α-relative divergence estimation. [sent-47, score-0.358]
</p><p>20 In the proposed approach, we estimate the α-relative divergence, which is the divergence from p(x) to the α-mixture density: qα (x) = αp(x) + (1 − α)p (x) for 0 ≤ α < 1. [sent-48, score-0.386]
</p><p>21 For example, the α-relative PE divergence is given by PEα [p(x), p (x)] := PE[p(x), qα (x)] = 1 Eqα (x) (rα (x) − 1)2 , 2  (1)  where rα (x) is the α-relative density-ratio of p(x) and p (x): rα (x) := p(x)/qα (x) = p(x)/ αp(x) + (1 − α)p (x) . [sent-49, score-0.358]
</p><p>22 (2)  We propose to estimate the α-relative divergence by direct approximation of the α-relative densityratio. [sent-50, score-0.419]
</p><p>23 Based on this feature, we theoretically show that the α-relative PE divergence estimator based on α-relative density-ratio approximation is more favorable than the ordinary density-ratio approach in terms of the non-parametric convergence speed. [sent-52, score-0.581]
</p><p>24 We further prove that, under a correctly-speciﬁed parametric setup, the asymptotic variance of our α-relative PE divergence estimator does not depend on the model complexity. [sent-53, score-0.625]
</p><p>25 This means that the proposed α-relative PE divergence estimator hardly overﬁts even with complex models. [sent-54, score-0.541]
</p><p>26 Through experiments on outlier detection, two-sample homogeneity test, and transfer learning, we demonstrate that our proposed α-relative PE divergence estimator compares favorably with alternative approaches. [sent-55, score-0.911]
</p><p>27 ) samples {xi }n from i=1 a d-dimensional distribution P with density p(x) and i. [sent-59, score-0.092]
</p><p>28 samples {xj }n from another dj=1 dimensional distribution P with density p (x). [sent-62, score-0.092]
</p><p>29 Our goal is to compare the two underlying distributions P and P only using the two sets of samples {xi }n and {xj }n . [sent-63, score-0.082]
</p><p>30 i=1 j=1 In this section, we give a method for estimating the α-relative PE divergence based on direct approximation of the α-relative density-ratio. [sent-64, score-0.447]
</p><p>31 Finally, a density-ratio estimator is given as rα (x) := g(x; θ) =  n =1  θ K(x, x ). [sent-76, score-0.11]
</p><p>32 When α = 0, the above method is reduced to a direct density-ratio estimator called unconstrained least-squares importance ﬁtting (uLSIF) [14]. [sent-77, score-0.196]
</p><p>33 The performance of RuLSIF depends on the choice of the kernel function (the kernel width in the case of the Gaussian kernel) and the regularization parameter λ. [sent-80, score-0.086]
</p><p>34 Using an estimator of the α-relative density-ratio rα (x), we can construct estimators of the αrelative PE divergence (1). [sent-82, score-0.503]
</p><p>35 After a few lines of calculation, we can show that the α-relative PE divergence (1) is equivalently expressed as PEα = − α Ep(x) rα (x)2 − 2  (1−α) 2 Ep (x)  rα (x)2 + Ep(x) [rα (x)] −  1 2  = 1 Ep(x) [rα (x)] − 1 . [sent-83, score-0.358]
</p><p>36 2 2  Note that the middle expression can also be obtained via Legendre-Fenchel convex duality of the divergence functional [17]. [sent-84, score-0.358]
</p><p>37 2  +  1 n  n i=1 rα (xi )  − 1, 2  (4) (5)  We note that the α-relative PE divergence (1) can have further different expressions than the above ones, and corresponding estimators can also be constructed similarly. [sent-86, score-0.393]
</p><p>38 However, the above two expressions will be particularly useful: the ﬁrst estimator PEα has superior theoretical properties (see Section 3) and the second one PEα is simple to compute. [sent-87, score-0.11]
</p><p>39 3 Theoretical Analysis In this section, we analyze theoretical properties of the proposed PE divergence estimators. [sent-88, score-0.386]
</p><p>40 3  For theoretical analysis, let us consider a rather abstract form of our relative density-ratio estimator described as argming∈G  α 2n  n i=1  g(xi )2 +  n j=1  (1−α) 2n  g(xj )2 −  n i=1  1 n  g(xi ) + λ R(g)2 , 2  (6)  where G is some function space (i. [sent-90, score-0.172]
</p><p>41 Non-Parametric Convergence Analysis: First, we elucidate the non-parametric convergence rate of the proposed PE estimators. [sent-93, score-0.086]
</p><p>42 Here, we practically regard the function space G as an inﬁnitedimensional reproducing kernel Hilbert space (RKHS) [18] such as the Gaussian kernel space, and R(·) as the associated RKHS norm. [sent-94, score-0.086]
</p><p>43 We analyze the convergence rate of our PE divergence estimators as n := min(n, n ) tends to inﬁnity ¯ for λ = λn under ¯ λn → o(1) and λ−1 = o(¯ 2/(2+γ) ). [sent-96, score-0.484]
</p><p>44 , the ﬁrst terms) of the asymptotic convergence rates become smaller as rα ∞ gets smaller. [sent-103, score-0.109]
</p><p>45 Since rα  ∞  =  α + (1 − α)/r(x)  −1 ∞  <  1 α  for α > 0,  larger α would be more preferable in terms of the asymptotic approximation error. [sent-104, score-0.102]
</p><p>46 Thus, our proposed approach of estimating the α-relative PE divergence (with α > 0) would be more advantageous than the naive approach of estimating the plain PE divergence (which corresponds to α = 0) in terms of the non-parametric convergence rate. [sent-106, score-0.895]
</p><p>47 The above results also show that PEα and PEα have different asymptotic convergence rates. [sent-107, score-0.109]
</p><p>48 4  Parametric Variance Analysis: Next, we analyze the asymptotic variance of the PE divergence estimator PEα (4) under a parametric setup. [sent-124, score-0.625]
</p><p>49 Let us denote the variance of PEα (4) by V[PEα ], where randomness comes from the draw of samples {xi }n and {xj }n . [sent-134, score-0.087]
</p><p>50 (9) shows that, up to O(n−1 , n −1 ), the variance of PEα depends only on the true relative density-ratio rα (x), not on the estimator of rα (x). [sent-139, score-0.203]
</p><p>51 Therefore, overﬁtting would hardly occur in the estimation of the relative PE divergence even when complex models are used. [sent-141, score-0.49]
</p><p>52 We note that the above superior property is applicable only to relative PE divergence estimation, not to relative density-ratio estimation. [sent-142, score-0.482]
</p><p>53 This implies that overﬁtting occurs in relative density-ratio estimation, but the approximation error cancels out in relative PE divergence estimation. [sent-143, score-0.511]
</p><p>54 Since rα ∞ monotonically decreases as α increases, our proposed approach of estimating the α-relative PE divergence (with α > 0) would be more advantageous than the naive approach of estimating the plain PE divergence (which corresponds to α = 0) in terms of the parametric asymptotic variance. [sent-150, score-0.985]
</p><p>55 4 Experiments In this section, we experimentally evaluate the performance of the proposed method in two-sample homogeneity test, outlier detection, and transfer learning tasks. [sent-152, score-0.378]
</p><p>56 Two-Sample Homogeneity Test: First, we apply the proposed divergence estimator to twosample homogeneity test. [sent-153, score-0.69]
</p><p>57 n Given two sets of samples X = {xi }n i=1 ∼ P and X = {xj }j=1 ∼ P , the goal of the twosample homogeneity test is to test the null hypothesis that the probability distributions P and P are the same against its complementary alternative (i. [sent-160, score-0.446]
</p><p>58 By using an estimator Div of some divergence between the two distributions P and P , homogeneity of two distributions can be tested based on the permutation test procedure [20]. [sent-163, score-0.692]
</p><p>59 The mean (and standard deviation in the bracket) rate of accepting the null hypothesis (i. [sent-165, score-0.161]
</p><p>60 Right: when the set of samples corresponding to the numerator of the density-ratio are taken from the positive training set and the set of samples corresponding to the denominator of the density-ratio are taken from the positive training set and the negative training set (i. [sent-173, score-0.231]
</p><p>61 The best method having the lowest mean accepting rate and comparable methods according to the two-sample t-test at the signiﬁcance level 5% are speciﬁed by bold face. [sent-176, score-0.101]
</p><p>62 Datasets  d  n=n  MMD  banana thyroid titanic diabetes b-cancer f-solar heart german ringnorm waveform  2 5 5 8 9 9 13 20 20 21  100 19 21 85 29 100 38 100 100 66  . [sent-177, score-0.181]
</p><p>63 00)  When an asymmetric divergence such as the KL divergence [7] or the PE divergence [11] is adopted for two-sample test, the test results depend on the choice of directions: a divergence from P to P or from P to P . [sent-343, score-1.469]
</p><p>64 We test LSTT with the RuLSIF-based PE divergence estimator for α = 0, 0. [sent-350, score-0.505]
</p><p>65 First, we investigate the rate of accepting the null hypothesis when the null hypothesis is correct (i. [sent-356, score-0.257]
</p><p>66 We split all the positive training samples into two sets and perform two-sample test for the two sets of samples. [sent-359, score-0.093]
</p><p>67 Next, we consider the situation where the null hypothesis is not correct (i. [sent-363, score-0.096]
</p><p>68 The numerator samples are generated in the same way as above, but a half of denominator samples are replaced with negative training samples. [sent-366, score-0.231]
</p><p>69 Thus, while the numerator sample set contains only positive training samples, the denominator sample set includes both positive and negative training samples. [sent-367, score-0.119]
</p><p>70 5 is shown to be a useful method for two-sample homogeneity test. [sent-374, score-0.135]
</p><p>71 Inlier-Based Outlier Detection: Next, we apply the proposed method to outlier detection. [sent-376, score-0.169]
</p><p>72 Let us consider an outlier detection problem of ﬁnding irregular samples in a dataset (called an “evaluation dataset”) based on another dataset (called a “model dataset”) that only contains regular samples. [sent-377, score-0.299]
</p><p>73 Deﬁning the density-ratio over the two sets of samples, we can see that the density-ratio 6  Table 2: Experimental results of outlier detection. [sent-378, score-0.141]
</p><p>74 Since the evaluation dataset usually has a wider support than the model dataset, we regard the evaluation dataset as samples corresponding to the denominator density p (x), and the model dataset as samples corresponding to the numerator density p(x). [sent-627, score-0.378]
</p><p>75 Thus, density-ratio approximators can be used for outlier detection. [sent-631, score-0.141]
</p><p>76 We evaluate the proposed method using various datasets: IDA benchmark repository [21], an inhouse French speech dataset, the 20 Newsgroup dataset, and the USPS hand-written digit dataset (the detailed speciﬁcation of the datasets is explained in the supplementary material). [sent-632, score-0.11]
</p><p>77 Transfer Learning: Finally, we apply the proposed method to transfer learning. [sent-643, score-0.102]
</p><p>78 tr Let us consider a transductive transfer learning setup where labeled training samples {(xtr , yj )}ntr j j=1 te nte drawn i. [sent-644, score-0.28]
</p><p>79 from p(y|x)ptr (x) and unlabeled test samples {xi }i=1 drawn i. [sent-647, score-0.127]
</p><p>80 from pte (x) (which is generally different from ptr (x)) are available. [sent-650, score-0.158]
</p><p>81 The use of exponentially-weighted importance weighting was shown to be useful for adaptation from ptr (x) to pte (x) [5]:  minf ∈F  1 ntr  ntr j=1  pte (xtr ) j ptr (xtr ) j  τ  tr loss(yj , f (xtr )) , j  where f (x) is a learned function and 0 ≤ τ ≤ 1 is the exponential ﬂattening parameter. [sent-651, score-0.605]
</p><p>82 We compare the plain kernel logistic regression (KLR) without importance weights, KLR with relative importance weights (RIW-KLR), KLR with exponentially-weighted importance weights (EIW-KLR), and KLR with plain importance weights (IW-KLR). [sent-656, score-0.439]
</p><p>83 Here we propose to use relative importance weights instead: minf ∈F  1 ntr  pte (xtr ) ntr j tr tr j=1 (1−α)pte (xtr )+αptr (xtr ) loss(yj , f (xj )) j j  . [sent-690, score-0.462]
</p><p>84 We apply the above transfer learning technique to human activity recognition using accelerometer data. [sent-691, score-0.133]
</p><p>85 However, since the new user is not willing to label his/her accelerometer data due to troublesomeness, no labeled sample is available for the new user. [sent-695, score-0.114]
</p><p>86 On the other hand, unlabeled samples for the new user and labeled data obtained from existing users are available. [sent-696, score-0.145]
</p><p>87 Let labeled training data tr {(xtr , yj )}ntr be the set of labeled accelerometer data for 20 existing users. [sent-697, score-0.159]
</p><p>88 Each user has at most j j=1 100 labeled samples for each action. [sent-698, score-0.111]
</p><p>89 Let unlabeled test data {xte }nte be unlabeled accelerometer i i=1 data obtained from the new user. [sent-699, score-0.164]
</p><p>90 The experiments are repeated 100 times with different sample choice for ntr = 500 and nte = 200. [sent-700, score-0.155]
</p><p>91 The classiﬁcation accuracy for 800 test samples from the new user (which are different from the 200 unlabeled samples) are summarized in Table 3, showing that the proposed method using relative importance weights for α = 0. [sent-701, score-0.303]
</p><p>92 5 Conclusion In this paper, we proposed to use a relative divergence for robust distribution comparison. [sent-703, score-0.448]
</p><p>93 We gave a computationally efﬁcient method for estimating the relative Pearson divergence based on direct relative density-ratio approximation. [sent-704, score-0.542]
</p><p>94 We theoretically elucidated the convergence rate of the proposed divergence estimator under non-parametric setup, which showed that the proposed approach of estimating the relative Pearson divergence is more preferable than the existing approach of estimating the plain Pearson divergence. [sent-705, score-1.117]
</p><p>95 Furthermore, we proved that the asymptotic variance of the proposed divergence estimator is independent of the model complexity under a correctly-speciﬁed parametric setup. [sent-706, score-0.653]
</p><p>96 Thus, the proposed divergence estimator hardly overﬁts even with complex models. [sent-707, score-0.541]
</p><p>97 Experimentally, we demonstrated the practical usefulness of the proposed divergence estimator in two-sample homogeneity test, inlier-based outlier detection, and transfer learning tasks. [sent-708, score-0.873]
</p><p>98 Thus, it would be promising to explore more applications of the proposed relative density-ratio approximator beyond two-sample homogeneity test, inlier-based outlier detection, and transfer learning. [sent-710, score-0.44]
</p><p>99 Estimating divergence functionals and the likelihood ratio by convex risk minimization. [sent-787, score-0.358]
</p><p>100 A general class of coefﬁcients of divergence of one distribution from another. [sent-798, score-0.358]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pe', 0.649), ('divergence', 0.358), ('ida', 0.219), ('lstt', 0.219), ('rulsif', 0.169), ('outlier', 0.141), ('homogeneity', 0.135), ('ep', 0.122), ('usps', 0.121), ('xtr', 0.119), ('estimator', 0.11), ('ntr', 0.104), ('vp', 0.097), ('mmd', 0.095), ('klr', 0.084), ('osvm', 0.084), ('pte', 0.084), ('ptr', 0.074), ('transfer', 0.074), ('asymptotic', 0.073), ('numerator', 0.068), ('ulsif', 0.067), ('null', 0.064), ('relative', 0.062), ('twosample', 0.059), ('kanamori', 0.059), ('accelerometer', 0.059), ('plain', 0.059), ('pearson', 0.058), ('samples', 0.056), ('sugiyama', 0.056), ('importance', 0.054), ('parametric', 0.053), ('detection', 0.052), ('kl', 0.051), ('denominator', 0.051), ('nte', 0.051), ('suzuki', 0.046), ('hardly', 0.045), ('kernel', 0.043), ('accepting', 0.043), ('favorably', 0.042), ('bracket', 0.041), ('densities', 0.039), ('divergences', 0.038), ('auc', 0.037), ('test', 0.037), ('bold', 0.036), ('cance', 0.036), ('density', 0.036), ('convergence', 0.036), ('xj', 0.035), ('estimators', 0.035), ('unlabeled', 0.034), ('banana', 0.034), ('hido', 0.034), ('titanic', 0.034), ('yamada', 0.034), ('tends', 0.033), ('sch', 0.033), ('supplementary', 0.032), ('user', 0.032), ('direct', 0.032), ('hypothesis', 0.032), ('variance', 0.031), ('walks', 0.031), ('tokyo', 0.03), ('diabetes', 0.03), ('thyroid', 0.03), ('ringnorm', 0.03), ('lkopf', 0.03), ('gretton', 0.029), ('approximation', 0.029), ('rkhs', 0.029), ('material', 0.028), ('proposed', 0.028), ('estimating', 0.028), ('tr', 0.027), ('kashima', 0.027), ('hachiya', 0.027), ('yj', 0.027), ('usefulness', 0.027), ('covariate', 0.027), ('distributions', 0.026), ('estimation', 0.025), ('favorable', 0.025), ('op', 0.025), ('dataset', 0.025), ('repository', 0.025), ('xi', 0.024), ('judged', 0.024), ('labeled', 0.023), ('ordinary', 0.023), ('rasch', 0.023), ('waveform', 0.023), ('compares', 0.023), ('shift', 0.023), ('setup', 0.022), ('rate', 0.022), ('bicycle', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="238-tfidf-1" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>2 0.096064813 <a title="238-tfidf-2" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>Author: David R. Karger, Sewoong Oh, Devavrat Shah</p><p>Abstract: Crowdsourcing systems, in which tasks are electronically distributed to numerous “information piece-workers”, have emerged as an effective paradigm for humanpowered solving of large scale problems in domains such as image classiﬁcation, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase conﬁdence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such crowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers’ answers. We show that our algorithm signiﬁcantly outperforms majority voting and, in fact, is asymptotically optimal through comparison to an oracle that knows the reliability of every worker. 1</p><p>3 0.081574187 <a title="238-tfidf-3" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>Author: Guido F. Montufar, Johannes Rauh, Nihat Ay</p><p>Abstract: We present explicit classes of probability distributions that can be learned by Restricted Boltzmann Machines (RBMs) depending on the number of units that they contain, and which are representative for the expressive power of the model. We use this to show that the maximal Kullback-Leibler divergence to the RBM model with n visible and m hidden units is bounded from above by (n−1)−log(m+1). In this way we can specify the number of hidden units that guarantees a sufﬁciently rich model containing different classes of distributions and respecting a given error tolerance. 1</p><p>4 0.080953926 <a title="238-tfidf-4" href="./nips-2011-Unifying_Non-Maximum_Likelihood_Learning_Objectives_with_Minimum_KL_Contraction.html">295 nips-2011-Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: When used to learn high dimensional parametric probabilistic models, the classical maximum likelihood (ML) learning often suffers from computational intractability, which motivates the active developments of non-ML learning methods. Yet, because of their divergent motivations and forms, the objective functions of many non-ML learning methods are seemingly unrelated, and there lacks a uniﬁed framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learning methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score matching [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be uniﬁed under the minimum KL contraction framework with different choices of the KL contraction operators. 1</p><p>5 0.072034709 <a title="238-tfidf-5" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<p>Author: Nan Ding, Yuan Qi, S.v.n. Vishwanathan</p><p>Abstract: Approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by deﬁning a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student’s t-prior. 1</p><p>6 0.067815006 <a title="238-tfidf-6" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>7 0.054646656 <a title="238-tfidf-7" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>8 0.054332212 <a title="238-tfidf-8" href="./nips-2011-Transfer_from_Multiple_MDPs.html">291 nips-2011-Transfer from Multiple MDPs</a></p>
<p>9 0.05036433 <a title="238-tfidf-9" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>10 0.049043678 <a title="238-tfidf-10" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>11 0.04896583 <a title="238-tfidf-11" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>12 0.044801541 <a title="238-tfidf-12" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>13 0.04475743 <a title="238-tfidf-13" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>14 0.042292409 <a title="238-tfidf-14" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>15 0.041806612 <a title="238-tfidf-15" href="./nips-2011-Higher-Order_Correlation_Clustering_for_Image_Segmentation.html">119 nips-2011-Higher-Order Correlation Clustering for Image Segmentation</a></p>
<p>16 0.041643325 <a title="238-tfidf-16" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>17 0.040282134 <a title="238-tfidf-17" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>18 0.039671995 <a title="238-tfidf-18" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>19 0.039433263 <a title="238-tfidf-19" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<p>20 0.039313454 <a title="238-tfidf-20" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.006), (2, -0.005), (3, -0.039), (4, -0.039), (5, -0.026), (6, 0.009), (7, -0.077), (8, 0.007), (9, 0.008), (10, -0.072), (11, -0.022), (12, 0.055), (13, 0.009), (14, -0.007), (15, 0.002), (16, 0.009), (17, 0.001), (18, 0.073), (19, 0.096), (20, -0.073), (21, 0.05), (22, 0.016), (23, -0.04), (24, 0.075), (25, -0.017), (26, 0.003), (27, -0.054), (28, -0.024), (29, 0.05), (30, 0.012), (31, 0.007), (32, -0.035), (33, 0.062), (34, -0.089), (35, -0.011), (36, 0.077), (37, 0.06), (38, 0.031), (39, 0.027), (40, -0.117), (41, -0.052), (42, 0.097), (43, -0.041), (44, 0.019), (45, 0.063), (46, 0.058), (47, -0.057), (48, 0.01), (49, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93523848 <a title="238-lsi-1" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>2 0.76038629 <a title="238-lsi-2" href="./nips-2011-Unifying_Non-Maximum_Likelihood_Learning_Objectives_with_Minimum_KL_Contraction.html">295 nips-2011-Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction</a></p>
<p>Author: Siwei Lyu</p><p>Abstract: When used to learn high dimensional parametric probabilistic models, the classical maximum likelihood (ML) learning often suffers from computational intractability, which motivates the active developments of non-ML learning methods. Yet, because of their divergent motivations and forms, the objective functions of many non-ML learning methods are seemingly unrelated, and there lacks a uniﬁed framework to understand them. In this work, based on an information geometric view of parametric learning, we introduce a general non-ML learning principle termed as minimum KL contraction, where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learning methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score matching [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be uniﬁed under the minimum KL contraction framework with different choices of the KL contraction operators. 1</p><p>3 0.73486197 <a title="238-lsi-3" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<p>Author: Nan Ding, Yuan Qi, S.v.n. Vishwanathan</p><p>Abstract: Approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by deﬁning a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student’s t-prior. 1</p><p>4 0.61022598 <a title="238-lsi-4" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>5 0.59346139 <a title="238-lsi-5" href="./nips-2011-Differentially_Private_M-Estimators.html">69 nips-2011-Differentially Private M-Estimators</a></p>
<p>Author: Jing Lei</p><p>Abstract: This paper studies privacy preserving M-estimators using perturbed histograms. The proposed approach allows the release of a wide class of M-estimators with both differential privacy and statistical utility without knowing a priori the particular inference procedure. The performance of the proposed method is demonstrated through a careful study of the convergence rates. A practical algorithm is given and applied on a real world data set containing both continuous and categorical variables. 1</p><p>6 0.558819 <a title="238-lsi-6" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>7 0.54445398 <a title="238-lsi-7" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>8 0.51431054 <a title="238-lsi-8" href="./nips-2011-Automated_Refinement_of_Bayes_Networks%27_Parameters_based_on_Test_Ordering_Constraints.html">40 nips-2011-Automated Refinement of Bayes Networks' Parameters based on Test Ordering Constraints</a></p>
<p>9 0.50963271 <a title="238-lsi-9" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>10 0.50363022 <a title="238-lsi-10" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>11 0.49995047 <a title="238-lsi-11" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>12 0.48549965 <a title="238-lsi-12" href="./nips-2011-Advice_Refinement_in_Knowledge-Based_SVMs.html">27 nips-2011-Advice Refinement in Knowledge-Based SVMs</a></p>
<p>13 0.48539221 <a title="238-lsi-13" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>14 0.46901071 <a title="238-lsi-14" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>15 0.46015373 <a title="238-lsi-15" href="./nips-2011-Continuous-Time_Regression_Models_for_Longitudinal_Networks.html">62 nips-2011-Continuous-Time Regression Models for Longitudinal Networks</a></p>
<p>16 0.45944989 <a title="238-lsi-16" href="./nips-2011-Ranking_annotators_for_crowdsourced_labeling_tasks.html">232 nips-2011-Ranking annotators for crowdsourced labeling tasks</a></p>
<p>17 0.43431753 <a title="238-lsi-17" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>18 0.43084198 <a title="238-lsi-18" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>19 0.43052965 <a title="238-lsi-19" href="./nips-2011-An_Exact_Algorithm_for_F-Measure_Maximization.html">33 nips-2011-An Exact Algorithm for F-Measure Maximization</a></p>
<p>20 0.42656502 <a title="238-lsi-20" href="./nips-2011-History_distribution_matching_method_for_predicting_effectiveness_of_HIV_combination_therapies.html">120 nips-2011-History distribution matching method for predicting effectiveness of HIV combination therapies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (4, 0.038), (20, 0.019), (26, 0.02), (31, 0.062), (33, 0.011), (43, 0.051), (45, 0.573), (57, 0.022), (74, 0.035), (83, 0.023), (99, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99855322 <a title="238-lda-1" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><p>2 0.9968698 <a title="238-lda-2" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>Author: Mark Schmidt, Nicolas L. Roux, Francis R. Bach</p><p>Abstract: We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen ﬁxed error level on a set of structured sparsity problems. 1</p><p>3 0.99443245 <a title="238-lda-3" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>4 0.99393243 <a title="238-lda-4" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>Author: Alekh Agarwal, Dean P. Foster, Daniel J. Hsu, Sham M. Kakade, Alexander Rakhlin</p><p>Abstract: This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact set X under a stochastic bandit feedback model. In this model, the algorithm is allowed to observe noisy realizations of the function value f (x) at any query point x ∈ X . We demonstrate √ a generalization of the ellipsoid algorithm that √ incurs O(poly(d) T ) regret. Since any algorithm has regret at least Ω( T ) on this problem, our algorithm is optimal in terms of the scaling with T . 1</p><p>5 0.98953074 <a title="238-lda-5" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>Author: Ziming Zhang, Lubor Ladicky, Philip Torr, Amir Saffari</p><p>Abstract: Local Coordinate Coding (LCC) [18] is a method for modeling functions of data lying on non-linear manifolds. It provides a set of anchor points which form a local coordinate system, such that each data point on the manifold can be approximated by a linear combination of its anchor points, and the linear weights become the local coordinate coding. In this paper we propose encoding data using orthogonal anchor planes, rather than anchor points. Our method needs only a few orthogonal anchor planes for coding, and it can linearize any (α, β, p)-Lipschitz smooth nonlinear function with a ﬁxed expected value of the upper-bound approximation error on any high dimensional data. In practice, the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (SVD). We apply our method to model the coordinates locally in linear SVMs for classiﬁcation tasks, and our experiment on MNIST shows that using only 50 anchor planes our method achieves 1.72% error rate, while LCC achieves 1.90% error rate using 4096 anchor points. 1</p><p>6 0.98758674 <a title="238-lda-6" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>same-paper 7 0.98742586 <a title="238-lda-7" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>8 0.95539212 <a title="238-lda-8" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>9 0.92255831 <a title="238-lda-9" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>10 0.90730429 <a title="238-lda-10" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>11 0.90421754 <a title="238-lda-11" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>12 0.89977866 <a title="238-lda-12" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>13 0.89867717 <a title="238-lda-13" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>14 0.89711207 <a title="238-lda-14" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>15 0.89641285 <a title="238-lda-15" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>16 0.8961044 <a title="238-lda-16" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>17 0.89312506 <a title="238-lda-17" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>18 0.88351059 <a title="238-lda-18" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>19 0.88133878 <a title="238-lda-19" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>20 0.88068444 <a title="238-lda-20" href="./nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
