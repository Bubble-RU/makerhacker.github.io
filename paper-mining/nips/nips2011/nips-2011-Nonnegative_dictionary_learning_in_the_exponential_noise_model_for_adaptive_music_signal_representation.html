<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-191" href="#">nips2011-191</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</h1>
<br/><p>Source: <a title="nips-2011-191-pdf" href="http://papers.nips.cc/paper/4273-nonnegative-dictionary-learning-in-the-exponential-noise-model-for-adaptive-music-signal-representation.pdf">pdf</a></p><p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>Reference: <a title="nips-2011-191-reference" href="../nips2011_reference/nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation  C´ dric F´ votte e e CNRS LTCI; T´ l´ com ParisTech ee 75014, Paris, France fevotte@telecom-paristech. [sent-1, score-0.391]
</p><p>2 fr  Abstract In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. [sent-3, score-0.426]
</p><p>3 This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. [sent-4, score-0.199]
</p><p>4 Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. [sent-5, score-0.552]
</p><p>5 In this paper we describe a variational procedure for optimization of the marginal likelihood, i. [sent-7, score-0.209]
</p><p>6 , the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). [sent-9, score-0.316]
</p><p>7 We compare the output of both maximum joint likelihood estimation (i. [sent-10, score-0.207]
</p><p>8 , standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. [sent-12, score-0.377]
</p><p>9 1 Introduction In this paper we address the task of nonnegative dictionary learning described by V ≈ W H,  (1)  where V , W , H are nonnegative matrices of dimensions F × N , F × K and K × N , respectively. [sent-14, score-0.432]
</p><p>10 V is the data matrix, where each column vn is a data point, W is the dictionary matrix, with columns {wk } acting as “patterns” or “explanatory variables” reprensentative of the data, and H is the activation matrix, with columns {hn }. [sent-15, score-0.258]
</p><p>11 For example, in this paper we will be interested in music data such that V is time-frequency spectrogram matrix and W is a collection of spectral signatures of latent elementary audio components. [sent-16, score-0.22]
</p><p>12 The most common approach to nonnegative dictionary learning is nonnegative matrix factorization (NMF) [1] which consists in retrieving the factorization (1) by solving def  min D(V |W H) = W,H  fn  d(vf n |[W H]f n ) s. [sent-17, score-0.632]
</p><p>13 W, H ≥ 0 ,  (2)  where d(x|y) is a measure of ﬁt between nonnegative scalars, vf n are the entries of V , and A ≥ 0 expresses nonnegativity of the entries of matrix A. [sent-19, score-0.432]
</p><p>14 The cost function D(V |W H) is often a likelihood function − log p(V |W, H) in disguise, e. [sent-20, score-0.132]
</p><p>15 , the Euclidean distance underlies additive Gaussian noise, the Kullback-Leibler (KL) divergence underlies Poissonian noise, while the Itakura-Saito (IS) divergence underlies multiplicative exponential noise [2]. [sent-22, score-0.417]
</p><p>16 The latter noise model will be central to this work because it underlies a suitable generative model of the power spectrogram, as shown in [3] and later recalled. [sent-23, score-0.17]
</p><p>17 1  A criticism about NMF is that little can be said about the asymptotical optimality of the learnt dictionary W . [sent-24, score-0.178]
</p><p>18 As such, this paper instead addresses optimization of the likelihood in the marginal model described by p(V |W ) =  H  p(V |W, H)p(H)dH,  (3)  where H is treated as a random latent variable with prior p(H). [sent-26, score-0.258]
</p><p>19 The evaluation and optimization of the marginal likelihood is not trivial in general, and this paper is precisely devoted to these tasks in the multiplicative exponential noise model. [sent-27, score-0.356]
</p><p>20 The maximum marginal likelihood estimation approach we seek here is related to IS-NMF in such a way that Latent Dirichlet Allocation (LDA) [4] is related to Latent Semantic Indexing (pLSI) [5]. [sent-28, score-0.282]
</p><p>21 LDA and pLSI are two estimators in the same model, but LDA seeks estimation of the topic distributions in the marginal model, from which the topic weights describing each document have been integrated out. [sent-29, score-0.171]
</p><p>22 In contrast, pLSI (which is essentially equivalent to KL-NMF as shown in [6]) performs maximum joint likelihood estimation (MJLE) for the topics and weights. [sent-30, score-0.207]
</p><p>23 A similar approach is Discrete Component Analysis (DCA) [8] which considers maximum marginal a posteriori estimation in the GammaPoisson (GaP) model [9], see also [10] for the maximum marginal likelihood estimation on the same model. [sent-37, score-0.459]
</p><p>24 In this paper, we will follow the same objective for the multiplicative exponential noise model. [sent-38, score-0.14]
</p><p>25 We will consider a nonnegative Generalized inverse-Gaussian (GIG) distribution as a prior for H, a ﬂexible distribution which takes the Gamma and inverse-Gamma as special cases. [sent-40, score-0.141]
</p><p>26 [11], which considers full Bayesian integration of W and H (both assumed random) in the exponential noise model, in a nonparametric setting allowing for model order selection. [sent-42, score-0.142]
</p><p>27 We will show that our more simple maximum likelihood approach inherently performs model selection as well by automatically pruning “irrelevant” dictionary elements. [sent-43, score-0.286]
</p><p>28 Applied to a short well structured piano sequence, our approach is shown to capture the correct number of components, corresponding to the expected note spectra, and outperforms the nonparametric Bayesian approach of [11]. [sent-44, score-0.109]
</p><p>29 Section 2 introduces the multiplicative exponential noise model with the prior distribution for the expansion coefﬁcients p(H). [sent-46, score-0.14]
</p><p>30 Section 5 reports results on synthetical and real audio data. [sent-48, score-0.137]
</p><p>31 2 Model The generative model assumed in this paper is vf n = vf n . [sent-50, score-0.553]
</p><p>32 ǫf n , ˆ  (4)  where vf n = ˆ k wf k hkn and ǫf n is a nonnegative multiplicative noise with exponential distribution ǫf n ∼ exp(−ǫf n ). [sent-51, score-1.227]
</p><p>33 In other words, and under independence assumptions, the likelihood function is p(V |W, H) =  fn  (1/ˆf n ) exp(−vf n /ˆf n ) . [sent-52, score-0.14]
</p><p>34 1 In other words, the exponential multiplicative noise model underlies a generative composite model of the STFT. [sent-54, score-0.271]
</p><p>35 The complexvalued matrix {cf kn }f n , referred to as k th component, is characterized by a spectral signature wk , amplitude-modulated in time by the frame-dependent coefﬁcient hkn , which accounts for nonstationarity. [sent-55, score-0.627]
</p><p>36 In analogy with LDA or DCA, if our data consisted of word counts, with f indexing words and n indexing documents, then the columns of W would describe topics and cf kn would denote the number of occurrences of word f stemming from topic k in document n. [sent-56, score-0.418]
</p><p>37 In contrast, H is treated as a nonnegative random latent variable over which we will integrate. [sent-58, score-0.183]
</p><p>38 It is assigned a GIG prior, such that hkn ∼ GIG(αk , βk , γk ) ,  (7)  with GIG(x|α, β, γ) =  (β/γ)α/2 α−1 γ √ x exp − βx + x 2Kα (2 βγ)  ,  (8)  where K is a modiﬁed Bessel function of the second kind and x, β and γ are nonnegative scalars. [sent-59, score-0.522]
</p><p>39 In such case, β parameter merely acts as a scale parameter, which we ﬁx so as to solve the scale ambiguity between the columns of W and the rows of H. [sent-63, score-0.101]
</p><p>40 The subscript JL stands for joint likelihood, and the estimation of W by maximization of CJL (W, H) will be referred to as maximum joint likelihood estimation (MJLE). [sent-70, score-0.334]
</p><p>41 Given W , our task consists in maximizing C(H) = −DIS (V |W H) − L(H), where L(H) = kn (1 − αk ) log hkn + βk hkn + γk /hkn . [sent-75, score-1.035]
</p><p>42 The update is easily shown to amount to solving an order 2 polynomial with a single positive root given by hkn =  (αk − 1) +  (αk − 1)2 + 4(pkn + γk )(qkn + βk ) . [sent-83, score-0.416]
</p><p>43 By exchangeability of W and H when the data is transposed (V T = H T W T ), and dropping the penalty term (αk = 1, βk = 0, γk = 0), the update of W is given by the multiplicative update n  wf k = wf k ˜  hkn vf n /˜f n v2 n  hkn /˜f n v  ,  (14)  which is known from [13]. [sent-85, score-1.757]
</p><p>44 1 Estimator We deﬁne the marginal log-likelihood objective function as def  CML (W ) = log  p(V |W, H)p(H) dH . [sent-87, score-0.185]
</p><p>45 (15)  The subscript ML stands for marginal likelihood, and the estimation of W by maximization of CML (W ) will be referred to as maximum marginal likelihood estimation (MMLE). [sent-88, score-0.484]
</p><p>46 Note that in Bayesian estimation the term marginal likelihood is sometimes used as a synonym for the model evidence, which is the likelihood of data given the model, i. [sent-89, score-0.356]
</p><p>47 This is not the case here where W is treated as a deterministic parameter and marginal likelihood only refers to the likelihood of W , where H has been integrated out. [sent-92, score-0.378]
</p><p>48 In the next section we resort to a variational Bayes procedure for the evaluation and maximization of CML (W ). [sent-94, score-0.154]
</p><p>49 We will construct a bound B(W, W ) such that ˜ ), CML (W ) ≥ B(W, W ), where W acts as the current iterate and W acts as the free pa˜ ˜ ∀(W, W rameter over which the bound is maximized. [sent-97, score-0.202]
</p><p>50 We propose to construct the bound from a variational Bayes perspective [14]. [sent-102, score-0.134]
</p><p>51 The following inequality holds for any distribution function q(H) CML (W ) ≥ log p(V |W, H)  q  + log p(H)  q  − log q(H)  def q  vb = Bq (W ) . [sent-103, score-0.238]
</p><p>52 (16)  The inequality becomes an equality when q(H) = p(H|V, W ); when the latter is available in close vb ˜ form, the EM algorithm consists in using q (H) = p(H|V, W ) and maximize Bq (W ) w. [sent-104, score-0.11]
</p><p>53 The true posterior of H being intractable in our case, we take q(H) to be a factorized, 4  vb ˜ ˜ parametric distribution qθ (H), whose parameter θ is updated so as to tighten Bq (W ) to C(W ). [sent-107, score-0.135]
</p><p>54 Like in [11], we choose qθ (H) to be in the same family as the prior, such that  qθ (H) =  ¯ ¯ GIG(¯ kn , βkn , γkn ) . [sent-108, score-0.246]
</p><p>55 α  kn  (17)  vb The ﬁrst term of Bq (W ) essentially involves the expectation of −DIS (V |W H) w. [sent-109, score-0.356]
</p><p>56 The product W H introduces some coupling of the coefﬁcients of H (via the sum k wf k hkn ) which makes the integration difﬁcult. [sent-112, score-0.684]
</p><p>57 This leads to  log p(V |H, W )  q  ≥−  φ2 kn f fn  k  vf n wf k  1 hkn  +  log ψf n +  q  1 ψf n  wf k hkn k  q  −1 , (18)  where {ψf n } and {φf kn } are nonnegative free parameters such that k φf kn = 1. [sent-118, score-2.598]
</p><p>58 We deﬁne vb Bθ,φ,ψ (W ) as Bq (W ) but where the expectation of the joint log-likelihood is replaced by its lower bound given right side of equation (18). [sent-119, score-0.182]
</p><p>59 Note that evaluation of the bound only involves expectations of hkn and 1/hkn w. [sent-124, score-0.417]
</p><p>60 ˜ Step 1: Tightening the bound Given current dictionary update W , run the following ﬁxed-point equations. [sent-127, score-0.221]
</p><p>61 wf k / 1/hkn ˜  φf kn = αkn = αk , ¯  q  ,  ˜ j wf j / 1/hjn ¯ βkn = βk + f  wf j hjn ˜  ψf n =  q  q  j  wf k ˜ , ψf n  γkn = γk + ¯ f  vf n φ2 kn f . [sent-128, score-2.004]
</p><p>62 wf k ˜  Step 2: Optimizing the bound Given the variational distribution q = qθ from previous step, ˜ ˜ update W as  wf k = wf k ˜  n vf n  j n  wf j 1/hjn ˜ ˜ j wf j hjn  −2 −1 q ˜ −1 q ˜  1/hkn  hkn  −1 q ˜  . [sent-129, score-2.365]
</p><p>63 3 Relation to other works A variational algorithm using the activation matrix H and the latent components C = {cf kn } as hidden data can easily be devised, as sketched in [2]. [sent-134, score-0.48]
</p><p>64 Including C in the variational distribution also allows to decouple the contributions of the activation coefﬁcients w. [sent-135, score-0.134]
</p><p>65 The model is such that vf n = k λk wf k hkn , where ˆ λk acts as a component weight parameter. [sent-140, score-1.051]
</p><p>66 The number of components is potentially inﬁnite but, in a nonparametric setting, the prior for λk favors a ﬁnite number of active components. [sent-141, score-0.117]
</p><p>67 65  5  K  (b) CJL by MJLE  10  15  20  25  K  (c) CML by MJLE  Figure 1: Marginal likelihood CML (a) and joint likelihood CJL (b) versus number of components K. [sent-158, score-0.304]
</p><p>68 We used 5000 algorithm iterations and nonnegative random initializations in all cases. [sent-164, score-0.141]
</p><p>69 Deterministic annealing is applied by multiplying the entropy term − log q(H) in the lower bound in (16) by 1/η (i) . [sent-166, score-0.112]
</p><p>70 1 Swimmer dataset First, we consider the synthetical Swimmer dataset [16], for which the ground truth of the dictionary is available. [sent-172, score-0.245]
</p><p>71 The dataset is composed of 256 images of size 32 × 32, representing a swimmer built of an invariant torso and 4 limbs. [sent-173, score-0.109]
</p><p>72 Hence, the ground truth dictionary corresponds to the collection of individual limb positions. [sent-175, score-0.15]
</p><p>73 20 and the joint and marginal log-likelihood end values (after the 5000 iterations) are displayed in Fig. [sent-182, score-0.147]
</p><p>74 The marginal log-likelihood is here approximated by its lower bound, as described in Section 4. [sent-184, score-0.111]
</p><p>75 1 (c) displays the corresponding marginal likelihood values, CML , of the dictionaries obtained by MJLE in Fig. [sent-190, score-0.259]
</p><p>76 1 (b); this ﬁgure empirically shows that maximizing the joint likelihood does not necessarily imply maximization of the marginal likelihood. [sent-191, score-0.308]
</p><p>77 The likelihood values increase with the number of components, as expected from nested models. [sent-193, score-0.105]
</p><p>78 However, the marginal likelihood stagnates after K = 16. [sent-194, score-0.216]
</p><p>79 The dictionaries learnt from MJLE and MMLE with K = 20 components are shown in Fig. [sent-197, score-0.129]
</p><p>80 fr/∼dikmen/nips11/  6  (a) Data  (b) WMJLE  (c) WMMLE  Figure 2: Data samples and dictionaries learnt on the swimmer dataset with K = 20. [sent-204, score-0.147]
</p><p>81 2 A piano excerpt In this section, we consider the piano data used in [3]. [sent-206, score-0.1]
</p><p>82 A power spectrogram with analysis window of size 46 ms was computed, leading to F = 513 frequency bins and N = 676 time frames. [sent-208, score-0.142]
</p><p>83 We reconstructed STFT component estimates from the factorization W H, where W is ˆ the MMLE dictionary estimate and H = H q . [sent-210, score-0.34]
</p><p>84 We used the minimum mean square error (MMSE) estimate given by cf kn = gf kn . [sent-211, score-0.596]
</p><p>85 xf n , where gf kn is the time-frequency Wiener mask deﬁned by ˆ ˆ jn . [sent-212, score-0.392]
</p><p>86 The estimated dictionary and the reconstructed components in the time doˆ kn / ˆ wf k h ˆ j wf j h main after inverse STFT are shown in Fig. [sent-213, score-1.151]
</p><p>87 3 of the nonzero dictionary columns have very small values, leading to inaudible reconstructions. [sent-217, score-0.186]
</p><p>88 The ﬁve signiﬁcant dictionary vectors correspond to the frequency templates of the four notes and the transients. [sent-218, score-0.207]
</p><p>89 The estimated dictionary and the reconstructed components are presented in Fig. [sent-221, score-0.299]
</p><p>90 We computed a power spectrogram with 46 ms analysis window and ran our VB algorithm with K = 50. [sent-234, score-0.113]
</p><p>91 4 displays the original data, and two examples of estimated time-frequency masks and reconstructed components. [sent-236, score-0.129]
</p><p>92 The ﬁgure also shows the variance of the reconstructed components and the evolution of the variational bound along iterations. [sent-237, score-0.283]
</p><p>93 In this example, 5 components out of the 50 are completely pruned in the factorization and 7 others are inaudible. [sent-238, score-0.141]
</p><p>94 Figure 3: The estimated dictionary and the reconstructed components by MMLE and the nonparametric approach by Hoffman et al. [sent-243, score-0.358]
</p><p>95 The ﬁrst example of reconstructed component captures the ﬁrst chord of the song, repeated 4 times in the intro. [sent-247, score-0.131]
</p><p>96 Acknowledgments This work is supported by project ANR-09-JCJC-0073-01 TANGERINE (Theory and applications of nonnegative matrix factorization). [sent-249, score-0.141]
</p><p>97 6 Conclusions In this paper we have challenged the standard NMF approach to nonnegative dictionary learning, based on maximum joint likelihood estimation, with a better-posed approach consisting in maximum marginal likelihood estimation. [sent-250, score-0.71]
</p><p>98 Our experiments on synthetical and real data have brought up a very attractive feature of MMLE, namely its self-ability to discard irrelevant columns in the dictionary, without resorting to elaborate schemes such as Bayesian nonparametrics. [sent-252, score-0.158]
</p><p>99 Learning the parts of objects with nonnegative matrix factorization. [sent-258, score-0.141]
</p><p>100 Maximum marginal likelihood estimation for nonnegative dictioe nary learning. [sent-315, score-0.392]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hkn', 0.381), ('mmle', 0.343), ('wf', 0.303), ('cml', 0.268), ('mjle', 0.267), ('vf', 0.262), ('kn', 0.246), ('dictionary', 0.15), ('nonnegative', 0.141), ('cjl', 0.133), ('gig', 0.117), ('marginal', 0.111), ('vb', 0.11), ('likelihood', 0.105), ('variational', 0.098), ('synthetical', 0.095), ('reconstructed', 0.091), ('spectrogram', 0.087), ('dis', 0.085), ('bq', 0.084), ('qkn', 0.076), ('swimmer', 0.076), ('underlies', 0.075), ('mm', 0.073), ('nmf', 0.072), ('xf', 0.067), ('hoffman', 0.066), ('cf', 0.066), ('acts', 0.065), ('lda', 0.063), ('factorization', 0.059), ('nonparametric', 0.059), ('components', 0.058), ('dca', 0.057), ('dikmen', 0.057), ('majorize', 0.057), ('pkn', 0.057), ('plsi', 0.057), ('votte', 0.057), ('multiplicative', 0.057), ('maximization', 0.056), ('wiener', 0.052), ('piano', 0.05), ('stft', 0.05), ('cst', 0.05), ('music', 0.049), ('annealing', 0.049), ('def', 0.047), ('exponential', 0.043), ('dictionaries', 0.043), ('audio', 0.042), ('latent', 0.042), ('mask', 0.041), ('sigir', 0.041), ('component', 0.04), ('noise', 0.04), ('gf', 0.038), ('hjn', 0.038), ('masks', 0.038), ('mono', 0.038), ('wmmle', 0.038), ('gamma', 0.038), ('activation', 0.036), ('joint', 0.036), ('columns', 0.036), ('bound', 0.036), ('indexing', 0.035), ('fn', 0.035), ('estimation', 0.035), ('update', 0.035), ('cients', 0.034), ('limbs', 0.033), ('torso', 0.033), ('paristech', 0.033), ('deterministic', 0.032), ('maximum', 0.031), ('nc', 0.031), ('cnrs', 0.031), ('coef', 0.031), ('generative', 0.029), ('nonnegativity', 0.029), ('jl', 0.029), ('com', 0.029), ('ltci', 0.029), ('song', 0.029), ('frequency', 0.029), ('notes', 0.028), ('learnt', 0.028), ('resorting', 0.027), ('log', 0.027), ('composite', 0.027), ('blei', 0.026), ('divergence', 0.026), ('power', 0.026), ('integrated', 0.025), ('tighten', 0.025), ('bayesian', 0.024), ('pruned', 0.024), ('automatic', 0.024), ('ee', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="191-tfidf-1" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>2 0.11901204 <a title="191-tfidf-2" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>Author: Jacquelyn A. Shelton, Abdul S. Sheikh, Pietro Berkes, Joerg Bornschein, Joerg Luecke</p><p>Abstract: An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. 1</p><p>3 0.10448124 <a title="191-tfidf-3" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>Author: David P. Wipf</p><p>Abstract: In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications). In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of ﬁnding, to the extent possible, regimes of dictionary invariant performance. In particular, a Type II Bayesian estimator with a dictionarydependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees no longer hold. This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions. 1</p><p>4 0.089222759 <a title="191-tfidf-4" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>Author: David A. Knowles, Tom Minka</p><p>Abstract: Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability. 1</p><p>5 0.084565908 <a title="191-tfidf-5" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>Author: Neil D. Lawrence, Michalis K. Titsias, Andreas Damianou</p><p>Abstract: High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences. 1</p><p>6 0.082607873 <a title="191-tfidf-6" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>7 0.080468975 <a title="191-tfidf-7" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>8 0.07386858 <a title="191-tfidf-8" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>9 0.073307954 <a title="191-tfidf-9" href="./nips-2011-Complexity_of_Inference_in_Latent_Dirichlet_Allocation.html">58 nips-2011-Complexity of Inference in Latent Dirichlet Allocation</a></p>
<p>10 0.070369542 <a title="191-tfidf-10" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>11 0.067418873 <a title="191-tfidf-11" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>12 0.06591469 <a title="191-tfidf-12" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>13 0.062349997 <a title="191-tfidf-13" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>14 0.05960609 <a title="191-tfidf-14" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>15 0.05432643 <a title="191-tfidf-15" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>16 0.053829424 <a title="191-tfidf-16" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>17 0.053227466 <a title="191-tfidf-17" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>18 0.050845783 <a title="191-tfidf-18" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>19 0.049361713 <a title="191-tfidf-19" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>20 0.049206432 <a title="191-tfidf-20" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.041), (2, 0.027), (3, -0.057), (4, -0.044), (5, -0.104), (6, 0.083), (7, 0.019), (8, 0.024), (9, 0.072), (10, -0.047), (11, -0.049), (12, 0.005), (13, -0.015), (14, -0.04), (15, -0.053), (16, -0.062), (17, -0.044), (18, -0.009), (19, 0.02), (20, 0.073), (21, -0.007), (22, 0.07), (23, -0.009), (24, 0.032), (25, -0.007), (26, 0.091), (27, 0.138), (28, -0.052), (29, 0.085), (30, 0.022), (31, 0.033), (32, -0.041), (33, -0.086), (34, 0.08), (35, -0.016), (36, -0.025), (37, 0.108), (38, -0.018), (39, 0.101), (40, -0.07), (41, 0.02), (42, -0.035), (43, -0.109), (44, -0.088), (45, -0.135), (46, 0.117), (47, 0.027), (48, -0.016), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91668689 <a title="191-lsi-1" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>2 0.68509048 <a title="191-lsi-2" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>Author: David A. Knowles, Tom Minka</p><p>Abstract: Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability. 1</p><p>3 0.67148834 <a title="191-lsi-3" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>4 0.6135301 <a title="191-lsi-4" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>Author: Artin Armagan, Merlise Clyde, David B. Dunson</p><p>Abstract: In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We ﬁrst propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efﬁciently to the types of truly massive data sets that are now encountered routinely. 1</p><p>5 0.57906383 <a title="191-lsi-5" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. D. Babacan</p><p>Abstract: Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. 1</p><p>6 0.50938815 <a title="191-lsi-6" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>7 0.50189477 <a title="191-lsi-7" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>8 0.49847171 <a title="191-lsi-8" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>9 0.47294861 <a title="191-lsi-9" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>10 0.4628329 <a title="191-lsi-10" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>11 0.44586685 <a title="191-lsi-11" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>12 0.43090725 <a title="191-lsi-12" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>13 0.43037403 <a title="191-lsi-13" href="./nips-2011-A_concave_regularization_technique_for_sparse_mixture_models.html">14 nips-2011-A concave regularization technique for sparse mixture models</a></p>
<p>14 0.40402031 <a title="191-lsi-14" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>15 0.3992199 <a title="191-lsi-15" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>16 0.39327469 <a title="191-lsi-16" href="./nips-2011-Unifying_Non-Maximum_Likelihood_Learning_Objectives_with_Minimum_KL_Contraction.html">295 nips-2011-Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction</a></p>
<p>17 0.386924 <a title="191-lsi-17" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>18 0.37809354 <a title="191-lsi-18" href="./nips-2011-On_the_Completeness_of_First-Order_Knowledge_Compilation_for_Lifted_Probabilistic_Inference.html">201 nips-2011-On the Completeness of First-Order Knowledge Compilation for Lifted Probabilistic Inference</a></p>
<p>19 0.37555546 <a title="191-lsi-19" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>20 0.37337241 <a title="191-lsi-20" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (2, 0.375), (4, 0.027), (20, 0.032), (26, 0.025), (31, 0.081), (33, 0.017), (43, 0.052), (45, 0.072), (57, 0.049), (74, 0.079), (83, 0.042), (84, 0.012), (99, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73274821 <a title="191-lda-1" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>2 0.56569558 <a title="191-lda-2" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>Author: Adrian Ion, Joao Carreira, Cristian Sminchisescu</p><p>Abstract: We present a joint image segmentation and labeling model (JSL) which, given a bag of ﬁgure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as ﬁrst sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect conﬁgurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.</p><p>3 0.4287495 <a title="191-lda-3" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>4 0.39601818 <a title="191-lda-4" href="./nips-2011-Demixed_Principal_Component_Analysis.html">68 nips-2011-Demixed Principal Component Analysis</a></p>
<p>Author: Wieland Brendel, Ranulfo Romo, Christian K. Machens</p><p>Abstract: In many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters. In electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difﬁcult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret. Here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model, and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response. 1</p><p>5 0.3923609 <a title="191-lda-5" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>6 0.3850944 <a title="191-lda-6" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>7 0.38496426 <a title="191-lda-7" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>8 0.3816267 <a title="191-lda-8" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>9 0.38144672 <a title="191-lda-9" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>10 0.38066852 <a title="191-lda-10" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>11 0.37825289 <a title="191-lda-11" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>12 0.3780373 <a title="191-lda-12" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>13 0.37798965 <a title="191-lda-13" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>14 0.37746537 <a title="191-lda-14" href="./nips-2011-Hierarchically_Supervised_Latent_Dirichlet_Allocation.html">116 nips-2011-Hierarchically Supervised Latent Dirichlet Allocation</a></p>
<p>15 0.37698343 <a title="191-lda-15" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>16 0.37632138 <a title="191-lda-16" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>17 0.37615958 <a title="191-lda-17" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>18 0.37597689 <a title="191-lda-18" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>19 0.37560624 <a title="191-lda-19" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<p>20 0.37463272 <a title="191-lda-20" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
