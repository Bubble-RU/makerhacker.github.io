<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-113" href="#">nips2011-113</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</h1>
<br/><p>Source: <a title="nips-2011-113-pdf" href="http://papers.nips.cc/paper/4473-hierarchical-matching-pursuit-for-image-classification-architecture-and-fast-algorithms.pdf">pdf</a></p><p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efﬁciently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). 1</p><p>Reference: <a title="nips-2011-113-reference" href="../nips2011_reference/nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. [sent-2, score-1.388]
</p><p>2 It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. [sent-3, score-0.73]
</p><p>3 To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. [sent-5, score-1.239]
</p><p>4 We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. [sent-8, score-0.545]
</p><p>5 HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). [sent-9, score-0.555]
</p><p>6 Deep belief nets [9] built a hierarchy of features by greedily training each layer separately using the restricted Boltzmann machine. [sent-14, score-0.302]
</p><p>7 [16] proposed convolutional deep belief networks (CDBN) that use a small receptive ﬁeld and share the weights between the hidden and visible layers among all locations in an image. [sent-17, score-0.254]
</p><p>8 Invariant predictive sparse decomposition [11, 13] used feed-forward neural networks to approximate sparse codes generated by sparse coding and avoided solving computationally expensive optimizations at runtime. [sent-18, score-0.569]
</p><p>9 Deconvolutional networks [26] reconstructed images using a group of latent feature maps in a convolutional way under a sparsity constraint. [sent-19, score-0.248]
</p><p>10 A fast optimization algorithm was introduced to solve the resulting sparse coding problem. [sent-20, score-0.244]
</p><p>11 Recent research has shown that single layer sparse coding on top of SIFT features works surprisingly well [15, 24, 23, 5, 6]. [sent-22, score-0.481]
</p><p>12 [24] proposed a single layer feature learning model ScSPM that uses SIFT features as the input to sparse coding instead of raw image patches. [sent-24, score-0.611]
</p><p>13 Their experiments have shown that this approach outperforms the classical bag-of-visual-words model and convolutional deep belief networks, and achieves the state-of-the-art performance on many image classiﬁcation benchmarks. [sent-25, score-0.262]
</p><p>14 presented a fast implementation of local coordinate coding [23] that obtains sparse representations of SIFT features by performing local linear embedding on several nearest visual words in the codebook. [sent-27, score-0.344]
</p><p>15 [5] compared many feature learning algorithms, and found that the SIFT based sparse coding in conjunction with max pooling performs remarkably well, and the macrofeatures can boost recognition performance further. [sent-29, score-0.569]
</p><p>16 Coates and Ng [6] evaluated many single layer feature learning systems by decomposing feature learning algorithms into training and encoding phases, and suggested that the choice of architecture and encoder is the key to a successful feature learning system. [sent-30, score-0.505]
</p><p>17 [25] showed that hierarchical sparse coding (HSC) at pixel level achieves similar performance with SIFT based sparse coding. [sent-32, score-0.476]
</p><p>18 However, single layer sparse coding heavily depends on hand-crafted SIFT features. [sent-33, score-0.433]
</p><p>19 Motivated by the recent work on deep networks, in this work we propose hierarchical matching pursuit (HMP) that uses the matching pursuit encoder to build a feature hierarchy layer by layer. [sent-35, score-1.782]
</p><p>20 The matching pursuit encoder consists of three modules: batch tree orthogonal matching pursuit coding, spatial pyramid max pooling, and contrast normalization. [sent-36, score-1.925]
</p><p>21 We discuss the architecture of HMP, and show that spatial pyramid max pooling, contrast normalization, and hierarchical structure are key components to learn good representations for recognition. [sent-37, score-0.438]
</p><p>22 We further present batch tree orthogonal matching pursuit that is able to speed up the search of sparse codes signiﬁcantly when a large number of observations share the same dictionary. [sent-38, score-1.061]
</p><p>23 Our experiments on object recognition, scene recognition, and static event recognition conﬁrm that HMP yields better accuracy than hierarchical feature learning, SIFT based single layer sparse coding, and many other state-of-the-art image classiﬁcation algorithms on standard datasets. [sent-40, score-0.809]
</p><p>24 2  Hierarchical Matching Pursuit  In this section, we introduce hierarchical matching pursuit. [sent-42, score-0.307]
</p><p>25 We then propose the matching pursuit encoder, and investigate its architecture and fast algorithms to compute sparse codes. [sent-44, score-0.763]
</p><p>26 Finally, we discuss how to build hierarchical matching pursuit based on the matching pursuit encoder. [sent-45, score-1.316]
</p><p>27 ∀i,  xi  0  ≤K  (1)  where the notation A F denotes the Frobenius norm, xi are the columns of X, the zero-norm · 0 counts the non-zero entries in the sparse code xi , and K is the sparsity level, which bounds the number of the non-zero entries. [sent-52, score-0.221]
</p><p>28 End This optimization problem is combinational and highly non-convex, but its approximate solution can be found by the orthogonal matching pursuit discussed in the next section. [sent-66, score-0.751]
</p><p>29 In the second stage, the dictionary D and its associated sparse coefﬁcients are updated simultaneously by the Singular Value Decomposition (SVD). [sent-67, score-0.265]
</p><p>30 To avoid the introduction of new non-zero entries in the sparse code matrix X, the update process only uses the observations whose sparse codes have used the k-th ﬁlter (the k-th entry of the associated sparse code is non-zero). [sent-70, score-0.499]
</p><p>31 When the sparsity level K is set to be 1 and the sparse code matrix is forced to be a binary(0/1) matrix, K-SVD exactly reproduces the K-Means algorithm. [sent-71, score-0.236]
</p><p>32 2  Matching Pursuit Encoder  Our matching pursuit encoder consists of three modules: batch tree orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. [sent-73, score-1.531]
</p><p>33 The orthogonal matching pursuit (OMP) [19] computes an approximate solution for the optimization problem Eq. [sent-75, score-0.751]
</p><p>34 In our application, sparse codes for a large number of image patches are computed by the same dictionary. [sent-84, score-0.334]
</p><p>35 The total cost of orthogonal matching pursuit can be reduced by batch orthogonal matching pursuit (BOMP) (Algorithm 1) that pre-computes some quantities [7, 22]. [sent-85, score-1.588]
</p><p>36 In orthogonal matching pursuit, we have K ≤ h since the h ﬁlters allow us to exactly reconstruct the observations. [sent-89, score-0.357]
</p><p>37 Note that the cost of searching sparse codes quickly dominates that of the pre-computation as observations increase. [sent-90, score-0.179]
</p><p>38 In our experiments, K is 10 and h is several hundreds in the second layer of HMP, and we have observed signiﬁcant speedup (Section 3) over orthogonal matching pursuit. [sent-92, score-0.546]
</p><p>39 To overcome this problem, we propose batch tree orthogonal matching pursuit (BTOMP) (Algorithm 2) that organizes the dictionary using a tree structure. [sent-105, score-1.084]
</p><p>40 If o = m, BTOMP exactly recovers the batch orthogonal matching pursuit. [sent-110, score-0.443]
</p><p>41 Spatial pyramid max pooling is a highly nonlinear operator that generates higher level representations from sparse codes of local patches which are spatially close. [sent-112, score-0.588]
</p><p>42 It aggregates these sparse codes using max pooling in a multi-level patch decomposition. [sent-113, score-0.368]
</p><p>43 The magnitude of sparse codes varies over a wide range due to local variations in illumination and foreground-background contrast, so effective local contrast normalization turns out to be essential for good recognition performance. [sent-118, score-0.33]
</p><p>44 We found that the best value in the ﬁrst layer is around 0. [sent-122, score-0.189]
</p><p>45 This is intuitive because a small threshold is able to make low contrast patches more separate from high contrast image patches, increasing the discrimination of features. [sent-125, score-0.207]
</p><p>46 3  Hierarchical Matching Pursuit  The matching pursuit encoder in the second layer is built on top of the outputs of the matching pursuit encoder in the ﬁrst layer. [sent-128, score-1.701]
</p><p>47 In the ﬁrst layer, sparse codes from small image patches are aggregated into patch-level features. [sent-130, score-0.334]
</p><p>48 In the second layer, sparse codes from patch-level features are aggregated across the whole image to produce image-level features. [sent-131, score-0.312]
</p><p>49 Batch tree orthogonal matching pursuit is used to compute sparse codes in each layer. [sent-132, score-0.975]
</p><p>50 layer is trained, its dictionary is ﬁxed, and its outputs are used as inputs to the next layer. [sent-133, score-0.346]
</p><p>51 We enforce that the patch size in the second layer is larger than that in the ﬁrst layer, which makes sure that a higher level representation is extracted in the higher layer. [sent-134, score-0.271]
</p><p>52 3  Experiments  We compare hierarchical matching pursuit with many state-of-the-art image classiﬁcation algorithms on three publicly available datasets: Caltech101, MIT-Scene, and UIUC-Sports. [sent-136, score-0.786]
</p><p>53 We use two-layer hierarchical matching pursuit in all experiments. [sent-139, score-0.701]
</p><p>54 We learn the dictionary in the two layers by performing K-SVD on 1,000,000 sampled patches. [sent-141, score-0.196]
</p><p>55 In the ﬁrst layer, we remove the zero frequency component from image patches by subtracting their means, and initialize K-SVD with the overcomplete discrete cosine transform (DCT) dictionary. [sent-142, score-0.278]
</p><p>56 Our pre-processing is simpler than other feature learning approaches that normalize image patches by dividing the standard deviation and then whitening the normalized image patches [6]. [sent-143, score-0.355]
</p><p>57 We set the number of the ﬁlters to be 3 times the ﬁlter size in the ﬁrst layer and to be 1000 in the second layer. [sent-145, score-0.189]
</p><p>58 We use batch orthogonal matching pursuit to compute sparse codes. [sent-146, score-0.945]
</p><p>59 We perform max pooling in a 3-level spatial pyramid, partitioned into 1 × 1, 2 × 2, and 4 × 4 sub-regions. [sent-148, score-0.247]
</p><p>60 In the ﬁrst layer, we run the matching pursuit encoder on 16 × 16 image patches over dense grids with a step size of 4 pixels. [sent-149, score-0.934]
</p><p>61 In the second layer, we run the matching pursuit encoder on the whole image to produce the image-level features. [sent-150, score-0.841]
</p><p>62 For computational efﬁciency, we perform our spatial pyramid max pooling across the image with a step size of 4 pixels, rather than at each pixel. [sent-151, score-0.463]
</p><p>63 The ﬁlter size in the ﬁrst layer is optimized by 5-fold cross validation on the training set. [sent-156, score-0.189]
</p><p>64 We compare HMP to SIFT based single layer sparse coding because of its success in both computer vision and machine learning communities [24, 23, 5, 6]. [sent-157, score-0.433]
</p><p>65 We extract SIFT with 16×16 image patches over dense regular grids with spacing of 8 pixels. [sent-158, score-0.178]
</p><p>66 We perform sparse coding feature extraction using 1,000 visual words learned from 1,000,000 SIFT features, and compute image-level features by running spatial pyramid max pooling on 1 × 1, 2 × 2 and 4 × 4 sub-regions [24]. [sent-163, score-0.763]
</p><p>67 5  Figure 2: Left: The overcomplete DCT dictionary with 144 ﬁlters of size 6 × 6. [sent-164, score-0.28]
</p><p>68 We consider the orthogonal and overcomplete DCT dictionaries, and the overcomplete K-SVD dictionary. [sent-210, score-0.382]
</p><p>69 We have found that the orthogonal DCT achieves the highest accuracy when all the ﬁlters are chosen (without sparsity), and the overcomplete DCT and K-SVD have good accuracy at the sparsity level T = 5. [sent-211, score-0.445]
</p><p>70 We keep the overcomplete DCT dictionary and the K-SVD dictionary to have roughly similar sizes. [sent-212, score-0.437]
</p><p>71 From Table 1, we see that the orthogonal DCT dictionary works surprisingly well, and is very competitive with current state-of-the-art feature learning algorithms (see Table 3). [sent-213, score-0.338]
</p><p>72 The overcomplete K-SVD dictionary performs consistently better than the DCT dictionary. [sent-214, score-0.28]
</p><p>73 We show the overcomplete DCT dictionary and the K-SVD dictionary in Fig. [sent-217, score-0.437]
</p><p>74 As we see, the KSVD dictionary not only includes the edge and dot ﬁlters, but also texture, multi-peaked, and high frequency ﬁlters, and is much more diverse than the overcomplete DCT dictionary. [sent-219, score-0.28]
</p><p>75 Spatial pyramid max pooling introduces the different levels of spatial information, and always outperforms ﬂat spatial max pooling (4×4) by about 2% in our experiments. [sent-221, score-0.625]
</p><p>76 Our experiments show that contrast normalization improves recognition accuracy by about 3%, which suggests this is a very useful module for feature learning. [sent-224, score-0.251]
</p><p>77 We show recognition accuracy as a function of the sparsity level K in Fig. [sent-226, score-0.218]
</p><p>78 When sparsity level in ﬁrst or second layer varies, the other parameters are ﬁxed to the default setting. [sent-229, score-0.289]
</p><p>79 We see that the accuracy is more robust to the zero-norm in the ﬁrst layer while more sensitive in the second layer. [sent-230, score-0.244]
</p><p>80 BOMP is about 10x faster in the second layer in our default setting, which dominates the running time of feature extraction. [sent-234, score-0.258]
</p><p>81 Efﬁcient feature-sign search algorithm [15] is used to solve the sparse coding problem with an L1 penalty. [sent-237, score-0.244]
</p><p>82 HMP is much faster than single layer sparse coding and deconvolutional networks. [sent-239, score-0.512]
</p><p>83 HMP(DCT) means that the orthogonal DCT dictionary is used in the ﬁrst layer. [sent-247, score-0.293]
</p><p>84 SIFT+SC denotes single layer sparse coding based on SIFT features. [sent-249, score-0.433]
</p><p>85 In the ﬁrst two columns, we see that HMP performs much better than other hierarchial feature learning approaches: invariant predictive sparse decomposition (IPSD) [12, 13], convolutional deep belief networks (CDBN) [16], and deconvolutional networks (DN) [26]. [sent-257, score-0.485]
</p><p>86 In the middle two columns, we show that HMP outperforms single layer sparse coding approaches on top of SIFT features: soft threshold coding (SIFT+T) [6], locality-constrained linear coding (LLC) [23] and Macrofeatures based sparse coding [5], and hierarchical sparse coding [25]. [sent-258, score-1.279]
</p><p>87 In the right two columns, we compare HMP with naive Bayesian nearest neighbor (NBNN) [4] and three representative kernel methods: spatial pyramid matching (SPM) [14], metric learning for CORR kernel (ML+CORR) [10], and gradient kernel descriptors (KDES-G) [3, 2]. [sent-260, score-0.556]
</p><p>88 Hierarchical matching pursuit is more than 10% better than SPM, a widely accepted baseline, and slightly better than NBNN and KDES-G in terms of accuracy. [sent-262, score-0.615]
</p><p>89 Slightly higher accuracy (around 80%) has been reported with multiple kernel learning that combines many different types of image features [8]. [sent-264, score-0.214]
</p><p>90 Hierarchical matching pursuit is compared to recently published object recognition algorithms. [sent-283, score-0.787]
</p><p>91 2  Scene Recognition  We evaluate hierarchical matching pursuit for scene recognition on the MIT-Scene dataset [20]. [sent-285, score-0.84]
</p><p>92 8% with the ﬁlter size 4 × 4, more than 15 percent higher than GIST features based approach, and about 5 percent higher than SIFT based sparse coding and object bank. [sent-302, score-0.355]
</p><p>93 Object bank is a recently proposed high-level feature, which trains 200 object detectors using the object bounding boxes from the LabelMe and ImageNet dataset, and runs them across an image at different scales to produce image features. [sent-303, score-0.33]
</p><p>94 3  Event Recognition  We evaluate hierarchical matching pursuit for static event recognition on the UIUC-Sports dataset [18]. [sent-306, score-0.827]
</p><p>95 As we see, HMP signiﬁcantly outperforms SIFT based generative graphical model, SIFT based single layer sparse coding, and the recent object bank approach signiﬁcantly. [sent-311, score-0.394]
</p><p>96 4  Conclusion  We have proposed hierarchical matching pursuit, to learn meaningful multi-level representations from images layer by layer. [sent-321, score-0.573]
</p><p>97 Hierarchical matching pursuit uses the matching pursuit encoder to build a feature hierarchy that consists of three modules: batch tree orthogonal matching pursuit, spatial pyramid matching, and contrast normalization. [sent-322, score-2.19]
</p><p>98 In addition, we have proposed batch tree orthogonal matching pursuit to speed up feature extraction at runtime. [sent-324, score-0.927]
</p><p>99 We have performed extensive comparisons on three types of image classiﬁcation tasks: object recognition, scene recognition, and event recognition. [sent-325, score-0.264]
</p><p>100 Our experiments have conﬁrmed that hierarchical matching pursuit outperforms both SIFT based single layer sparse coding and other hierarchical feature learning approaches: convolutional deep belief networks, convolutional neural networks and deconvolutional networks. [sent-326, score-1.634]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmp', 0.54), ('pursuit', 0.394), ('matching', 0.221), ('dct', 0.209), ('sift', 0.208), ('layer', 0.189), ('dictionary', 0.157), ('btomp', 0.143), ('encoder', 0.141), ('coding', 0.136), ('orthogonal', 0.136), ('pyramid', 0.131), ('overcomplete', 0.123), ('pooling', 0.117), ('sparse', 0.108), ('spatial', 0.102), ('lter', 0.101), ('bomp', 0.095), ('recognition', 0.087), ('batch', 0.086), ('hierarchical', 0.086), ('image', 0.085), ('deconvolutional', 0.079), ('lters', 0.076), ('convolutional', 0.075), ('di', 0.075), ('codes', 0.071), ('patches', 0.07), ('deep', 0.064), ('object', 0.063), ('accuracy', 0.055), ('code', 0.052), ('scene', 0.052), ('images', 0.052), ('features', 0.048), ('macrofeatures', 0.048), ('cvpr', 0.046), ('tree', 0.045), ('feature', 0.045), ('patch', 0.044), ('modules', 0.044), ('nbnn', 0.042), ('spm', 0.042), ('architecture', 0.04), ('sc', 0.04), ('dk', 0.039), ('layers', 0.039), ('event', 0.039), ('argmaxk', 0.038), ('cdbn', 0.038), ('level', 0.038), ('belief', 0.038), ('normalization', 0.038), ('networks', 0.038), ('sparsity', 0.038), ('ob', 0.036), ('llc', 0.034), ('bank', 0.034), ('residual', 0.033), ('seattle', 0.033), ('ggm', 0.032), ('hsc', 0.032), ('ipsd', 0.032), ('kavukcuoglu', 0.031), ('indoor', 0.03), ('bo', 0.029), ('boureau', 0.028), ('max', 0.028), ('rubinstein', 0.028), ('aharon', 0.028), ('hierarchy', 0.027), ('updating', 0.027), ('visual', 0.027), ('wa', 0.027), ('dictionaries', 0.027), ('ek', 0.027), ('svm', 0.026), ('kernel', 0.026), ('contrast', 0.026), ('dx', 0.026), ('gist', 0.026), ('roi', 0.026), ('classi', 0.025), ('scalable', 0.025), ('representations', 0.025), ('comparisons', 0.025), ('default', 0.024), ('corr', 0.024), ('descriptors', 0.024), ('table', 0.024), ('grids', 0.023), ('dn', 0.023), ('columns', 0.023), ('published', 0.022), ('intel', 0.022), ('coates', 0.022), ('yu', 0.022), ('mh', 0.021), ('learned', 0.021), ('rh', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="113-tfidf-1" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efﬁciently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). 1</p><p>2 0.18739389 <a title="113-tfidf-2" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Sonia A. Bhaskar, Pang W. Koh, Andrew Y. Ng</p><p>Abstract: Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classiﬁcation. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse ﬁltering, a simple new algorithm which is efﬁcient and only has one hyperparameter, the number of features to learn. In contrast to most other feature learning methods, sparse ﬁltering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function – the sparsity of 2 -normalized features – which can easily be implemented in a few lines of MATLAB code. Sparse ﬁltering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse ﬁltering on natural images, object classiﬁcation (STL-10), and phone classiﬁcation (TIMIT), and show that our method works well on a range of different modalities. 1</p><p>3 0.18612789 <a title="113-tfidf-3" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>Author: Adam Coates, Andrew Y. Ng</p><p>Abstract: Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded “local receptive ﬁelds” that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Speciﬁcally, we choose local receptive ﬁelds that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive ﬁelds (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive ﬁelds by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered networks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively. 1</p><p>4 0.15307686 <a title="113-tfidf-4" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><p>5 0.14746055 <a title="113-tfidf-5" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>Author: Arthur D. Szlam, Karol Gregor, Yann L. Cun</p><p>Abstract: This work describes a conceptually simple method for structured sparse coding and dictionary design. Supposing a dictionary with K atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms. We describe modiﬁcations of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efﬁcient. We show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures. Finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-speciﬁed. 1</p><p>6 0.14300501 <a title="113-tfidf-6" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>7 0.13871041 <a title="113-tfidf-7" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>8 0.13618723 <a title="113-tfidf-8" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>9 0.1359155 <a title="113-tfidf-9" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>10 0.12891071 <a title="113-tfidf-10" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>11 0.11396056 <a title="113-tfidf-11" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>12 0.10982729 <a title="113-tfidf-12" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>13 0.10758066 <a title="113-tfidf-13" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>14 0.1034798 <a title="113-tfidf-14" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>15 0.095516749 <a title="113-tfidf-15" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>16 0.091753878 <a title="113-tfidf-16" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>17 0.087824099 <a title="113-tfidf-17" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>18 0.085992843 <a title="113-tfidf-18" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>19 0.084833525 <a title="113-tfidf-19" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>20 0.084137626 <a title="113-tfidf-20" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.199), (1, 0.169), (2, -0.066), (3, 0.089), (4, 0.022), (5, 0.117), (6, 0.18), (7, 0.241), (8, 0.013), (9, -0.116), (10, -0.039), (11, -0.021), (12, 0.015), (13, 0.063), (14, -0.04), (15, -0.005), (16, 0.022), (17, -0.007), (18, -0.028), (19, 0.029), (20, 0.128), (21, -0.027), (22, -0.02), (23, -0.084), (24, 0.027), (25, -0.051), (26, 0.051), (27, 0.076), (28, -0.074), (29, 0.09), (30, -0.061), (31, -0.011), (32, -0.083), (33, 0.053), (34, -0.049), (35, -0.078), (36, 0.067), (37, 0.035), (38, 0.028), (39, 0.083), (40, 0.042), (41, 0.07), (42, -0.014), (43, -0.019), (44, 0.015), (45, -0.034), (46, 0.007), (47, 0.036), (48, 0.061), (49, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96515542 <a title="113-lsi-1" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efﬁciently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). 1</p><p>2 0.84296024 <a title="113-lsi-2" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>Author: Nobuyuki Morioka, Shin'ichi Satoh</p><p>Abstract: Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, 1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents “Generalized Lasso based Approximation of Sparse coding” (GLAS). By representing the distribution of sparse coefﬁcients with slice transform, we ﬁt a piece-wise linear mapping function with the generalized lasso. We also propose an efﬁcient post-reﬁnement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to 1 regularized sparse coding, yet achieves a signiﬁcant speed up demonstrating its effectiveness for large-scale visual recognition problems. 1</p><p>3 0.72018266 <a title="113-lsi-3" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Sonia A. Bhaskar, Pang W. Koh, Andrew Y. Ng</p><p>Abstract: Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classiﬁcation. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse ﬁltering, a simple new algorithm which is efﬁcient and only has one hyperparameter, the number of features to learn. In contrast to most other feature learning methods, sparse ﬁltering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function – the sparsity of 2 -normalized features – which can easily be implemented in a few lines of MATLAB code. Sparse ﬁltering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse ﬁltering on natural images, object classiﬁcation (STL-10), and phone classiﬁcation (TIMIT), and show that our method works well on a range of different modalities. 1</p><p>4 0.67982423 <a title="113-lsi-4" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><p>5 0.63356078 <a title="113-lsi-5" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>6 0.6310339 <a title="113-lsi-6" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>7 0.62545699 <a title="113-lsi-7" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>8 0.59305376 <a title="113-lsi-8" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>9 0.56460238 <a title="113-lsi-9" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>10 0.56053555 <a title="113-lsi-10" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>11 0.54093134 <a title="113-lsi-11" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>12 0.53768176 <a title="113-lsi-12" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>13 0.5324806 <a title="113-lsi-13" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>14 0.5000394 <a title="113-lsi-14" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>15 0.49820223 <a title="113-lsi-15" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>16 0.49437392 <a title="113-lsi-16" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>17 0.48780167 <a title="113-lsi-17" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>18 0.4760161 <a title="113-lsi-18" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>19 0.44763866 <a title="113-lsi-19" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>20 0.44568345 <a title="113-lsi-20" href="./nips-2011-Extracting_Speaker-Specific_Information_with_a_Regularized_Siamese_Deep_Network.html">93 nips-2011-Extracting Speaker-Specific Information with a Regularized Siamese Deep Network</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.036), (4, 0.019), (20, 0.038), (26, 0.011), (31, 0.047), (33, 0.055), (43, 0.044), (45, 0.141), (52, 0.151), (57, 0.034), (65, 0.138), (74, 0.068), (83, 0.032), (84, 0.044), (99, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85267425 <a title="113-lda-1" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efﬁciently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). 1</p><p>2 0.78438008 <a title="113-lda-2" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>Author: Quoc V. Le, Alexandre Karpenko, Jiquan Ngiam, Andrew Y. Ng</p><p>Abstract: Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an orthonoramlity constraint to be enforced, which makes it difﬁcult to learn overcomplete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets. 1</p><p>3 0.78375739 <a title="113-lda-3" href="./nips-2011-Extracting_Speaker-Specific_Information_with_a_Regularized_Siamese_Deep_Network.html">93 nips-2011-Extracting Speaker-Specific Information with a Regularized Siamese Deep Network</a></p>
<p>Author: Ke Chen, Ahmad Salman</p><p>Abstract: Speech conveys different yet mixed information ranging from linguistic to speaker-speciﬁc components, and each of them should be exclusively used in a speciﬁc task. However, it is extremely difﬁcult to extract a speciﬁc information component given the fact that nearly all existing acoustic representations carry all types of speech information. Thus, the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information. In this paper, we present a deep neural architecture to extract speaker-speciﬁc information from MFCCs. As a result, a multi-objective loss function is proposed for learning speaker-speciﬁc characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss. With LDC benchmark corpora and a Chinese speech corpus, we demonstrate that a resultant speaker-speciﬁc representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other state-of-the-art techniques in speaker recognition. We discuss relevant issues and relate our approach to previous work. 1</p><p>4 0.78194433 <a title="113-lda-4" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>5 0.77189314 <a title="113-lda-5" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>Author: Jiquan Ngiam, Zhenghao Chen, Sonia A. Bhaskar, Pang W. Koh, Andrew Y. Ng</p><p>Abstract: Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classiﬁcation. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse ﬁltering, a simple new algorithm which is efﬁcient and only has one hyperparameter, the number of features to learn. In contrast to most other feature learning methods, sparse ﬁltering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function – the sparsity of 2 -normalized features – which can easily be implemented in a few lines of MATLAB code. Sparse ﬁltering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse ﬁltering on natural images, object classiﬁcation (STL-10), and phone classiﬁcation (TIMIT), and show that our method works well on a range of different modalities. 1</p><p>6 0.77051622 <a title="113-lda-6" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>7 0.76102555 <a title="113-lda-7" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>8 0.75921476 <a title="113-lda-8" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>9 0.7508862 <a title="113-lda-9" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>10 0.73965317 <a title="113-lda-10" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>11 0.68397379 <a title="113-lda-11" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>12 0.67958373 <a title="113-lda-12" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>13 0.67721087 <a title="113-lda-13" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>14 0.67425072 <a title="113-lda-14" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>15 0.67409396 <a title="113-lda-15" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>16 0.67255956 <a title="113-lda-16" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>17 0.67252147 <a title="113-lda-17" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>18 0.67228597 <a title="113-lda-18" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>19 0.67140269 <a title="113-lda-19" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>20 0.6701411 <a title="113-lda-20" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
