<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-190" href="#">nips2011-190</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2011-190-pdf" href="http://papers.nips.cc/paper/4420-nonlinear-inverse-reinforcement-learning-with-gaussian-processes.pdf">pdf</a></p><p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert’s policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions. 1</p><p>Reference: <a title="nips-2011-190-reference" href="../nips2011_reference/nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a probabilistic algorithm for nonlinear inverse reinforcement learning. [sent-7, score-0.19]
</p><p>2 The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. [sent-8, score-0.726]
</p><p>3 While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert’s policy. [sent-9, score-1.102]
</p><p>4 Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions. [sent-10, score-0.502]
</p><p>5 1  Introduction  Inverse reinforcement learning (IRL) methods learn a reward function in a Markov decision process (MDP) from expert demonstrations, allowing the expert’s policy to be generalized to unobserved situations [7]. [sent-11, score-0.774]
</p><p>6 Each task is consistent with many reward functions, but not all rewards provide a compact, portable representation of the task, so the central challenge in IRL is to ﬁnd a reward with meaningful structure [7]. [sent-12, score-0.869]
</p><p>7 Many prior methods impose structure by describing the reward as a linear combination of hand selected features [1, 12]. [sent-13, score-0.507]
</p><p>8 In this paper, we extend the Gaussian process model to learn highly nonlinear reward functions that still compactly capture the demonstrated behavior. [sent-14, score-0.461]
</p><p>9 This allows GPIRL to balance the simplicity of the learned reward function against its consistency with the expert’s actions, without assuming the expert to be optimal. [sent-17, score-0.592]
</p><p>10 The learned GP kernel hyperparameters capture the structure of the reward, including the relevance of each feature. [sent-18, score-0.174]
</p><p>11 Once learned, the GP can recover the reward for the current state space, and can predict the reward for any unseen state space within the domain of the features. [sent-19, score-0.822]
</p><p>12 While several margin-based methods learn nonlinear reward functions through feature construction [13, 14, 5], such methods assume optimal expert behavior. [sent-21, score-0.636]
</p><p>13 The optimal policy π maximizes ∞ the expected discounted sum of rewards E [ t=0 γ t rst |π ]. [sent-24, score-0.245]
</p><p>14 In inverse reinforcement learning, the algorithm is presented with M \ r, as well as expert demonstrations, denoted D = {ζ1 , . [sent-25, score-0.304]
</p><p>15 The algorithm is also presented with features of the form f : S → R that can be used to represent the unknown reward r. [sent-32, score-0.463]
</p><p>16 IRL aims to ﬁnd a reward function r under which the optimal policy matches the expert’s demonstrations. [sent-33, score-0.489]
</p><p>17 To this end, we could assume that the examples D are drawn from the optimal policy π . [sent-34, score-0.136]
</p><p>18 One approach to learning from a suboptimal expert is to use a probabilistic model of the expert’s behavior. [sent-36, score-0.197]
</p><p>19 We employ the maximum entropy IRL (MaxEnt) model [17], which is closely related to linearly-solvable MDPs [3], and has been used extensively to learn from human demonstrations [16, 17]. [sent-37, score-0.197]
</p><p>20 This model is convenient for IRL, because its likelihood is differentiable [17], and a complete stochastic policy uniquely determines the reward function [3]. [sent-39, score-0.532]
</p><p>21 Intuitively, such a stochastic policy is more deterministic when the stakes are high, and more random when all choices have similar value. [sent-40, score-0.125]
</p><p>22 Under this policy, the probability of an action a in state s can be shown to be proportional to the exponential of the expected total reward after taking the action, denoted P (a|s) ∝ exp(Qr ), where sa Qr = r + γT Vr . [sent-41, score-0.472]
</p><p>23 3  The Gaussian Process Inverse Reinforcement Learning Algorithm  GPIRL represents the reward as a nonlinear function of feature values. [sent-49, score-0.444]
</p><p>24 Since the reward is not known, we use Equation 1 to specify a distribution over GP outputs, and learn both the output values and the kernel function. [sent-52, score-0.483]
</p><p>25 We also learn the kernel hyperparameters θ in order to recover the structure of the reward. [sent-57, score-0.135]
</p><p>26 When Λ is learned, less relevant features receive low weights, and more relevant features receive high weights. [sent-62, score-0.16]
</p><p>27 States distinguished by highly-weighted features can take on different reward values, while those that have similar values for all highly-weighted features take on similar rewards. [sent-63, score-0.543]
</p><p>28 Kr,u is the covariance of the rewards at all states with the inducing point r,u u,u values u, located respectively at Xr and Xu [11]. [sent-65, score-0.206]
</p><p>29 Instead, we can consider this problem as analogous to sparse approximation for GP regression [8], where a small set of inducing points u acts as the support for the full set of training points r. [sent-67, score-0.126]
</p><p>30 This approximation is particularly appropriate in our case, because if the learned GP is used to predict a reward for a novel state space, the most likely reward would have the same form as the mean of the training conditional. [sent-70, score-0.85]
</p><p>31 The resulting log likelihood is simply r,u u,u log P (D, u, θ|Xu ) = log P (D|r = KT K−1 u) + log P (u, θ|Xu ) r,u u,u IRL log likelihood  (4)  GP log likelihood  Once the likelihood is optimized, the reward r = KT K−1 u can be used to recover the expert’s r,u u,u policy on the entire state space. [sent-72, score-0.775]
</p><p>32 The GP can also predict the reward function for any novel state space in the domain of the features. [sent-73, score-0.411]
</p><p>33 The most likely reward for a novel state space is the mean posterior KT,u K−1 u, where K ,u is the covariance of the new states and the inducing points. [sent-74, score-0.551]
</p><p>34 Intuitively, this indicates that two or more inducing points are deterministically covarying, and therefore redundant. [sent-81, score-0.13]
</p><p>35 While the regularized kernel prevents singular covariance matrices when many features become irrelevant, the log likelihood can still increase to inﬁnity as Λ → 0 or β → 0: in 1 both cases, − 2 log |Ku,u | → ∞ and, so long as u → 0, all other terms remain ﬁnite. [sent-84, score-0.219]
</p><p>36 To prevent such degeneracies, we use a hyperparameter prior that discourages kernels under which two inducing points become deterministically covarying. [sent-85, score-0.248]
</p><p>37 We can u,u therefore prevent degeneracies with a prior term of the form − 1 ij [K−1 ]2 = − 1 tr(K−2 ), which u,u ij u,u 2 2 discourages large partial correlations between inducing points. [sent-87, score-0.19]
</p><p>38 However, unlike in GP regression, Xu and u are parameters of the algorithm rather than data, and since the inducing point positions are ﬁxed in advance, it is possible to condition the prior on Xu . [sent-89, score-0.136]
</p><p>39 5  Inducing Points and Large State Spaces  A straightforward choice for the inducing points Xu is the feature values of all states in the state space S. [sent-93, score-0.19]
</p><p>40 In principle, the minimum size of Xu corresponds to the complexity of the reward function. [sent-97, score-0.383]
</p><p>41 For example, if the true reward has two constant regions, it can be represented by just two properly placed inducing points. [sent-98, score-0.475]
</p><p>42 In practice, Xu must cover the space of feature values well enough to represent an unknown reward function, but we can nonetheless use many fewer points than there are states in S. [sent-99, score-0.453]
</p><p>43 The stationary kernel in Equation 5 favors rewards that are smooth with respect to feature values. [sent-104, score-0.166]
</p><p>44 For example, a reward function might have wide regions with uniform values, punctuated by regions of high-frequency variation, as is the case for piecewise constant rewards. [sent-106, score-0.419]
</p><p>45 Instead, we can warp each coordinate xik of xi by a function wk (xik ) to give high resolution to one region, and low resolution everywhere else. [sent-108, score-0.194]
</p><p>46 Note that this procedure is not equivalent to merely ﬁtting a sigmoid to the reward function, since the reward can still vary nonlinearly in the high resolution regions around each sigmoid center mk . [sent-112, score-0.901]
</p><p>47 The accompanying supplement includes details about the priors placed on the warp parameters in our implementation, a σ complete derivation of wk , and the derivatives of the warped kernel function. [sent-113, score-0.237]
</p><p>48 We presented just one example of how an alternative kernel allows us to learn a reward with a particular structure. [sent-119, score-0.483]
</p><p>49 7  Experiments  We compared GPIRL with prior methods on several IRL tasks, using examples sampled from the stochastic MaxEnt policy (see Section 2) as well as human demonstrations. [sent-121, score-0.238]
</p><p>50 Examples drawn from the stochastic policy can intuitively be viewed as noisy samples of an underlying optimal policy, while the human demonstrations contain the stochasticity inherent in human behavior. [sent-122, score-0.322]
</p><p>51 We compare the algorithms using the “expected value difference” score, which is a measure of how suboptimal the learned policy is under the true reward. [sent-126, score-0.206]
</p><p>52 To compute this score, we ﬁnd the optimal deterministic policy under each learned reward, measure its expected sum of discounted rewards under the true reward function, and subtract this quantity from the expected sum of discounted rewards under the true policy. [sent-127, score-0.823]
</p><p>53 To determine how well each algorithm captured the structure of the reward function, we evaluated the learned reward on the environment on which it was learned, and on 4 additional random environments (denoted “transfer”). [sent-129, score-0.874]
</p><p>54 Algorithms that do not express the reward function in terms of the correct features are expected to perform poorly on the transfer environments, even if they perform well on the training environment. [sent-130, score-0.564]
</p><p>55 In the latter case, GPIRL used the warped kernel in Section 6 and FIRL, which requires discrete features, was not tested. [sent-133, score-0.162]
</p><p>56 1  Objectworld Experiments  The objectworld is an N × N grid of states with ﬁve actions per state, corresponding to steps in each direction and staying in place. [sent-136, score-0.123]
</p><p>57 The true reward is positive in states that are both within 3 cells of outer color 1 and 2 cells of outer color 2, negative within 3 cells of outer color 1, and zero otherwise. [sent-145, score-0.642]
</p><p>58 Inner colors and all other outer colors are distractors. [sent-146, score-0.157]
</p><p>59 GPIRL learned accurate rewards that generalized well to new state spaces. [sent-150, score-0.167]
</p><p>60 Because of the large number of irrelevant features and the nonlinearity of the reward, this example is particularly challenging for methods that learn linear reward functions. [sent-153, score-0.522]
</p><p>61 With 16 or more examples, GPIRL consistently learned reward functions that performed as well as the true reward, as shown in Figure 1, and was able to sustain this performance as the number of distractors increased, as shown in Figure 2. [sent-154, score-0.481]
</p><p>62 In the case of FIRL, this was likely due to the suboptimal expert examples. [sent-156, score-0.197]
</p><p>63 In the case of MaxEnt, although the Laplace prior improved the results, the inability to represent nonlinear rewards limited the algorithm’s accuracy. [sent-157, score-0.166]
</p><p>64 These issues are evident in Figure 3, which shows part of a reward function learned by each method. [sent-158, score-0.439]
</p><p>65 When using continuous features, the performance of MaxEnt suffered even more from the increased nonlinearity of the reward function, while GPIRL maintained a similar level of accuracy. [sent-159, score-0.439]
</p><p>66 True Reward  outer color 1 objects  GPIRL  MaxEnt/Lp  outer color 2 objects  other objects (distractors)  FIRL  expert actions  Figure 3: Part of a reward function learned by each algorithm on an objectworld. [sent-160, score-0.769]
</p><p>67 While GPIRL learned the correct reward function, MaxEnt was unable to represent the nonlinearities, and FIRL learned an overly complex reward under which the suboptimal expert would have been optimal. [sent-161, score-1.075]
</p><p>68 While GPIRL achieved only modest improvement over prior methods on the training environment, the large improvement in the transfer tests indicates that the underlying reward structure was captured more accurately. [sent-163, score-0.49]
</p><p>69 GPIRL learned a reward function that more accurately reﬂected the true policy the expert was attempting to emulate. [sent-165, score-0.698]
</p><p>70 2  Highway Driving Behavior  In addition to the objectworld environment, we evaluated the algorithms on more concrete behaviors in the context of a simple highway driving simulator, modeled on the experiment in [5] and similar evaluations in other work [1]. [sent-167, score-0.153]
</p><p>71 Continuous features indicate the distance to the nearest vehicle of a speciﬁc class (car or motorcycle) or category (civilian or police) in front of the agent, either in the same lane, the lane to the right, the lane to the left, or any lane. [sent-171, score-0.193]
</p><p>72 Another set of features gives the distance to the nearest such vehicle in a given lane behind the agent. [sent-172, score-0.15]
</p><p>73 In this section, we present results from synthetic and manmade demonstrations of a policy that drives as fast as possible, but avoids driving more than double the speed of trafﬁc within two car-lengths of a police vehicle. [sent-175, score-0.36]
</p><p>74 Due to the connection between the police and speed features, the reward for this policy is nonlinear. [sent-176, score-0.588]
</p><p>75 We also evaluated a second policy that instead avoids driving more than double the speed of trafﬁc in the rightmost lane. [sent-177, score-0.166]
</p><p>76 Figure 4 shows a comparison of GPIRL and prior algorithms on highways with varying numbers of 32-step synthetic demonstrations of the “police” task. [sent-179, score-0.19]
</p><p>77 GPIRL only modestly outperformed prior methods on the training environments with discrete features, but achieved large improvement on the transfer experiment. [sent-180, score-0.167]
</p><p>78 This indicates that, while prior algorithms learned a reasonable reward, this reward was not expressed in terms of the correct features, and did not generalize correctly. [sent-181, score-0.483]
</p><p>79 With continuous features, the nonlinearity of the reward was further exacerbated, making it difﬁcult for linear methods to represent it even on the training environment. [sent-182, score-0.439]
</p><p>80 7  MaxEnt/Lp  True Reward  FIRL  GPIRL  Figure 6: Highway reward functions learned from human demonstration. [sent-184, score-0.478]
</p><p>81 Road color indicates the reward at the highest speed, when the agent should be penalized for driving fast near police vehicles. [sent-185, score-0.534]
</p><p>82 The reward learned by GPIRL most closely resembles the true one. [sent-186, score-0.439]
</p><p>83 Although the human demonstrations were suboptimal, GPIRL was still able to learn a reward function that reﬂected the true policy more accurately than prior methods. [sent-187, score-0.73]
</p><p>84 Furthermore, the similarity of GPIRL’s performance with the human and synthetic demonstrations suggests that its model of suboptimal expert behavior is a reasonable reﬂection of actual human suboptimality. [sent-188, score-0.394]
</p><p>85 An example of rewards learned from human demonstrations is shown in Figure 6. [sent-189, score-0.297]
</p><p>86 htm  8  Discussion and Future Work  We presented an algorithm for inverse reinforcement learning that represents nonlinear reward functions with Gaussian processes. [sent-193, score-0.573]
</p><p>87 Using a probabilistic model of a stochastic expert with a GP prior on reward values, our method is able to recover both a reward function and the hyperparameters of a kernel function that describes the structure of the reward. [sent-194, score-1.078]
</p><p>88 The learned GP can be used to predict a reward function consistent with the expert on any state space in the domain of the features. [sent-195, score-0.62]
</p><p>89 In experiments with nonlinear reward functions, GPIRL consistently outperformed prior methods, especially when generalizing the learned reward to new state spaces. [sent-196, score-0.955]
</p><p>90 When using the warped kernel function, a random restart procedure was needed to consistently ﬁnd a good optimum. [sent-198, score-0.182]
</p><p>91 When good features that form a linear basis for the reward are already known, prior methods such as MaxEnt would be expected to perform comparably to GPIRL. [sent-201, score-0.545]
</p><p>92 When presented with a novel state space, GPIRL currently uses the mean posterior of the GP to estimate the reward function. [sent-203, score-0.428]
</p><p>93 In principle, we could leverage the fact that GPs learn distributions over functions to account for the uncertainty about the reward in states that are different from any of the inducing points. [sent-204, score-0.545]
</p><p>94 For example, such an approach could be used to learn a “conservative” policy that aims to achieve high rewards with some degree of certainty, avoiding regions where the reward distribution has high variance. [sent-205, score-0.629]
</p><p>95 In an interactive training setting, such a method could also inform the expert about states that have high reward variance and require additional demonstrations. [sent-206, score-0.567]
</p><p>96 More generally, by introducing Gaussian processes into inverse reinforcement learning, GPIRL can beneﬁt from the wealth of prior work on Gaussian process regression. [sent-207, score-0.214]
</p><p>97 For instance, we apply ideas from sparse GP approximation in the use of a small set of inducing points to learn the reward function in time linear in the number of states. [sent-208, score-0.531]
</p><p>98 A substantial body of prior work discusses techniques for automatically choosing or optimizing these inducing points [8], and such methods could be incorporated into GPIRL to learn reward functions with even smaller active sets. [sent-209, score-0.575]
</p><p>99 We also demonstrate how different kernels can be used to learn different types of reward structure, and further investigation into the kinds of kernel functions that are useful for IRL is another exciting avenue for future work. [sent-210, score-0.523]
</p><p>100 Apprenticeship learning using inverse reinforcement learning and a gradient methods. [sent-250, score-0.151]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gpirl', 0.666), ('reward', 0.383), ('maxent', 0.29), ('firl', 0.213), ('gp', 0.165), ('expert', 0.153), ('xu', 0.15), ('irl', 0.129), ('demonstrations', 0.119), ('policy', 0.106), ('xik', 0.097), ('reinforcement', 0.093), ('inducing', 0.092), ('rewards', 0.083), ('features', 0.08), ('police', 0.075), ('objectworld', 0.067), ('warped', 0.067), ('transfer', 0.063), ('kernel', 0.061), ('inverse', 0.058), ('learned', 0.056), ('wk', 0.053), ('outer', 0.053), ('colors', 0.052), ('highway', 0.05), ('sigmoid', 0.049), ('suboptimal', 0.044), ('prior', 0.044), ('lane', 0.043), ('learn', 0.039), ('human', 0.039), ('nonlinear', 0.039), ('expected', 0.038), ('qr', 0.037), ('continuous', 0.036), ('driving', 0.036), ('xjk', 0.035), ('kt', 0.035), ('hyperparameters', 0.035), ('discrete', 0.034), ('restart', 0.032), ('ratliff', 0.032), ('states', 0.031), ('examples', 0.03), ('apprenticeship', 0.03), ('bagnell', 0.029), ('traf', 0.029), ('state', 0.028), ('xj', 0.028), ('vehicle', 0.027), ('difference', 0.027), ('log', 0.027), ('civilian', 0.027), ('degeneracies', 0.027), ('discourages', 0.027), ('dvijotham', 0.027), ('highways', 0.027), ('popovi', 0.027), ('zoran', 0.027), ('environments', 0.026), ('environment', 0.026), ('actions', 0.025), ('hyperparameter', 0.024), ('gaussian', 0.024), ('likelihood', 0.024), ('speed', 0.024), ('vladlen', 0.023), ('levine', 0.023), ('warp', 0.023), ('color', 0.023), ('sa', 0.023), ('kernels', 0.023), ('relevance', 0.022), ('feature', 0.022), ('consistently', 0.022), ('imitation', 0.022), ('xi', 0.021), ('deterministically', 0.021), ('vehicles', 0.02), ('abbeel', 0.02), ('portable', 0.02), ('distractors', 0.02), ('rasmussen', 0.02), ('nonlinearity', 0.02), ('stochastic', 0.019), ('optima', 0.019), ('mk', 0.019), ('processes', 0.019), ('vr', 0.018), ('discounted', 0.018), ('regions', 0.018), ('car', 0.018), ('tr', 0.017), ('agent', 0.017), ('avenue', 0.017), ('supplement', 0.017), ('posterior', 0.017), ('points', 0.017), ('priors', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="190-tfidf-1" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert’s policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions. 1</p><p>2 0.37749866 <a title="190-tfidf-2" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an inﬁnite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms. 1</p><p>3 0.19678469 <a title="190-tfidf-3" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>4 0.1204536 <a title="190-tfidf-4" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>5 0.1160742 <a title="190-tfidf-5" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>6 0.11535129 <a title="190-tfidf-6" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>7 0.10680538 <a title="190-tfidf-7" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>8 0.10532475 <a title="190-tfidf-8" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>9 0.10386361 <a title="190-tfidf-9" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>10 0.10364238 <a title="190-tfidf-10" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>11 0.10090902 <a title="190-tfidf-11" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>12 0.094115235 <a title="190-tfidf-12" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>13 0.08787401 <a title="190-tfidf-13" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>14 0.078298844 <a title="190-tfidf-14" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>15 0.076094866 <a title="190-tfidf-15" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>16 0.075316392 <a title="190-tfidf-16" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>17 0.074818492 <a title="190-tfidf-17" href="./nips-2011-Transfer_from_Multiple_MDPs.html">291 nips-2011-Transfer from Multiple MDPs</a></p>
<p>18 0.071310192 <a title="190-tfidf-18" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>19 0.068984196 <a title="190-tfidf-19" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>20 0.066110857 <a title="190-tfidf-20" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, -0.14), (2, 0.088), (3, 0.193), (4, -0.189), (5, 0.014), (6, 0.05), (7, 0.04), (8, 0.073), (9, 0.082), (10, -0.128), (11, 0.023), (12, 0.007), (13, 0.03), (14, -0.042), (15, 0.033), (16, 0.061), (17, 0.186), (18, -0.014), (19, -0.122), (20, -0.019), (21, 0.051), (22, -0.062), (23, -0.039), (24, 0.068), (25, -0.043), (26, 0.111), (27, -0.065), (28, 0.093), (29, -0.008), (30, 0.077), (31, -0.064), (32, 0.072), (33, -0.026), (34, 0.055), (35, -0.075), (36, -0.053), (37, -0.02), (38, 0.061), (39, -0.038), (40, 0.02), (41, -0.252), (42, -0.139), (43, -0.111), (44, 0.008), (45, -0.088), (46, 0.137), (47, 0.054), (48, -0.045), (49, -0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95107704 <a title="190-lsi-1" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert’s policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions. 1</p><p>2 0.86971211 <a title="190-lsi-2" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an inﬁnite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms. 1</p><p>3 0.72320139 <a title="190-lsi-3" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>Author: Mehdi Keramati, Boris S. Gutkin</p><p>Abstract: Reinforcement learning models address animal’s behavioral adaptation to its changing “external” environment, and are based on the assumption that Pavlovian, habitual and goal-directed responses seek to maximize reward acquisition. Negative-feedback models of homeostatic regulation, on the other hand, are concerned with behavioral adaptation in response to the “internal” state of the animal, and assume that animals’ behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints. Building upon the drive-reduction theory of reward, we propose a new analytical framework that integrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identical. The proposed theory shows behavioral adaptation to both internal and external states in a disciplined way. We further show that the proposed framework allows for a uniﬁed explanation of some behavioral pattern like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.</p><p>4 0.55307478 <a title="190-lsi-4" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>5 0.52093834 <a title="190-lsi-5" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>Author: Oliver B. Kroemer, Jan R. Peters</p><p>Abstract: In this paper, we consider the problem of policy evaluation for continuousstate systems. We present a non-parametric approach to policy evaluation, which uses kernel density estimation to represent the system. The true form of the value function for this model can be determined, and can be computed using Galerkin’s method. Furthermore, we also present a uniﬁed view of several well-known policy evaluation methods. In particular, we show that the same Galerkin method can be used to derive Least-Squares Temporal Diﬀerence learning, Kernelized Temporal Diﬀerence learning, and a discrete-state Dynamic Programming solution, as well as our proposed method. In a numerical evaluation of these algorithms, the proposed approach performed better than the other methods. 1</p><p>6 0.44499281 <a title="190-lsi-6" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>7 0.42527115 <a title="190-lsi-7" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>8 0.42426029 <a title="190-lsi-8" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>9 0.41497728 <a title="190-lsi-9" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>10 0.4038724 <a title="190-lsi-10" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>11 0.37995887 <a title="190-lsi-11" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>12 0.37733844 <a title="190-lsi-12" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>13 0.37622237 <a title="190-lsi-13" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>14 0.34135759 <a title="190-lsi-14" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>15 0.33312654 <a title="190-lsi-15" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>16 0.31658533 <a title="190-lsi-16" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>17 0.31529984 <a title="190-lsi-17" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>18 0.3112804 <a title="190-lsi-18" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>19 0.3048158 <a title="190-lsi-19" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>20 0.30417508 <a title="190-lsi-20" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (4, 0.033), (20, 0.026), (22, 0.025), (26, 0.015), (31, 0.094), (33, 0.014), (43, 0.077), (45, 0.44), (57, 0.03), (60, 0.011), (74, 0.034), (83, 0.034), (99, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99657333 <a title="190-lda-1" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>2 0.9942314 <a title="190-lda-2" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>Author: Alekh Agarwal, Dean P. Foster, Daniel J. Hsu, Sham M. Kakade, Alexander Rakhlin</p><p>Abstract: This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact set X under a stochastic bandit feedback model. In this model, the algorithm is allowed to observe noisy realizations of the function value f (x) at any query point x ∈ X . We demonstrate √ a generalization of the ellipsoid algorithm that √ incurs O(poly(d) T ) regret. Since any algorithm has regret at least Ω( T ) on this problem, our algorithm is optimal in terms of the scaling with T . 1</p><p>3 0.994205 <a title="190-lda-3" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>Author: Phillip Isola, Devi Parikh, Antonio Torralba, Aude Oliva</p><p>Abstract: Artists, advertisers, and photographers are routinely presented with the task of creating an image that a viewer will remember. While it may seem like image memorability is purely subjective, recent work shows that it is not an inexplicable phenomenon: variation in memorability of images is consistent across subjects, suggesting that some images are intrinsically more memorable than others, independent of a subjects’ contexts and biases. In this paper, we used the publicly available memorability dataset of Isola et al. [13], and augmented the object and scene annotations with interpretable spatial, content, and aesthetic image properties. We used a feature-selection scheme with desirable explaining-away properties to determine a compact set of attributes that characterizes the memorability of any individual image. We ﬁnd that images of enclosed spaces containing people with visible faces are memorable, while images of vistas and peaceful scenes are not. Contrary to popular belief, unusual or aesthetically pleasing scenes do not tend to be highly memorable. This work represents one of the ﬁrst attempts at understanding intrinsic image memorability, and opens a new domain of investigation at the interface between human cognition and computer vision. 1</p><p>4 0.99410874 <a title="190-lda-4" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>Author: Ziming Zhang, Lubor Ladicky, Philip Torr, Amir Saffari</p><p>Abstract: Local Coordinate Coding (LCC) [18] is a method for modeling functions of data lying on non-linear manifolds. It provides a set of anchor points which form a local coordinate system, such that each data point on the manifold can be approximated by a linear combination of its anchor points, and the linear weights become the local coordinate coding. In this paper we propose encoding data using orthogonal anchor planes, rather than anchor points. Our method needs only a few orthogonal anchor planes for coding, and it can linearize any (α, β, p)-Lipschitz smooth nonlinear function with a ﬁxed expected value of the upper-bound approximation error on any high dimensional data. In practice, the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (SVD). We apply our method to model the coordinates locally in linear SVMs for classiﬁcation tasks, and our experiment on MNIST shows that using only 50 anchor planes our method achieves 1.72% error rate, while LCC achieves 1.90% error rate using 4096 anchor points. 1</p><p>5 0.99158722 <a title="190-lda-5" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>Author: Mark Schmidt, Nicolas L. Roux, Francis R. Bach</p><p>Abstract: We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen ﬁxed error level on a set of structured sparsity problems. 1</p><p>6 0.99053073 <a title="190-lda-6" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>7 0.99035853 <a title="190-lda-7" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>same-paper 8 0.98533785 <a title="190-lda-8" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>9 0.95837009 <a title="190-lda-9" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>10 0.94312555 <a title="190-lda-10" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>11 0.94100326 <a title="190-lda-11" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>12 0.93974584 <a title="190-lda-12" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>13 0.93535781 <a title="190-lda-13" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>14 0.93419373 <a title="190-lda-14" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>15 0.93401057 <a title="190-lda-15" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>16 0.93121463 <a title="190-lda-16" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>17 0.92989641 <a title="190-lda-17" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>18 0.92942578 <a title="190-lda-18" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>19 0.92842293 <a title="190-lda-19" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>20 0.9180038 <a title="190-lda-20" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
