<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-81" href="#">nips2011-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</h1>
<br/><p>Source: <a title="nips-2011-81-pdf" href="http://papers.nips.cc/paper/4287-efficient-anomaly-detection-using-bipartite-k-nn-graphs.pdf">pdf</a></p><p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>Reference: <a title="nips-2011-81-reference" href="../nips2011_reference/nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Efﬁcient anomaly detection using bipartite k-NN graphs Kumar Sricharan Department of EECS University of Michigan Ann Arbor, MI 48104 kksreddy@umich. [sent-1, score-0.969]
</p><p>2 edu  Abstract Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. [sent-4, score-0.871]
</p><p>3 Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. [sent-5, score-0.416]
</p><p>4 In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. [sent-7, score-1.215]
</p><p>5 Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. [sent-8, score-0.253]
</p><p>6 Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes. [sent-10, score-0.747]
</p><p>7 1 Introduction Given a training set of normal events, the anomaly detection problem aims to identify unknown, anomalous events that deviate from the normal set. [sent-11, score-0.844]
</p><p>8 This novelty detection problem arises in applications where failure to detect anomalous activity could lead to catastrophic outcomes, for example, detection of faults in mission-critical systems, quality control in manufacturing and medical diagnosis. [sent-12, score-0.532]
</p><p>9 One class of algorithms assumes a family of parametrically deﬁned nominal distributions. [sent-14, score-0.194]
</p><p>10 The drawback of these algorithms is model mismatch: the supposed distribution need not be a correct representation of the nominal data, which can then lead to poor false alarm rates. [sent-16, score-0.674]
</p><p>11 More recently, several non-parametric methods based on minimum volume (MV) set estimation have been proposed. [sent-17, score-0.136]
</p><p>12 These methods aim to ﬁnd the minimum volume set that recovers a certain probability mass α with respect to the unknown probability density of the nominal events. [sent-18, score-0.471]
</p><p>13 Estimation of minimum volume sets is a difﬁcult problem, especially for high dimensional data. [sent-20, score-0.136]
</p><p>14 Both types of approaches involve explicit approximation of high dimensional quant1  ities - the multivariate density function in the ﬁrst case and the boundary of the minimum volume set in the second and are therefore not easily applied to high dimensional problems. [sent-22, score-0.231]
</p><p>15 However, the GEM based K-kNNG anomaly detection scheme proposed in [4] is computationally difﬁcult. [sent-24, score-0.713]
</p><p>16 To address this issue, a surrogate L1O-kNNG anomaly detection scheme was proposed in [4]. [sent-25, score-0.742]
</p><p>17 In this paper, we use the GEM principle to develop a bipartite k-nearest neighbor (k-NN) graphbased anomaly detection algorithm. [sent-27, score-1.049]
</p><p>18 K-LPE [13] and RRS [7] are anomaly detection methods which are also based on k-NN graphs. [sent-29, score-0.713]
</p><p>19 L1O-kNNG, K-LPE and RRS do not use bipartite graphs. [sent-31, score-0.224]
</p><p>20 We will show that the bipartite nature of BP-kNNG results in signiﬁcant computational savings. [sent-32, score-0.224]
</p><p>21 In addition, the K-LPE and RRS test statistics involve only the k-th nearest neighbor distance, while the statistic in BP-kNNG, like the L1O-kNNG, involves summation of the power weighted distance of all the edges in the k-NN graph. [sent-33, score-0.229]
</p><p>22 Finally, we will show that the mean square rate of convergence of p-values in BP-kNNG (O(T −2/(2+d) )) is faster as compared to the convergence rate of K-LPE (O(T −2/5 +T −6/5d )), where T is the size of the nominal training sample and d is the dimension of the data. [sent-35, score-0.343]
</p><p>23 In Section 2, we outline the statistical framework for minimum volume set anomaly detection. [sent-37, score-0.677]
</p><p>24 In Section 3, we describe the GEM principle and the K-kNNG and L1O-kNNG anomaly detection schemes proposed in [4]. [sent-38, score-0.795]
</p><p>25 Next, in Section 4, we develop our bipartite k-NN graph (BP-kNNG) method for anomaly detection. [sent-39, score-0.842]
</p><p>26 We also show that our method compares favorably to other state of the art anomaly detection schemes when applied to real world data from the UCI repository [1]. [sent-42, score-0.855]
</p><p>27 2 Statistical novelty detection The problem setup is as follows. [sent-44, score-0.223]
</p><p>28 We seek to ﬁnd a functional D and corresponding detection rule D(x) > 0 so that X is declared to be nominal if D(x) > 0 holds and anomalous otherwise. [sent-50, score-0.46]
</p><p>29 We seek to further constrain the choice of D to allow as few false negatives as possible for a ﬁxed allowance of false positives. [sent-52, score-0.404]
</p><p>30 The anomaly detection problem can be formulated as testing the hypotheses H 0 : f = f0 versus H1 : f = f0 . [sent-58, score-0.713]
</p><p>31 This requirement maintains the false positive rate at a level no greater than α. [sent-60, score-0.282]
</p><p>32 The most suitable A 0 acceptance region from the collection A would be the set which minimizes the false negative rate. [sent-62, score-0.255]
</p><p>33 In this case the false negative rate is bounded by Cλ(A) where λ(. [sent-64, score-0.244]
</p><p>34 The optimal acceptance region with a maximum false alarm rate α is therefore given by the minimum volume set of level α: Λα = min{λ(A) : A f0 (x)dx ≥ α}. [sent-67, score-0.749]
</p><p>35 Deﬁne the minimum entropy set of level α to be Ω α = min{Hν (A) : A f0 (x)dx ≥ 1 − α} where ν e Hν (A) = (1 − ν)−1 A f0 (x)dx is the R´ nyi ν-entropy of the density f 0 over the set A. [sent-68, score-0.26]
</p><p>36 It can be shown that when f 0 is a Lebesgue density in R d , the minimum volume set and the minimum entropy set are equivalent, i. [sent-69, score-0.358]
</p><p>37 Therefore, the optimal decision rule for a given level of false alarm α is to declare an anomaly if X ∈ Ω α . [sent-72, score-1.147]
</p><p>38 3 GEM principle In this section, we brieﬂy review the geometric entropy minimization (GEM) principle method [4] for determining minimum entropy sets Ω α of level α. [sent-75, score-0.318]
</p><p>39 The GEM method directly estimates the critical region Ωα for detecting anomalies using minimum coverings of subsets of points in a nominal training sample. [sent-76, score-0.49]
</p><p>40 Points in the training sample that are not covered by the K-point minimal graphs are identiﬁed as tail events. [sent-80, score-0.142]
</p><p>41 For any subset XK,T , deﬁne the total power weighted edge length of the k-NN graph on X K,T with power weighting γ (0 < γ < d), as K  k  LkN N (XK,T ) =  |eti (l) |γ , i=1 l=1  where {t1 , . [sent-87, score-0.155]
</p><p>42 Deﬁne the K-kNNG graph to be the K-point T k-NN graph having minimal length min XT ,K ∈XT LkN N (XT,K ) over all K subsets XK,T . [sent-91, score-0.199]
</p><p>43 sample from a multivariate density f 0 (x) and ∗ if limK,T →∞ K/T = ρ, then the set XK,T converges a. [sent-98, score-0.159]
</p><p>44 This set can be used to perform anomaly detection. [sent-101, score-0.541]
</p><p>45 1 K-kNNG anomaly detection Given a test sample X, denote the pooled sample X T +1 = XT ∪ {X} and determine the K-kNNG / ∗ graph over X T +1 . [sent-103, score-0.897]
</p><p>46 Declare X to be an anomaly if X ∈ X K,T +1 and nominal otherwise. [sent-104, score-0.735]
</p><p>47 When the density f0 is Lebesgue continuous, it follows from [4] that as K, T → ∞, this anomaly detection algorithm has false alarm rate that converges to α = 1 − K/T and power that converges to that of the minimum volume set test of level α. [sent-105, score-1.666]
</p><p>48 An identical detection scheme based on the K-minimal spanning tree has also been developed in [4]. [sent-106, score-0.201]
</p><p>49 The K-kNNG anomaly detection scheme therefore offers a direct approach to detecting outliers while bypassing the more difﬁcult problems of density estimation and level set estimation in high dimensions. [sent-107, score-0.893]
</p><p>50 As a result, the K-kNNG method is not well suited for anomaly detection for large sample sizes. [sent-110, score-0.741]
</p><p>51 However, the L1OkNNG detects anomalies at a ﬁxed false alarm rate 1/(T + 1), where T is the training sample size. [sent-115, score-0.754]
</p><p>52 To detect anomalies at a higher false alarm rate α ∗ , one would have to subsample the training set and only use T ∗ = 1/α∗ − 1 training samples. [sent-116, score-0.779]
</p><p>53 In the next section, we propose a different GEM based algorithm that uses bipartite graphs. [sent-118, score-0.224]
</p><p>54 The algorithm has algorithm has a much faster runtime than the L1O-kNNG, and unlike the L1O-kNNG, is asymptotically consistent and can operate at any speciﬁed alarm rate α. [sent-119, score-0.404]
</p><p>55 K Deﬁne the bipartite k-NN graph on {X K,N , XM } to be the set of edges linking each X i ∈ XK,N to its k nearest neighbors in X M . [sent-123, score-0.375]
</p><p>56 Deﬁne the total power weighted edge length of this bipartite k-NN graph with power weighting γ (0 < γ < d) and a ﬁxed number of edges s (1 ≤ s ≤ k) corresponding to each vertex X i ∈ XK,N to be K  k  Ls,k (XK,N , XM ) =  |eti (l) |γ , i=1 l=k−s+1  where {t1 , . [sent-124, score-0.421]
</p><p>57 , eti (k) } are the k-NN edges in the bipartite graph originating from X ti ∈ XK,N . [sent-130, score-0.42]
</p><p>58 Deﬁne the bipartite K-kNNG graph to be the one having minimal weighted length min XN,K ∈XN Ls,k (XN,K , XM ) over all N subsets XK,N . [sent-131, score-0.346]
</p><p>59 XK,N ∈X  Using the theory of partitioned k-NN graph entropy estimators [11], it follows that as k/M → ∗ 0, k, N → ∞ and for ﬁxed s, the set X K,N converges a. [sent-133, score-0.172]
</p><p>60 This suggests using the bipartite k-NN graph to detect anomalies in the following way. [sent-136, score-0.484]
</p><p>61 Given a test point X, denote the pooled sample X N +1 = XN ∪ {X} and determine the optimal bipartite ∗ / ∗ K-kNNG graph X K,N +1 over {XK,N +1 , XM }. [sent-137, score-0.38]
</p><p>62 Now declare X to be an anomaly if X ∈ X K,N +1 and nominal otherwise. [sent-138, score-0.823]
</p><p>63 It is clear that by the GEM principle, this algorithm detects false alarms at a rate that converges to α = 1 − K/T and power that converges to that of the minimum volume set test of level α. [sent-139, score-0.607]
</p><p>64 Because of ∗ the bipartite nature of the construction, this is equivalent to choosing X K,N +1 . [sent-147, score-0.224]
</p><p>65 This leads to the proposed BP-kNNG anomaly detection algorithm described by Algorithm 1. [sent-148, score-0.713]
</p><p>66 1 BP-kNNG p-value estimates The p-value is a score between 0 and 1 that is associated with the likelihood that a given point X 0 comes from a speciﬁed nominal distribution. [sent-150, score-0.194]
</p><p>67 The BP-kNNG generates an estimate of the p-value 4  Algorithm 1 Anomaly detection scheme using bipartite k-NN graphs 1. [sent-151, score-0.428]
</p><p>68 Input: Training samples X T , test samples X, false alarm rate α 2. [sent-152, score-0.573]
</p><p>69 Speciﬁcally, for a given test point X 0 , the true p-value associated with a point X 0 in a minimum volume set test is given by p true (X0 ) = S(X0 ) f0 (z)dz where S(X0 ) = {z : f0 (z) ≤ f0 (X0 )} and E(X0 ) = {z : f0 (z) = f0 (X0 )}. [sent-158, score-0.238]
</p><p>70 ptrue (X0 ) is the minimal level α at which X 0 would be rejected. [sent-159, score-0.179]
</p><p>71 2 Asymptotic consistency and optimal convergence rates Here we prove that the BP-kNNG detector is asymptotically consistent by showing that for a ﬁxed number of edges s, E[(p bp (X0 ) − ptrue (X0 ))2 ] → 0 as k/M → 0, k, N → ∞. [sent-162, score-0.317]
</p><p>72 Bias: We ﬁrst introduce the oracle p-value p orac (X0 ) = (1/N ) Xi ∈XN 1(f0 (Xi ) ≤ f0 (X0 )) and note that E[p orac (X0 )] = ptrue (X0 ). [sent-177, score-0.174]
</p><p>73 The distance ei(l) of a point X i ∈ XN to its l-th ˆ nearest neighbor in X M is related to the bipartite l-nearest neighbor density estimate fl (Xi ) = d (l − 1)/(M cd ei(l) ) (section 2. [sent-178, score-0.481]
</p><p>74 (3)  Consistency of p-values: From (2) and (3), we obtain an asymptotic representation of the estimated p-value E[(p bp (X0 ) − ptrue (X0 ))2 ] = O((k/M )2/d ) + O(1/k) + O(1/N ). [sent-190, score-0.207]
</p><p>75 This implies that pbp converges in mean square to p true , for a ﬁxed number of edges s, as k/M → 0, k, N → ∞. [sent-191, score-0.213]
</p><p>76 On the other hand, because of the bipartite construction of our k-NN graph, dk (Xi ) for each Xi ∈ XN needs to be computed and stored only once. [sent-203, score-0.262]
</p><p>77 For a total of L query points, the overall runtime complexity of our algorithm is therefore much smaller than the L1O-kNNG, KLPE and K-kNNG anomaly detection schemes (O(dT (T (4+d)/(4+2d) + L)) compared to O(dLT 2 ), T O(dLT 2 ) and O(dLK 2 K ) respectively). [sent-205, score-0.829]
</p><p>78 5 Simulation comparisons We compare the L1O-kNNG and the bipartite K-kNNG schemes on a simulated data set. [sent-206, score-0.259]
</p><p>79 The percentage of anomalies in the test set is therefore 20%. [sent-213, score-0.191]
</p><p>80 (b) Comparison of observed false alarm rates for The labeled ’clairvoyant’ curve is the ROC of the L1O-kNNG and BP-kNNG with the desired false UMP anomaly detector. [sent-247, score-1.268]
</p><p>81 03%) class 2,3,5,6,7 vs class 1 (7%)  Table 1: Description of data used in anomaly detection experiments. [sent-254, score-0.713]
</p><p>82 For this simple case the minimum √ volume set of level α is a disk centered at the origin with radius 2σ 2 log(1/α). [sent-256, score-0.174]
</p><p>83 1(a), we compare the detection performance of L1O-kNNG and BP-kNNG against the ’clairvoyant’ UMP detector in terms of the ROC. [sent-263, score-0.208]
</p><p>84 1(b) we note the close agreement between desired and observed false alarm rates for BP-kNNG. [sent-266, score-0.525]
</p><p>85 Note that the L1O-kNNG signiﬁcantly underestimates its false alarm rate for higher levels of true false alarm. [sent-267, score-0.724]
</p><p>86 This signiﬁcant savings in runtime is due to the fact that the bipartite graph does not have to be constructed separately for each new test instance; it sufﬁces to construct it once on the entire data set. [sent-272, score-0.404]
</p><p>87 One of the anomaly data generators is Mulcross [8] and the other four are from the UCI repository [1]. [sent-277, score-0.578]
</p><p>88 16/i 4  iF 147 79 75 26 15  ORCA 9487 6995 2512 267 157  Table 2: Comparison of anomaly detection schemes in terms of AUC and run-time for BP-kNNG (BP) against L1O-kNNG (L10), K-LPE, MassAD (Mass), iForest (iF) and ORCA. [sent-319, score-0.748]
</p><p>89 179  Table 3: Comparison of desired and observed false alarm rates for BP-kNNG. [sent-353, score-0.525]
</p><p>90 We are unable to report the AUC for K-LPE because of the large processing time and for L1O-kNNG because it cannot operate at high false alarm rates. [sent-364, score-0.48]
</p><p>91 In addition, BP-kNNG allows the speciﬁcation of a threshold for anomaly detection at a desired false alarm rate. [sent-366, score-1.238]
</p><p>92 This is corroborated by the results in Table 3, where we see that the observed false alarm rates across the different data sets are close to the desired false alarm rate. [sent-367, score-1.005]
</p><p>93 6 Conclusions The geometric entropy minimization (GEM) principle was introduced in [4] to extract minimal set coverings that can be used to detect anomalies from a set of training samples. [sent-368, score-0.422]
</p><p>94 In this paper we propose a bipartite k-nearest neighbor graph (BP-kNNG) anomaly detection algorithm based on the GEM principle. [sent-369, score-1.079]
</p><p>95 We compared BP-kNNG against state of the art anomaly detection algorithms and showed that BPkNNG compares favorably in terms of both ROC performance and computation time. [sent-371, score-0.783]
</p><p>96 In addition, BP-kNNG enjoys several other advantages including the ability to detect anomalies at a desired false alarm rate. [sent-372, score-0.708]
</p><p>97 In BP-kNNG, the p-values of each test point can also be easily computed (1), making BP-kNNG easily extendable to incorporating false discovery rate constraints. [sent-373, score-0.295]
</p><p>98 Geometric entropy minimization (gem) for anomaly detection and localization. [sent-399, score-0.772]
</p><p>99 A computable plug-in estimator of minimum volume sets for novelty detection. [sent-417, score-0.187]
</p><p>100 Anomaly detection with score functions based on nearest neighbor graphs. [sent-469, score-0.269]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anomaly', 0.541), ('gem', 0.289), ('alarm', 0.278), ('bipartite', 0.224), ('false', 0.202), ('nominal', 0.194), ('detection', 0.172), ('anomalies', 0.14), ('pbp', 0.135), ('orca', 0.119), ('iforest', 0.116), ('massad', 0.116), ('ump', 0.116), ('knng', 0.096), ('lof', 0.096), ('ptrue', 0.096), ('density', 0.095), ('anomalous', 0.094), ('na', 0.092), ('declare', 0.088), ('xn', 0.081), ('bp', 0.081), ('xm', 0.08), ('eti', 0.077), ('hero', 0.077), ('mulcross', 0.077), ('graph', 0.077), ('auc', 0.075), ('volume', 0.068), ('rrs', 0.068), ('minimum', 0.068), ('neighbor', 0.065), ('entropy', 0.059), ('bpknng', 0.058), ('lkn', 0.058), ('smtp', 0.058), ('ghz', 0.055), ('kdd', 0.053), ('acceptance', 0.053), ('runtime', 0.052), ('clairvoyant', 0.051), ('coverings', 0.051), ('shuttle', 0.051), ('novelty', 0.051), ('test', 0.051), ('mv', 0.049), ('principle', 0.047), ('outliers', 0.047), ('sigmod', 0.047), ('mass', 0.046), ('minimal', 0.045), ('xi', 0.045), ('lebesgue', 0.045), ('desired', 0.045), ('detect', 0.043), ('rate', 0.042), ('xt', 0.042), ('edges', 0.042), ('roc', 0.041), ('ei', 0.04), ('power', 0.039), ('arbor', 0.039), ('dlt', 0.039), ('opteron', 0.039), ('orac', 0.039), ('sricharan', 0.039), ('dt', 0.038), ('processor', 0.038), ('dk', 0.038), ('level', 0.038), ('training', 0.037), ('forest', 0.037), ('repository', 0.037), ('detector', 0.036), ('ex', 0.036), ('gb', 0.036), ('favorably', 0.036), ('converges', 0.036), ('schemes', 0.035), ('ddimensional', 0.034), ('michigan', 0.034), ('art', 0.034), ('asymptotically', 0.032), ('graphs', 0.032), ('nearest', 0.032), ('dx', 0.031), ('ting', 0.031), ('mining', 0.031), ('consistency', 0.03), ('asymptotic', 0.03), ('amd', 0.029), ('spanning', 0.029), ('query', 0.029), ('surrogate', 0.029), ('sample', 0.028), ('attack', 0.028), ('ann', 0.028), ('uci', 0.028), ('detects', 0.027), ('card', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="81-tfidf-1" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>2 0.25352332 <a title="81-tfidf-2" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>3 0.079553083 <a title="81-tfidf-3" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>4 0.068364747 <a title="81-tfidf-4" href="./nips-2011-On_fast_approximate_submodular_minimization.html">199 nips-2011-On fast approximate submodular minimization</a></p>
<p>Author: Stefanie Jegelka, Hui Lin, Jeff A. Bilmes</p><p>Abstract: We are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm. In fact, for our application, minimum norm can have a running time of about O(n7 ) (O(n5 ) oracle calls). We therefore propose a fast approximate method to minimize arbitrary submodular functions. For a large sub-class of submodular functions, the algorithm is exact. Other submodular functions are iteratively approximated by tight submodular upper bounds, and then repeatedly optimized. We show theoretical properties, and empirical results suggest signiﬁcant speedups over minimum norm while retaining higher accuracies. 1</p><p>5 0.057896383 <a title="81-tfidf-5" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>Author: Jakob H. Macke, Iain Murray, Peter E. Latham</p><p>Abstract: Maximum entropy models have become popular statistical models in neuroscience and other areas in biology, and can be useful tools for obtaining estimates of mutual information in biological systems. However, maximum entropy models ﬁt to small data sets can be subject to sampling bias; i.e. the true entropy of the data can be severely underestimated. Here we study the sampling properties of estimates of the entropy obtained from maximum entropy models. We show that if the data is generated by a distribution that lies in the model class, the bias is equal to the number of parameters divided by twice the number of observations. However, in practice, the true distribution is usually outside the model class, and we show here that this misspeciﬁcation can lead to much larger bias. We provide a perturbative approximation of the maximally expected bias when the true model is out of model class, and we illustrate our results using numerical simulations of an Ising model; i.e. the second-order maximum entropy distribution on binary data. 1</p><p>6 0.055216238 <a title="81-tfidf-6" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>7 0.054518726 <a title="81-tfidf-7" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>8 0.051710289 <a title="81-tfidf-8" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>9 0.050953738 <a title="81-tfidf-9" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>10 0.050361216 <a title="81-tfidf-10" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>11 0.04929398 <a title="81-tfidf-11" href="./nips-2011-Phase_transition_in_the_family_of_p-resistances.html">213 nips-2011-Phase transition in the family of p-resistances</a></p>
<p>12 0.047320209 <a title="81-tfidf-12" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>13 0.045414802 <a title="81-tfidf-13" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>14 0.044157103 <a title="81-tfidf-14" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>15 0.044104416 <a title="81-tfidf-15" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>16 0.042292409 <a title="81-tfidf-16" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>17 0.042036001 <a title="81-tfidf-17" href="./nips-2011-Energetically_Optimal_Action_Potentials.html">87 nips-2011-Energetically Optimal Action Potentials</a></p>
<p>18 0.041045517 <a title="81-tfidf-18" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>19 0.040708385 <a title="81-tfidf-19" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>20 0.04033101 <a title="81-tfidf-20" href="./nips-2011-An_Application_of_Tree-Structured_Expectation_Propagation_for_Channel_Decoding.html">31 nips-2011-An Application of Tree-Structured Expectation Propagation for Channel Decoding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.013), (2, -0.031), (3, -0.023), (4, -0.009), (5, -0.078), (6, -0.063), (7, -0.032), (8, -0.033), (9, -0.015), (10, 0.026), (11, 0.014), (12, -0.013), (13, -0.014), (14, 0.01), (15, 0.043), (16, 0.068), (17, 0.013), (18, -0.002), (19, 0.083), (20, -0.043), (21, 0.085), (22, 0.073), (23, 0.053), (24, 0.093), (25, -0.04), (26, -0.014), (27, 0.082), (28, 0.017), (29, 0.022), (30, 0.053), (31, 0.028), (32, -0.067), (33, 0.13), (34, -0.127), (35, -0.033), (36, 0.101), (37, -0.037), (38, 0.007), (39, -0.007), (40, 0.027), (41, -0.081), (42, 0.146), (43, 0.061), (44, 0.053), (45, -0.082), (46, -0.019), (47, 0.187), (48, 0.018), (49, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91607654 <a title="81-lsi-1" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>2 0.61809957 <a title="81-lsi-2" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>3 0.4926078 <a title="81-lsi-3" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>Author: Dan Feldman, Matthew Faulkner, Andreas Krause</p><p>Abstract: How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models ﬁtting the coreset will also provide a good ﬁt for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3 /ε2 ) data points sufﬁces for computing a (1 + ε)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efﬁciently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones. 1</p><p>4 0.47701749 <a title="81-lsi-4" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>5 0.46753722 <a title="81-lsi-5" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>Author: Animashree Anandkumar, Vincent Tan, Alan S. Willsky</p><p>Abstract: We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efﬁcient threshold-based algorithm for structure estimation based on conditional mutual information thresholding. This simple local algorithm requires only loworder statistics of the data and decides whether two nodes are neighbors in the unknown graph. We identify graph families for which the proposed algorithm has low sample and computational complexities. Under some transparent assumptions, we establish that the proposed algorithm is −4 structurally consistent (or sparsistent) when the number of samples scales as n = Ω(Jmin log p), where p is the number of nodes and Jmin is the minimum edge potential. We also develop novel non-asymptotic techniques for obtaining necessary conditions for graphical model selection. Keywords: Graphical model selection, high-dimensional learning, local-separation property, necessary conditions, typical sets, Fano’s inequality.</p><p>6 0.46477085 <a title="81-lsi-6" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>7 0.46418476 <a title="81-lsi-7" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>8 0.4488866 <a title="81-lsi-8" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>9 0.44859937 <a title="81-lsi-9" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>10 0.44410011 <a title="81-lsi-10" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>11 0.44282058 <a title="81-lsi-11" href="./nips-2011-Phase_transition_in_the_family_of_p-resistances.html">213 nips-2011-Phase transition in the family of p-resistances</a></p>
<p>12 0.43868023 <a title="81-lsi-12" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<p>13 0.4144226 <a title="81-lsi-13" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>14 0.41377416 <a title="81-lsi-14" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>15 0.40994555 <a title="81-lsi-15" href="./nips-2011-Data_Skeletonization_via_Reeb_Graphs.html">67 nips-2011-Data Skeletonization via Reeb Graphs</a></p>
<p>16 0.40253809 <a title="81-lsi-16" href="./nips-2011-Learning_Patient-Specific_Cancer_Survival_Distributions_as_a_Sequence_of_Dependent_Regressors.html">147 nips-2011-Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors</a></p>
<p>17 0.40210035 <a title="81-lsi-17" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>18 0.40154484 <a title="81-lsi-18" href="./nips-2011-Regularized_Laplacian_Estimation_and_Fast_Eigenvector_Approximation.html">236 nips-2011-Regularized Laplacian Estimation and Fast Eigenvector Approximation</a></p>
<p>19 0.39325747 <a title="81-lsi-19" href="./nips-2011-Continuous-Time_Regression_Models_for_Longitudinal_Networks.html">62 nips-2011-Continuous-Time Regression Models for Longitudinal Networks</a></p>
<p>20 0.36218384 <a title="81-lsi-20" href="./nips-2011-Confidence_Sets_for_Network_Structure.html">60 nips-2011-Confidence Sets for Network Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (4, 0.077), (15, 0.318), (20, 0.031), (26, 0.025), (31, 0.079), (33, 0.014), (43, 0.066), (45, 0.122), (57, 0.021), (74, 0.037), (83, 0.053), (99, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72829932 <a title="81-lda-1" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>2 0.65533328 <a title="81-lda-2" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>Author: Bo Chen, Vidhya Navalpakkam, Pietro Perona</p><p>Abstract: A model of human visual search is proposed. It predicts both response time (RT) and error rates (RT) as a function of image parameters such as target contrast and clutter. The model is an ideal observer, in that it optimizes the Bayes ratio of target present vs target absent. The ratio is computed on the ﬁring pattern of V1/V2 neurons, modeled by Poisson distributions. The optimal mechanism for integrating information over time is shown to be a ‘soft max’ of diffusions, computed over the visual ﬁeld by ‘hypercolumns’ of neurons that share the same receptive ﬁeld and have different response properties to image features. An approximation of the optimal Bayesian observer, based on integrating local decisions, rather than diffusions, is also derived; it is shown experimentally to produce very similar predictions to the optimal observer in common psychophysics conditions. A psychophyisics experiment is proposed that may discriminate between which mechanism is used in the human brain. A B C Figure 1: Visual search. (A) Clutter and camouﬂage make visual search difﬁcult. (B,C) Psychologists and neuroscientists build synthetic displays to study visual search. In (B) the target ‘pops out’ (∆θ = 450 ), while in (C) the target requires more time to be detected (∆θ = 100 ) [1]. 1</p><p>3 0.58844578 <a title="81-lda-3" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><p>4 0.51183546 <a title="81-lda-4" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>Author: Dominique Tschopp, Suhas Diggavi, Payam Delgosha, Soheil Mohajer</p><p>Abstract: This paper addresses the problem of ﬁnding the nearest neighbor (or one of the R-nearest neighbors) of a query object q in a database of n objects, when we can only use a comparison oracle. The comparison oracle, given two reference objects and a query object, returns the reference object most similar to the query object. The main problem we study is how to search the database for the nearest neighbor (NN) of a query, while minimizing the questions. The difﬁculty of this problem depends on properties of the underlying database. We show the importance of a characterization: combinatorial disorder D which deﬁnes approximate triangle n inequalities on ranks. We present a lower bound of Ω(D log D + D2 ) average number of questions in the search phase for any randomized algorithm, which demonstrates the fundamental role of D for worst case behavior. We develop 3 a randomized scheme for NN retrieval in O(D3 log2 n + D log2 n log log nD ) 3 questions. The learning requires asking O(nD3 log2 n + D log2 n log log nD ) questions and O(n log2 n/ log(2D)) bits to store.</p><p>5 0.50865769 <a title="81-lda-5" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>6 0.50844896 <a title="81-lda-6" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>7 0.50671828 <a title="81-lda-7" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>8 0.50607711 <a title="81-lda-8" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>9 0.50502831 <a title="81-lda-9" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>10 0.50103873 <a title="81-lda-10" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>11 0.50084358 <a title="81-lda-11" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<p>12 0.50066191 <a title="81-lda-12" href="./nips-2011-On_fast_approximate_submodular_minimization.html">199 nips-2011-On fast approximate submodular minimization</a></p>
<p>13 0.49986529 <a title="81-lda-13" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>14 0.49966037 <a title="81-lda-14" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>15 0.49937546 <a title="81-lda-15" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>16 0.49937174 <a title="81-lda-16" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>17 0.4983786 <a title="81-lda-17" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>18 0.49796087 <a title="81-lda-18" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>19 0.49773306 <a title="81-lda-19" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>20 0.49624664 <a title="81-lda-20" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
