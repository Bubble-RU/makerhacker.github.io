<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-81" href="#">nips2011-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</h1>
<br/><p>Source: <a title="nips-2011-81-pdf" href="http://papers.nips.cc/paper/4287-efficient-anomaly-detection-using-bipartite-k-nn-graphs.pdf">pdf</a></p><p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>Reference: <a title="nips-2011-81-reference" href="../nips2011_reference/nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('anoma', 0.647), ('alarm', 0.281), ('bipartit', 0.222), ('gem', 0.202), ('fals', 0.2), ('nomin', 0.188), ('detect', 0.187), ('pbp', 0.137), ('orc', 0.12), ('iforest', 0.117), ('massad', 0.117), ('ump', 0.117), ('knng', 0.098), ('lof', 0.098), ('ptrue', 0.098), ('anom', 0.095), ('na', 0.093), ('xn', 0.082), ('bp', 0.082), ('xm', 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="81-tfidf-1" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>2 0.40427557 <a title="81-tfidf-2" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>3 0.081646107 <a title="81-tfidf-3" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>4 0.066436969 <a title="81-tfidf-4" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>Author: Vincent Delaitre, Josef Sivic, Ivan Laptev</p><p>Abstract: We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-speciﬁc body part and object interactions bypasses the difﬁcult problem of estimating the complete human body pose conﬁguration. Beneﬁts of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline. 1</p><p>5 0.063326068 <a title="81-tfidf-5" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>6 0.06219542 <a title="81-tfidf-6" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>7 0.058063865 <a title="81-tfidf-7" href="./nips-2011-On_fast_approximate_submodular_minimization.html">199 nips-2011-On fast approximate submodular minimization</a></p>
<p>8 0.057451993 <a title="81-tfidf-8" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>9 0.056189101 <a title="81-tfidf-9" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>10 0.052130975 <a title="81-tfidf-10" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>11 0.049585991 <a title="81-tfidf-11" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>12 0.049101915 <a title="81-tfidf-12" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>13 0.047651611 <a title="81-tfidf-13" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>14 0.046361201 <a title="81-tfidf-14" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>15 0.045011349 <a title="81-tfidf-15" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>16 0.044657439 <a title="81-tfidf-16" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>17 0.044629306 <a title="81-tfidf-17" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>18 0.04392777 <a title="81-tfidf-18" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>19 0.043395117 <a title="81-tfidf-19" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>20 0.042804591 <a title="81-tfidf-20" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, -0.013), (2, 0.046), (3, 0.002), (4, -0.011), (5, -0.039), (6, -0.05), (7, -0.011), (8, -0.057), (9, 0.007), (10, 0.002), (11, 0.017), (12, -0.025), (13, -0.013), (14, -0.071), (15, 0.028), (16, -0.017), (17, -0.015), (18, 0.023), (19, 0.041), (20, -0.107), (21, -0.007), (22, 0.195), (23, -0.105), (24, -0.047), (25, -0.217), (26, 0.237), (27, 0.03), (28, 0.049), (29, -0.125), (30, -0.037), (31, -0.091), (32, -0.04), (33, 0.146), (34, 0.217), (35, 0.009), (36, -0.055), (37, 0.057), (38, -0.247), (39, 0.067), (40, -0.062), (41, 0.142), (42, -0.136), (43, 0.237), (44, 0.085), (45, 0.172), (46, -0.022), (47, -0.008), (48, 0.08), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91145271 <a title="81-lsi-1" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>2 0.79562461 <a title="81-lsi-2" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>3 0.40362552 <a title="81-lsi-3" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>4 0.39475867 <a title="81-lsi-4" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>5 0.35009924 <a title="81-lsi-5" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>Author: Dan Feldman, Matthew Faulkner, Andreas Krause</p><p>Abstract: How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models ﬁtting the coreset will also provide a good ﬁt for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3 /ε2 ) data points sufﬁces for computing a (1 + ε)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efﬁciently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones. 1</p><p>6 0.31086558 <a title="81-lsi-6" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>7 0.30035466 <a title="81-lsi-7" href="./nips-2011-Phase_transition_in_the_family_of_p-resistances.html">213 nips-2011-Phase transition in the family of p-resistances</a></p>
<p>8 0.29872233 <a title="81-lsi-8" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>9 0.29696682 <a title="81-lsi-9" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>10 0.29501602 <a title="81-lsi-10" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>11 0.28756264 <a title="81-lsi-11" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>12 0.27883109 <a title="81-lsi-12" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>13 0.27387613 <a title="81-lsi-13" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>14 0.27169096 <a title="81-lsi-14" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>15 0.25313237 <a title="81-lsi-15" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>16 0.25223991 <a title="81-lsi-16" href="./nips-2011-A_Global_Structural_EM_Algorithm_for_a_Model_of_Cancer_Progression.html">6 nips-2011-A Global Structural EM Algorithm for a Model of Cancer Progression</a></p>
<p>17 0.25093019 <a title="81-lsi-17" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>18 0.24034542 <a title="81-lsi-18" href="./nips-2011-Learning_Patient-Specific_Cancer_Survival_Distributions_as_a_Sequence_of_Dependent_Regressors.html">147 nips-2011-Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors</a></p>
<p>19 0.23964477 <a title="81-lsi-19" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>20 0.2389783 <a title="81-lsi-20" href="./nips-2011-Data_Skeletonization_via_Reeb_Graphs.html">67 nips-2011-Data Skeletonization via Reeb Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.086), (22, 0.048), (28, 0.392), (36, 0.035), (55, 0.116), (65, 0.061), (68, 0.143), (79, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70232719 <a title="81-lda-1" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>2 0.64363515 <a title="81-lda-2" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>Author: Alyson K. Fletcher, Sundeep Rangan, Lav R. Varshney, Aniruddha Bhargava</p><p>Abstract: Many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear ﬁltering stage that produces a lowdimensional signal that drives subsequent nonlinear stages. This paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive ﬁelds in sensory neurons. The proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (GAMP) method. The GAMP method is based on Gaussian approximations of loopy belief propagation. In the neural connectivity problem, the GAMP-based method is shown to be computational efﬁcient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and signiﬁcantly outperforms previous compressed-sensing methods. For the receptive ﬁeld estimation, the GAMP method can also exploit inherent structured sparsity in the linear weights. The method is validated on estimation of linear nonlinear Poisson (LNP) cascade models for receptive ﬁelds of salamander retinal ganglion cells. 1</p><p>3 0.63910258 <a title="81-lda-3" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>Author: Animashree Anandkumar, Vincent Tan, Alan S. Willsky</p><p>Abstract: We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efﬁcient threshold-based algorithm for structure estimation based on conditional mutual information thresholding. This simple local algorithm requires only loworder statistics of the data and decides whether two nodes are neighbors in the unknown graph. We identify graph families for which the proposed algorithm has low sample and computational complexities. Under some transparent assumptions, we establish that the proposed algorithm is −4 structurally consistent (or sparsistent) when the number of samples scales as n = Ω(Jmin log p), where p is the number of nodes and Jmin is the minimum edge potential. We also develop novel non-asymptotic techniques for obtaining necessary conditions for graphical model selection. Keywords: Graphical model selection, high-dimensional learning, local-separation property, necessary conditions, typical sets, Fano’s inequality.</p><p>4 0.62832129 <a title="81-lda-4" href="./nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</a></p>
<p>Author: Jiarong Jiang, Piyush Rai, Hal Daume</p><p>Abstract: We consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (MAP) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes). We present a hybrid message-passing algorithm to accomplish this. The hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max). We derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework. We further show that the Expectation Maximization algorithm can be seen as an approximation to our algorithm. Experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efﬁcacy of our proposed algorithm. 1</p><p>5 0.59501046 <a title="81-lda-5" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>Author: Vicente Ordonez, Girish Kulkarni, Tamara L. Berg</p><p>Abstract: We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset – performing a huge number of Flickr queries and then ﬁltering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning. 1</p><p>6 0.54945159 <a title="81-lda-6" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>7 0.52744651 <a title="81-lda-7" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>8 0.49374506 <a title="81-lda-8" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>9 0.48562831 <a title="81-lda-9" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>10 0.48253074 <a title="81-lda-10" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>11 0.4813413 <a title="81-lda-11" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>12 0.48119932 <a title="81-lda-12" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>13 0.47999674 <a title="81-lda-13" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>14 0.47945896 <a title="81-lda-14" href="./nips-2011-Non-Asymptotic_Analysis_of_Stochastic_Approximation_Algorithms_for_Machine_Learning.html">187 nips-2011-Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning</a></p>
<p>15 0.47834045 <a title="81-lda-15" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>16 0.47831061 <a title="81-lda-16" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>17 0.47823933 <a title="81-lda-17" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>18 0.4773159 <a title="81-lda-18" href="./nips-2011-Testing_a_Bayesian_Measure_of_Representativeness_Using_a_Large_Image_Database.html">280 nips-2011-Testing a Bayesian Measure of Representativeness Using a Large Image Database</a></p>
<p>19 0.47651261 <a title="81-lda-19" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>20 0.47631615 <a title="81-lda-20" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
