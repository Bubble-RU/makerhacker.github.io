<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 nips-2011-Contextual Gaussian Process Bandit Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-61" href="#">nips2011-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 nips-2011-Contextual Gaussian Process Bandit Optimization</h1>
<br/><p>Source: <a title="nips-2011-61-pdf" href="http://papers.nips.cc/paper/4487-contextual-gaussian-process-bandit-optimization.pdf">pdf</a></p><p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>Reference: <a title="nips-2011-61-reference" href="../nips2011_reference/nips-2011-Contextual_Gaussian_Process_Bandit_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). [sent-7, score-0.695]
</p><p>2 The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. [sent-8, score-0.465]
</p><p>3 We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. [sent-9, score-0.233]
</p><p>4 We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. [sent-10, score-0.294]
</p><p>5 We further provide generic tools for deriving regret bounds when using such composite kernel functions. [sent-11, score-0.545]
</p><p>6 Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. [sent-12, score-0.335]
</p><p>7 All these problems can be phrased as a contextual bandit problem (c. [sent-17, score-0.436]
</p><p>8 , [1, 2], we review related work in Section 7), where in each round, we receive context (about the experimental conditions, the query, or the task), and have to choose an action (system parameters, document to retrieve). [sent-19, score-0.259]
</p><p>9 The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the contextaction space, and to exploit by choosing an action deemed optimal based on the gathered data. [sent-21, score-0.498]
</p><p>10 Without making any assumptions about the class of payoff functions under consideration, we cannot expect to do well. [sent-22, score-0.298]
</p><p>11 A natural approach is to choose a regularizer, encoding assumptions about smoothness of the payoff function. [sent-23, score-0.302]
</p><p>12 In this paper, we take a nonparametric approach, and model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space (or having low norm in the associated RKHS). [sent-24, score-0.233]
</p><p>13 This approach allows us to estimate the predictive uncertainty in the payoff function estimated from previous experiments, guiding the tradeoff between exploration and exploitation. [sent-25, score-0.292]
</p><p>14 By constructing a composite kernel function for the regularizer from kernels deﬁned over the action and context spaces (e. [sent-28, score-0.521]
</p><p>15 , a linear kernel on the actions, and Gaussian kernel on the contexts), we can capture several natural contextual bandit problem formulations. [sent-30, score-0.728]
</p><p>16 We prove that CGP-UCB incurs 1  sublinear contextual regret (i. [sent-31, score-0.639]
</p><p>17 , prove that it competes with the optimal mapping from context to actions) for a large class of composite kernel functions constructed in this manner. [sent-33, score-0.378]
</p><p>18 Lastly, we evaluate our algorithm on two real-world case studies in the context of automated vaccine design, and management of sensor networks. [sent-34, score-0.308]
</p><p>19 We show that in both these problems, properly taking into account contextual information outperforms ignoring or naively using context. [sent-35, score-0.34]
</p><p>20 In each round, we receive a context zt ∈ Z from a (not necessarily ﬁnite) set Z of contexts, and have to choose an action st ∈ S from a (not necessarily ﬁnite) set S of actions. [sent-38, score-0.38]
</p><p>21 We then receive a payoff yt = f (st , zt ) + t , where f : S × Z → R is an (unknown) function, and t is zero mean random noise (independent across the rounds). [sent-39, score-0.4]
</p><p>22 The addition of (externally chosen) contextual information captures a critical component in many applications, and generalizes the k-armed bandit setting. [sent-40, score-0.436]
</p><p>23 Since f is unknown, we will not generally be able to choose the optimal action, and thus incur T regret rt = sups ∈S f (s , zt ) − f (st , zt ). [sent-41, score-0.491]
</p><p>24 After T rounds, our cumulative regret is RT = t=1 rt . [sent-42, score-0.329]
</p><p>25 The context-speciﬁc best action is a more demanding benchmark than the best action used in the (context-free) deﬁnition of regret. [sent-43, score-0.21]
</p><p>26 Our goal will be to develop an algorithm which achieves sublinear contextual regret, i. [sent-44, score-0.376]
</p><p>27 Note that achieving sublinear contextual regret requires learning (and competing with) the optimal mapping from contexts to actions. [sent-47, score-0.87]
</p><p>28 Regularity assumptions are required, since without any there could be a single action s∗ ∈ S that obtains payoff of 1, and all other actions obtain payoff 0. [sent-48, score-0.73]
</p><p>29 Since the random variables are action-context pairs, often there is a natural decomposition of the covariance function k into the corresponding covariance functions on actions and contexts (Section 5). [sent-57, score-0.449]
</p><p>30 The choice of the kernel function turns out to be crucial in regularizing the function class to achieve sublinear regret (Section 4). [sent-72, score-0.49]
</p><p>31 2  3  The Contextual Upper Conﬁdence Bound Algorithm  In the context-free case Z = ∅, the problem of trading off exploration and exploitation with payoff functions sampled from a Gaussian process is studied by [3]. [sent-74, score-0.363]
</p><p>32 At round t, GP-UCB picks action st = xt such that 1/2  st = argmax µt−1 (s) + βt  σt−1 (s),  (1)  s∈S  where βt are appropriate constants. [sent-76, score-0.218]
</p><p>33 Thus, when presented with context zt , this algorithm uses posterior inference to predict mean and variance for each possible decision s, conditioned on all past observations (involving both the chosen actions, the observed contexts as well as the noisy payoffs). [sent-90, score-0.425]
</p><p>34 We call the greedy algorithm implementing rule 2 the contextual Gaussian process UCB algorithm (CGP-UCB). [sent-91, score-0.295]
</p><p>35 As we will show in Section 5, this algorithm allows to incorporate various assumptions about the dependencies of the payoff function on the chosen actions and observed contexts. [sent-92, score-0.392]
</p><p>36 In the following, we will prove that in many practical applications, CGP-UCB attains sublinear contextual regret (i. [sent-94, score-0.639]
</p><p>37 , is able to compete with the optimal mapping from contexts to actions). [sent-96, score-0.231]
</p><p>38 4  Bounds on the Contextual Regret  Bounding the contextual regret of CGP-UCB is a challenging problem, since the regret is measured with respect to the best action for each context. [sent-97, score-0.926]
</p><p>39 Intuitively, the amount of regret we incur should depend on how quickly we can gather information about the payoff function, which now jointly depends on context and actions. [sent-98, score-0.609]
</p><p>40 In the following, we show that the contextual regret of CGP-UCB is bounded by an intuitive information-theoretic quantity, which quantiﬁes the mutual information between the observed context-action pairs and the estimated payoff function f . [sent-99, score-0.791]
</p><p>41 Using this notion of information gain γT , we lift the results of [3] to the much more general contextual bandit setting, shedding further light on the connection between bandit optimization and information gain. [sent-105, score-0.612]
</p><p>42 In Section 5, we show how to bound γT for composite kernels, combining possibly different assumptions about the regularity of f in the action space S and context space Z. [sent-106, score-0.397]
</p><p>43 √ Then the contextual regret of CGP-UCB is bounded by O∗ ( T γT βT ) w. [sent-127, score-0.558]
</p><p>44 Theorem 1 (proof given in the supplemental material) shows that, in case (1) and (2), with high probability over samples from the GP, the cumulative contextual regret is bounded in terms of the maximum information gain with respect to the GP deﬁned over S × Z. [sent-132, score-0.593]
</p><p>45 In case of assumption (3), a regret bound is obtained in a more agnostic setting, where no prior on f is assumed, and much weaker assumptions are made about the noise process. [sent-133, score-0.303]
</p><p>46 A natural approach is to start with kernel functions kZ : Z ×Z → R and kS : S × S → R on the space of contexts and actions, and use them to derive the kernel on the product space. [sent-137, score-0.548]
</p><p>47 The intuition behind this product kernel is a conjunction of the notions of similarities induced by the kernels over context and action spaces: Two context-action pairs are similar (large correlation) if the contexts are similar and actions are similar (Figure 1(a)). [sent-140, score-0.777]
</p><p>48 For example, if kZ and kS are squared exponential kernels (or Mat´ rn kernels with smoothness parameters ν), then the product k = kZ ⊗kS e is a squared exponential kernel (or Mat´ rn kernels with smoothness parameters ν). [sent-142, score-0.437]
</p><p>49 5 −1  −1  Actions  (b)  Figure 1: Illustrations of composite kernel functions that can be incorporated into CGP-UCB. [sent-151, score-0.265]
</p><p>50 (a) Product of squared exponential kernel and linear kernel; (b) additive combination of a payoff function that smoothly depends on context, and exhibits clusters of actions. [sent-152, score-0.43]
</p><p>51 In general, context and action spaces are higher dimensional. [sent-153, score-0.218]
</p><p>52 Since the key quantity governing the regret is the information gain γT , we would like to ﬁnd a convenient way of bounding γT for composite kernels (kS ⊗ kZ and kS ⊕ kZ ), plugging in different regularity assumptions for the contexts (via kZ ) and actions (via kS ). [sent-167, score-0.935]
</p><p>53 More formally, let us deﬁne γ(T ; k; V ) =  max A⊆V,|A|≤T  1 log I + σ −2 [k(v, v )]v,v ∈A , 2  which quantiﬁes the maximum possible information gain achievable by sampling T points in a GP deﬁned over set V with kernel function k. [sent-168, score-0.232]
</p><p>54 , γT for the product of a d1 dimensional linear kernel and a d2 dimensional Gaussian kernel is O(d1 (log T )d2 +1 ). [sent-182, score-0.292]
</p><p>55 [9] model the expected payoff for each action as a (unknown) ∗ ∗ linear function µ(s, z) = zT θs . [sent-195, score-0.338]
</p><p>56 Hereby, θs models the dependence of action s on the context z. [sent-196, score-0.218]
</p><p>57 s Besides online advertising, a similar model has been proposed and experimentally studied by [6] for the problem of contextual news recommendation (see Section 7 for a discussion). [sent-197, score-0.318]
</p><p>58 5  50  100  150 200 Trial t  250  300  350  0 0  CGP−UCB 10  20 30 Trial t per task  40  50  (a) Average regret (b) Maximum regret (c) Context similarity Figure 2: CGP-UCB applied to the average (a) and maximum regret over all molecules (b) for three methods on MHC benchmark. [sent-208, score-0.941]
</p><p>59 In this application, additive kernel combinations may be useful to model temporal dependencies of the overall click probabilities (e. [sent-211, score-0.219]
</p><p>60 In addition to controller parameters s ∈ S ⊆ RdS , the system may be exposed to changing (in an uncontrollable manner) environmental conditions, which are provided as context z ∈ Z ⊆ RdZ . [sent-217, score-0.214]
</p><p>61 In this case, we may consider using a linear kernel kZ (z, z ) = zT z to model the dependence of the performance on environmental features, and a squared exponential kernel kS (s, s ) to model the smooth but nonlinear response of the system to the chosen control parameters. [sent-219, score-0.353]
</p><p>62 Additive kernel combinations may allow to model the fact that control in some contexts (environments) is inherently more difﬁcult (or noisy). [sent-221, score-0.398]
</p><p>63 In this case, we may consider using a ﬁnite inter-task covariance kernel KZ with rank mZ to model the similarity of different experiments, and a Gaussian kernel kS (s, s ) to model the smooth but nonlinear dependency of the stimulus response on the experimental parameters. [sent-230, score-0.379]
</p><p>64 We can cast this problem in the contextual bandit setting, where time of day is considered as the context z ∈ Z, and each action s ∈ S corresponds to picking a sensor. [sent-235, score-0.717]
</p><p>65 We compare three methods: Ignoring (correlation between) contexts by running a separate instance of GP-UCB for every context (i. [sent-243, score-0.344]
</p><p>66 5 4 GP−UCB ignore context  GP−UCB merge context  Temperature (C)  1. [sent-249, score-0.287]
</p><p>67 , ignoring the molecule or time information); and running CGP-UCB, conditioning on measurements made at different contexts (MHC molecules considered / times of day) using the product kernel. [sent-258, score-0.462]
</p><p>68 1  Multi-task Bayesian Optimization of MHC class-I binding afﬁnity  We perform experiments in the multi-task vaccine design problem introduced in Section 5. [sent-260, score-0.229]
</p><p>69 In our experiments, we focus on a subset of MHC class I molecules that have afﬁnity binding scores available. [sent-262, score-0.212]
</p><p>70 In total, we consider identifying peptides for seven different MHC molecules (i. [sent-270, score-0.213]
</p><p>71 The context similarity was obtained using the hamming distance between amino acids in the binding pocket [11] (see Figure 2(c)), and we used the Gaussian kernel on the extracted features. [sent-273, score-0.369]
</p><p>72 From Figure 2(a) we see that for the ﬁrst three molecules (up to trial 150), which are strongly correlated, merging contexts and CGP-UCB perform similarly, and both perform better than ignoring observations from other MHC molecules previously considered. [sent-276, score-0.568]
</p><p>73 However, the fourth molecule (A 0201) has little correlation with the earlier ones, and hence simply merging contexts performs poorly. [sent-277, score-0.328]
</p><p>74 Merging contexts (single instance of context-free GP-UCB) performs best for the ﬁrst few timesteps (since temperature is very similar, and the highest temperature sensor does not change). [sent-290, score-0.509]
</p><p>75 , until context reoccurs), it outperforms the other methods, since it is able to learn to query the maximum temperature sensors as a function of the time of the day. [sent-293, score-0.294]
</p><p>76 The approach for the classical k-armed bandit setting [17] has been generalized to more complex settings, such as inﬁnite action sets and linear payoff functions [14, 18], Lipschitz continuous payoff functions [15] and locally-Lipschitz functions [19]. [sent-295, score-0.787]
</p><p>77 However, there is a strong tradeoff between strength of the assumptions and achievable √ regret bounds. [sent-296, score-0.328]
</p><p>78 For example, while O(d T log T ) can be achieved in the linear setting [14], if only d+1 Lipschitz continuity is assumed, regret bounds scale as Ω(T d+2 ) [15]. [sent-297, score-0.331]
</p><p>79 Srinivas et al [3] analyze the case where the payoff function is sampled from a GP, which encodes conﬁgurable assumptions. [sent-298, score-0.233]
</p><p>80 The ability to incorporate contextual information, however, signiﬁcantly expands the class of applications of GP-UCB. [sent-301, score-0.295]
</p><p>81 Besides handling context and bounding the stronger notion of contextual regret, in this paper we provide generic techniques for obtaining regret bounds for composite kernels. [sent-302, score-0.829]
</p><p>82 An alternative rule (in the context free setting) is the Expected Improvement algorithm [20], for which no bounds on the cumulative regret are known. [sent-303, score-0.418]
</p><p>83 For contextual bandit problems, work has focused on the case of ﬁnitely many actions, where the goal is to obtain sublinear contextual regret against classes of functions mapping context to actions [1]. [sent-304, score-1.332]
</p><p>84 This setting resembles (multi-class) classiﬁcation problems, and regret bounds can be given in terms of the VC dimension of the hypothesis space [2]. [sent-305, score-0.305]
</p><p>85 [6] present an approach, LinUCB, that assumes that payoffs for each action are linear combinations (with unknown coefﬁcients) of context features. [sent-306, score-0.287]
</p><p>86 In [5], it is proven that a modiﬁed variant of LinUCB achieves sublinear contextual regret. [sent-307, score-0.376]
</p><p>87 Theirs is a special case of our setting (assuming a linear kernel for the contexts and diagonal kernel for the actions). [sent-308, score-0.523]
</p><p>88 Another related approach is taken by Slivkins [21], who presents several algorithms with sublinear contextual regret for the case of inﬁnite actions and contexts, assuming Lipschitz continuity of the payoff function in the context-action space. [sent-309, score-0.991]
</p><p>89 However, in contrast to CGP-UCB, this approach does not enable stronger guarantees for smoother or more structured payoff functions. [sent-311, score-0.233]
</p><p>90 The construction of composite kernels is common in the context of multitask learning with GPs [23, 24, 25]. [sent-312, score-0.27]
</p><p>91 Theorems 2 and 3 provide convenient ways for deriving regret bounds for such problems. [sent-315, score-0.305]
</p><p>92 8  Conclusions  We have described an algorithm, CGP-UCB, which addresses the exploration–exploitation tradeoff in a large class of contextual bandit problems, where the regularity of the payoff function deﬁned over the action–context space is expressed in terms of a GP prior. [sent-319, score-0.714]
</p><p>93 As we discuss in Section 5, by considering various kernel functions on actions and contexts this approach allows to handle a variety of applications. [sent-320, score-0.521]
</p><p>94 We show that, similar as in the context free case studied by [3], the key quantity governing the regret is a mutual information between experiments performed by CGP-UCB and the GP prior (Theorem 1). [sent-321, score-0.399]
</p><p>95 In contrast to prior work, however, our approach bounds the much stronger notion of contextual regret (competing with the optimal mapping from contexts to actions). [sent-322, score-0.831]
</p><p>96 We prove that in many practical settings, as discussed in Section 5, the contextual regret is sublinear. [sent-323, score-0.558]
</p><p>97 We also demonstrate the effectiveness of CGP-UCB on two applications: computational vaccine design and sensor network management. [sent-325, score-0.2]
</p><p>98 In both applications, we show that utilizing context information in the joint covariance function reduces regret in comparison to ignoring or naively using the context. [sent-326, score-0.458]
</p><p>99 Gaussian process optimization in the bandit setting: No regret and experimental design. [sent-340, score-0.404]
</p><p>100 Regret bounds for gaussian process bandit u a problems. [sent-448, score-0.22]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kz', 0.412), ('ks', 0.342), ('contextual', 0.295), ('regret', 0.263), ('payoff', 0.233), ('contexts', 0.231), ('gp', 0.224), ('mhc', 0.217), ('kernel', 0.146), ('bandit', 0.141), ('molecules', 0.127), ('ucb', 0.126), ('actions', 0.119), ('vaccine', 0.117), ('context', 0.113), ('temperature', 0.111), ('action', 0.105), ('composite', 0.094), ('binding', 0.085), ('zt', 0.081), ('sublinear', 0.081), ('sensors', 0.07), ('mat', 0.068), ('cgp', 0.067), ('rt', 0.066), ('mz', 0.063), ('kernels', 0.063), ('kt', 0.062), ('molecule', 0.059), ('exploration', 0.059), ('sensor', 0.056), ('peptide', 0.05), ('peptides', 0.05), ('payoffs', 0.048), ('exploitation', 0.046), ('yt', 0.045), ('regularity', 0.045), ('ignoring', 0.045), ('brochu', 0.044), ('aleksandrs', 0.044), ('slivkins', 0.043), ('bounds', 0.042), ('trade', 0.041), ('receive', 0.041), ('fz', 0.041), ('af', 0.04), ('day', 0.04), ('st', 0.04), ('assumptions', 0.04), ('environmental', 0.039), ('merging', 0.038), ('nity', 0.038), ('gaussian', 0.037), ('covariance', 0.037), ('seven', 0.036), ('bandits', 0.035), ('gain', 0.035), ('contextaction', 0.033), ('widmer', 0.033), ('fs', 0.033), ('controller', 0.033), ('merge', 0.033), ('round', 0.033), ('ads', 0.031), ('theorems', 0.03), ('gps', 0.03), ('uncontrollable', 0.029), ('smoothness', 0.029), ('additive', 0.029), ('rkhs', 0.028), ('ignore', 0.028), ('dence', 0.028), ('nite', 0.027), ('gathering', 0.027), ('linucb', 0.027), ('hereby', 0.027), ('animation', 0.027), ('multioutput', 0.027), ('quanti', 0.027), ('design', 0.027), ('log', 0.026), ('functions', 0.025), ('achievable', 0.025), ('similarity', 0.025), ('rank', 0.025), ('lihong', 0.024), ('activate', 0.024), ('picking', 0.023), ('srinivas', 0.023), ('click', 0.023), ('governing', 0.023), ('hayes', 0.023), ('zurich', 0.023), ('news', 0.023), ('squared', 0.022), ('bounding', 0.022), ('deployed', 0.022), ('dani', 0.022), ('automated', 0.022), ('combinations', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="61-tfidf-1" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>2 0.25858468 <a title="61-tfidf-2" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>Author: Yasin Abbasi-yadkori, Csaba Szepesvári, David Tax</p><p>Abstract: We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller conﬁdence sets. For their construction we use a novel tail inequality for vector-valued martingales. 1</p><p>3 0.24193618 <a title="61-tfidf-3" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>4 0.20362526 <a title="61-tfidf-4" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>Author: Yevgeny Seldin, Peter Auer, John S. Shawe-taylor, Ronald Ortner, François Laviolette</p><p>Abstract: We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The p scaling of our regret bound with the number of states (contexts) N goes as N I⇢t (S; A), where I⇢t (S; A) is the mutual information between states and actions (the side information) used by the algorithm at round t. If the algorithm p uses all the side information, the regret bound scales as N ln K, where K is the number of actions (arms). However, if the side information I⇢t (S; A) is not fully used, the regret bound is signiﬁcantly tighter. In the extreme case, when I⇢t (S; A) = 0, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round. 1</p><p>5 0.18262039 <a title="61-tfidf-5" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>Author: Odalric-ambrym Maillard, Daniil Ryabko, Rémi Munos</p><p>Abstract: The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a ﬁnite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T 2/3 where T is the horizon time. 1</p><p>6 0.17848459 <a title="61-tfidf-6" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>7 0.1682979 <a title="61-tfidf-7" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>8 0.16420226 <a title="61-tfidf-8" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>9 0.14302708 <a title="61-tfidf-9" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>10 0.11919926 <a title="61-tfidf-10" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>11 0.1152861 <a title="61-tfidf-11" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>12 0.10967711 <a title="61-tfidf-12" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>13 0.10913555 <a title="61-tfidf-13" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>14 0.10857867 <a title="61-tfidf-14" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>15 0.10717384 <a title="61-tfidf-15" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>16 0.10700877 <a title="61-tfidf-16" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>17 0.10626417 <a title="61-tfidf-17" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>18 0.10211843 <a title="61-tfidf-18" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>19 0.1002882 <a title="61-tfidf-19" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>20 0.098082684 <a title="61-tfidf-20" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, -0.269), (2, 0.037), (3, 0.041), (4, 0.18), (5, -0.051), (6, 0.075), (7, 0.075), (8, 0.176), (9, 0.124), (10, -0.167), (11, 0.058), (12, -0.012), (13, 0.063), (14, -0.022), (15, 0.128), (16, 0.007), (17, 0.058), (18, -0.134), (19, -0.092), (20, -0.028), (21, 0.033), (22, -0.096), (23, -0.079), (24, 0.044), (25, 0.024), (26, 0.015), (27, -0.005), (28, -0.029), (29, -0.07), (30, -0.014), (31, -0.013), (32, 0.02), (33, -0.017), (34, 0.009), (35, -0.008), (36, -0.032), (37, -0.111), (38, -0.076), (39, 0.012), (40, 0.033), (41, 0.034), (42, 0.042), (43, 0.011), (44, -0.056), (45, 0.076), (46, -0.035), (47, -0.047), (48, 0.086), (49, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95889628 <a title="61-lsi-1" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>2 0.69073677 <a title="61-lsi-2" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>Author: Yasin Abbasi-yadkori, Csaba Szepesvári, David Tax</p><p>Abstract: We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller conﬁdence sets. For their construction we use a novel tail inequality for vector-valued martingales. 1</p><p>3 0.67952579 <a title="61-lsi-3" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>Author: Olivier Chapelle, Lihong Li</p><p>Abstract: Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against. 1</p><p>4 0.66052932 <a title="61-lsi-4" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>Author: Alekh Agarwal, Dean P. Foster, Daniel J. Hsu, Sham M. Kakade, Alexander Rakhlin</p><p>Abstract: This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact set X under a stochastic bandit feedback model. In this model, the algorithm is allowed to observe noisy realizations of the function value f (x) at any query point x ∈ X . We demonstrate √ a generalization of the ellipsoid algorithm that √ incurs O(poly(d) T ) regret. Since any algorithm has regret at least Ω( T ) on this problem, our algorithm is optimal in terms of the scaling with T . 1</p><p>5 0.6328544 <a title="61-lsi-5" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>Author: Odalric-ambrym Maillard, Daniil Ryabko, Rémi Munos</p><p>Abstract: The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a ﬁnite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T 2/3 where T is the horizon time. 1</p><p>6 0.60563004 <a title="61-lsi-6" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>7 0.5989626 <a title="61-lsi-7" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>8 0.5715245 <a title="61-lsi-8" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>9 0.56561327 <a title="61-lsi-9" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>10 0.54652607 <a title="61-lsi-10" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>11 0.51577419 <a title="61-lsi-11" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>12 0.50437009 <a title="61-lsi-12" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>13 0.49634412 <a title="61-lsi-13" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>14 0.4870742 <a title="61-lsi-14" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>15 0.44695216 <a title="61-lsi-15" href="./nips-2011-Linear_Submodular_Bandits_and_their_Application_to_Diversified_Retrieval.html">160 nips-2011-Linear Submodular Bandits and their Application to Diversified Retrieval</a></p>
<p>16 0.43513381 <a title="61-lsi-16" href="./nips-2011-Finite_Time_Analysis_of_Stratified_Sampling_for_Monte_Carlo.html">97 nips-2011-Finite Time Analysis of Stratified Sampling for Monte Carlo</a></p>
<p>17 0.42382371 <a title="61-lsi-17" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>18 0.4210659 <a title="61-lsi-18" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>19 0.4043211 <a title="61-lsi-19" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>20 0.39515904 <a title="61-lsi-20" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.028), (4, 0.042), (14, 0.22), (20, 0.034), (26, 0.019), (31, 0.09), (33, 0.022), (43, 0.099), (45, 0.121), (47, 0.012), (57, 0.044), (74, 0.048), (79, 0.031), (83, 0.039), (99, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8185268 <a title="61-lda-1" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>2 0.74763644 <a title="61-lda-2" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>Author: Nati Srebro, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: We show that for a general class of convex online learning problems, Mirror Descent can always achieve a (nearly) optimal regret guarantee. 1</p><p>3 0.73998094 <a title="61-lda-3" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>Author: Hua Wang, Heng Huang, Farhad Kamangar, Feiping Nie, Chris H. Ding</p><p>Abstract: Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reﬂect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class speciﬁc distance metrics and the locally adaptive signiﬁcance coefﬁcients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.</p><p>4 0.69706309 <a title="61-lda-4" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>Author: J. Z. Kolter</p><p>Abstract: Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling. It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation. In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well. In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case. Furthermore, we show that we can efﬁciently project on to this convex set using only samples generated from the system. The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods. 1</p><p>5 0.67993206 <a title="61-lda-5" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>Author: Shengbo Guo, Onno Zoeter, Cédric Archambeau</p><p>Abstract: We propose a new sparse Bayesian model for multi-task regression and classiﬁcation. The model is able to capture correlations between tasks, or more speciﬁcally a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classiﬁcation tasks it achieves competitive predictive performance compared to previously proposed methods. 1</p><p>6 0.67511111 <a title="61-lda-6" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>7 0.66984785 <a title="61-lda-7" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>8 0.66946852 <a title="61-lda-8" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>9 0.66815531 <a title="61-lda-9" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>10 0.66717672 <a title="61-lda-10" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>11 0.66614455 <a title="61-lda-11" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>12 0.66573775 <a title="61-lda-12" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>13 0.66510642 <a title="61-lda-13" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>14 0.66486466 <a title="61-lda-14" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>15 0.66440445 <a title="61-lda-15" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>16 0.66438425 <a title="61-lda-16" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>17 0.66346723 <a title="61-lda-17" href="./nips-2011-Spectral_Methods_for_Learning_Multivariate_Latent_Tree_Structure.html">267 nips-2011-Spectral Methods for Learning Multivariate Latent Tree Structure</a></p>
<p>18 0.66346538 <a title="61-lda-18" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>19 0.66305059 <a title="61-lda-19" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>20 0.6627143 <a title="61-lda-20" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
