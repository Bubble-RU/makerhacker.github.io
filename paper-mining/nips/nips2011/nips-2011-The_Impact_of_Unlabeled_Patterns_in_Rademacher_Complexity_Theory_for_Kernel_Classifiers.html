<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-284" href="#">nips2011-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</h1>
<br/><p>Source: <a title="nips-2011-284-pdf" href="http://papers.nips.cc/paper/4234-the-impact-of-unlabeled-patterns-in-rademacher-complexity-theory-for-kernel-classifiers.pdf">pdf</a></p><p>Author: Luca Oneto, Davide Anguita, Alessandro Ghio, Sandro Ridella</p><p>Abstract: We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. In particular, two results are obtained: the ﬁrst one shows that, using the unlabeled samples, the conﬁdence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classiﬁer. 1</p><p>Reference: <a title="nips-2011-284-reference" href="../nips2011_reference/nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 it  Abstract We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. [sent-7, score-0.327]
</p><p>2 1  Introduction  Understanding the factors that inﬂuence the performance of a statistical procedure is a key step for ﬁnding a way to improve it. [sent-9, score-0.018]
</p><p>3 For reaching this target, several approaches have been proposed [1, 2, 3, 4], which provide an upper bound on the generalization ability of the classiﬁer, which can be used for model selection purposes as well. [sent-11, score-0.093]
</p><p>4 Typically, all these bounds consists of three terms: the ﬁrst one is the empirical error of the classiﬁer (i. [sent-12, score-0.058]
</p><p>5 the error performed on the training data), the second term is a bias that takes into account the complexity of the class of functions, which the classiﬁer belongs to, and the third one is a conﬁdence term, which depends on the cardinality of the training set. [sent-14, score-0.172]
</p><p>6 One of the most recent methods for obtaining these bounds is to exploit the Rademacher Complexity, which is a powerful statistical tool that has been deeply investigated during the last years [5, 6, 7]. [sent-16, score-0.076]
</p><p>7 This approach has shown to be of practical use, by outperforming more traditional methods [8, 9] for model selection in the small–sample regime [10, 5, 6], i. [sent-17, score-0.02]
</p><p>8 when the dimensionality of the samples is comparable, or even larger, than the cardinality of the training set. [sent-19, score-0.085]
</p><p>9 We show in this work how its performance can be further improved by exploiting some extra knowledge on the problem. [sent-20, score-0.052]
</p><p>10 In fact, real–world classiﬁcation problems often are composed of datasets with labeled and unlabeled data [11, 12]: for this reason an interesting challenge is ﬁnding a way to exploit the unlabeled data for obtaining tighter bounds and, therefore, better error estimations. [sent-21, score-0.664]
</p><p>11 In this paper, we present two methods for exploiting the unlabeled data in the Rademacher Complexity theory [2]. [sent-22, score-0.284]
</p><p>12 First, we show how the unlabeled data can have a role in reducing the conﬁdence 1 See, for example, the NIPS 2004 Workshop (Ab)Use of Bounds or the 2002 Neurocolt Workshop on Bounds less than 0. [sent-23, score-0.232]
</p><p>13 5  1  term, by obtaining a new bound that takes into account both labeled and unlabeled data. [sent-24, score-0.34]
</p><p>14 Then, we propose a method, based on [7], which exploits the unlabeled data for selecting a better hypothesis space, which the classiﬁer belongs to, resulting in a much sharper and accurate bound. [sent-25, score-0.375]
</p><p>15 2  Theoretical framework and results  We consider the following prediction problem: based on a random observation of X ∈ X ⊆ Rd one has to estimate Y ∈ Y ⊆ {−1, 1} by choosing a suitable prediction rule f : X → [−1, 1]. [sent-26, score-0.051]
</p><p>16 The generalization error L(f ) = E{X ,Y} ℓ(f (X), Y ) associated to the prediction rule is deﬁned through a bounded loss function ℓ(f (X), Y ) : [−1, 1] × Y → [0, 1]. [sent-27, score-0.074]
</p><p>17 We observe a set of labeled u u l l l samples Dnl : (X1 , Y1l ), · · · , (Xnl , Ynl ) and a set of unlabeled ones Dnu : (X1 ), · · · , (Xnu ) . [sent-28, score-0.323]
</p><p>18 ) samples with the same distribution P (X , Y) for Dnl and Dnu . [sent-32, score-0.043]
</p><p>19 The goal is to obtain a bound on L(f ) that takes into account both the labeled and unlabeled data. [sent-33, score-0.318]
</p><p>20 As we do not know the distribution that have generated nl l the data, we do not know L(f ) but only its empirical estimation Lnl (f ) = 1/nl i=1 ℓ(f (Xi ), Yil ). [sent-34, score-0.695]
</p><p>21 In the typical context of Structural Risk Minimization (SRM) [13] we deﬁne an inﬁnite sequence of hypothesis spaces of increasing complexity {Fi , i = 1, 2, · · · }, then we choose a suitable function space Fi and, consequently, a model f ∗ ∈ Fi that ﬁts the data. [sent-35, score-0.177]
</p><p>22 Note that both the hard loss ℓH (fH (x), y) and the soft loss (or ramp loss) [14] ℓS (fS (x), y) are bounded ([0, 1]) and symmetric (ℓ(f (x), y) = 1 − ℓ(f (x), −y)). [sent-39, score-0.034]
</p><p>23 Then, we recall the deﬁnition of Rademacher Complexity (R) for a class of functions F: 2 ˆ Rnl (F) = Eσ sup nl f ∈F  nl  σi ℓ(f (xi ), yi ) = Eσ sup  f ∈F  i=1  1 nl  nl  σi f (xi )  (6)  i=1  where σ1 , . [sent-40, score-3.085]
</p><p>24 , σnl are nl independent Rademacher random variables, i. [sent-43, score-0.695]
</p><p>25 independent random variables for which P(σi = +1) = P(σi = −1) = 1/2, and the last equality holds if we use one ˆ of the losses deﬁned before. [sent-45, score-0.02]
</p><p>26 Note that R is a computable realization of the expected Rademacher ˆ Complexity R(F) = E(X ,Y) R(F). [sent-46, score-0.019]
</p><p>27 The most renowed result in Rademacher Complexity theory states that [2]: ˆ L(f )f ∈F ≤ Lnl (f )f ∈F + Rnl (F) + 3  log 2 δ 2nl  which holds with probability (1 − δ) and allows to solve the problem of Eq. [sent-47, score-0.02]
</p><p>28 The last quantity (that we call Expected Extended Rademacher ˆ Complexity E{X ,Y} Rnu (F)) and the expected generalization bias are both deterministic quantities and we know only one realization of them, dependent on the sample. [sent-52, score-0.078]
</p><p>29 Then, we can use the McDiarmid’s inequality [15] to obtain: ˆ P sup {L(f ) − Lnl (f )} ≥ Rnu (F) + ǫ ≤  (8)  P sup {L(f ) − Lnl (f )} ≥ E{X ,Y} sup {L(f ) − Lnl (f )} + aǫ +  (9)  ˆ ˆ P E{X ,Y} Rnu (F) ≥ Rnu (F) + (1 − a)ǫ ≤  (10)  f ∈F  f ∈F  f ∈F  e−2nl a with a ∈ [0, 1]. [sent-53, score-0.384]
</p><p>30 Note that for m = 1 the training set does not contain any unlabeled data and the bound given by Eq. [sent-55, score-0.289]
</p><p>31 (3) is recovered, while for large m the conﬁdence term is reˆ duced by a factor of 3. [sent-56, score-0.023]
</p><p>32 At a ﬁrst sight, it would seem impossible to compute the term Ri l n without knowing the labels of the data, but it is easy to show that this is not the case. [sent-57, score-0.023]
</p><p>33 In − + fact, let us deﬁne Ki = k ∈ {k = (i − 1) · nl + 1, . [sent-58, score-0.695]
</p><p>34 , i · nl } : σ|k|nl = +1 and Ki = 2  we deﬁne ℓ(f (xi ), yi ) ≡ ℓi to simplify the notation  3  (a) Coventional function classes  (b) Localized function classes  Figure 1: The effect of selecting a better center for the hypothesis classes. [sent-61, score-0.953]
</p><p>35 2  Exploiting the unlabeled data for tightening the bound  Another way of exploiting the unlabeled data is to use them for selecting a more suitable sequence of hypothesis spaces. [sent-68, score-0.759]
</p><p>36 For this purpose we could use some of the unlabeled samples or, even better, the nc = nu − ⌊nu /nl ⌋ nl samples left from the procedure of the previous section. [sent-69, score-1.185]
</p><p>37 The idea is inspired by the work of [3] and [7], which propose to inﬂate the hypothesis classes by centering them around a ‘good’ classiﬁer. [sent-70, score-0.121]
</p><p>38 However, if it happens that the center is ‘close’ to the optimal classiﬁer, the search for a suitable class will stop very soon and the resulting Rademacher Complexity will be consequently reduced (see Figure 1(b)). [sent-72, score-0.061]
</p><p>39 We propose here a method for ﬁnding two possible ‘good’ centers for the hypothesis classes. [sent-73, score-0.131]
</p><p>40 Let us consider nc unlabeled samples and run a clustering algorithm on them, by setting the number of clusters to 2, and obtaining two clusters C1 and C2 . [sent-74, score-0.328]
</p><p>41 We build two distinct labeled datasets by assigning the labels +1 and −1 to C1 and C2 , respectively, and then vice-versa. [sent-75, score-0.082]
</p><p>42 Finally, we build two classiﬁers fC1 (x) and fC2 (x) = −fC1 (x) by learning the two datasets3 . [sent-76, score-0.018]
</p><p>43 The two classiﬁers, which have been found using only unlabeled samples, can then be used as centers for searching a better hypothesis class. [sent-77, score-0.363]
</p><p>44 It is worthwhile noting that any supervised learning algorithm can be used [16], because the centers are only a hint for a better centered hypothesis space: their actual classiﬁcation performance is not of paramount importance. [sent-78, score-0.165]
</p><p>45 4  this procedure relies on the reasonable hypothesis that P(X ) is correlated with P(X , Y): in fact, in an unlucky scenario, where the two classes are heavily overlapped, the method would obviously fail. [sent-80, score-0.139]
</p><p>46 Choosing a good center for the SRM procedure can greatly reduce the second term of the bound given by Eq. [sent-81, score-0.107]
</p><p>47 Note, however, that the conﬁdence term is not ˆn affected, so we propose here an improved bound, which makes this term depending on Ri l (F) as well. [sent-83, score-0.046]
</p><p>48 We use a recent concentration result for Self Bounding Functions [17], instead of the looser McDiarmid’s inequality. [sent-84, score-0.03]
</p><p>49 Unfortunately, the Expected Extended Rademacher Complexity cannot be computed, but we can upper bound it with its empirical version (see, for example, [19], pages 420–422, for a justiﬁcaton of this step) as in Eq. [sent-88, score-0.038]
</p><p>50 (10) to obtain: (mnl )(1−a)2 ǫ2  2 2 − ˆ ˆ P sup {L(f ) − Lnl (f )} ≥ Rnu (F) + ǫ ≤ e−2nl a ǫ + e 2(Rnu (F )+(1−a)ǫ)  (17)  f ∈F  with a ∈ [0, 1]. [sent-89, score-0.128]
</p><p>51 (15) the previous expression cannot be put in explicit form, but it can be simply computed numerically by writing it as: L(f )f ∈F ≤ Lnl (f )f ∈F +  1 m  m  i=1  ˆn Ri l (F) + ǫb u  (18)  The value ǫb can be obtained by upper bounding with δ the last term of Eq. [sent-91, score-0.042]
</p><p>52 (17) and solving the u inequality respect to a and ǫ, so that the bound holds with probability (1 − δ). [sent-92, score-0.058]
</p><p>53 We can show the improvements obtained through these new results, by plotting the values of the conﬁdence terms and comparing them with the conventional one [2]. [sent-93, score-0.033]
</p><p>54 3  Performing the Structural Risk Minimization procedure  Computing the values of the bounds described in the previous sections is a straightforward process, at least in theory. [sent-97, score-0.054]
</p><p>55 The empirical error Lnl (f ) is found by learning a classiﬁer with the original ˆn labeled dataset, while the (Extended) Rademacher Complexity Ri l (F) is computed by learning the dataset composed of both labeled and unlabeled samples with random labels. [sent-98, score-0.413]
</p><p>56 In order apply in practice the results of the previous section and to better control the hypothesis space, we formulate the learning phase of the classiﬁer as the following optimization problem, based 5  m ∈ [1,10]  m = 1, R ∈ [0,1]  1  1 ε  ε  l  0. [sent-99, score-0.102]
</p><p>57 1 120 n  140  160  180  0  200  40  60  80  100  120 n  140  160  180  200  (b) ǫnl VS ǫb with m = 1 u  (a) ǫl VS ǫu  Figure 2: Comparison of the new conﬁdence terms with the conventional one. [sent-118, score-0.033]
</p><p>58 Furthermore, we derive a dual formulation of problem (19) that allows us exploiting the well known Sequential Minimal Optimization (SMO) algorithm for SVM learning [23]. [sent-123, score-0.052]
</p><p>59 Problem (19) can be rewritten in the equivalent Tikhonov formulation: min  w,b,ξ  1 ˆ w−w 2  n 2  ηi  +C  (20)  i=1  yi wT φ(xi ) + b ≥ 1 − ξi ξi ≥ 0,  ηi = min (2, ξi )  which gives the same solution of the Ivanov formulation for some value of C [13]. [sent-124, score-0.093]
</p><p>60 The method for ﬁnding the value of C, corresponding to a given value of ρ, is reported in [10], where it is also shown that C cannot be used directly to control the hypothesis space. [sent-125, score-0.126]
</p><p>61 4  A case study  We consider the MNIST dataset [25], which consists of 62000 images, representing the numbers from 0 to 9: in particular, we consider the 13074 patterns containing 0’s and 1’s, allowing us to deal with a binary classiﬁcation problem. [sent-129, score-0.022]
</p><p>62 We simulate the small–sample regime by randomly sampling a training set with low cardinality (nl < 500), while the remaining 13074 − nl images are used as a test set or as an unlabeled dataset, by simply discarding the labels. [sent-130, score-0.969]
</p><p>63 In order to build statistically relevant results, this procedure is repeated 30 times. [sent-131, score-0.036]
</p><p>64 In Table 1 we compare the conventional bound with our proposal. [sent-132, score-0.071]
</p><p>65 In the ﬁrst column the number of labeled patterns (nl ) is reported, while the second column shows the number of unlabeled ones (nu ). [sent-133, score-0.302]
</p><p>66 The optimal classiﬁer f ∗ is selected by varying ρ in the range [10−6 , 1], and selecting the function corresponding to the minimum of the generalization error estimate provided by each bound. [sent-134, score-0.098]
</p><p>67 Then, for each case, the selected f ∗ is tested on the remaining 13074 − (nl + nu ) samples and the classiﬁcation results are reported in column three and four, respectively. [sent-135, score-0.19]
</p><p>68 The results show that the f ∗ selected by exploiting the unlabeled patterns behaves better than the other and, furthermore, the estimated L(f ), reported in column ﬁve and six, shows that the bound is tighter, as expected by theory. [sent-136, score-0.368]
</p><p>69 The most interesting result, however, derives from the use of the new bound of Eq. [sent-137, score-0.038]
</p><p>70 (18), as reported in Table 2, where the unlabeled data is exploited for selecting a more suitable center of the hypothesis space. [sent-138, score-0.46]
</p><p>71 Note that, for each experiment, 30% Algorithm 1 CCCP procedure Initialize θ 0 repeat θ t+1 = arg minθ Jconvex (θ) + until θ  t+1  =θ  t  dJconcave (θ) dθ  θ θt  7  Table 1: Model selection and error estimation, exploiting unlabeled data for tightening the bound. [sent-140, score-0.373]
</p><p>72 nl 10 20 40 60 80 100 120 150 170 200 250 300 400  nu 20 40 80 120 160 200 240 300 340 400 500 600 800  Test error of f ∗ Eq. [sent-141, score-0.84]
</p><p>73 19  Table 2: Model selection and error estimation, exploiting unlabeled data for selecting a more suitable hypothesis center. [sent-249, score-0.502]
</p><p>74 nl 7 14 28 42 56 70 84 105 119 140 175 210 280  nu 3 6 12 18 24 30 36 45 51 60 75 90 120  Test error of f ∗ Eq. [sent-250, score-0.84]
</p><p>75 00  of the data (nu ) are used for selecting the hypothesis center and the remaining ones (nl ) are used for training the classiﬁer. [sent-358, score-0.19]
</p><p>76 The proposed method consistently selects a better classiﬁer, which registers a threefold classiﬁcation error reduction on the test set, especially for training sets of smaller cardinality. [sent-359, score-0.058]
</p><p>77 We have to consider that this very clear performance increase is also favoured by the characteristics of the MNIST dataset, which consists of well–separated classes: this particular data distribution implies that only few samples sufﬁce for identifying a good hypothesis center. [sent-361, score-0.162]
</p><p>78 Many more experiments with different datasets and varying the ratio between labeled and unlabeled samples are needed, and are currently underway, for establishing the general validity of our proposal but, in any case, these results appear to be very promising. [sent-362, score-0.323]
</p><p>79 5  Conclusion  In this paper we have studied two methods which exploit unlabeled samples to tighten the Rademacher Complexity bounds on the generalization error of linear (kernel) classiﬁers. [sent-363, score-0.386]
</p><p>80 The ﬁrst method improves a very well–known result, while the second one aims at changing the entire approach by selecting more suitable hypothesis spaces, not only acting on the bound itself. [sent-364, score-0.214]
</p><p>81 The recent literature on the theory of bounds attempts to obtain tighter bounds through more reﬁned concentration inequalities (e. [sent-365, score-0.136]
</p><p>82 improving Mc Diarmid’s inequality), but we believe that the idea of reducing the size of the hypothesis space is a more appealing ﬁeld of research because it opens the road to possible signiﬁcant improvements. [sent-367, score-0.102]
</p><p>83 Rademacher and Gaussian complexities: Risk bounds and structural results. [sent-379, score-0.053]
</p><p>84 On efﬁcient large margin semisupervised learning: Method and theory. [sent-493, score-0.023]
</p><p>85 Sequential minimal optimization: A fast algorithm for training support vector machines. [sent-502, score-0.036]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nl', 0.695), ('lnl', 0.401), ('rnu', 0.267), ('unlabeled', 0.232), ('rademacher', 0.173), ('sup', 0.128), ('nu', 0.123), ('hypothesis', 0.102), ('fi', 0.097), ('ki', 0.08), ('anguita', 0.076), ('ghio', 0.076), ('mnl', 0.076), ('wt', 0.073), ('classi', 0.065), ('srm', 0.062), ('fk', 0.06), ('ri', 0.06), ('oneto', 0.057), ('exploiting', 0.052), ('yi', 0.049), ('er', 0.049), ('labeled', 0.048), ('yk', 0.047), ('boucheron', 0.046), ('samples', 0.043), ('dence', 0.042), ('complexity', 0.042), ('fh', 0.041), ('selecting', 0.041), ('cccp', 0.039), ('bound', 0.038), ('djconcave', 0.038), ('dnl', 0.038), ('dnu', 0.038), ('genova', 0.038), ('jconvex', 0.038), ('rnl', 0.038), ('fs', 0.038), ('bounds', 0.036), ('generalization', 0.035), ('tighter', 0.034), ('ijcnn', 0.034), ('ivanov', 0.034), ('suitable', 0.033), ('conventional', 0.033), ('nc', 0.031), ('lugosi', 0.031), ('mcdiarmid', 0.031), ('concentration', 0.03), ('tightening', 0.029), ('centers', 0.029), ('center', 0.028), ('jose', 0.026), ('concave', 0.026), ('ers', 0.026), ('xi', 0.025), ('bias', 0.024), ('reported', 0.024), ('bousquet', 0.024), ('margin', 0.023), ('bartlett', 0.023), ('term', 0.023), ('cardinality', 0.023), ('min', 0.022), ('patterns', 0.022), ('obtaining', 0.022), ('error', 0.022), ('mnist', 0.021), ('composed', 0.02), ('selection', 0.02), ('surveys', 0.02), ('holds', 0.02), ('con', 0.02), ('bounding', 0.019), ('classes', 0.019), ('complexities', 0.019), ('realization', 0.019), ('rd', 0.019), ('training', 0.019), ('localized', 0.019), ('exploit', 0.018), ('electronic', 0.018), ('procedure', 0.018), ('choosing', 0.018), ('build', 0.018), ('risk', 0.018), ('kernel', 0.017), ('structural', 0.017), ('loss', 0.017), ('support', 0.017), ('favoured', 0.017), ('paramount', 0.017), ('threefold', 0.017), ('hint', 0.017), ('underway', 0.017), ('ptr', 0.017), ('luca', 0.017), ('vs', 0.017), ('assigning', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="284-tfidf-1" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>Author: Luca Oneto, Davide Anguita, Alessandro Ghio, Sandro Ridella</p><p>Abstract: We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. In particular, two results are obtained: the ﬁrst one shows that, using the unlabeled samples, the conﬁdence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classiﬁer. 1</p><p>2 0.18708773 <a title="284-tfidf-2" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>Author: Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, Jieping Ye</p><p>Abstract: Discriminative learning when training and test data belong to different distributions is a challenging and complex task. Often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions. The difference in distributions may be both in marginal and conditional probabilities. Most of the existing domain adaptation work focuses on the marginal probability distribution difference between the domains, assuming that the conditional probabilities are similar. However in many real world applications, conditional probability distribution differences are as commonplace as marginal probability differences. In this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (ﬁrst stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted Rademacher complexity measure. Empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach. 1</p><p>3 0.083552361 <a title="284-tfidf-3" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>Author: Stéphan J. Clémençcon</p><p>Abstract: Many clustering techniques aim at optimizing empirical criteria that are of the form of a U -statistic of degree two. Given a measure of dissimilarity between pairs of observations, the goal is to minimize the within cluster point scatter over a class of partitions of the feature space. It is the purpose of this paper to deﬁne a general statistical framework, relying on the theory of U -processes, for studying the performance of such clustering methods. In this setup, under adequate assumptions on the complexity of the subsets forming the partition candidates, the √ excess of clustering risk is proved to be of the order OP (1/ n). Based on recent results related to the tail behavior of degenerate U -processes, it is also shown how to establish tighter rate bounds. Model selection issues, related to the number of clusters forming the data partition in particular, are also considered. 1</p><p>4 0.081770815 <a title="284-tfidf-4" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>Author: Kamiar R. Rad, Liam Paninski</p><p>Abstract: Many fundamental questions in theoretical neuroscience involve optimal decoding and the computation of Shannon information rates in populations of spiking neurons. In this paper, we apply methods from the asymptotic theory of statistical inference to obtain a clearer analytical understanding of these quantities. We ﬁnd that for large neural populations carrying a ﬁnite total amount of information, the full spiking population response is asymptotically as informative as a single observation from a Gaussian process whose mean and covariance can be characterized explicitly in terms of network and single neuron properties. The Gaussian form of this asymptotic sufﬁcient statistic allows us in certain cases to perform optimal Bayesian decoding by simple linear transformations, and to obtain closed-form expressions of the Shannon information carried by the network. One technical advantage of the theory is that it may be applied easily even to non-Poisson point process network models; for example, we ﬁnd that under some conditions, neural populations with strong history-dependent (non-Poisson) effects carry exactly the same information as do simpler equivalent populations of non-interacting Poisson neurons with matched ﬁring rates. We argue that our ﬁndings help to clarify some results from the recent literature on neural decoding and neuroprosthetic design.</p><p>5 0.071610168 <a title="284-tfidf-5" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a ﬁxed distribution, and the adversarial scenario wherein, at every time step, an adversarially chosen instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We deﬁne the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we deﬁne a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with inﬁnite Littlestone dimension learnable. 1</p><p>6 0.066667706 <a title="284-tfidf-6" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>7 0.062291425 <a title="284-tfidf-7" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>8 0.058020685 <a title="284-tfidf-8" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>9 0.053873498 <a title="284-tfidf-9" href="./nips-2011-Co-Training_for_Domain_Adaptation.html">53 nips-2011-Co-Training for Domain Adaptation</a></p>
<p>10 0.05304677 <a title="284-tfidf-10" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>11 0.050620388 <a title="284-tfidf-11" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>12 0.050035089 <a title="284-tfidf-12" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>13 0.044282775 <a title="284-tfidf-13" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>14 0.041831505 <a title="284-tfidf-14" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>15 0.041696098 <a title="284-tfidf-15" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>16 0.041276932 <a title="284-tfidf-16" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>17 0.040951431 <a title="284-tfidf-17" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>18 0.040450826 <a title="284-tfidf-18" href="./nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">178 nips-2011-Multiclass Boosting: Theory and Algorithms</a></p>
<p>19 0.038987007 <a title="284-tfidf-19" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>20 0.038539667 <a title="284-tfidf-20" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, -0.021), (2, -0.034), (3, -0.029), (4, 0.031), (5, 0.052), (6, 0.017), (7, -0.083), (8, -0.05), (9, -0.009), (10, -0.073), (11, 0.077), (12, 0.076), (13, 0.057), (14, -0.028), (15, -0.005), (16, -0.049), (17, -0.065), (18, 0.092), (19, 0.033), (20, -0.077), (21, 0.016), (22, -0.019), (23, -0.026), (24, 0.025), (25, 0.048), (26, -0.05), (27, 0.001), (28, 0.016), (29, 0.0), (30, 0.064), (31, -0.074), (32, -0.011), (33, 0.049), (34, 0.05), (35, 0.041), (36, 0.072), (37, 0.057), (38, -0.047), (39, 0.048), (40, -0.059), (41, -0.005), (42, 0.033), (43, -0.11), (44, 0.014), (45, 0.067), (46, 0.033), (47, -0.009), (48, 0.062), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90576923 <a title="284-lsi-1" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>Author: Luca Oneto, Davide Anguita, Alessandro Ghio, Sandro Ridella</p><p>Abstract: We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. In particular, two results are obtained: the ﬁrst one shows that, using the unlabeled samples, the conﬁdence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classiﬁer. 1</p><p>2 0.67505985 <a title="284-lsi-2" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>Author: Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, Jieping Ye</p><p>Abstract: Discriminative learning when training and test data belong to different distributions is a challenging and complex task. Often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions. The difference in distributions may be both in marginal and conditional probabilities. Most of the existing domain adaptation work focuses on the marginal probability distribution difference between the domains, assuming that the conditional probabilities are similar. However in many real world applications, conditional probability distribution differences are as commonplace as marginal probability differences. In this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (ﬁrst stage) as well as conditional probability differences (second stage), with the target domain data. The weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional probability differences are computed simultaneously by exploiting the potential interaction among multiple sources. We also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted Rademacher complexity measure. Empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach. 1</p><p>3 0.66146439 <a title="284-lsi-3" href="./nips-2011-Co-Training_for_Domain_Adaptation.html">53 nips-2011-Co-Training for Domain Adaptation</a></p>
<p>Author: Minmin Chen, Kilian Q. Weinberger, John Blitzer</p><p>Abstract: Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most conﬁdent. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of cotraining, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA signiﬁcantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance. 1</p><p>4 0.57788301 <a title="284-lsi-4" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>5 0.57672781 <a title="284-lsi-5" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>Author: Purushottam Kar, Prateek Jain</p><p>Abstract: We consider the problem of classiﬁcation using similarity/distance functions over data. Speciﬁcally, we propose a framework for deﬁning the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions. Our framework uniﬁes and generalizes the frameworks proposed by [1] and [2]. An attractive feature of our framework is its adaptability to data - we do not promote a ﬁxed notion of goodness but rather let data dictate it. We show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems. We propose a landmarking-based approach to obtaining a classiﬁer from such learned goodness criteria. We then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection. We demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark UCI datasets on which our method consistently outperforms existing approaches by a signiﬁcant margin. 1</p><p>6 0.56316388 <a title="284-lsi-6" href="./nips-2011-An_Exact_Algorithm_for_F-Measure_Maximization.html">33 nips-2011-An Exact Algorithm for F-Measure Maximization</a></p>
<p>7 0.53933561 <a title="284-lsi-7" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>8 0.48556948 <a title="284-lsi-8" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>9 0.47026998 <a title="284-lsi-9" href="./nips-2011-Differentially_Private_M-Estimators.html">69 nips-2011-Differentially Private M-Estimators</a></p>
<p>10 0.46956208 <a title="284-lsi-10" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>11 0.46450669 <a title="284-lsi-11" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>12 0.45906299 <a title="284-lsi-12" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>13 0.45876259 <a title="284-lsi-13" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>14 0.44895726 <a title="284-lsi-14" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>15 0.44637573 <a title="284-lsi-15" href="./nips-2011-Submodular_Multi-Label_Learning.html">277 nips-2011-Submodular Multi-Label Learning</a></p>
<p>16 0.42650908 <a title="284-lsi-16" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>17 0.42337558 <a title="284-lsi-17" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>18 0.41660127 <a title="284-lsi-18" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>19 0.41634205 <a title="284-lsi-19" href="./nips-2011-Advice_Refinement_in_Knowledge-Based_SVMs.html">27 nips-2011-Advice Refinement in Knowledge-Based SVMs</a></p>
<p>20 0.41485995 <a title="284-lsi-20" href="./nips-2011-History_distribution_matching_method_for_predicting_effectiveness_of_HIV_combination_therapies.html">120 nips-2011-History distribution matching method for predicting effectiveness of HIV combination therapies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (4, 0.034), (20, 0.038), (26, 0.03), (31, 0.057), (33, 0.03), (43, 0.064), (45, 0.173), (48, 0.032), (57, 0.032), (74, 0.037), (83, 0.045), (84, 0.014), (93, 0.251), (99, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78589439 <a title="284-lda-1" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>Author: Luca Oneto, Davide Anguita, Alessandro Ghio, Sandro Ridella</p><p>Abstract: We derive here new generalization bounds, based on Rademacher Complexity theory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. In particular, two results are obtained: the ﬁrst one shows that, using the unlabeled samples, the conﬁdence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classiﬁer. 1</p><p>2 0.64312905 <a title="284-lda-2" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><p>3 0.6400193 <a title="284-lda-3" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>4 0.63975883 <a title="284-lda-4" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>Author: Nicolò Cesa-bianchi, Ohad Shamir</p><p>Abstract: Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely diﬀerent approach, which combines “random playout” and randomized rounding of loss subgradients. As an application of our approach, we provide the ﬁrst computationally eﬃcient online algorithm for collaborative ﬁltering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning. 1</p><p>5 0.63894266 <a title="284-lda-5" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study multi-label prediction for structured output sets, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multilabel classiﬁcation techniques are typically not applicable in this situation, because they require explicit enumeration of the label set, which is infeasible in case of structured outputs. Relying on techniques originally designed for single-label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems. In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneﬁcial properties with single-label maximum-margin approaches, in particular formulation as a convex optimization problem, efﬁcient working set training, and PAC-Bayesian generalization bounds. 1</p><p>6 0.6387583 <a title="284-lda-6" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>7 0.63737035 <a title="284-lda-7" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>8 0.63734573 <a title="284-lda-8" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>9 0.63676327 <a title="284-lda-9" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>10 0.63650239 <a title="284-lda-10" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>11 0.63610488 <a title="284-lda-11" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>12 0.63594019 <a title="284-lda-12" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>13 0.63402051 <a title="284-lda-13" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>14 0.63391274 <a title="284-lda-14" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<p>15 0.63365132 <a title="284-lda-15" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>16 0.63274527 <a title="284-lda-16" href="./nips-2011-Shaping_Level_Sets_with_Submodular_Functions.html">251 nips-2011-Shaping Level Sets with Submodular Functions</a></p>
<p>17 0.63190919 <a title="284-lda-17" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>18 0.63189125 <a title="284-lda-18" href="./nips-2011-Boosting_with_Maximum_Adaptive_Sampling.html">49 nips-2011-Boosting with Maximum Adaptive Sampling</a></p>
<p>19 0.6314804 <a title="284-lda-19" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>20 0.63126206 <a title="284-lda-20" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
