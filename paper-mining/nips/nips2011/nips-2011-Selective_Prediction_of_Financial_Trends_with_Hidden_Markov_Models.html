<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-246" href="#">nips2011-246</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</h1>
<br/><p>Source: <a title="nips-2011-246-pdf" href="http://papers.nips.cc/paper/4301-selective-prediction-of-financial-trends-with-hidden-markov-models.pdf">pdf</a></p><p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>Reference: <a title="nips-2011-246-reference" href="../nips2011_reference/nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. [sent-5, score-0.684]
</p><p>2 We examine two types of selective mechanisms for HMM predictors. [sent-6, score-0.378]
</p><p>3 Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. [sent-8, score-0.333]
</p><p>4 Ĺš es low quality HMM states and abstain from prediction in those states. [sent-10, score-0.322]
</p><p>5 In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. [sent-12, score-0.374]
</p><p>6 We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. [sent-13, score-0.218]
</p><p>7 Currently, manifestations of selective prediction within machine learning mainly exist in the realm of inductive classiÄ? [sent-19, score-0.42]
</p><p>8 Ĺš er or predictor equipped with a rejection mechanism we can quantify its performance proÄ? [sent-24, score-0.269]
</p><p>9 The RC curve represents a trade-off: the more coverage we compromise, the more accurate we can expect to be, up to the point where we reject everything and (trivially) never err. [sent-26, score-0.451]
</p><p>10 Ĺš ers achieving useful (and optimal) RC trade-offs, thus providing the user with control over the choice of desired risk (with its associated coverage compromise). [sent-29, score-0.473]
</p><p>11 Our longer term goal is to study selective prediction models for general sequential prediction tasks. [sent-30, score-0.525]
</p><p>12 The second is a novel and specialized technique utilizing the HMM state structure. [sent-42, score-0.237]
</p><p>13 In this approach we identify latent states whose prediction quality is systematically inferior, and abstain from predictions while the underlying source is likely to be in 1  those states. [sent-43, score-0.402]
</p><p>14 1  Preliminaries Hidden Markov Models in brief  A Hidden Markov Model (HMM) is a generative probabilistic state machine with latent states, in which state transitions and observations emissions represent Ä? [sent-58, score-0.324]
</p><p>15 the most likely (in a Bayesian sense) state machine giving rise to O, with associated latent state sequence S = S1 , . [sent-64, score-0.393]
</p><p>16 Ĺš ne the performance parameters in selective prediction we utilize the following deÄ? [sent-82, score-0.42]
</p><p>17 The purpose of a selective prediction model is to provide Ă˘&euro;&oelig;sufÄ? [sent-96, score-0.42]
</p><p>18 The functional relation between risk and coverage is called the risk coverage (RC) trade-off. [sent-101, score-0.83]
</p><p>19 Generally, the user of a selective model would like to bound one measure (either risk or coverage) and then obtain the best model in terms of the other measure. [sent-102, score-0.515]
</p><p>20 A selective predictor is useful if its RC curve is Ă˘&euro;&oelig;non trivialĂ˘&euro;? [sent-104, score-0.373]
</p><p>21 in the sense that progressively smaller risk can be obtained with progressively smaller coverage. [sent-105, score-0.196]
</p><p>22 Interpolated RC curve can be obtained by selecting a number of coverage bounds at certain grid points of choice, and learning (and testing) a selective model aiming at achieving the best possible risk for each coverage level. [sent-109, score-1.09]
</p><p>23 Obviously, each such model should respect the corresponding coverage bound. [sent-110, score-0.269]
</p><p>24 Ĺš er, similar to the one used in [3], and endow it with a rejection mechanism in the spirit of Chow [5]. [sent-115, score-0.227]
</p><p>25 2  State-Based Selectivity  We propose a different approach for implementing selective prediction with HMMs. [sent-139, score-0.42]
</p><p>26 The proposed approach is suitable for prediction problems whose observation sequences are labeled. [sent-142, score-0.226]
</p><p>27 Each state is assigned risk and visit rate estimates. [sent-146, score-0.598]
</p><p>28 For each state q, its risk estimate is used as a proxy to the probability of making erroneous predictions from q, and its visit rate quantiÄ? [sent-147, score-0.671]
</p><p>29 A subset of the highest risk states is selected so that their total expected visit rate does not exceed the user speciÄ? [sent-149, score-0.651]
</p><p>30 These states are called rejective and predictions from them are ignored. [sent-151, score-0.331]
</p><p>31 We associate with each state q a label Lq representing the HMM prediction while at this state (see Section 3. [sent-154, score-0.454]
</p><p>32 Denote Ă&#x17D;Ĺ&sbquo;t (i) P [St = qi | O, Ă&#x17D;ĹĽ], and note that Ă&#x17D;Ĺ&sbquo;t (i) can be efÄ? [sent-156, score-0.211]
</p><p>33 Given an observation sequence, the empirical visit rate, v(i), T 1 of a state qi , is the fraction of time the HMM spends in state qi , that is v(i) T t=1 Ă&#x17D;Ĺ&sbquo;t (i). [sent-161, score-1.037]
</p><p>34 Given an observation sequence, the empirical risk, r(i), of a T 1 t=1 state qi , is the rate of erroneous visits to qi , that is r(i) v(i)T Ă&#x17D;Ĺ&sbquo;t (i). [sent-165, score-0.737]
</p><p>35 To achieve this we apply the following greedy selection procedure of rejective states whereby highest risk states are sequentially selected as long as their overall visit rate does not exceed B. [sent-170, score-0.936]
</p><p>36 If our model does not include a large number of states, or includes states with very high visit rates (as it is often the case in applications), the total visit rate of the rejective states might be far from the requested bound 3  B, entailing that selectivity cannot be fully exploited. [sent-190, score-1.063]
</p><p>37 Let q be the non-rejective state with the highest risk rate. [sent-198, score-0.333]
</p><p>38 The probability to reject predictions emerging 1 from this state is taken to be pq q Ă˘&circ;&circ;RS v(q ) . [sent-199, score-0.358]
</p><p>39 Ĺš ned, the v(q) B Ă˘&circ;&rsquo; total expected rejection rate is precisely B, when expectation is taken over random choices. [sent-201, score-0.238]
</p><p>40 Ĺš nement approach is to construct an approximate HMM whose states have Ä? [sent-206, score-0.244]
</p><p>41 This smaller granularity enables a selection of rejective states whose total visit rate is closer to the required bound. [sent-208, score-0.624]
</p><p>42 Ĺš nement is achieved by replacing every highly visited state with a complete HMM. [sent-210, score-0.241]
</p><p>43 In Ă&#x17D;ĹĽ0 , states that have visit rate greater than a certain bound are identiÄ? [sent-212, score-0.484]
</p><p>44 For each such state qi (called a heavy state), a new HMM Ă&#x17D;ĹĽi (called a reÄ? [sent-214, score-0.453]
</p><p>45 Ĺš nally, every transition from qi to another state entails transition from a state in Ă&#x17D;ĹĽi whose probability is the original transition probability from qi . [sent-216, score-0.947]
</p><p>46 States of Ă&#x17D;ĹĽi are assigned the label of qi . [sent-217, score-0.236]
</p><p>47 Ĺš nement continues in a recursive manner and terminates when all the heavy states have reÄ? [sent-219, score-0.349]
</p><p>48 Ĺš ned) states, and states 3,5,6,7,8 are leaf (emitting) states. [sent-234, score-0.242]
</p><p>49 Ĺš nes state 1, the model consisting of states 5 and 6 reÄ? [sent-236, score-0.358]
</p><p>50 An aggregate state of the complete hierarchical model corresponds to a set of inner HMM states, each of which is a state on a path from the root through reÄ? [sent-238, score-0.484]
</p><p>51 Transition to the next aggregate state always starts at Ă&#x17D;ĹĽ0 , and recursively progresses to the leaf states, as shown in the following example. [sent-245, score-0.403]
</p><p>52 Suppose that the model in Figure 1 is at aggregate state {1,4,7} at time t. [sent-246, score-0.291]
</p><p>53 The aggregate state at time t + 1 is calculated as follows. [sent-247, score-0.338]
</p><p>54 Ă&#x17D;ĹĽ0 is in state 1, so its next state (say 1 again) is chosen according to the distribution {a11 , a12 }. [sent-248, score-0.324]
</p><p>55 Ĺš nes state 1, which was in state 4 at 4  time t. [sent-250, score-0.355]
</p><p>56 State 3 is a leaf state that emits observations, and the aggregate state at time t + 1, is {1,3}. [sent-252, score-0.53]
</p><p>57 On the other hand, if state 2 is chosen at the root, a new state (say 6) in its reÄ? [sent-253, score-0.324]
</p><p>58 Ĺš ning model is chosen according to the initial distribution {Ä&#x17D;&euro;5 , Ä&#x17D;&euro;6 } (transition into the heavy state from another state). [sent-254, score-0.273]
</p><p>59 The chosen state 6 is a leaf state so the new aggregate state becomes {2,6}. [sent-255, score-0.692]
</p><p>60 qn+N qj 3: Remove state qi with the corresponding {bim }i=M from Ă&#x17D;ĹĽ, and record it as a state reÄ? [sent-263, score-0.678]
</p><p>61 In steps 1-3, a random Ă&#x17D;ĹĽi is generated and connected to the HMM Ă&#x17D;ĹĽ instead of qi . [sent-271, score-0.211]
</p><p>62 The algorithm is applied on heavy states until all states in the HMM have visit rates lower than a required bound. [sent-276, score-0.679]
</p><p>63 2), re-estimation formulas for the parameters of newly added states (Step 8) are presented, where Ă&#x17D;Ĺžt (j, k) = P [qt = j, qt+1 = k | O, Ă&#x17D;ĹĽ]. [sent-286, score-0.203]
</p><p>64 Ĺš nement process, transitions from other states into heavy state qi also affect the initial distribution of its reÄ? [sent-289, score-0.697]
</p><p>65 The most likely aggregate state at time t, given sequence O, is found in a top-down manner using the hierarchical structure of the model. [sent-291, score-0.391]
</p><p>66 Starting with the root model, Ă&#x17D;ĹĽ0 , the most likely individual state in it, say qi , is identiÄ? [sent-292, score-0.406]
</p><p>67 Otherwise, the most likely individual state in Ă&#x17D;ĹĽi (HMM that reÄ? [sent-296, score-0.195]
</p><p>68 Ĺš ed, and the aggregate state is updated to be {qi , qj }. [sent-298, score-0.434]
</p><p>69 The above procedure requires calculation of the quantity Ă&#x17D;Ĺ&sbquo;t (i) not only for the leaf states (where it is calculated using a standard forward-backward procedure), but also for the reÄ? [sent-301, score-0.289]
</p><p>70 Ĺš nes qi  The rejection subset is found using the Eq. [sent-305, score-0.432]
</p><p>71 Visit and risk estimates for the aggregate state {qi1 . [sent-309, score-0.437]
</p><p>72 qik } are calculated using Ă&#x17D;Ĺ&sbquo;t (ik ), of a leaf state qik that identiÄ? [sent-312, score-0.382]
</p><p>73 5  The outcome of the RR procedure is a tree of HMMs whose main purpose is to redistribute visit rates among states. [sent-314, score-0.269]
</p><p>74 Ĺš rst glance, they do not address the visit rate re-distribution objective. [sent-318, score-0.29]
</p><p>75 Alternatively, state labels can be calculated from the statistics of the states, if an unsupervised training method is used. [sent-325, score-0.209]
</p><p>76 For a state qi , and given observation label l, we calculate the average number of visits (at qi ) whose corresponding label is l, as E [St = qi | lt = l, O, Ă&#x17D;ĹĽ] = 1Ă˘&permil;Â¤tĂ˘&permil;Â¤T,lt =l Ă&#x17D;Ĺ&sbquo;t (i). [sent-327, score-0.965]
</p><p>77 4  Experimental Results  We compared empirically the four selection mechanisms presented in Section 3, namely, the ambiguity model and the Naive, RLI, and RR sHMMs. [sent-329, score-0.232]
</p><p>78 For our prediction task, we took as observation sequence directions of the S&P500; price changes. [sent-336, score-0.253]
</p><p>79 For the ambiguity model, the partial sequences dtĂ˘&circ;&rsquo; +1 , . [sent-343, score-0.215]
</p><p>80 For the ambiguity model we constructed two 8-state HMMs, where the length of a single observation sequence ( ) is 5. [sent-353, score-0.228]
</p><p>81 Ĺš ning model in the RR procedure had the same structure, and the upper bound on the visit rate was Ä? [sent-356, score-0.35]
</p><p>82 RC curves were computed for each technique by taking the linear grid of rejection rate bounds from 0 to 0. [sent-363, score-0.266]
</p><p>83 1 Coverage Bound  (a) Error rate vs coverage bound  Amb. [sent-406, score-0.346]
</p><p>84 131  (b) Coverage rate vs coverage bound  Figure 2: S&P500; RC-curves Figure 2a shows that all four methods exhibited meaningful RC-curves; namely, the error rates decreased monotonically with decreasing coverage bounds. [sent-443, score-0.642]
</p><p>85 The RLI and RR models (curves 3 and 4, respectively) outperformed the Naive one (curve 2), by better exploiting the allotted coverage bound, as is evident from Table 2b. [sent-444, score-0.269]
</p><p>86 In addition, the RR model outperformed the RLI model, and moreover, its effective coverage is higher for every required coverage bound. [sent-445, score-0.538]
</p><p>87 Ĺš nes a state and the resulting sub-states have different risk rates, the selection procedure will tend to reject riskier states Ä? [sent-449, score-0.654]
</p><p>88 Comparing the state-based models (curves 2-4) to the ambiguity model (curve 1), we see that all the state-based models outperformed the ambiguity model through the entire coverage range (despite the advantage we provided to the ambiguity model). [sent-451, score-0.698]
</p><p>89 As can be seen in Figure 2a, the selective techniques can also improve the accuracy obtained by these methods (with full coverage). [sent-455, score-0.315]
</p><p>90 5  1  (b) Risk  Figure 3: Distributions of visit and risk train/test differences Figure 3a depicts the distribution of differences between empirical visit rates, measured on the training set, and those rates on the test set. [sent-463, score-0.689]
</p><p>91 This means that our empirical visit estimates are quite robust and useful. [sent-465, score-0.242]
</p><p>92 Ĺš cation was introduced by Chow [5], who took a Bayesian route to infer the optimal rejection rule and analyze the risk-coverage trade-off under complete knowledge of the underlying probabilistic source. [sent-471, score-0.216]
</p><p>93 There is a substantial volume of research contributions on selective classiÄ? [sent-475, score-0.315]
</p><p>94 Therefore, this technique falls within selective prediction but the selection function has been manually predeÄ? [sent-511, score-0.474]
</p><p>95 6  Concluding Remarks  The structure and modularity of HMMs make them particularly convenient for incorporating selective prediction mechanisms. [sent-513, score-0.42]
</p><p>96 We focused on selective prediction of trends in Ä? [sent-515, score-0.468]
</p><p>97 Ĺš cult prediction tasks our models can provide non-trivial prediction improvements. [sent-518, score-0.21]
</p><p>98 We expect that the relative advantage of these selective prediction techniques will be higher in easier tasks, or even in the same task by utilizing more elaborate HMM modeling, perhaps including other sources of specialized information including prices of other correlated indices. [sent-519, score-0.467]
</p><p>99 We believe that a major bottleneck in attaining smaller test errors is the noisy risk estimates we obtain for the hidden states (see Figure 3b). [sent-520, score-0.394]
</p><p>100 Finally, it will be very interesting to examine selective prediction mechanisms in the more general context of Bayesian networks and other types of graphical models. [sent-523, score-0.483]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmm', 0.386), ('selective', 0.315), ('coverage', 0.269), ('visit', 0.242), ('qi', 0.211), ('rejection', 0.19), ('states', 0.165), ('hmms', 0.164), ('state', 0.162), ('rc', 0.154), ('risk', 0.146), ('qj', 0.143), ('ambiguity', 0.143), ('aggregate', 0.129), ('reject', 0.124), ('chow', 0.123), ('rli', 0.122), ('rejective', 0.119), ('nancial', 0.107), ('prediction', 0.105), ('re', 0.1), ('hijk', 0.099), ('onml', 0.099), ('rr', 0.094), ('ajk', 0.087), ('hidden', 0.083), ('heavy', 0.08), ('shmm', 0.079), ('nement', 0.079), ('leaf', 0.077), ('sequences', 0.072), ('coarseness', 0.07), ('markov', 0.067), ('transition', 0.067), ('classi', 0.064), ('mechanisms', 0.063), ('bjm', 0.06), ('shmms', 0.06), ('ot', 0.059), ('curve', 0.058), ('qn', 0.052), ('kk', 0.052), ('abstain', 0.052), ('observation', 0.049), ('trends', 0.048), ('qik', 0.048), ('rate', 0.048), ('day', 0.048), ('calculated', 0.047), ('predictions', 0.047), ('specialized', 0.047), ('wf', 0.045), ('hypothesized', 0.045), ('er', 0.042), ('st', 0.041), ('lt', 0.041), ('lqi', 0.04), ('dt', 0.039), ('formulas', 0.038), ('instances', 0.038), ('mechanism', 0.037), ('price', 0.037), ('sequence', 0.036), ('recursively', 0.035), ('emit', 0.035), ('likely', 0.033), ('achieving', 0.033), ('depicts', 0.032), ('aij', 0.032), ('rao', 0.032), ('rabiner', 0.032), ('hong', 0.032), ('trained', 0.032), ('hierarchical', 0.031), ('ning', 0.031), ('nes', 0.031), ('visits', 0.03), ('baum', 0.03), ('bound', 0.029), ('naive', 0.029), ('technique', 0.028), ('compromise', 0.027), ('wp', 0.027), ('rates', 0.027), ('option', 0.027), ('trend', 0.027), ('selection', 0.026), ('erroneous', 0.026), ('siddiqi', 0.026), ('selectivity', 0.026), ('reliable', 0.026), ('took', 0.026), ('pq', 0.025), ('progressively', 0.025), ('recursive', 0.025), ('user', 0.025), ('label', 0.025), ('highest', 0.025), ('granularity', 0.024), ('lq', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="246-tfidf-1" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>2 0.34572953 <a title="246-tfidf-2" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>3 0.17738266 <a title="246-tfidf-3" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>4 0.11160544 <a title="246-tfidf-4" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>Author: Jia Deng, Sanjeev Satheesh, Alexander C. Berg, Fei Li</p><p>Abstract: We present a novel approach to efﬁciently learn a label tree for large scale classiﬁcation with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classiﬁers for each node in the tree. This approach also allows ﬁne grained control over the efﬁciency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classiﬁcation with 10184 classes and 9 million images. We demonstrate signiﬁcant improvements in test accuracy and efﬁciency with less training time and more balanced trees compared to the previous state of the art by Bengio et al. 1</p><p>5 0.095276989 <a title="246-tfidf-5" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>Author: Matthew D. Zeiler, Graham W. Taylor, Leonid Sigal, Iain Matthews, Rob Fergus</p><p>Abstract: We present a type of Temporal Restricted Boltzmann Machine that deﬁnes a probability distribution over an output sequence conditional on an input sequence. It shares the desirable properties of RBMs: efﬁcient exact inference, an exponentially more expressive latent state than HMMs, and the ability to model nonlinear structure and dynamics. We apply our model to a challenging real-world graphics problem: facial expression transfer. Our results demonstrate improved performance over several baselines modeling high-dimensional 2D and 3D data. 1</p><p>6 0.084259652 <a title="246-tfidf-6" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>7 0.078875884 <a title="246-tfidf-7" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>8 0.078256637 <a title="246-tfidf-8" href="./nips-2011-Signal_Estimation_Under_Random_Time-Warpings_and_Nonlinear_Signal_Alignment.html">253 nips-2011-Signal Estimation Under Random Time-Warpings and Nonlinear Signal Alignment</a></p>
<p>9 0.077170089 <a title="246-tfidf-9" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>10 0.075229339 <a title="246-tfidf-10" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>11 0.070800684 <a title="246-tfidf-11" href="./nips-2011-Modelling_Genetic_Variations_using_Fragmentation-Coagulation_Processes.html">173 nips-2011-Modelling Genetic Variations using Fragmentation-Coagulation Processes</a></p>
<p>12 0.067193426 <a title="246-tfidf-12" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>13 0.065912776 <a title="246-tfidf-13" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>14 0.065413728 <a title="246-tfidf-14" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>15 0.063417859 <a title="246-tfidf-15" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>16 0.063040644 <a title="246-tfidf-16" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>17 0.059664577 <a title="246-tfidf-17" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>18 0.058242697 <a title="246-tfidf-18" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>19 0.0559544 <a title="246-tfidf-19" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>20 0.055676598 <a title="246-tfidf-20" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.194), (1, -0.032), (2, 0.014), (3, 0.028), (4, -0.031), (5, -0.027), (6, 0.001), (7, -0.128), (8, -0.086), (9, -0.068), (10, -0.08), (11, -0.067), (12, 0.099), (13, 0.007), (14, -0.061), (15, -0.121), (16, -0.072), (17, -0.053), (18, 0.09), (19, 0.045), (20, 0.196), (21, 0.016), (22, 0.011), (23, 0.058), (24, 0.021), (25, 0.208), (26, -0.154), (27, -0.022), (28, 0.19), (29, -0.067), (30, 0.043), (31, 0.05), (32, 0.098), (33, 0.024), (34, -0.107), (35, -0.123), (36, -0.167), (37, -0.148), (38, -0.031), (39, 0.124), (40, 0.164), (41, 0.022), (42, -0.013), (43, 0.036), (44, -0.12), (45, -0.058), (46, -0.017), (47, -0.074), (48, 0.043), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96511543 <a title="246-lsi-1" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>2 0.77669525 <a title="246-lsi-2" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>3 0.5616408 <a title="246-lsi-3" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>Author: Armen Allahverdyan, Aram Galstyan</p><p>Abstract: We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only ﬁnite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam’s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. 1</p><p>4 0.50684619 <a title="246-lsi-4" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study multi-label prediction for structured output sets, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multilabel classiﬁcation techniques are typically not applicable in this situation, because they require explicit enumeration of the label set, which is infeasible in case of structured outputs. Relying on techniques originally designed for single-label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems. In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneﬁcial properties with single-label maximum-margin approaches, in particular formulation as a convex optimization problem, efﬁcient working set training, and PAC-Bayesian generalization bounds. 1</p><p>5 0.48784822 <a title="246-lsi-5" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>Author: Tianshi Gao, Daphne Koller</p><p>Abstract: Modern classiﬁcation tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classiﬁers, which are then applied at test time and then pieced together in a ﬁxed procedure determined in advance or at training time. We present an active classiﬁcation process at the test time, where each classiﬁer in a large ensemble is viewed as a potential observation that might inform our classiﬁcation process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classiﬁcation gain from each observation as well as its computational cost. The expected classiﬁcation gain is computed using a probabilistic model that uses the outcome from previous observations. This active classiﬁcation process is applied at test time for each individual test instance, resulting in an efﬁcient instance-speciﬁc decision path. We demonstrate the beneﬁt of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classiﬁcation accuracy at a fraction of the computational costs of traditional methods.</p><p>6 0.45042482 <a title="246-lsi-6" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>7 0.44913056 <a title="246-lsi-7" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>8 0.44248122 <a title="246-lsi-8" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>9 0.41187108 <a title="246-lsi-9" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>10 0.40619203 <a title="246-lsi-10" href="./nips-2011-A_Model_for_Temporal_Dependencies_in_Event_Streams.html">8 nips-2011-A Model for Temporal Dependencies in Event Streams</a></p>
<p>11 0.40094942 <a title="246-lsi-11" href="./nips-2011-Modelling_Genetic_Variations_using_Fragmentation-Coagulation_Processes.html">173 nips-2011-Modelling Genetic Variations using Fragmentation-Coagulation Processes</a></p>
<p>12 0.39349401 <a title="246-lsi-12" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>13 0.39084911 <a title="246-lsi-13" href="./nips-2011-Collective_Graphical_Models.html">55 nips-2011-Collective Graphical Models</a></p>
<p>14 0.37734428 <a title="246-lsi-14" href="./nips-2011-An_Exact_Algorithm_for_F-Measure_Maximization.html">33 nips-2011-An Exact Algorithm for F-Measure Maximization</a></p>
<p>15 0.36977458 <a title="246-lsi-15" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>16 0.3645753 <a title="246-lsi-16" href="./nips-2011-Learning_Patient-Specific_Cancer_Survival_Distributions_as_a_Sequence_of_Dependent_Regressors.html">147 nips-2011-Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors</a></p>
<p>17 0.36289424 <a title="246-lsi-17" href="./nips-2011-Composite_Multiclass_Losses.html">59 nips-2011-Composite Multiclass Losses</a></p>
<p>18 0.35364822 <a title="246-lsi-18" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>19 0.34652776 <a title="246-lsi-19" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>20 0.34553495 <a title="246-lsi-20" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.043), (2, 0.119), (4, 0.035), (20, 0.024), (26, 0.023), (31, 0.184), (33, 0.044), (43, 0.051), (45, 0.194), (57, 0.032), (65, 0.018), (74, 0.049), (83, 0.041), (84, 0.021), (99, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94263327 <a title="246-lda-1" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>2 0.93287706 <a title="246-lda-2" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>3 0.91114414 <a title="246-lda-3" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>Author: Adrian Ion, Joao Carreira, Cristian Sminchisescu</p><p>Abstract: We present a joint image segmentation and labeling model (JSL) which, given a bag of ﬁgure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as ﬁrst sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect conﬁgurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.</p><p>4 0.90383065 <a title="246-lda-4" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>Author: Dan Feldman, Matthew Faulkner, Andreas Krause</p><p>Abstract: How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models ﬁtting the coreset will also provide a good ﬁt for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3 /ε2 ) data points sufﬁces for computing a (1 + ε)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efﬁciently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones. 1</p><p>5 0.89828873 <a title="246-lda-5" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>Author: J. Z. Kolter</p><p>Abstract: Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling. It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation. In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well. In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case. Furthermore, we show that we can efﬁciently project on to this convex set using only samples generated from the system. The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods. 1</p><p>6 0.88899362 <a title="246-lda-6" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>7 0.88733369 <a title="246-lda-7" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>8 0.88593477 <a title="246-lda-8" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>9 0.88534796 <a title="246-lda-9" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>10 0.88499808 <a title="246-lda-10" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>11 0.88168621 <a title="246-lda-11" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>12 0.88040197 <a title="246-lda-12" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>13 0.87949759 <a title="246-lda-13" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>14 0.8769654 <a title="246-lda-14" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>15 0.87655991 <a title="246-lda-15" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>16 0.87646711 <a title="246-lda-16" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>17 0.87560034 <a title="246-lda-17" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>18 0.87079924 <a title="246-lda-18" href="./nips-2011-Complexity_of_Inference_in_Latent_Dirichlet_Allocation.html">58 nips-2011-Complexity of Inference in Latent Dirichlet Allocation</a></p>
<p>19 0.87033522 <a title="246-lda-19" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>20 0.86850023 <a title="246-lda-20" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
