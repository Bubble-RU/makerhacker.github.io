<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 nips-2011-An ideal observer model for identifying the reference frame of objects</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-35" href="#">nips2011-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 nips-2011-An ideal observer model for identifying the reference frame of objects</h1>
<br/><p>Source: <a title="nips-2011-35-pdf" href="http://papers.nips.cc/paper/4354-an-ideal-observer-model-for-identifying-the-reference-frame-of-objects.pdf">pdf</a></p><p>Author: Joseph L. Austerweil, Abram L. Friesen, Thomas L. Griffiths</p><p>Abstract: The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols × and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conﬂicting reference frames, the model predicts two factors should inﬂuence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. 1</p><p>Reference: <a title="nips-2011-35-reference" href="../nips2011_reference/nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An ideal observer model for identifying the reference frame of objects  Joseph L. [sent-1, score-1.323]
</p><p>2 edu  Abstract The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). [sent-9, score-1.341]
</p><p>3 Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. [sent-11, score-1.524]
</p><p>4 We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. [sent-12, score-1.011]
</p><p>5 We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. [sent-14, score-1.339]
</p><p>6 1 Although people recognize and categorize objects successfully and effortlessly, object recognition in machine learning is an incredibly difﬁcult problem and people’s success is a puzzle to cognitive scientists. [sent-16, score-0.422]
</p><p>7 The general goal of these methods is to extract features from images that are useful for identifying the objects that generated the images after whatever transformations occurred while producing them (e. [sent-22, score-0.374]
</p><p>8 This is a sensible strategy given that people typically perceive the same object even when it is transformed in its image (e. [sent-25, score-0.374]
</p><p>9 However, not all transformations should be ignored: The perceived identity of some objects depends on the orientation of its features with respect to the scene it is in (e. [sent-28, score-0.563]
</p><p>10 An image is a part of the visual input that is generated by a single object, which is ambiguous as two or more objects could generate the same image. [sent-34, score-0.426]
</p><p>11 Developing proper object recognition and fully understanding how people do it depends on explaining how people determine the orientation of objects with respect to the scene they are in. [sent-37, score-0.9]
</p><p>12 The importance of orientation for object recognition leads us to the following question: If two objects project to the same image under different viewing conditions (e. [sent-38, score-0.563]
</p><p>13 The coordinate axes set the orientation and scale of the objects, and thus + and × can be identiﬁed as different objects. [sent-42, score-0.369]
</p><p>14 In some situations the orientation of an image’s reference frame is simply the orientation of the retina; however, this is not the case when we rotate our heads (as our retinal image rotates) or look at a rotated object (e. [sent-44, score-1.784]
</p><p>15 Thus, the reference frame of an image is ambiguous without additional information. [sent-47, score-1.382]
</p><p>16 However, if there is another object in the scene whose orientation is unambiguous (like a 5), then the orientation of the ambiguous image can be inferred. [sent-48, score-1.004]
</p><p>17 The solution people adopt is indicative of the reference frame they inferred for the operator (multiplication implies an upright reference frame and addition implies a diagonal reference frame). [sent-50, score-3.215]
</p><p>18 This is a novel experimental method that allows us to explore reference frame inference in a wide range of contexts. [sent-51, score-1.125]
</p><p>19 In real life, we typically view scenes with multiple reference frames. [sent-52, score-0.698]
</p><p>20 Yet there has been little work investigating how people infer the number of reference frames, their orientations, and which images belong to each reference frame. [sent-54, score-1.538]
</p><p>21 To solve this problem, we note that each image in a scene belongs to a single reference frame, and thus reference frames form a partition of the images in a scene (where each block in the partition corresponds to a reference frame). [sent-55, score-2.532]
</p><p>22 Using a standard nonparametric Bayesian model for partitions, we formulate an ideal observer model to infer multiple reference frames and their parameters. [sent-56, score-0.908]
</p><p>23 The model predicts that people should be sensitive to two cues when inferring the reference frames of a scene: the proximity of the ambiguous image to two unambiguous ﬂanking images in conﬂicting orientations, and the difference in the number of objects aligned in the competing reference frames. [sent-57, score-2.384]
</p><p>24 First, Section 2 summarizes relevant psychological research on how orientation affects the objects perceived in ambiguous images. [sent-60, score-0.594]
</p><p>25 Next, Section 3 develops a novel method for online testing of the reference frame people infer for an image and establishes its efﬁcacy. [sent-61, score-1.417]
</p><p>26 Section 4 presents an ideal observer model for reference frame inference in scenes with multiple reference frames. [sent-62, score-1.908]
</p><p>27 The model predicts that the ambiguous image’s proximity to other reference frames should affect the inferred reference frame and Section 5 conﬁrms that people act in accordance with this prediction in a behavioral experiment. [sent-63, score-2.398]
</p><p>28 The model also predicts that the number of aligned objects in a reference frame should affect the reference frame inferred for an ambiguous image. [sent-64, score-2.703]
</p><p>29 2  Orientation in psychological theories of object representation  Though the perceived object of some images does not depend on its orientation (like a 5), there are many examples where the perceived object does depend on its orientation [7, 8], including + vs. [sent-67, score-0.898]
</p><p>30 This has led psychologists to believe that people represent objects within a reference frame (a set of coordinate axes). [sent-70, score-1.438]
</p><p>31 3 Figure 1 (a) shows that reference frames predict the image + is interpreted as a + when 2 3  We view the ambiguity of a reference frame as essentially the same as the strength of the intrinsic axes [6]. [sent-71, score-2.117]
</p><p>32 (a) The ambiguity of the + image can be resolved using reference frames: a + with horizontal orientation (solid axes) or a × rotated 45 degrees (dashed axes). [sent-76, score-1.038]
</p><p>33 (c) The reference frame of ambiguous objects is inﬂuenced by objects with unambiguous reference frames. [sent-78, score-2.249]
</p><p>34 the coordinate axes are aligned with the document’s axes and as × when the coordinate axes are diagonal to the document’s axes. [sent-81, score-0.548]
</p><p>35 For objects that are rotationally invariant, there is only one object that generates the observed image and so it is identiﬁable in any orientation (see Figure 1 (b)). [sent-82, score-0.542]
</p><p>36 The dependence of object perception on orientation is a well established norm and has been demonstrated with novel and familiar 2-D objects, faces, handwriting [8, 9], and 3-D objects [10, 11]. [sent-83, score-0.48]
</p><p>37 Central to the reference frame hypothesis is the ability of our perceptual system to infer a reference frame for a given image. [sent-84, score-2.257]
</p><p>38 As more than one reference frame may be consistent with an observed image, psychologists have explored how people infer the appropriate reference frame for an image. [sent-85, score-2.414]
</p><p>39 Though reference frame inference is strongly inﬂuenced by the top-down axis of the retinal image and by the axis of gravity (given by our proprioceptive and vestibular senses) [8], the scene itself can inﬂuence the inferred reference frame. [sent-86, score-2.086]
</p><p>40 , the text on a poster as the poster is rotated), and so it is sensible that the inferred reference frame for an ambiguous image is inﬂuenced by the orientations of the images surrounding it. [sent-89, score-1.656]
</p><p>41 Figures 1 (c) and (d) are phenomenological demonstrations of how the alignment of the orientations of other objects in a scene can bias the inferred reference frame for an image whose reference frame is ambiguous (and there is strong corroborating empirical evidence for this principle [12, 13]). [sent-90, score-2.932]
</p><p>42 Thus, there is one reference frame shared by all the objects in a group. [sent-92, score-1.238]
</p><p>43 Before exploring what cues inﬂuence human reference frame inference in scenes with multiple reference frames, we develop a novel method for testing human reference frame inference. [sent-94, score-3.035]
</p><p>44 3  Testing reference frame inference using arithmetic  To test how different factors inﬂuence the reference frame people infer for an image, we ask people to solve an arithmetic problem without specifying the appropriate operation. [sent-95, score-2.813]
</p><p>45 If people view × and their response is the multiplication answer, then their reference frame for × is aligned with the horizontal and vertical axes of the page. [sent-96, score-1.482]
</p><p>46 Alternatively, if people view the same ×, but their response is the addition answer, then their reference frame for × is aligned with the axes diagonal to the page (and thus, relative to its own reference frame, it is treated as +). [sent-97, score-2.107]
</p><p>47 5 Although we use + and × as the ambiguous images, this method works with any ambiguous images by teaching the participant to use addition in one orientation of the image and multiplication in the other. [sent-101, score-0.868]
</p><p>48 3  (a)  (b)  Axis Oriented  (c)  (d)  Diagonal Oriented  Axis Oriented  +  +  5  5  5  0  Diagonal Oriented 10 Frequency  5  5  Frequency  10  10 25 Response  5  0  10 25 Response  Figure 2: Effect of the orientations of other objects in the same reference frame. [sent-102, score-0.835]
</p><p>49 Most participants respond with 25, the solution to the product of 5 and 5, meaning their reference frame is aligned with the axes of the page. [sent-106, score-1.471]
</p><p>50 Most participants respond with 10, meaning their reference frame is aligned with the diagonals of the page. [sent-108, score-1.345]
</p><p>51 frame inference on a seemingly unrelated cognitive behavior (solving an arithmetic problem). [sent-109, score-0.632]
</p><p>52 We conﬁrm its validity by reproducing a previously found effect – the inﬂuence of orientation on other images in the scene [12]. [sent-110, score-0.456]
</p><p>53 When the reference frame for an image is ambiguous, one factor that inﬂuences the inferred reference frame is the orientation of other images it is grouped with, especially when those images are identiﬁable in any orientation. [sent-111, score-2.812]
</p><p>54 Thus, if we ask people to solve an arithmetic problem, where the operator × is paired with the numbers 5 aligned with the top-down axes of the page (Figure 2 (a)), they should respond 25, the result of multiplication. [sent-112, score-0.671]
</p><p>55 Alternatively, if people solve the same problem except the numbers 5 are aligned diagonally, they should infer the diagonal axes to be the reference frame and respond 10, the result of addition (Figure 2 (b)). [sent-113, score-1.615]
</p><p>56 The participants were counterbalanced over the axis or diagonally oriented conditions (Figures 2 (a) and (b) respectively) and all participants gave either the addition (10) or multiplication (25) solution. [sent-115, score-0.445]
</p><p>57 By changing the orientation of the numbers, the solutions to the arithmetic problems given by participants in Figures 2 (a) and (b) are different despite having identical numbers and the identical operator image. [sent-116, score-0.629]
</p><p>58 Thus, asking participants to solve arithmetic problems is an effective method for testing reference frame inference and perceived orientations can inﬂuence higher level cognition. [sent-120, score-1.558]
</p><p>59 4  Modeling reference frame inference  Before describing our model of reference frame inference with multiple reference frames, we ﬁrst present a probabilistic model for scenes of multiple images with only a single reference frame. [sent-121, score-3.671]
</p><p>60 1  Reference frame inference for scenes with one reference frame  We assume that a vocabulary of possible objects is known ahead of time of size V and that there are R possible rotations. [sent-123, score-1.827]
</p><p>61 For each image i in a scene, the model is given its visual properties yi and its spatial location xi = (xi1 , xi2 ) The visual properties of the image yi are generated by an unknown object vi rotated by r, the orientation of the scene’s reference frame. [sent-129, score-1.254]
</p><p>62 A V × R binary image-object alignment matrix A(i) encodes the object-rotation pairs consistent with the observed image yi such that A(i) (v, r) = 1 if the image of object v rotated r degrees is consistent with yi . [sent-130, score-0.463]
</p><p>63 The model assumes that the spatial locations of the images are independent identically distributed draws from a Gaussian distribution with shared parameters µ, the center point for the reference frame, and Σ, the spread of objects around its center point. [sent-131, score-0.885]
</p><p>64 The unobserved objects and the orientation of the reference frame r are drawn from independent discrete distributions 4  with parameters φ and θ, the prior over objects and reference frame orientations, respectively. [sent-132, score-2.692]
</p><p>65 We extend the model by partitioning the images of a scene into reference frames, where each image of the scene belongs to exactly one reference frame and a reference frame is a block of the partition. [sent-141, score-3.284]
</p><p>66 From this perspective, inferring multiple reference frames for a scene of images is equivalent to partitioning the scene or clustering the images. [sent-142, score-1.166]
</p><p>67 With the insight that grouping images into reference frames is like ﬁnding a partition of a scene, we can extend our model to select the reference frames of a scene (with an unknown number of reference frames). [sent-143, score-2.425]
</p><p>68 , ci−1 ) = α k =K +1 α+i−1 where K is the current number of reference frames and nk is the number of objects assigned to reference frame k. [sent-148, score-2.043]
</p><p>69 ci denotes the reference frame that object i is assigned to and if ci = K + 1, it is assigned a new reference frame containing none of the previous objects and K increments by one (to initialize, the ﬁrst object starts its own reference frame and K = 1). [sent-149, score-3.782]
</p><p>70 This gives us an assignment vector c, where ci = j denotes reference frame j contains image i. [sent-150, score-1.256]
</p><p>71 We begin by assigning each image to its own reference frame and then iterating. [sent-154, score-1.196]
</p><p>72 3  Predictions for human reference frame inference  What factors inﬂuence the reference frame assigned to an ambiguous image according to our ideal observer model? [sent-158, score-2.668]
</p><p>73 proximity or how close the image is to unambiguous images (as images in the same reference frame are coupled in spatial location) and alignment or the difference in the number of images assigned to each reference frame. [sent-171, score-2.344]
</p><p>74 It is clear that the two numbers should have their own reference frame, but it is ambiguous which reference frame the operator should be assigned to. [sent-173, score-2.099]
</p><p>75 We compare how each of these factors inﬂuences the reference frames inferred in the scene by people and our model in two behavioral experiments. [sent-174, score-1.155]
</p><p>76 We explore this question using the method presented above, where participants are asked to solve an arithmetic problem where the operator is ambiguous between + or × and the two numbers have conﬂicting reference frames (orientations). [sent-176, score-1.392]
</p><p>77 This allows us to deduce the reference frame inferred for the operator image from the answer given by participants. [sent-177, score-1.397]
</p><p>78 Four participants did not give a correct solution to the arithmetic problem (neither the addition nor multiplication solution) leaving 130 participants for analysis. [sent-182, score-0.412]
</p><p>79 All factors were manipulated between subjects as preliminary testing demonstrated a strong effect of trial order on the selected reference frame (probably because reference frames rarely change in the world). [sent-184, score-1.919]
</p><p>80 6  The primary factor of interest of the experiment was the position of the operator scored from -2 (far to the left) to 2 (far to the right), which was counterbalanced over participants (without the 0 position). [sent-185, score-0.367]
</p><p>81 The problem was viewed through a simulated aperture (to minimize the effect of the monitor’s reference frame). [sent-186, score-0.617]
</p><p>82 2  Results and Discussion  Figure 4 (a) shows that participants are more likely to infer the orientation of the left number for the operator the closer it is to the left number. [sent-190, score-0.603]
</p><p>83 The results conﬁrm our hypothesis: the closer the operator is to an image with an unambiguous reference frame, the more likely participants are to infer that reference frame for the operator (χ2 (1) = 3. [sent-191, score-2.359]
</p><p>84 This left 150 samples that formed our estimate for the proportion of times the operator grouped with the left reference frame. [sent-199, score-0.835]
</p><p>85 The model and human results clearly exhibit the same qualitative behavior: As the distance between the operator and the left number decreased, the probability the operator took the orientation of the left number increased. [sent-206, score-0.55]
</p><p>86 6  Experiment 2: Alignment effects on reference frame inference  Our model also predicts that the difference in the number of unambiguous images assigned to the conﬂicting reference frames should affect the reference frame adopted by the operator image. [sent-207, score-3.414]
</p><p>87 In this experiment, we test the prediction using the same method as above, but manipulate the number of extra oriented unambiguous objects in each of the competing reference frames (see Figure 5 (a)). [sent-208, score-1.053]
</p><p>88 2  Results and Discussion  Figure 5 (b) shows that participants were more likely to infer the operator’s orientation to be the orientation of whichever side had more objects and it was closer to, replicating the effect of Experiment 1 (χ2 (1) = 12. [sent-216, score-0.784]
</p><p>89 7  Conclusions and future directions  In this paper, we introduced the ﬁrst study of how people infer the reference frame of images in scenes with multiple reference frames. [sent-220, score-2.099]
</p><p>90 We presented an implicit method for testing reference frame inference, an ideal observer model that predicts people should be sensitive to two scene cues, and 7  (b)  1 Percent grouped left  (a) 5  5L1R +  5  Alignment effects on participant responses 5L1R 1L5R  0. [sent-221, score-1.688]
</p><p>91 5L1R denotes ﬁve objects in the left reference frame and one object in the right, and 1L5R indicates the opposite arrangement. [sent-231, score-1.345]
</p><p>92 Because the objects people perceive depend on the orientation of their images in the scene, these results improve our understanding of how the conﬁguration of objects in scenes affects object perception. [sent-234, score-0.96]
</p><p>93 We can capture the elongation cue (that the orientation of the spread of images in a scene biases the orientation of the reference frame of the images in the scene [5]) by coupling the covariance matrix (Σ) and rotation (r) of a reference frame. [sent-237, score-2.672]
</p><p>94 Currently, our model assumes the positions of images in a reference frame are Gaussian distributed; however, people have strong expectations about the arrangement of images in a scene [18]. [sent-238, score-1.594]
</p><p>95 We are also interested in cues that depend on the structure of the images or the orientation of the agent in the world, like axes of symmetry [5] or gravitational axes [8]. [sent-240, score-0.646]
</p><p>96 Another direction for future work is to address an assumption of the model: How do people learn the set of objects and whether or not those objects are orientation-invariant? [sent-241, score-0.433]
</p><p>97 Hopefully, incorporating our model into this feature learning method will yield better inferred features and, in turn, will help create better feature generation and object recognition techniques by providing better understanding of how people perceive objects from ambiguous image data. [sent-243, score-0.771]
</p><p>98 Our paradigm provides a principled starting point for investigating how reference frames are identiﬁed in scenes with multiple reference frames. [sent-245, score-1.474]
</p><p>99 It is easily extended to more complex scenes by associating different orientations (or rotations in depth) of an ambiguous image with different arithmetic operators. [sent-246, score-0.576]
</p><p>100 Our hope is that this leads to a better understanding of object identiﬁcation and reference frame identiﬁcation. [sent-247, score-1.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reference', 0.617), ('frame', 0.48), ('orientation', 0.216), ('ambiguous', 0.186), ('frames', 0.159), ('people', 0.151), ('participants', 0.142), ('objects', 0.141), ('scene', 0.134), ('operator', 0.134), ('axes', 0.126), ('images', 0.106), ('arithmetic', 0.101), ('image', 0.099), ('object', 0.086), ('aligned', 0.081), ('scenes', 0.081), ('orientations', 0.077), ('rotated', 0.07), ('unambiguous', 0.067), ('proximity', 0.065), ('ci', 0.06), ('observer', 0.056), ('alignment', 0.052), ('oriented', 0.051), ('perceived', 0.051), ('position', 0.049), ('icting', 0.049), ('inferred', 0.049), ('participant', 0.048), ('infer', 0.047), ('cues', 0.044), ('grouped', 0.042), ('perceive', 0.038), ('psychology', 0.037), ('perception', 0.037), ('numbers', 0.036), ('diagonal', 0.035), ('upright', 0.035), ('predicts', 0.035), ('rotations', 0.032), ('axis', 0.031), ('diagonally', 0.031), ('tilted', 0.031), ('ideal', 0.029), ('assigned', 0.029), ('symmetry', 0.028), ('inference', 0.028), ('effects', 0.028), ('multiplication', 0.027), ('books', 0.027), ('coordinate', 0.027), ('vi', 0.027), ('crp', 0.025), ('respond', 0.025), ('uence', 0.025), ('rotation', 0.025), ('con', 0.024), ('percent', 0.024), ('human', 0.024), ('responses', 0.024), ('berkeley', 0.023), ('cognitive', 0.023), ('anking', 0.023), ('austerweil', 0.023), ('rci', 0.023), ('testing', 0.023), ('iid', 0.023), ('factors', 0.023), ('psychologists', 0.022), ('behavioral', 0.022), ('asking', 0.022), ('exchange', 0.022), ('closer', 0.022), ('recognition', 0.021), ('transformations', 0.021), ('spread', 0.021), ('experiment', 0.021), ('left', 0.021), ('poster', 0.021), ('counterbalanced', 0.021), ('giw', 0.021), ('yi', 0.02), ('rj', 0.02), ('ambiguity', 0.019), ('figures', 0.019), ('palmer', 0.019), ('answer', 0.018), ('manipulate', 0.018), ('uenced', 0.018), ('frequency', 0.018), ('identi', 0.018), ('trials', 0.018), ('degrees', 0.017), ('affect', 0.017), ('solve', 0.017), ('inferring', 0.016), ('triangles', 0.016), ('hypothesis', 0.016), ('partition', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="35-tfidf-1" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>Author: Joseph L. Austerweil, Abram L. Friesen, Thomas L. Griffiths</p><p>Abstract: The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols × and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conﬂicting reference frames, the model predicts two factors should inﬂuence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. 1</p><p>2 0.22303148 <a title="35-tfidf-2" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>Author: Carl Vondrick, Deva Ramanan</p><p>Abstract: We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efﬁcient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost. 1</p><p>3 0.17434628 <a title="35-tfidf-3" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>4 0.16705956 <a title="35-tfidf-4" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>Author: Kevin G. Jamieson, Robert Nowak</p><p>Abstract: This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects). In general, the ranking of n objects can be identiﬁed by standard sorting methods using n log2 n pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. Speciﬁcally, we assume that the objects can be embedded into a d-dimensional Euclidean space and that the rankings reﬂect their relative distances from a common reference point in Rd . We show that under this assumption the number of possible rankings grows like n2d and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than d log n adaptively selected pairwise comparisons, on average. If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis. 1</p><p>5 0.10854119 <a title="35-tfidf-5" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>Author: Kamil A. Wnuk, Stefano Soatto</p><p>Abstract: We propose a robust ﬁltering approach based on semi-supervised and multiple instance learning (MIL). We assume that the posterior density would be unimodal if not for the eﬀect of outliers that we do not wish to explicitly model. Therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior. Our approach can be thought of as a combination of standard ﬁnite-dimensional ﬁltering (Extended Kalman Filter, or Unscented Filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements. We show how both the state (regression) and the inlier set (classiﬁcation) can be estimated iteratively and causally by processing only the current measurement. We illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set). 1</p><p>6 0.10757392 <a title="35-tfidf-6" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>7 0.098982818 <a title="35-tfidf-7" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>8 0.0984843 <a title="35-tfidf-8" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>9 0.094200745 <a title="35-tfidf-9" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>10 0.092906147 <a title="35-tfidf-10" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>11 0.08684051 <a title="35-tfidf-11" href="./nips-2011-A_rational_model_of_causal_inference_with_continuous_causes.html">15 nips-2011-A rational model of causal inference with continuous causes</a></p>
<p>12 0.081300408 <a title="35-tfidf-12" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>13 0.080477856 <a title="35-tfidf-13" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>14 0.078485921 <a title="35-tfidf-14" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>15 0.077206604 <a title="35-tfidf-15" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>16 0.074612506 <a title="35-tfidf-16" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>17 0.070574626 <a title="35-tfidf-17" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>18 0.066191733 <a title="35-tfidf-18" href="./nips-2011-Learning_Probabilistic_Non-Linear_Latent_Variable_Models_for_Tracking_Complex_Activities.html">148 nips-2011-Learning Probabilistic Non-Linear Latent Variable Models for Tracking Complex Activities</a></p>
<p>19 0.063421845 <a title="35-tfidf-19" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>20 0.062717527 <a title="35-tfidf-20" href="./nips-2011-An_Unsupervised_Decontamination_Procedure_For_Improving_The_Reliability_Of_Human_Judgments.html">34 nips-2011-An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.115), (2, -0.072), (3, 0.16), (4, 0.074), (5, 0.035), (6, -0.01), (7, -0.057), (8, 0.089), (9, 0.117), (10, 0.05), (11, -0.065), (12, 0.022), (13, -0.024), (14, 0.245), (15, -0.081), (16, 0.101), (17, 0.084), (18, 0.006), (19, 0.06), (20, 0.01), (21, -0.018), (22, -0.076), (23, 0.098), (24, -0.109), (25, -0.11), (26, -0.076), (27, 0.051), (28, -0.033), (29, -0.006), (30, -0.072), (31, 0.05), (32, 0.057), (33, 0.032), (34, 0.012), (35, 0.09), (36, -0.111), (37, 0.022), (38, 0.026), (39, 0.023), (40, -0.07), (41, 0.085), (42, -0.011), (43, -0.033), (44, 0.016), (45, -0.022), (46, 0.039), (47, -0.029), (48, 0.072), (49, 0.102)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97643209 <a title="35-lsi-1" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>Author: Joseph L. Austerweil, Abram L. Friesen, Thomas L. Griffiths</p><p>Abstract: The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols × and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conﬂicting reference frames, the model predicts two factors should inﬂuence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. 1</p><p>2 0.69982207 <a title="35-lsi-2" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>Author: Carl Vondrick, Deva Ramanan</p><p>Abstract: We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efﬁcient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost. 1</p><p>3 0.68068099 <a title="35-lsi-3" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>4 0.57398891 <a title="35-lsi-4" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>Author: Joel Z. Leibo, Jim Mutch, Tomaso Poggio</p><p>Abstract: Many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes. Recent electrophysiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpointspeciﬁc cells projecting to downstream viewpoint-invariant identity-speciﬁc cells [1]. A separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-speciﬁc. In particular, the 2D images evoked by a face undergoing a 3D rotation are not produced by the same image transformation (2D) that would produce the images evoked by an object of another class undergoing the same 3D rotation. However, within the class of faces, knowledge of the image transformation evoked by 3D rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint. We show, through computational simulations, that an architecture which applies this method of gaining invariance to class-speciﬁc transformations is effective when restricted to faces and fails spectacularly when applied to other object classes. We argue here that in order to accomplish viewpoint-invariant face identiﬁcation from a single example view, visual cortex must separate the circuitry involved in discounting 3D rotations of faces from the generic circuitry involved in processing other objects. The resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network. 1</p><p>5 0.54598475 <a title="35-lsi-5" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>Author: Kevin G. Jamieson, Robert Nowak</p><p>Abstract: This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects). In general, the ranking of n objects can be identiﬁed by standard sorting methods using n log2 n pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. Speciﬁcally, we assume that the objects can be embedded into a d-dimensional Euclidean space and that the rankings reﬂect their relative distances from a common reference point in Rd . We show that under this assumption the number of possible rankings grows like n2d and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than d log n adaptively selected pairwise comparisons, on average. If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis. 1</p><p>6 0.51628476 <a title="35-lsi-6" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>7 0.50569475 <a title="35-lsi-7" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>8 0.47151378 <a title="35-lsi-8" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>9 0.46465242 <a title="35-lsi-9" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>10 0.45741367 <a title="35-lsi-10" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>11 0.45141008 <a title="35-lsi-11" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>12 0.43939281 <a title="35-lsi-12" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>13 0.43290442 <a title="35-lsi-13" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>14 0.43122429 <a title="35-lsi-14" href="./nips-2011-Testing_a_Bayesian_Measure_of_Representativeness_Using_a_Large_Image_Database.html">280 nips-2011-Testing a Bayesian Measure of Representativeness Using a Large Image Database</a></p>
<p>15 0.42951867 <a title="35-lsi-15" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>16 0.4226678 <a title="35-lsi-16" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>17 0.41889966 <a title="35-lsi-17" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>18 0.41325346 <a title="35-lsi-18" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>19 0.40211308 <a title="35-lsi-19" href="./nips-2011-How_Do_Humans_Teach%3A_On_Curriculum_Learning_and_Teaching_Dimension.html">122 nips-2011-How Do Humans Teach: On Curriculum Learning and Teaching Dimension</a></p>
<p>20 0.37952128 <a title="35-lsi-20" href="./nips-2011-An_Unsupervised_Decontamination_Procedure_For_Improving_The_Reliability_Of_Human_Judgments.html">34 nips-2011-An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (4, 0.049), (20, 0.043), (26, 0.012), (31, 0.112), (33, 0.049), (43, 0.036), (45, 0.124), (57, 0.044), (65, 0.022), (74, 0.055), (83, 0.03), (84, 0.018), (85, 0.238), (99, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85017449 <a title="35-lda-1" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>Author: Joseph L. Austerweil, Abram L. Friesen, Thomas L. Griffiths</p><p>Abstract: The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols × and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conﬂicting reference frames, the model predicts two factors should inﬂuence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. 1</p><p>2 0.76400405 <a title="35-lda-2" href="./nips-2011-Variance_Penalizing_AdaBoost.html">299 nips-2011-Variance Penalizing AdaBoost</a></p>
<p>Author: Pannagadatta K. Shivaswamy, Tony Jebara</p><p>Abstract: This paper proposes a novel boosting algorithm called VadaBoost which is motivated by recent empirical Bernstein bounds. VadaBoost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss. Each step of the proposed algorithm minimizes the cost efﬁciently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners. Thus, the proposed algorithm solves a key limitation of previous empirical Bernstein boosting methods which required brute force enumeration of all possible weak learners. Experimental results conﬁrm that the new algorithm achieves the performance improvements of EBBoost yet goes beyond decision stumps to handle any weak learner. Signiﬁcant performance gains are obtained over AdaBoost for arbitrary weak learners including decision trees (CART). 1</p><p>3 0.76286572 <a title="35-lda-3" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>Author: Tzu-kuo Huang, Jeff G. Schneider</p><p>Abstract: Vector Auto-regressive models (VAR) are useful tools for analyzing time series data. In quite a few modern time series modelling tasks, the collection of reliable time series turns out to be a major challenge, either due to the slow progression of the dynamic process of interest, or inaccessibility of repetitive measurements of the same dynamic process over time. In those situations, however, we observe that it is often easier to collect a large amount of non-sequence samples, or snapshots of the dynamic process of interest. In this work, we assume a small amount of time series data are available, and propose methods to incorporate non-sequence data into penalized least-square estimation of VAR models. We consider non-sequence data as samples drawn from the stationary distribution of the underlying VAR model, and devise a novel penalization scheme based on the Lyapunov equation concerning the covariance of the stationary distribution. Experiments on synthetic and video data demonstrate the effectiveness of the proposed methods. 1</p><p>4 0.66078794 <a title="35-lda-4" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>Author: Levi Boyles, Anoop Korattikara, Deva Ramanan, Max Welling</p><p>Abstract: Learning problems, such as logistic regression, are typically formulated as pure optimization problems deﬁned on some loss function. We argue that this view ignores the fact that the loss function depends on stochastically generated data which in turn determines an intrinsic scale of precision for statistical estimation. By considering the statistical properties of the update variables used during the optimization (e.g. gradients), we can construct frequentist hypothesis tests to determine the reliability of these updates. We utilize subsets of the data for computing updates, and use the hypothesis tests for determining when the batch-size needs to be increased. This provides computational beneﬁts and avoids overﬁtting by stopping when the batch-size has become equal to size of the full dataset. Moreover, the proposed algorithms depend on a single interpretable parameter – the probability for an update to be in the wrong direction – which is set to a single value across all algorithms and datasets. In this paper, we illustrate these ideas on three L1 regularized coordinate descent algorithms: L1 -regularized L2 -loss SVMs, L1 -regularized logistic regression, and the Lasso, but we emphasize that the underlying methods are much more generally applicable. 1</p><p>5 0.62810189 <a title="35-lda-5" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>Author: Kamil A. Wnuk, Stefano Soatto</p><p>Abstract: We propose a robust ﬁltering approach based on semi-supervised and multiple instance learning (MIL). We assume that the posterior density would be unimodal if not for the eﬀect of outliers that we do not wish to explicitly model. Therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior. Our approach can be thought of as a combination of standard ﬁnite-dimensional ﬁltering (Extended Kalman Filter, or Unscented Filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements. We show how both the state (regression) and the inlier set (classiﬁcation) can be estimated iteratively and causally by processing only the current measurement. We illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set). 1</p><p>6 0.62312919 <a title="35-lda-6" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>7 0.62306261 <a title="35-lda-7" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>8 0.62242597 <a title="35-lda-8" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>9 0.62173426 <a title="35-lda-9" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>10 0.62012202 <a title="35-lda-10" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>11 0.61966658 <a title="35-lda-11" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>12 0.61877191 <a title="35-lda-12" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>13 0.61685055 <a title="35-lda-13" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>14 0.61683166 <a title="35-lda-14" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>15 0.61659181 <a title="35-lda-15" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>16 0.61528713 <a title="35-lda-16" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>17 0.61490571 <a title="35-lda-17" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>18 0.61439353 <a title="35-lda-18" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>19 0.61437494 <a title="35-lda-19" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>20 0.6142717 <a title="35-lda-20" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
