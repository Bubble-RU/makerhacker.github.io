<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-38" href="#">nips2011-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</h1>
<br/><p>Source: <a title="nips-2011-38-pdf" href="http://papers.nips.cc/paper/4279-anatomically-constrained-decoding-of-finger-flexion-from-electrocorticographic-signals.pdf">pdf</a></p><p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>Reference: <a title="nips-2011-38-reference" href="../nips2011_reference/nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. [sent-6, score-0.129]
</p><p>2 Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. [sent-7, score-0.172]
</p><p>3 Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e. [sent-8, score-0.101]
</p><p>4 The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. [sent-11, score-0.189]
</p><p>5 In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. [sent-13, score-0.089]
</p><p>6 Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. [sent-16, score-0.133]
</p><p>7 1  Introduction  Brain computer interfaces (BCIs) allow people to control devices directly using brain signals [19]. [sent-18, score-0.143]
</p><p>8 Because BCI systems directly convert brain signals into commands to control output devices, they can be used by people with severe paralysis. [sent-19, score-0.119]
</p><p>9 Core components of any BCI system are the feature extraction algorithm that extracts those brain signal features that represent the subject’s intent, and the decoding algorithm that translates those features into output commands to control artiﬁcial actuators. [sent-20, score-0.146]
</p><p>10 Substantial efforts in signal processing and machine learning have been devoted to decoding algorithms. [sent-21, score-0.108]
</p><p>11 The simplest translation algorithms use linear models to model the relationship between brain signals and limb movements. [sent-24, score-0.101]
</p><p>12 This linear relationship can be deﬁned using different algorithms, including multiple linear regression, pace regression [8], or ridge regression [13]. [sent-25, score-0.204]
</p><p>13 What is common to current linear and non-linear methods is that they are often used to model the instantaneous relationship between brain signals and particular behavioral parameters. [sent-28, score-0.101]
</p><p>14 (b) A diagram of possible state transitions for ﬁnger movements. [sent-30, score-0.118]
</p><p>15 do not account for the temporal evolution of movement parameters, and can also not directly provide uncertainty in their predictions. [sent-31, score-0.176]
</p><p>16 These include the Kalman ﬁlter (KF) that explicitly characterizes the temporal evolution of movement parameters [20]. [sent-36, score-0.176]
</p><p>17 We applied the SNDS technique to a dataset used in previous studies ([8]) to decode from ECoG signals the ﬂexion of individual ﬁngers, and we compared decoding results when we did and did not use anatomical constraints (i. [sent-44, score-0.222]
</p><p>18 Our results show that incorporation of anatomical constraints substantially improved decoding results compared to when we did not incorporate this information. [sent-47, score-0.183]
</p><p>19 Second, to effectively model the duration of movement patterns, our model solves the “Markov assumption” problem more efﬁciently by modeling the dependence of state transition on the continuous state variable. [sent-50, score-0.46]
</p><p>20 Third, because estimation of continuous transition is crucial to accurate prediction, we applied kernel density estimation to model the continuous state transition. [sent-51, score-0.295]
</p><p>21 The movement of ﬁngers can be categorized into three states: extension (state S1 ), ﬂexion (state S2 ) and rest (rest state S3 ). [sent-56, score-0.316]
</p><p>22 For each state, there are particular predominant movement patterns. [sent-58, score-0.135]
</p><p>23 In the extension state S1 , the ﬁnger keeps moving away from the rest position. [sent-59, score-0.213]
</p><p>24 In the ﬂexion state S2 , the ﬁnger moves back to the rest position. [sent-60, score-0.127]
</p><p>25 In the rest state S3 , there are only very small movements. [sent-61, score-0.127]
</p><p>26 For either state S1 or state S2 , the movement speed is relatively low toward full ﬂexion or full extension, but faster in between. [sent-63, score-0.341]
</p><p>27 The extension state and ﬂexion state can transfer to each other, while the rest state can only follow the ﬂexion state and can only precede the extension state. [sent-70, score-0.517]
</p><p>28 When the ﬁnger is extended, it is impossible for it to directly transition into the rest state without experiencing ﬂexion ﬁrst. [sent-72, score-0.205]
</p><p>29 Similarly, ﬁngers can not transition from rest state to ﬂexion state without ﬁrst going through the extension state. [sent-73, score-0.353]
</p><p>30 Figure 1 (b) discusses four possible ways of state transitions. [sent-75, score-0.094]
</p><p>31 For example, in the situation at hand, it is unlikely that the extension state transfers to the ﬂexion state right after the extension state begins. [sent-77, score-0.39]
</p><p>32 Using the methods described below, we will build a computational model that incorporates these constraints and that can systematically learn the movement patterns from data. [sent-81, score-0.163]
</p><p>33 The top layer S represents moving states that include the extension state (S1 ), ﬂexion state (S2 ), and rest state (S3 ). [sent-84, score-0.431]
</p><p>34 The middle layer (continuous state variable) represents the real ﬁnger position, and the bottom layer (observation) Z the measurements of ﬁnger positions. [sent-85, score-0.122]
</p><p>35 1  State Transitions  In the standard SLDS, the probability of duration τ of state i is, according to the Markov assumption, deﬁned as follows: τ P (τ ) = qii (1 − qii ) (1) where qii denotes the transition probability of state i when it makes a self transition. [sent-88, score-0.382]
</p><p>36 Equation 1 states that the probability of staying in a given state decreases exponentially with time. [sent-89, score-0.11]
</p><p>37 5  1  NORMALIZED AMPLITUDE  Figure 3: (a) Probabilistic density function (PDF) of Yt−1 given St−1 = extension and St = f lexion; (b) Probabilistic density function of Yt−1 given St−1 = f lexion and St = extension. [sent-99, score-0.122]
</p><p>38 Thus, the duration of certain movement patterns will deviate from the distribution described by Equation 1. [sent-102, score-0.161]
</p><p>39 This limitation of the state duration model has been investigated by [2, 14]. [sent-103, score-0.12]
</p><p>40 In the context of ﬁnger ﬂexion, as discussed in Section 2, the transition of moving states is dependent on ﬁnger position. [sent-107, score-0.126]
</p><p>41 P (St |St−1 ) is the state transition, which is same with that in HMM and standard SLDS. [sent-109, score-0.094]
</p><p>42 P (Yt−1 |St−1 , St ) is the posterior probability of Yt−1 given state transition from St−1 to St . [sent-110, score-0.172]
</p><p>43 P (Yt−1 |St−1 , St ) plays a central role in controlling state transition. [sent-111, score-0.094]
</p><p>44 We take the transition between extension state and ﬂexion state as an example to give an intuitive explanation. [sent-113, score-0.32]
</p><p>45 Figure 3(a) shows that the transition from extension state to ﬂexion state most probably happens at the ﬁnger position between 1. [sent-114, score-0.345]
</p><p>46 3, which is the ﬂexion end of the ﬁnger movement, the transition from ﬂexion state to extension state has a high probability. [sent-119, score-0.32]
</p><p>47 However, in our model, the continuous state transition is still highly nonlinear during the extension and ﬂexion states. [sent-122, score-0.259]
</p><p>48 This is mainly because the ﬁnger movement speed is uneven (fast in the middle but slow at the beginning and end). [sent-123, score-0.153]
</p><p>49 Modeling the continuous state transition properly is important for accurate decoding of ﬁnger movement. [sent-124, score-0.293]
</p><p>50 Here we propose a nonparametric method with which continuous state transitions are modeled using kernel density estimation [3]. [sent-125, score-0.208]
</p><p>51 With a Gaussian kernel, the joint estimated joint distribution p(Yt−1 , Yt ) under each ˆ state can be obtained by: p(Yt−1 = yt−1 , Yt = yt ) = ˆ  N  1 N hYt−1 hYt  K j=1  yt−1 − yj−1 hYt−1  K  yt − yj hYt  . [sent-127, score-0.794]
</p><p>52 2  Yt −1  Figure 4: (a) kernel locations for p(Yt−1 , Yt ) under extension state; (b) kernel locations for ˆ p(Yt−1 , Yt ) under ﬂexion state; kernel locations for p(Yt−1 , Yt ) under rest state. [sent-142, score-0.258]
</p><p>53 p(Yt−1 , Yt ) ˆ ˆ provides a much more accurate representation of continuous state transition than does a linear model. [sent-145, score-0.205]
</p><p>54 Figure 4 gives an example of the kernel locations for p(Yt−1 , Yt ) under each of the three states ˆ (trained with part of the data from thumb ﬂexion of subject A). [sent-146, score-0.109]
</p><p>55 Each graph in Figure 4 describes a temporal transition pattern for each movement pattern. [sent-148, score-0.237]
</p><p>56 For the extension state, all kernel locations are above the diagonal, which means that statistically Yt is greater than Yt−1 , i. [sent-149, score-0.111]
</p><p>57 Also the farther the kernel locations are from the diagonal, the larger the value of Yt − Yt−1 , which implies greater moving speed at time t. [sent-152, score-0.107]
</p><p>58 In the extension state, the moving speed around average ﬂexion is statistically greater that around the two extremes (full ﬂexion and extension). [sent-153, score-0.104]
</p><p>59 Similar arguments can be applied to the ﬂexion state in Figure 4(b). [sent-154, score-0.094]
</p><p>60 For the rest state, kernel locations are almost along the diagonal, which means Yt = Yt−1 , i. [sent-155, score-0.09]
</p><p>61 The capability of being able to model the non-linear dependence of speed on position under each state is critical to make a precise prediction of the ﬂexion trace. [sent-158, score-0.137]
</p><p>62 3  Observation Model  Z is the observation which is the ﬁnger ﬂexion trace directly mapped from ECoG signals through other regression algorithms. [sent-160, score-0.115]
</p><p>63 In this paper, we employ the pace regression for this mapping. [sent-161, score-0.178]
</p><p>64 Here we make an assumption that under each movement pattern, Zt depends linearly on Yt , and corrupted by Gaussian noise. [sent-162, score-0.135]
</p><p>65 Speciﬁcally, samples on the extension parts of the traces are labeled with state “extension,” samples on the ﬂexion parts of the traces are labeled with state “ﬂexion,” and samples during rest are labeled with state “rest. [sent-171, score-0.403]
</p><p>66 Z is the observation for which we use the output of pace regression. [sent-173, score-0.152]
</p><p>67 5  ¯ All parameters Φ in our model (Figure 2) consist of three components: the state transition parameter ¯ ¯ ¯ ΦS , continuous state transition parameter ΦY , and observation parameter ΦO . [sent-174, score-0.377]
</p><p>68 For state transition ¯ parameter ΦS , as discussed in Equation 2, P (St |St−1 ) and P (Yt−1 |St−1 , St ) are learned from the training data. [sent-175, score-0.172]
</p><p>69 The elements in the conditional probability table of P (St |St−1 ) corresponding to the impossible state transitions are set to zero. [sent-178, score-0.118]
</p><p>70 Y transition ¯ parameter ΦY includes the joint distribution p(Yt−1 , Yt ), which can be estimated using Equation 3 ˆ 2 ¯ in which bandwidths were selected using the criteria in Equation 4. [sent-180, score-0.092]
</p><p>71 However we note that not all the continuous variables in our model follow Gaussian distribution, because kernel density estimation was used to model the dynamics of the continuous state variable. [sent-189, score-0.217]
</p><p>72 In short, we ﬁrst re-referenced the signals using a H 1 common average reference (CAR), which subtracted H q=1 sq from each channel, where H is the total number of channels and sq is the collected signal at the qth channel and at the particular 6  time. [sent-209, score-0.102]
</p><p>73 3  Evaluation  We deﬁned a movement period as the time between 1000 ms prior to movement onset and 1000 ms after movement offset. [sent-220, score-0.456]
</p><p>74 Conversely, movement offset was deﬁned as the time when the ﬁnger’s ﬂexion value fell below that threshold and no movement onset was detected within the next 1200 ms [8]. [sent-222, score-0.305]
</p><p>75 To achieve a dataset with relatively balanced movement and rest periods, we discarded all data outside the movement period. [sent-223, score-0.303]
</p><p>76 Finally, we compared the performance with that achieved using pace regression (which had been used in [8]). [sent-227, score-0.178]
</p><p>77 4  Results  To give an impression of the qualitative improvement of our modeling algorithms described above compared to pace regression, we ﬁrst provide a qualitative example of the results achieved with each method on the index ﬁnger of subject A. [sent-230, score-0.17]
</p><p>78 In this ﬁgure, the top panel shows results achieved using pace regression and the middle ﬁgure shows results achieved using SNDS. [sent-232, score-0.178]
</p><p>79 In each of these two panels, the thin dotted line shows the actual ﬂexion of the index ﬁnger (concatenated for ﬁve movement periods), and the thick solid line shows the ﬂexion decoded using pace regression/SNDS. [sent-233, score-0.379]
</p><p>80 This ﬁgure demonstrates qualitatively that the decoding of ﬁnger ﬂexion achieved using SNDS much better approximates the actual ﬁnger ﬂexion than does pace regression. [sent-234, score-0.262]
</p><p>81 We also observe that SNDS produces much smoother predictions, which is mainly due to the consideration of temporal evolution of movement parameters in SNDS. [sent-235, score-0.176]
</p><p>82 The bottom panel again shows the actual ﬂexion pattern (thin dotted line) as well as the ﬁnger ﬂexion state (1=ﬂexion, 2=extension, 3=rest; thick solid line). [sent-236, score-0.157]
</p><p>83 These results demonstrate that the state of ﬁnger ﬂexion (which cannot be directly inferred using a method that does not incorporate a state machine (such as pace regression)) can be accurately inferred using SNDS. [sent-237, score-0.362]
</p><p>84 In addition to the qualitative comparison provided above, Table 1 gives a quantitative comparison between the results achieved using SNDS and pace regression. [sent-238, score-0.152]
</p><p>85 They show that for all ﬁngers and all subjects, the results achieved using SNDS are superior to those achieved using pace regression. [sent-240, score-0.152]
</p><p>86 This improvement of SNDS compared to pace regression was highly statistically signiﬁcant: when computed a paired t-test on the mean square errors for all ﬁngers and subjects and between pace regression and SNDS, the resulting p-value was << 0. [sent-244, score-0.401]
</p><p>87 We also showed that the resulting computational models are more accurately able to infer the ﬂexion of individual ﬁngers than does pace regression, an established technique that has recently been used on the same dataset. [sent-247, score-0.152]
</p><p>88 5  20  (B)  0 (C)  0  TIME (s)  Figure 5: (a) Actual ﬁnger ﬂexion (dotted trace) and decoded ﬁnger ﬂexion (solid trace) using pace regression (mean square error 0. [sent-261, score-0.222]
</p><p>89 40); (c) Actual ﬁnger ﬂexion (dotted trace) and state prediction (solid trace). [sent-263, score-0.094]
</p><p>90 Table 1: Comparison of decoding performance between pace regression and SNDS. [sent-264, score-0.266]
</p><p>91 Results are given, for a particular ﬁnger and subject, as mean square errors between actual and decoded movement (minimum, maximum and mean across all cross validation folds). [sent-265, score-0.201]
</p><p>92 pace SNDS pace SNDS pace SNDS pace SNDS pace SNDS  Thumb 0. [sent-267, score-0.76]
</p><p>93 71  Generally, this improvement in decoding performance is possible, because the computational model puts different types of constraints on the possible ﬂexion predictions. [sent-428, score-0.116]
</p><p>94 First, to reduce the computational complexity caused by kernel density estimation, non-linear transition functions can be used to model the continuous state transitions. [sent-433, score-0.262]
</p><p>95 Finally, the methods presented in this paper could be extended to allow for simultaneous decoding of all ﬁve ﬁngers instead of one at a time. [sent-435, score-0.088]
</p><p>96 Superiority of nonlinear mapping in decoding multiple single-unit neuronal spike trains: A simulation study. [sent-470, score-0.088]
</p><p>97 Decoding ﬂexion of a individual ﬁngers using electrocorticographic signals in humans. [sent-474, score-0.096]
</p><p>98 Explicit modelling of state occupancy in hidden Markov models for automatic speech recognition. [sent-513, score-0.094]
</p><p>99 Comparison between nonlinear mappings and linear state estimation to model the relation from motor cortical neuronal ﬁring to hand movements. [sent-518, score-0.116]
</p><p>100 Decoding two-dimensional movement trajectories using electrocorticographic signals in humans. [sent-534, score-0.231]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exion', 0.623), ('nger', 0.468), ('yt', 0.35), ('snds', 0.211), ('st', 0.188), ('pace', 0.152), ('ngers', 0.141), ('movement', 0.135), ('hyt', 0.11), ('ecog', 0.097), ('state', 0.094), ('decoding', 0.088), ('transition', 0.078), ('finger', 0.062), ('signals', 0.061), ('bci', 0.06), ('schalk', 0.06), ('extension', 0.054), ('anatomical', 0.045), ('slds', 0.04), ('wolpaw', 0.04), ('brain', 0.04), ('hz', 0.036), ('issn', 0.036), ('electrocorticographic', 0.035), ('zt', 0.034), ('continuous', 0.033), ('kernel', 0.033), ('rest', 0.033), ('moving', 0.032), ('bcis', 0.03), ('exions', 0.03), ('flexion', 0.03), ('qii', 0.03), ('subjects', 0.03), ('decoded', 0.029), ('constraints', 0.028), ('trace', 0.028), ('interfaces', 0.028), ('kinematic', 0.026), ('eng', 0.026), ('glove', 0.026), ('duration', 0.026), ('regression', 0.026), ('position', 0.025), ('transitions', 0.024), ('locations', 0.024), ('temporal', 0.024), ('density', 0.024), ('dotted', 0.023), ('incorporate', 0.022), ('motor', 0.022), ('actual', 0.022), ('channel', 0.021), ('amplitude', 0.021), ('kalman', 0.021), ('channels', 0.02), ('digitized', 0.02), ('ecse', 0.02), ('eibe', 0.02), ('kfs', 0.02), ('kub', 0.02), ('lexion', 0.02), ('nek', 0.02), ('ojemann', 0.02), ('sldss', 0.02), ('doi', 0.02), ('efforts', 0.02), ('onset', 0.019), ('solid', 0.018), ('speed', 0.018), ('subject', 0.018), ('birch', 0.018), ('commands', 0.018), ('rensselaer', 0.018), ('sung', 0.018), ('thumb', 0.018), ('yun', 0.018), ('movements', 0.018), ('traces', 0.017), ('evolution', 0.017), ('states', 0.016), ('govern', 0.016), ('weka', 0.016), ('polytechnic', 0.016), ('troy', 0.016), ('zy', 0.016), ('ms', 0.016), ('square', 0.015), ('ian', 0.015), ('frequency', 0.015), ('layer', 0.014), ('bandwidths', 0.014), ('amplitudes', 0.014), ('bienenstock', 0.014), ('devices', 0.014), ('holmes', 0.014), ('switched', 0.014), ('kim', 0.014), ('witten', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="38-tfidf-1" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>2 0.1680641 <a title="38-tfidf-2" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>Author: Nicolò Cesa-bianchi, Ohad Shamir</p><p>Abstract: Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely diﬀerent approach, which combines “random playout” and randomized rounding of loss subgradients. As an application of our approach, we provide the ﬁrst computationally eﬃcient online algorithm for collaborative ﬁltering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning. 1</p><p>3 0.090754755 <a title="38-tfidf-3" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>Author: Dan Garber, Elad Hazan</p><p>Abstract: In recent years semideﬁnite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the ﬁrst sublinear time approximation algorithm for semideﬁnite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric. 1</p><p>4 0.084514901 <a title="38-tfidf-4" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>5 0.08221522 <a title="38-tfidf-5" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>Author: Liu Yang</p><p>Abstract: We study the problem of active learning in a stream-based setting, allowing the distribution of the examples to change over time. We prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms, both in the realizable case and under Tsybakov noise. We further prove minimax lower bounds for this problem. 1</p><p>6 0.081945486 <a title="38-tfidf-6" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>7 0.059889324 <a title="38-tfidf-7" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>8 0.057939105 <a title="38-tfidf-8" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>9 0.056411359 <a title="38-tfidf-9" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>10 0.055391442 <a title="38-tfidf-10" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>11 0.052926928 <a title="38-tfidf-11" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>12 0.052347455 <a title="38-tfidf-12" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>13 0.045658268 <a title="38-tfidf-13" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>14 0.04209324 <a title="38-tfidf-14" href="./nips-2011-Multilinear_Subspace_Regression%3A_An_Orthogonal_Tensor_Decomposition_Approach.html">179 nips-2011-Multilinear Subspace Regression: An Orthogonal Tensor Decomposition Approach</a></p>
<p>15 0.039702218 <a title="38-tfidf-15" href="./nips-2011-Message-Passing_for_Approximate_MAP_Inference_with_Latent_Variables.html">170 nips-2011-Message-Passing for Approximate MAP Inference with Latent Variables</a></p>
<p>16 0.038267631 <a title="38-tfidf-16" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>17 0.037854325 <a title="38-tfidf-17" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>18 0.037167594 <a title="38-tfidf-18" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>19 0.036885332 <a title="38-tfidf-19" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>20 0.036552854 <a title="38-tfidf-20" href="./nips-2011-Learning_to_Agglomerate_Superpixel_Hierarchies.html">155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, -0.055), (2, 0.061), (3, -0.007), (4, 0.037), (5, -0.005), (6, 0.042), (7, -0.035), (8, 0.037), (9, -0.046), (10, 0.099), (11, -0.011), (12, 0.123), (13, 0.06), (14, -0.011), (15, -0.023), (16, 0.037), (17, 0.04), (18, 0.015), (19, -0.051), (20, -0.026), (21, -0.03), (22, 0.026), (23, 0.015), (24, 0.052), (25, 0.036), (26, 0.002), (27, 0.077), (28, -0.018), (29, -0.023), (30, -0.033), (31, 0.056), (32, 0.015), (33, -0.041), (34, -0.095), (35, -0.015), (36, -0.114), (37, 0.081), (38, 0.044), (39, 0.052), (40, 0.126), (41, 0.008), (42, 0.07), (43, -0.016), (44, -0.054), (45, 0.002), (46, 0.007), (47, 0.007), (48, -0.22), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93825847 <a title="38-lsi-1" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>2 0.62422091 <a title="38-lsi-2" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>Author: Richard Turner, Maneesh Sahani</p><p>Abstract: A number of recent scientiﬁc and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time. Although signal processing provides algorithms for so-called amplitude- and frequencydemodulation (AFD), there are well known problems with all of the existing methods. Motivated by the fact that AFD is ill-posed, we approach the problem using probabilistic inference. The new approach, called probabilistic amplitude and frequency demodulation (PAFD), models instantaneous frequency using an auto-regressive generalization of the von Mises distribution, and the envelopes using Gaussian auto-regressive dynamics with a positivity constraint. A novel form of expectation propagation is used for inference. We demonstrate that although PAFD is computationally demanding, it outperforms previous approaches on synthetic and real signals in clean, noisy and missing data settings. 1</p><p>3 0.56754875 <a title="38-lsi-3" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>Author: Liu Yang</p><p>Abstract: We study the problem of active learning in a stream-based setting, allowing the distribution of the examples to change over time. We prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms, both in the realizable case and under Tsybakov noise. We further prove minimax lower bounds for this problem. 1</p><p>4 0.52538115 <a title="38-lsi-4" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>Author: Nicolò Cesa-bianchi, Ohad Shamir</p><p>Abstract: Most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely diﬀerent approach, which combines “random playout” and randomized rounding of loss subgradients. As an application of our approach, we provide the ﬁrst computationally eﬃcient online algorithm for collaborative ﬁltering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning. 1</p><p>5 0.44702166 <a title="38-lsi-5" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a ﬁxed distribution, and the adversarial scenario wherein, at every time step, an adversarially chosen instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We deﬁne the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we deﬁne a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with inﬁnite Littlestone dimension learnable. 1</p><p>6 0.43428075 <a title="38-lsi-6" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>7 0.35901627 <a title="38-lsi-7" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>8 0.33637315 <a title="38-lsi-8" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>9 0.33542207 <a title="38-lsi-9" href="./nips-2011-Maximum_Covariance_Unfolding_%3A_Manifold_Learning_for_Bimodal_Data.html">167 nips-2011-Maximum Covariance Unfolding : Manifold Learning for Bimodal Data</a></p>
<p>10 0.32143208 <a title="38-lsi-10" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>11 0.31422812 <a title="38-lsi-11" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>12 0.31243464 <a title="38-lsi-12" href="./nips-2011-How_Do_Humans_Teach%3A_On_Curriculum_Learning_and_Teaching_Dimension.html">122 nips-2011-How Do Humans Teach: On Curriculum Learning and Teaching Dimension</a></p>
<p>13 0.307605 <a title="38-lsi-13" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>14 0.30318865 <a title="38-lsi-14" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>15 0.30135262 <a title="38-lsi-15" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>16 0.29435784 <a title="38-lsi-16" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>17 0.29311472 <a title="38-lsi-17" href="./nips-2011-Blending_Autonomous_Exploration_and_Apprenticeship_Learning.html">48 nips-2011-Blending Autonomous Exploration and Apprenticeship Learning</a></p>
<p>18 0.28249013 <a title="38-lsi-18" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>19 0.27311265 <a title="38-lsi-19" href="./nips-2011-Demixed_Principal_Component_Analysis.html">68 nips-2011-Demixed Principal Component Analysis</a></p>
<p>20 0.26774129 <a title="38-lsi-20" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (4, 0.076), (20, 0.021), (26, 0.016), (31, 0.116), (33, 0.016), (43, 0.039), (45, 0.053), (57, 0.053), (63, 0.014), (65, 0.017), (74, 0.034), (83, 0.048), (84, 0.031), (92, 0.315), (99, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74075508 <a title="38-lda-1" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>2 0.56834435 <a title="38-lda-2" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>3 0.54510623 <a title="38-lda-3" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>4 0.49844834 <a title="38-lda-4" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>5 0.4797135 <a title="38-lda-5" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>Author: Biljana Petreska, Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: Simultaneous recordings of many neurons embedded within a recurrentlyconnected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function. In principle, these dynamics might be identiﬁed by purely unsupervised, statistical means. Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model— in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process—is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements. The regimes are identiﬁed without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial. The HSLDS model also performs better than recent comparable models in predicting the ﬁring rate of an isolated neuron based on the ﬁring rates of others, suggesting that it captures more of the “shared variance” of the data. Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reﬂect its computational role. 1</p><p>6 0.44926071 <a title="38-lda-6" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>7 0.44791508 <a title="38-lda-7" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>8 0.44771916 <a title="38-lda-8" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>9 0.44664112 <a title="38-lda-9" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>10 0.44623879 <a title="38-lda-10" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>11 0.44566071 <a title="38-lda-11" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>12 0.44344068 <a title="38-lda-12" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>13 0.43907788 <a title="38-lda-13" href="./nips-2011-An_Application_of_Tree-Structured_Expectation_Propagation_for_Channel_Decoding.html">31 nips-2011-An Application of Tree-Structured Expectation Propagation for Channel Decoding</a></p>
<p>14 0.43645287 <a title="38-lda-14" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>15 0.43570918 <a title="38-lda-15" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>16 0.4355543 <a title="38-lda-16" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>17 0.43555343 <a title="38-lda-17" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>18 0.43555257 <a title="38-lda-18" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>19 0.4353781 <a title="38-lda-19" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>20 0.43497312 <a title="38-lda-20" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
