<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-214" href="#">nips2011-214</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</h1>
<br/><p>Source: <a title="nips-2011-214-pdf" href="http://papers.nips.cc/paper/4319-picodes-learning-a-compact-code-for-novel-category-recognition.pdf">pdf</a></p><p>Author: Alessandro Bergamo, Lorenzo Torresani, Andrew W. Fitzgibbon</p><p>Abstract: We introduce P I C O D ES: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of deﬁning indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images deﬁning the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classiﬁcation performance. Optimization directly for binary features is difﬁcult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. P I C O D ES of 256 bytes match the accuracy of the current best known classiﬁer for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.</p><p>Reference: <a title="nips-2011-214-reference" href="../nips2011_reference/nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We introduce P I C O D ES: a very compact image descriptor which nevertheless allows high performance on object category recognition. [sent-7, score-0.648]
</p><p>2 In particular, we address novel-category recognition: the task of deﬁning indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. [sent-8, score-0.426]
</p><p>3 Instead, the training images deﬁning the category are supplied at query time. [sent-9, score-0.356]
</p><p>4 We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. [sent-10, score-0.306]
</p><p>5 In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classiﬁcation performance. [sent-12, score-0.177]
</p><p>6 P I C O D ES of 256 bytes match the accuracy of the current best known classiﬁer for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude. [sent-14, score-0.413]
</p><p>7 1 Introduction In this work we consider the problem of efﬁcient object-class recognition in large image collections. [sent-15, score-0.223]
</p><p>8 The motivating application is “object-class search by example” where a user provides at query time a small set of training images deﬁning an arbitrary novel category and the system must retrieve from a large database images belonging to this class. [sent-17, score-0.564]
</p><p>9 Traditional object categorization methods do not meet these requirements as they typically use nonlinear kernels on high-dimensional descriptors, which renders them computationally expensive to train and test, and causes them to occupy large amounts of storage. [sent-19, score-0.287]
</p><p>10 For example, the LP-β multiple kernel combiner [11] achieves state-of-the-art accuracy on several categorization benchmarks but it requires over 23 Kbytes to represent each image and it uses 39 feature-speciﬁc nonlinear kernels. [sent-20, score-0.58]
</p><p>11 This recognition model is impractical for our application because it would require costly query-time kernel evaluations for each image in the database since the training set varies with every new query and thus pre-calculation of kernel distances is not possible. [sent-21, score-0.54]
</p><p>12 Six of the 128 bits are illustrated as follows: for bit c, all images are sorted by non-binarized classiﬁer outputs a⊤ x and the 10 smallest and largest are presented on c each row. [sent-24, score-0.283]
</p><p>13 Note that ac is deﬁned only up to sign, so the patterns to which the bits are specialized may appear in either the “positive” or “negative” columns. [sent-25, score-0.424]
</p><p>14 The binary entries in our image descriptor are thresholded nonlinear projections of low-level visual features extracted from the image, such as descriptors encoding texture or the appearance of local image patches. [sent-29, score-0.942]
</p><p>15 The search for compact codes for images has been the subject of much recent work, which we loosely divide into “designed” and “learned” codes. [sent-32, score-0.456]
</p><p>16 In the former category we include min-hash [6], VLAD [14], and attributes [10, 18, 17] which are fully-supervised classiﬁers trained to recognize certain visual properties in the image. [sent-33, score-0.249]
</p><p>17 A related idea is the representation of images in terms of distances to basis classes. [sent-34, score-0.184]
</p><p>18 This has been previously investigated as a way to deﬁne image similarities [30], to perform video search [12], or to enable natural scene recognition and retrieval [29]. [sent-35, score-0.32]
</p><p>19 [27] deﬁne a compact image code as a bitvector, the entries of which are the outputs of a large set of weakly-trained basis classiﬁers (“classemes”) evaluated on the image. [sent-37, score-0.424]
</p><p>20 Simple linear classiﬁers trained on classeme vectors produce near state-of-the-art categorization accuracy. [sent-38, score-0.482]
</p><p>21 [19] use the localized outputs of object detectors as an image representation. [sent-40, score-0.249]
</p><p>22 The second strand of related work is the learning of compact codes for images [31, 26, 24, 15, 22, 8] where binary image codes are learned such that the Hamming distance between codewords approximates a kernelized distance between image descriptors, most typically GIST. [sent-44, score-1.001]
</p><p>23 Autoencoder learning [23], on the other hand, produces a compact code which has good image reconstruction properties, but again is not specialized for category recognition. [sent-45, score-0.446]
</p><p>24 All the above descriptors can produce very compact codes, but few (excepting [27, 19]) have been shown to be effective at category-level recognition beyond simpliﬁed problems such as Caltech20 [2] or Caltech-101 [14, 16]. [sent-46, score-0.434]
</p><p>25 In contrast, we consider Caltech-256 a baseline competence, and also test compact codes on a large-scale class retrieval task using ImageNet [7]. [sent-47, score-0.426]
</p><p>26 The goal of this paper then is to learn a compact binary code (as short as 128 bits) which has good object-category recognition accuracy. [sent-48, score-0.356]
</p><p>27 2  2 Technical approach  Accuracy (%)  We start by introducing the basic classiﬁer archi32 tecture used by state-of-the-art category recognizers, 30 6415 which we want to leverage as effectively as possible 28 to deﬁne our image descriptor. [sent-50, score-0.258]
</p><p>28 Given an image I, a 26 bank of feature descriptors is computed (e. [sent-51, score-0.398]
</p><p>29 SIFT, 52K PHOG, GIST), to yield a feature vector f I ∈ RF 24 (the feature vector used in our implementation has 22 Linear SVM on x dimensionality F = 17360 and is described in the LP−β using explicit feature maps 20 experimental section). [sent-53, score-0.316]
</p><p>30 State-of-the-art recognizers LP−β using nonlinear kernels use kernel matching between these descriptors to de18 ﬁne powerful classiﬁers, nonlinear in f I . [sent-54, score-0.447]
</p><p>31 For our feature learning algorithm, we chose to use a PCA subspace of dimensionality n = 6415 since, as suggested by the plot, this setting gives a good tradeoff in terms of compact dimensionality and good recognition accuracy. [sent-72, score-0.371]
</p><p>32 [27] have shown that an effective image descriptor for categorization can be built by collecting in a vector the thresholded outputs of a large set of nonlinear classiﬁers evaluated on the image. [sent-74, score-0.561]
</p><p>33 This “classeme” descriptor can produce recognition accuracies within 10% of the state of the art for novel classes even with simple linear classiﬁcation models. [sent-75, score-0.408]
</p><p>34 Using our formulation based on explicit feature maps, we can approximately express each classeme entry (which in [27] is implemented as an LP-β classiﬁer) as the output of a linear classiﬁer h(x; ac ) = 1[aT x > 0] c  (1)  where 1[. [sent-76, score-0.721]
</p><p>35 [27], we would collect C training categories, and learn the parameters ac for each class from ofﬂine training data using some standard training objective such as hinge loss. [sent-81, score-0.687]
</p><p>36 Then, for image x, the “classeme” descriptor h(x) is computed as the concatenation of the outputs of the classiﬁers learned for the training categories:   h(x; a1 )   . [sent-86, score-0.49]
</p><p>37 h(x; aC ) The P I C O D E descriptor is also of this form. [sent-89, score-0.183]
</p><p>38 However, the key-difference with respect to [27] lies in our training procedure, and the fact that the dimensionality C is no longer restricted to be the same as the number of training classes. [sent-90, score-0.205]
</p><p>39 Given that we want to use attributes as features for linear classiﬁcation, we propose to formalize as learning objective that linear combinations of such attributes must yield good accuracy. [sent-92, score-0.268]
</p><p>40 • Unlike the attribute or classeme approach, our method decouples the number of training classes from the target dimensionality of the binary descriptor. [sent-93, score-0.635]
</p><p>41 • Finally, we directly optimize the learning parameters with respect to binary features while prior attribute systems binarized the features in a quantization stage after the learning. [sent-95, score-0.249]
</p><p>42 1 Learning the basis classiﬁers We assume that we are given a set of N training images, with each image coming from one of K training classes. [sent-98, score-0.349]
</p><p>43 We then deﬁne our c-th basis classiﬁer to be a boolean function of the form (1), a thresholded nonlinear projection of the original low-level features f , parameterized by ac ∈ Rn . [sent-103, score-0.528]
</p><p>44 We then optimize these parameters so that linear combinations of these basis classiﬁers yield good categorization accuracy on D. [sent-104, score-0.273]
</p><p>45 The learning objective introduces auxiliary variables (wk , bk ) for each training class, which parameterize the linear classiﬁer for that training class, operating on the P I C O D E representation of the training examples, and the objective for A simply minimizes over these auxiliaries: E(A, w1. [sent-105, score-0.415]
</p><p>46 K ) = k=1  1 wk 2  2  +  λ N  N  C  wkc 1[aT xi > 0]) c  ℓ yi,k (bk + i=1  4  c=1  . [sent-122, score-0.203]
</p><p>47 Let us consider the update of ac with ﬁxed parameters w1. [sent-133, score-0.289]
</p><p>48 It can be seen (Appendix A) that in this case the objective becomes: N  vi 1[zi aT xi > 0] + const c  E(ac ) =  (5)  i=1  where zi ∈ {−1, +1} and vi ∈ R+ are known values computed from the ﬁxed parameters. [sent-142, score-0.224]
</p><p>49 After learning, at test-time we replaced back σ(x; ac ) with h(x; ac ) to obtain binary descriptors. [sent-149, score-0.627]
</p><p>50 However, we found that these binary codes performed much worse than those directly learned via the coordinate descent procedure described above. [sent-150, score-0.272]
</p><p>51 In order to allow a fair comparison, we reimplemented the “classeme descriptor” based on the same set of low-level features and settings described in [27] but using the explicit feature map framework to replace the expensive nonlinear kernel distance computations. [sent-152, score-0.269]
</p><p>52 The low-level features are: color GIST [21], spatial pyramid of histograms of oriented gradients (PHOG) [4], spatial pyramid of self-similarity descriptors [25], and a histogram of SIFT features [20] quantized using a dictionary of 5000 visual words. [sent-153, score-0.507]
</p><p>53 Each spatial pyramid level of each descriptor was treated as a separate feature, thus producing a total of 13 low-level features. [sent-154, score-0.256]
</p><p>54 Each of these features was lifted up to a higher-dimensional space using the explicit feature maps of Vedaldi and Zisserman [28]. [sent-155, score-0.193]
</p><p>55 We chose the mapping approximating the histogram intersection kernels for n = 1, which effectively mapped each low-level feature descriptor to a space 3 times larger than its original one. [sent-156, score-0.27]
</p><p>56 We compared P I C O D ES with binary classeme vectors. [sent-159, score-0.385]
</p><p>57 For both descriptors we used a training set of K = 2659 classes randomly sampled from the ImageNet dataset [7], with 30 images for each category for a total of N = 2659 × 30 = 79, 770 images. [sent-160, score-0.589]
</p><p>58 P I C O D ES of 2048 bits match the accuracy of the state-of-the-art LP-β classiﬁer. [sent-164, score-0.221]
</p><p>59 1329 2659 Number of training classes for the descriptor (K)  3988  Figure 4: Caltech256 classiﬁcation accuracy for P I C O D ES and classemes as a function of the number of training classes used to learn the descriptors. [sent-165, score-0.916]
</p><p>60 We also present comparisons with binary codes trained to directly approximate the Eucliden distances between the vectors x, using the following previously proposed algorithms: locality sensitive hashing (LSH) [13], spectral hashing (SH) [31], and binary reconstructive embeddings (BRE) [15]. [sent-167, score-0.486]
</p><p>61 Since these descriptors in the past have been used predominantly with the k-NN classiﬁer, we have also tested this classiﬁcation model but obtained inferior results compared to when using a linear SVM. [sent-168, score-0.208]
</p><p>62 We ﬁrst report in ﬁgure 3 the results showing multiclass classiﬁcation accuracy achieved with binary codes on the Caltech256 data set. [sent-171, score-0.37]
</p><p>63 For each Caltech256 category, the classiﬁer was trained using 10 positive examples and a total of 2550 negative examples obtained by sampling 10 images from each of the other classes. [sent-173, score-0.211]
</p><p>64 As usual, accuracy is computed as the average over the mean recognition rates per class. [sent-175, score-0.17]
</p><p>65 Figure 3 shows the results obtained with binary descriptors of varying dimensionality. [sent-176, score-0.257]
</p><p>66 While our approach can accommodate easily the case were the number of feature dimensions (C) is different from the number of feature-training categories (K), the classeme learning method can only produce descriptors of size K. [sent-177, score-0.642]
</p><p>67 Thus, the descriptor size is typically reduced through a subsequent feature selection stage [27, 19]. [sent-178, score-0.234]
</p><p>68 We also report accuracy obtained with the original classeme vectors of [27], which were learned with exact kernels on a different training set, consisting of weakly-supervised images retrieved with text-based image search engines. [sent-180, score-0.855]
</p><p>69 From this ﬁgure we see that P I C O D ES greatly outperform all the other compact codes considered here (classemes, LSH, SH, BRE) for all descriptor sizes. [sent-181, score-0.504]
</p><p>70 In addition, perhaps surprisingly, P I C O D ES of 2048 bits yield even higher accuracy than the-state-of-the-art multiple kernel combiner LP-β [11] trained on our low-level features f (30. [sent-182, score-0.528]
</p><p>71 At the same time, our codes are 100 times smaller and reduce the training and testing time by two-orders of magnitude compared to LP-β. [sent-185, score-0.258]
</p><p>72 , the number of training classes used to learn the descriptor. [sent-188, score-0.194]
</p><p>73 We learned different P I C O D ES and classeme descriptors by varying K while keeping the number of training examples per class ﬁxed to 30. [sent-189, score-0.737]
</p><p>74 Figure 4 shows the multiclass categorization accuracy on Caltech256 as a function of K. [sent-190, score-0.24]
</p><p>75 proﬁt more than classemes from a larger number of training classes, producing further improvement in generalization on novel classes. [sent-194, score-0.4]
</p><p>76 An advantage of the classeme learning setup presented in [27] is the intrinsic parallelization that can be achieved during the learning of the C classeme classiﬁers (which are disjointly trained), enabling the use of more training data. [sent-195, score-0.788]
</p><p>77 We have considered this scenario, and tried learning the classeme descriptors from ImageNet using 5 times more images than for P I C O D ES, i. [sent-196, score-0.647]
</p><p>78 , 150 images for each training category for a total of N = 2659 × 150 = 398, 850 examples. [sent-198, score-0.301]
</p><p>79 Despite the disparate training set sizes, we found that P I C O D ES still outperformed classemes (22. [sent-199, score-0.373]
</p><p>80 In ﬁgure 5 we present results corresponding to our motivating application of object-class search, using codes of 256 bytes. [sent-203, score-0.179]
</p><p>81 We then used the learned classiﬁer to ﬁnd images of that category in a database containing 6400 Caltech256 test images, with 25 images per class. [sent-205, score-0.415]
</p><p>82 The retrieval accuracy is measured as precision at 25, which is the proportion of true positives (i. [sent-206, score-0.188]
</p><p>83 Again, we see that our features yield consistently better ranking precision compared to classeme vectors learned on the same ImageNet training set, and produce an average improvement of about 28% over the original classeme features. [sent-209, score-0.932]
</p><p>84 Finally, we present experiments on the 150K image data set of the Large Scale Visual Recognition Challenge 2010 (ILSVRC2010) [3], which includes images of 1000 different categories, different from those used to train P I C O D ES. [sent-211, score-0.242]
</p><p>85 Again, we evaluate our binary codes on the task of object-class retrieval. [sent-212, score-0.228]
</p><p>86 Figure 6 shows a comparison between P I C O D ES and classemes in terms of precision at k for varying k. [sent-215, score-0.331]
</p><p>87 Despite the very large number of distractors (149,850 for each query), search with our codes yields precisions exceeding 38%. [sent-216, score-0.243]
</p><p>88 7  4 Conclusion We have described a new type of compact code, which is learned by directly minimizing a multiclass classiﬁcation objective on a large set of ofﬂine training classes. [sent-219, score-0.379]
</p><p>89 This allows recognition of novel categories to be performed using extremely compact codes with state-of-the-art accuracy. [sent-220, score-0.479]
</p><p>90 Although there is much existing work on learning compact codes, we know of no other compact code which offers this performance on a category recognition task. [sent-221, score-0.533]
</p><p>91 Our experiments have focussed on whole-image “Caltech-like” category recognition, while it is clear that subimage recognition is also an important application. [sent-222, score-0.203]
</p><p>92 However, we argue that for many image search tasks, whole-image performance is relevant, and for a very compact code, one could possibly encode several windows (dozens, say) in each image, while retaining a relatively compact representation. [sent-223, score-0.455]
</p><p>93 K ) = k=1  1 wk 2  2  +  λ N  C  N  wkc 1[aT xi > 0]) c  ℓ yik (bk +  . [sent-237, score-0.33]
</p><p>94 k=1  Finally, it can be seen that optimizing this objective is equivalent to minimizing N  vi 1[zi aT xi > 0] c  E(ac ) = i=1  K where vi = k=1 ℓ(αikc + βikc ) − ℓ(βikc ) and zi = sign This yields eq. [sent-249, score-0.224]
</p><p>95 Combining attributes and ﬁsher vectors for efﬁcient image retrieval. [sent-310, score-0.194]
</p><p>96 Aggregating local descriptors into a compact image e e representation. [sent-364, score-0.489]
</p><p>97 Learning to detect unseen object classes by between-class attribute transfer. [sent-398, score-0.189]
</p><p>98 Object Bank: A high-level image representation for scene classiﬁcation & semantic feature sparsiﬁcation. [sent-405, score-0.22]
</p><p>99 Building the gist of a scene: The role of global image features in recognition. [sent-417, score-0.236]
</p><p>100 Learning image similarity from ﬂickr using stochastic intersection kernel machines. [sent-477, score-0.193]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('classeme', 0.336), ('ikc', 0.336), ('classemes', 0.294), ('ac', 0.289), ('descriptors', 0.208), ('descriptor', 0.183), ('codes', 0.179), ('rfe', 0.168), ('classi', 0.154), ('imagenet', 0.147), ('compact', 0.142), ('image', 0.139), ('bits', 0.135), ('yik', 0.127), ('wkc', 0.126), ('category', 0.119), ('ers', 0.107), ('combiner', 0.105), ('picodes', 0.105), ('images', 0.103), ('er', 0.102), ('categorization', 0.098), ('torresani', 0.092), ('accuracy', 0.086), ('recognition', 0.084), ('classes', 0.08), ('training', 0.079), ('svm', 0.075), ('retrieval', 0.065), ('object', 0.065), ('features', 0.063), ('bytes', 0.063), ('pca', 0.063), ('bk', 0.062), ('objective', 0.058), ('nonlinear', 0.056), ('multiclass', 0.056), ('bre', 0.055), ('attributes', 0.055), ('query', 0.055), ('gure', 0.054), ('kernel', 0.054), ('basis', 0.052), ('feature', 0.051), ('hashing', 0.049), ('binary', 0.049), ('cvpr', 0.048), ('trained', 0.048), ('lsh', 0.048), ('dimensionality', 0.047), ('categories', 0.047), ('database', 0.046), ('code', 0.046), ('explicit', 0.045), ('outputs', 0.045), ('attribute', 0.044), ('learned', 0.044), ('zi', 0.044), ('vision', 0.043), ('lorenzo', 0.042), ('benchmarks', 0.042), ('pyramid', 0.041), ('vi', 0.041), ('thresholded', 0.04), ('xi', 0.04), ('class', 0.04), ('projections', 0.038), ('wk', 0.037), ('yield', 0.037), ('precision', 0.037), ('douze', 0.037), ('recognizers', 0.037), ('phog', 0.037), ('disjointly', 0.037), ('vedaldi', 0.037), ('rf', 0.037), ('kernels', 0.036), ('lp', 0.036), ('learn', 0.035), ('es', 0.035), ('maps', 0.034), ('accuracies', 0.034), ('gist', 0.034), ('reconstructive', 0.034), ('spatial', 0.032), ('search', 0.032), ('distractors', 0.032), ('storage', 0.032), ('requirements', 0.032), ('examples', 0.03), ('quantization', 0.03), ('semantic', 0.03), ('distances', 0.029), ('projection', 0.028), ('liblinear', 0.028), ('hinge', 0.028), ('visual', 0.027), ('novel', 0.027), ('kernelized', 0.027), ('zisserman', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="214-tfidf-1" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani, Andrew W. Fitzgibbon</p><p>Abstract: We introduce P I C O D ES: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of deﬁning indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images deﬁning the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classiﬁcation performance. Optimization directly for binary features is difﬁcult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. P I C O D ES of 256 bytes match the accuracy of the current best known classiﬁer for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.</p><p>2 0.19710582 <a title="214-tfidf-2" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>3 0.15844126 <a title="214-tfidf-3" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>Author: Jia Deng, Sanjeev Satheesh, Alexander C. Berg, Fei Li</p><p>Abstract: We present a novel approach to efﬁciently learn a label tree for large scale classiﬁcation with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classiﬁers for each node in the tree. This approach also allows ﬁne grained control over the efﬁciency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classiﬁcation with 10184 classes and 9 million images. We demonstrate signiﬁcant improvements in test accuracy and efﬁciency with less training time and more balanced trees compared to the previous state of the art by Bengio et al. 1</p><p>4 0.15826787 <a title="214-tfidf-4" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>5 0.15717067 <a title="214-tfidf-5" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>Author: Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, Thomas S. Huang</p><p>Abstract: High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efﬁciency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efﬁciency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efﬁciency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efﬁcient large scale search. Our approach takes both search quality and computational cost into consideration. Speciﬁcally, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efﬁciently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efﬁciency. Experimental results show that our approach signiﬁcantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p><p>6 0.14881973 <a title="214-tfidf-6" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>7 0.13424878 <a title="214-tfidf-7" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>8 0.13411278 <a title="214-tfidf-8" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>9 0.13027181 <a title="214-tfidf-9" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>10 0.1294522 <a title="214-tfidf-10" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>11 0.11548043 <a title="214-tfidf-11" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>12 0.11396056 <a title="214-tfidf-12" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>13 0.11296013 <a title="214-tfidf-13" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>14 0.10595272 <a title="214-tfidf-14" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>15 0.10396607 <a title="214-tfidf-15" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>16 0.099817954 <a title="214-tfidf-16" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>17 0.094026178 <a title="214-tfidf-17" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>18 0.091107346 <a title="214-tfidf-18" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>19 0.085950047 <a title="214-tfidf-19" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>20 0.083620615 <a title="214-tfidf-20" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.228), (1, 0.151), (2, -0.163), (3, 0.148), (4, 0.078), (5, 0.116), (6, 0.105), (7, 0.013), (8, -0.068), (9, 0.01), (10, -0.06), (11, 0.083), (12, -0.006), (13, 0.092), (14, -0.009), (15, 0.009), (16, -0.136), (17, 0.077), (18, -0.002), (19, -0.003), (20, 0.076), (21, 0.02), (22, 0.103), (23, -0.06), (24, 0.028), (25, 0.068), (26, 0.126), (27, -0.06), (28, 0.061), (29, 0.099), (30, 0.006), (31, 0.037), (32, 0.012), (33, -0.029), (34, -0.059), (35, -0.058), (36, 0.113), (37, -0.093), (38, 0.012), (39, -0.008), (40, 0.021), (41, 0.111), (42, -0.006), (43, 0.001), (44, 0.023), (45, -0.059), (46, -0.064), (47, -0.066), (48, -0.027), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.954431 <a title="214-lsi-1" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani, Andrew W. Fitzgibbon</p><p>Abstract: We introduce P I C O D ES: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of deﬁning indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images deﬁning the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classiﬁcation performance. Optimization directly for binary features is difﬁcult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. P I C O D ES of 256 bytes match the accuracy of the current best known classiﬁer for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.</p><p>2 0.80427444 <a title="214-lsi-2" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>3 0.7615965 <a title="214-lsi-3" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>Author: Fahad S. Khan, Joost Weijer, Andrew D. Bagdanov, Maria Vanrell</p><p>Abstract: We describe a novel technique for feature combination in the bag-of-words model of image classiﬁcation. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classiﬁcation problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to ﬁnd discriminative combinations of cues and the resulting vocabulary of portmanteau1 words is compact, has the cue binding property, and supports individual weighting of cues in the ﬁnal image representation. State-of-theart results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, signiﬁcantly more complex approaches to multi-cue image representation. 1</p><p>4 0.751136 <a title="214-lsi-4" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>Author: Vicente Ordonez, Girish Kulkarni, Tamara L. Berg</p><p>Abstract: We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset – performing a huge number of Flickr queries and then ﬁltering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning. 1</p><p>5 0.73853338 <a title="214-lsi-5" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>Author: Phillip Isola, Devi Parikh, Antonio Torralba, Aude Oliva</p><p>Abstract: Artists, advertisers, and photographers are routinely presented with the task of creating an image that a viewer will remember. While it may seem like image memorability is purely subjective, recent work shows that it is not an inexplicable phenomenon: variation in memorability of images is consistent across subjects, suggesting that some images are intrinsically more memorable than others, independent of a subjects’ contexts and biases. In this paper, we used the publicly available memorability dataset of Isola et al. [13], and augmented the object and scene annotations with interpretable spatial, content, and aesthetic image properties. We used a feature-selection scheme with desirable explaining-away properties to determine a compact set of attributes that characterizes the memorability of any individual image. We ﬁnd that images of enclosed spaces containing people with visible faces are memorable, while images of vistas and peaceful scenes are not. Contrary to popular belief, unusual or aesthetically pleasing scenes do not tend to be highly memorable. This work represents one of the ﬁrst attempts at understanding intrinsic image memorability, and opens a new domain of investigation at the interface between human cognition and computer vision. 1</p><p>6 0.73320496 <a title="214-lsi-6" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>7 0.68562013 <a title="214-lsi-7" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>8 0.66775846 <a title="214-lsi-8" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>9 0.66616017 <a title="214-lsi-9" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>10 0.6641472 <a title="214-lsi-10" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>11 0.65463066 <a title="214-lsi-11" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>12 0.6517728 <a title="214-lsi-12" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>13 0.64742696 <a title="214-lsi-13" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>14 0.63738388 <a title="214-lsi-14" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>15 0.62935066 <a title="214-lsi-15" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>16 0.62259424 <a title="214-lsi-16" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>17 0.6023069 <a title="214-lsi-17" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>18 0.58805639 <a title="214-lsi-18" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>19 0.57540447 <a title="214-lsi-19" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>20 0.55581403 <a title="214-lsi-20" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (4, 0.038), (20, 0.053), (26, 0.023), (31, 0.047), (33, 0.122), (43, 0.055), (45, 0.167), (57, 0.048), (65, 0.016), (74, 0.035), (78, 0.21), (83, 0.027), (84, 0.013), (99, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82362592 <a title="214-lda-1" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>Author: Nitesh Shroff, Pavan Turaga, Rama Chellappa</p><p>Abstract: In this paper, we consider the Pr´ cis problem of sampling K representative yet e diverse data points from a large dataset. This problem arises frequently in applications such as video and document summarization, exploratory data analysis, and pre-ﬁltering. We formulate a general theory which encompasses not just traditional techniques devised for vector spaces, but also non-Euclidean manifolds, thereby enabling these techniques to shapes, human activities, textures and many other image and video based datasets. We propose intrinsic manifold measures for measuring the quality of a selection of points with respect to their representative power, and their diversity. We then propose efﬁcient algorithms to optimize the cost function using a novel annealing-based iterative alternation algorithm. The proposed formulation is applicable to manifolds of known geometry as well as to manifolds whose geometry needs to be estimated from samples. Experimental results show the strength and generality of the proposed approach.</p><p>same-paper 2 0.8127147 <a title="214-lda-2" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani, Andrew W. Fitzgibbon</p><p>Abstract: We introduce P I C O D ES: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of deﬁning indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images deﬁning the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classiﬁcation performance. Optimization directly for binary features is difﬁcult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. P I C O D ES of 256 bytes match the accuracy of the current best known classiﬁer for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.</p><p>3 0.76259619 <a title="214-lda-3" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>Author: Ehsan Elhamifar, René Vidal</p><p>Abstract: We propose an algorithm called Sparse Manifold Clustering and Embedding (SMCE) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds. Similar to most dimensionality reduction methods, SMCE ﬁnds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights. The key difference is that SMCE ﬁnds both the neighbors and the weights automatically. This is done by solving a sparse optimization problem, which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional afﬁne subspace. The optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding. Moreover, the size of the optimal neighborhood of a data point, which can be different for different points, provides an estimate of the dimension of the manifold to which the point belongs. Experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other, manifolds with non-uniform sampling and holes, as well as estimate the intrinsic dimensions of the manifolds. 1 1.1</p><p>4 0.74288327 <a title="214-lda-4" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>5 0.74206179 <a title="214-lda-5" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>Author: Ricardo S. Cabral, Fernando Torre, Joao P. Costeira, Alexandre Bernardino</p><p>Abstract: Recently, image categorization has been an active research topic due to the urgent need to retrieve and browse digital images via semantic keywords. This paper formulates image categorization as a multi-label classiﬁcation problem using recent advances in matrix completion. Under this setting, classiﬁcation of testing data is posed as a problem of completing unknown label entries on a data matrix that concatenates training and testing features with training labels. We propose two convex algorithms for matrix completion based on a Rank Minimization criterion speciﬁcally tailored to visual data, and prove its convergence properties. A major advantage of our approach w.r.t. standard discriminative classiﬁcation methods for image categorization is its robustness to outliers, background noise and partial occlusions both in the feature and label space. Experimental validation on several datasets shows how our method outperforms state-of-the-art algorithms, while effectively capturing semantic concepts of classes. 1</p><p>6 0.71264625 <a title="214-lda-6" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>7 0.71164834 <a title="214-lda-7" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>8 0.70535636 <a title="214-lda-8" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>9 0.70227039 <a title="214-lda-9" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>10 0.6957671 <a title="214-lda-10" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>11 0.69368809 <a title="214-lda-11" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>12 0.69208342 <a title="214-lda-12" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>13 0.69162828 <a title="214-lda-13" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>14 0.6901688 <a title="214-lda-14" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>15 0.69007057 <a title="214-lda-15" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>16 0.68748474 <a title="214-lda-16" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>17 0.68633687 <a title="214-lda-17" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>18 0.68607175 <a title="214-lda-18" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>19 0.68170935 <a title="214-lda-19" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>20 0.67864656 <a title="214-lda-20" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
