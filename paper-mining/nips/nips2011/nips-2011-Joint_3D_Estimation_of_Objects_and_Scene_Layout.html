<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-138" href="#">nips2011-138</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</h1>
<br/><p>Source: <a title="nips-2011-138-pdf" href="http://papers.nips.cc/paper/4189-joint-3d-estimation-of-objects-and-scene-layout.pdf">pdf</a></p><p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>Reference: <a title="nips-2011-138-reference" href="../nips2011_reference/nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. [sent-5, score-0.922]
</p><p>2 In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. [sent-6, score-0.794]
</p><p>3 Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i. [sent-7, score-0.747]
</p><p>4 Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. [sent-11, score-0.35]
</p><p>5 1  Introduction  Visual 3D scene understanding is an important component in applications such as autonomous driving and robot navigation. [sent-12, score-0.321]
</p><p>6 , semantic labels [10, 26], object detection [5] or rough 3D [15, 24]. [sent-15, score-0.3]
</p><p>7 A notable exception are approaches that try to infer the scene layout of indoor scenes in the form of 3D bounding boxes [13, 22]. [sent-16, score-0.676]
</p><p>8 , walls (and often objects) are aligned with the three dominant vanishing points. [sent-21, score-0.258]
</p><p>9 In contrast, outdoor scenarios often show more clutter, vanishing points are not necessarily orthogonal [25, 2], and objects often do not agree with the dominant vanishing points. [sent-22, score-0.561]
</p><p>10 Prior work on 3D urban scene analysis is mostly limited to simple ground plane estimation [4, 29] or models for which the objects and the scene are inferred separately [6, 7]. [sent-23, score-0.831]
</p><p>11 In contrast, in this paper we propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. [sent-24, score-0.922]
</p><p>12 In particular, given a video sequence of short duration acquired with a single camera mounted on a moving car, we estimate the scene topology and geometry, as well as the trafﬁc activities and 3D objects present in the scene (see Fig. [sent-25, score-1.103]
</p><p>13 Towards this goal we propose a novel image likelihood which takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i. [sent-27, score-0.767]
</p><p>14 Furthermore, we propose a novel learning-based approach to detecting vanishing points and experimentally show improved performance in the presence of clutter when compared to existing approaches [19]. [sent-31, score-0.298]
</p><p>15 We focus our evaluation mainly on estimating the layout of intersections, as this is the most challenging inference task in urban scenes. [sent-32, score-0.375]
</p><p>16 We evaluate our method on a wide range of metrics including the accuracy of estimating the topology and geometry of the scene, as well as detecting 1  Vehicle Tracklets  Vanishing Points  ⇒ Scene Labels  Figure 1: Monocular 3D Urban Scene Understanding. [sent-36, score-0.241]
</p><p>17 (Right) Estimated layout: Detections belonging to a tracklet are depicted with the same color, trafﬁc activities are depicted with red lines. [sent-38, score-0.293]
</p><p>18 Furthermore, we show that we are able to signiﬁcantly increase the performance of state-of-the-art object detectors [5] in terms of estimating object orientation. [sent-42, score-0.307]
</p><p>19 2  Related Work  While outdoor scenarios remain fairly unexplored, estimating the 3D layout of indoor scenes has experienced increased popularity in the past few years [13, 27, 22]. [sent-43, score-0.354]
</p><p>20 , edges on the image can be associated with parallel lines deﬁned in terms of the three dominant vanishing points which are orthonormal. [sent-46, score-0.314]
</p><p>21 [2] proposed to jointly perform line detection as well as vanishing point, azimut and zenith estimation. [sent-51, score-0.284]
</p><p>22 However, their approach does not tackle the problem of 3D scene understanding and 3D object detection. [sent-52, score-0.459]
</p><p>23 , object A supports object B) have also been introduced [11]. [sent-57, score-0.276]
</p><p>24 Prior work on 3D trafﬁc scene analysis is mostly limited to simple ground plane estimation [4], or models for which the objects and scene are inferred separately [6]. [sent-59, score-0.706]
</p><p>25 In contrast, our model offers a much richer scene description and reasons jointly about 3D objects and the scene layout. [sent-60, score-0.684]
</p><p>26 The most successful approaches use tracklets to prune spurious detections by linking consistent evidence in successive frames [18, 16]. [sent-62, score-0.479]
</p><p>27 However, these models are either designed for static camera setups in surveillance applications [16] or do not provide a rich scene description [18]. [sent-63, score-0.431]
</p><p>28 Notable exceptions are [3, 29] which jointly infer the camera pose and the location of objects. [sent-64, score-0.348]
</p><p>29 However, the employed scene models are rather simplistic containing only a single ﬂat ground plane. [sent-65, score-0.351]
</p><p>30 [7], where a generative model is proposed in order to estimate the scene topology, geometry as well as trafﬁc activities at intersections. [sent-67, score-0.485]
</p><p>31 Towards this goal we develop a richer image likelihood model that takes advantage of vehicle tracklets, vanishing points as well as segmentations of the scene into semantic labels. [sent-71, score-0.749]
</p><p>32 [7] estimate only the scene layout, while we reason jointly about the layout as well as the 3D location and orientation of objects in the scene (i. [sent-73, score-1.168]
</p><p>33 Finally, non-parametric models have been proposed to perform trafﬁc scene analysis from a stationary camera with a view similar to bird’s eye perspective [20, 28]. [sent-78, score-0.607]
</p><p>34 In our work we aim to infer similar activities but use video sequences from a camera mounted on a moving car with a substantially lower viewpoint. [sent-79, score-0.462]
</p><p>35 3  3D Urban Scene Understanding  We tackle the problem of estimating the 3D layout of urban scenes (i. [sent-82, score-0.385]
</p><p>36 In this paper 2D refers to observations in the image plane while 3D refers to the bird’s eye perspective (in our scenario the height above ground is non-informative). [sent-85, score-0.344]
</p><p>37 We assume that the road surface is ﬂat, and model the bird’s eye perspective as the y = 0 plane of the standard camera coordinate system. [sent-86, score-0.51]
</p><p>38 The intrinsic parameters of the camera are obtained using camera calibration and the extrinsics using a standard Structure-from-Motion (SfM) pipeline [12]. [sent-88, score-0.348]
</p><p>39 We take advantage of dynamic and static information in the form of 3D vehicle tracklets, semantic labels (i. [sent-89, score-0.268]
</p><p>40 In order to compute 3D tracklets, we ﬁrst detect vehicles in each frame independently using a semi-supervised version of the partbased detector of [5] in order to obtain orientation estimates. [sent-92, score-0.383]
</p><p>41 2D tracklets are then estimated using ’tracking-by-detection’: First adjacent frames are linked and then short tracklets are associated to create longer ones via the hungarian method. [sent-93, score-0.764]
</p><p>42 Finally, 3D vehicle tracklets are obtained by projecting the 2D tracklets into bird’s eye perspective, employing error-propagation to obtain covariance estimates. [sent-94, score-1.006]
</p><p>43 1 where detections belonging to the same tracklet are grouped by color. [sent-96, score-0.284]
</p><p>44 We model lanes with splines (see red lines for active lanes in Fig. [sent-105, score-0.242]
</p><p>45 ﬁg:motivation), and place parking spots at equidistant places along the street boundaries (see Fig. [sent-106, score-0.258]
</p><p>46 Our model then infers whether the cars participate in trafﬁc or are parked in order to get more accurate layout estimations. [sent-108, score-0.302]
</p><p>47 Latent variables are employed to associate each detected vehicle with positions in one of these lanes or parking spaces. [sent-109, score-0.531]
</p><p>48 Each of these layouts is associated with a set of geometric random variables: The intersection center c, the street width w, the global scene rotation r and the angle of the crossing street α with respect to r (see Fig. [sent-114, score-0.664]
</p><p>49 Joint Distribution: Our goal is to estimate the most likely conﬁguration R = (θ, c, w, r, α) given the image evidence E = {T, V, S}, which comprises vehicle tracklets T = {t1 , . [sent-117, score-0.572]
</p><p>50 , tN }, vanish3  (a) Graphical model (b) Road model Figure 3: Graphical model and road model with lanes represented as B-splines. [sent-119, score-0.275]
</p><p>51 Prior:  Let us ﬁrst deﬁne a scene prior, which factorizes as p(R) = p(θ)p(c, w)p(r)p(α)  (2)  where c and w are modeled jointly to capture their correlation. [sent-126, score-0.328]
</p><p>52 2  Image Likelihood  This section details our image likelihood for tracklets, vanishing points and semantic labels. [sent-132, score-0.313]
</p><p>53 Let us deﬁne a 3D tracklet as a set of object detections t = {d1 , . [sent-134, score-0.422]
</p><p>54 Here, each object detection dm = (fm , bm , om ) contains the frame index fm ∈ N, the object bounding box bm ∈ R4 deﬁned as 2D position and size, as well as a normalized orientation histogram om ∈ R8 with 8 bins. [sent-137, score-1.058]
</p><p>55 We compute the bounding box bm and orientation om by supervised training of a part-based object detector [5], where each component contains examples from a single orientation. [sent-138, score-0.602]
</p><p>56 , K(K − 1) + 2K} in total, where K(K − 1) is the number of lanes and 2K is the number of parking areas. [sent-146, score-0.311]
</p><p>57 We use the latent variable l to index the lane or parking position associated with a tracklet. [sent-147, score-0.301]
</p><p>58 The joint probability of a tracklet t and its lane index l is given by p(t, l|R, C) = p(t|l, R, C)p(l). [sent-148, score-0.318]
</p><p>59 We assume a uniform prior over lanes and parking positions l ∼ U(1, K(K − 1) + 2K), and denote the posterior by pl when l corresponds to a lane, and pp when it is a parking position. [sent-149, score-0.699]
</p><p>60 In order to evaluate the tracklet posterior for lanes pl (t|l, R, C), we need to associate all object detections t = {d1 , . [sent-150, score-0.72]
</p><p>61 The posterior is modeled using a left-to-right Hidden Markov Model (HMM), deﬁned as: M  pl (t|l, R, C) =  pl (s1 )pl (d1 |s1 , l, R, C) s1 ,. [sent-156, score-0.286]
</p><p>62 ,sM  pl (sm |sm−1 )pl (dm |sm , l, R, C)  (3)  m=2  We constrain all tracklets to move forward in 3D by deﬁning the transition probability p(sm |sm−1 ) as uniform on sm ≥ sm−1 and 0 otherwise. [sent-158, score-0.612]
</p><p>63 We assume that the emission likelihood pl (dm |sm , l, R, C) factorizes into the object location and its orientation. [sent-160, score-0.376]
</p><p>64 We impose a multinomial distribution over the orientation pl (fm , om |sm , l, R, C), where each object orientation votes for its bin as well as neighboring bins, accounting for the uncertainty of the object detector. [sent-161, score-0.875]
</p><p>65 We now describe how we transform the 2D tracklets into 3D tracklets {π 1 , Σ1 , . [sent-163, score-0.724]
</p><p>66 , π M , ΣM }, which we use in pl (dm |sm , l, R, C): We project the image coordinates into bird’s eye perspective by backprojecting objects into 3D using several complementary cues. [sent-165, score-0.449]
</p><p>67 Towards this goal we use the 2D bounding box foot-point in combination with the estimated road plane. [sent-166, score-0.251]
</p><p>68 Assuming typical vehicle dimensions obtained from annotated ground truth, we also exploit the width and height of the bounding box. [sent-167, score-0.324]
</p><p>69 Our parking posterior model is similar to the lane posterior described above, except that we do not allow parked vehicles to move; We assume them to have arbitrary orientations and place them at the sides of the road. [sent-170, score-0.52]
</p><p>70 For inference, we subsample each tracklet trajectory equidistantly in intervals of 5 meters in order to reduce the number of detections within a tracklet and keep the total evaluation time of p(R, E|C) low. [sent-173, score-0.491]
</p><p>71 Vanishing Points: We detect two types of dominant vanishing points (VP) in the last frame of each sequence: vf corresponding to the forward facing street and vc corresponding to the crossing street. [sent-174, score-0.632]
</p><p>72 As a consequence, we represent vf ∈ R by its image u-coordinate and vc ∈ [− π , π ] 4 4 by the angle of the crossing road, back projected into the image. [sent-177, score-0.35]
</p><p>73 Our feature set comprises geometric information in the form of position, length, orientation and number of lines with the same orientation as well as perpendicular orientation in a local window. [sent-200, score-0.618]
</p><p>74 Finally, we add texton-like features using a Gabor ﬁlter bank, as well as 3 principal components of the scene GIST [23]. [sent-202, score-0.282]
</p><p>75 We assume that vf and vc are independent given the road parameters. [sent-207, score-0.323]
</p><p>76 Let µf = µf (R, C) be the image u-coordinate (in pixels) of the forward facing street’s VP and let µc = µc (R, C) be the orientation (in radians) of the crossing street in the image. [sent-208, score-0.421]
</p><p>77 4 illustrates an example of the scene labeling returned by boosting (left) as well as the labeling generated from the reprojection of our model (right). [sent-220, score-0.318]
</p><p>78 4 requires a function ϕ : f, b, C → π, Σ which takes a frame index f ∈ N, an object bounding box b ∈ R4 and the calibration parameters C as input and maps them to the object location π ∈ R2 and uncertainty Σ ∈ R2×2 in bird’s eye perspective. [sent-259, score-0.692]
</p><p>79 As cues for this mapping we use the bounding box width and height, as well as the location of the bounding box foot-point. [sent-260, score-0.326]
</p><p>80 The unknown parameters of the mapping are the uncertainty in bounding box location σu , σv , width σ∆u and height σ∆v as well as the real-world object dimensions ∆x , ∆y along with their uncertainties σ∆x , σ∆y . [sent-262, score-0.411]
</p><p>81 To avoid trans-dimensional jumps, the road layout θ is estimated separately beforehand using MAP estimation θM AP provided by joint boosting [30]. [sent-269, score-0.373]
</p><p>82 4  Experimental Evaluation  In this section, we ﬁrst show that learning which line features convey structural information improves dominant vanishing point detection. [sent-271, score-0.258]
</p><p>83 Next, we compare our method to a multiple kernel learning (MKL) baseline in estimating scene topology, geometry and trafﬁc activities on the dataset of [7], but only employing information from a single camera. [sent-272, score-0.522]
</p><p>84 Finally, we show that our model can signiﬁcantly improve object orientation estimates compared to state-of-the-art part based models [5]. [sent-273, score-0.344]
</p><p>85 3D Urban Scene Inference: We evaluate our method’s ability to infer the scene layout by building a competitive baseline based on multi-kernel Gaussian process regression [17]. [sent-283, score-0.565]
</p><p>86 We employ a total of 4 kernels built on GIST [23], tracklet histograms, VPs as well as scene labels. [sent-284, score-0.489]
</p><p>87 Note that these are the same features employed by our model to estimate the scene topology, θM AP . [sent-285, score-0.314]
</p><p>88 Following [7] we measure error in terms of the location of the intersection center in meters, the orientation of the intersection arms in degrees, the overlap of road area with ground truth as well as the percentage of correctly discovered intersection crossing activities. [sent-290, score-0.875]
</p><p>89 The inferred intersection layout is shown in gray, ground truth labels are given in blue. [sent-295, score-0.351]
</p><p>90 8 shows qualitative results, with detections belonging to the same tracklet depicted with the same color. [sent-306, score-0.284]
</p><p>91 As cars are mostly aligned with the road surface, we only focus on the orientation angle in bird’s eye coordinates. [sent-312, score-0.565]
</p><p>92 We correct for the ego motion and project the highest scoring orientation into bird’s eye perspective. [sent-314, score-0.334]
</p><p>93 For our method, we infer the scene layout R using our approach and associate every tracklet to its lane by maximizing pl (l|t, R, C) over l using Viterbi decoding. [sent-315, score-1.018]
</p><p>94 We then select the tangent angle at the associated spline’s footpoint s on the inferred lane l as our orientation estimate. [sent-316, score-0.351]
</p><p>95 5  Conclusions  We have proposed a generative model which is able to perform joint 3D inference over the scene layout as well as the location and orientation of objects. [sent-321, score-0.838]
</p><p>96 Our approach is able to infer the scene topology and geometry, as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a car driving around a mid-size city. [sent-322, score-0.805]
</p><p>97 A generative model for 3d urban scene understanding from movable platforms. [sent-368, score-0.482]
</p><p>98 Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. [sent-461, score-0.257]
</p><p>99 Discriminative learning with latent variables for cluttered indoor scene understanding. [sent-492, score-0.337]
</p><p>100 A dynamic CRF model for joint labeling of object and scene classes. [sent-510, score-0.42]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tracklets', 0.362), ('scene', 0.282), ('tracklet', 0.207), ('orientation', 0.206), ('vanishing', 0.19), ('parking', 0.19), ('layout', 0.183), ('road', 0.154), ('vehicle', 0.154), ('traf', 0.149), ('camera', 0.149), ('pl', 0.143), ('object', 0.138), ('bird', 0.132), ('eye', 0.128), ('urban', 0.125), ('lanes', 0.121), ('lane', 0.111), ('vf', 0.111), ('vp', 0.11), ('sm', 0.107), ('monocular', 0.105), ('vps', 0.103), ('location', 0.095), ('topology', 0.092), ('vehicles', 0.091), ('crossing', 0.091), ('deg', 0.091), ('activities', 0.086), ('intersection', 0.084), ('geiger', 0.084), ('geometry', 0.081), ('stereo', 0.078), ('detections', 0.077), ('bm', 0.077), ('parked', 0.076), ('objects', 0.074), ('fm', 0.072), ('dm', 0.071), ('clutter', 0.071), ('street', 0.068), ('dominant', 0.068), ('semantic', 0.067), ('mounted', 0.061), ('infer', 0.058), ('vc', 0.058), ('image', 0.056), ('ijcv', 0.056), ('pp', 0.055), ('indoor', 0.055), ('mkl', 0.055), ('eccv', 0.052), ('bounding', 0.052), ('manhattan', 0.052), ('wojek', 0.052), ('orientations', 0.052), ('calibration', 0.05), ('detection', 0.048), ('perspective', 0.048), ('labels', 0.047), ('scenes', 0.046), ('frame', 0.046), ('video', 0.046), ('jointly', 0.046), ('schindler', 0.045), ('box', 0.045), ('height', 0.044), ('om', 0.044), ('ap', 0.043), ('cars', 0.043), ('baseline', 0.042), ('tracking', 0.041), ('detector', 0.04), ('frames', 0.04), ('arms', 0.04), ('understanding', 0.039), ('outdoor', 0.039), ('depth', 0.038), ('detecting', 0.037), ('width', 0.037), ('ground', 0.037), ('inference', 0.036), ('bins', 0.036), ('generative', 0.036), ('boosting', 0.036), ('barinova', 0.034), ('facade', 0.034), ('kosecka', 0.034), ('trackets', 0.034), ('angle', 0.034), ('associate', 0.034), ('efros', 0.033), ('employed', 0.032), ('estimating', 0.031), ('hoiem', 0.031), ('cc', 0.031), ('car', 0.031), ('plane', 0.031), ('moving', 0.031), ('buildings', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="138-tfidf-1" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>2 0.25498071 <a title="138-tfidf-2" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>3 0.18296461 <a title="138-tfidf-3" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>Author: Hema S. Koppula, Abhishek Anand, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an ofﬁce and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model’s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efﬁcient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and ofﬁces (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for ofﬁces, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of ﬁnding objects in large cluttered rooms.1 1</p><p>4 0.17434628 <a title="138-tfidf-4" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>Author: Joseph L. Austerweil, Abram L. Friesen, Thomas L. Griffiths</p><p>Abstract: The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols × and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conﬂicting reference frames, the model predicts two factors should inﬂuence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. 1</p><p>5 0.15604213 <a title="138-tfidf-5" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>Author: Yibiao Zhao, Song-chun Zhu</p><p>Abstract: This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classiﬁers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules represent an ensemble of visual entities. Contextual relations: (i) Cooperative “+” relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive “-” relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efﬁcient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene conﬁgurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to ﬁnd the most probable conﬁguration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree. 1</p><p>6 0.12955272 <a title="138-tfidf-6" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>7 0.11303417 <a title="138-tfidf-7" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>8 0.1082734 <a title="138-tfidf-8" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>9 0.10268115 <a title="138-tfidf-9" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>10 0.098891519 <a title="138-tfidf-10" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>11 0.09493129 <a title="138-tfidf-11" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>12 0.083812989 <a title="138-tfidf-12" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>13 0.082942165 <a title="138-tfidf-13" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>14 0.082236357 <a title="138-tfidf-14" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>15 0.079503424 <a title="138-tfidf-15" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>16 0.078788273 <a title="138-tfidf-16" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>17 0.074522711 <a title="138-tfidf-17" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>18 0.073564209 <a title="138-tfidf-18" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>19 0.071828537 <a title="138-tfidf-19" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>20 0.071439996 <a title="138-tfidf-20" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, 0.128), (2, -0.106), (3, 0.208), (4, 0.101), (5, 0.051), (6, 0.005), (7, -0.059), (8, 0.079), (9, 0.15), (10, 0.026), (11, -0.039), (12, -0.025), (13, 0.033), (14, 0.015), (15, -0.024), (16, 0.089), (17, 0.009), (18, -0.058), (19, 0.097), (20, -0.046), (21, -0.059), (22, -0.058), (23, 0.122), (24, 0.02), (25, -0.106), (26, -0.061), (27, 0.037), (28, 0.061), (29, 0.005), (30, -0.089), (31, 0.046), (32, -0.002), (33, 0.033), (34, 0.021), (35, 0.004), (36, -0.109), (37, 0.078), (38, -0.126), (39, 0.01), (40, -0.046), (41, 0.01), (42, -0.081), (43, 0.046), (44, 0.022), (45, 0.014), (46, 0.042), (47, 0.005), (48, 0.028), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9604525 <a title="138-lsi-1" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>2 0.75612104 <a title="138-lsi-2" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>Author: Joseph L. Austerweil, Abram L. Friesen, Thomas L. Griffiths</p><p>Abstract: The object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame). For example, the images of the symbols × and + differ by a 45 degree rotation. Although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame. We propose an ideal observer model based on nonparametric Bayesian statistics for inferring the number of reference frames in a scene and their parameters. When an ambiguous image could be assigned to two conﬂicting reference frames, the model predicts two factors should inﬂuence the reference frame inferred for the image: The image should be more likely to share the reference frame of the closer object (proximity) and it should be more likely to share the reference frame containing the most objects (alignment). We conﬁrm people use both cues using a novel methodology that allows for easy testing of human reference frame inference. 1</p><p>3 0.74086976 <a title="138-lsi-3" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>4 0.71513641 <a title="138-lsi-4" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>Author: Hema S. Koppula, Abhishek Anand, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an ofﬁce and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model’s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efﬁcient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and ofﬁces (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for ofﬁces, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of ﬁnding objects in large cluttered rooms.1 1</p><p>5 0.70447373 <a title="138-lsi-5" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>Author: Vincent Delaitre, Josef Sivic, Ivan Laptev</p><p>Abstract: We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-speciﬁc body part and object interactions bypasses the difﬁcult problem of estimating the complete human body pose conﬁguration. Beneﬁts of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline. 1</p><p>6 0.69146734 <a title="138-lsi-6" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>7 0.6697194 <a title="138-lsi-7" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>8 0.62961614 <a title="138-lsi-8" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>9 0.5984782 <a title="138-lsi-9" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>10 0.57112002 <a title="138-lsi-10" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>11 0.5584892 <a title="138-lsi-11" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>12 0.54844576 <a title="138-lsi-12" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>13 0.50580299 <a title="138-lsi-13" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>14 0.49986687 <a title="138-lsi-14" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>15 0.49314409 <a title="138-lsi-15" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>16 0.49191245 <a title="138-lsi-16" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>17 0.49174753 <a title="138-lsi-17" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>18 0.46257231 <a title="138-lsi-18" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>19 0.45954543 <a title="138-lsi-19" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>20 0.40744418 <a title="138-lsi-20" href="./nips-2011-Emergence_of_Multiplication_in_a_Biophysical_Model_of_a_Wide-Field_Visual_Neuron_for_Computing_Object_Approaches%3A_Dynamics%2C_Peaks%2C_%26_Fits.html">85 nips-2011-Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, & Fits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.015), (4, 0.062), (20, 0.058), (26, 0.016), (31, 0.078), (33, 0.052), (43, 0.055), (45, 0.079), (57, 0.035), (60, 0.352), (65, 0.015), (74, 0.031), (83, 0.018), (84, 0.015), (99, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82189238 <a title="138-lda-1" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>2 0.71214938 <a title="138-lda-2" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>Author: Hema S. Koppula, Abhishek Anand, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an ofﬁce and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model’s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efﬁcient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and ofﬁces (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for ofﬁces, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of ﬁnding objects in large cluttered rooms.1 1</p><p>3 0.48612911 <a title="138-lda-3" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>Author: Yibiao Zhao, Song-chun Zhu</p><p>Abstract: This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classiﬁers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules represent an ensemble of visual entities. Contextual relations: (i) Cooperative “+” relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive “-” relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efﬁcient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene conﬁgurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to ﬁnd the most probable conﬁguration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree. 1</p><p>4 0.47807473 <a title="138-lda-4" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>Author: Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: Graph cut optimization is one of the standard workhorses of image segmentation since for binary random ﬁeld representations of the image, it gives globally optimal results and there are efﬁcient polynomial time implementations. Often, the random ﬁeld is applied over a ﬂat partitioning of the image into non-intersecting elements, such as pixels or super-pixels. In the paper we show that if, instead of a ﬂat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding beneﬁts of global optimality and efﬁciency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree. We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a ﬂat partitioning) and demonstrate that the optimization in the pylon model is able to ﬂexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches. 1</p><p>5 0.47793901 <a title="138-lda-5" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>Author: Adrian Ion, Joao Carreira, Cristian Sminchisescu</p><p>Abstract: We present a joint image segmentation and labeling model (JSL) which, given a bag of ﬁgure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as ﬁrst sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect conﬁgurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.</p><p>6 0.45211041 <a title="138-lda-6" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>7 0.43490139 <a title="138-lda-7" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>8 0.41848576 <a title="138-lda-8" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>9 0.41649598 <a title="138-lda-9" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>10 0.41494274 <a title="138-lda-10" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>11 0.41035092 <a title="138-lda-11" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>12 0.40833718 <a title="138-lda-12" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>13 0.40657625 <a title="138-lda-13" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>14 0.40396416 <a title="138-lda-14" href="./nips-2011-Collective_Graphical_Models.html">55 nips-2011-Collective Graphical Models</a></p>
<p>15 0.40363777 <a title="138-lda-15" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>16 0.40317607 <a title="138-lda-16" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>17 0.40176561 <a title="138-lda-17" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>18 0.40176484 <a title="138-lda-18" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>19 0.40039298 <a title="138-lda-19" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>20 0.39978233 <a title="138-lda-20" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
