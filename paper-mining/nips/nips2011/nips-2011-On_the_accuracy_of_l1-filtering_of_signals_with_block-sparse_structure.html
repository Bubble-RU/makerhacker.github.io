<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-203" href="#">nips2011-203</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</h1>
<br/><p>Source: <a title="nips-2011-203-pdf" href="http://papers.nips.cc/paper/4270-on-the-accuracy-of-l1-filtering-of-signals-with-block-sparse-structure.pdf">pdf</a></p><p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>Reference: <a title="nips-2011-203-reference" href="../nips2011_reference/nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 On the accuracy of 1-ﬁltering of signals with block-sparse structure  Anatoli Juditsky∗  Fatma Kılınc Karzan† ¸  Arkadi Nemirovski‡  Boris Polyak§  Abstract We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. [sent-1, score-0.459]
</p><p>2 Our emphasis is on the efﬁciently computable error bounds for the recovery routines. [sent-2, score-0.33]
</p><p>3 We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. [sent-4, score-0.306]
</p><p>4 (1)  Here A is a given m × n sensing matrix, x ∈ Rn is an unknown vector, u is an unknown (deterministic) nuisance parameter, known to belong to a certain set U ⊂ Rm , D ∈ Rm×m is known noise intensity matrix, and ξ ∈ Rm is random noise with standard normal distribution. [sent-6, score-0.314]
</p><p>5 × RnK , so that w = Bx ∈ RN is a block vector: w = [w[1]; . [sent-11, score-0.123]
</p><p>6 ; w[K]] with blocks w[k] = B[k]x ∈ Rnk , 1 ≤ k ≤ K, where B[k], 1 ≤ k ≤ K are nk × n matrices. [sent-14, score-0.42]
</p><p>7 z[K]] + σξ, where the matrix A ∈ Rm×d(K+1) and the noise intensity σ > 0 are known, and ξ ∼ N (0, Im ). [sent-31, score-0.112]
</p><p>8 Here the Kd × Kd covariance matrix V of ζ has the block structure with blocks ∧k  V k, = Cov(ζ[k], ζ[ ]) =  G  −i  F F T (GT )k−i ,  i=1  with  0 1  = 0, by convention. [sent-53, score-0.307]
</p><p>9 Image reconstruction with regularization by Total Variation (TV) [21, 7] Here one looks to recover an image Z ∈ Rn1 ×n2 from a blurred noisy observation y: y = Az + σξ, y ∈ Rm , where z = Col(Z) ∈ Rn , n = n1 n2 , A ∈ Rm×n is the matrix of discrete convolution, σ > 0 is known, and ξ ∼ N (0, Im ). [sent-54, score-0.179]
</p><p>10 In this example Bx naturally splits into 2-dimensional blocks and TV is nothing but the sum of 2 norms of these blocks. [sent-56, score-0.178]
</p><p>11 The recovery routines we consider are based on the block- 1 minimization, i. [sent-59, score-0.449]
</p><p>12 , the estimate w(y) K of w = Bx is w = Bx(y), where x(y) is obtained by minimizing the norm k=1 B[k]z (k) over n signals z ∈ R with Az “ﬁtting,” in certain precise sense, the observations y. [sent-61, score-0.141]
</p><p>13 Above, · (k) are given in advance norms on the spaces Rnk where the blocks of w take their values. [sent-62, score-0.154]
</p><p>14 Given such a structure B and matrix A, our goal is to understand how well one can recover the s-block-sparse transform Bx by appropriately implemented block- 1 minimization. [sent-70, score-0.175]
</p><p>15 Related Compressed Sensing research Our situation and goal form a straightforward extension of the usual block sparsity Compressed Sensing framework. [sent-71, score-0.153]
</p><p>16 Indeed, the standard representation structure B = In , nk = 1, · (k) = |·|, 1 ≤ k ≤ K = n, leads to the standard Compressed Sensing setting – recovering a sparse signal x ∈ Rn from its noisy observations (1) via 1 minimization. [sent-72, score-0.536]
</p><p>17 With the same B = In and nontrivial block structure {nk , · (k) }K , we arrive at block-sparsity and k=1 related block- 1 minimization routines considered in numerous recent papers. [sent-73, score-0.359]
</p><p>18 Several methods of estimation and selection extending the plain 1 -minimization to block sparsity were proposed and investigated recently. [sent-77, score-0.151]
</p><p>19 Most of the related research is focused so far on block regularization schemes – Lasso-type algorithms K  x(y) ∈  Argmin  Az − y  z=[z[1];. [sent-78, score-0.123]
</p><p>20 The available theoretical results allows to bound the errors of recovery in terms of magnitude of the observation noise and “s-concentration” of the true signal x (that is, its L[1,2] distance from the space of signals with at most s nonzero blocks). [sent-89, score-0.464]
</p><p>21 Typically, these results deal with the quadratic risks of estimation and rely on a natural block analogy (“Block RIP,” see, e. [sent-90, score-0.123]
</p><p>22 , [10]) of the celebrated Restricted Isometry property for the sensing matrix A, introduced by Cand´ s e and Tao [5], or on a block analogy [18] of the Restricted Eigenvalue property from [3]. [sent-92, score-0.335]
</p><p>23 Contributions of this work To the best of our knowledge, the conditions used when studying theoretical properties of block-sparse recovery (with a notable exception of the Mutual Block Incoherence condition of [9]) are unveriﬁable. [sent-93, score-0.422]
</p><p>24 While the efﬁcient veriﬁability of a condition is by no means necessary for a condition to be meaningful and useful, we believe also that veriﬁability has its value and is worthy of being investigated. [sent-95, score-0.246]
</p><p>25 In particular, the veriﬁability property allows us to design new recovery routines with explicit conﬁdence bounds for the recovery error and optimize these bounds with respect to the method parameters. [sent-96, score-0.791]
</p><p>26 Thus, the major novelty in what follows is the emphasis on veriﬁable conditions on A and the representation structure which guarantee good recovery of Bx from noisy observations of Ax, provided that Bx is nearly s-block-sparse, and the observation noise is low. [sent-97, score-0.605]
</p><p>27 In this respect, this work extends the results of [15, 13, 14], where 1 -recovery of the “usual” sparse vectors was considered (in the ﬁrst two papers – in the case of uncertain-but-bounded observation errors, and in the third – in the case of Gaussian observation noise). [sent-98, score-0.112]
</p><p>28 We propose new routines of block-sparse recovery which explicitly utilize the veriﬁability certiﬁcate – the contrast matrix, and show how these routines may be tuned to attain the best performance bounds. [sent-99, score-0.64]
</p><p>29 3 we introduce the recovery routines and provide the bounds for their risks. [sent-103, score-0.491]
</p><p>30 1 we show how one can efﬁciently verify (the strongest from the family Qs,q ) condition Qs,∞ . [sent-106, score-0.15]
</p><p>31 2 we provide an oracle inequality which shows that the condition Qs,∞ is also necessary for recovery of block-sparse signals in ∞ -norm. [sent-108, score-0.507]
</p><p>32 1  Accuracy bounds for  1 -recovery  routines  Problem statement and notations  Let w = Bx ∈ W = Rn1 × . [sent-110, score-0.233]
</p><p>33 For w ∈ W, we call the number w[k] r the magnitude of k-th block in w, and denote by ws the representation vector obtained from w by zeroing out all but the s largest in magnitude blocks in w (with the ties resolved arbitrarily). [sent-125, score-0.28]
</p><p>34 ; w[K] r ], so that L[p,r] (·) is a norm on W with the conjugate p norm L∗ (w) = [ w[1] (r∗ ) ; . [sent-129, score-0.126]
</p><p>35 Given a positive integer s ≤ K, we [p,r] s set Ls,[p,r] (w) = L[p,r] (w ); note that Ls,[p,r] (·) is a norm on W. [sent-133, score-0.138]
</p><p>36 When the representation structure B of x (and thus the norm · r ) is ﬁxed, we use the notation Lp , L∗ , and Ls,p , instead of L[p,r] , p L∗ , and Ls,[p,r] . [sent-134, score-0.187]
</p><p>37 [p,r] The recovery problem we are interested in is as follows: suppose we are given an indirect observation (cf (1)) y = Ax + u + Dξ of unknown signal x ∈ Rn . [sent-135, score-0.359]
</p><p>38 Here A ∈ Rm×n , u + Dξ is the observation error; in this error, u is an unknown nuisance known to belong to a given compact convex set U ⊂ Rm symmetric w. [sent-136, score-0.141]
</p><p>39 We want to recover x and the representation w = Bx of x, knowing in advance that this representation is nearly s-block-sparse, for some given s. [sent-140, score-0.267]
</p><p>40 A recovery routine is a Borel function x(y) : Rm → Rn and we characterize the performance of such a routine by its Lp -risk of recovery w(y) = Bx(y) of w = Bx: Riskp (w(·)|s, D, υ, ) = inf {δ : Probξ {Lp (w(y) − w) ≤ δ ∀(u ∈ U, x ∈ X(s, υ))} ≥ 1 − } . [sent-142, score-0.616]
</p><p>41 2  Condition Qs,q (κ)  Let a sensing matrix A and a representation structure B = (B, n1 , . [sent-146, score-0.336]
</p><p>42 We say that a pair (H, · ), where H ∈ Rm×M and · is a norm on RM , satisﬁes the condition Qs,q (κ) associated with the matrix A and B, if 1  1  ∀x ∈ Rn : Ls,q (Bx) ≤ s q H T Ax + κs q −1 L1 (Bx). [sent-150, score-0.247]
</p><p>43 1 Given A and a representation structure B, let (H, · ) satisfy Qs,q (κ). [sent-153, score-0.192]
</p><p>44 , nK , · r ) is the standard representation structure, meaning that B is the identity matrix, n1 = 1, and · r = | · |, the condition Qs,q (κ) reduces to the condition Hs,q (κ) introduced in [14]. [sent-159, score-0.325]
</p><p>45 3  1 -Recovery  Routines  We consider two block-sparse recovery routines. [sent-161, score-0.258]
</p><p>46 Regular  1  recovery  is given by xreg (y) ∈ Argmin L1 (Bz) : H T (Az − y) ≤ ρ , z  where H ∈ Rm×M , · and ρ > 0 are parameters of the construction. [sent-162, score-0.258]
</p><p>47 (H, · ) satisﬁes the condition Qs,q (κ) associated with the matrix A and the representation structure B; B. [sent-167, score-0.308]
</p><p>48 1 − 2κ  recovery is xpen (y) ∈ Argmin L1 (Bz) + 2s H T (Az − y)  (4)  ,  z  where H ∈ Rm×M , · and a positive integer s are parameters of the construction. [sent-170, score-0.333]
</p><p>49 The accuracy of the penalized recovery is given by the following analogue of Theorem 2. [sent-171, score-0.309]
</p><p>50 Assume that the parameters H, · , s of the penalized recovery and a ρ ≥ 0 satisfy conditions A, B from Theorem 2. [sent-175, score-0.389]
</p><p>51 4  3  Evaluating Condition Qs,∞ (κ)  The condition Qs,q (κ), of Section 2. [sent-179, score-0.123]
</p><p>52 2, is closely related to known conditions, introduced to study the properties of recovery routines in the context of block-sparsity. [sent-180, score-0.449]
</p><p>53 Let us consider the representation structure with B = In . [sent-181, score-0.124]
</p><p>54 ) Let H satisfy Qs,q (κ) and let λ be the maximum of the Euclidean norms of columns in H. [sent-183, score-0.116]
</p><p>55 For example, the error bounds for Lasso recovery, obtained in [18] rely upon the Restricted Eigenvalue assumption RE(s, κ) which is as follows: there is κ > 0 such that 1 L2 (xs ) ≤ Ax 2 whenever 3L1 (xs ) ≥ L1 (x − xs ). [sent-186, score-0.126]
</p><p>56 κ √ √ Hence, Ls,1 (x) ≤ sLs,2 (x) ≤ κs Ax 2 whenever 4Ls,1 (x) ≥ L1 (x), so that s1/2 1 Ax 2 + L1 (x) κ 4 (observe that (6) is nothing but the “block version” of the Compatibility condition from [4]). [sent-187, score-0.223]
</p><p>57 ∀x ∈ Rn : Ls,1 (x) ≤  (6)  The bad news is that, in general, condition Qs,q (κ), as well as RE and Compatibility conditions, cannot be veriﬁed. [sent-188, score-0.123]
</p><p>58 The good news is that when · r is the uniform norm · ∞ and, in addition, q = ∞ the condition Qs,q (κ) becomes “fully computationally tractable”. [sent-190, score-0.186]
</p><p>59 1 We intend to demonstrate also that this condition Qs,∞ (κ) is in fact necessary for the bounds of the form (4), (5) to be valid when p = ∞. [sent-191, score-0.165]
</p><p>60 1  Condition Qs,∞ (κ), case r = ∞: tractability  Consider the case of the representation structure B∞ = (B, n1 , . [sent-193, score-0.124]
</p><p>61 We have the following  for all k ≤ K, and let a positive integer s and reals κ > 0,  (i) Assume that a triple (H, · , ρ), where H ∈ RM ×m , · is a norm on RM , and ρ ≥ 0, is such that (! [sent-200, score-0.272]
</p><p>62 , hN in Rm and N ×N block matrix V = [V k ]K =1 k, (the blocks V k of V are nk × n matrices) such that (a) B = V B + [h1 , . [sent-208, score-0.604]
</p><p>63 , hN ]T A, (b) V k ∞,∞ ≤ s−1 κ ∀k, ≤ K (here V k ∞,∞ = max1≤j≤n Rowj (V k ) 1 , Rowj (M ) being the j-th row of M ),(7) (c)  Prob Ξ+ := {ξ : max uT hi + |(Dξ)T hi | ≤ ρ, 1 ≤ i ≤ N } u∈U  ≥1− . [sent-211, score-0.345]
</p><p>64 , hN ∈ Rm and a matrix V = [V k ]K =1 satisfy (7), the m × N matrix k, H = [h1 , . [sent-215, score-0.161]
</p><p>65 , hN ], the norm · ∞ on RN and ρ form a triple satisfying (! [sent-218, score-0.168]
</p><p>66 1, q = ∞ corresponds to the strongest among the conditions Qs,q (κ) associated with A and a given representation structure B and ensures the validity of the bounds (4) and (5) in the largest possible range, 1 ≤ p ≤ ∞, of values of p. [sent-221, score-0.262]
</p><p>67 Let a sensing matrix A ∈ Rm×n and a representation structure B∞ be given, along with a positive integer s, an uncertainty set U, and quantities D and . [sent-223, score-0.411]
</p><p>68 1 states that when applying this result, we lose nothing by restricting ourselves with triples H = [h1 , . [sent-228, score-0.095]
</p><p>69 + nK , · = · ∞ on RN , ρ ≥ 0 which can be augmented by an appropriately chosen N × N matrix V to satisfy relations (7). [sent-234, score-0.132]
</p><p>70 Note that the restriction u∈U  Prob Ξ+ = ξ : µU (hi ) + |(Dξ)T hi | ≤ ρ, 1 ≤ i ≤ N  ≥1− ,  (9)  implies that ρ ≥ max  1≤i≤N  µU (hi ) + erﬁnv  2  DT hi  2  ,  where erﬁnv(·) is the inverse error function2 , and it is implied by ρ ≥ µU (hi ) + erﬁnv  DT hi  2N  2  , 1 ≤ i ≤ N. [sent-243, score-0.491]
</p><p>71 , hN ] ∈ Rm×N , V = [V k ∈ Rnk ×n ]K k, B = BV + H T A, V k max uT hi + erﬁnv  2N  u∈U  ∞,∞  ≤  DT hi  =1  s. [sent-248, score-0.345]
</p><p>72 2  Let the representation structure B∞ = (B, n1 , . [sent-251, score-0.124]
</p><p>73 , hN ] ∈ Rm×N and V = [V k ∈ Rnk ×n ]K =1 satisfying (7), then regular 1 -recovery with appropriate choice of k, parameters ensures that 2ρ + s−1 υ Risk∞ (Bxreg |s, D, υ, ) ≤ . [sent-258, score-0.092]
</p><p>74 2 Let a sensing matrix A, an uncertainty set U, and reals κ > 0, ∈ (0, 1) be given. [sent-263, score-0.247]
</p><p>75 Given a positive integer S, assume that there exists a recovering routine x satisfying an error bound of the form (11), namely, ∀(x ∈ Rn , u ∈ U) : Probξ { B[x(y) − x]  ∞  ≤ α + S −1 L1 (Bx − [Bx]S )} ≥ 1 − . [sent-265, score-0.16]
</p><p>76 The latter exactly means that the exhibited H satisﬁes the condition Qs,∞ (κ) (see Proposition 3. [sent-271, score-0.123]
</p><p>77 , hk ], ρ satisfy conditions (10) 2 erﬁnv( ) (and thus – condition B of Theorem 2. [sent-276, score-0.23]
</p><p>78 erﬁnv( 2 )  Condition Qs,∞ (κ), case r = 2: a veriﬁable sufﬁcient condition  In this section we consider the case of the representation structure B2 = (B, n1 , . [sent-280, score-0.247]
</p><p>79 , nK , · veriﬁable sufﬁcient condition for Qs,∞ (κ) is given by the following  2 ). [sent-283, score-0.123]
</p><p>80 3 Let a sensing matrix A, a representation structure B2 be given. [sent-285, score-0.336]
</p><p>81 + nK , and let N × N matrix V = [V k ]K =1 (V k are nk × n ) and m × N matrix H satisfy the k, relation B = V B + H T A. [sent-289, score-0.532]
</p><p>82 (15)  Suppose that the matrix A, the representation structure B2 , the uncertainty set U, and the parameters D, are given. [sent-292, score-0.185]
</p><p>83 Let us assume that the triple H, · = L∞ (·), and ρ can be augmented by an appropriately chosen block N × N matrix V to satisfy the system of convex constraints (13), (14). [sent-293, score-0.325]
</p><p>84 Our objective now is to synthesize the matrix H = [H k ∈ Rm×nk ]K which satisﬁes the k=1 relationship (3) with “as good as possible” value of ρ. [sent-294, score-0.098]
</p><p>85 Note that the nk 2 distribution of (H k )T Dξ 2 coincides with that of the random variable ζk = k=1 vi [k]ηi , where 2 2 2 η, . [sent-296, score-0.342]
</p><p>86 To bound the deviation probabilities for ζk we use the bound of [6] for the deviation of the weighted χ2 : nk 2 vi [k]ηi ≥ v[k]  Prob  1  +  √  2 v[k] 2 τ  i=1  7  ≤ 2 exp −  τ2 4 v[k] 2 + 4τ v[k] 2  . [sent-305, score-0.342]
</p><p>87 Then the chance constraint u∈U  Prob ξ : µU (H k ) + (Dξ)T H[k]  2  ≤ ρ, 1 ≤ k ≤ K ≥ 1 − ,  is satisﬁed for ρ ≥ max µU (H k ) + DT H[k] k  F  + σmax (DT H[k]) 4 ln(2K  −1 )  +2  nk ln(2K  −1 )  (here · F stands for the Frobenius norm). [sent-308, score-0.395]
</p><p>88 We have mentioned in Introduction that, to the best of our knowledge, the only previously proposed veriﬁable sufﬁcient condition for the validity of block 1 recovery is the Mutual Block Incoherence condition [9]. [sent-310, score-0.655]
</p><p>89 We aim now to demonstrate that this condition is covered by Proposition 3. [sent-311, score-0.123]
</p><p>90 The Mutual Block Incoherence condition deals with the case where B = I and all block norms are · 2 -norms. [sent-313, score-0.294]
</p><p>91 Let the sensing matrix A in question be partitioned as A = [A[1], . [sent-314, score-0.212]
</p><p>92 the representation structure in question as follows: µ=  −1 max σmax Ck AT [k]A[ ] ,  1≤k, ≤K, k=  [Ck := AT [k]A[k]]  (16)  provided that all matrices Ck , 1 ≤ k ≤ K, are nonsingular, otherwise µ = ∞. [sent-324, score-0.177]
</p><p>93 Note that in the case of the standard representation structure, the just deﬁned quantity is nothing but the standard mutual incoherence known from the Compressed Sensing literature. [sent-325, score-0.24]
</p><p>94 4 Given m × n sensing matrix A and a representation structure B2 with B = I, 1 ≤ k ≤ K, let A = [A[1], . [sent-328, score-0.365]
</p><p>95 (17)  Then the contrast matrix H along with the matrix V = I − H T A satisﬁes condition (13) (where µs B = I) and condition (14) with ν ∗ (V ) ≤ 1+µ . [sent-337, score-0.368]
</p><p>96 4 essentially covers the results of [9] where the authors prove, under a condition which is marginally stronger than that of (18), that an appropriate version of block- 1 recovery allows to recover exactly every block-sparse signal from the noiseless observation y = Ax. [sent-341, score-0.519]
</p><p>97 Consistency of the group lasso and multiple kernel learning. [sent-344, score-0.098]
</p><p>98 Robust recovery of signals from a structured union of subspaces. [sent-416, score-0.336]
</p><p>99 Veriﬁable conditions of 1 recovery ¸ for sparse signals with sign restrictions. [sent-444, score-0.377]
</p><p>100 On veriﬁable sufﬁcient conditions for sparse signal recovery via 1 minimization. [sent-468, score-0.344]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bx', 0.382), ('nk', 0.342), ('recovery', 0.258), ('qs', 0.254), ('rnk', 0.234), ('nv', 0.224), ('rm', 0.205), ('routines', 0.191), ('prob', 0.191), ('sensing', 0.151), ('hi', 0.146), ('ax', 0.134), ('condition', 0.123), ('block', 0.123), ('riskp', 0.121), ('hn', 0.112), ('veri', 0.098), ('rn', 0.093), ('representation', 0.079), ('blocks', 0.078), ('signals', 0.078), ('az', 0.077), ('dantzig', 0.077), ('proposition', 0.075), ('integer', 0.075), ('bxreg', 0.073), ('dt', 0.072), ('er', 0.072), ('triple', 0.07), ('norm', 0.063), ('juditsky', 0.062), ('matrix', 0.061), ('ut', 0.06), ('lasso', 0.059), ('nuisance', 0.059), ('im', 0.058), ('regular', 0.057), ('observation', 0.056), ('ls', 0.056), ('incoherence', 0.055), ('mutual', 0.054), ('compressed', 0.053), ('ck', 0.053), ('max', 0.053), ('nothing', 0.052), ('penalized', 0.051), ('routine', 0.05), ('satis', 0.049), ('norms', 0.048), ('oracle', 0.048), ('bz', 0.048), ('rowj', 0.048), ('whenever', 0.048), ('structure', 0.045), ('safely', 0.045), ('signal', 0.045), ('ln', 0.044), ('nearly', 0.044), ('triples', 0.043), ('av', 0.042), ('bounds', 0.042), ('conditions', 0.041), ('satisfy', 0.039), ('boris', 0.039), ('eldar', 0.039), ('group', 0.039), ('recover', 0.037), ('georgia', 0.037), ('synthesize', 0.037), ('bv', 0.037), ('xs', 0.036), ('satisfying', 0.035), ('reals', 0.035), ('gs', 0.033), ('appropriately', 0.032), ('admissible', 0.032), ('selector', 0.032), ('kd', 0.031), ('tv', 0.031), ('lp', 0.031), ('emphasis', 0.03), ('rd', 0.03), ('usual', 0.03), ('let', 0.029), ('es', 0.028), ('advance', 0.028), ('validity', 0.028), ('plain', 0.028), ('cf', 0.028), ('perturbations', 0.028), ('noise', 0.027), ('hk', 0.027), ('argmin', 0.027), ('strongest', 0.027), ('belong', 0.026), ('compatibility', 0.026), ('noisy', 0.025), ('intensity', 0.024), ('namely', 0.024), ('singular', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="203-tfidf-1" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>2 0.18771844 <a title="203-tfidf-2" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>Author: Alexandra Carpentier, Odalric-ambrym Maillard, Rémi Munos</p><p>Abstract: We consider the problem of recovering the parameter α ∈ RK of a sparse function f (i.e. the number of non-zero entries of α is small compared to the number K of features) given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomization process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven, independently on the number of sampling points N , even when the features are arbitrarily non-orthogonal. Under the assumption that f is H¨ lder continuous with exponent at least √ we proo 1/2, vide an estimate α of the parameter such that �α − α�2 = O(�η�2 / N ), where � � η is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method. 1</p><p>3 0.16813293 <a title="203-tfidf-3" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, Hisashi Kashima</p><p>Abstract: We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.</p><p>4 0.12866329 <a title="203-tfidf-4" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><p>5 0.10318542 <a title="203-tfidf-5" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>Author: Andrew E. Waters, Aswin C. Sankaranarayanan, Richard Baraniuk</p><p>Abstract: We consider the problem of recovering a matrix M that is the sum of a low-rank matrix L and a sparse matrix S from a small set of linear measurements of the form y = A(M) = A(L + S). This model subsumes three important classes of signal recovery problems: compressive sensing, afﬁne rank minimization, and robust principal component analysis. We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it. Empirically, SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efﬁcient implementation. Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efﬁcacy of the algorithm. 1</p><p>6 0.10102165 <a title="203-tfidf-6" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>7 0.099430293 <a title="203-tfidf-7" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>8 0.097351253 <a title="203-tfidf-8" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>9 0.09682183 <a title="203-tfidf-9" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>10 0.089647926 <a title="203-tfidf-10" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>11 0.088033997 <a title="203-tfidf-11" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>12 0.086788379 <a title="203-tfidf-12" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>13 0.083935656 <a title="203-tfidf-13" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>14 0.081381522 <a title="203-tfidf-14" href="./nips-2011-The_Fast_Convergence_of_Boosting.html">282 nips-2011-The Fast Convergence of Boosting</a></p>
<p>15 0.065174431 <a title="203-tfidf-15" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>16 0.063515037 <a title="203-tfidf-16" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>17 0.060846705 <a title="203-tfidf-17" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>18 0.059887841 <a title="203-tfidf-18" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>19 0.058520224 <a title="203-tfidf-19" href="./nips-2011-Collective_Graphical_Models.html">55 nips-2011-Collective Graphical Models</a></p>
<p>20 0.057310373 <a title="203-tfidf-20" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, -0.006), (2, -0.038), (3, -0.167), (4, -0.083), (5, 0.077), (6, -0.003), (7, 0.046), (8, 0.059), (9, 0.06), (10, 0.042), (11, 0.012), (12, -0.08), (13, 0.047), (14, 0.08), (15, -0.174), (16, 0.073), (17, -0.116), (18, -0.06), (19, 0.054), (20, 0.047), (21, 0.05), (22, -0.036), (23, -0.11), (24, 0.007), (25, 0.006), (26, -0.065), (27, -0.048), (28, 0.072), (29, 0.021), (30, 0.036), (31, 0.066), (32, 0.067), (33, -0.066), (34, 0.079), (35, -0.113), (36, -0.053), (37, -0.035), (38, 0.109), (39, -0.03), (40, -0.047), (41, 0.016), (42, 0.073), (43, -0.033), (44, 0.034), (45, 0.023), (46, -0.025), (47, 0.035), (48, 0.013), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95771426 <a title="203-lsi-1" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>2 0.85168707 <a title="203-lsi-2" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>Author: Alexandra Carpentier, Odalric-ambrym Maillard, Rémi Munos</p><p>Abstract: We consider the problem of recovering the parameter α ∈ RK of a sparse function f (i.e. the number of non-zero entries of α is small compared to the number K of features) given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomization process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven, independently on the number of sampling points N , even when the features are arbitrarily non-orthogonal. Under the assumption that f is H¨ lder continuous with exponent at least √ we proo 1/2, vide an estimate α of the parameter such that �α − α�2 = O(�η�2 / N ), where � � η is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method. 1</p><p>3 0.80386806 <a title="203-lsi-3" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>Author: Andrew E. Waters, Aswin C. Sankaranarayanan, Richard Baraniuk</p><p>Abstract: We consider the problem of recovering a matrix M that is the sum of a low-rank matrix L and a sparse matrix S from a small set of linear measurements of the form y = A(M) = A(L + S). This model subsumes three important classes of signal recovery problems: compressive sensing, afﬁne rank minimization, and robust principal component analysis. We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it. Empirically, SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efﬁcient implementation. Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efﬁcacy of the algorithm. 1</p><p>4 0.79961818 <a title="203-lsi-4" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>5 0.79635489 <a title="203-lsi-5" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>Author: Yi-kai Liu</p><p>Abstract: We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefﬁcients. We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rankr restricted isometry property (RIP). This implies that M can be recovered from a ﬁxed (“universal”) set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley’s inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality. 1</p><p>6 0.73356193 <a title="203-lsi-6" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>7 0.65593129 <a title="203-lsi-7" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>8 0.65288705 <a title="203-lsi-8" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>9 0.6108613 <a title="203-lsi-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.53789729 <a title="203-lsi-10" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>11 0.52133733 <a title="203-lsi-11" href="./nips-2011-RTRMC%3A_A_Riemannian_trust-region_method_for_low-rank_matrix_completion.html">230 nips-2011-RTRMC: A Riemannian trust-region method for low-rank matrix completion</a></p>
<p>12 0.48487872 <a title="203-lsi-12" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>13 0.46678442 <a title="203-lsi-13" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<p>14 0.45923328 <a title="203-lsi-14" href="./nips-2011-A_Denoising_View_of_Matrix_Completion.html">5 nips-2011-A Denoising View of Matrix Completion</a></p>
<p>15 0.44588447 <a title="203-lsi-15" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>16 0.43805748 <a title="203-lsi-16" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>17 0.43305895 <a title="203-lsi-17" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>18 0.43216091 <a title="203-lsi-18" href="./nips-2011-Differentially_Private_M-Estimators.html">69 nips-2011-Differentially Private M-Estimators</a></p>
<p>19 0.42036667 <a title="203-lsi-19" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>20 0.40593776 <a title="203-lsi-20" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.033), (4, 0.033), (20, 0.057), (26, 0.037), (31, 0.053), (33, 0.014), (39, 0.291), (43, 0.145), (45, 0.099), (57, 0.027), (74, 0.046), (83, 0.041), (99, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80683964 <a title="203-lda-1" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>2 0.75866705 <a title="203-lda-2" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>Author: Yi-kai Liu</p><p>Abstract: We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefﬁcients. We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rankr restricted isometry property (RIP). This implies that M can be recovered from a ﬁxed (“universal”) set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley’s inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality. 1</p><p>3 0.72003728 <a title="203-lda-3" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>Author: Yan Karklin, Eero P. Simoncelli</p><p>Abstract: Efﬁcient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. The ﬁlters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal ﬁlters are localized and oriented.</p><p>4 0.69418597 <a title="203-lda-4" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>Author: Kristen Grauman, Fei Sha, Sung J. Hwang</p><p>Abstract: We introduce an approach to learn discriminative visual representations while exploiting external semantic knowledge about object category relationships. Given a hierarchical taxonomy that captures semantic similarity between the objects, we learn a corresponding tree of metrics (ToM). In this tree, we have one metric for each non-leaf node of the object hierarchy, and each metric is responsible for discriminating among its immediate subcategory children. Speciﬁcally, a Mahalanobis metric learned for a given node must satisfy the appropriate (dis)similarity constraints generated only among its subtree members’ training instances. To further exploit the semantics, we introduce a novel regularizer coupling the metrics that prefers a sparse disjoint set of features to be selected for each metric relative to its ancestor (supercategory) nodes’ metrics. Intuitively, this reﬂects that visual cues most useful to distinguish the generic classes (e.g., feline vs. canine) should be different than those cues most useful to distinguish their component ﬁne-grained classes (e.g., Persian cat vs. Siamese cat). We validate our approach with multiple image datasets using the WordNet taxonomy, show its advantages over alternative metric learning approaches, and analyze the meaning of attribute features selected by our algorithm. 1</p><p>5 0.6117847 <a title="203-lda-5" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>Author: Inderjit S. Dhillon, Pradeep K. Ravikumar, Ambuj Tewari</p><p>Abstract: Increasingly, optimization problems in machine learning, especially those arising from bigh-dimensional statistical estimation, bave a large number of variables. Modem statistical estimators developed over the past decade have statistical or sample complexity that depends only weakly on the number of parameters when there is some structore to the problem, such as sparsity. A central question is whether similar advances can be made in their computational complexity as well. In this paper, we propose strategies that indicate that such advances can indeed be made. In particular, we investigate the greedy coordinate descent algorithm, and note that performing the greedy step efficiently weakens the costly dependence on the problem size provided the solution is sparse. We then propose a snite of methods that perform these greedy steps efficiently by a reduction to nearest neighbor search. We also devise a more amenable form of greedy descent for composite non-smooth objectives; as well as several approximate variants of such greedy descent. We develop a practical implementation of our algorithm that combines greedy coordinate descent with locality sensitive hashing. Without tuning the latter data structore, we are not only able to significantly speed up the vanilla greedy method, hot also outperform cyclic descent when the problem size becomes large. Our resnlts indicate the effectiveness of our nearest neighbor strategies, and also point to many open questions regarding the development of computational geometric techniques tailored towards first-order optimization methods.</p><p>6 0.58476883 <a title="203-lda-6" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>7 0.5690465 <a title="203-lda-7" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>8 0.56585526 <a title="203-lda-8" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>9 0.56413949 <a title="203-lda-9" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>10 0.56121367 <a title="203-lda-10" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>11 0.56014138 <a title="203-lda-11" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>12 0.55814099 <a title="203-lda-12" href="./nips-2011-Bayesian_Spike-Triggered_Covariance_Analysis.html">44 nips-2011-Bayesian Spike-Triggered Covariance Analysis</a></p>
<p>13 0.55801946 <a title="203-lda-13" href="./nips-2011-The_Fast_Convergence_of_Boosting.html">282 nips-2011-The Fast Convergence of Boosting</a></p>
<p>14 0.55633593 <a title="203-lda-14" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>15 0.55329823 <a title="203-lda-15" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>16 0.55252522 <a title="203-lda-16" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>17 0.55206525 <a title="203-lda-17" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>18 0.55055308 <a title="203-lda-18" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>19 0.54753047 <a title="203-lda-19" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>20 0.54635406 <a title="203-lda-20" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
