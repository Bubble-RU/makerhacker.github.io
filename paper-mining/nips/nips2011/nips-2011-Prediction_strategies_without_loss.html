<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 nips-2011-Prediction strategies without loss</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-220" href="#">nips2011-220</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>220 nips-2011-Prediction strategies without loss</h1>
<br/><p>Source: <a title="nips-2011-220-pdf" href="http://papers.nips.cc/paper/4388-prediction-strategies-without-loss.pdf">pdf</a></p><p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>Reference: <a title="nips-2011-220-reference" href="../nips2011_reference/nips-2011-Prediction_strategies_without_loss_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Consider a sequence of bits where we are trying to predict the next bit from the previous bits. [sent-3, score-0.254]
</p><p>2 Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. [sent-4, score-0.418]
</p><p>3 In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. [sent-6, score-0.869]
</p><p>4 For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. [sent-7, score-0.672]
</p><p>5 We show that the tradeoff between loss and regret is optimal up to constant factors. [sent-8, score-0.681]
</p><p>6 Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. [sent-9, score-1.356]
</p><p>7 We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. [sent-11, score-0.451]
</p><p>8 First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i. [sent-13, score-0.769]
</p><p>9 Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. [sent-16, score-1.228]
</p><p>10 1  Introduction  Consider a gambler who is trying to predict the next bit in a sequence of bits. [sent-17, score-0.323]
</p><p>11 One could think of the bits as indications of whether a stock price goes up or down on a given day, where we assume that the stock always goes up or down by 1 (this is, of course, a very simpliﬁed model of the stock market). [sent-18, score-0.451]
</p><p>12 that the stock will go up), she buys one stock to sell it the next day, and short sells one stock if her prediction is 0. [sent-21, score-0.405]
</p><p>13 If the prediction is right the gambler gets a payoff of c otherwise −c. [sent-23, score-0.556]
</p><p>14 Clearly there is the strategy of never predicting (by setting conﬁdence 0) all the time that never has a loss but also never has a positive payoff. [sent-26, score-0.408]
</p><p>15 However, if the sequence is very imbalanced and has many more 0’s than 1’s then this never predict strategy has a high regret with respect to the strategy that predicts the majority bit. [sent-27, score-0.963]
</p><p>16 Thus, one is interested in a strategy that has a small regret with respect to predicting the majority bit and incurs no loss at the same time. [sent-28, score-1.005]
</p><p>17 More precisely, we show that for any > 1/ T 1  2 √ there exists an algorithm that achieves regret at most 14 T and loss at most 2e− T T , where T is the time horizon. [sent-30, score-0.625]
</p><p>18 The bit prediction problem can be cast as the experts problem with two experts: S+ , that always predicts 1 and S− that always predicts 0. [sent-32, score-0.544]
</p><p>19 The weighted majority algorithm of [12] is known to give optimal √ regret guarantees. [sent-34, score-0.644]
</p><p>20 However, it can be seen that weighted majority may result in a loss of Ω( T ). [sent-35, score-0.249]
</p><p>21 [7] on the problem of trading off regret to the best expert for regret to the average expert, which is equivalent to our problem. [sent-37, score-1.168]
</p><p>22 √ Stated as a result on bounding loss, they were able to obtain a constant loss and regret O( √ log T ). [sent-38, score-0.663]
</p><p>23 T Their work left the question open as to whether it is possible to even get a regret of O( T log T ) and constant loss. [sent-39, score-0.537]
</p><p>24 In this paper we give an optimal regret/loss tradeoff, in particular showing that this regret can be achieved even with subconstant loss. [sent-40, score-0.489]
</p><p>25 Our results extend to the general setting of prediction with expert advice when there are multiple experts. [sent-41, score-0.263]
</p><p>26 In this problem the decision maker iteratively chooses among N available alternatives without knowledge of their payoffs, and gets payoff based on the chosen alternative. [sent-42, score-0.422]
</p><p>27 This process is repeated over T rounds, and the goal of the decision maker is to maximize her cumulative payoff over all time steps t = 1, . [sent-44, score-0.398]
</p><p>28 The most widely used measure of performance of an online decision making algorithm is regret, which is deﬁned as the difference between the payoff of the best ﬁxed alternative and the payoff of the algorithm. [sent-51, score-0.764]
</p><p>29 The well-known weighted majority √ algorithm of [12] obtains regret O( T log N ) even when no assumptions are made on the process generating the payoff. [sent-52, score-0.692]
</p><p>30 Regret to the best ﬁxed alternative in hindsight is a very natural notion when the payoffs are sampled from an unknown distribution, and in fact such scenarios show that the √ bound of O( T log N ) on regret achieved by the weighted majority algorithm is optimal. [sent-53, score-0.812]
</p><p>31 [7] gave an√ algorithm that has constant regret to any ﬁxed distribution on the experts at the expense of regret O( T log N (log T + log log N )) with respect to all other experts1 . [sent-55, score-1.37]
</p><p>32 We obtain an optimal tradeoff between the two, getting an algorithm with regret O( T (log N + log T )) to the best and O((N T )−Ω(1) ) to the average as a special case. [sent-56, score-0.729]
</p><p>33 The strong loss bounds of our algorithm allow us to achieve lossless boosting, i. [sent-59, score-0.272]
</p><p>34 we use available expert to continuously improve upon the performance of the base expert whenever possible while essentially never hurting its performance. [sent-61, score-0.534]
</p><p>35 This property allows us to easily obtain optimal adaptive regret bounds, i. [sent-66, score-0.535]
</p><p>36 For the bit prediction problem one would set N = 2 and use the uniform distribution over the ‘predict 0’ and ‘predict 1’ strategy as the special distribution. [sent-69, score-0.311]
</p><p>37 2  worse than the payoff of the strategy that is best in that window (see Theorem 11). [sent-72, score-0.53]
</p><p>38 In the full version of the paper ([11]) we also obtain bounds against the class of strategies that are allowed to change experts multiple times while maintaining the essentially zero loss property. [sent-73, score-0.523]
</p><p>39 In the full version of the paper, we also show how our algorithm yields regret bounds that depend on the lp norm of the costs, regret bounds dependent on Kolmogorov complexity as well as applications of our framework to multi-armed bandits with partial information and online convex optimization. [sent-75, score-1.074]
</p><p>40 Tradeoffs between regret and loss were also examined in [13], where the author studied the set of values of a, b for which an algorithm can have payoff aOP T + b log N , where OP T is the payoff of the best arm and a, b are constants. [sent-78, score-1.426]
</p><p>41 The problem of bit prediction was also considered in [8], where several loss functions are considered. [sent-79, score-0.32]
</p><p>42 In recent work on the NormalHedge algorithm[4] the authors use a potential function which is very similar to our function g(x) (see (2) below), getting strong regret guarantees to the -quantile of best experts. [sent-81, score-0.54]
</p><p>43 It will be convenient to adopt the convention that bt ∈ {−1, +1} instead of bt ∈ {0, 1} since it simpliﬁes the formula for the payoff. [sent-89, score-0.696]
</p><p>44 In fact, in what follows we will only assume that −1 ≤ bt ≤ 1, allowing bt to be real numbers. [sent-90, score-0.696]
</p><p>45 , T the algorithm is required to output a conﬁdence level ft ∈ [−1, 1], and then the value of bt is revealed to it. [sent-94, score-0.446]
</p><p>46 The payoff of the algorithm by time t is t  At =  ft bt . [sent-95, score-0.789]
</p><p>47 (1)  t=1  For example, if bt ∈ {−1, +1}, then this setup is analogous to a prediction process in which a player observes a sequence of bits and at each point in time predicts that the value of the next bit will be sign(ft ) with conﬁdence |ft |. [sent-96, score-0.701]
</p><p>48 We deﬁne the loss of the algorithm on a string b as loss = min{−At , 0}, i. [sent-98, score-0.336]
</p><p>49 the absolute value of the smallest negative payoff over all time steps. [sent-100, score-0.343]
</p><p>50 It is easy to see that any algorithm that has a positive expected payoff on some sequence necessarily loses on another sequence. [sent-101, score-0.422]
</p><p>51 Thus, we are concerned with ﬁnding a prediction strategy that has exponentially small loss bounds but also has low regret against a number of given prediction strategies. [sent-102, score-0.929]
</p><p>52 In the simplest setting we would like to design an algorithm that has low regret against two basic strategies: S+ , which always predicts +1 and S− , which always predicts −1. [sent-103, score-0.685]
</p><p>53 Note that the T maximum of the payoffs of S+ and S− is always equal to t=1 bt . [sent-104, score-0.472]
</p><p>54 In what follows we will use the notation AT for the cumulative payoff of the algorithm by time T as deﬁned above. [sent-106, score-0.375]
</p><p>55 As we will show in section 3, our techniques extend easily to give an algorithm that has low regret with respect to the best of any N bit prediction strategies and exponentially small loss. [sent-107, score-0.911]
</p><p>56 Our techniques work for the general experts problem, where loss corresponds to regret with respect to the ’special’ expert S0 , and hence we give the proof in this setting. [sent-108, score-1.021]
</p><p>57 In section 2 we give an algorithm for the case of two prediction strategies S+ and S− , and in section 3 we extend it to the general experts problem, additionally giving the claimed adaptive regret bounds. [sent-110, score-0.887]
</p><p>58 the algorithm has at most 14 T regret against S+ and S− as well as a exponentially small loss. [sent-113, score-0.543]
</p><p>59 √ By setting so that the loss bound is 2Z T , we get a regret bound of T log(1/Z). [sent-114, score-0.593]
</p><p>60 We note that the algorithm is a strict generalization of weighted majority, which can be seen by letting Z = Θ(1) (this property will also hold for the generalization to N experts in section 3). [sent-115, score-0.273]
</p><p>61 For a chosen discount factor ρ = 1 − 1/n, 0 ≤ ρ ≤ 1 t−1 the algorithm maintains a discounted deviation xt = j=1 ρt−1−j bj at each time t = 1, . [sent-117, score-0.596]
</p><p>62 The value of the prediction at time t is then given by g(xt ) for a function g(·) to be deﬁned (note that xt depends only on bt for t < t, so this is an online algorithm). [sent-121, score-0.882]
</p><p>63 The function g as well as the discount factor ρ depend on the desired bound on expected loss and regret against S+ and S− . [sent-122, score-0.614]
</p><p>64 In 1 particular, we will set ρ = 1 − T for our main result on regret/loss tradeoff, and will use the freedom to choose different values of ρ to obtain adaptive regret guarantees in section 3. [sent-123, score-0.524]
</p><p>65 In particular, we will choose the conﬁdence function g(x) so that xt  Φt =  g(s)ds. [sent-128, score-0.459]
</p><p>66 In particular, we will choose g(x) so that the change of Φt lower bounds the payoff of the algorithm. [sent-130, score-0.397]
</p><p>67 If we let Φt = G(xt ) (assuming for sake of clarity that xt > 0), where x  G(x) =  g(s)ds, 0  we have Φt+1 − Φt = G(xt+1 ) − G(xt ) ≈ G (x)∆x + G (x)∆x2 /2 ≈ g(x) [(ρ − 1)x + bt ] + g (x)/2. [sent-131, score-0.807]
</p><p>68 Since the payoff of the algorithm at time step t is g(xt )bt , we have ∆Φt − g(xt )bt = −g(xt )(1 − ρ)xt + g (xt )/2, so the condition becomes −g(xt )(1 − ρ)xt + g (xt )/2 ≤ Z, where Z is the desired bound on per step loss of the algorithm. [sent-132, score-0.501]
</p><p>69 We will later choose n = T to prove Theorem 1, but we will use different value ¯ of n for the adaptive regret guarantees in section 3. [sent-144, score-0.524]
</p><p>70 Then the payoff of the algorithm is at least T  ρxt g(xt )(1 − h(x)) + ΦT +1 − Z T ¯ t=1  as long as |bt | ≤ 1 for all t. [sent-148, score-0.375]
</p><p>71 Proof: We will show that at each t Φt+1 − Φt ≤ bt g(xt ) + Z − ρxt g(xt )(1 − h(xt )), ¯ i. [sent-149, score-0.348]
</p><p>72 T  T  bt g(xt ) ≥ −Z T + t=1  ρxt g(xt )(1 − h(xt )) + ΦT +1 − Φ1 , ¯ t=1  thus implying the claim of the lemma since Φ1 = 0. [sent-151, score-0.424]
</p><p>73 0 ≤ bt ≤ 1: We have xt+1 = ρxt + bt = xt − ρxt + bt , and the expected payoff of the algorithm ¯ is g(xt )bt . [sent-155, score-1.878]
</p><p>74 Then xt −ρxt +bt ¯  Φt+1 − Φt =  g(s)ds xt  1 ≤ g(xt )(bt − ρxt ) + (¯xt + bt )2 · ¯ ρ max |g (s)| 2 s∈[xt ,xt −ρxt +bt ] ¯ 1 ≤ g(xt )bt + −g(xt )¯xt + (¯xt + 1)2 · ρ ρ max |g (s)| 2 s∈[xt ,xt −ρxt +bt ] ¯ ≤ g(xt )bt + (−1 + h(xt ))¯xt g(xt ) + Z . [sent-156, score-1.304]
</p><p>75 The following lemma shows that the function g(x) satisﬁes the properties stated in Lemma 2: 5  (3)  x U  +1 tanh g(x)  −U  +U  x −1 Figure 1: The shape of the conﬁdence function g(x) (solid line) and the tanh(x) function used by weighted majority (dotted line). [sent-165, score-0.289]
</p><p>76 ¯ We can now lower bound the payoff of Algorithm 1. [sent-172, score-0.343]
</p><p>77 ¯ U U  t=1  Proof: By Lemma 3 we have that the function g(x) satisﬁes the conditions of Lemma 2, and so from the bounds stated in Lemma 2 the payoff of the algorithm is at least T  √ ρ|xt |+ + ΦT +1 − 2ZT / n. [sent-175, score-0.451]
</p><p>78  T ¯ t=1 ρxt T −1  Proof: In light of Theorem 4 it remains to bound T −1  T  ρ ¯  t  t=1  t=1 j=1  T  bt (1 − ρT −t ) +  ρt−j bj + xT +1 =  xt + xT +1 = ρ ¯  + xT +1 . [sent-178, score-0.86]
</p><p>79 Our loss/regret tradeoff is optimal up to constant factors (proof deferred to the full version): 6  √ Theorem 6 Any algorithm that has regret O( T log(1/Z)) incurs loss Ω(Z T ) on at least one sequence of bits bt , t = 1, . [sent-182, score-1.189]
</p><p>80 Note that if Z = o(1/T ), then the payoff of the algorithm is positive whenever the absolute value of √ the deviation xt is larger than, say 8 n log T in at least one window of size n. [sent-186, score-1.007]
</p><p>81 3  Combining strategies (lossless boosting)  In the previous section we derived an algorithm for the bit prediction problem with low regret to the S+ and S− strategies and exponentially small loss. [sent-187, score-0.981]
</p><p>82 We now show how our techniques yield an algorithm that has low regret to the best of N bit prediction strategies S1 , . [sent-188, score-0.837]
</p><p>83 However, since the proof works for the general experts problem, where loss corresponds to regret to a ’special’ expert S0 , we state it in the general experts setting. [sent-192, score-1.155]
</p><p>84 In what follows we will refer to regret to S0 as loss. [sent-193, score-0.467]
</p><p>85 We will also prove optimal bounds on regret that hold in every window of length n at the end of the section. [sent-194, score-0.62]
</p><p>86 We start by proving Theorem 7 For any Z < 1/e there exists an algorithm for combining N strategies that has regret √ O( T log(N/Z)) against the best of N strategies and loss at most O(ZN T ) with respect to any strategy S0 ﬁxed a priori. [sent-195, score-1.029]
</p><p>87 A prediction strategy S given a bit string bt , produces a sequence of weights N wjt on the set of experts j = 1, . [sent-198, score-0.973]
</p><p>88 , N such that wjt depends only on bt , t < t and j=1 wjt = 1, wjt ≥ 0 for all t. [sent-201, score-0.588]
</p><p>89 Thus, using strategy S amounts to using expert j with probability wj,t at time t, for all t = 1, . [sent-202, score-0.307]
</p><p>90 For a strategy S we denote its payoff at time t by st . [sent-207, score-0.451]
</p><p>91 Our algorithm will consider S1 as the base strategy (corresponding to the null strategy S0 in the previous section) and will use S2 to improve on S1 whenever possible, without introducing signiﬁcant loss over S1 in the process. [sent-209, score-0.413]
</p><p>92 The intuition behind the algorithm is that since the difference in payoff obtained by using S2 instead of S1 is given by (s2,t − s1,t ), it is sufﬁcient to emulate t−1 t−1−j Algorithm 1 on this sequence. [sent-215, score-0.375]
</p><p>93 In particular, we set xt = (s2,j − s1,j ) and predict j=1 ρ 1 ¯ g (xt ) (note that since |s1,t − s2,t | ≤ 2, we need to use g( 2 x) in the deﬁnition of g to scale the ¯ payoffs). [sent-216, score-0.502]
</p><p>94 7  Lemma 8 There exists an algorithm that given two strategies S1 and S2 gets payoff at least T  T  (s2,t − s1,t ) − O  s1,t + max t=1  T log(1/Z) , 0  √ − O(Z T ). [sent-221, score-0.54]
</p><p>95 We emphasize the property that Algorithm 2 combines two strategies S1 and S2 , improving on the performance of S1 using S2 whenever possible, essentially without introducing any loss with respect to S1 . [sent-224, score-0.379]
</p><p>96 The regret and loss guarantees follow by Lemma 8. [sent-233, score-0.623]
</p><p>97 Corollary 9 Setting Z = (N T )−1−γ for γ > 0, we get regret O( γT (log N + log T )) to the best of N strategies and loss at most O((N T )−γ ) wrt strategy S0 ﬁxed a priori. [sent-234, score-0.895]
</p><p>98 Finally, we show another adaptive regret property of Algorithm 3. [sent-243, score-0.513]
</p><p>99 We now prove that the difference between the payoff of our algorithm and the payoff of any expert is Z-uniform, i. [sent-250, score-0.906]
</p><p>100 Moreover, the loss of the algorithm with respect to the o((N base strategy is at most 2ZN T . [sent-254, score-0.305]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('regret', 0.467), ('xt', 0.459), ('bt', 0.348), ('payoff', 0.343), ('expert', 0.188), ('experts', 0.164), ('loss', 0.126), ('strategies', 0.122), ('bit', 0.119), ('gambler', 0.114), ('stock', 0.11), ('payoffs', 0.098), ('strategy', 0.088), ('majority', 0.086), ('wjt', 0.08), ('erf', 0.08), ('window', 0.077), ('lemma', 0.076), ('prediction', 0.075), ('log', 0.07), ('kapralov', 0.068), ('xg', 0.068), ('predicts', 0.067), ('tradeoff', 0.066), ('ft', 0.066), ('lossless', 0.06), ('bounds', 0.054), ('predicting', 0.053), ('bj', 0.053), ('string', 0.052), ('dence', 0.05), ('imbalance', 0.049), ('never', 0.047), ('sequence', 0.047), ('boosting', 0.047), ('proof', 0.046), ('bits', 0.045), ('exponentially', 0.044), ('tanh', 0.043), ('predict', 0.043), ('weighted', 0.037), ('incurs', 0.036), ('combine', 0.036), ('theorem', 0.036), ('essentially', 0.036), ('ds', 0.033), ('rina', 0.033), ('algorithm', 0.032), ('maker', 0.031), ('amounts', 0.031), ('discounted', 0.031), ('guarantees', 0.03), ('respect', 0.03), ('freund', 0.03), ('base', 0.029), ('special', 0.029), ('odd', 0.029), ('geometrically', 0.028), ('day', 0.027), ('adaptive', 0.027), ('always', 0.026), ('arms', 0.026), ('whenever', 0.026), ('goes', 0.025), ('shape', 0.025), ('trading', 0.024), ('null', 0.024), ('decision', 0.024), ('gets', 0.024), ('con', 0.024), ('predictions', 0.024), ('wt', 0.024), ('colt', 0.023), ('arm', 0.023), ('yielding', 0.023), ('best', 0.022), ('optimal', 0.022), ('adversarial', 0.022), ('stated', 0.022), ('ex', 0.021), ('nj', 0.021), ('satis', 0.021), ('getting', 0.021), ('letting', 0.021), ('discount', 0.021), ('sign', 0.021), ('maintaining', 0.021), ('st', 0.02), ('game', 0.02), ('bandit', 0.02), ('sn', 0.02), ('panigrahy', 0.02), ('hurting', 0.02), ('tempted', 0.02), ('improving', 0.02), ('start', 0.02), ('property', 0.019), ('extensively', 0.019), ('predictors', 0.019), ('max', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="220-tfidf-1" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>2 0.35031316 <a title="220-tfidf-2" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>Author: Yasin Abbasi-yadkori, Csaba Szepesvári, David Tax</p><p>Abstract: We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller conﬁdence sets. For their construction we use a novel tail inequality for vector-valued martingales. 1</p><p>3 0.31816661 <a title="220-tfidf-3" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>Author: Wouter M. Koolen, Wojciech Kotlowski, Manfred K. Warmuth</p><p>Abstract: We extend the classical problem of predicting a sequence of outcomes from a ﬁnite alphabet to the matrix domain. In this extension, the alphabet of n outcomes is replaced by the set of all dyads, i.e. outer products uu where u is a vector in Rn of unit length. Whereas in the classical case the goal is to learn (i.e. sequentially predict as well as) the best multinomial distribution, in the matrix case we desire to learn the density matrix that best explains the observed sequence of dyads. We show how popular online algorithms for learning a multinomial distribution can be extended to learn density matrices. Intuitively, learning the n2 parameters of a density matrix is much harder than learning the n parameters of a multinomial distribution. Completely surprisingly, we prove that the worst-case regrets of certain classical algorithms and their matrix generalizations are identical. The reason is that the worst-case sequence of dyads share a common eigensystem, i.e. the worst case regret is achieved in the classical case. So these matrix algorithms learn the eigenvectors without any regret. 1</p><p>4 0.26385716 <a title="220-tfidf-4" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a ﬁxed distribution, and the adversarial scenario wherein, at every time step, an adversarially chosen instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We deﬁne the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we deﬁne a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with inﬁnite Littlestone dimension learnable. 1</p><p>5 0.25271094 <a title="220-tfidf-5" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>6 0.24388166 <a title="220-tfidf-6" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>7 0.24193618 <a title="220-tfidf-7" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>8 0.22850771 <a title="220-tfidf-8" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>9 0.20205085 <a title="220-tfidf-9" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>10 0.19756469 <a title="220-tfidf-10" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>11 0.18415262 <a title="220-tfidf-11" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>12 0.17833924 <a title="220-tfidf-12" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>13 0.16011849 <a title="220-tfidf-13" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>14 0.15703684 <a title="220-tfidf-14" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>15 0.15035516 <a title="220-tfidf-15" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>16 0.14775227 <a title="220-tfidf-16" href="./nips-2011-On_Strategy_Stitching_in_Large_Extensive_Form_Multiplayer_Games.html">196 nips-2011-On Strategy Stitching in Large Extensive Form Multiplayer Games</a></p>
<p>17 0.14099026 <a title="220-tfidf-17" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>18 0.12929882 <a title="220-tfidf-18" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>19 0.1271264 <a title="220-tfidf-19" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<p>20 0.1229978 <a title="220-tfidf-20" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.227), (1, -0.431), (2, -0.01), (3, -0.029), (4, 0.362), (5, -0.016), (6, 0.097), (7, 0.028), (8, 0.094), (9, -0.021), (10, 0.093), (11, -0.027), (12, 0.057), (13, 0.012), (14, -0.025), (15, 0.01), (16, 0.028), (17, 0.055), (18, 0.035), (19, -0.053), (20, 0.022), (21, 0.028), (22, 0.004), (23, -0.041), (24, -0.019), (25, -0.122), (26, -0.035), (27, -0.046), (28, -0.01), (29, 0.032), (30, 0.039), (31, -0.048), (32, 0.025), (33, 0.012), (34, -0.005), (35, -0.023), (36, 0.037), (37, -0.114), (38, 0.023), (39, 0.032), (40, -0.065), (41, 0.046), (42, -0.048), (43, -0.007), (44, -0.032), (45, -0.021), (46, -0.005), (47, 0.048), (48, 0.124), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9746213 <a title="220-lsi-1" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>2 0.83951598 <a title="220-lsi-2" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>3 0.81755638 <a title="220-lsi-3" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>Author: Yasin Abbasi-yadkori, Csaba Szepesvári, David Tax</p><p>Abstract: We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modiﬁcation of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modiﬁcation improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller conﬁdence sets. For their construction we use a novel tail inequality for vector-valued martingales. 1</p><p>4 0.76976746 <a title="220-lsi-4" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>Author: Wouter M. Koolen, Wojciech Kotlowski, Manfred K. Warmuth</p><p>Abstract: We extend the classical problem of predicting a sequence of outcomes from a ﬁnite alphabet to the matrix domain. In this extension, the alphabet of n outcomes is replaced by the set of all dyads, i.e. outer products uu where u is a vector in Rn of unit length. Whereas in the classical case the goal is to learn (i.e. sequentially predict as well as) the best multinomial distribution, in the matrix case we desire to learn the density matrix that best explains the observed sequence of dyads. We show how popular online algorithms for learning a multinomial distribution can be extended to learn density matrices. Intuitively, learning the n2 parameters of a density matrix is much harder than learning the n parameters of a multinomial distribution. Completely surprisingly, we prove that the worst-case regrets of certain classical algorithms and their matrix generalizations are identical. The reason is that the worst-case sequence of dyads share a common eigensystem, i.e. the worst case regret is achieved in the classical case. So these matrix algorithms learn the eigenvectors without any regret. 1</p><p>5 0.71396232 <a title="220-lsi-5" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>Author: Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a ﬁxed distribution, and the adversarial scenario wherein, at every time step, an adversarially chosen instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We deﬁne the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we deﬁne a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with inﬁnite Littlestone dimension learnable. 1</p><p>6 0.70871627 <a title="220-lsi-6" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>7 0.6989029 <a title="220-lsi-7" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>8 0.61425036 <a title="220-lsi-8" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>9 0.59335387 <a title="220-lsi-9" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>10 0.591555 <a title="220-lsi-10" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>11 0.59085196 <a title="220-lsi-11" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>12 0.58904439 <a title="220-lsi-12" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>13 0.5838185 <a title="220-lsi-13" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>14 0.57547015 <a title="220-lsi-14" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>15 0.57104528 <a title="220-lsi-15" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>16 0.48102751 <a title="220-lsi-16" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>17 0.47307289 <a title="220-lsi-17" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>18 0.45402274 <a title="220-lsi-18" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>19 0.4385519 <a title="220-lsi-19" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>20 0.41621894 <a title="220-lsi-20" href="./nips-2011-Multi-armed_bandits_on_implicit_metric_spaces.html">177 nips-2011-Multi-armed bandits on implicit metric spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (4, 0.073), (9, 0.143), (20, 0.028), (26, 0.022), (31, 0.073), (33, 0.012), (43, 0.07), (45, 0.188), (57, 0.049), (74, 0.097), (79, 0.015), (83, 0.053), (99, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94276291 <a title="220-lda-1" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>Author: Dan Zhang, Yan Liu, Luo Si, Jian Zhang, Richard D. Lawrence</p><p>Abstract: Most existing Multiple-Instance Learning (MIL) algorithms assume data instances and/or data bags are independently and identically distributed. But there often exists rich additional dependency/structure information between instances/bags within many applications of MIL. Ignoring this structure information limits the performance of existing MIL algorithms. This paper explores the research problem as multiple instance learning on structured data (MILSD) and formulates a novel framework that considers additional structure information. In particular, an effective and efﬁcient optimization algorithm has been proposed to solve the original non-convex optimization problem by using a combination of ConcaveConvex Constraint Programming (CCCP) method and an adapted Cutting Plane method, which deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data. Our method has the nice convergence property, with speciﬁed precision on each set of constraints. Experimental results on three different applications, i.e., webpage classiﬁcation, market targeting, and protein fold identiﬁcation, clearly demonstrate the advantages of the proposed method over state-of-the-art methods. 1</p><p>2 0.93790859 <a title="220-lda-2" href="./nips-2011-Differentially_Private_M-Estimators.html">69 nips-2011-Differentially Private M-Estimators</a></p>
<p>Author: Jing Lei</p><p>Abstract: This paper studies privacy preserving M-estimators using perturbed histograms. The proposed approach allows the release of a wide class of M-estimators with both differential privacy and statistical utility without knowing a priori the particular inference procedure. The performance of the proposed method is demonstrated through a careful study of the convergence rates. A practical algorithm is given and applied on a real world data set containing both continuous and categorical variables. 1</p><p>same-paper 3 0.89847767 <a title="220-lda-3" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>Author: Michael Kapralov, Rina Panigrahy</p><p>Abstract: Consider a sequence of bits where we are trying to predict the next bit from the previous bits. Assume we are allowed to say ‘predict 0’ or ‘predict 1’, and our payoff is +1 if the prediction is correct and −1 otherwise. We will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far. In this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting 0 or always predicting 1. For a sequence of length T our algorithm has regret 14 T and loss √ 2 2 T e− T in expectation for all strings. We show that the tradeoff between loss and regret is optimal up to constant factors. Our techniques extend to the general setting of N experts, where the related problem of trading off regret to the best expert for regret to the ’special’ expert has been studied by Even-Dar et al. (COLT’07). We obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff, improving upon the results of Even-Dar et al and settling the main question left open in their paper. The strong loss bounds of the algorithm have some surprising consequences. First, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to k-shifting optima, i.e. bounds with respect to the optimum that is allowed to change arms multiple times. Moreover, for any window of size n the regret of our algorithm to any expert never exceeds O( n(log N + log T )), where N is the number of experts and T is the time horizon, while maintaining the essentially zero loss property. 1</p><p>4 0.8830868 <a title="220-lda-4" href="./nips-2011-Inferring_Interaction_Networks_using_the_IBP_applied_to_microRNA_Target_Prediction.html">132 nips-2011-Inferring Interaction Networks using the IBP applied to microRNA Target Prediction</a></p>
<p>Author: Hai-son P. Le, Ziv Bar-joseph</p><p>Abstract: Determining interactions between entities and the overall organization and clustering of nodes in networks is a major challenge when analyzing biological and social network data. Here we extend the Indian Buffet Process (IBP), a nonparametric Bayesian model, to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks. We present an application of this method to study how microRNAs regulate mRNAs in cells. Analysis of synthetic and real data indicates that the method improves upon prior methods, correctly recovers interactions and clusters, and provides accurate biological predictions. 1</p><p>5 0.84966058 <a title="220-lda-5" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>6 0.8444466 <a title="220-lda-6" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>7 0.84073281 <a title="220-lda-7" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>8 0.84060085 <a title="220-lda-8" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>9 0.8391217 <a title="220-lda-9" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>10 0.83834851 <a title="220-lda-10" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>11 0.83834279 <a title="220-lda-11" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>12 0.8377282 <a title="220-lda-12" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>13 0.83596051 <a title="220-lda-13" href="./nips-2011-Shaping_Level_Sets_with_Submodular_Functions.html">251 nips-2011-Shaping Level Sets with Submodular Functions</a></p>
<p>14 0.83593959 <a title="220-lda-14" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>15 0.83500397 <a title="220-lda-15" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>16 0.83487314 <a title="220-lda-16" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>17 0.8345241 <a title="220-lda-17" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>18 0.83387929 <a title="220-lda-18" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>19 0.83327651 <a title="220-lda-19" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>20 0.83121812 <a title="220-lda-20" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
