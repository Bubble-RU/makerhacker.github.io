<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>26 nips-2011-Additive Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-26" href="#">nips2011-26</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>26 nips-2011-Additive Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2011-26-pdf" href="http://papers.nips.cc/paper/4221-additive-gaussian-processes.pdf">pdf</a></p><p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>Reference: <a title="nips-2011-26-reference" href="../nips2011_reference/nips-2011-Additive_Gaussian_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We introduce a Gaussian process model of functions which are additive. [sent-7, score-0.15]
</p><p>2 An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. [sent-8, score-0.538]
</p><p>3 Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. [sent-9, score-0.091]
</p><p>4 We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. [sent-11, score-0.818]
</p><p>5 The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. [sent-12, score-0.099]
</p><p>6 1  Introduction  Most statistical regression models in use today are of the form: g(y) = f (x1 )+f (x2 )+· · ·+f (xD ). [sent-13, score-0.161]
</p><p>7 This family of functions, known as Generalized Additive Models (GAM) [2], are typically easy to ﬁt and interpret. [sent-15, score-0.039]
</p><p>8 Some extensions of this family, such as smoothing-splines ANOVA [3], add terms depending on more than one variable. [sent-16, score-0.039]
</p><p>9 However, such models generally become intractable and difﬁcult to ﬁt as the number of terms increases. [sent-17, score-0.041]
</p><p>10 At the other end of the spectrum are kernel-based models, which typically allow the response to depend on all input variables simultaneously. [sent-18, score-0.121]
</p><p>11 A popular example would be a Gaussian process model using a squared-exponential (or Gaussian) kernel. [sent-23, score-0.09]
</p><p>12 This model is much more ﬂexible than the GAM, but its ﬂexibility makes it difﬁcult to generalize to new combinations of input variables. [sent-25, score-0.145]
</p><p>13 In this paper, we introduce a Gaussian process model that generalizes both GAMs and the SE-GP. [sent-26, score-0.128]
</p><p>14 This is achieved through a kernel which allow additive interactions of all orders, ranging from ﬁrst order interactions (as in a GAM) all the way to Dth-order interactions (as in a SE-GP). [sent-27, score-1.047]
</p><p>15 Although this kernel amounts to a sum over an exponential number of terms, we show how to compute this kernel efﬁciently, and introduce a parameterization which limits the number of hyperparameters to O(D). [sent-28, score-1.021]
</p><p>16 A Gaussian process with this kernel function (an additive GP) constitutes a powerful model that allows one to automatically determine which orders of interaction are important. [sent-29, score-0.962]
</p><p>17 We note that a similar breakthrough has recently been made, called Hierarchical Kernel Learning (HKL) [4]. [sent-32, score-0.071]
</p><p>18 HKL explores a similar class of models, and sidesteps the possibly exponential number of interaction terms by cleverly selecting only a tractable subset. [sent-33, score-0.317]
</p><p>19 However, this method suffers considerably from the fact that cross-validation must be used to set hyperparameters. [sent-34, score-0.072]
</p><p>20 In addition, the machinery necessary to train these models is immense. [sent-35, score-0.094]
</p><p>21 Finally, on real datasets, HKL is outperformed by the standard SE-GP [4]. [sent-36, score-0.034]
</p><p>22 2  0 4  0 4 2  4 2  0  +  −2 −4  −4  0 4  4  =  −2 −4  −4  −4  0  −2  −2 −4  −4  −4  k1 (x1 )k2 (x2 ) 2nd order kernel ↓  4  2  3  1. [sent-52, score-0.366]
</p><p>23 5  4 2  0  −2  k1 (x1 ) + k2 (x2 ) 1st order kernel ↓  2  1  2  2 0  −2  k2 (x2 ) 1D kernel ↓  1. [sent-54, score-0.703]
</p><p>24 5  4 0  0  −2  k1 (x1 ) 1D kernel ↓  2  2  0  0  −2  0. [sent-55, score-0.337]
</p><p>25 Left: a draw from a ﬁrst-order additive kernel corresponds to a sum of draws from one-dimensional kernels. [sent-60, score-0.954]
</p><p>26 Right: functions drawn from a product kernel prior have weaker long-range dependencies, and less long-range structure. [sent-61, score-0.538]
</p><p>27 2  Gaussian Process Models  Gaussian processes are a ﬂexible and tractable prior over functions, useful for solving regression and classiﬁcation tasks [5]. [sent-62, score-0.256]
</p><p>28 The kind of structure which can be captured by a GP model is mainly determined by its kernel: the covariance function. [sent-63, score-0.093]
</p><p>29 One of the main difﬁculties in specifying a Gaussian process model is in choosing a kernel which can represent the structure present in the data. [sent-64, score-0.429]
</p><p>30 For small to medium-sized datasets, the kernel has a large impact on modeling efﬁcacy. [sent-65, score-0.367]
</p><p>31 Figure 1 compares, for two-dimensional functions, a ﬁrst-order additive kernel with a second-order kernel. [sent-66, score-0.688]
</p><p>32 We can see that a GP with a ﬁrst-order additive kernel is an example of a GAM: Each function drawn from this model is a sum of orthogonal one-dimensional functions. [sent-67, score-0.786]
</p><p>33 Compared to functions drawn from the higher-order GP, draws from the ﬁrst-order GP have more long-range structure. [sent-68, score-0.177]
</p><p>34 We can expect many natural functions to depend only on sums of low-order interactions. [sent-69, score-0.128]
</p><p>35 For example, the price of a house or car will presumably be well approximated by a sum of prices of individual features, such as a sun-roof. [sent-70, score-0.35]
</p><p>36 Other parts of the price may depend jointly on a small set of features, such as the size and building materials of a house. [sent-71, score-0.174]
</p><p>37 Capturing these regularities will mean that a model can conﬁdently extrapolate to unseen combinations of features. [sent-72, score-0.225]
</p><p>38 3  Additive Kernels  We now give a precise deﬁnition of additive kernels. [sent-73, score-0.351]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gp', 0.436), ('hkl', 0.352), ('additive', 0.351), ('kernel', 0.337), ('gam', 0.285), ('xid', 0.176), ('ki', 0.158), ('draw', 0.129), ('xd', 0.105), ('parameterization', 0.102), ('interactions', 0.098), ('interaction', 0.086), ('orders', 0.084), ('draws', 0.081), ('price', 0.081), ('gaussian', 0.081), ('tractable', 0.079), ('dently', 0.077), ('kid', 0.077), ('breakthrough', 0.071), ('extrapolate', 0.071), ('regression', 0.07), ('prices', 0.067), ('anova', 0.067), ('mpi', 0.067), ('prior', 0.066), ('edward', 0.063), ('hannes', 0.063), ('cleverly', 0.061), ('xi', 0.059), ('regularities', 0.058), ('bingen', 0.058), ('exible', 0.056), ('combinations', 0.056), ('kj', 0.056), ('sum', 0.056), ('process', 0.055), ('cacy', 0.054), ('house', 0.054), ('materials', 0.054), ('generalized', 0.054), ('functions', 0.054), ('nickisch', 0.053), ('decomposes', 0.053), ('carl', 0.053), ('presumably', 0.053), ('gps', 0.053), ('machinery', 0.053), ('nth', 0.051), ('hn', 0.051), ('exponential', 0.05), ('generalize', 0.05), ('today', 0.05), ('constitutes', 0.049), ('interpretability', 0.046), ('expressive', 0.045), ('rasmussen', 0.044), ('culties', 0.044), ('spectrum', 0.043), ('drawn', 0.042), ('introduce', 0.041), ('processes', 0.041), ('models', 0.041), ('engineering', 0.041), ('xj', 0.041), ('hyperparameter', 0.041), ('germany', 0.041), ('explores', 0.041), ('unseen', 0.04), ('exibility', 0.04), ('hierarchical', 0.04), ('car', 0.039), ('depending', 0.039), ('family', 0.039), ('depend', 0.039), ('input', 0.039), ('weaker', 0.039), ('suffers', 0.039), ('specifying', 0.037), ('ranging', 0.036), ('sums', 0.035), ('limits', 0.035), ('popular', 0.035), ('intelligent', 0.035), ('outperformed', 0.034), ('hyperparameters', 0.033), ('kind', 0.033), ('considerably', 0.033), ('datasets', 0.032), ('generalizes', 0.032), ('captured', 0.031), ('capturing', 0.031), ('amounts', 0.03), ('compares', 0.03), ('cambridge', 0.03), ('impact', 0.03), ('order', 0.029), ('mainly', 0.029), ('assign', 0.029), ('increased', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="26-tfidf-1" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>2 0.26524282 <a title="26-tfidf-2" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>Author: Andrew Mchutchon, Carl E. Rasmussen</p><p>Abstract: In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods. 1</p><p>3 0.18009998 <a title="26-tfidf-3" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>4 0.17848459 <a title="26-tfidf-4" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>5 0.12359213 <a title="26-tfidf-5" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>6 0.11951327 <a title="26-tfidf-6" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>7 0.1160742 <a title="26-tfidf-7" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>8 0.10285322 <a title="26-tfidf-8" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>9 0.092200756 <a title="26-tfidf-9" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>10 0.086983666 <a title="26-tfidf-10" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>11 0.086422987 <a title="26-tfidf-11" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>12 0.084936008 <a title="26-tfidf-12" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>13 0.080666602 <a title="26-tfidf-13" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>14 0.080046259 <a title="26-tfidf-14" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>15 0.067700312 <a title="26-tfidf-15" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>16 0.06609869 <a title="26-tfidf-16" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>17 0.065772101 <a title="26-tfidf-17" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>18 0.064439148 <a title="26-tfidf-18" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>19 0.061731704 <a title="26-tfidf-19" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>20 0.059264451 <a title="26-tfidf-20" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.147), (1, 0.019), (2, 0.034), (3, -0.039), (4, -0.05), (5, -0.065), (6, 0.073), (7, -0.033), (8, 0.122), (9, 0.135), (10, -0.247), (11, 0.028), (12, 0.065), (13, 0.108), (14, -0.107), (15, 0.234), (16, 0.085), (17, 0.198), (18, -0.205), (19, -0.139), (20, -0.018), (21, 0.081), (22, -0.126), (23, -0.113), (24, 0.052), (25, -0.001), (26, 0.018), (27, -0.048), (28, -0.005), (29, -0.137), (30, -0.057), (31, -0.012), (32, 0.17), (33, 0.114), (34, 0.077), (35, -0.003), (36, 0.002), (37, -0.044), (38, -0.019), (39, 0.043), (40, 0.052), (41, -0.019), (42, -0.002), (43, -0.05), (44, -0.018), (45, 0.006), (46, -0.023), (47, 0.02), (48, -0.015), (49, 0.118)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97041148 <a title="26-lsi-1" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>2 0.79045635 <a title="26-lsi-2" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>Author: Andrew Mchutchon, Carl E. Rasmussen</p><p>Abstract: In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods. 1</p><p>3 0.7550236 <a title="26-lsi-3" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>4 0.61453807 <a title="26-lsi-4" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>5 0.51609832 <a title="26-lsi-5" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A nonparametric kernel-based method for realizing Bayes’ rule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample. The kernel Bayes’ rule can be applied to a wide variety of Bayesian inference problems: we demonstrate Bayesian computation without likelihood, and ﬁltering with a nonparametric statespace model. A consistency rate for the posterior estimate is established. 1</p><p>6 0.5062232 <a title="26-lsi-6" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>7 0.46513078 <a title="26-lsi-7" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>8 0.42346701 <a title="26-lsi-8" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>9 0.42107227 <a title="26-lsi-9" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>10 0.38722548 <a title="26-lsi-10" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>11 0.37372056 <a title="26-lsi-11" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>12 0.36018902 <a title="26-lsi-12" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>13 0.34828597 <a title="26-lsi-13" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>14 0.30663359 <a title="26-lsi-14" href="./nips-2011-On_Causal_Discovery_with_Cyclic_Additive_Noise_Models.html">194 nips-2011-On Causal Discovery with Cyclic Additive Noise Models</a></p>
<p>15 0.2948252 <a title="26-lsi-15" href="./nips-2011-Continuous-Time_Regression_Models_for_Longitudinal_Networks.html">62 nips-2011-Continuous-Time Regression Models for Longitudinal Networks</a></p>
<p>16 0.28672957 <a title="26-lsi-16" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>17 0.28300744 <a title="26-lsi-17" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>18 0.27975842 <a title="26-lsi-18" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>19 0.27914709 <a title="26-lsi-19" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>20 0.27776167 <a title="26-lsi-20" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.011), (20, 0.013), (31, 0.04), (43, 0.728), (45, 0.033), (57, 0.036), (74, 0.011), (99, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98982781 <a title="26-lda-1" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>2 0.90181506 <a title="26-lda-2" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>Author: Shilin Ding, Grace Wahba, Xiaojin Zhu</p><p>Abstract: In discrete undirected graphical models, the conditional independence of node labels Y is speciﬁed by the graph structure. We study the case where there is another input random vector X (e.g. observed features) such that the distribution P (Y | X) is determined by functions of X that characterize the (higher-order) interactions among the Y ’s. The main contribution of this paper is to learn the graph structure and the functions conditioned on X at the same time. We prove that discrete undirected graphical models with feature X are equivalent to multivariate discrete models. The reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure. The functional spaces can be ﬂexibly determined by kernels. Additionally, we impose a Structure Lasso (SLasso) penalty on groups of functions to learn the graph structure. These groups with overlaps are designed to enforce hierarchical function selection. In this way, we are able to shrink higher order interactions to obtain a sparse graph structure. 1</p><p>3 0.89334536 <a title="26-lda-3" href="./nips-2011-The_Fast_Convergence_of_Boosting.html">282 nips-2011-The Fast Convergence of Boosting</a></p>
<p>Author: Matus J. Telgarsky</p><p>Abstract: This manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was O(exp(1/✏2 )). First, it is established that the setting of weak learnability aids the entire class, granting a rate O(ln(1/✏)). Next, the (disjoint) conditions under which the inﬁmal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate O(ln(1/✏)). Finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate O(1/✏), with a matching lower bound for the logistic loss. The principal technical hurdle throughout this work is the potential unattainability of the inﬁmal empirical risk; the technique for overcoming this barrier may be of general interest. 1</p><p>4 0.87777776 <a title="26-lda-4" href="./nips-2011-Bayesian_Spike-Triggered_Covariance_Analysis.html">44 nips-2011-Bayesian Spike-Triggered Covariance Analysis</a></p>
<p>Author: Jonathan W. Pillow, Il M. Park</p><p>Abstract: Neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli. Here we describe an explicit modelbased interpretation of traditional estimators for a neuron’s multi-dimensional feature space, which allows for several important generalizations and extensions. First, we show that traditional estimators based on the spike-triggered average (STA) and spike-triggered covariance (STC) can be formalized in terms of the “expected log-likelihood” of a Linear-Nonlinear-Poisson (LNP) model with Gaussian stimuli. This model-based formulation allows us to deﬁne maximum-likelihood and Bayesian estimators that are statistically consistent and efﬁcient in a wider variety of settings, such as with naturalistic (non-Gaussian) stimuli. It also allows us to employ Bayesian methods for regularization, smoothing, sparsiﬁcation, and model comparison, and provides Bayesian conﬁdence intervals on model parameters. We describe an empirical Bayes method for selecting the number of features, and extend the model to accommodate an arbitrary elliptical nonlinear response function, which results in a more powerful and more ﬂexible model for feature space inference. We validate these methods using neural data recorded extracellularly from macaque primary visual cortex. 1</p><p>5 0.86763525 <a title="26-lda-5" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>Author: Alexandra Carpentier, Odalric-ambrym Maillard, Rémi Munos</p><p>Abstract: We consider the problem of recovering the parameter α ∈ RK of a sparse function f (i.e. the number of non-zero entries of α is small compared to the number K of features) given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomization process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven, independently on the number of sampling points N , even when the features are arbitrarily non-orthogonal. Under the assumption that f is H¨ lder continuous with exponent at least √ we proo 1/2, vide an estimate α of the parameter such that �α − α�2 = O(�η�2 / N ), where � � η is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method. 1</p><p>6 0.8550511 <a title="26-lda-6" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>7 0.65274179 <a title="26-lda-7" href="./nips-2011-High-Dimensional_Graphical_Model_Selection%3A_Tractable_Graph_Families_and_Necessary_Conditions.html">117 nips-2011-High-Dimensional Graphical Model Selection: Tractable Graph Families and Necessary Conditions</a></p>
<p>8 0.62260371 <a title="26-lda-8" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>9 0.61982101 <a title="26-lda-9" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>10 0.61021036 <a title="26-lda-10" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>11 0.59244978 <a title="26-lda-11" href="./nips-2011-Inferring_Interaction_Networks_using_the_IBP_applied_to_microRNA_Target_Prediction.html">132 nips-2011-Inferring Interaction Networks using the IBP applied to microRNA Target Prediction</a></p>
<p>12 0.58892131 <a title="26-lda-12" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>13 0.57081926 <a title="26-lda-13" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>14 0.56485909 <a title="26-lda-14" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>15 0.56457943 <a title="26-lda-15" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>16 0.56370449 <a title="26-lda-16" href="./nips-2011-Uniqueness_of_Belief_Propagation_on_Signed_Graphs.html">296 nips-2011-Uniqueness of Belief Propagation on Signed Graphs</a></p>
<p>17 0.56035829 <a title="26-lda-17" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>18 0.5539394 <a title="26-lda-18" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<p>19 0.55349743 <a title="26-lda-19" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<p>20 0.54725742 <a title="26-lda-20" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
