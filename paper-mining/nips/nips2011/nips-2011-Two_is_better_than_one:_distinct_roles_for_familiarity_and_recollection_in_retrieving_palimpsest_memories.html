<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-292" href="#">nips2011-292</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</h1>
<br/><p>Source: <a title="nips-2011-292-pdf" href="http://papers.nips.cc/paper/4364-two-is-better-than-one-distinct-roles-for-familiarity-and-recollection-in-retrieving-palimpsest-memories.pdf">pdf</a></p><p>Author: Cristina Savin, Peter Dayan, Máté Lengyel</p><p>Abstract: Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This ﬁnding provides a new window onto actively contentious psychological and neural aspects of recognition memory. 1</p><p>Reference: <a title="nips-2011-292-reference" href="../nips2011_reference/nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories  Cristina Savin1 cs664@cam. [sent-1, score-1.197]
</p><p>2 of Engineering, University of Cambridge, UK 2 Gatsby Computational Neuroscience Unit, University College London, UK  Abstract Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. [sent-11, score-0.661]
</p><p>3 Knowing the age of a pattern thus becomes critical for recalling it faithfully. [sent-12, score-0.306]
</p><p>4 Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. [sent-14, score-1.342]
</p><p>5 1  Introduction  Episodic memory such as that in the hippocampus acts like a palimpsest – each new entity to be stored is overlaid on top of its predecessors, and, in turn, is submerged by its successors. [sent-16, score-0.452]
</p><p>6 This implies both anterograde interference (existing memories hinder the processing of new ones) and retrograde interference (new memories overwrite information about old ones). [sent-17, score-0.268]
</p><p>7 Some aspects of these challenges have been addressed in two theoretical frameworks – one focusing on anterograde interference through the interaction of novelty and storage [1]; the other on retrograde interference in individual synapses [2]. [sent-19, score-0.408]
</p><p>8 First, [1] made the critical observation that autoassociative memories only work if normal recall dynamics are suppressed on presentation of new patterns that need to be stored. [sent-21, score-0.371]
</p><p>9 Otherwise, rather than memorizing the new pattern, the memory associated with the existing pattern that most closely matches the new input will be strengthened. [sent-22, score-0.234]
</p><p>10 Second, [2] considered the palimpsest problem of overwriting information in synapses whose efﬁcacies have limited dynamic ranges. [sent-24, score-0.269]
</p><p>11 They pointed out that this can be at least partially addressed through allowing multiple internal states (for instance forming a cascade) for each observable synaptic efﬁcacy level. [sent-25, score-0.225]
</p><p>12 However, although [2] provide an attractive formalism for analyzing and optimizing synaptic storage, a retrieval mechanism associated with this storage is missing. [sent-26, score-0.303]
</p><p>13 The ˜ recall cue x is a noisy version of one of the stored patterns x. [sent-34, score-0.32]
</p><p>14 Upon storing pattern x synaptic states changed from V0 (sampled from the stationary distribution of synaptic dynamics) to V1 . [sent-35, score-0.509]
</p><p>15 Recall occurs after the presentation of t − 1 intervening patterns, when synapses are in states Vt , with ˜ corresponding synaptic efﬁcacies Wt . [sent-36, score-0.352]
</p><p>16 In the case of a palimpsest, the trace of a memory in the synaptic efﬁcacies depends critically on the age of the memory, i. [sent-40, score-0.489]
</p><p>17 Indeed, we show retrieval is substantially worse when familiarity is not explicitly represented than when it is. [sent-44, score-0.642]
</p><p>18 Dual system models for recognition memory are the topic of a heated debate [3, 4]. [sent-45, score-0.25]
</p><p>19 2  Storage in a palimpsest memory  We consider the task of autoassociative recall of binary patterns from a palimpsest memory. [sent-48, score-0.657]
</p><p>20 During storage, network activity is clamped to the presented pattern x, inducing changes in the synapses’ ‘internal’ states V and corresponding observed binary efﬁcacies W (Fig. [sent-50, score-0.224]
</p><p>21 Hence, learning is speciﬁed through a set of transition matrices M (xi , xj ), with M (xi , xj )l l = P(Vij = l |Vij = l, xi , xj ). [sent-57, score-0.315]
</p><p>22 Furthermore, we assume synaptic changes occur only when the postsynaptic neuron is active, leading to potentiation if the presynaptic neuron is also active and to depression otherwise. [sent-61, score-0.369]
</p><p>23 Additionally, we deﬁne the column vectors π V (xi , xj ) and π W (xi , xj ) for the distribution of the synaptic states and observable efﬁcacies, respectively, when one of the patterns stored was (xi , xj ), W V such that πl (xi , xj ) = P(Wij = l|xi , xj ) and πl (xi , xj ) = P(Vij = l|xi , xj ). [sent-64, score-1.006]
</p><p>24 The corresponding weight distribution is π W (xi , xj ) = T · π V (xi , xj ), where T is a 2 × 2n matrix deﬁning the deterministic mapping from synaptic states to observable efﬁcacies. [sent-66, score-0.395]
</p><p>25 The fact that the recency of the pattern to be recalled, t, appears in equation 5 implies that pattern age will strongly inﬂuence information retrieval. [sent-67, score-0.414]
</p><p>26 We ﬁrst show the limitations of recall dynamics that involve a single, monolithic module which averages over t. [sent-69, score-0.445]
</p><p>27 We then prove the beneﬁts of a dual system with two qualitatively different modules, one of which explicitly represents an estimate of pattern age. [sent-70, score-0.213]
</p><p>28 1  A single module recollection system Optimal retrieval dynamics  Since information storage by synaptic plasticity is lossy, the recollection task described above is a probabilistic inference problem [5,6]. [sent-72, score-1.512]
</p><p>29 Essentially, neural dynamics should represent (aspects of) the posterior over stored patterns, P (x|˜ , W), that expresses the probability of any pattern x being the x ˜ correct response for the recall query given a noisy recall cue, x, and the synaptic efﬁcacies W. [sent-73, score-0.64]
</p><p>30 In more detail, the posterior over possible stored patterns can be computed as: ˜ P (x|W, x) ∝ Pstore (x) · Pnoise (˜ |x) · P(W|x) x  (6) 1  where we assume that evidence from the weights factorizes over synapses , P (W|x) = ij P (Wij |xi , xj ). [sent-74, score-0.429]
</p><p>31 1 This assumption is never exactly true in practice, as synapses that share a pre- or post- synaptic partner are bound to be correlated. [sent-75, score-0.282]
</p><p>32 3  Previous Bayesian recall dynamics derivations assumed learning rules for which the contribution of each pattern to the ﬁnal weight were the same, irrespective of the order of pattern presentation [5,6]. [sent-77, score-0.36]
</p><p>33 By contrast, the Markov chain behaviour of our synaptic learning rule forces us to explicitly consider pattern age. [sent-78, score-0.288]
</p><p>34 Furthermore, as pattern age is unknown at recall, we need to integrate over all possible t values (Eq. [sent-79, score-0.274]
</p><p>35 This results in asynchronous updates, in which the activity of a neuron xi changes stochastically as a function of its input cue xi , the activity of all other neurons, x\i , and neighbouring synapses, Wi,· ˜ and W·,i . [sent-89, score-0.338]
</p><p>36 Note that the optimal dynamics include two homeostatic processes, corresponding to global inhibition, j xj , and neuronal excitability regulation, j Wij , that stabilize network activity during recall. [sent-93, score-0.281]
</p><p>37 Performance is as expected when pattern age is assumed known: as the available information from the weights decreases, so does performance, ﬁnally converging to control levels, deﬁned by the retrieval performance of a network without plastic recurrent connections, i. [sent-99, score-0.428]
</p><p>38 when inference uses only the recall cue and the prior over stored patterns (Fig. [sent-101, score-0.32]
</p><p>39 We therefore ran simulations without this term in the dynamics and found that although it did decrease recall performance, this decrease was similar to that obtained by randomly pruning half of the connections in the network and keeping this term in the dynamics (not shown). [sent-106, score-0.271]
</p><p>40 4  40  t known t unknown control  30  b  20  10  control  5  10 0 0  15 error (%)  error (%)  a  50  t  100  150  0  t known  single module dual system Gibbs tempered transitions  Figure 2: a. [sent-109, score-0.51]
</p><p>41 Average recollection error comparison for the single and dual memory system. [sent-112, score-0.621]
</p><p>42 When the pattern that was actually presented is older than this estimate, the resulting memory signal is weaker than expected, suggesting that the initial pattern was very sparse (since a pair of inactive elements does not induce any synaptic changes according to our learning rule). [sent-115, score-0.497]
</p><p>43 It is worth noting that with more complex synaptic dynamics (e. [sent-125, score-0.247]
</p><p>44 4  A dual memory system  An alternative to implicitly marginalizing over the age of the pattern throughout the inference process is to estimate it at the same time as performing recollection. [sent-128, score-0.533]
</p><p>45 x The module that computes familiarity can also be seen as a palimpsest, with each pattern overlaying, and being overlaid by, its predecessors and successors. [sent-131, score-1.033]
</p><p>46 As a separate module, the neural network estimating familiarity cannot however access the weights W of the recollection module. [sent-133, score-1.055]
</p><p>47 A biologically plausible approximation is to assume that the familiarity module uses a separate set of weights, which we call Wfam . [sent-134, score-0.886]
</p><p>48 1b that t is independent of x conditioned on x, thus the conditioning on x can be dropped when computing the posterior over t, that is, external input need only feed directly into the recollection but not the familiarity module (Fig. [sent-136, score-1.329]
</p><p>49 In particular, we assume a feedforward network structure in the familiarity module, with each neuron receiving the output of the recollection module as inputs through synapses Wfam . [sent-138, score-1.502]
</p><p>50 These synaptic 5  b  cue  familiarity  recollection  activation  0. [sent-139, score-1.24]
</p><p>51 05  0  1  100  neuron index  familiarity signal  a  10 1  10 0  10 −1  200  0  50  t  100  150  Figure 3: a. [sent-140, score-0.645]
</p><p>52 The familiarity network has a feedforward structure, with the activity of individual neurons estimating the probability of the true pattern age being a certain value t, see example in inset. [sent-142, score-1.022]
</p><p>53 The estimated pattern age translates into a familiarity signal, which scales the contribution of the recurrent inputs in the network dynamics. [sent-143, score-0.986]
</p><p>54 Dependence of the familiarity signal on the estimated pattern age. [sent-145, score-0.681]
</p><p>55 3 For simplicity, we assume that the familiarity neurons are always activated during encoding, so that synapses can change state (either by potentiation or depression) with every storage event. [sent-147, score-0.887]
</p><p>56 Concretely, the familiarity module consists of Nfam neurons, each corresponding to a certain pattern age in the range 1–Nfam (the last unit codes for t ≥ Nfam ). [sent-148, score-1.16]
</p><p>57 As mentioned above, we treat the activity of the familiarity module as a sample from the posterior over age t. [sent-151, score-1.158]
</p><p>58 Critically, this familiarity module is not just a convenient theoretical construct associated with retrieval. [sent-154, score-0.886]
</p><p>59 First, as we mentioned before, the assessment of novelty actually plays a key part in memory storage – in making the decision as to whether a pattern that is presented is novel, and so should be stored, or familiar, and so should have its details be recalled. [sent-155, score-0.403]
</p><p>60 The graded familiarity module that we have suggested is an obvious extension of this idea; the use for retrieval is new. [sent-157, score-0.941]
</p><p>61 Second, it is in general accord with substantial data on the role of perirhinal cortex and the activity of neurons in this structure [3]. [sent-158, score-0.307]
</p><p>62 Recency neurons would be associated with small values of t; novelty neurons with large or effectively inﬁnite values of t [14], although perirhinal cortex appears to adopt a population coding strategy for age, rather than just one-of-n. [sent-159, score-0.387]
</p><p>63 The recollection module has the same dynamics as before, with constants ci computed assuming t ﬁxed to the output of the familiarity module. [sent-160, score-1.362]
</p><p>64 Thus we predict that familiarity multiplicatively modulates recurrent interactions in the recollection module during recall. [sent-161, score-1.411]
</p><p>65 3b), it can be computed using a linear unit pooling the outputs of all the neurons in the familiarity module, with weights given by the corresponding values for cfam (t). [sent-163, score-0.739]
</p><p>66 i 3 There is nothing to say that the learning rule that optimizes the recollection network’s ability to recall patterns should be equally appropriate for assessing familiarity. [sent-164, score-0.582]
</p><p>67 Hence, the familiarity module could have their own learning rule, optimized for its speciﬁc task. [sent-165, score-0.886]
</p><p>68 Performance comparison when the decision layer uses signals from the familiarity module, the recollection module, or both. [sent-188, score-1.015]
</p><p>69 In order to compare single and dual module systems fairly, the computational resources employed by each should be the same. [sent-192, score-0.382]
</p><p>70 Regardless, the dual memory system performs signiﬁcantly better than the single module system (Fig. [sent-195, score-0.594]
</p><p>71 5  Recognition memory  We have so far considered familiarity merely as an instrument for effective recollection. [sent-197, score-0.727]
</p><p>72 It is these tasks that have been used to elucidate the role of perirhinal cortex in recognition memory. [sent-199, score-0.242]
</p><p>73 In the dual module system, information about recognition is available from both the familiarity module (patterns judged to have young ages are recognized) and the recollection module (patterns recalled with higher certainty are recognized). [sent-200, score-2.062]
</p><p>74 We therefore construct an additional decision module which takes the outputs of the familiarity and recollection modules and maps them into a binary behavioral response (familiar vs. [sent-201, score-1.402]
</p><p>75 Speciﬁcally, we use the average of the entropies associated with the activities of neurons in the recollection module and the mean estimate of t from the familiarity module. [sent-203, score-1.351]
</p><p>76 Since the palimpsest property implicitly assumes that all patterns have been presented at some point, we deﬁne a pattern to be familiar if its age is less than a ﬁxed threshold tth . [sent-204, score-0.565]
</p><p>77 We train the decision module using a Gaussian process classiﬁer4 [15], which yields as outcome the probability of a hit, P(familiar|t∗ , x∗ ), shown in Fig. [sent-205, score-0.329]
</p><p>78 Lastly, as recognition is known to rely more on familiarity for relatively recent patterns [18], we estimate recognition performance for recent patterns, which we deﬁne as having th age t ≤ t2 . [sent-212, score-0.956]
</p><p>79 To determine the contribution of each module in recognition outcomes in this case, we estimate performance of classiﬁers trained on single input dimensions for this test data. [sent-213, score-0.374]
</p><p>80 Consistent with experimental data, our analysis reveals that the familiarity signal gives a more reliable estimate of novelty, compared to the recollection output for relatively recent items (Fig. [sent-214, score-0.985]
</p><p>81 7  6  Conclusions and discussion  Knowing the age of a pattern is critical for retrieval from palimpsest memories, a consideration that has so far eluded theoretical inquiry. [sent-218, score-0.517]
</p><p>82 We showed that a memory system could either treat this information implicitly, by marginalizing over all possible ages, or it could estimate age explicitly as a form of familiarity. [sent-219, score-0.356]
</p><p>83 In our model, the posterior over possible stored patterns was represented in neural activities via samples. [sent-222, score-0.254]
</p><p>84 Instead, a dual memory system with two functionally distinct but closely interacting modules, yielded the best performance both for efﬁcient recollection and for recognition. [sent-224, score-0.657]
</p><p>85 Importantly, though Gibbs sampling and tempered transitions provide a useful framework for understanding the performance differences between different memory systems, the presented results are not restricted to a sampling-based implementation. [sent-225, score-0.256]
</p><p>86 Similarly, the speciﬁc details of the familiarity module are not critical for these effects, which should be apparent for any alternative implementation correctly estimating pattern age. [sent-227, score-1.012]
</p><p>87 The rationale for our familiarity network was improving recollection; however, the form of the network was motivated by the substantial experimental data [14] on recognition, and indeed standard models of perirhinal cortex activity [20]. [sent-232, score-0.921]
</p><p>88 These, for instance, also rely on some form of inhibition to mediate interactions between different familiarity neurons. [sent-233, score-0.659]
</p><p>89 Nevertheless, our model is the ﬁrst to link the computational function of familiarity networks to recall; it is distinct also in that it considers palimpsest synapses, as previous models use purely additive learning rules [20]. [sent-234, score-0.743]
</p><p>90 Future work with the extended model should allow us to address familiarity, novelty, and recency neurons in the perirhinal cortex, and indeed provide a foundation for new thinking about this region. [sent-236, score-0.241]
</p><p>91 In our model familiarity interacts with recollection by multiplicatively (or divisively) modulating the contribution of recurrent inputs in the recollection module. [sent-237, score-1.483]
</p><p>92 Neurally, this effect could be mediated by shunting inhibition via speciﬁc classes of hippocampal interneurons which target the dendritic segment corresponding to recurrent connections, thus rescaling the relative contribution of external versus recurrent inputs [21]. [sent-238, score-0.241]
</p><p>93 Whether pathways reaching CA3 from perirhinal cortex through entorhinal cortex preserve a sufﬁcient amount of input speciﬁcity of feed-forward inhibition is unknown. [sent-239, score-0.309]
</p><p>94 In particular, by optimizing our dual system solely for memory recall we also predicted non-trivial ROC curves for recognition that are in at least broad qualitative agreement with experiments. [sent-241, score-0.376]
</p><p>95 Future work will be needed to explore whether the ROC curves in our model show similar dissociations in response to speciﬁc lesions of the two modules to those found in recent experiments [22,23] and the relation to other recognition memory models [24]. [sent-242, score-0.277]
</p><p>96 Differential neuronal encoding of novelty, familiarity and recency in regions of the anterior temporal lobe. [sent-312, score-0.657]
</p><p>97 Findings from animals concerning when interactions between perirhinal cortex, hippocampus and medial prefrontal cortex are necessary for recognition memory. [sent-324, score-0.322]
</p><p>98 Components of episodic memory: the contribution of recollection and familiarity. [sent-328, score-0.444]
</p><p>99 The nature of recollection and familiarity: A review of 30 years of research. [sent-331, score-0.398]
</p><p>100 Comparison of computational models of familiarity discrimination in the perirhinal cortex. [sent-340, score-0.715]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('familiarity', 0.587), ('recollection', 0.398), ('module', 0.299), ('age', 0.18), ('synaptic', 0.169), ('palimpsest', 0.156), ('memory', 0.14), ('perirhinal', 0.128), ('synapses', 0.113), ('cacies', 0.112), ('fam', 0.1), ('stored', 0.095), ('pattern', 0.094), ('patterns', 0.091), ('modules', 0.088), ('cfam', 0.085), ('pstore', 0.085), ('xj', 0.085), ('dual', 0.083), ('storage', 0.079), ('dynamics', 0.078), ('recall', 0.068), ('neurons', 0.067), ('cue', 0.066), ('cortex', 0.065), ('xi', 0.06), ('novelty', 0.06), ('neuron', 0.058), ('tempered', 0.057), ('cout', 0.057), ('pnoise', 0.057), ('wfam', 0.057), ('memories', 0.056), ('retrieval', 0.055), ('recurrent', 0.052), ('inhibition', 0.051), ('cascade', 0.05), ('interference', 0.05), ('cin', 0.05), ('recognition', 0.049), ('wij', 0.048), ('network', 0.047), ('activity', 0.047), ('recency', 0.046), ('autoassociative', 0.046), ('posterior', 0.045), ('familiar', 0.044), ('rec', 0.043), ('depression', 0.043), ('nfam', 0.043), ('precall', 0.043), ('storing', 0.041), ('potentiation', 0.041), ('vij', 0.041), ('gibbs', 0.039), ('roc', 0.037), ('hippocampus', 0.036), ('states', 0.036), ('system', 0.036), ('hippocampal', 0.035), ('transitions', 0.035), ('intervening', 0.034), ('modulates', 0.032), ('critical', 0.032), ('dayan', 0.031), ('hit', 0.031), ('decision', 0.03), ('neurally', 0.029), ('anterograde', 0.028), ('contentious', 0.028), ('irec', 0.028), ('metastates', 0.028), ('predecessors', 0.028), ('retrograde', 0.028), ('yonelinas', 0.028), ('lengyel', 0.028), ('contribution', 0.026), ('rule', 0.025), ('ages', 0.025), ('shunting', 0.025), ('overlaid', 0.025), ('tempering', 0.025), ('bogacz', 0.025), ('debate', 0.025), ('ii', 0.025), ('sampling', 0.024), ('neuronal', 0.024), ('alarms', 0.023), ('medial', 0.023), ('recalled', 0.023), ('neural', 0.023), ('wji', 0.022), ('multiplicatively', 0.022), ('false', 0.021), ('interactions', 0.021), ('episodic', 0.02), ('neocortical', 0.02), ('observable', 0.02), ('activation', 0.02), ('brown', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="292-tfidf-1" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>Author: Cristina Savin, Peter Dayan, Máté Lengyel</p><p>Abstract: Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This ﬁnding provides a new window onto actively contentious psychological and neural aspects of recognition memory. 1</p><p>2 0.12573835 <a title="292-tfidf-2" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>3 0.12519042 <a title="292-tfidf-3" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>4 0.11948993 <a title="292-tfidf-4" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>5 0.10185651 <a title="292-tfidf-5" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>Author: Balazs B. Ujfalussy, Máté Lengyel</p><p>Abstract: Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment ﬂuctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity. 1</p><p>6 0.097333565 <a title="292-tfidf-6" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>7 0.072380908 <a title="292-tfidf-7" href="./nips-2011-Estimating_time-varying_input_signals_and_ion_channel_states_from_a_single_voltage_trace_of_a_neuron.html">89 nips-2011-Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron</a></p>
<p>8 0.071735442 <a title="292-tfidf-8" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>9 0.065201148 <a title="292-tfidf-9" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>10 0.058275778 <a title="292-tfidf-10" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>11 0.05685455 <a title="292-tfidf-11" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>12 0.05625996 <a title="292-tfidf-12" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>13 0.053563565 <a title="292-tfidf-13" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>14 0.053158361 <a title="292-tfidf-14" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>15 0.052511346 <a title="292-tfidf-15" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>16 0.051473208 <a title="292-tfidf-16" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>17 0.051306069 <a title="292-tfidf-17" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>18 0.05117593 <a title="292-tfidf-18" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>19 0.050759487 <a title="292-tfidf-19" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>20 0.047784548 <a title="292-tfidf-20" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.057), (2, 0.145), (3, 0.019), (4, 0.009), (5, 0.018), (6, -0.018), (7, -0.041), (8, -0.046), (9, -0.057), (10, 0.028), (11, -0.005), (12, -0.036), (13, -0.003), (14, 0.03), (15, 0.022), (16, -0.048), (17, 0.047), (18, -0.057), (19, 0.031), (20, 0.064), (21, 0.013), (22, -0.003), (23, 0.061), (24, -0.012), (25, 0.005), (26, 0.034), (27, -0.116), (28, 0.102), (29, -0.009), (30, 0.007), (31, -0.026), (32, -0.03), (33, -0.043), (34, -0.045), (35, 0.023), (36, -0.009), (37, 0.01), (38, 0.02), (39, -0.058), (40, -0.087), (41, 0.026), (42, -0.038), (43, -0.046), (44, -0.14), (45, -0.005), (46, -0.092), (47, 0.057), (48, -0.025), (49, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92459333 <a title="292-lsi-1" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>Author: Cristina Savin, Peter Dayan, Máté Lengyel</p><p>Abstract: Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This ﬁnding provides a new window onto actively contentious psychological and neural aspects of recognition memory. 1</p><p>2 0.67722631 <a title="292-lsi-2" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>Author: Balazs B. Ujfalussy, Máté Lengyel</p><p>Abstract: Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment ﬂuctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity. 1</p><p>3 0.66739106 <a title="292-lsi-3" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>4 0.60125738 <a title="292-lsi-4" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>5 0.58383149 <a title="292-lsi-5" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>Author: Konrad Koerding, Ian Stevenson</p><p>Abstract: Synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury. However, it is often difﬁcult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging. Here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains. First, using a generalized bilinear model with Poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent. This approach allows model-based estimation of STDP modiﬁcation functions from pairs of spike trains. Then, using recursive point-process adaptive ﬁltering methods we estimate more general variation in coupling strength over time. Using simulations of neurons undergoing spike-timing dependent modiﬁcation, we show that the true modiﬁcation function can be recovered. Using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data. 1</p><p>6 0.5474906 <a title="292-lsi-6" href="./nips-2011-Emergence_of_Multiplication_in_a_Biophysical_Model_of_a_Wide-Field_Visual_Neuron_for_Computing_Object_Approaches%3A_Dynamics%2C_Peaks%2C_%26_Fits.html">85 nips-2011-Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, & Fits</a></p>
<p>7 0.53361994 <a title="292-lsi-7" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>8 0.4773857 <a title="292-lsi-8" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>9 0.47701922 <a title="292-lsi-9" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>10 0.45509246 <a title="292-lsi-10" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>11 0.44759595 <a title="292-lsi-11" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>12 0.41415659 <a title="292-lsi-12" href="./nips-2011-Quasi-Newton_Methods_for_Markov_Chain_Monte_Carlo.html">228 nips-2011-Quasi-Newton Methods for Markov Chain Monte Carlo</a></p>
<p>13 0.39762095 <a title="292-lsi-13" href="./nips-2011-Estimating_time-varying_input_signals_and_ion_channel_states_from_a_single_voltage_trace_of_a_neuron.html">89 nips-2011-Estimating time-varying input signals and ion channel states from a single voltage trace of a neuron</a></p>
<p>14 0.39437574 <a title="292-lsi-14" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>15 0.39316699 <a title="292-lsi-15" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>16 0.385153 <a title="292-lsi-16" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>17 0.37425792 <a title="292-lsi-17" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>18 0.37349063 <a title="292-lsi-18" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>19 0.35584787 <a title="292-lsi-19" href="./nips-2011-From_Stochastic_Nonlinear_Integrate-and-Fire_to_Generalized_Linear_Models.html">99 nips-2011-From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models</a></p>
<p>20 0.34670088 <a title="292-lsi-20" href="./nips-2011-A_Machine_Learning_Approach_to_Predict_Chemical_Reactions.html">7 nips-2011-A Machine Learning Approach to Predict Chemical Reactions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.021), (4, 0.037), (20, 0.015), (26, 0.015), (31, 0.148), (33, 0.02), (43, 0.045), (44, 0.295), (45, 0.062), (57, 0.07), (65, 0.024), (74, 0.038), (83, 0.079), (99, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85622877 <a title="292-lda-1" href="./nips-2011-A_Machine_Learning_Approach_to_Predict_Chemical_Reactions.html">7 nips-2011-A Machine Learning Approach to Predict Chemical Reactions</a></p>
<p>Author: Matthew A. Kayala, Pierre F. Baldi</p><p>Abstract: Being able to predict the course of arbitrary chemical reactions is essential to the theory and applications of organic chemistry. Previous approaches are not highthroughput, are not generalizable or scalable, or lack sufﬁcient data to be effective. We describe single mechanistic reactions as concerted electron movements from an electron orbital source to an electron orbital sink. We use an existing rule-based expert system to derive a dataset consisting of 2,989 productive mechanistic steps and 6.14 million non-productive mechanistic steps. We then pose identifying productive mechanistic steps as a ranking problem: rank potential orbital interactions such that the top ranked interactions yield the major products. The machine learning implementation follows a two-stage approach, in which we ﬁrst train atom level reactivity ﬁlters to prune 94.0% of non-productive reactions with less than a 0.1% false negative rate. Then, we train an ensemble of ranking models on pairs of interacting orbitals to learn a relative productivity function over single mechanistic reactions in a given system. Without the use of explicit transformation patterns, the ensemble perfectly ranks the productive mechanisms at the top 89.1% of the time, rising to 99.9% of the time when top ranked lists with at most four nonproductive reactions are considered. The ﬁnal system allows multi-step reaction prediction. Furthermore, it is generalizable, making reasonable predictions over reactants and conditions which the rule-based expert system does not handle.</p><p>same-paper 2 0.7875253 <a title="292-lda-2" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>Author: Cristina Savin, Peter Dayan, Máté Lengyel</p><p>Abstract: Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This ﬁnding provides a new window onto actively contentious psychological and neural aspects of recognition memory. 1</p><p>3 0.64386666 <a title="292-lda-3" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>Author: Michael L. Wick, Andrew McCallum</p><p>Abstract: Traditional approaches to probabilistic inference such as loopy belief propagation and Gibbs sampling typically compute marginals for all the unobserved variables in a graphical model. However, in many real-world applications the user’s interests are focused on a subset of the variables, speciﬁed by a query. In this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten. In this paper we propose a query-speciﬁc approach to MCMC that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efﬁciency. Surprisingly there has been almost no previous work on query-aware MCMC. We demonstrate the success of our approach with positive experimental results on a wide range of graphical models. 1</p><p>4 0.55343735 <a title="292-lda-4" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>5 0.55193073 <a title="292-lda-5" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>Author: Biljana Petreska, Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: Simultaneous recordings of many neurons embedded within a recurrentlyconnected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function. In principle, these dynamics might be identiﬁed by purely unsupervised, statistical means. Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model— in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process—is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements. The regimes are identiﬁed without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial. The HSLDS model also performs better than recent comparable models in predicting the ﬁring rate of an isolated neuron based on the ﬁring rates of others, suggesting that it captures more of the “shared variance” of the data. Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reﬂect its computational role. 1</p><p>6 0.54265863 <a title="292-lda-6" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>7 0.53362113 <a title="292-lda-7" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>8 0.53233045 <a title="292-lda-8" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>9 0.52636242 <a title="292-lda-9" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>10 0.52558798 <a title="292-lda-10" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>11 0.52308291 <a title="292-lda-11" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>12 0.52113235 <a title="292-lda-12" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>13 0.52101284 <a title="292-lda-13" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>14 0.52087539 <a title="292-lda-14" href="./nips-2011-An_Application_of_Tree-Structured_Expectation_Propagation_for_Channel_Decoding.html">31 nips-2011-An Application of Tree-Structured Expectation Propagation for Channel Decoding</a></p>
<p>15 0.52075279 <a title="292-lda-15" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>16 0.5184502 <a title="292-lda-16" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>17 0.51796412 <a title="292-lda-17" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>18 0.51685208 <a title="292-lda-18" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>19 0.51648587 <a title="292-lda-19" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>20 0.51565039 <a title="292-lda-20" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
