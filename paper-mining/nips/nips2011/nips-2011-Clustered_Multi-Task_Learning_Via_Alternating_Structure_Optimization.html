<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-51" href="#">nips2011-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</h1>
<br/><p>Source: <a title="nips-2011-51-pdf" href="http://papers.nips.cc/paper/4292-clustered-multi-task-learning-via-alternating-structure-optimization.pdf">pdf</a></p><p>Author: Jiayu Zhou, Jianhui Chen, Jieping Ye</p><p>Abstract: Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing signiﬁcant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is signiﬁcantly more efﬁcient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efﬁciency of the proposed algorithms. 1</p><p>Reference: <a title="nips-2011-51-reference" href="../nips2011_reference/nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. [sent-5, score-0.109]
</p><p>2 Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. [sent-6, score-0.17]
</p><p>3 As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i. [sent-8, score-0.295]
</p><p>4 , tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. [sent-10, score-0.292]
</p><p>5 The objectives in ASO and CMTL differ in how multiple tasks are related. [sent-11, score-0.135]
</p><p>6 Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing signiﬁcant new insights into ASO and CMTL as well as their inherent relationship. [sent-12, score-0.119]
</p><p>7 The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. [sent-13, score-0.226]
</p><p>8 We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is signiﬁcantly more efﬁcient especially for high-dimensional data. [sent-14, score-0.565]
</p><p>9 In addition, we present three algorithms for solving the convex CMTL formulation. [sent-15, score-0.084]
</p><p>10 A naive approach is to apply single task learning (STL) where each task is solved independently and thus the task relatedness is not exploited. [sent-18, score-0.21]
</p><p>11 Recently, there is a growing interest in multi-task learning (MTL), where we learn multiple related tasks simultaneously by extracting appropriate shared information across tasks. [sent-19, score-0.154]
</p><p>12 In MTL, multiple tasks are expected to beneﬁt from each other, resulting in improved generalization performance. [sent-20, score-0.109]
</p><p>13 Many different MTL approaches have been proposed in the past; they differ in how the relatedness among different tasks is modeled. [sent-23, score-0.161]
</p><p>14 [8] proposed the regularized MTL which constrained the models of all tasks to be close to each other. [sent-25, score-0.107]
</p><p>15 The task relatedness can also be modeled by constraining multiple tasks to share a common underlying structure [4, 6, 9, 10]. [sent-26, score-0.236]
</p><p>16 Ando and Zhang [5] proposed a structural learning formulation, which assumed multiple predictors for different tasks shared a common structure on the underlying predictor space. [sent-27, score-0.193]
</p><p>17 For linear predictors, they proposed the alternating structure optimization (ASO) that simultaneously performed inference on multiple tasks and discovered the shared low-dimensional predictive structure. [sent-28, score-0.309]
</p><p>18 One limitation of the original ASO formulation is that it involves a non-convex optimization problem and a globally optimal solution is not guaranteed. [sent-30, score-0.123]
</p><p>19 A convex relaxation of ASO called CASO was proposed and analyzed in [13]. [sent-31, score-0.151]
</p><p>20 Many existing MTL formulations are based on the assumption that all tasks are related. [sent-32, score-0.118]
</p><p>21 In practical applications, the tasks may exhibit a more sophisticated group structure where the models of tasks from the same group are closer to each other than those from a different group. [sent-33, score-0.199]
</p><p>22 There have been many prior work along this line of research, known as clustered multi-task learning (CMTL). [sent-34, score-0.093]
</p><p>23 In [14], the mutual relatedness of tasks was estimated and knowledge of one task could be transferred to other tasks in the same cluster. [sent-35, score-0.284]
</p><p>24 Bakker and Heskes [15] used clustered multi-task learning in a Bayesian setting by considering a mixture of Gaussians instead of single Gaussian priors. [sent-36, score-0.093]
</p><p>25 [8] proposed the task clustering regularization and showed how cluster information could be encoded in MTL, and however the group structure was required to be known a priori. [sent-38, score-0.19]
</p><p>26 In [17], a clustered MTL framework was proposed that simultaneously identiﬁed clusters and performed multi-task inference. [sent-41, score-0.111]
</p><p>27 Because the formulation is non-convex, they also proposed a convex relaxation to obtain a global optimum [17]. [sent-42, score-0.244]
</p><p>28 [18] used a similar idea to consider clustered tasks by introducing an inter-task regularization. [sent-44, score-0.182]
</p><p>29 , ASO which aims to identify a shared low-dimensional predictive structure for all tasks) which are based on the standard assumption that each task can learn equally well from any other task. [sent-47, score-0.151]
</p><p>30 Next, we show that the spectral relaxation of the clustering (on tasks) in CMTL and the projection (on the features) in ASO lead to an identical regularization, related to the negative Ky Fan k-norm of the weight matrix involving all task models, thus establishing their equivalence relationship. [sent-50, score-0.257]
</p><p>31 One major limitation of the ASO/CMTL formulation is that it involves a non-convex optimization, as the negative Ky Fan k-norm is concave. [sent-53, score-0.093]
</p><p>32 We propose a convex relaxation of CMTL, and establish the equivalence relationship between the proposed convex relaxation of CMTL and the convex ASO formulation proposed in [13]. [sent-54, score-0.565]
</p><p>33 We show that the proposed convex CMTL formulation is signiﬁcantly more efﬁcient especially for high-dimensional data. [sent-55, score-0.169]
</p><p>34 We further develop three algorithms for solving the convex CMTL formulation based on the block coordinate descent, accelerated projected gradient, and gradient descent, respectively. [sent-56, score-0.251]
</p><p>35 , (xi i , yni )} ⊂ Rd × R, and a linear predictive function fi : n 1 i T i fi (xj ) = wi xj , where wi is the weight vector of the i-th task, d is the data dimensionality, and ni is the number of samples of the i-th task. [sent-67, score-0.21]
</p><p>36 1  Alternating structure optimization  In ASO [5], all tasks are assumed to share a common feature space Θ ∈ Rh×d , where h ≤ min(m, d) is the dimensionality of the shared feature space and Θ has orthonormal columns, i. [sent-76, score-0.245]
</p><p>37 The predictive function of ASO is: fi (xi ) = wi xi = uT xi +vi Θxi , where the weight j j i j j T wi = ui + Θ vi consists of two components including the weight ui for the high-dimensional feature space and the weight vi for the low-dimensional space based on Θ. [sent-79, score-0.355]
</p><p>38 ASO minimizes the m following objective function: L(W ) + α i=1 ui 2 , subject to: ΘΘT = Ih , where α is the reg2 ularization parameter for task relatedness. [sent-80, score-0.092]
</p><p>39 We can further improve the formulation by including m a penalty, β i=1 wi 2 , to improve the generalization performance as in traditional supervised 2 learning. [sent-81, score-0.153]
</p><p>40 Since ui = wi − ΘT vi , we obtain the following ASO formulation: m  min  W,{vi },Θ:ΘΘT =Ih  2. [sent-82, score-0.145]
</p><p>41 2  L(W ) +  i=1  α wi − Θ T v i  2 2  + β wi  2 2  . [sent-83, score-0.12]
</p><p>42 (1)  Clustered multi-task learning  In CMTL, we assume that the tasks are clustered into k < m clusters, and the index set of the j-th cluster is deﬁned as Ij = {v|v ∈ cluster j}. [sent-84, score-0.268]
</p><p>43 3  Equivalence of ASO and CMTL  ∗ In the ASO formulation in Eq. [sent-91, score-0.093]
</p><p>44 Thus, the penalty in ASO has the following equivalent form: m  ΩASO (W, Θ) = i=1  α wi − ΘT Θwi  2 2  + β wi  2 2  = α tr W T W − tr W T ΘT ΘW + β tr W T W , resulting in the following equivalent ASO formulation: min L(W ) + ΩASO (W, Θ). [sent-93, score-0.674]
</p><p>45 W,Θ:ΘΘT =Ih  (6) (7)  The penalty of the ASO formulation in Eq. [sent-94, score-0.119]
</p><p>46 (7) looks very similar to the penalty of the CMTL formulation in Eq. [sent-95, score-0.119]
</p><p>47 (5), the matrix F is operated on the task dimension, as it is derived from the K-means clustering on the tasks; while in the ASO formulation in Eq. [sent-98, score-0.203]
</p><p>48 (7), the matrix Θ is operated on the feature dimension, as it aims to identify a shared low-dimensional predictive structure for all tasks. [sent-99, score-0.139]
</p><p>49 (7) are equivalent if the cluster number, k, in K-means equals to the size, h, of the shared low-dimensional feature space. [sent-105, score-0.105]
</p><p>50 Denote Q(W ) = L(W ) + (α + β) tr W T W , with α, β > 0. [sent-107, score-0.165]
</p><p>51 Then, CMTL and ASO solve the following optimization problems: min  W,F :F T F =Ip  Q(W ) − α tr W F F T W T ,  min  W,Θ:ΘΘT =Ip  Q(W ) − α tr W T ΘT ΘW ,  respectively. [sent-108, score-0.426]
</p><p>52 Thus, the optimal F and Θ for these two optimization problems are given by solving: [CMTL]  max  F :F T F =Ik  tr W F F T W T ,  [ASO]  max  Θ:ΘΘT =Ik  tr W T ΘT ΘW . [sent-110, score-0.36]
</p><p>53 3  Convex Relaxation of CMTL  The formulation in Eq. [sent-114, score-0.093]
</p><p>54 A natural approach is to perform a convex relaxation on CMTL. [sent-116, score-0.133]
</p><p>55 (5) as follows: ΩCMTL0 (W, F ) = α tr W ((1 + η)I − F F T )W T ,  (8)  where η is deﬁned as η = β/α > 0. [sent-118, score-0.165]
</p><p>56 (8) as the following equivalent form: ΩCMTL1 (W, F ) = αη(1 + η) tr W (ηI + F F T )−1 W T . [sent-121, score-0.165]
</p><p>57 (10)  Following [13, 17], we obtain the following convex relaxation of Eq. [sent-123, score-0.133]
</p><p>58 +  (11)  where ΩcCMTL (W, M ) is deﬁned as: ΩcCMTL (W, M ) = αη(1 + η) tr W (ηI + M )−1 W T . [sent-127, score-0.165]
</p><p>59 1  Equivalence of cASO and cCMTL  A convex relaxation (cASO) of the ASO formulation in Eq. [sent-131, score-0.226]
</p><p>60 tr (S) = h, S W,S  I, S ∈ Sd , +  (13)  where ΩcASO is deﬁned as: ΩcASO (W, S) = αη(1 + η) tr W T (ηI + S)−1 W . [sent-134, score-0.33]
</p><p>61 (13) are equivalent if the cluster number, k, in K-means equals to the size, h, of the shared low-dimensional feature space. [sent-144, score-0.105]
</p><p>62 Deﬁne the following two convex functions of W : gcCMTL (W ) = min tr W (ηI + M )−1 W T , s. [sent-146, score-0.256]
</p><p>63 tr (M ) = k, M  I, M ∈ Sm , +  M  (15)  and gcASO (W ) = min tr W T (ηI + S)−1 W , s. [sent-148, score-0.363]
</p><p>64 W : [cCMTL] min L(W ) + c · gCMTL (W ),  [cASO] min L(W ) + c · gASO (W ),  W  W  where c = αη(1 + η). [sent-154, score-0.066]
</p><p>65 It follows from the basic properties of the trace that: T tr W (ηI + M )−1 W T = tr (ηI + Λ1 )−1 P1 Q2 Σ2 QT P1 . [sent-168, score-0.33]
</p><p>66 (15) is thus equivalent to: d (1)  T T T min tr (ηI + Λ1 )−1 P1 Q2 Σ2 QT P1 , s. [sent-170, score-0.198]
</p><p>67 (18)  It follows that gcCMTL (W ) = tr (ηI + Λ∗ )−1 Σ2 . [sent-176, score-0.165]
</p><p>68 Similarly, we can show that gcASO (W ) = 1 tr (ηI + Λ∗ )−1 Σ2 , where 2 q  Λ∗ = argmin 2 Λ2  i=1  2 σi  q (2)  , s. [sent-177, score-0.165]
</p><p>69 In many practical + MTL problems the data dimensionality d is much larger than the task number m, and in such cases cCMTL is signiﬁcantly more efﬁcient in terms of both time and space. [sent-188, score-0.078]
</p><p>70 , Alternating Optimization Method (altCMTL), Accelerated Projected Gradient Method (apgCMTL), and Direct Gradient Descent Method (graCMTL), respectively, for solving the convex relaxation in Eq. [sent-193, score-0.159]
</p><p>71 The altCMTL algorithm involves the following two steps in each iteration: Optimization of W For a ﬁxed M , the optimal W can be obtained via solving: min L(W ) + c tr W (ηI + M )−1 W T . [sent-201, score-0.198]
</p><p>72 Optimization of M For a ﬁxed W , the optimal M can be obtained via solving: min tr W (ηI + M )−1 W T , s. [sent-206, score-0.198]
</p><p>73 2  Accelerated Projected Gradient Method  The accelerated projected gradient method (APG) has been applied to solve many machine learning formulations [24]. [sent-216, score-0.103]
</p><p>74 We apply APG to solve the cCMTL formulation in Eq. [sent-217, score-0.093]
</p><p>75 tr (MZ ) = k, MZ  I, MZ ∈ Sm , +  (21)  ˆ ˆ where the details about the construction of WS and MS can be found in [24]. [sent-222, score-0.165]
</p><p>76 tr (MZ ) = k, MZ  I, MZ ∈ Sm , +  (23)  ˆ where MS is not guaranteed to be positive semideﬁnite. [sent-234, score-0.165]
</p><p>77 (23) admits an analytical solution via solving a simple convex projection problem. [sent-236, score-0.1]
</p><p>78 Given the intermediate solution Wk−1 from the (k − 1)-th iteration of graCMTL, we compute the gradient of gCMTL (W ) and then apply the general gradient descent scheme [25] to obtain Wk . [sent-242, score-0.086]
</p><p>79 In the ground truth there are 100 tasks clustered into 5 groups. [sent-250, score-0.182]
</p><p>80 5  Experiments  In this section, we empirically evaluate the effectiveness and the efﬁciency of the proposed algorithms on synthetic and real-world data sets. [sent-257, score-0.065]
</p><p>81 Simulation Study We apply the proposed cCMTL formulation in Eq. [sent-262, score-0.111]
</p><p>82 (11) on a synthetic data set (with a pre-deﬁned cluster structure). [sent-263, score-0.07]
</p><p>83 We construct the synthetic data set following a procedure similar to the one in [17]: the constructed synthetic data set consists of 5 clusters, where each cluster includes 20 (regression) tasks and each task is represented by a weight vector of length d = 300. [sent-265, score-0.259]
</p><p>84 From the result we can observe (1) cCMTL is able to capture the cluster structure among tasks and achieves a small test error; (2) RegMTL is better than RidgeSTL in terms of test error. [sent-269, score-0.153]
</p><p>85 It however introduces unnecessary correlation among tasks possibly due to the assumption that all tasks are related; (3) In cCMTL we also notice some ‘noisy’ correlation, which may because of the spectral relaxation. [sent-270, score-0.196]
</p><p>86 0016 Effectiveness Comparison Next, we empirically evaluate the effectiveness of the cCMTL formulation in comparison with RidgeSTL and RegMTL using real world benchmark datasets including the School data1 and the Sarcos data2 . [sent-335, score-0.131]
</p><p>87 Efﬁciency Comparison We compare the efﬁciency of the three algorithms including altCMTL, apgCMTLand graCMTL for solving the cCMTL formulation in Eq. [sent-349, score-0.119]
</p><p>88 Speciﬁcally, we study how the feature dimensionality, the sample size, and the task number affect the required computation cost (in seconds) for convergence. [sent-353, score-0.069]
</p><p>89 Because in Yahoo data the task number is very small, we construct a synthetic data for the third experiment. [sent-356, score-0.079]
</p><p>90 In the ﬁrst experiment, we vary the feature dimensionality in the set [500 : 500 : 2500] with the sample size ﬁxed at 4000 and the task numbers ﬁxed at 17. [sent-357, score-0.111]
</p><p>91 In the second experiment, we vary the sample size in the set [3000 : 1000 : 9000] with the dimensionality ﬁxed at 500 and the task number ﬁxed at 17. [sent-359, score-0.094]
</p><p>92 In the third experiment, we vary the task number in the set [10 : 10 : 190] with the feature dimensionality ﬁxed at 600 and the sample size ﬁxed at 2000. [sent-362, score-0.111]
</p><p>93 The employed synthetic data set is constructed as follows: for each task, we generate the entries of the data matrix Xi from N (0, 1), and generate the entries of the weight vector from N (0, 1), the response vector yi is computed as yi = Xi wi + ξ, where ξ ∼ N (0, 0. [sent-363, score-0.108]
</p><p>94 6  Conclusion  In this paper we establish the equivalence relationship between two multi-task learning techniques: alternating structure optimization (ASO) and clustered multi-task learning (CMTL). [sent-367, score-0.309]
</p><p>95 We further establish the equivalence relationship between our proposed convex relaxation of CMTL and an existing convex relaxation of ASO. [sent-368, score-0.396]
</p><p>96 In addition, we propose three algorithms for solving the convex CMTL formulation and demonstrate their effectiveness and efﬁciency on benchmark datasets. [sent-369, score-0.215]
</p><p>97 A convex optimization approach to modeling consumer heterogeneity in conjoint estimation. [sent-379, score-0.088]
</p><p>98 A framework for learning predictive structures from multiple tasks and unlabeled data. [sent-406, score-0.142]
</p><p>99 A convex formulation for learning shared structures from multiple tasks. [sent-459, score-0.216]
</p><p>100 Clustering learning tasks and the selective cross-task transfer of knowledge. [sent-465, score-0.089]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cmtl', 0.539), ('aso', 0.447), ('ccmtl', 0.355), ('mtl', 0.206), ('altcmtl', 0.199), ('caso', 0.184), ('gracmtl', 0.17), ('tr', 0.165), ('mz', 0.129), ('apgcmtl', 0.128), ('regmtl', 0.114), ('ridgestl', 0.114), ('gcmtl', 0.099), ('formulation', 0.093), ('clustered', 0.093), ('wz', 0.092), ('tasks', 0.089), ('relaxation', 0.075), ('nmse', 0.071), ('wi', 0.06), ('convex', 0.058), ('equivalence', 0.056), ('relatedness', 0.054), ('alternating', 0.053), ('task', 0.052), ('evgeniou', 0.048), ('sm', 0.047), ('shared', 0.045), ('cluster', 0.043), ('amse', 0.043), ('gcaso', 0.043), ('gccmtl', 0.043), ('sarcos', 0.043), ('ik', 0.039), ('ih', 0.039), ('wk', 0.038), ('ws', 0.037), ('clustering', 0.035), ('supplemental', 0.034), ('min', 0.033), ('predictive', 0.033), ('vi', 0.032), ('relationship', 0.032), ('inherent', 0.031), ('optimization', 0.03), ('gradient', 0.03), ('formulations', 0.029), ('gaso', 0.028), ('ky', 0.028), ('ciency', 0.027), ('synthetic', 0.027), ('solving', 0.026), ('accelerated', 0.026), ('objectives', 0.026), ('descent', 0.026), ('dimensionality', 0.026), ('penalty', 0.026), ('seconds', 0.026), ('establish', 0.024), ('tenth', 0.023), ('operated', 0.023), ('sse', 0.023), ('ms', 0.022), ('sd', 0.022), ('apg', 0.022), ('stl', 0.022), ('regularization', 0.021), ('weight', 0.021), ('structure', 0.021), ('effectiveness', 0.02), ('marketing', 0.02), ('bakker', 0.02), ('wv', 0.02), ('ui', 0.02), ('multiple', 0.02), ('qt', 0.02), ('objective', 0.02), ('completes', 0.019), ('yahoo', 0.019), ('minw', 0.019), ('ando', 0.019), ('xue', 0.019), ('fan', 0.019), ('wm', 0.019), ('school', 0.018), ('benchmark', 0.018), ('projected', 0.018), ('ding', 0.018), ('pontil', 0.018), ('fi', 0.018), ('proposed', 0.018), ('spectral', 0.018), ('micchelli', 0.017), ('argyriou', 0.017), ('reformulate', 0.017), ('feature', 0.017), ('vary', 0.016), ('admits', 0.016), ('ij', 0.016), ('record', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="51-tfidf-1" href="./nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization.html">51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p>
<p>Author: Jiayu Zhou, Jianhui Chen, Jieping Ye</p><p>Abstract: Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing signiﬁcant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is signiﬁcantly more efﬁcient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efﬁciency of the proposed algorithms. 1</p><p>2 0.12506786 <a title="51-tfidf-2" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>Author: Nico Goernitz, Christian Widmer, Georg Zeller, Andre Kahles, Gunnar Rätsch, Sören Sonnenburg</p><p>Abstract: We present a novel regularization-based Multitask Learning (MTL) formulation for Structured Output (SO) prediction for the case of hierarchical task relations. Structured output prediction often leads to difﬁcult inference problems and hence requires large amounts of training data to obtain accurate models. We propose to use MTL to exploit additional information from related learning tasks by means of hierarchical regularization. Training SO models on the combined set of examples from multiple tasks can easily become infeasible for real world applications. To be able to solve the optimization problems underlying multitask structured output learning, we propose an efﬁcient algorithm based on bundle-methods. We demonstrate the performance of our approach in applications from the domain of computational biology addressing the key problem of gene ﬁnding. We show that 1) our proposed solver achieves much faster convergence than previous methods and 2) that the Hierarchical SO-MTL approach outperforms considered non-MTL methods. 1</p><p>3 0.083583795 <a title="51-tfidf-3" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>Author: Shengbo Guo, Onno Zoeter, Cédric Archambeau</p><p>Abstract: We propose a new sparse Bayesian model for multi-task regression and classiﬁcation. The model is able to capture correlations between tasks, or more speciﬁcally a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classiﬁcation tasks it achieves competitive predictive performance compared to previously proposed methods. 1</p><p>4 0.064059548 <a title="51-tfidf-4" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<p>Author: Abhishek Kumar, Piyush Rai, Hal Daume</p><p>Abstract: In many clustering problems, we have access to multiple views of the data each of which could be individually used for clustering. Exploiting information from multiple views, one can hope to ﬁnd a clustering that is more accurate than the ones obtained using the individual views. Often these different views admit same underlying clustering of the data, so we can approach this problem by looking for clusterings that are consistent across the views, i.e., corresponding data points in each view should have same cluster membership. We propose a spectral clustering framework that achieves this goal by co-regularizing the clustering hypotheses, and propose two co-regularization schemes to accomplish this. Experimental comparisons with a number of baselines on two synthetic and three real-world datasets establish the efﬁcacy of our proposed approaches.</p><p>5 0.061339792 <a title="51-tfidf-5" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><p>6 0.053057764 <a title="51-tfidf-6" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>7 0.050166205 <a title="51-tfidf-7" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>8 0.049577534 <a title="51-tfidf-8" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>9 0.0451927 <a title="51-tfidf-9" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>10 0.043968529 <a title="51-tfidf-10" href="./nips-2011-Better_Mini-Batch_Algorithms_via_Accelerated_Gradient_Methods.html">46 nips-2011-Better Mini-Batch Algorithms via Accelerated Gradient Methods</a></p>
<p>11 0.043089811 <a title="51-tfidf-11" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>12 0.042123433 <a title="51-tfidf-12" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>13 0.041672248 <a title="51-tfidf-13" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>14 0.041512061 <a title="51-tfidf-14" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>15 0.041180365 <a title="51-tfidf-15" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>16 0.037714697 <a title="51-tfidf-16" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>17 0.036426011 <a title="51-tfidf-17" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>18 0.035247143 <a title="51-tfidf-18" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>19 0.03481672 <a title="51-tfidf-19" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>20 0.034720886 <a title="51-tfidf-20" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, 0.015), (2, -0.04), (3, -0.062), (4, -0.042), (5, 0.028), (6, -0.004), (7, 0.006), (8, -0.008), (9, 0.038), (10, 0.013), (11, 0.048), (12, 0.008), (13, -0.039), (14, -0.04), (15, 0.069), (16, -0.094), (17, 0.05), (18, 0.093), (19, -0.057), (20, 0.033), (21, -0.021), (22, 0.009), (23, 0.07), (24, -0.045), (25, -0.014), (26, 0.017), (27, -0.026), (28, -0.017), (29, -0.021), (30, -0.014), (31, 0.005), (32, -0.016), (33, 0.054), (34, -0.025), (35, 0.087), (36, -0.041), (37, -0.024), (38, -0.107), (39, -0.074), (40, -0.028), (41, -0.015), (42, 0.059), (43, -0.052), (44, -0.048), (45, 0.001), (46, 0.01), (47, 0.045), (48, 0.062), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90008134 <a title="51-lsi-1" href="./nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization.html">51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p>
<p>Author: Jiayu Zhou, Jianhui Chen, Jieping Ye</p><p>Abstract: Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing signiﬁcant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is signiﬁcantly more efﬁcient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efﬁciency of the proposed algorithms. 1</p><p>2 0.65899855 <a title="51-lsi-2" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>Author: Nico Goernitz, Christian Widmer, Georg Zeller, Andre Kahles, Gunnar Rätsch, Sören Sonnenburg</p><p>Abstract: We present a novel regularization-based Multitask Learning (MTL) formulation for Structured Output (SO) prediction for the case of hierarchical task relations. Structured output prediction often leads to difﬁcult inference problems and hence requires large amounts of training data to obtain accurate models. We propose to use MTL to exploit additional information from related learning tasks by means of hierarchical regularization. Training SO models on the combined set of examples from multiple tasks can easily become infeasible for real world applications. To be able to solve the optimization problems underlying multitask structured output learning, we propose an efﬁcient algorithm based on bundle-methods. We demonstrate the performance of our approach in applications from the domain of computational biology addressing the key problem of gene ﬁnding. We show that 1) our proposed solver achieves much faster convergence than previous methods and 2) that the Hierarchical SO-MTL approach outperforms considered non-MTL methods. 1</p><p>3 0.59828371 <a title="51-lsi-3" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>Author: Dan Zhang, Yan Liu, Luo Si, Jian Zhang, Richard D. Lawrence</p><p>Abstract: Most existing Multiple-Instance Learning (MIL) algorithms assume data instances and/or data bags are independently and identically distributed. But there often exists rich additional dependency/structure information between instances/bags within many applications of MIL. Ignoring this structure information limits the performance of existing MIL algorithms. This paper explores the research problem as multiple instance learning on structured data (MILSD) and formulates a novel framework that considers additional structure information. In particular, an effective and efﬁcient optimization algorithm has been proposed to solve the original non-convex optimization problem by using a combination of ConcaveConvex Constraint Programming (CCCP) method and an adapted Cutting Plane method, which deals with two sets of constraints caused by learning on instances within individual bags and learning on structured data. Our method has the nice convergence property, with speciﬁed precision on each set of constraints. Experimental results on three different applications, i.e., webpage classiﬁcation, market targeting, and protein fold identiﬁcation, clearly demonstrate the advantages of the proposed method over state-of-the-art methods. 1</p><p>4 0.51974553 <a title="51-lsi-4" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>Author: David Adametz, Volker Roth</p><p>Abstract: A Bayesian approach to partitioning distance matrices is presented. It is inspired by the Translation-invariant Wishart-Dirichlet process (TIWD) in [1] and shares a number of advantageous properties like the fully probabilistic nature of the inference model, automatic selection of the number of clusters and applicability in semi-supervised settings. In addition, our method (which we call fastTIWD) overcomes the main shortcoming of the original TIWD, namely its high computational costs. The fastTIWD reduces the workload in each iteration of a Gibbs sampler from O(n3 ) in the TIWD to O(n2 ). Our experiments show that the cost reduction does not compromise the quality of the inferred partitions. With this new method it is now possible to ‘mine’ large relational datasets with a probabilistic model, thereby automatically detecting new and potentially interesting clusters. 1</p><p>5 0.5167796 <a title="51-lsi-5" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><p>6 0.50469226 <a title="51-lsi-6" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>7 0.4826059 <a title="51-lsi-7" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>8 0.46138239 <a title="51-lsi-8" href="./nips-2011-Submodular_Multi-Label_Learning.html">277 nips-2011-Submodular Multi-Label Learning</a></p>
<p>9 0.45526171 <a title="51-lsi-9" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>10 0.45392668 <a title="51-lsi-10" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>11 0.45015693 <a title="51-lsi-11" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>12 0.44888574 <a title="51-lsi-12" href="./nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</a></p>
<p>13 0.44806194 <a title="51-lsi-13" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>14 0.44322887 <a title="51-lsi-14" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>15 0.43229893 <a title="51-lsi-15" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>16 0.42542925 <a title="51-lsi-16" href="./nips-2011-Regularized_Laplacian_Estimation_and_Fast_Eigenvector_Approximation.html">236 nips-2011-Regularized Laplacian Estimation and Fast Eigenvector Approximation</a></p>
<p>17 0.4238849 <a title="51-lsi-17" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>18 0.4233281 <a title="51-lsi-18" href="./nips-2011-RTRMC%3A_A_Riemannian_trust-region_method_for_low-rank_matrix_completion.html">230 nips-2011-RTRMC: A Riemannian trust-region method for low-rank matrix completion</a></p>
<p>19 0.41719925 <a title="51-lsi-19" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>20 0.41636848 <a title="51-lsi-20" href="./nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">260 nips-2011-Sparse Features for PCA-Like Linear Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.035), (4, 0.032), (20, 0.026), (26, 0.023), (31, 0.062), (33, 0.036), (43, 0.05), (45, 0.12), (57, 0.026), (69, 0.34), (74, 0.043), (83, 0.031), (99, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69950378 <a title="51-lda-1" href="./nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization.html">51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p>
<p>Author: Jiayu Zhou, Jianhui Chen, Jieping Ye</p><p>Abstract: Multi-task learning (MTL) learns multiple related tasks simultaneously to improve generalization performance. Alternating structure optimization (ASO) is a popular MTL method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks. It has been applied successfully in many real world applications. As an alternative MTL approach, clustered multi-task learning (CMTL) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori. The objectives in ASO and CMTL differ in how multiple tasks are related. Interestingly, we show in this paper the equivalence relationship between ASO and CMTL, providing signiﬁcant new insights into ASO and CMTL as well as their inherent relationship. The CMTL formulation is non-convex, and we adopt a convex relaxation to the CMTL formulation. We further establish the equivalence relationship between the proposed convex relaxation of CMTL and an existing convex relaxation of ASO, and show that the proposed convex CMTL formulation is signiﬁcantly more efﬁcient especially for high-dimensional data. In addition, we present three algorithms for solving the convex CMTL formulation. We report experimental results on benchmark datasets to demonstrate the efﬁciency of the proposed algorithms. 1</p><p>2 0.65613008 <a title="51-lda-2" href="./nips-2011-A_rational_model_of_causal_inference_with_continuous_causes.html">15 nips-2011-A rational model of causal inference with continuous causes</a></p>
<p>Author: Thomas L. Griffiths, Michael James</p><p>Abstract: Rational models of causal induction have been successful in accounting for people’s judgments about causal relationships. However, these models have focused on explaining inferences from discrete data of the kind that can be summarized in a 2× 2 contingency table. This severely limits the scope of these models, since the world often provides non-binary data. We develop a new rational model of causal induction using continuous dimensions, which aims to diminish the gap between empirical and theoretical approaches and real-world causal induction. This model successfully predicts human judgments from previous studies better than models of discrete causal inference, and outperforms several other plausible models of causal induction with continuous causes in accounting for people’s inferences in a new experiment. 1</p><p>3 0.46315771 <a title="51-lda-3" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>4 0.4546659 <a title="51-lda-4" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><p>5 0.45430976 <a title="51-lda-5" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>Author: Ehsan Elhamifar, René Vidal</p><p>Abstract: We propose an algorithm called Sparse Manifold Clustering and Embedding (SMCE) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds. Similar to most dimensionality reduction methods, SMCE ﬁnds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights. The key difference is that SMCE ﬁnds both the neighbors and the weights automatically. This is done by solving a sparse optimization problem, which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional afﬁne subspace. The optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding. Moreover, the size of the optimal neighborhood of a data point, which can be different for different points, provides an estimate of the dimension of the manifold to which the point belongs. Experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other, manifolds with non-uniform sampling and holes, as well as estimate the intrinsic dimensions of the manifolds. 1 1.1</p><p>6 0.45372078 <a title="51-lda-6" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>7 0.45271894 <a title="51-lda-7" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>8 0.45121631 <a title="51-lda-8" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>9 0.45020267 <a title="51-lda-9" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>10 0.44956744 <a title="51-lda-10" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>11 0.44948447 <a title="51-lda-11" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>12 0.44945249 <a title="51-lda-12" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>13 0.4493134 <a title="51-lda-13" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>14 0.44902256 <a title="51-lda-14" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>15 0.44779781 <a title="51-lda-15" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>16 0.4475477 <a title="51-lda-16" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>17 0.44708854 <a title="51-lda-17" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>18 0.4461146 <a title="51-lda-18" href="./nips-2011-Maximum_Covariance_Unfolding_%3A_Manifold_Learning_for_Bimodal_Data.html">167 nips-2011-Maximum Covariance Unfolding : Manifold Learning for Bimodal Data</a></p>
<p>19 0.44584626 <a title="51-lda-19" href="./nips-2011-Shaping_Level_Sets_with_Submodular_Functions.html">251 nips-2011-Shaping Level Sets with Submodular Functions</a></p>
<p>20 0.44579494 <a title="51-lda-20" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
