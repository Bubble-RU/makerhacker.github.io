<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-189" href="#">nips2011-189</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</h1>
<br/><p>Source: <a title="nips-2011-189-pdf" href="http://papers.nips.cc/paper/4265-non-parametric-group-orthogonal-matching-pursuit-for-sparse-learning-with-multiple-kernels.pdf">pdf</a></p><p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>Reference: <a title="nips-2011-189-reference" href="../nips2011_reference/nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. [sent-6, score-0.62]
</p><p>2 This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. [sent-7, score-0.215]
</p><p>3 While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e. [sent-8, score-0.125]
</p><p>4 , OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. [sent-12, score-0.198]
</p><p>5 Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. [sent-14, score-0.387]
</p><p>6 The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. [sent-15, score-0.153]
</p><p>7 However, choosing an appropriate kernel and tuning the corresponding hyper-parameters can be highly challenging, especially when little is known about the task at hand. [sent-17, score-0.241]
</p><p>8 This strongly suggests avoiding the risks and limitations of single kernel selection by considering ﬂexible combinations of multiple kernels. [sent-21, score-0.336]
</p><p>9 Furthermore, it is appealing to impose sparsity to discard noisy data sources. [sent-22, score-0.122]
</p><p>10 As several papers have provided evidence in favor of using multiple kernels (e. [sent-23, score-0.283]
</p><p>11 [19, 14, 7]), the multiple kernel learning problem (MKL) has generated a large body of recent work [13, 5, 24, 33], and become the focal point of the intersection between non-parametric function estimation and sparse learning methods traditionally explored in linear settings. [sent-25, score-0.36]
</p><p>12 Given a convex loss function, the MKL problem is usually formulated as the minimization of empirical risk together with a mixed norm regularizer, e. [sent-26, score-0.202]
</p><p>13 In constraining the combination of kernels, the l1 penalty is of particular interest as it encourages sparsity in the supporting kernels, which is highly desirable when the number of kernels considered is large. [sent-30, score-0.409]
</p><p>14 For linear models, some strong theoretical performance guarantees and empirical support have been provided for OMP [31] and its extension for variable group selection, Group-OMP [16]. [sent-34, score-0.123]
</p><p>15 In terms of the feature space (as opposed to function space) perspective of kernel methods, this allows Group-OMP to handle groups that can potentially contain inﬁnite features. [sent-39, score-0.241]
</p><p>16 Rather than imposing a mixed l1 /RKHSnorm regularizer as in group-Lasso based MKL, a group-OMP based approach allows us to consider the exact sparse kernel selection problem via l0 regularization instead. [sent-41, score-0.524]
</p><p>17 Note that in contrast to the group-lasso penalty, the l0 penalty by itself has no effect on the smoothness of each individual component. [sent-42, score-0.111]
</p><p>18 This allows for a clear decoupling between the role of the smoothness regularizer (namely, an RKHS regularizer) and the sparsity regularizer (via the l0 penalty). [sent-43, score-0.357]
</p><p>19 In this paper, we focus on multiple kernel learning with Regularized least squares (RLS). [sent-45, score-0.316]
</p><p>20 We derive conditions analogous to OMP [27] and Group-OMP [16] to guarantee the “correctness” of kernel selection. [sent-47, score-0.241]
</p><p>21 We close this paper with empirical studies on simulated and real-world datasets that conﬁrm the value of our methods. [sent-48, score-0.087]
</p><p>22 ⊕ HN = {f : X → R|f (x) =  j=1  fj (x), x ∈ X , fj ∈ Hj , j = 1 . [sent-60, score-0.122]
</p><p>23 N }  Let us equip this space with the following lp norms,   1 p  N  N   fj p j  : f (x) = f lp (H) = inf  fj (x), x ∈ X , fj ∈ Hj , j = 1 . [sent-63, score-0.317]
</p><p>24 2  Let q =  p 2−p  and let us deﬁne the q-convex hull of the set of kernel functions to be the following,   N N   q γj = 1, γj ≥ 0 γj kj (x, z), coq (k1 . [sent-68, score-0.472]
</p><p>25 It is easy to see that the non-negative combination of kernels, kγ , is itself a valid kernel with an associated RKHS Hkγ . [sent-72, score-0.241]
</p><p>26 With this deﬁnition, [17] show the following, f  lp (H)  f  = inf γ  Hk γ , k γ  ∈ coq (k1 . [sent-73, score-0.112]
</p><p>27 kN )  (3)  This relationship connects Tikhonov regularization with lp norms over H to regularization over RKHSs parameterized by the kernel functions kγ . [sent-76, score-0.472]
</p><p>28 This leads to a large family of “multiple kernel learning” algorithms (whose variants are also sometimes referred to as lq -MKL) where the basic idea is to solve an equivalent problem, arg min f ∈Hkγ ,γ∈△q  1 l  l  V (yi , f (xi )) + λ f  2 Hk γ  (4)  i=1  where △q = {γ ∈ RN : γ q = 1, ∀n γj ≥ 0}. [sent-77, score-0.435]
</p><p>29 The case where p = 1 is of particular interest in the setting when the size of the RKHS dictionary is large but the unknown target function can be approximated in a much smaller number of RKHSs. [sent-82, score-0.151]
</p><p>30 This leads to a large family of sparse multiple kernel learning algorithms that have a strong connection to the Group Lasso [2, 20, 29]. [sent-83, score-0.36]
</p><p>31 We now pose the following exact sparse kernel selection problem, arg min f ∈H  1 l  l  V (yi , f (xi )) + λ f i=1  2 l2 (H)  subject to  f  l0 (H)  ≤s  (5)  It is important to note the following: when using a dictionary of universal kernels, e. [sent-85, score-0.558]
</p><p>32 , Gaussian kernels with different bandwidths, the presence of the regularization term f 22 (H) is critical (i. [sent-87, score-0.285]
</p><p>33 In other words, the kernel selection problem is ill-posed. [sent-90, score-0.292]
</p><p>34 While conceptually simple, our formulation is quite different from those proposed earlier since the role of a smoothness regularizer (via the f 22 (H) penalty) is l decoupled from the role of a sparsity regularizer (via the constraint on f l0 (H) ≤ s). [sent-91, score-0.357]
</p><p>35 It has been observed in recent work [10, 29] on l1 -MKL that sparsity alone does not lead it to improvements in real-world empirical tasks and hence several methods have been proposed to explore lq -norm MKL with q > 1 in Eqn. [sent-93, score-0.304]
</p><p>36 4, making MKL depart away from sparsity in kernel combinations. [sent-94, score-0.363]
</p><p>37 Our approach gives a direct knob both on smoothness (via λ) and sparsity (via s) with a solution path along these dimensions that differs from that offered by Group-Lasso based lq -MKL as q is varied. [sent-96, score-0.337]
</p><p>38 If kernels arise from different subsets of input variables, our approach is also related to sparse additive models [18]. [sent-98, score-0.314]
</p><p>39 In the description of the algorithm, our notation is as follows: For any function f belonging to an RKHS Fk with kernel function l k(·, ·), we denote the regularized objective function as, Rλ (f, y) = 1 i=1 (yi −f (xi ))2 +λ f Fk l 3  where · F denotes the RKHS norm. [sent-103, score-0.304]
</p><p>40 Recall that the minimizer f ⋆ = arg minf ∈F Rλ (f, y) is given by solving the linear system, α = (K + λlI)−1 y where K is the gram matrix of the kerl nel on the labeled data, and by setting f ⋆ (x) = i=1 αi k(x, xi ). [sent-104, score-0.209]
</p><p>41 The MKL-GOMP procedure iteratively expands the hypothesis space, HG (1) ⊆ HG (2) . [sent-107, score-0.094]
</p><p>42 ⊆ HG (i) , by greedily selecting kernels from a given dictionary, where G (i) ⊂ {1 . [sent-110, score-0.239]
</p><p>43 Note that each HG is an RKHS with kernel j∈G kj (see Section 6 in [1]). [sent-114, score-0.427]
</p><p>44 The selection criteria is the best improvement, I(f (i) , Hj ), given by a new hypothesis space Hj in reducing the norm of the current residual r(i) = y − f (i) where f (i) = [f (i) (x1 ) . [sent-115, score-0.205]
</p><p>45 Once a kernel is selected, the function is re-estimated by learning in HG (i) . [sent-120, score-0.241]
</p><p>46 Note that since HG is an RKHS whose kernel function is the sum j∈G kj , we can use a simple RLS linear system solver for reﬁtting. [sent-121, score-0.452]
</p><p>47 Unlike group-Lasso based MKL, we do not need an iterative kernel reweighting step which essentially arises as a mechanism to transform the less convenient group sparsity norms into reweighted squared RKHS norms. [sent-122, score-0.528]
</p><p>48 xl ]T , Label vector y ∈ Rl , Kernel Dictionary {kj (·, ·)}N , Precision ǫ > 0 j=1  ◮ Output: Selected Kernels G (i) and a function f (i) ∈ HG (i) ◮ Initialization: G (0) = ∅, f (0) = 0, set residual r(0) = y ◮ for i = 0, 1, 2, . [sent-127, score-0.09]
</p><p>49 Kernel Selection: For all j ∈ G (i) , set: / I(f (i) , Hj ) = r(i) 2 − ming∈Hj Rλ (g, r(i) ) 2 = r(i)T I − λ(Kj + λlI)−1 r(i) (i) Pick j = arg maxj ∈G (i) I(f (i) , Hj ) /  2. [sent-131, score-0.079]
</p><p>50 end  Remarks: Note that our algorithm can be applied to multivariate problems with group structure among outputs similar to Multivariate Group-OMP [15]. [sent-139, score-0.165]
</p><p>51 In particular, in our experiments on multiclass datasets, we treat all outputs as a single group and evaluate each kernel for selection based on how well the total residual is reduced across all outputs simultaneously. [sent-140, score-0.535]
</p><p>52 This is because matching pursuit methods can be deemed to solve an exact sparse problem approximately, while regularized methods (e. [sent-148, score-0.357]
</p><p>53 We therefore need to show that MKL-GOMP recovers a solution that is close to an optimum solution of the exact sparse problem. [sent-151, score-0.1]
</p><p>54 Consider the hypothesis space of sufﬁciently sparse and smooth functions1 , Hτ,s = f ∈ H : f  2 l2 (H)  ≤ τ, f  l0 (H)  ≤s  Let δ ∈ (0, 1) and κ = supx∈X ,j=1. [sent-154, score-0.152]
</p><p>55 Deﬁne, f = arg minf ∈Hτ,s 1 i=1 (yi − f (xi )) to be the empirical risk minimizer and f ⋆ = l 2 arg minf ∈Hτ,s R(f ) to be the true risk minimizer in Hτ,s where R(f ) = E(x,y)∼ρ (y − f (x)) denotes the true risk. [sent-159, score-0.42]
</p><p>56 In our case, any collection of s-sparse functions from a dictionary of N RKHSs reduces to a single RKHS whose kernel is the sum of s base kernels, and hence the corresponding trace can be bounded √ by lsκ for all possible subsets of size s. [sent-163, score-0.365]
</p><p>57 Once it is established that the empirical Rademacher complexity of Hλ,s is upper bounded by sκτ , the generalization bound follows from well-known l results [6] tailored to regularized least squares regression with bounded target variable. [sent-164, score-0.151]
</p><p>58 By contrast, our greedy approach with sequential regularized risk minimization imposes direct control over degree of sparsity as well as smoothness, and hence the Rademacher complexity in our case is independent of N . [sent-169, score-0.323]
</p><p>59 A critical difference between l1 -norm MKL and sparse greedy approximations, however, is that the former is convex and hence the empirical risk can be minimized exactly in the hypothesis space whose complexity is bounded by Rademacher analysis. [sent-171, score-0.256]
</p><p>60 , fρ ∈ HGgood for some subset Ggood of s “good” kernels and that it is sufﬁciently smooth in the sense that for some ˆ Rλ (f, y) gives near λ > 0, given sufﬁcient samples, the empirical minimizer f = arg minf ∈HG good optimal generalization as per Theorem 1. [sent-176, score-0.438]
</p><p>61 1 Note that Tikhonov regularization using a penalty term λ · 2 , and Ivanov Regularization which uses a ball constraint · 2 ≤ τ return identical solutions for some one-to-one correspondence between λ and τ . [sent-178, score-0.094]
</p><p>62 , a good kernel of/ fers better greedy improvement, then it is clear that the algorithm correctly expands the hypothesis space and never makes a mistake. [sent-183, score-0.383]
</p><p>63 Without loss of generality, let us rearrange the dictionary so that Ggood = {1 . [sent-184, score-0.156]
</p><p>64 Given the kernel dictionary {kj (·, ·)}N with associated gram matrices {Kj }N over j=1 i=1 the labeled data, MKL-GOMP correctly recovers the good kernels, i. [sent-208, score-0.434]
</p><p>65 This result is analogous to sparse recovery conditions for OMP and l1 methods and their (linear) group counterparts. [sent-215, score-0.216]
</p><p>66 For Group-OMP [16], the condition generalizes to involve a group sensitive matrix norm on the same matrix objects. [sent-218, score-0.143]
</p><p>67 5  Empirical Studies  We report empirical results on a collection of simulated datasets and 3 classiﬁcation problems from computational cell biology. [sent-223, score-0.087]
</p><p>68 In all experiments, as in [10, 33], candidate kernels are normalized multiplicatively to have uniform variance of data points in their associated feature spaces. [sent-224, score-0.239]
</p><p>69 1  Adaptability to Data Sparsity - Simulated Setting  We adapt the experimental setting proposed by [10] where the sparsity of the target function is explicitly controlled, and the optimal subset of kernels is varied from requiring the entire dictionary to requiring a single kernel. [sent-226, score-0.572]
</p><p>70 Our goal is to study the solution paths offered by MKL-GOMP in comparison to lq -norm MKL. [sent-227, score-0.152]
</p><p>71 We implemented 2  lq -MKL with SVM hinge loss behaves similarly. [sent-229, score-0.223]
</p><p>72 16 0 44 66 82 92 98 v(θ) = fraction of noise kernels [in %]  0. [sent-234, score-0.268]
</p><p>73 04 0  44  66  82  92  98  0  v(θ) = fraction of noise kernels [in %]  44  66  82  92  98  v(θ) = fraction of noise kernels [in %]  lq -norm MKL for regularized least squares (RLS) using an alternating minimization scheme adapted from [17, 29]. [sent-240, score-0.845]
</p><p>74 The fraction of zero components in θ is a measure for the feature sparsity of the learning problem. [sent-243, score-0.151]
</p><p>75 For each dataset, a linear kernel (normalized as in [10]) is generated from each feature and the resulting dictionary is input to MKL-GOMP and lq -norm MKL. [sent-244, score-0.517]
</p><p>76 For each run, the validation error is monitored as kernel selection progresses in MKL-GOMP and the number of kernels with smallest validation error are chosen. [sent-246, score-0.597]
</p><p>77 The regularization parameters for both MKL-GOMP and lq norm MKL are similarly chosen using the validation set. [sent-247, score-0.281]
</p><p>78 1 shows test error rates as a function of sparsity of the target function: from non-sparse (all kernels needed) to extremely sparse (only 1 kernel needed). [sent-249, score-0.704]
</p><p>79 We recover the observations also made in [10]: l1 -norm MKL excels in extremely sparse settings where a single kernel carries the whole discriminative information of the learning problem. [sent-250, score-0.316]
</p><p>80 As reported in [11], the elastic net MKL approach of [26] performs similar to l1 -MKL in the hinge loss case. [sent-253, score-0.112]
</p><p>81 As can be seen in the Figure, the error curve of MKL-GOMP tends to be below the lower envelope of the error rates given by lq -MKL solutions. [sent-254, score-0.152]
</p><p>82 To adapt to the sparsity of the problem, lq methods clearly need to tune q requiring several fresh invocations of the appropriate lq -MKL solver. [sent-255, score-0.456]
</p><p>83 On the other hand, in MKL-GOMP the hypothesis space grows as function of the iteration number and the solution trajectory naturally expands sequentially in the direction of decreasing sparsity. [sent-256, score-0.094]
</p><p>84 1 shows the number of kernels selected by MKL-GOMP and the optimal value of λ, suggesting that MKL-GOMP adapts to the sparsity and smoothness of the learning problem. [sent-258, score-0.424]
</p><p>85 de/raetsch/suppl/protsubloc together with a dictionary of 69 kernels derived with biological insight: 2 kernels on phylogenetic trees, 3 kernels based on similarity to known proteins (BLAST E-values), and 64 kernels based on amino-acid sequence patterns. [sent-265, score-1.12]
</p><p>86 The statistics of the three datasets are as follows: PSORT + has 541 proteins labeled with 4 location classes, PSORT- has 1444 proteins in 5 classes and PLANT is 3  Provided by the authors of [10] at mldata. [sent-266, score-0.134]
</p><p>87 org/repository/data/viewslug/mkl-toy/  7  Performance (higher is better)  100  mklgomp  mcmkl sum  95  single other  90  85  80  75 psort+  psort−  plant  Figure 2: Protein Subcellular Localization Results a 4-class problem with 940 proteins. [sent-267, score-0.202]
</p><p>88 Figure 2 compare MKL-GOMP against MCMKL, baselines such as using the sum of all the kernels and using the best single kernel, and results from other prediction systems proposed in the literature. [sent-270, score-0.239]
</p><p>89 As can be seen, MKL-GOMP slightly outperforms MCMKL on PSORT + an PSORT- datasets and is slightly worse on PLANT where RLS with the sum of all the kernels also performs very well. [sent-271, score-0.267]
</p><p>90 On the two PSORT datasets, [33] report selecting 25 kernels using MCMKL. [sent-272, score-0.239]
</p><p>91 On the other hand, on average, MKL-GOMP selects 14 kernels on PSORT +, 15 on PSORT- and 24 kernels on PLANT. [sent-273, score-0.478]
</p><p>92 Note that MKL-GOMP is applied in multivariate mode: the kernels are selected based on their utility to reduce the total residual error across all target classes. [sent-274, score-0.367]
</p><p>93 , by generalizing the multivariate version of Group-OMP [15], and extending our algorithm to incorporate interesting structured kernel dictionaries [3]. [sent-278, score-0.286]
</p><p>94 Multiple kernel learning, conic duality, and the smo algorithm. [sent-313, score-0.241]
</p><p>95 Orthogonal matching pursuit from noisy measurements: A new analysis. [sent-335, score-0.194]
</p><p>96 Block variable selection in multivariate regression and high-dimensional causal inference. [sent-386, score-0.096]
</p><p>97 Group orthogonal matching pursuit for variable selection and prediction. [sent-393, score-0.293]
</p><p>98 Minimax-optimal rates for sparse additive models over kernel classes via convex programming. [sent-426, score-0.316]
</p><p>99 Simple and efﬁcient multiple kernel learning by group lasso. [sent-477, score-0.378]
</p><p>100 On the consistency of feature selection using greedy least squares regression. [sent-483, score-0.13]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ggood', 0.404), ('mkl', 0.371), ('kernel', 0.241), ('kernels', 0.239), ('rkhs', 0.203), ('hj', 0.195), ('kj', 0.186), ('psort', 0.179), ('rls', 0.158), ('hg', 0.154), ('lq', 0.152), ('pursuit', 0.133), ('omp', 0.129), ('rademacher', 0.125), ('dictionary', 0.124), ('sparsity', 0.122), ('plant', 0.112), ('group', 0.093), ('hggood', 0.09), ('kggood', 0.09), ('mcmkl', 0.09), ('regularizer', 0.086), ('sparse', 0.075), ('norms', 0.072), ('protein', 0.067), ('lp', 0.067), ('hk', 0.063), ('smoothness', 0.063), ('regularized', 0.063), ('qj', 0.062), ('matching', 0.061), ('fj', 0.061), ('subcellular', 0.059), ('minf', 0.057), ('residual', 0.056), ('lasso', 0.055), ('risk', 0.055), ('lozano', 0.051), ('selection', 0.051), ('norm', 0.05), ('penalty', 0.048), ('orthogonal', 0.048), ('hypothesis', 0.048), ('recovery', 0.048), ('greedy', 0.048), ('expands', 0.046), ('rkhss', 0.046), ('regularization', 0.046), ('coq', 0.045), ('xbad', 0.045), ('multivariate', 0.045), ('multiple', 0.044), ('ming', 0.043), ('gram', 0.043), ('arg', 0.042), ('elastic', 0.041), ('minimizer', 0.041), ('lanckriet', 0.04), ('proteins', 0.04), ('multiclass', 0.04), ('adaptability', 0.039), ('hinge', 0.039), ('hn', 0.039), ('maxj', 0.037), ('kloft', 0.036), ('minimization', 0.035), ('concerns', 0.035), ('kn', 0.035), ('sonnenburg', 0.034), ('xl', 0.034), ('yi', 0.034), ('validation', 0.033), ('tikhonov', 0.032), ('li', 0.032), ('reproducing', 0.032), ('loss', 0.032), ('squares', 0.031), ('rl', 0.031), ('cortes', 0.031), ('empirical', 0.03), ('localization', 0.03), ('requiring', 0.03), ('hal', 0.03), ('simulated', 0.029), ('fraction', 0.029), ('smooth', 0.029), ('cristianini', 0.029), ('hilbert', 0.029), ('datasets', 0.028), ('alternating', 0.028), ('hs', 0.028), ('outputs', 0.027), ('spaces', 0.027), ('target', 0.027), ('jmlr', 0.026), ('ls', 0.026), ('bach', 0.026), ('labeled', 0.026), ('solver', 0.025), ('exact', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="189-tfidf-1" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>2 0.24525914 <a title="189-tfidf-2" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>Author: Taiji Suzuki</p><p>Abstract: In this paper, we give a new generalization error bound of Multiple Kernel Learning (MKL) for a general class of regularizations. Our main target in this paper is dense type regularizations including ℓp -MKL that imposes ℓp -mixed-norm regularization instead of ℓ1 -mixed-norm regularization. According to the recent numerical experiments, the sparse regularization does not necessarily show a good performance compared with dense type regularizations. Motivated by this fact, this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary mixed-norm-type regularizations in a unifying manner. As a by-product of our general result, we show a fast learning rate of ℓp -MKL that is tightest among existing bounds. We also show that our general learning rate achieves the minimax lower bound. Finally, we show that, when the complexities of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type regularization shows better learning rate compared with sparse ℓ1 regularization. 1</p><p>3 0.2284397 <a title="189-tfidf-3" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><p>4 0.18347271 <a title="189-tfidf-4" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>Author: Jun Wang, Huyen T. Do, Adam Woznica, Alexandros Kalousis</p><p>Abstract: Metric learning has become a very active research ﬁeld. The most popular representative–Mahalanobis metric learning–can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist. However, the problem then becomes ﬁnding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predeﬁned kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly, and despite the extensive work on multiple kernel learning for SVMs, there has been no work in the area of metric learning with multiple kernel learning. In this paper we ﬁll this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection. 1</p><p>5 0.14227284 <a title="189-tfidf-5" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><p>6 0.13550627 <a title="189-tfidf-6" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>7 0.12891071 <a title="189-tfidf-7" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>8 0.12631878 <a title="189-tfidf-8" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>9 0.12420009 <a title="189-tfidf-9" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<p>10 0.12359213 <a title="189-tfidf-10" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>11 0.10918193 <a title="189-tfidf-11" href="./nips-2011-Optimal_learning_rates_for_least_squares_SVMs_using_Gaussian_kernels.html">207 nips-2011-Optimal learning rates for least squares SVMs using Gaussian kernels</a></p>
<p>12 0.099895947 <a title="189-tfidf-12" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>13 0.095258124 <a title="189-tfidf-13" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>14 0.091656961 <a title="189-tfidf-14" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>15 0.085860223 <a title="189-tfidf-15" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>16 0.082306363 <a title="189-tfidf-16" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>17 0.082278609 <a title="189-tfidf-17" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>18 0.075910926 <a title="189-tfidf-18" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>19 0.075298384 <a title="189-tfidf-19" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<p>20 0.075289756 <a title="189-tfidf-20" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.224), (1, 0.019), (2, -0.083), (3, -0.16), (4, -0.103), (5, 0.109), (6, 0.095), (7, 0.033), (8, 0.035), (9, 0.079), (10, -0.187), (11, 0.173), (12, 0.084), (13, 0.224), (14, -0.129), (15, 0.019), (16, 0.127), (17, 0.027), (18, 0.005), (19, 0.164), (20, 0.054), (21, -0.111), (22, -0.112), (23, -0.005), (24, 0.009), (25, -0.05), (26, 0.005), (27, -0.039), (28, 0.013), (29, -0.002), (30, -0.148), (31, -0.141), (32, -0.08), (33, 0.036), (34, -0.023), (35, 0.043), (36, -0.05), (37, 0.017), (38, 0.089), (39, 0.002), (40, 0.027), (41, 0.024), (42, -0.03), (43, 0.035), (44, -0.001), (45, 0.031), (46, -0.023), (47, -0.023), (48, 0.07), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95197523 <a title="189-lsi-1" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>2 0.85878932 <a title="189-lsi-2" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>Author: Marius Kloft, Gilles Blanchard</p><p>Abstract: We derive an upper bound on the local Rademacher complexity of p -norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our analysis covers all cases 1 ≤ p ≤ ∞, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding exα cess loss, namely fast convergence rates of the order O(n− 1+α ), where α is the minimum eigenvalue decay rate of the individual kernels. 1</p><p>3 0.85856783 <a title="189-lsi-3" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>Author: Taiji Suzuki</p><p>Abstract: In this paper, we give a new generalization error bound of Multiple Kernel Learning (MKL) for a general class of regularizations. Our main target in this paper is dense type regularizations including ℓp -MKL that imposes ℓp -mixed-norm regularization instead of ℓ1 -mixed-norm regularization. According to the recent numerical experiments, the sparse regularization does not necessarily show a good performance compared with dense type regularizations. Motivated by this fact, this paper gives a general theoretical tool to derive fast learning rates that is applicable to arbitrary mixed-norm-type regularizations in a unifying manner. As a by-product of our general result, we show a fast learning rate of ℓp -MKL that is tightest among existing bounds. We also show that our general learning rate achieves the minimax lower bound. Finally, we show that, when the complexities of candidate reproducing kernel Hilbert spaces are inhomogeneous, dense type regularization shows better learning rate compared with sparse ℓ1 regularization. 1</p><p>4 0.64736843 <a title="189-lsi-4" href="./nips-2011-Metric_Learning_with_Multiple_Kernels.html">171 nips-2011-Metric Learning with Multiple Kernels</a></p>
<p>Author: Jun Wang, Huyen T. Do, Adam Woznica, Alexandros Kalousis</p><p>Abstract: Metric learning has become a very active research ﬁeld. The most popular representative–Mahalanobis metric learning–can be seen as learning a linear transformation and then computing the Euclidean metric in the transformed space. Since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist. However, the problem then becomes ﬁnding the appropriate kernel function. Multiple kernel learning addresses this limitation by learning a linear combination of a number of predeﬁned kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources. Surprisingly, and despite the extensive work on multiple kernel learning for SVMs, there has been no work in the area of metric learning with multiple kernel learning. In this paper we ﬁll this gap and present a general approach for metric learning with multiple kernel learning. Our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints. Experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection. 1</p><p>5 0.56963885 <a title="189-lsi-5" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A nonparametric kernel-based method for realizing Bayes’ rule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample. The kernel Bayes’ rule can be applied to a wide variety of Bayesian inference problems: we demonstrate Bayesian computation without likelihood, and ﬁltering with a nonparametric statespace model. A consistency rate for the posterior estimate is established. 1</p><p>6 0.56059343 <a title="189-lsi-6" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<p>7 0.54303956 <a title="189-lsi-7" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>8 0.46890491 <a title="189-lsi-8" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>9 0.45708552 <a title="189-lsi-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.43086678 <a title="189-lsi-10" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>11 0.40040097 <a title="189-lsi-11" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>12 0.39393929 <a title="189-lsi-12" href="./nips-2011-Optimal_learning_rates_for_least_squares_SVMs_using_Gaussian_kernels.html">207 nips-2011-Optimal learning rates for least squares SVMs using Gaussian kernels</a></p>
<p>13 0.38925335 <a title="189-lsi-13" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<p>14 0.38420498 <a title="189-lsi-14" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>15 0.37767178 <a title="189-lsi-15" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>16 0.37705585 <a title="189-lsi-16" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>17 0.37663844 <a title="189-lsi-17" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>18 0.37571725 <a title="189-lsi-18" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>19 0.36424986 <a title="189-lsi-19" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>20 0.36351168 <a title="189-lsi-20" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (4, 0.035), (20, 0.033), (26, 0.044), (31, 0.056), (33, 0.014), (43, 0.082), (45, 0.116), (48, 0.014), (57, 0.032), (65, 0.324), (74, 0.062), (83, 0.036), (99, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95062906 <a title="189-lda-1" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>Author: Maneesh Bhand, Ritvik Mudur, Bipin Suresh, Andrew Saxe, Andrew Y. Ng</p><p>Abstract: The efﬁcient coding hypothesis holds that neural receptive ﬁelds are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism’s lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive ﬁeld properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive ﬁeld properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive ﬁeld plasticity during an organism’s lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, ﬁrst proposed by Mountcastle [1], that a qualitatively similar learning algorithm acts throughout primary sensory cortices. 1</p><p>2 0.94569498 <a title="189-lda-2" href="./nips-2011-Extracting_Speaker-Specific_Information_with_a_Regularized_Siamese_Deep_Network.html">93 nips-2011-Extracting Speaker-Specific Information with a Regularized Siamese Deep Network</a></p>
<p>Author: Ke Chen, Ahmad Salman</p><p>Abstract: Speech conveys different yet mixed information ranging from linguistic to speaker-speciﬁc components, and each of them should be exclusively used in a speciﬁc task. However, it is extremely difﬁcult to extract a speciﬁc information component given the fact that nearly all existing acoustic representations carry all types of speech information. Thus, the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information. In this paper, we present a deep neural architecture to extract speaker-speciﬁc information from MFCCs. As a result, a multi-objective loss function is proposed for learning speaker-speciﬁc characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss. With LDC benchmark corpora and a Chinese speech corpus, we demonstrate that a resultant speaker-speciﬁc representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other state-of-the-art techniques in speaker recognition. We discuss relevant issues and relate our approach to previous work. 1</p><p>same-paper 3 0.80376846 <a title="189-lda-3" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>4 0.76553488 <a title="189-lda-4" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>Author: Quoc V. Le, Alexandre Karpenko, Jiquan Ngiam, Andrew Y. Ng</p><p>Abstract: Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an orthonoramlity constraint to be enforced, which makes it difﬁcult to learn overcomplete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets. 1</p><p>5 0.75618905 <a title="189-lda-5" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>6 0.74629456 <a title="189-lda-6" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>7 0.69418013 <a title="189-lda-7" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>8 0.67696977 <a title="189-lda-8" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>9 0.60505748 <a title="189-lda-9" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>10 0.56913817 <a title="189-lda-10" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>11 0.56065899 <a title="189-lda-11" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>12 0.55997872 <a title="189-lda-12" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>13 0.5594517 <a title="189-lda-13" href="./nips-2011-Dynamic_Pooling_and_Unfolding_Recursive_Autoencoders_for_Paraphrase_Detection.html">74 nips-2011-Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</a></p>
<p>14 0.55311406 <a title="189-lda-14" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>15 0.55216157 <a title="189-lda-15" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>16 0.54976887 <a title="189-lda-16" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>17 0.54648495 <a title="189-lda-17" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>18 0.5438112 <a title="189-lda-18" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>19 0.53827786 <a title="189-lda-19" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>20 0.53684229 <a title="189-lda-20" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
