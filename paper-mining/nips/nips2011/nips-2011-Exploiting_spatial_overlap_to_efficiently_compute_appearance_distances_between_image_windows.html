<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-91" href="#">nips2011-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</h1>
<br/><p>Source: <a title="nips-2011-91-pdf" href="http://papers.nips.cc/paper/4447-exploiting-spatial-overlap-to-efficiently-compute-appearance-distances-between-image-windows.pdf">pdf</a></p><p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efﬁcient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>Reference: <a title="nips-2011-91-reference" href="../nips2011_reference/nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The method exploits the relation between appearance distance and spatial overlap. [sent-2, score-0.548]
</p><p>2 We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. [sent-3, score-1.715]
</p><p>3 We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. [sent-4, score-1.007]
</p><p>4 In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. [sent-5, score-0.555]
</p><p>5 For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. [sent-6, score-0.76]
</p><p>6 1  Introduction  Computing the appearance distance between two windows is a fundamental operation in a wide variety of computer vision techniques. [sent-7, score-0.888]
</p><p>7 Algorithms for weakly supervised learning of object classes [7, 11, 16] typically compare large sets of windows between images trying to ﬁnd recurring patterns of appearance. [sent-8, score-0.623]
</p><p>8 Sliding-window object detectors based on kernel SVMs [13, 24] compute appearance distances between the support vectors and a large number of windows in the test image. [sent-9, score-1.005]
</p><p>9 In human pose estimation, [22] computes the color histogram dissimilarity between many candidate windows for lower and upper arms. [sent-10, score-0.591]
</p><p>10 In image retrieval the user can search a large image database for a query object speciﬁed by an image window [20]. [sent-11, score-0.412]
</p><p>11 Finally, many tracking algorithms [4, 5] compare a window around the target object in the current frame to all windows in a surrounding region of the next frame. [sent-12, score-0.636]
</p><p>12 In most cases one is not interested in computing the distance between all pairs of windows from two sets, but in a small subset of low distances, such as all pairs below a given threshold, or the single best pair. [sent-13, score-0.935]
</p><p>13 Exact nearest neighbour algorithms organize the appearance descriptors into trees which can be efﬁciently searched [17]. [sent-15, score-0.494]
</p><p>14 All the above methods consider windows only as points in appearance space. [sent-21, score-0.728]
</p><p>15 However, windows exist also as points in the geometric space deﬁned as their 4D coordinates in the image they lie in. [sent-22, score-0.577]
</p><p>16 In this geometric space, a natural distance between two windows is their spatial overlap (ﬁg. [sent-23, score-1.01]
</p><p>17 In this paper we propose to take advantage of an important relation between the geometric and appearance spaces: the apparance distance between two windows decreases as their spatial overlap increases. [sent-25, score-1.327]
</p><p>18 We derive an upper bound on the appearance distance between two windows in the same image, 1  Fig. [sent-26, score-0.991]
</p><p>19 Windows w1 , w2 in an image I are embedded in geometric space and in appearance space. [sent-28, score-0.375]
</p><p>20 All windows overlapping more than r with w1 are at most at distance B(r) in appearance space. [sent-29, score-0.968]
</p><p>21 We then use this bound in conjuction with the triangle inequality to bound the appearance distances of many pairs of windows between two images, given the distance of just one pair. [sent-35, score-1.339]
</p><p>22 Building on these basic operations, we design algorithms to efﬁciently ﬁnd all pairs with distance smaller than a threshold (sec. [sent-36, score-0.366]
</p><p>23 The techniques we propose reduce computation by minimizing the number of times appearance distances are computed. [sent-39, score-0.43]
</p><p>24 5 that the proposed algorithms accurately solve the above problems while greatly reducing the number of appearance distances computed. [sent-42, score-0.43]
</p><p>25 2  Relation between spatial overlap and appearance distance  Windows w in an image I are emdebbed in two spaces at the same time (ﬁg. [sent-45, score-0.856]
</p><p>26 The distance between two windows is deﬁned based on their spatial overlap o(w1 , w2 ) = |w1 \w2 | 2 [0, 1], |w1 [w2 | where \ denotes the area of the intersection and [ the area of the union. [sent-50, score-0.978]
</p><p>27 In appearance space, w is represented by a high dimensional vector describing the pixel pattern inside it, as computed by a function fapp (w) : I ! [sent-51, score-0.447]
</p><p>28 In appearance space, two windows are compared using a distance d(fapp (w1 ), fapp (w2 )). [sent-55, score-1.05]
</p><p>29 Two overlapping windows w1 , w2 in an image I share the pixels contained in their intersection (ﬁg. [sent-56, score-0.625]
</p><p>30 The spatial overlap of the two windows correlates with the proportion of common pixels input to fapp when computing the descriptor for each window. [sent-58, score-1.046]
</p><p>31 In general, fapp varies smoothly with the geometry of w, so that windows of similar geometry are close in appearance space. [sent-59, score-0.89]
</p><p>32 Consequently, the spatial overlap o and appearance distance d are related. [sent-60, score-0.776]
</p><p>33 In this paper we exploit this relation to derive an upper bound B(o(w1 , w2 )) on the appearance distance between two overlapping windows. [sent-61, score-0.632]
</p><p>34 To simplify the notation we use d(w1 , w2 ) to denote the appearance distance d(fapp (w1 ), fapp (w2 )). [sent-66, score-0.585]
</p><p>35 We refer to it simply as distance and we say overlap for spatial overlap. [sent-67, score-0.513]
</p><p>36 The upper bound B is a function of the overlap o(w1 , w2 ), and has the following property d(w1 , w2 )  B(o(w1 , w2 ))  8w1 , w2  (1)  o2  (2)  Moreover, B is a monotonic decreasing function B(o1 )  B(o2 ) 2  8o1  (a)  (b)  (c)  Fig. [sent-68, score-0.329]
</p><p>37 The triangle inequality (4) holds for any three points fapp (w1 ), fapp (w2 ) and fapp (w3 ) in appearance space. [sent-70, score-0.78]
</p><p>38 Therefore, all pairs of windows within an overlap radius r (i. [sent-73, score-0.958]
</p><p>39 1) d(w1 , w2 )  B(o(w1 , w2 ))  B(r)  8w1 , w2 , o(w1 , w2 )  r  (3)  As deﬁned above, B bounds the appearance distance between two windows in the same image. [sent-76, score-0.888]
</p><p>40 Now we show how it can be used to derive a bound on the distances between windows in two different images I 1 , I 2 . [sent-77, score-0.765]
</p><p>41 Given two windows w1 , w2 in I 1 and a window w3 in I 2 , we use the triangle inequality to derive (ﬁg. [sent-78, score-0.61]
</p><p>42 These bounds will form the basis of our algorithms for reducing the number of times the appearance distance is computed when solving two classic tasks (sec. [sent-82, score-0.445]
</p><p>43 For each image I m we sample N m m m windows {wi }, and then compute for all window pairs their overlap om = o(wi , wj ) and distance ij m m m m m dij = d(wi , wj ). [sent-94, score-1.676]
</p><p>44 The overall training dataset D is composed of (oij , dij ) for every window pair D = { (om , dm ) | k 2 {1, M } , i, j 2 {1, N }} ij ij  (7)  We now quantize the overlap values into 100 bins and estimate B↵ (o) for each bin o separately. [sent-95, score-0.597]
</p><p>45 3a shows the binned distanceij ij overlap pairs and the bound B0. [sent-100, score-0.485]
</p><p>46 The data comes from 100 windows sampled from more than 1000 images (details in sec. [sent-102, score-0.521]
</p><p>47 Each column of this matrix is roughly Gaussian distributed, and its mean continuously decreases with increasing overlap, conﬁrming our assumptions about the relation between overlap and distance (sec. [sent-104, score-0.468]
</p><p>48 Given a window w1 and a distance ✏ we can use B↵ to ﬁnd windows w2 overlapping with w1 that are at most distance ✏ from w1 . [sent-114, score-0.951]
</p><p>49 From B↵ we can derive what is the smallest overlap omin (✏) so that all pairs of windows overlapping more than omin (✏) have distance smaller than ✏ (with probability more than ↵). [sent-117, score-1.759]
</p><p>50 Formally P ( d(w1 , w2 )  ✏ )  ↵  8w1 , w2 , o(w1 , w2 )  omin (✏)  (9)  and omin (✏) is deﬁned as the smallest overlap o for which the bound is smaller than ✏ (ﬁg. [sent-118, score-0.92]
</p><p>51 2  (10)  Exact bounds for histogram descriptors  The statistical bounds of the previous subsection can be estimated from images for any appearance descriptor. [sent-120, score-0.533]
</p><p>52 The upper bound B for two windows w1 and w2 corresponds to the limit case where the three regions w1 \ w2 , w1 \ w2 and w2 \ w1 contain three disjoint sets of colors (or visual word in general). [sent-130, score-0.54]
</p><p>53 3  Efﬁciently computing all window pairs with distance smaller than ✏  In this section we present an algorithm to efﬁciently ﬁnd all pairs of windows with distance smaller 1 than a threshold ✏ between two images I 1 , I 2 . [sent-136, score-1.317]
</p><p>54 Formally, given an input set of windows W 1 = {wi } 2 in image I 1 and a set W 2 = {wj } in image I 2 , the algorithm should return the set of pairs P✏ = 1 2 1 2 { (wi , wj ) | d(wi , wj )  ✏ }. [sent-137, score-1.154]
</p><p>55 The core of the algorithm (Block 3) explores pairs overlapping with a seed, looking for all appearance distances smaller than ✏. [sent-141, score-0.694]
</p><p>56 When 4  Algorithm 1 Efﬁciently computing all distances smaller than ✏ m Input: windows W m = {wi }, threshold ✏, lookup table omin , number of initial samples F Output: set P✏ of all pairs p with d(p)  ✏ 1. [sent-142, score-1.154]
</p><p>57 Compute seed pairs PF  1 2 (a) sample F random pairs pij = (wi , wj ) from P = W 1 ⇥ W 2 , giving PF  1 2 (b) compute dij = d(wi , wj ), 8pij 2 PF  2. [sent-143, score-0.904]
</p><p>58 Determine a sequence S of all pairs from P (gives schedule of block 3 below) (a) sort the seed pairs in PF in order of decreasing distance (b) set S(1 : F ) = PF (c) ﬁll S((F + 1) : end) with random pairs from P \ PF  3. [sent-144, score-0.792]
</p><p>59 For pc = S(1 : end) (explore the pairs in the S order) (a) compute d(pc ) (b) if d(pc )  ✏ i. [sent-145, score-0.376]
</p><p>60 exploring a seed, the algorithm can decide to discard many pairs overlapping with it, as the bound predicts that their distance cannot be lower than ✏. [sent-154, score-0.478]
</p><p>61 Block 3 takes one of two actions based on the distance of the pair pc currently being explored. [sent-162, score-0.428]
</p><p>62 If d(pc )  ✏, then all pairs in the overlap neighborhood N of pc have distance smaller than ✏. [sent-163, score-0.903]
</p><p>63 This overlap neighborhood has a radius r = omin (✏ d(pc )) predicted by the bound lookup table omin (ﬁg. [sent-164, score-1.071]
</p><p>64 Therefore, Block 3 computes the distance of all pairs in N (step 3. [sent-166, score-0.343]
</p><p>65 Instead, if d(pc ) > ✏, Block 3 determines the radius r = omin (d(pc ) ✏) of the overlap neighborhood containing pairs with distance greater than ✏, and then discards all pairs in it (step 3. [sent-168, score-1.225]
</p><p>66 The overlap neighborhood of a pair pij = (wi , wj ) with radius r con1 2 2 2 1 2 1 1 tains all pairs (wi , wv ) such that o(wj , wv ) r, and all pairs (wu , wj ) such that o(wi , wu ) r (ﬁg. [sent-171, score-1.49]
</p><p>67 4  Efﬁciently computing the single window pair with the smallest distance  We give an algorithm to efﬁciently ﬁnd the single pair of windows with the smallest appearance distance between two images. [sent-173, score-1.418]
</p><p>68 Given as input the two sets of windows W 1 , W 2 , the algorithm 1 2 1 2 1 2 should return the pair p⇤ = (wi⇤ , wj ⇤ ) with the smallest distance: d(wi⇤ , wj ⇤ ) = minij d(wi , wj ). [sent-174, score-1.153]
</p><p>69 (a) The overlap neighborhood of radius r of a pair (wi , wj ) contains all 1 2 blue pairs. [sent-177, score-0.691]
</p><p>70 (b) The joint overlap neighborhood of radius s of a pair (wi , wj ) contains all blue and green pairs. [sent-178, score-0.691]
</p><p>71 Block 1 computes distances for the seed pairs and it selectes the pair with the smallest distance as initial approximation to p⇤ . [sent-181, score-0.755]
</p><p>72 Block 3 explores pairs overlapping with a seed, looking for a distance smaller than d(p⇤ ). [sent-182, score-0.424]
</p><p>73 If d(pc )  d(p⇤ ) + B↵ (s), then there might be a better pair than d(p⇤ ) within radius s in the joint overlap neighborhood of pc . [sent-193, score-0.715]
</p><p>74 Therefore, the algorithm computes the distance of all pairs in this neighborhood (step 3. [sent-194, score-0.452]
</p><p>75 Instead, if d(pc ) > d(p⇤ ) + B↵ (s), the algorithm determines the radius r = omin (d(pc ) d(p⇤ )) of the overlap neighborhood that contains only pairs with distance greater than d(p⇤ ), and then discards all pairs in it (step 3. [sent-197, score-1.225]
</p><p>76 The joint overlap neighborhood of a pair pij = (wi , wj ) with 1 2 1 1 2 2 radius s contains all pairs (wu , wv ) such that o(wi , wu ) s and o(wj , wv ) s. [sent-200, score-1.163]
</p><p>77 This setting is relevant for various applications, such as object detection [13, 24], and ensures a balanced distribution of appearance distances in each image pair (some pairs of windows will have a low distance while others high distances). [sent-202, score-1.416]
</p><p>78 We experiment with three appearance descriptors: GIST [19] (960D), color histograms (CHIST, 4000D), and bag-of-words [11, 25] on the dense SURF descriptor [3] (BOW, 2000D). [sent-203, score-0.426]
</p><p>79 As appearance distances we use the Euclidean for GIST, and 2 for CHIST and SURF BOW. [sent-204, score-0.43]
</p><p>80 Task 1: all pairs of windows with distance smaller than ✏. [sent-208, score-0.809]
</p><p>81 The task is to ﬁnd all pairs of windows with distance smaller than a user-deﬁned threshold ✏ between two images I 1 , I 2 (sec. [sent-209, score-0.93]
</p><p>82 This task occurs in weakly supervised learning of object classes [7, 11, 16], where algorithms search for recurring patterns over training images containing thousands of overlapping windows, and in human pose estimation [22], which compares many overlapping candidate body part locations. [sent-211, score-0.361]
</p><p>83 We random sample 3000 windows in each image (|W 1 | = |W 2 | = 3000) and set ✏ so that 10% of all distances are below it. [sent-212, score-0.712]
</p><p>84 It maps descriptors to binary strings, such that the Hamming distance between two strings is related to the value of a Gaussian kernel between the original descriptors [21]. [sent-217, score-0.473]
</p><p>85 6  Algorithm 2 Efﬁciently computing the smallest distance m Input: windows W m = {wi }, lookup table omin , search radius s, number of initial samples F Output: pair p⇤ with the smallest distance 1. [sent-219, score-1.397]
</p><p>86 For pc = S(1 : end) (explore the pairs in the S order) (a) compute d(pc ) (b) if d(pc )  d(p⇤ ) + B↵ (s) i. [sent-222, score-0.376]
</p><p>87 2) compute all distances d in the original 1 2 space between wi and all windows wj 2 b1 (unless already computed when inspecting a previous t,i 1 2 table); (H3) return all computed d(wi , wj )  ✏. [sent-230, score-1.24]
</p><p>88 1) compute the ✏-NN between wi 2 and all windows wj 2 W 2 and return them all. [sent-233, score-0.857]
</p><p>89 object detectors based on kernel SVMs compare a support vector (query) to a large set of overlapping windows in the test image [13, 24]. [sent-256, score-0.71]
</p><p>90 Task 3: single pair of windows with smallest distance. [sent-313, score-0.607]
</p><p>91 The task is to ﬁnd the single pair of windows with the smallest distance between I 1 and I 2 , out of 3000 windows in each image (sec. [sent-314, score-1.355]
</p><p>92 (ii) distance ratio: the ratio between the smallest distance returned by the algorithm and the true smallest distance. [sent-317, score-0.521]
</p><p>93 While we have measured only the number of computed appearance distances, our algorithms also compute spatial overlaps. [sent-333, score-0.409]
</p><p>94 Crucially, spatial overlaps are computed in the 4D geometric space, compared to 1000+ dimensions for the appearance space. [sent-334, score-0.416]
</p><p>95 In practice, when using 5000 windows per image with 4000D dense SURF BOW descriptors, the total runtime of our algorithms is 71s for Task 1 or 16s for Task 3, compared to 335s for exhaustive search. [sent-336, score-0.639]
</p><p>96 Importantly, the cost of computing the descriptors is small compared to the cost of evaluating distances, as it is roughly linear in the number of windows and can be implemented very rapidly. [sent-337, score-0.723]
</p><p>97 In practice, computing dense SURF BOW for 5000 windows in two images takes 5 seconds. [sent-338, score-0.548]
</p><p>98 We have proposed efﬁcient algorithms for computing distances of appearance descriptors between two sets of image windows, by taking advantage of the overlap structure in the sets. [sent-340, score-0.908]
</p><p>99 Our experiments demonstrate that these algorithms greatly reduce the number of appearance distances computed when solving several tasks relevant to computer vision and outperform LSH and ANN for these tasks. [sent-341, score-0.452]
</p><p>100 Exploiting spatial overlap to efﬁciently compute appearance distances between image windows - supplementary material. [sent-353, score-1.353]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('windows', 0.465), ('appearance', 0.263), ('omin', 0.259), ('lsh', 0.258), ('overlap', 0.254), ('ann', 0.233), ('pc', 0.196), ('wj', 0.172), ('distances', 0.167), ('wi', 0.165), ('fapp', 0.162), ('distance', 0.16), ('pairs', 0.155), ('descriptors', 0.144), ('neighborhood', 0.109), ('seed', 0.103), ('surf', 0.1), ('spatial', 0.099), ('wv', 0.093), ('pf', 0.092), ('window', 0.086), ('radius', 0.084), ('pij', 0.083), ('overlapping', 0.08), ('image', 0.08), ('gist', 0.079), ('pair', 0.072), ('smallest', 0.07), ('descriptor', 0.066), ('block', 0.064), ('cost', 0.057), ('images', 0.056), ('object', 0.054), ('hash', 0.054), ('speedup', 0.052), ('bow', 0.049), ('bound', 0.049), ('alexe', 0.049), ('chist', 0.049), ('wu', 0.048), ('accuracy', 0.046), ('neighbour', 0.045), ('runtime', 0.044), ('task', 0.043), ('chum', 0.043), ('nearest', 0.042), ('om', 0.041), ('histogram', 0.039), ('dij', 0.039), ('hashing', 0.038), ('histograms', 0.037), ('lookup', 0.035), ('bin', 0.034), ('discard', 0.034), ('bins', 0.033), ('zurich', 0.033), ('color', 0.033), ('cvpr', 0.033), ('petrescu', 0.032), ('geometric', 0.032), ('retrieval', 0.032), ('ratio', 0.031), ('eth', 0.031), ('detectors', 0.031), ('tracking', 0.031), ('subsection', 0.031), ('triangle', 0.031), ('eccv', 0.031), ('library', 0.03), ('returned', 0.03), ('return', 0.03), ('smaller', 0.029), ('isard', 0.028), ('philbin', 0.028), ('seeds', 0.028), ('derive', 0.028), ('decreases', 0.028), ('computes', 0.028), ('ij', 0.027), ('nn', 0.027), ('dense', 0.027), ('upper', 0.026), ('relation', 0.026), ('dm', 0.025), ('bag', 0.025), ('compute', 0.025), ('strings', 0.025), ('discards', 0.025), ('recurring', 0.025), ('greater', 0.024), ('rank', 0.024), ('hamming', 0.024), ('kernels', 0.023), ('weakly', 0.023), ('pascal', 0.023), ('exhaustive', 0.023), ('computed', 0.022), ('threshold', 0.022), ('table', 0.022), ('ciently', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="91-tfidf-1" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efﬁcient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>2 0.15795797 <a title="91-tfidf-2" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>Author: Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, Thomas S. Huang</p><p>Abstract: High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efﬁciency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efﬁciency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efﬁciency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efﬁcient large scale search. Our approach takes both search quality and computational cost into consideration. Speciﬁcally, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efﬁciently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efﬁciency. Experimental results show that our approach signiﬁcantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p><p>3 0.15646395 <a title="91-tfidf-3" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>Author: Ricardo S. Cabral, Fernando Torre, Joao P. Costeira, Alexandre Bernardino</p><p>Abstract: Recently, image categorization has been an active research topic due to the urgent need to retrieve and browse digital images via semantic keywords. This paper formulates image categorization as a multi-label classiﬁcation problem using recent advances in matrix completion. Under this setting, classiﬁcation of testing data is posed as a problem of completing unknown label entries on a data matrix that concatenates training and testing features with training labels. We propose two convex algorithms for matrix completion based on a Rank Minimization criterion speciﬁcally tailored to visual data, and prove its convergence properties. A major advantage of our approach w.r.t. standard discriminative classiﬁcation methods for image categorization is its robustness to outliers, background noise and partial occlusions both in the feature and label space. Experimental validation on several datasets shows how our method outperforms state-of-the-art algorithms, while effectively capturing semantic concepts of classes. 1</p><p>4 0.13411278 <a title="91-tfidf-4" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>Author: Alessandro Bergamo, Lorenzo Torresani, Andrew W. Fitzgibbon</p><p>Abstract: We introduce P I C O D ES: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of deﬁning indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images deﬁning the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classiﬁcation performance. Optimization directly for binary features is difﬁcult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. P I C O D ES of 256 bytes match the accuracy of the current best known classiﬁer for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.</p><p>5 0.11883032 <a title="91-tfidf-5" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that ﬁt the empirical data distribution. We instantiate this similarity measure with the Gammacompound-Laplace distribution, and show signiﬁcant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost. 1</p><p>6 0.10780519 <a title="91-tfidf-6" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>7 0.10766479 <a title="91-tfidf-7" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>8 0.10605154 <a title="91-tfidf-8" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>9 0.088539444 <a title="91-tfidf-9" href="./nips-2011-Variance_Penalizing_AdaBoost.html">299 nips-2011-Variance Penalizing AdaBoost</a></p>
<p>10 0.085942902 <a title="91-tfidf-10" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>11 0.08572419 <a title="91-tfidf-11" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>12 0.085643411 <a title="91-tfidf-12" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>13 0.082035638 <a title="91-tfidf-13" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>14 0.07241524 <a title="91-tfidf-14" href="./nips-2011-Better_Mini-Batch_Algorithms_via_Accelerated_Gradient_Methods.html">46 nips-2011-Better Mini-Batch Algorithms via Accelerated Gradient Methods</a></p>
<p>15 0.069316715 <a title="91-tfidf-15" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>16 0.068768688 <a title="91-tfidf-16" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>17 0.068237863 <a title="91-tfidf-17" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>18 0.064050287 <a title="91-tfidf-18" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>19 0.063704982 <a title="91-tfidf-19" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>20 0.063656367 <a title="91-tfidf-20" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, 0.1), (2, -0.124), (3, 0.094), (4, 0.064), (5, 0.097), (6, 0.014), (7, -0.006), (8, -0.022), (9, 0.042), (10, 0.034), (11, 0.048), (12, -0.081), (13, -0.008), (14, 0.007), (15, 0.029), (16, -0.072), (17, 0.051), (18, -0.074), (19, 0.021), (20, 0.025), (21, 0.066), (22, 0.11), (23, -0.047), (24, 0.058), (25, 0.036), (26, 0.068), (27, 0.058), (28, 0.116), (29, 0.102), (30, -0.034), (31, -0.019), (32, 0.079), (33, 0.022), (34, -0.026), (35, 0.065), (36, 0.147), (37, -0.058), (38, 0.025), (39, -0.031), (40, -0.009), (41, -0.105), (42, 0.054), (43, 0.147), (44, 0.047), (45, -0.075), (46, -0.05), (47, 0.096), (48, -0.061), (49, -0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97247046 <a title="91-lsi-1" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efﬁcient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>2 0.64868671 <a title="91-lsi-2" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>Author: Hua Wang, Heng Huang, Farhad Kamangar, Feiping Nie, Chris H. Ding</p><p>Abstract: Multi-instance learning (MIL) considers input as bags of instances, in which labels are assigned to the bags. MIL is useful in many real-world applications. For example, in image categorization semantic meanings (labels) of an image mostly arise from its regions (instances) instead of the entire image (bag). Existing MIL methods typically build their models using the Bag-to-Bag (B2B) distance, which are often computationally expensive and may not truly reﬂect the semantic similarities. To tackle this, in this paper we approach MIL problems from a new perspective using the Class-to-Bag (C2B) distance, which directly assesses the relationships between the classes and the bags. Taking into account the two major challenges in MIL, high heterogeneity on data and weak label association, we propose a novel Maximum Margin Multi-Instance Learning (M3 I) approach to parameterize the C2B distance by introducing the class speciﬁc distance metrics and the locally adaptive signiﬁcance coefﬁcients. We apply our new approach to the automatic image categorization tasks on three (one single-label and two multilabel) benchmark data sets. Extensive experiments have demonstrated promising results that validate the proposed method.</p><p>3 0.57311994 <a title="91-lsi-3" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>Author: Fahad S. Khan, Joost Weijer, Andrew D. Bagdanov, Maria Vanrell</p><p>Abstract: We describe a novel technique for feature combination in the bag-of-words model of image classiﬁcation. Our approach builds discriminative compound words from primitive cues learned independently from training images. Our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classiﬁcation problems than attempting to empirically estimate the dependent, joint-cue distribution directly. We use Information theoretic vocabulary compression to ﬁnd discriminative combinations of cues and the resulting vocabulary of portmanteau1 words is compact, has the cue binding property, and supports individual weighting of cues in the ﬁnal image representation. State-of-theart results on both the Oxford Flower-102 and Caltech-UCSD Bird-200 datasets demonstrate the effectiveness of our technique compared to other, signiﬁcantly more complex approaches to multi-cue image representation. 1</p><p>4 0.57182378 <a title="91-lsi-4" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>Author: Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, Thomas S. Huang</p><p>Abstract: High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efﬁciency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efﬁciency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efﬁciency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efﬁcient large scale search. Our approach takes both search quality and computational cost into consideration. Speciﬁcally, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efﬁciently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efﬁciency. Experimental results show that our approach signiﬁcantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p><p>5 0.56627727 <a title="91-lsi-5" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>Author: Yangqing Jia, Trevor Darrell</p><p>Abstract: Many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients. These are often modeled implicitly or explicitly with a Gaussian noise assumption, leading to the use of the Euclidean distance when comparing image descriptors. In this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of Euclidean distances. We advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that ﬁt the empirical data distribution. We instantiate this similarity measure with the Gammacompound-Laplace distribution, and show signiﬁcant improvement over existing distance measures in the application of SIFT feature matching, at relatively low computational cost. 1</p><p>6 0.55751568 <a title="91-lsi-6" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>7 0.55722249 <a title="91-lsi-7" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>8 0.55298221 <a title="91-lsi-8" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>9 0.54959309 <a title="91-lsi-9" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>10 0.54424858 <a title="91-lsi-10" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>11 0.53889686 <a title="91-lsi-11" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>12 0.52939385 <a title="91-lsi-12" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>13 0.49044925 <a title="91-lsi-13" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>14 0.47701401 <a title="91-lsi-14" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>15 0.46450633 <a title="91-lsi-15" href="./nips-2011-Variance_Penalizing_AdaBoost.html">299 nips-2011-Variance Penalizing AdaBoost</a></p>
<p>16 0.46181044 <a title="91-lsi-16" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>17 0.45808679 <a title="91-lsi-17" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>18 0.45353428 <a title="91-lsi-18" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>19 0.45249093 <a title="91-lsi-19" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>20 0.43252909 <a title="91-lsi-20" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (4, 0.037), (20, 0.041), (26, 0.016), (31, 0.053), (33, 0.497), (34, 0.012), (43, 0.032), (45, 0.084), (57, 0.03), (74, 0.012), (83, 0.023), (99, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93856609 <a title="91-lda-1" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efﬁcient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>2 0.92516088 <a title="91-lda-2" href="./nips-2011-Modelling_Genetic_Variations_using_Fragmentation-Coagulation_Processes.html">173 nips-2011-Modelling Genetic Variations using Fragmentation-Coagulation Processes</a></p>
<p>Author: Yee W. Teh, Charles Blundell, Lloyd Elliott</p><p>Abstract: We propose a novel class of Bayesian nonparametric models for sequential data called fragmentation-coagulation processes (FCPs). FCPs model a set of sequences using a partition-valued Markov process which evolves by splitting and merging clusters. An FCP is exchangeable, projective, stationary and reversible, and its equilibrium distributions are given by the Chinese restaurant process. As opposed to hidden Markov models, FCPs allow for ﬂexible modelling of the number of clusters, and they avoid label switching non-identiﬁability problems. We develop an efﬁcient Gibbs sampler for FCPs which uses uniformization and the forward-backward algorithm. Our development of FCPs is motivated by applications in population genetics, and we demonstrate the utility of FCPs on problems of genotype imputation with phased and unphased SNP data. 1</p><p>3 0.79049778 <a title="91-lda-3" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>4 0.71782076 <a title="91-lda-4" href="./nips-2011-From_Stochastic_Nonlinear_Integrate-and-Fire_to_Generalized_Linear_Models.html">99 nips-2011-From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models</a></p>
<p>Author: Skander Mensi, Richard Naud, Wulfram Gerstner</p><p>Abstract: Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-theart models from both schools of thought. First we ﬁnd the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-andFire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the ﬁring probability given a deterministic membrane potential. We ﬁnd a mathematical expression for this link-function and test the ability of the GLM to predict the ﬁring probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we ﬁnd that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models. 1 Motivation When it comes to modeling the intrinsic variability in simple neuron models, we can distinguish two traditional approaches. One approach is inspired by the stochastic Leaky Integrate-and-Fire (LIF) hypothesis of Stein (1967) [1], where a noise term is added to the system of differential equations implementing the leaky integration to a threshold. There are multiple versions of such a stochastic LIF [2]. How the noise affects the ﬁring probability is also a function of the parameters of the neuron model. Therefore, it is important to take into account the reﬁnements of simple neuron models in terms of subthreshold resonance [3, 4], spike-triggered adaptation [5, 6] and non-linear spike 1 initiation [7, 5]. All these improvements are encompassed by the Adaptive Exponential Integrateand-Fire model (AdEx [8, 9]). The other approach is to start with some deterministic dynamics for the the state of the neuron (for instance the instantaneous distance from the membrane potential to the threshold) and link the probability intensity of emitting a spike with a non-linear function of the state variable. Under some conditions, this type of model is part of a greater class of statistical models called Generalized Linear Models (GLM [10]). As a single neuron model, the Spike Response Model (SRM) with escape noise is a GLM in which the state variable is explicitly the distance between a deterministic voltage and the threshold. The original SRM could account for subthreshold resonance, refractory effects and spike-frequency adaptation [11]. Mathematically similar models were developed independently in the study of the visual system [12] where spike-frequency adaptation has also been modeled [13]. Recently, this approach has retained increased attention since the probabilistic framework can be linked with the Bayesian theory of neural systems [14] and because Bayesian inference can be applied to the population of neurons [15]. In this paper, we investigate the similarity and differences between the state-of-the-art GLM and the stochastic AdEx. The motivation behind this work is to relate the traditional threshold neuron models to Bayesian theory. Our results extend the work of Plesser and Gerstner (2000) [16] since we include the non-linearity for spike initiation and spike-frequency adaptation. We also provide relationships between the parameters of the AdEx and the equivalent GLM. These precise relationships can be used to relate analog implementations of threshold models [17] to the probabilistic models used in the Bayesian approach. The paper is organized as follows: We ﬁrst describe the expressions relating the SRM state-variable to the parameters of the AdEx (Sect. 3.1) in the subthreshold regime. Then, we use numerical methods to ﬁnd the non-linear link-function that models the ﬁring probability (Sect. 3.2). We ﬁnd a functional form for the SRM link-function that best describes the ﬁring probability of a stochastic AdEx. We then compare the performance of this link-function with the often used exponential or linear-rectiﬁer link-functions (also called half-wave linear rectiﬁer) in terms of predicting the ﬁring probability of an AdEx under complex stimulus (Sect. 3.3). We ﬁnd that the exponential linkfunction yields almost perfect prediction. Finally, we explore the relations between the statistic of the noise and the sharpness of the non-linearity for spike initiation with the parameters of the SRM. 2 Presentation of the Models In this section we present the general formula for the stochastic AdEx model (Sect. 2.1) and the SRM (Sect 2.2). 2.1 The Stochastic Adaptive Exponential Integrate-and-Fire Model The voltage dynamics of the stochastic AdEx is given by: V −Θ ˙ τm V = El − V + ∆T exp − Rw + RI + R (1) ∆T τw w = a(V − El ) − w ˙ (2) where τm is the membrane time constant, El the reverse potential, R the membrane resistance, Θ is the threshold, ∆T is the shape factor and I(t) the input current which is chosen to be an Ornstein−Θ Uhlenbeck process with correlation time-constant of 5 ms. The exponential term ∆T exp( V∆T ) is a non-linear function responsible for the emission of spikes and is a diffusive white noise with standard deviation σ (i.e. ∼ N (0, σ)). Note that the diffusive white-noise does not imply white noise ﬂuctuations of the voltage V (t), the probability distribution of V (t) will depend on ∆T and Θ. The second variable, w, describes the subthreshold as well as the spike-triggered adaptation both ˆ parametrized by the coupling strength a and the time constant τw . Each time tj the voltage goes to inﬁnity, we assumed that a spike is emitted. Then the voltage is reset to a ﬁxed value Vr and w is increased by a constant value b. 2.2 The Generalized Linear Model In the SRM, The voltage V (t) is given by the convolution of the injected current I(t) with the membrane ﬁlter κ(t) plus the additional kernel η(t) that acts after each spikes (here we split the 2 spike-triggered kernel in two η(t) = ηv (t) + ηw (t) for reasons that will become clear later): V (t) = ˆ ˆ ηv (t − tj ) + ηw (t − tj ) El + [κ ∗ I](t) + (3) ˆ {tj } ˆ Then at each time tj a spike is emitted which results in a change of voltage described by η(t) = ηv (t) + ηw (t). Given the deterministic voltage, (Eq. 3) a spike is emitted according to the ﬁring intensity λ(V ): λ(t) = f (V (t)) (4) where f (·) is an arbitrary function called the link-function. Then the ﬁring behavior of the SRM depends on the choice of the link-function and its parameters. The most common link-function used to model single neuron activities are the linear-rectiﬁer and the exponential function. 3 Mapping In order to map the stochastic AdEx to the SRM we follow a two-step procedure. First we derive the ﬁlter κ(t) and the kernels ηv (t) and ηw (t) analytically as a function of AdEx parameters. Second, we derive the link-function of the SRM from the stochastic spike emission of the AdEx. Figure 1: Mapping of the subthreshold dynamics of an AdEx to an equivalent SRM. A. Membrane ﬁlter κ(t) for three different sets of parameters of the AdEx leading to over-damped, critically damped and under-damped cases (upper, middle and lower panel, respectively). B. Spike-Triggered η(t) (black), ηv (t) (light gray) and ηw (gray) for the three cases. C. Example of voltage trace produced when an AdEx is stimulated with a step of colored noise (black). The corresponding voltage from a SRM stimulated with the same current and where we forced the spikes to match those of the AdEx (red). D. Error in the subthreshold voltage (VAdEx − VGLM ) as a function of the mean voltage of the AdEx, for the three different cases: over-, critically and under-damped (light gray, gray and black, respectively) with ∆T = 1 mV. Red line represents the voltage threshold Θ. E. Root Mean Square Error (RMSE) ratio for the three cases with ∆T = 1 mV. The RMSE ratio is the RMSE between the deterministic VSRM and the stochastic VAdEx divided by the RMSE between repetitions of the stochastic AdEx voltage. The error bar shows a single standard deviation as the RMSE ratio is averaged accross multiple value of σ. 3.1 Subthreshold voltage dynamics We start by assuming that the non-linearity for spike initiation does not affect the mean subthreshold voltage of the stochastic AdEx (see Figure 1 D). This assumption is motivated by the small ∆T 3 observed in in-vitro recordings (from 0.5 to 2 mV [8, 9]) which suggest that the subthreshold dynamics are mainly linear except very close to Θ. Also, we expect that the non-linear link-function will capture some of the dynamics due to the non-linearity for spike initiation. Thus it is possible to rewrite the deterministic subthreshold part of the AdEx (Eq. 1-2 without and without ∆T exp((V − Θ)/∆T )) using matrices: ˙ x = Ax (5) with x = V w and A = − τ1 m a τw − gl1m τ − τ1 w (6) In this form, the dynamics of the deterministic AdEx voltage is a damped oscillator with a driving force. Depending on the eigenvalues of A the system could be over-damped, critically damped or under-damped. The ﬁlter κ(t) of the GLM is given by the impulse response of the system of coupled differential equations of the AdEx, described by Eq. 5 and 6. In other words, one has to derive the response of the system when stimulating with a Dirac-delta function. The type of damping gives three different qualitative shapes of the kernel κ(t), which are summarized in Table 3.1 and Figure 1 A. Since the three different ﬁlters also affect the nature of the stochastic voltage ﬂuctuations, we will keep the distinction between over-damped, critically damped and under-damped scenarios throughout the paper. This means that our approach is valid for at least 3 types of diffusive voltage-noise (i.e. the white noise in Eq. 1 ﬁltered by 3 different membrane ﬁlters κ(t)). To complete the description of the deterministic voltage, we need an expression for the spiketriggered kernels. The voltage reset at each spike brings a spike-triggered jump in voltage of magˆ nitude ∆ = Vr − V (t). This perturbation is superposed to the current ﬂuctuations due to I(t) and can be mediated by a Delta-diract pulse of current. Thus we can write the voltage reset kernel by: ηv (t) = ∆ ∆ [δ ∗ κ] (t) = κ(t) κ(0) κ(0) (7) where δ(t) is the Dirac-delta function. The shape of this kernel depends on κ(t) and can be computed from Table 3.1 (see Figure 1 B). Finally, the AdEx mediates spike-frequency adaptation by the jump of the second variables w. From Eq. 2 we can see that this produces a current wspike (t) = b exp (−t/τw ) that can cumulate over subsequent spikes. The effect of this current on voltage is then given by the convolution of wspike (t) with the membrane ﬁlter κ(t). Thus in the SRM framework the spike-frequency adaptation is taken into account by: ηw (t) = [wspike ∗ κ](t) (8) Again the precise form of ηw (t) depends on κ(t) and can be computed from Table 3.1 (see Figure 1 B). At this point, we would like to verify our assumption that the non-linearity for spike emission can be neglected. Fig. 1 C and D shows that the error between the voltage from Eq. 3 and the voltage from the stochastic AdEx is generally small. Moreover, we see that the main contribution to the voltage prediction error is due to the mismatch close to the spikes. However the non-linearity for spike initiation may change the probability distribution of the voltage ﬂuctuations, which in turn inﬂuences the probability of spiking. This will inﬂuence the choice of the link-function, as we will see in the next section. 3.2 Spike Generation Using κ(t), ηv (t) and ηw (t), we must relate the spiking probability of the stochastic AdEx as a function of its deterministic voltage. According to [2] the probability of spiking in time bin dt given the deterministic voltage V (t) is given by: p(V ) = prob{spike in [t, t + dt]} = 1 − exp (−f (V (t))dt) (9) where f (·) gives the ﬁring intensity as a function of the deterministic V (t) (Eq. 3). Thus to extract the link-function f we have to compute the probability of spiking given V (t) for our SRM. To do so we apply the method proposed by Jolivet et al. (2004) [18], where the probability of spiking is simply given by the distribution of the deterministic voltage estimated at the spike times divided by the distribution of the SRM voltage when there is no spike (see ﬁgure 2 A). One can numerically compute these two quantities for our models using N repetitions of the same stimulus. 4 Table 1: Analytical expressions for the membrane ﬁlter κ(t) in terms of the parameters of the AdEx for over-, critically-, and under-damped cases. Membrane Filter: κ(t) over-damped if: (τm + τw )2 > 4τm τw (gl +a) gl κ(t) = k1 eλ1 t + k2 eλ2 t λ1 = 1 2τm τw (−(τm + τw ) + critically-damped if: (τm + τw )2 = 4τm τw (gl +a) gl κ(t) = (αt + β)eλt λ= under-damped if: (τm + τw )2 < 4τm τw (gl +a) gl κ(t) = (k1 cos (ωt) + k2 sin (ωt)) eλt −(τm +τw ) 2τm τw λ= −(τm +τw ) 2τm τw (τm + τw )2 − 4 τm τw (gl + a) gl λ2 = 1 2τm τw (−(τm + τw ) − α= τm −τw 2Cτm τw ω= τw −τm 2τm τw 2 − a g l τm τw (τm + τw )2 − 4 τm τw (gl + a) gl k1 = −(1+(τm λ2 )) Cτm (λ1 −λ2 ) k2 = 1+(τm λ1 ) Cτm (λ1 −λ2 ) β= 1 C k1 = k2 = 1 C −(1+τm λ) Cωτm The standard deviation σ of the noise and the parameter ∆T of the AdEx non-linearity may affect the shape of the link-function. We thus extract p(V ) for different σ and ∆T (Fig. 2 B). Then using visual heuristics and previous knowledge about the potential analytical expression of the link-funtion, we try to ﬁnd a simple analytical function that captures p(V ) for a large range of combinations of σ and ∆T . We observed that the log(− log(p)) is close to linear in most studied conditions Fig. 2 B suggesting the following two distributions of p(V ): V − VT (10) p(V ) = 1 − exp − exp ∆V V − VT p(V ) = exp − exp − (11) ∆V Once we have p(V ), we can use Eq. 4 to obtain the equivalent SRM link-function, which leads to: −1 f (V ) = log (1 − p(V )) (12) dt Then the two potential link-functions of the SRM can be derived from Eq. 10 and Eq. 11 (respectively): V − VT f (V ) = λ0 exp (13) ∆V V − VT (14) f (V ) = −λ0 log 1 − exp − exp − ∆V 1 with λ0 = dt , VT the threshold of the SRM and ∆V the sharpness of the link-function (i.e. the parameters that governs the degree of the stochasticity). Note that the exact value of λ0 has no importance since it is redundant with VT . Eq. 13 is the standard exponential link-function, but we call Eq. 14 the log-exp-exp link-function. 3.3 Prediction The next point is to evaluate the ﬁt quality of each link-function. To do this, we ﬁrst estimate the parameters VT and ∆V of the GLM link-function that maximize the likelihood of observing a spike 5 Figure 2: SRM link-function. A. Histogram of the SRM voltage at the AdEx ﬁring times (red) and at non-ﬁring times (gray). The ratio of the two distributions gives p(V ) (Eq. 9, dashed lines). Inset, zoom to see the voltage histogram evaluated at the ﬁring time (red). B. log(− log(p)) as a function of the SRM voltage for three different noise levels σ = 0.07, 0.14, 0.18 nA (pale gray, gray, black dots, respectively) and ∆T = 1 mV. The line is a linear ﬁt corresponding to the log-exp-exp linkfunction and the dashed line corresponds to a ﬁt with the exponential link-function. C. Same data and labeling scheme as B, but plotting f (V ) according to Eq. 12. The lines are produced with Eq. 14 with parameters ﬁtted as described in B. and the dashed lines are produced with Eq. 13. Inset, same plot but on a semi-log(y) axis. train generated with an AdEx. Second we look at the predictive power of the resulting SRM in terms of Peri-Stimulus Time Histogram (PSTH). In other words we ask how close the spike trains generated with a GLM are from the spike train generated with a stochastic AdEx when both models are stimulated with the same input current. For any GLM with link-function f (V ) ≡ f (t|I, θ) and parameters θ regulating the shape of κ(t), ˆ ηv (t) and ηw (t), the Negative Log-Likelihood (NLL) of observing a spike-train {t} is given by:   NLL = − log(f (t|I, θ)) − f (t|I, θ) (15) t ˆ t It has been shown that the negative log-likelihood is convex in the parameters if f is convex and logconcave [19]. It is easy to show that a linear-rectiﬁer link-function, the exponential link-function and the log-exp-exp link-function all satisfy these conditions. This allows efﬁcient estimation of ˆ ˆ the optimal parameters VT and ∆V using a simple gradient descent. One can thus estimate from a single AdEx spike train the optimal parameters of a given link-function, which is more efﬁcient than the method used in Sect. 3.2. The minimal NLL resulting from the gradient descent gives an estimation of the ﬁt quality. A better estimate of the ﬁt quality is given by the distance between the PSTHs in response to stimuli not used for parameter ﬁtting . Let ν1 (t) be the PSTH of the AdEx, and ν2 (t) be the PSTH of the ﬁtted SRM, 6 Figure 3: PSTH prediction. A. Injected current. B. Voltage traces produced by an AdEx (black) and the equivalent SRM (red), when stimulated with the current in A. C. Raster plot for 20 realizations of AdEx (black tick marks) and equivalent SRM (red tick marks). D. PSTH of the AdEx (black) and the SRM (red) obtained by averaging 10,000 repetitions. E. Optimal log-likelihood for the three cases of the AdEx, using three different link-functions, a linear-rectiﬁer (light gray), an exponential link-function (gray) and the link-function deﬁned by Eq. 14 (dark gray), these values are obtained by averaging over 40 different combinations σ and ∆T (see Fig. 4). Error bars are one standard deviation, the stars denote a signiﬁcant difference, two-sample t-test with α = 0.01. F. same as E. but for Md (Eq. 16). then we use Md ∈ [0, 1] as a measure of match: Md = 2 2 (ν1 (t) − ν2 (t)) dt ν1 (t)2 dt + ν2 (t)2 dt (16) Md = 1 means that it is impossible to differentiate the SRM from the AdEx in terms of their PSTHs, whereas a Md of 0 means that the two PSTHs are completely different. Thus Md is a normalized similarity measure between two PSTHs. In practice, Md is estimated from the smoothed (boxcar average of 1 ms half-width) averaged spike train of 1 000 repetitions for each models. We use both the NLL and Md to quantify the ﬁt quality for each of the three damping cases and each of the three link-functions. Figure 3 shows the match between the stochastic AdEx used as a reference and the derived GLM when both are stimulated with the same input current (Fig. 3 A). The resulting voltage traces are almost identical (Fig. 3 B) and both models predict almost the same spike trains and so the same PSTHs (Fig. 3 C and D). More quantitalively, we see on Fig. 3 E and F, that the linear-rectiﬁer ﬁts signiﬁcantly worse than both the exponential and log-exp-exp link-functions, both in terms of NLL and of Md . The exponential link-function performs as well as the log-exp-exp link-function, with a spike train similarity measure Md being almost 1 for both. Finally the likelihood-based method described above gives us the opportunity to look at the relationship between the AdEx parameters σ and ∆T that governs its spike emission and the parameters VT and ∆V of the link-function (Fig. 4). We observe that an increase of the noise level produces a ﬂatter link-function (greater ∆V ) while an increase in ∆T also produces an increase in ∆V and VT (note that Fig. 4 shows ∆V and VT for the exponential link-function only, but equivalent results are obtained with the log-exp-exp link-function). 4 Discussion In Sect. 3.3 we have shown that it is possible to predict with almost perfect accuracy the PSTH of a stochastic AdEx model using an appropriate set of parameters in the SRM. Moreover, since 7 Figure 4: Inﬂuence of the AdEx parameters on the parameters of the exponential link-function. A. VT as a function of ∆T and σ. B. ∆V as a function of ∆T and σ. the subthreshold voltage of the AdEx also gives a good match with the deterministic voltage of the SRM, we expect that the AdEx and the SRM will not differ in higher moments of the spike train probability distributions beyond the PSTH. We therefore conclude that diffusive noise models of the type of Eq. 1-2 are equivalent to GLM of the type of Eq. 3-4. Once combined with similar results on other types of stochastic LIF (e.g. correlated noise), we could bridge the gap between the literature on GLM and the literature on diffusive noise models. Another noteworthy observation pertains to the nature of the link-function. The link-function has been hypothesized to be a linear-rectiﬁer, an exponential, a sigmoidal or a Gaussian [16]. We have observed that for the AdEx the link-function follows Eq. 14 that we called the log-exp-exp linkfunction. Although the link-function is log-exp-exp for most of the AdEx parameters, the exponential link-function gives an equivalently good prediction of the PSTH. This can be explained by the fact that the difference between log-exp-exp and exponential link-functions happens mainly at low voltage (i.e. far from the threshold), where the probability of emitting a spike is so low (Figure 2 C, until -50 mv). Therefore, even if the exponential link-function overestimates the ﬁring probability at these low voltages it rarely produces extra spikes. At voltages closer to the threshold, where most of the spikes are emitted, the two link-functions behave almost identically and hence produce the same PSTH. The Gaussian link-function can be seen as lying in-between the exponential link-function and the log-exp-exp link-function in Fig. 2. This means that the work of Plesser and Gerstner (2000) [16] is in agreement with the results presented here. The importance of the time-derivative of the ˙ voltage stressed by Plesser and Gerstner (leading to a two-dimensional link-function f (V, V )) was not studied here to remain consistent with the typical usage of GLM in neural systems [14]. Finally we restricted our study to exponential non-linearity for spike initiation and do not consider other cases such as the Quadratic Integrate-and-ﬁre (QIF, [5]) or other polynomial functional shapes. We overlooked these cases for two reasons. First, there are many evidences that the non-linearity in neurons (estimated from in-vitro recordings of Pyramidal neurons) is well approximated by a single exponential [9]. Second, the exponential non-linearity of the AdEx only affects the subthreshold voltage at high voltage (close to threshold) and thus can be neglected to derive the ﬁlters κ(t) and η(t). Polynomial non-linearities on the other hand affect a larger range of the subthreshold voltage so that it would be difﬁcult to justify the linearization of subthreshold dynamics essential to the method presented here. References [1] R. B. Stein, “Some models of neuronal variability,” Biophys J, vol. 7, no. 1, pp. 37–68, 1967. [2] W. Gerstner and W. Kistler, Spiking neuron models. Cambridge University Press New York, 2002. [3] E. Izhikevich, “Resonate-and-ﬁre neurons,” Neural Networks, vol. 14, no. 883-894, 2001. [4] M. J. E. Richardson, N. Brunel, and V. Hakim, “From subthreshold to ﬁring-rate resonance,” Journal of Neurophysiology, vol. 89, pp. 2538–2554, 2003. 8 [5] E. Izhikevich, “Simple model of spiking neurons,” IEEE Transactions on Neural Networks, vol. 14, pp. 1569–1572, 2003. [6] S. Mensi, R. Naud, M. Avermann, C. C. H. Petersen, and W. Gerstner, “Parameter extraction and classiﬁcation of three neuron types reveals two different adaptation mechanisms,” Under review. [7] N. Fourcaud-Trocme, D. Hansel, C. V. Vreeswijk, and N. Brunel, “How spike generation mechanisms determine the neuronal response to ﬂuctuating inputs,” Journal of Neuroscience, vol. 23, no. 37, pp. 11 628–11 640, 2003. [8] R. Brette and W. Gerstner, “Adaptive exponential integrate-and-ﬁre model as an effective description of neuronal activity,” Journal of Neurophysiology, vol. 94, pp. 3637–3642, 2005. [9] L. Badel, W. Gerstner, and M. Richardson, “Dependence of the spike-triggered average voltage on membrane response properties,” Neurocomputing, vol. 69, pp. 1062–1065, 2007. [10] P. McCullagh and J. A. Nelder, Generalized linear models, 2nd ed. Chapman & Hall/CRC, 1998, vol. 37. [11] W. Gerstner, J. van Hemmen, and J. Cowan, “What matters in neuronal locking?” Neural computation, vol. 8, pp. 1653–1676, 1996. [12] D. Hubel and T. Wiesel, “Receptive ﬁelds and functional architecture of monkey striate cortex,” Journal of Physiology, vol. 195, pp. 215–243, 1968. [13] J. Pillow, L. Paninski, V. Uzzell, E. Simoncelli, and E. Chichilnisky, “Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model,” Journal of Neuroscience, vol. 25, no. 47, pp. 11 003–11 013, 2005. [14] K. Doya, S. Ishii, A. Pouget, and R. P. N. Rao, Bayesian brain: Probabilistic approaches to neural coding. The MIT Press, 2007. [15] S. Gerwinn, J. H. Macke, M. Seeger, and M. Bethge, “Bayesian inference for spiking neuron models with a sparsity prior,” in Advances in Neural Information Processing Systems, 2007. [16] H. Plesser and W. Gerstner, “Noise in integrate-and-ﬁre neurons: From stochastic input to escape rates,” Neural Computation, vol. 12, pp. 367–384, 2000. [17] J. Schemmel, J. Fieres, and K. Meier, “Wafer-scale integration of analog neural networks,” in Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, june 2008, pp. 431 –438. [18] R. Jolivet, T. Lewis, and W. Gerstner, “Generalized integrate-and-ﬁre models of neuronal activity approximate spike trains of a detailed model to a high degree of accuracy,” Journal of Neurophysiology, vol. 92, pp. 959–976, 2004. [19] L. Paninski, “Maximum likelihood estimation of cascade point-process neural encoding models,” Network: Computation in Neural Systems, vol. 15, pp. 243–262, 2004. 9</p><p>5 0.68954742 <a title="91-lda-5" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><p>6 0.64552218 <a title="91-lda-6" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>7 0.61093557 <a title="91-lda-7" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>8 0.59962034 <a title="91-lda-8" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>9 0.54342973 <a title="91-lda-9" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>10 0.51975292 <a title="91-lda-10" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>11 0.50364053 <a title="91-lda-11" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>12 0.50295216 <a title="91-lda-12" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>13 0.49803218 <a title="91-lda-13" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>14 0.49103069 <a title="91-lda-14" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>15 0.47191125 <a title="91-lda-15" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>16 0.46151689 <a title="91-lda-16" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>17 0.45794433 <a title="91-lda-17" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>18 0.44465518 <a title="91-lda-18" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>19 0.4415181 <a title="91-lda-19" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>20 0.43881252 <a title="91-lda-20" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
