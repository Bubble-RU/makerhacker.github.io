<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2011-Agnostic Selective Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-28" href="#">nips2011-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 nips-2011-Agnostic Selective Classification</h1>
<br/><p>Source: <a title="nips-2011-28-pdf" href="http://papers.nips.cc/paper/4339-agnostic-selective-classification.pdf">pdf</a></p><p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>Reference: <a title="nips-2011-28-reference" href="../nips2011_reference/nips-2011-Agnostic_Selective_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. [sent-5, score-0.341]
</p><p>2 The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. [sent-6, score-0.307]
</p><p>3 Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. [sent-7, score-0.526]
</p><p>4 We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. [sent-8, score-0.444]
</p><p>5 Indeed, consider a game where our classiﬁer is allowed to abstain from prediction, without penalty, in some region of our choice. [sent-12, score-0.187]
</p><p>6 Perhaps surprisingly it was shown that the volume of this rejection region is bounded, and in fact, this volume diminishes with increasing training set sizes (under certain conditions). [sent-16, score-0.438]
</p><p>7 The setting under consideration, where classiﬁers can abstain from prediction, is called classiﬁcation with a reject option [2, 3], or selective classiﬁcation [1]. [sent-19, score-0.749]
</p><p>8 We ﬁrst show that the concept of “perfect classiﬁcation” that was introduced for the realizable case in [1], can be extended to the agnostic setting. [sent-21, score-0.199]
</p><p>9 While pure perfection is impossible to accomplish in a noisy environment, a more realistic objective is to perform as well as the best hypothesis in the class within a region of our choice. [sent-22, score-0.318]
</p><p>10 We call this type of learning “weakly optimal” selective classiﬁcation and show that a novel strategy accomplishes this type of learning with diminishing rejection rate under certain Bernstein type conditions (a stronger notion of optimality is mentioned later as well). [sent-23, score-0.811]
</p><p>11 This strategy relies on empirical risk minimization, which is computationally difﬁcult. [sent-24, score-0.313]
</p><p>12 We conclude with numerical examples that examine the empirical performance of the new algorithm and compare its performance with that of the widely used selective classiﬁcation method for rejection, based on distance from decision boundary. [sent-26, score-0.553]
</p><p>13 1  2  Selective classiﬁcation and other deﬁnitions  Consider a standard agnostic binary classiﬁcation setting where X is some feature space, and H is our hypothesis class of binary classiﬁers, h : X → {±1}. [sent-27, score-0.294]
</p><p>14 m i=1 m  R(h)  Pr (X,Y )∼P  {h(X) ̸= Y } ,  ˆ R(h)  ˆ ˆ Let h arg inf h∈H R(h) be the empirical risk minimizer (ERM), and h∗ true risk minimizer. [sent-33, score-0.535]
</p><p>15 arg inf h∈H R(h), the  In selective classiﬁcation [1], given Sm we need to select a binary selective classiﬁer deﬁned to be a pair (h, g), with h ∈ H being a standard binary classiﬁer, and g : X → {0, 1} is a selection function deﬁning the sub-region of activity of h in X . [sent-34, score-0.846]
</p><p>16 For a bounded loss function ℓ : Y × Y → [0, 1], the risk of (h, g) is deﬁned as the average loss on the accepted samples, E [ℓ(h(X), Y ) · g(X)] R(h, g) . [sent-38, score-0.235]
</p><p>17 Φ(h, g) As pointed out in [1], the trade-off between risk and coverage is the main characteristic of a selective classiﬁer. [sent-39, score-0.723]
</p><p>18 For any hypothesis class H, target hypothesis h ∈ H, distribution P , sample Sm , and real r > 0, deﬁne { } ˆ ˆ ˆ V(h, r) = {h′ ∈ H : R(h′ ) ≤ R(h) + r} and V(h, r) = h′ ∈ H : R(h′ ) ≤ R(h) + r . [sent-47, score-0.27]
</p><p>19 X∼P  3  Perfect and weakly optimal selective classiﬁers  The concept of perfect classiﬁcation was introduced in [1] within a realizable selective classiﬁcation setting. [sent-50, score-1.067]
</p><p>20 Perfect classiﬁcation is an extreme case of selective classiﬁcation where a selective classiﬁer (h, g) achieves R(h, g) = 0 with certainty; that is, the classiﬁer never errs on its region of activity. [sent-51, score-0.877]
</p><p>21 , ﬁnite hypothesis class) the rejected region diminishes at rate Ω(1/m), where m is the size of the training set. [sent-55, score-0.312]
</p><p>22 In agnostic environments, as we consider here, such perfect classiﬁcation appears to be out of reach. [sent-56, score-0.216]
</p><p>23 In general, in the worst case no hypothesis can achieve zero error over any nonempty subset of the 1  Some authors refer to an equivalent variant of this curve as “Accuracy-Rejection Curve” or ARC. [sent-57, score-0.227]
</p><p>24 We consider here the following weaker, but still extremely desirable behavior, which we call “weakly optimal selective classiﬁcation. [sent-59, score-0.409]
</p><p>25 ” Let h∗ ∈ H be the true risk minimizer of our problem. [sent-60, score-0.278]
</p><p>26 Let (h, g) be a selective classiﬁer selected after observing the training set Sm . [sent-61, score-0.481]
</p><p>27 We say that (h, g) is a weakly optimal selective classiﬁer if, for any 0 < δ < 1, with probability of at least 1 − δ over random choices of Sm , R(h, g) ≤ R(h∗ , g). [sent-62, score-0.542]
</p><p>28 That is, with high probability our classiﬁer is at least as good as the true risk minimizer over its region of activity. [sent-63, score-0.366]
</p><p>29 We call this classiﬁer ‘weakly optimal’ because a stronger requirement would be that the classiﬁer should achieve the best possible error among all hypotheses in H restricted to the region of activity deﬁned by g. [sent-64, score-0.197]
</p><p>30 4  A learning strategy  We now present a strategy that will be shown later to achieve non-trivial weakly optimal selective classiﬁcation under certain conditions. [sent-65, score-0.71]
</p><p>31 Using standard concentration inequalities one can show that the training error of the true risk minimizer, h∗ , cannot be “too far” from the training error of the ˆ empirical risk minimizer, h. [sent-68, score-0.601]
</p><p>32 Therefore, we can guarantee, with high probability, that the class of all hypothesis with “sufﬁciently low” empirical error includes the true risk minimizer h∗ . [sent-69, score-0.525]
</p><p>33 Selecting only subset of the domain, for which all hypothesis in that class agree, is then sufﬁcient to guarantee weak optimality. [sent-70, score-0.159]
</p><p>34 Strategy 1 Learning strategy for weakly optimal selective classiﬁers Input: Sm , m, δ, d Output: a selective classiﬁer (h, g) such that R(h, g) = R(h∗ , g) w. [sent-73, score-1.006]
</p><p>35 , h is any empirical risk minimizer from H ) ( √ 8 d(ln 2me )+ln δ d ˆ ˆ (see Eq. [sent-77, score-0.328]
</p><p>36 Consider an instance of a binary learning problem with hypothesis class H, an underlying distribution P over X × Y, and a loss function ℓ(Y, Y). [sent-79, score-0.187]
</p><p>37 Let h∗ = arg inf h∈H {Eℓ(h(X), Y )} be the true risk minimizer. [sent-80, score-0.207]
</p><p>38 In the following sequence of lemmas and theorems we assume a binary hypothesis class H with VC-dimension d, an underlying distribution P over X × {±1}, and ℓ is the 0/1 loss function. [sent-85, score-0.187]
</p><p>39 Hanneke introduced a complexity measure for active learning problems termed the disagreement coefﬁcient [5]. [sent-114, score-0.298]
</p><p>40 The disagreement coefﬁcient of h with respect to H under distribution P is, θh  sup r>ϵ  ∆B(h, r) , r  (4)  where ϵ = 0. [sent-115, score-0.201]
</p><p>41 The disagreement coefﬁcient of the hypothesis class H with respect to P is deﬁned as θ  lim sup θh(k) , k→∞  { } where h(k) is any sequence of h(k) ∈ H with R(h(k) ) monotonically decreasing. [sent-116, score-0.36]
</p><p>42 Assume that H has disagreement coefﬁcient θ and that F is a (β, B)-Bernstein class w. [sent-119, score-0.249]
</p><p>43 By the deﬁnition of the disagreement coefﬁcient, for any r′ > 0, ∆B(h∗ , r′ ) ≤ θr′ . [sent-129, score-0.201]
</p><p>44 Assume that H has disagreement coefﬁcient θ and that F is a (β, B)-Bernstein class w. [sent-132, score-0.249]
</p><p>45 Let (h, g) be the selective classiﬁer chosen by Algorithm 1. [sent-136, score-0.409]
</p><p>46 Hanneke introduced, in his original work [5], an alternative deﬁnition of the disagreement coefﬁcient θ, for which the supermum in (4) is taken with respect to any ﬁxed ϵ > 0. [sent-151, score-0.201]
</p><p>47 Using this alternative definition it is possible to show that fast coverage rates are achievable, not only for ﬁnite disagreement coefﬁcients (Theorem 5. [sent-152, score-0.336]
</p><p>48 5), but also if the disagreement coefﬁcient grows slowly with respect to 1/ϵ (as shown by Wang [12], under sufﬁcient smoothness conditions). [sent-153, score-0.201]
</p><p>49 6 A disbelief principle and the risk-coverage trade-off Theorem 5. [sent-155, score-0.438]
</p><p>50 5 tells us that the strategy presented in Section 4 not only outputs a weakly optimal selective classiﬁer, but this classiﬁer also has guaranteed coverage (under some conditions). [sent-156, score-0.732]
</p><p>51 The result is an alternative characterization of the selection function g, of the weakly optimal selective classiﬁer chosen by Strategy 1. [sent-160, score-0.513]
</p><p>52 Let (h, g) be a selective classiﬁer chosen by Strategy 1 after observing the training ˆ sample Sm . [sent-164, score-0.481]
</p><p>53 Let x be any point in X and { ( )} ˆ ˆ hx argmin R(h) | h(x) = −sign h(x) , h∈H  ˆ an empirical risk minimizer forced to label x the opposite from h(x). [sent-166, score-0.571]
</p><p>54 1 tells us that in order to decide if point x should be rejected we need to measure the ˆ empirical error R(hx ) of a special empirical risk minimizer, hx , which is constrained to label x the ˆ ˆ ˆ opposite from h(x). [sent-173, score-0.589]
</p><p>55 If this error is sufﬁciently close to R(h) our classiﬁer cannot be too sure about the label of x and we must reject it. [sent-174, score-0.263]
</p><p>56 Observe that D(x) is large whenever our model is sensitive to label of x in the sense that when we are forced to bend our best model to ﬁt the opposite label of x, our model substantially deteriorates, giving rise to a large disbelief index. [sent-182, score-0.593]
</p><p>57 This large D(x) can be interpreted as our disbelief in the possibility that x can be labeled so differently. [sent-183, score-0.438]
</p><p>58 Given a pool of test points we can rank these test points according to their disbelief index, and points with low index should be rejected ﬁrst. [sent-188, score-0.622]
</p><p>59 As in our disbelief index, the difference between the empirical risk (or importance weighted empirical risk [14]) of two ERM oracles (with different constraints) is used to estimate prediction conﬁdence. [sent-191, score-0.952]
</p><p>60 7  Implementation  At this point in the paper we switch from theory to practice, aiming at implementing rejection methods inspired by the disbelief principle and see how well they work on real world (well, . [sent-192, score-0.729]
</p><p>61 Attempting to implement a learning algorithm driven by the disbelief index we face a major bottleneck because the calculation of the index requires the identiﬁcation of ERM hypotheses. [sent-196, score-0.574]
</p><p>62 Another problem we face is that the disbelief index is a noisy statistic that highly depends on the sample Sm . [sent-200, score-0.506]
</p><p>63 For each sample we calculate the disbelief index for all test points and for each point take the median of these measurements as the ﬁnal index. [sent-206, score-0.537]
</p><p>64 We note that for any ﬁnite training sample the disbelief index is a discrete variable. [sent-207, score-0.551]
</p><p>65 It is often the case that several test points share the same disbelief index. [sent-208, score-0.469]
</p><p>66 8  Empirical results  Focusing on SVMs with a linear kernel we compared the RC (Risk-Coverage) curves achieved by the proposed method with those achieved by SVM with rejection based on distance from decision boundary. [sent-215, score-0.385]
</p><p>67 This latter approach is very common in practical applications of selective classiﬁcation. [sent-216, score-0.409]
</p><p>68 Before presenting these results we wish to emphasize that the proposed method leads to rejection regions fundamentally different than those obtained by the traditional distance-based technique. [sent-218, score-0.35]
</p><p>69 (a)  (b)  Figure 1: conﬁdence height map using (a) disbelief index; (b) distance from decision boundary. [sent-221, score-0.557]
</p><p>70 SVM was trained on the entire training set and test samples were sorted according to conﬁdence (either using distance from decision boundary or disbelief index). [sent-227, score-0.654]
</p><p>71 Figure 2 depicts the RC curves of our technique (red solid line) and rejection based on distance from decision boundary (green dashed line) for linear kernel on all 6 datasets. [sent-228, score-0.431]
</p><p>72 Our method in solid red, and rejection based on distance from decision boundary in dashed green. [sent-275, score-0.431]
</p><p>73 While the traditional approach cannot achieve error less than 8% for any rejection rate, in our approach the test error decreases monotonically to zero with rejection rate. [sent-279, score-0.777]
</p><p>74 Furthermore, a clear advantage for our method over a large range of rejection rates is evident in the Haberman dataset. [sent-280, score-0.291]
</p><p>75 9  Related work  The literature on theoretical studies of selective classiﬁcation is rather sparse. [sent-287, score-0.409]
</p><p>76 El-Yaniv and Wiener [1] studied the performance of a simple selective learning strategy for the realizable case. [sent-288, score-0.557]
</p><p>77 Given an hypothesis class H, and a sample Sm , their method abstain from prediction if all hypotheses in the version space do not agree on the target sample. [sent-289, score-0.42]
</p><p>78 They were able to show that their selective classiﬁer achieves perfect classiﬁcation with meaningful coverage under some conditions. [sent-290, score-0.625]
</p><p>79 Given an hypothesis class H, the method outputs a weighted average of all the hypotheses in H, where the weight of each hypothesis exponentially depends on its individual training error. [sent-294, score-0.416]
</p><p>80 They were able to bound the probability of misclassiﬁcation by 2R(h∗ ) + ϵ(m) and, under some conditions, they proved a bound of 5R(h∗ ) + ϵ(F, m) on the rejection rate. [sent-296, score-0.291]
</p><p>81 We include in our “ensemble” only hypotheses with sufﬁciently low empirical error and we abstain if the weighted average of all predictions is not deﬁnitive ( ̸= ±1). [sent-299, score-0.317]
</p><p>82 Excess risk bounds were developed by Herbei and Wegkamp [19] for a model where each rejection incurs a cost 0 ≤ d ≤ 1/2. [sent-301, score-0.47]
</p><p>83 Their bound applies to any empirical risk minimizer over a hypothesis class of ternary hypotheses (whose output is in {±1, reject}). [sent-302, score-0.558]
</p><p>84 A rejection mechanism for SVMs based on distance from decision boundary is perhaps the most widely known and used rejection technique. [sent-304, score-0.722]
</p><p>85 Few papers proposed alternative techniques for rejection in the case of SVMs. [sent-306, score-0.291]
</p><p>86 Those include taking the reject area into account during optimization [25], training two SVM classiﬁers with asymmetric cost [26], and using a hinge loss [20]. [sent-307, score-0.284]
</p><p>87 [16] proposed an efﬁcient implementation of SVM with a reject option using a double hinge loss. [sent-309, score-0.244]
</p><p>88 They empirically compared their results with two other selective classiﬁers: the one proposed by Bartlett and Wegkamp [20] and the traditional rejection based on distance from decision boundary. [sent-310, score-0.853]
</p><p>89 In their experiments there was no statistically signiﬁcant advantage to either method compared to the traditional approach for high rejection rates. [sent-311, score-0.35]
</p><p>90 10  Conclusion  We presented and analyzed a learning strategy for selective classiﬁcation that achieves weak optimality. [sent-312, score-0.493]
</p><p>91 We showed that the coverage rate directly depends on the disagreement coefﬁcient, thus linking between active learning and selective classiﬁcation. [sent-313, score-0.816]
</p><p>92 Recently it has been shown that, for the noise-free case, active learning can be reduced to selective classiﬁcation [27]. [sent-314, score-0.48]
</p><p>93 Exact implementation of our strategy, or exact computation of the disbelief index may be too difﬁcult to achieve or even obtain with approximation guarantees. [sent-316, score-0.535]
</p><p>94 Our empirical examination of the proposed algorithm indicate that it can provide signiﬁcant and consistent advantage over the traditional rejection technique with SVMs. [sent-318, score-0.4]
</p><p>95 A bound on the label complexity of agnostic active learning. [sent-342, score-0.252]
</p><p>96 2004 IMS medallion lecture: Local rademacher complexities and oracle inequalities in risk minimization. [sent-357, score-0.32]
</p><p>97 Discussion of ”2004 IMS medallion lecture: Local rademacher complexities and oracle inequalities in risk minimization” by V. [sent-363, score-0.32]
</p><p>98 Smoothness, disagreement coefﬁcient, and the label complexity of agnostic active learning. [sent-387, score-0.453]
</p><p>99 Classiﬁcation with a reject option using a hinge loss. [sent-443, score-0.244]
</p><p>100 An ordinal data method for the classiﬁcation with reject option. [sent-484, score-0.179]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('disbelief', 0.438), ('selective', 0.409), ('rejection', 0.291), ('disagreement', 0.201), ('classi', 0.197), ('sm', 0.197), ('reject', 0.179), ('risk', 0.179), ('agnostic', 0.135), ('coverage', 0.135), ('hx', 0.134), ('abstain', 0.128), ('er', 0.124), ('hypothesis', 0.111), ('weakly', 0.104), ('minimizer', 0.099), ('erm', 0.096), ('strategy', 0.084), ('perfect', 0.081), ('dis', 0.077), ('perfection', 0.073), ('hypotheses', 0.071), ('active', 0.071), ('index', 0.068), ('excess', 0.067), ('realizable', 0.064), ('breast', 0.064), ('haberman', 0.064), ('traditional', 0.059), ('region', 0.059), ('coef', 0.059), ('rc', 0.058), ('decision', 0.057), ('cancer', 0.054), ('rejected', 0.054), ('hanneke', 0.052), ('ers', 0.052), ('cation', 0.051), ('empirical', 0.05), ('curve', 0.049), ('grandvalet', 0.049), ('herbei', 0.049), ('ims', 0.049), ('medallion', 0.049), ('wegkamp', 0.049), ('class', 0.048), ('boundary', 0.046), ('label', 0.046), ('training', 0.045), ('bernstein', 0.044), ('diminishes', 0.043), ('hepatitis', 0.043), ('svms', 0.04), ('svm', 0.04), ('bartlett', 0.039), ('pima', 0.039), ('opposite', 0.038), ('br', 0.038), ('error', 0.038), ('distance', 0.037), ('complexities', 0.036), ('agree', 0.036), ('focusing', 0.035), ('dence', 0.034), ('facilitates', 0.033), ('heuristically', 0.033), ('microarray', 0.033), ('wiener', 0.033), ('medical', 0.033), ('option', 0.033), ('beygelzimer', 0.032), ('certainty', 0.032), ('freund', 0.032), ('hinge', 0.032), ('test', 0.031), ('optimizer', 0.03), ('libsvm', 0.03), ('weighted', 0.03), ('ln', 0.029), ('oracle', 0.029), ('diagnosis', 0.029), ('achieve', 0.029), ('least', 0.029), ('lecture', 0.028), ('loss', 0.028), ('inf', 0.028), ('observing', 0.027), ('diminishing', 0.027), ('accomplish', 0.027), ('inequalities', 0.027), ('pr', 0.027), ('union', 0.027), ('termed', 0.026), ('prediction', 0.026), ('dasgupta', 0.026), ('lemma', 0.025), ('support', 0.025), ('nition', 0.025), ('forced', 0.025), ('height', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="28-tfidf-1" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>2 0.34572953 <a title="28-tfidf-2" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>3 0.15039597 <a title="28-tfidf-3" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>Author: Maxim Raginsky, Alexander Rakhlin</p><p>Abstract: We develop uniﬁed information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander’s capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of “disagreement coefﬁcient.” For passive learning, our lower bounds match the upper bounds of Gin´ and Koltchinskii up to constants and generalize analogous results of Mase sart and N´ d´ lec. For active learning, we provide ﬁrst known lower bounds based e e on the capacity function rather than the disagreement coefﬁcient. 1</p><p>4 0.1065411 <a title="28-tfidf-4" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>Author: Jia Deng, Sanjeev Satheesh, Alexander C. Berg, Fei Li</p><p>Abstract: We present a novel approach to efﬁciently learn a label tree for large scale classiﬁcation with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classiﬁers for each node in the tree. This approach also allows ﬁne grained control over the efﬁciency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classiﬁcation with 10184 classes and 9 million images. We demonstrate signiﬁcant improvements in test accuracy and efﬁciency with less training time and more balanced trees compared to the previous state of the art by Bengio et al. 1</p><p>5 0.10112198 <a title="28-tfidf-5" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>Author: Stéphan J. Clémençcon</p><p>Abstract: Many clustering techniques aim at optimizing empirical criteria that are of the form of a U -statistic of degree two. Given a measure of dissimilarity between pairs of observations, the goal is to minimize the within cluster point scatter over a class of partitions of the feature space. It is the purpose of this paper to deﬁne a general statistical framework, relying on the theory of U -processes, for studying the performance of such clustering methods. In this setup, under adequate assumptions on the complexity of the subsets forming the partition candidates, the √ excess of clustering risk is proved to be of the order OP (1/ n). Based on recent results related to the tail behavior of degenerate U -processes, it is also shown how to establish tighter rate bounds. Model selection issues, related to the number of clusters forming the data partition in particular, are also considered. 1</p><p>6 0.088405296 <a title="28-tfidf-6" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>7 0.088252418 <a title="28-tfidf-7" href="./nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">178 nips-2011-Multiclass Boosting: Theory and Algorithms</a></p>
<p>8 0.085478686 <a title="28-tfidf-8" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>9 0.085205995 <a title="28-tfidf-9" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>10 0.078377128 <a title="28-tfidf-10" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>11 0.076365039 <a title="28-tfidf-11" href="./nips-2011-A_Two-Stage_Weighting_Framework_for_Multi-Source_Domain_Adaptation.html">12 nips-2011-A Two-Stage Weighting Framework for Multi-Source Domain Adaptation</a></p>
<p>12 0.072056577 <a title="28-tfidf-12" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>13 0.067119405 <a title="28-tfidf-13" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>14 0.066667706 <a title="28-tfidf-14" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>15 0.062886685 <a title="28-tfidf-15" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>16 0.06281881 <a title="28-tfidf-16" href="./nips-2011-Co-Training_for_Domain_Adaptation.html">53 nips-2011-Co-Training for Domain Adaptation</a></p>
<p>17 0.061466742 <a title="28-tfidf-17" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>18 0.06022625 <a title="28-tfidf-18" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>19 0.059840698 <a title="28-tfidf-19" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>20 0.059827056 <a title="28-tfidf-20" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.191), (1, -0.028), (2, -0.074), (3, -0.02), (4, 0.03), (5, 0.047), (6, 0.035), (7, -0.139), (8, -0.148), (9, -0.006), (10, -0.113), (11, 0.031), (12, 0.115), (13, 0.055), (14, -0.023), (15, -0.11), (16, -0.039), (17, -0.09), (18, 0.108), (19, 0.034), (20, 0.123), (21, 0.005), (22, 0.01), (23, 0.068), (24, 0.014), (25, 0.167), (26, -0.153), (27, -0.028), (28, 0.18), (29, -0.046), (30, 0.214), (31, -0.017), (32, 0.089), (33, 0.052), (34, -0.116), (35, -0.158), (36, -0.169), (37, -0.085), (38, -0.036), (39, 0.053), (40, 0.107), (41, 0.025), (42, -0.017), (43, 0.046), (44, -0.05), (45, -0.125), (46, 0.025), (47, -0.012), (48, 0.043), (49, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94095653 <a title="28-lsi-1" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>2 0.84100831 <a title="28-lsi-2" href="./nips-2011-Selective_Prediction_of_Financial_Trends_with_Hidden_Markov_Models.html">246 nips-2011-Selective Prediction of Financial Trends with Hidden Markov Models</a></p>
<p>Author: Dmitry Pidan, Ran El-Yaniv</p><p>Abstract: Focusing on short term trend prediction in a Ä?Ĺš nancial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance. We examine two types of selective mechanisms for HMM predictors. The Ä?Ĺš rst is a rejection in the spirit of ChowĂ˘&euro;&trade;s well-known ambiguity principle. The second is a specialized mechanism for HMMs that identiÄ?Ĺš es low quality HMM states and abstain from prediction in those states. We call this model selective HMM (sHMM). In both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner. We compare performance of the ambiguity-based rejection technique with that of the sHMM approach. Our results indicate that both methods are effective, and that the sHMM model is superior. 1</p><p>3 0.63046616 <a title="28-lsi-3" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>Author: Tianshi Gao, Daphne Koller</p><p>Abstract: Modern classiﬁcation tasks usually involve many class labels and can be informed by a broad range of features. Many of these tasks are tackled by constructing a set of classiﬁers, which are then applied at test time and then pieced together in a ﬁxed procedure determined in advance or at training time. We present an active classiﬁcation process at the test time, where each classiﬁer in a large ensemble is viewed as a potential observation that might inform our classiﬁcation process. Observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classiﬁcation gain from each observation as well as its computational cost. The expected classiﬁcation gain is computed using a probabilistic model that uses the outcome from previous observations. This active classiﬁcation process is applied at test time for each individual test instance, resulting in an efﬁcient instance-speciﬁc decision path. We demonstrate the beneﬁt of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classiﬁcation accuracy at a fraction of the computational costs of traditional methods.</p><p>4 0.59663081 <a title="28-lsi-4" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study multi-label prediction for structured output sets, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multilabel classiﬁcation techniques are typically not applicable in this situation, because they require explicit enumeration of the label set, which is infeasible in case of structured outputs. Relying on techniques originally designed for single-label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems. In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneﬁcial properties with single-label maximum-margin approaches, in particular formulation as a convex optimization problem, efﬁcient working set training, and PAC-Bayesian generalization bounds. 1</p><p>5 0.52892107 <a title="28-lsi-5" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>Author: Maxim Raginsky, Alexander Rakhlin</p><p>Abstract: We develop uniﬁed information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander’s capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of “disagreement coefﬁcient.” For passive learning, our lower bounds match the upper bounds of Gin´ and Koltchinskii up to constants and generalize analogous results of Mase sart and N´ d´ lec. For active learning, we provide ﬁrst known lower bounds based e e on the capacity function rather than the disagreement coefﬁcient. 1</p><p>6 0.50103462 <a title="28-lsi-6" href="./nips-2011-Learning_Patient-Specific_Cancer_Survival_Distributions_as_a_Sequence_of_Dependent_Regressors.html">147 nips-2011-Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors</a></p>
<p>7 0.48805577 <a title="28-lsi-7" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>8 0.48101446 <a title="28-lsi-8" href="./nips-2011-An_Exact_Algorithm_for_F-Measure_Maximization.html">33 nips-2011-An Exact Algorithm for F-Measure Maximization</a></p>
<p>9 0.47683308 <a title="28-lsi-9" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>10 0.46123835 <a title="28-lsi-10" href="./nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">178 nips-2011-Multiclass Boosting: Theory and Algorithms</a></p>
<p>11 0.46029961 <a title="28-lsi-11" href="./nips-2011-Composite_Multiclass_Losses.html">59 nips-2011-Composite Multiclass Losses</a></p>
<p>12 0.45608884 <a title="28-lsi-12" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>13 0.45286119 <a title="28-lsi-13" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>14 0.44938189 <a title="28-lsi-14" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>15 0.44859117 <a title="28-lsi-15" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>16 0.41624504 <a title="28-lsi-16" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>17 0.40147313 <a title="28-lsi-17" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>18 0.39264837 <a title="28-lsi-18" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>19 0.3911663 <a title="28-lsi-19" href="./nips-2011-A_Collaborative_Mechanism_for_Crowdsourcing_Prediction_Problems.html">3 nips-2011-A Collaborative Mechanism for Crowdsourcing Prediction Problems</a></p>
<p>20 0.3898935 <a title="28-lsi-20" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.028), (4, 0.046), (20, 0.018), (26, 0.037), (31, 0.052), (33, 0.019), (43, 0.045), (45, 0.589), (57, 0.019), (74, 0.037), (83, 0.017), (99, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99686193 <a title="28-lda-1" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><p>same-paper 2 0.99487609 <a title="28-lda-2" href="./nips-2011-Agnostic_Selective_Classification.html">28 nips-2011-Agnostic Selective Classification</a></p>
<p>Author: Yair Wiener, Ran El-Yaniv</p><p>Abstract: For a learning problem whose associated excess loss class is (β, B)-Bernstein, we show that it is theoretically possible to track the same classiﬁcation performance of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this √ rejected region of the domain is shown to be diminishing at rate O(Bθ( 1/m)β ), where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless, we heuristically approximate this strategy and develop a novel selective classiﬁcation algorithm using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms the traditional rejection mechanism based on distance from decision boundary. 1</p><p>3 0.99484539 <a title="28-lda-3" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>Author: Mark Schmidt, Nicolas L. Roux, Francis R. Bach</p><p>Abstract: We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen ﬁxed error level on a set of structured sparsity problems. 1</p><p>4 0.99209744 <a title="28-lda-4" href="./nips-2011-Stochastic_convex_optimization_with_bandit_feedback.html">272 nips-2011-Stochastic convex optimization with bandit feedback</a></p>
<p>Author: Alekh Agarwal, Dean P. Foster, Daniel J. Hsu, Sham M. Kakade, Alexander Rakhlin</p><p>Abstract: This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact set X under a stochastic bandit feedback model. In this model, the algorithm is allowed to observe noisy realizations of the function value f (x) at any query point x ∈ X . We demonstrate √ a generalization of the ellipsoid algorithm that √ incurs O(poly(d) T ) regret. Since any algorithm has regret at least Ω( T ) on this problem, our algorithm is optimal in terms of the scaling with T . 1</p><p>5 0.9869675 <a title="28-lda-5" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>Author: Ziming Zhang, Lubor Ladicky, Philip Torr, Amir Saffari</p><p>Abstract: Local Coordinate Coding (LCC) [18] is a method for modeling functions of data lying on non-linear manifolds. It provides a set of anchor points which form a local coordinate system, such that each data point on the manifold can be approximated by a linear combination of its anchor points, and the linear weights become the local coordinate coding. In this paper we propose encoding data using orthogonal anchor planes, rather than anchor points. Our method needs only a few orthogonal anchor planes for coding, and it can linearize any (α, β, p)-Lipschitz smooth nonlinear function with a ﬁxed expected value of the upper-bound approximation error on any high dimensional data. In practice, the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (SVD). We apply our method to model the coordinates locally in linear SVMs for classiﬁcation tasks, and our experiment on MNIST shows that using only 50 anchor planes our method achieves 1.72% error rate, while LCC achieves 1.90% error rate using 4096 anchor points. 1</p><p>6 0.98431033 <a title="28-lda-6" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>7 0.98324752 <a title="28-lda-7" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>8 0.94692171 <a title="28-lda-8" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>9 0.91567129 <a title="28-lda-9" href="./nips-2011-Active_Classification_based_on_Value_of_Classifier.html">19 nips-2011-Active Classification based on Value of Classifier</a></p>
<p>10 0.90237719 <a title="28-lda-10" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>11 0.9020434 <a title="28-lda-11" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>12 0.89601052 <a title="28-lda-12" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>13 0.89380753 <a title="28-lda-13" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>14 0.89300728 <a title="28-lda-14" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>15 0.88872874 <a title="28-lda-15" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>16 0.88809854 <a title="28-lda-16" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>17 0.88732243 <a title="28-lda-17" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>18 0.87688065 <a title="28-lda-18" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>19 0.87615323 <a title="28-lda-19" href="./nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</a></p>
<p>20 0.87466139 <a title="28-lda-20" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
