<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-172" href="#">nips2011-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</h1>
<br/><p>Source: <a title="nips-2011-172-pdf" href="http://papers.nips.cc/paper/4218-minimax-localization-of-structural-information-in-large-noisy-matrices.pdf">pdf</a></p><p>Author: Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh</p><p>Abstract: We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a ﬂurry of activity. We make the following contributions 1. We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest. 2. We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. 3. We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition. 1</p><p>Reference: <a title="nips-2011-172-reference" href="../nips2011_reference/nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu †  School of Computer Science and †† Department of Statistics, Carnegie Mellon University  Abstract We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. [sent-9, score-0.232]
</p><p>2 This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. [sent-10, score-0.872]
</p><p>3 We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest. [sent-14, score-1.402]
</p><p>4 We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. [sent-16, score-0.219]
</p><p>5 We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition. [sent-18, score-1.111]
</p><p>6 1  Introduction  Biclustering is the problem of identifying a (typically) sparse set of relevant columns and rows in a large, noisy data matrix. [sent-19, score-0.219]
</p><p>7 In these applications, the data matrix is often indexed by (object, feature) pairs and the goal is to identify clusters in this set of bipartite variables. [sent-22, score-0.141]
</p><p>8 Thus, while clustering aims to identify global structure in the data, biclustering take a more local approach by jointly clustering both objects and features. [sent-28, score-0.7]
</p><p>9 In order to study, understand and compare biclustering algorithms we consider a simple theoretical model of biclustering [18, 17, 26]. [sent-30, score-1.148]
</p><p>10 Notice that the magnitude of the signal for all the coordinates in the bicluster K1 ⇥ K2 is pk k . [sent-40, score-0.657]
</p><p>11 The parameter measures the strength of the signal, 1 2 and is the key quantity we will be studying. [sent-41, score-0.157]
</p><p>12 2  2  We focus on the case of a single bicluster that appears as an elevated sub-matrix of size k1 ⇥ k2 with signal strength embedded in a large n1 ⇥n2 data matrix with entries corrupted by additive Gaussian noise with variance 2 . [sent-42, score-0.974]
</p><p>13 Under this model, the biclustering problem is formulated as the problem of estimating the sets K1 and K2 , based on a single noisy observation A of the unknown signal matrix uv0 . [sent-43, score-0.794]
</p><p>14 Biclustering is most subtle when the matrix is large with several irrelevant variables, the entries are highly noisy, and the bicluster is small as deﬁned by a sparse set of rows/columns. [sent-44, score-0.627]
</p><p>15 We provide a sharp characterization of tuples of ( , n1 , n2 , k1 , k2 , 2 ) under which it is possible to recover the bicluster and study several common methods and establish the regimes under which they succeed. [sent-45, score-0.601]
</p><p>16 We establish minimax lower and upper bounds for the following class of models. [sent-46, score-0.212]
</p><p>17 I We derive a lower bound that identiﬁes tuples of ( , n1 , n2 , k1 , k2 , 2 ) under which we can recover the true biclustering from a noisy high dimensional matrix. [sent-49, score-0.719]
</p><p>18 We show that a combinatorial procedure based on the scan statistic achieves the minimax optimal limits, however it is impractical as it requires enumerating all possible sub-matrices of a given size in a large matrix. [sent-50, score-0.405]
</p><p>19 the relation between and (n1 , n2 , k1 , k2 , 2 )) under which some computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and sparse singular vector decomposition (SSVD) succeed with high probability. [sent-53, score-1.155]
</p><p>20 We consider the detection of both small and large biclusters of weak activation, and show that at the minimax scaling the problem is surprisingly subtle (e. [sent-54, score-0.328]
</p><p>21 Minimax:  ⇠  2  1  k2 )  ◆  Element-wise thresholding does not take advantage of any structure in the data matrix and hence does not achieve the minimax scaling for any bicluster size. [sent-61, score-0.913]
</p><p>22 If the clusters are big enough row/column averaging performs better than element-wise thresholding since it can take advantage of structure. [sent-62, score-0.352]
</p><p>23 We also study a convex relaxation for sparse SVD, based on the DSPCA algorithm proposed by [11] that encourages the singular vectors of the matrix to be supported over a sparse set of variables. [sent-63, score-0.358]
</p><p>24 However, despite the increasing popularity of this method, we show that it is only guaranteed to yield a sparse set of singular vectors when the SNR is quite high, equivalent to element-wise thresholding, and fails for stronger scalings of the SNR. [sent-64, score-0.266]
</p><p>25 1  Related work  Due to its practical importance and difﬁculty biclustering has attracted considerable attention (for some recent surveys see [9, 27, 20, 22]). [sent-66, score-0.574]
</p><p>26 Broadly algorithms for biclustering can be categorized as either score-based searches, or spectral algorithms. [sent-67, score-0.574]
</p><p>27 Many of the proposed algorithms for identifying relevant clusters are based on heuristic searches whose goal is to identify large average sub-matrices or sub-matrices that are well ﬁt by a two-way ANOVA model. [sent-68, score-0.182]
</p><p>28 Therefore, A0 A is a spiked covariance matrix and it is possible to use the existing theoretical results on the ﬁrst eigenvalue to characterize the left singular vector of A. [sent-76, score-0.248]
</p><p>29 For biclustering applications, the assumption that exactly one u or v is random, is not justiﬁable, therefore, theoretical results for the spiked covariance model do not translate directly. [sent-78, score-0.687]
</p><p>30 Our setup for the biclustering problem also falls in the framework of structured normal means multiple hypothesis testing problems, where for each entry in the matrix the hypotheses are that the entry has mean 0 versus an elevated mean. [sent-81, score-0.795]
</p><p>31 The presence of a bicluster (sub-matrix) however imposes structure on which elements are elevated concurrently. [sent-82, score-0.618]
</p><p>32 For example, [5] consider the detection of elevated intervals and other parametric structures along an ordered line or grid, [4] consider detection of elevated connected paths in tree and lattice topologies, [3] considers nonparametric cluster structures in a regular grid. [sent-84, score-0.422]
</p><p>33 In addition, [1] consider testing of different elevated structures in a general but known graph topology. [sent-85, score-0.183]
</p><p>34 Our setup for the biclustering problem requires identiﬁcation of an elevated submatrix in an unordered matrix. [sent-86, score-0.727]
</p><p>35 However, computationally efﬁcient procedures that achieve the minimax SNR thresholds are only known for a few of these problems. [sent-88, score-0.265]
</p><p>36 Our results for biclustering have a similar ﬂavor, in that the minimax threshold requires a combinatorial procedure whereas the computationally efﬁcient procedures we investigate are often sub-optimal. [sent-89, score-1.026]
</p><p>37 In Section 2, we provide a lower bound on the minimum signal strength needed for successfully identifying the bicluster. [sent-91, score-0.467]
</p><p>38 Section 3 presents a combinatorial procedure which achieves the lower bound and hence is minimax optimal. [sent-92, score-0.352]
</p><p>39 We investigate some computationally efﬁcient procedures in Section 4. [sent-93, score-0.142]
</p><p>40 2  Lower bound  In this section, we derive a lower bound for the problem of identifying the correct bicluster, indexed by K1 and K2 , in model (1). [sent-96, score-0.175]
</p><p>41 Intuitively, if either the signal-to-noise ratio / or the cluster size is small, the minimum signal strength needed will be high since it is harder to distinguish the bicluster from the noise. [sent-98, score-0.858]
</p><p>42 The result can be interpreted in the following way: for any biclustering procedure , if 0  min , then there exists some element in the model class ⇥( 0 , k1 , k2 ) such that the probability of incorrectly identifying the sets K1 and K2 is bounded away from zero. [sent-105, score-0.714]
</p><p>43 3  Minimax optimal combinatorial procedure  We now investigate a combinatorial procedure achieving the lower bound of Theorem 1, in the sense that, for any ✓ 2 ⇥( min , k1 , k2 ), the probability of recovering the true bicluster (K1 , K2 ) tends to one as n1 and n2 grow unbounded. [sent-112, score-0.889]
</p><p>44 This scan procedure consists in enumerating all possible pairs of subsets of the row and column indexes of size k1 and k2 , respectively, and choosing the one whose corresponding submatrix has the largest overall sum. [sent-113, score-0.23]
</p><p>45 The estimated bicluster is the pair of subsets of sizes k1 and k2 achieving the ˜ ˜ i2K1 j2K2 highest score: ˜ ˜ ˜ ˜ (A) := argmax S(K1 , K2 ) subject to |K1 | = k1 , |K2 | = k2 . [sent-115, score-0.492]
</p><p>46 (6) ˜ ˜ (K 1 ,K 2 )  The following theorem determines the signal strength bicluster. [sent-116, score-0.384]
</p><p>47 Comparing to the lower bound in Theorem 1, we observe that the combinatorial procedure using the decoder that looks for all possible clusters and chooses the one with largest score achieves the lower bound up to constants. [sent-120, score-0.399]
</p><p>48 The combinatorial procedure requires the signal to be positive, but not necessarily constant throughout the bicluster. [sent-122, score-0.303]
</p><p>49 In fact it is easy to see that provided the average signal in the bicluster is larger than that stipulated by the theorem this procedure succeeds with high probability irrespective of how the signal is distributed across the bicluster. [sent-123, score-0.972]
</p><p>50 Establishing minimax lower bounds and a procedure that adapts to unknown k1 and k2 is an open problem. [sent-125, score-0.249]
</p><p>51 4  Computationally efﬁcient biclustering procedures  In this section we investigate the performance of various procedures for biclustering, that, unlike the optimal scan statistic procedure studied in the previous section, are computationally tractable. [sent-126, score-0.932]
</p><p>52 For each of these procedures however, computational ease comes at the cost of suboptimal performance: recovery of the true bicluster is only possible if the is much larger than the minimax signal strength of Theorem 1. [sent-127, score-1.05]
</p><p>53 1  Element-wise thresholding  The simplest procedure that we analyze is based on element-wise thresholding. [sent-129, score-0.302]
</p><p>54 The bicluster is estimated as ⌧} (8) thr (A, ⌧ ) := {(i, j) 2 [n1 ] ⇥ [n2 ] : |aij | where ⌧ > 0 is a parameter. [sent-130, score-0.533]
</p><p>55 The following theorem characterizes the signal strength the element-wise thresholding to succeed in recovering the bicluster. [sent-131, score-0.691]
</p><p>56 If  then P[  p k1 k2 thr (A, ⌧ )  r  2 log  k1 k2  +  r  2 log  (n1  k1 )(n2  k2 ) + k1 (n2  k2 ) + k2 (n1  k1 )  ! [sent-135, score-0.135]
</p><p>57 Comparing Theorem 3 with theplower bound in Theorem 1, we observe that the signal p strength needs to be O(max( k1 , k2 )) larger than the lower bound. [sent-137, score-0.412]
</p><p>58 This is not surprising, since the element-wise thresholding is not exploiting the structure of the problem, but is assuming that the large elements of the matrix A are positioned randomly. [sent-138, score-0.237]
</p><p>59 if  ✓q q p c k1 k2 2 log k1 k2 + 2 log (n1 k1 )(n2 k2 )+k1 (n2 k2 )+k2 (n1 k1 ) for a small enough constant c then thresholding will no longer recover the bicluster with probability at least 1 . [sent-141, score-0.81]
</p><p>60 It is also worth noting that thresholding neither requires the signal in the bicluster to be constant nor positive provided it is larger in magnitude, at every entry, than the threshold speciﬁed in the theorem. [sent-142, score-0.888]
</p><p>61 2  Row/Column averaging  Next, we analyze another a procedure based on column and row averaging. [sent-144, score-0.23]
</p><p>62 When the bicluster is large this procedure exploits the structure of the problem and outperforms the simple elementwise thresholding and the sparse SVD, which is discussed in the following section. [sent-145, score-0.838]
</p><p>63 The averaging procedure works only well if the bicluster is “large”, as speciﬁed below, since otherwise the row or column average is dominated by the noise. [sent-146, score-0.688]
</p><p>64 More precisely, the averaging procedure computes the average of each row and column of A and outputs the k1 rows and k2 columns with the largest average. [sent-147, score-0.241]
</p><p>65 The bicluster is estimated then as avg (A)  (9)  := {i 2 [n1 ] : rr,i  k1 } ⇥ {j 2 [n2 ] : rc,j  k2 }. [sent-149, score-0.492]
</p><p>66 The following theorem characterizes the signal strength succeed in recovering the bicluster. [sent-150, score-0.488]
</p><p>67 required for the averaging procedure to 1/2+↵  1/2+↵  Theorem 4. [sent-151, score-0.154]
</p><p>68 Comparing to Theorem 3, we observe that the averaging requires lower signal strength than the p p element-wise thresholding when the bicluster is large, that is, k1 = ⌦( n1 ) and k2 = ⌦( n2 ). [sent-155, score-1.166]
</p><p>69 Unless both k1 = O(n1 ) and k2 = O(n2 ), the procedure does not achieve the lower bound of Theorem 1, however, the procedure is simple and computationally efﬁcient. [sent-156, score-0.251]
</p><p>70 It is also not hard to show that this theorem is sharp in its characterization of the averaging procedure. [sent-157, score-0.178]
</p><p>71 Further, unlike thresholding, averaging requires the signal to be positive in the bicluster. [sent-158, score-0.254]
</p><p>72 It is interesting to note that a large bicluster can also be identiﬁed without assuming the normality of the noise matrix . [sent-159, score-0.526]
</p><p>73 3  Sparse singular value decomposition (SSVD)  An alternate way to estimate K1 and K2 would be based on the singular value decomposition (SVD), ˜ ˜ ˜ ˜ i. [sent-162, score-0.25]
</p><p>74 Unfortuu v nately, such a method would perform poorly when the signal is weak and the dimensionality is ˜ ˜ high, since, due to the accumulation of noise, u and v are poor estimates of u and v and and do not exploit the fact that u and v are sparse. [sent-165, score-0.216]
</p><p>75 In fact, it is now well understood [8] that SVD is strongly inconsistent when the signal strength is weak, i. [sent-166, score-0.322]
</p><p>76 Based on the sparse singular vectors u and v, we estimate the bicluster b u as b b K1 = {j 2 [n1 ] : uj 6= 0} b and K2 = {j 2 [n2 ] : vj 6= 0}. [sent-175, score-0.704]
</p><p>77 b (12) b 21 , and, therefore, provided The user deﬁned parameter controls the sparsity of the solution X b b the solution is of rank one, it also controls the sparsity of the vectors u and v and of the estimated bicluster. [sent-176, score-0.186]
</p><p>78 b The following theorem provides sufﬁcient conditions for the solution X to be rank one and to recover the bicluster. [sent-177, score-0.148]
</p><p>79 It is worth noting that SSVD correctly recovers signed vectors u and v under this signal strength. [sent-183, score-0.198]
</p><p>80 If p 2 ck1 k2 log max(n1 k1 , n2 k2 ), (14) with  then the optimization problem (11) does not have a rank 1 solution that correctly p p recovers the sparsity pattern with probability at least 1 O(exp( ( k1 + k2 )2 ) for sufﬁciently large n1 and n2 . [sent-190, score-0.146]
</p><p>81 The signal strength needs to be of the same order as for the element-wise thresholding, which is somewhat surprising since from the formulation of the SSVD optimization problem it seems that the procedure uses the structure of the problem. [sent-193, score-0.387]
</p><p>82 From numerical simulations in Section 5 we observe that although SSVD requires the same scaling as thresholding, it consistently performs slightly better at a ﬁxed signal strength. [sent-194, score-0.225]
</p><p>83 5  Simulation results  We test the performance of the three computationally efﬁcient procedures on synthetic data: thresholding, averaging and sparse SVD. [sent-195, score-0.288]
</p><p>84 the Hamming distance between su and su b rescaled to be between 0 and 1) against the rescaled sample size. [sent-200, score-0.17]
</p><p>85 For thresholding and sparse SVD the rescaled scaling (x-axis) is p k  rescaled scaling (x-axis) is p k  ↵  n . [sent-202, score-0.531]
</p><p>86 log(n k)  log(n k)  and for averaging the  We observe that there is a sharp threshold between success  and failure of the algorithms, and the curves show good agreement with our theory. [sent-203, score-0.164]
</p><p>87 We can make a direct comparison between thresholding and sparse SVD (since the curves are identically rescaled) to see that at least empirically sparse SVD succeeds at a smaller scaling constant than thresholding even though their asymptotic rates are identical. [sent-205, score-0.625]
</p><p>88 8  1  1  n = 100 n = 200 n = 300 n = 400 n = 500  Hamming fraction  Hamming fraction  1  2  3  4 5 Signal strength  6  n = 100 n = 200 n = 300 n = 400 n = 500  0. [sent-215, score-0.317]
</p><p>89 2 0 1  7  2  3  4 5 Signal strength  6  7  Figure 1: Thresholding: Hamming fraction versus rescaled signal strength. [sent-219, score-0.513]
</p><p>90 2 0 0  1  2  3 4 Signal strength  5  6  n = 100 n = 200 n = 300 n = 400 n = 500  0. [sent-226, score-0.157]
</p><p>91 2 0 0  7  1  2  3 4 Signal strength  5  6  7  Figure 2: Averaging: Hamming fraction versus rescaled signal strength. [sent-230, score-0.513]
</p><p>92 6  Hamming fraction  Hamming fraction  1  n = 100 n = 200 n = 300 n = 400 n = 500  0. [sent-239, score-0.16]
</p><p>93 5  3  Figure 3: Sparse SVD: Hamming fraction versus rescaled signal strength. [sent-250, score-0.356]
</p><p>94 6  Discussion  In this paper, we analyze biclustering using a simple statistical model (1), where a sparse rank one matrix is perturbed with noise. [sent-251, score-0.765]
</p><p>95 Using this model, we have characterized the minimal signal strength below which no procedure can succeed in recovering the bicluster. [sent-252, score-0.471]
</p><p>96 However, it is still an open problem to ﬁnd a computationally efﬁcient procedure that is minimax optimal. [sent-254, score-0.26]
</p><p>97 [2] analyze the convex relaxation procedure proposed in [11] for high-dimensional sparse PCA. [sent-257, score-0.211]
</p><p>98 Under the minimax scaling for this problem they show that provided a rank-1 solution exists it has the desired sparsity pattern (they were however not able to show that a rank-1 solution exists with high probability). [sent-258, score-0.258]
</p><p>99 The singular values and vectors of low rank perturbations of large rectangular random matrices. [sent-312, score-0.179]
</p><p>100 A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. [sent-443, score-0.167]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('biclustering', 0.574), ('bicluster', 0.492), ('thresholding', 0.203), ('ssvd', 0.185), ('signal', 0.165), ('strength', 0.157), ('minimax', 0.144), ('hamming', 0.129), ('elevated', 0.126), ('svd', 0.124), ('singular', 0.101), ('spiked', 0.09), ('averaging', 0.089), ('rescaled', 0.085), ('fraction', 0.08), ('sparse', 0.078), ('identifying', 0.075), ('combinatorial', 0.073), ('procedures', 0.07), ('procedure', 0.065), ('snr', 0.063), ('theorem', 0.062), ('ery', 0.062), ('malware', 0.062), ('clusters', 0.06), ('principal', 0.055), ('scan', 0.054), ('succeed', 0.054), ('scalings', 0.054), ('computationally', 0.051), ('weak', 0.051), ('amini', 0.05), ('log', 0.047), ('emmanuel', 0.047), ('rank', 0.045), ('aij', 0.044), ('cluster', 0.044), ('enumerating', 0.042), ('biclusters', 0.041), ('thr', 0.041), ('decoder', 0.041), ('scaling', 0.04), ('lower', 0.04), ('genes', 0.037), ('clustering', 0.037), ('aarti', 0.036), ('testing', 0.035), ('gene', 0.035), ('matrix', 0.034), ('sparsity', 0.034), ('analyze', 0.034), ('relaxation', 0.034), ('tuples', 0.033), ('vectors', 0.033), ('signatures', 0.031), ('groups', 0.03), ('recovering', 0.03), ('bound', 0.03), ('drugs', 0.029), ('cand', 0.029), ('exhaustive', 0.029), ('detection', 0.029), ('threshold', 0.028), ('establish', 0.028), ('objects', 0.027), ('sharp', 0.027), ('submatrix', 0.027), ('statistic', 0.027), ('tr', 0.027), ('biological', 0.027), ('identi', 0.026), ('versus', 0.026), ('identify', 0.025), ('shen', 0.025), ('proteins', 0.025), ('banach', 0.025), ('decomposition', 0.024), ('ordered', 0.024), ('columns', 0.024), ('max', 0.023), ('subtle', 0.023), ('succeeds', 0.023), ('covariance', 0.023), ('recovery', 0.022), ('column', 0.022), ('handbook', 0.022), ('searches', 0.022), ('bipartite', 0.022), ('species', 0.022), ('structures', 0.022), ('investigate', 0.021), ('noisy', 0.021), ('recover', 0.021), ('rows', 0.021), ('establishing', 0.021), ('observe', 0.02), ('pca', 0.02), ('characterizes', 0.02), ('solution', 0.02), ('row', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="172-tfidf-1" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>Author: Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh</p><p>Abstract: We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a ﬂurry of activity. We make the following contributions 1. We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest. 2. We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. 3. We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition. 1</p><p>2 0.11801832 <a title="172-tfidf-2" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>3 0.097653143 <a title="172-tfidf-3" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><p>4 0.091888897 <a title="172-tfidf-4" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><p>5 0.078335926 <a title="172-tfidf-5" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>6 0.068929031 <a title="172-tfidf-6" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>7 0.068838075 <a title="172-tfidf-7" href="./nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">260 nips-2011-Sparse Features for PCA-Like Linear Regression</a></p>
<p>8 0.066045523 <a title="172-tfidf-8" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>9 0.063048713 <a title="172-tfidf-9" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>10 0.059740163 <a title="172-tfidf-10" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>11 0.059110869 <a title="172-tfidf-11" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>12 0.058152642 <a title="172-tfidf-12" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>13 0.055290826 <a title="172-tfidf-13" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>14 0.053352814 <a title="172-tfidf-14" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>15 0.052843761 <a title="172-tfidf-15" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>16 0.051143825 <a title="172-tfidf-16" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>17 0.050620697 <a title="172-tfidf-17" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>18 0.050392192 <a title="172-tfidf-18" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>19 0.049406253 <a title="172-tfidf-19" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>20 0.047119696 <a title="172-tfidf-20" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.006), (2, -0.039), (3, -0.112), (4, -0.042), (5, 0.042), (6, -0.006), (7, 0.042), (8, 0.057), (9, 0.018), (10, 0.056), (11, 0.084), (12, -0.001), (13, -0.053), (14, 0.06), (15, -0.03), (16, 0.019), (17, -0.06), (18, 0.041), (19, -0.012), (20, 0.034), (21, 0.009), (22, -0.001), (23, -0.007), (24, 0.006), (25, -0.014), (26, 0.018), (27, 0.063), (28, 0.023), (29, 0.008), (30, -0.032), (31, 0.043), (32, 0.047), (33, 0.048), (34, 0.005), (35, -0.043), (36, 0.01), (37, 0.03), (38, -0.103), (39, -0.06), (40, 0.01), (41, 0.041), (42, 0.009), (43, -0.082), (44, -0.002), (45, 0.043), (46, 0.033), (47, 0.035), (48, -0.035), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93432271 <a title="172-lsi-1" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>Author: Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh</p><p>Abstract: We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a ﬂurry of activity. We make the following contributions 1. We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest. 2. We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. 3. We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition. 1</p><p>2 0.67373222 <a title="172-lsi-2" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>3 0.66088957 <a title="172-lsi-3" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>Author: Youwei Zhang, Laurent E. Ghaoui</p><p>Abstract: Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing ﬁrst-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. 1</p><p>4 0.64695722 <a title="172-lsi-4" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>Author: Lester W. Mackey, Michael I. Jordan, Ameet Talwalkar</p><p>Abstract: This work introduces Divide-Factor-Combine (DFC), a parallel divide-andconquer framework for noisy matrix factorization. DFC divides a large-scale matrix factorization task into smaller subproblems, solves each subproblem in parallel using an arbitrary base matrix factorization algorithm, and combines the subproblem solutions using techniques from randomized matrix approximation. Our experiments with collaborative ﬁltering, video background modeling, and simulated data demonstrate the near-linear to super-linear speed-ups attainable with this approach. Moreover, our analysis shows that DFC enjoys high-probability recovery guarantees comparable to those of its base algorithm.</p><p>5 0.63701302 <a title="172-lsi-5" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>Author: Andrew E. Waters, Aswin C. Sankaranarayanan, Richard Baraniuk</p><p>Abstract: We consider the problem of recovering a matrix M that is the sum of a low-rank matrix L and a sparse matrix S from a small set of linear measurements of the form y = A(M) = A(L + S). This model subsumes three important classes of signal recovery problems: compressive sensing, afﬁne rank minimization, and robust principal component analysis. We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it. Empirically, SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efﬁcient implementation. Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efﬁcacy of the algorithm. 1</p><p>6 0.63386428 <a title="172-lsi-6" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>7 0.62777686 <a title="172-lsi-7" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>8 0.62548155 <a title="172-lsi-8" href="./nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">260 nips-2011-Sparse Features for PCA-Like Linear Regression</a></p>
<p>9 0.6134572 <a title="172-lsi-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.60811567 <a title="172-lsi-10" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>11 0.60001683 <a title="172-lsi-11" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>12 0.59603208 <a title="172-lsi-12" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>13 0.56886995 <a title="172-lsi-13" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>14 0.55898267 <a title="172-lsi-14" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>15 0.55761266 <a title="172-lsi-15" href="./nips-2011-Regularized_Laplacian_Estimation_and_Fast_Eigenvector_Approximation.html">236 nips-2011-Regularized Laplacian Estimation and Fast Eigenvector Approximation</a></p>
<p>16 0.5381937 <a title="172-lsi-16" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>17 0.5381577 <a title="172-lsi-17" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>18 0.5223549 <a title="172-lsi-18" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>19 0.5215677 <a title="172-lsi-19" href="./nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization.html">51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p>
<p>20 0.51453024 <a title="172-lsi-20" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (4, 0.052), (20, 0.045), (26, 0.031), (31, 0.09), (33, 0.02), (43, 0.066), (45, 0.15), (48, 0.012), (57, 0.031), (65, 0.015), (74, 0.054), (82, 0.214), (83, 0.037), (84, 0.016), (99, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86369854 <a title="172-lda-1" href="./nips-2011-Composite_Multiclass_Losses.html">59 nips-2011-Composite Multiclass Losses</a></p>
<p>Author: Elodie Vernet, Mark D. Reid, Robert C. Williamson</p><p>Abstract: We consider loss functions for multiclass prediction problems. We show when a multiclass loss can be expressed as a “proper composite loss”, which is the composition of a proper loss and a link function. We extend existing results for binary losses to multiclass losses. We determine the stationarity condition, Bregman representation, order-sensitivity, existence and uniqueness of the composite representation for multiclass losses. We subsume existing results on “classiﬁcation calibration” by relating it to properness and show that the simple integral representation for binary proper losses can not be extended to multiclass losses. 1</p><p>same-paper 2 0.80005455 <a title="172-lda-2" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>Author: Mladen Kolar, Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh</p><p>Abstract: We consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries. This problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering. Despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent. The problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a ﬂurry of activity. We make the following contributions 1. We prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest. 2. We show that a combinatorial procedure based on the scan statistic achieves this optimal limit. 3. We characterize the SNR required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition. 1</p><p>3 0.75304735 <a title="172-lda-3" href="./nips-2011-Non-Asymptotic_Analysis_of_Stochastic_Approximation_Algorithms_for_Machine_Learning.html">187 nips-2011-Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning</a></p>
<p>Author: Eric Moulines, Francis R. Bach</p><p>Abstract: We consider the minimization of a convex objective function deﬁned on a Hilbert space, which is only available through unbiased estimates of its gradients. This problem includes standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community. We provide a non-asymptotic analysis of the convergence of two well-known algorithms, stochastic gradient descent (a.k.a. Robbins-Monro algorithm) as well as a simple modiﬁcation where iterates are averaged (a.k.a. Polyak-Ruppert averaging). Our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant. This situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence. We illustrate our theoretical results with simulations on synthetic and standard datasets.</p><p>4 0.70508444 <a title="172-lda-4" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>Author: Rina Foygel, Ohad Shamir, Nati Srebro, Ruslan Salakhutdinov</p><p>Abstract: We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneﬁcial. 1</p><p>5 0.70490021 <a title="172-lda-5" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>6 0.70351893 <a title="172-lda-6" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>7 0.70302987 <a title="172-lda-7" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>8 0.70244557 <a title="172-lda-8" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>9 0.70051301 <a title="172-lda-9" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>10 0.69854987 <a title="172-lda-10" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>11 0.69830948 <a title="172-lda-11" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>12 0.69781327 <a title="172-lda-12" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>13 0.69741392 <a title="172-lda-13" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>14 0.69737679 <a title="172-lda-14" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>15 0.69697559 <a title="172-lda-15" href="./nips-2011-Generalizing_from_Several_Related_Classification_Tasks_to_a_New_Unlabeled_Sample.html">106 nips-2011-Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</a></p>
<p>16 0.69539285 <a title="172-lda-16" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>17 0.69536102 <a title="172-lda-17" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>18 0.6949994 <a title="172-lda-18" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>19 0.69489098 <a title="172-lda-19" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>20 0.69488448 <a title="172-lda-20" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
