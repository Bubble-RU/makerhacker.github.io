<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-155" href="#">nips2011-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</h1>
<br/><p>Source: <a title="nips-2011-155-pdf" href="http://papers.nips.cc/paper/4249-learning-to-agglomerate-superpixel-hierarchies.pdf">pdf</a></p><p>Author: Viren Jain, Srinivas C. Turaga, K Briggman, Moritz N. Helmstaedter, Winfried Denk, H. S. Seung</p><p>Abstract: An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. The function that evaluates similarity is traditionally handdesigned, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement. 1</p><p>Reference: <a title="nips-2011-155-reference" href="../nips2011_reference/nips-2011-Learning_to_Agglomerate_Superpixel_Hierarchies_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sebastian Seung Howard Hughes Medical Institute Massachusetts Institute of Technology  Abstract An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. [sent-5, score-0.366]
</p><p>2 The function that evaluates similarity is traditionally handdesigned, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. [sent-6, score-0.126]
</p><p>3 Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. [sent-7, score-0.141]
</p><p>4 We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). [sent-8, score-0.203]
</p><p>5 When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. [sent-9, score-0.889]
</p><p>6 The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement. [sent-10, score-0.7]
</p><p>7 1  Introduction  A clustering is deﬁned as a partitioning of a set of elements into subsets called clusters. [sent-11, score-0.153]
</p><p>8 The goal is to learn a clustering algorithm that generalizes to new elements and new clusters. [sent-15, score-0.153]
</p><p>9 One prominent example is image segmentation, the division of an image into clusters of pixels that correspond to distinct objects in the scene. [sent-18, score-0.201]
</p><p>10 However, it is becoming popular to utilize a supervised clustering approach in which a segmentation algorithm is trained on a set of images for which ground truth is known [23, 32]. [sent-20, score-0.482]
</p><p>11 The Rand index has become increasingly popular for evaluating the accuracy of image segmentation [34, 3, 13, 15, 35], and has recently been used as an objective function for supervised learning of this task [32]. [sent-21, score-0.275]
</p><p>12 This paper focuses on agglomerative algorithms for clustering, which iteratively merge pairs of clusters that maximize a similarity function. [sent-22, score-0.383]
</p><p>13 Equivalently, the merged pairs may be those that minimize 1  a distance or dissimilarity function, which is like a similarity function up to a change of sign. [sent-23, score-0.135]
</p><p>14 Here we avoid such issues by basing learning on agglomerative clustering, which is an efﬁcient inference procedure in the ﬁrst place. [sent-30, score-0.162]
</p><p>15 In this formulation, the optimal action-value function turns out to be the optimal similarity function for agglomerative clustering. [sent-32, score-0.258]
</p><p>16 This DMDP formulation is helpful because it enables the application of ideas from reinforcement learning (RL) to ﬁnd an approximation to the optimal similarity function. [sent-33, score-0.141]
</p><p>17 Our formalism is generally applicable to any type of clustering, but is illustrated with a speciﬁc application to segmenting images by clustering superpixels. [sent-34, score-0.271]
</p><p>18 Recent research has shown that agglomerating superpixels using a hand-designed similarity function can improve segmentation accuracy [3]. [sent-36, score-0.347]
</p><p>19 It is plausible that it would be even more powerful to learn the similarity function from training data. [sent-37, score-0.138]
</p><p>20 Therefore we empirically evaluated LASH on the problem of segmenting images of brain tissue from serial electron microscopy, which has recently attracted a great deal of interest [6, 15]. [sent-44, score-0.419]
</p><p>21 We ﬁnd that LASH substantially improves upon state of the art convolutional network and random forest boundary-detection methods for this problem, reducing segmentation error (as measured by the Rand error) by 50% as compared to the next best technique. [sent-45, score-0.218]
</p><p>22 We also tried the simpler strategy of directly training superpixel similarities, and then applying single linkage clustering [2]. [sent-46, score-0.369]
</p><p>23 The goal of reinforcement learning (RL) is to ﬁnd a policy π that maximizes the expected value of total reward. [sent-50, score-0.105]
</p><p>24 Many RL methods are based on ﬁnding an optimal action-value function Q∗ (s, a), which is deﬁned as the sum of discounted rewards obtained by taking action a at state s and following the optimal policy thereafter. [sent-53, score-0.141]
</p><p>25 2  We can deﬁne agglomerative clustering as an MDP. [sent-55, score-0.315]
</p><p>26 For each pair of clusters in st , there is an action at ∈ A(st ) that merges them to yield the clustering st+1 = at (st ). [sent-57, score-0.379]
</p><p>27 To deﬁne the rewards of the MDP, we make use of the Rand index, a standard measure of agreement between two clusterings of the same set [25]. [sent-59, score-0.125]
</p><p>28 A clustering is equivalent to classifying all pairs of objects as belonging to the same cluster or different clusters. [sent-60, score-0.192]
</p><p>29 The Rand index RI(s, s￿ ) is the fraction of object pairs on which the clusterings s and s￿ agree. [sent-61, score-0.161]
</p><p>30 Therefore, we can deﬁne the immediate reward of action a as the resulting increase in the Rand index with respect to a ground truth clustering s∗ , R(s, a) = RI(a(s), s∗ ) − RI(s, s∗ ). [sent-62, score-0.352]
</p><p>31 An agglomerative clustering algorithm is a policy of this MDP, and the optimal similarity function is given by the optimal action-value function Q∗ . [sent-63, score-0.471]
</p><p>32 Therefore RL for a ﬁnite time horizon T is equivalent to maximizing the Rand index RI(sT , s∗ ) of the clustering at time T . [sent-65, score-0.191]
</p><p>33 We know R(s, a) exactly for the training data, but we would also like it to apply to data for which ground truth is unknown. [sent-69, score-0.123]
</p><p>34 , sT ) by using R(s, a) as a similarity function: iterate at = argmaxa R(st , a) and st+1 = at (st ), terminating when maxa R(st , a) ≤ 0. [sent-76, score-0.131]
</p><p>35 Train the parameters θ so that Qθ (st , a) ≈ R(st , a) for all st and for all a ∈ A(st ). [sent-78, score-0.135]
</p><p>36 Generate a new sequence of clusterings by using Qθ (s, a) as a similarity function: iterate at = argmaxa Qθ (st , a) and st+1 = at (st ), terminating when maxa Qθ (st , a) ≤ 0. [sent-80, score-0.215]
</p><p>37 Here the clustering s1 is the trivial one in which each element is its own cluster. [sent-83, score-0.185]
</p><p>38 (The termination of the clustering is equivalent to the continued selection of a “do-nothing” action that leaves the clustering the same, st+1 = st . [sent-84, score-0.481]
</p><p>39 ) This is an example of “on-policy” learning, because the function approximator Qθ is trained on clusterings generated by using it as a policy. [sent-85, score-0.161]
</p><p>40 If our approximation to the actionvalue function were perfect, Qθ (s, a) = R(s, a), then agglomerative clustering would amount to greedy maximization of the Rand index. [sent-93, score-0.315]
</p><p>41 It is straightforward to show that this yields the clustering that is the global maximum. [sent-94, score-0.153]
</p><p>42 3  Agglomerating superpixels for image segmentation  The introduction of the Berkeley segmentation database (BSD) provoked a renaissance of the boundary detection and segmentation literature. [sent-96, score-0.683]
</p><p>43 The creation of a ground-truth segmentation database enabled learning-driven methods for low-level boundary detection, which were found to outperform classic methods such as Canny’s [23, 10]. [sent-97, score-0.231]
</p><p>44 Global and multi-scale features were added to improve performance even further [26, 22, 29], and recently learning methods have been developed that directly optimize measures of segmentation performance [32, 13]. [sent-98, score-0.132]
</p><p>45 98  1  Figure 1: Performance comparison on a one megavoxel test set; parameters, such as the binarization threshold for the convolutional network (CN) afﬁnity graphs, were determined based on the optimal value on the training set. [sent-135, score-0.132]
</p><p>46 CN’s used a ﬁeld of view of 16 × 16 × 16, ilastik used a ﬁeld of view of 23 × 23 × 23, and LASH used a ﬁeld of view of 50 × 50 × 50. [sent-136, score-0.122]
</p><p>47 LASH leads to a substantial decrease in Rand error (1-Rand Index), and much higher connected pixel-pair precision at similar levels of recall as compared to other state of the art methods. [sent-137, score-0.106]
</p><p>48 The ‘Connected pixel-pairs’ curve measures the accuracy of connected pixel pairs pairs relative to ground truth. [sent-138, score-0.164]
</p><p>49 This measure corrects for the imbalance in the Rand error for segmentations in which most pixels are disconnected from one another, as in the case of EM reconstruction of dense brain wiring. [sent-139, score-0.104]
</p><p>50 For example, ‘Trivial Baseline’ above represents the trivial segmentation in which all pixels are disconnected from one another, and achieves relatively low Rand error but of course zero connected-pair recall. [sent-140, score-0.164]
</p><p>51 However, boundary detectors alone have so far failed to produce segmentations that rival human levels of accuracy. [sent-141, score-0.168]
</p><p>52 Therefore many recent studies use boundary detectors to generate an oversegmentation of the image into fragments, and then attempt to cluster the “superpixels” . [sent-142, score-0.22]
</p><p>53 This approach has been shown to improve the accuracy of segmenting natural images [3, 30]. [sent-143, score-0.118]
</p><p>54 A similar approach [2, 1, 17, 35, 16] has also been employed to segment 3d nanoscale images from serial electron microscopy [11, 9]. [sent-144, score-0.389]
</p><p>55 The training sets were augmented with their eight 3d orthogonal rotations and reﬂections to yield 16 training images that contained roughly 80 megavoxels of labeled training data. [sent-158, score-0.211]
</p><p>56 1  Boundary Detectors  For comparison purposes, as well as to provide supervoxels for LASH, we tested several state of the art boundary detection algorithms on the data. [sent-161, score-0.334]
</p><p>57 A convolutional network (CN) was trained to produce afﬁnity graphs that can be segmented using connected components or watershed [14, 33]. [sent-162, score-0.181]
</p><p>58 We also trained CNs using MALIS and BLOTC, which are recently proposed machine learning algorithms that optimize true metrics of segmentation performance. [sent-163, score-0.168]
</p><p>59 Image and segmentations are from a Z-X axis resectioning of the 100 × 100 × 100 voxel test set. [sent-166, score-0.111]
</p><p>60 (Right) Distribution of supervoxel sizes, as measured by percentage of image volume occupied by speciﬁc size ranges of supervoxels. [sent-170, score-0.39]
</p><p>61 Finally, we trained ‘ilastik,’ a random-forest based boundary detector [31]. [sent-172, score-0.168]
</p><p>62 Unlike the CNs, which operated on the raw image and learned features as part of the training process, ilastik uses a predeﬁned set of image features that represented low-level image structure such as intensity gradients and texture. [sent-173, score-0.389]
</p><p>63 The CNs used a ﬁeld of view of 16 × 16 × 16 voxels to make decisions about any particular image location, while ilastik used features from a ﬁeld of view of up to 23 × 23 × 23 voxels. [sent-174, score-0.247]
</p><p>64 To generate segmentations of the test set, we found connected components of the thresholded boundary detector output, and then performed marker-based watershed to grow out these regions until they touched. [sent-175, score-0.291]
</p><p>65 Segmentation performance is sensitive to the threshold used to binarize boundary detector output, so we used the threshold that minimized Rand error on the training set. [sent-178, score-0.174]
</p><p>66 9) to avoid undersegmented regions (in the test set, there was only one supervoxel in the initial oversegmentation which contained more than one ground truth region). [sent-181, score-0.442]
</p><p>67 The size of the supervoxels varied considerably, but the majority of the image volume was assigned to supervoxels larger than 1, 000 voxels in size (as shown in Figure 3). [sent-183, score-0.475]
</p><p>68 First the examples used in each training iteration were collected by segmenting all the images in the training set, not only a single image. [sent-187, score-0.202]
</p><p>69 Then the learned similarity function was applied to agglomerate supervoxels in the test set to yield the results in Figure 1. [sent-191, score-0.341]
</p><p>70 This classiﬁcation task is highly imbalanced, because the vast majority of voxel pairs belong to different ground truth clusters. [sent-196, score-0.162]
</p><p>71 Hence even a completely trivial segmentation in which every voxel is its own cluster can achieve fairly low Rand error (Figure 1). [sent-197, score-0.206]
</p><p>72 Visual comparison of segmentation performance is shown in Figure 2. [sent-201, score-0.132]
</p><p>73 3  Naive training of the similarity function on superpixel pairs  In the conventional algorithms for agglomerative clustering, the similarity S(A, B) of two clusters A and B can be reduced to the similarities S(x, y) of elements x ∈ A and y ∈ B. [sent-205, score-0.633]
</p><p>74 For example, single linkage clustering assumes that S(A, B) = maxx∈A,y∈B S(x, y). [sent-206, score-0.246]
</p><p>75 Consequently, LASH must truly compute new similarities after each agglomerative step. [sent-209, score-0.228]
</p><p>76 In contrast, conventional algorithms can start by computing the matrix of similarities between the elements to be clustered, and all further similarities between clusters follow from trivial computations. [sent-210, score-0.215]
</p><p>77 Therefore another method of learning agglomerative clustering is to train a similarity function on pairs of superpixels only, and then apply a standard agglomerative algorithm such as single linkage clustering. [sent-211, score-0.789]
</p><p>78 This has been previously been done for images from serial electron microscopy [2]. [sent-212, score-0.389]
</p><p>79 (Note that single linkage clustering is equivalent to creating a graph in which nodes are superpixels and edge weights are their similarities, and then ﬁnding the connected components of the thresholded graph. [sent-213, score-0.374]
</p><p>80 ) As shown in Figure 1, clustering superpixels in this way improves upon boundary detection algorithms. [sent-214, score-0.365]
</p><p>81 One might argue that the comparison is unfair, because the CNs and ilastik detected boundaries using a ﬁeld of view considerably smaller than that used in the LASH features (up to 50 × 50 × 50 for the SVF feature computation). [sent-217, score-0.122]
</p><p>82 This can be attributed to the efﬁciency gains associated with computations on supervoxels rather than voxels. [sent-223, score-0.175]
</p><p>83 Why does LASH outperform the naive method of directly training superpixel similarities used in single linkage clustering? [sent-225, score-0.313]
</p><p>84 The naive method uses the same amount of image context. [sent-226, score-0.106]
</p><p>85 LASH is probably superior because it trains the similarities by optimizing the clustering that they actually yield. [sent-232, score-0.219]
</p><p>86 The naive method resembles LASH, but with the modiﬁcation that the action-value function is trained only for the actions possible on the clustering s1 rather than on the entire sequence of clusterings (see Step 2 of the procedure in Section 2). [sent-233, score-0.304]
</p><p>87 In paticular, SEARN begins with a manually speciﬁed policy (given by ground truth or heuristics) and then iteratively degrades the policy as a classiﬁer is trained and ‘replaces’ the initial policy. [sent-239, score-0.237]
</p><p>88 We have implemented LASH with inﬁnite discounting of future rewards, but extending to ﬁnite discounting might produce better results. [sent-243, score-0.144]
</p><p>89 Generalizing the action space to include splitting of clusters as well as agglomeration might also be advantageous. [sent-244, score-0.161]
</p><p>90 Finally, the objective function optimized by learning might be tailored to better reﬂect more task-speciﬁc criteria, such as the number of locations that human might have to correct (‘proofread’) to yield an error-free segmentation by semiautomated means. [sent-245, score-0.132]
</p><p>91 Appendix Features of supervoxel pairs used by the similarity function The similarity function that we trained with LASH required as input a set of features for each supervoxel pair that might be merged. [sent-247, score-0.897]
</p><p>92 For each supervoxel pair, we ﬁrst computed a ‘decision point,’ deﬁned as the midpoint of the shortest line that connects any two points of the supervoxels. [sent-248, score-0.315]
</p><p>93 This feature measures the orientation of each supervoxel near the decision point. [sent-250, score-0.388]
</p><p>94 Finally, for each supervoxel in the pair we also included the above features for the closest 4 other decision points that involve that supervoxel. [sent-251, score-0.348]
</p><p>95 Overall, this feature set yielded a 138 dimensional feature vector for each supervoxel pair. [sent-252, score-0.315]
</p><p>96 The smoothed vector ﬁeld (SVF) shape feature attempts to determine the orientation of a supervoxel near some speciﬁc location (e. [sent-253, score-0.397]
</p><p>97 A spherical mask of radius 5 is selected around each image location Ix,y,z , and ￿x,y,z is v then computed as the largest eigenvector of the 3 × 3 second order image moment matrix for that window. [sent-259, score-0.15]
</p><p>98 The smoothed vector at the center of mass of the supervoxel is used to compute angular orientation of the supervoxel (see Figure 3). [sent-263, score-0.712]
</p><p>99 Towards neural circuit reconstruction with volume electron microscopy techniques. [sent-300, score-0.246]
</p><p>100 Serial block-face scanning electron microscopy to reconstruct threedimensional tissue nanostructure. [sent-331, score-0.296]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lash', 0.525), ('supervoxel', 0.315), ('rand', 0.223), ('blotc', 0.21), ('supervoxels', 0.175), ('agglomerative', 0.162), ('clustering', 0.153), ('cn', 0.136), ('st', 0.135), ('segmentation', 0.132), ('electron', 0.123), ('microscopy', 0.123), ('briggman', 0.122), ('denk', 0.122), ('helmstaedter', 0.122), ('ilastik', 0.122), ('cns', 0.105), ('boundary', 0.099), ('similarity', 0.096), ('serial', 0.093), ('linkage', 0.093), ('malis', 0.087), ('svf', 0.087), ('superpixels', 0.084), ('clusterings', 0.084), ('superpixel', 0.081), ('image', 0.075), ('discounting', 0.072), ('agglomerate', 0.07), ('agglomeration', 0.07), ('segmentations', 0.069), ('segmenting', 0.068), ('similarities', 0.066), ('policy', 0.06), ('convolutional', 0.055), ('turaga', 0.053), ('eld', 0.051), ('clusters', 0.051), ('tissue', 0.05), ('voxels', 0.05), ('images', 0.05), ('rl', 0.048), ('searn', 0.046), ('watershed', 0.046), ('oversegmentation', 0.046), ('sharon', 0.046), ('jain', 0.046), ('nity', 0.046), ('reinforcement', 0.045), ('connected', 0.044), ('smoothed', 0.042), ('ground', 0.042), ('voxel', 0.042), ('af', 0.042), ('neurobiology', 0.042), ('training', 0.042), ('rewards', 0.041), ('approximator', 0.041), ('immediate', 0.04), ('orientation', 0.04), ('action', 0.04), ('opinion', 0.04), ('truth', 0.039), ('pairs', 0.039), ('index', 0.038), ('ri', 0.036), ('thick', 0.036), ('fowlkes', 0.036), ('trained', 0.036), ('hierarchies', 0.035), ('agglomerating', 0.035), ('dmdp', 0.035), ('kasthuri', 0.035), ('lichtman', 0.035), ('megavoxel', 0.035), ('megavoxels', 0.035), ('ome', 0.035), ('reslice', 0.035), ('schalek', 0.035), ('vitaladevuni', 0.035), ('brain', 0.035), ('argmaxa', 0.035), ('merge', 0.035), ('decision', 0.033), ('detector', 0.033), ('mdp', 0.033), ('trivial', 0.032), ('naive', 0.031), ('pattern', 0.031), ('precision', 0.031), ('hayworth', 0.031), ('andres', 0.031), ('isbi', 0.031), ('art', 0.031), ('roth', 0.03), ('supervised', 0.03), ('ieee', 0.03), ('biomedical', 0.03), ('detection', 0.029), ('vision', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="155-tfidf-1" href="./nips-2011-Learning_to_Agglomerate_Superpixel_Hierarchies.html">155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</a></p>
<p>Author: Viren Jain, Srinivas C. Turaga, K Briggman, Moritz N. Helmstaedter, Winfried Denk, H. S. Seung</p><p>Abstract: An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. The function that evaluates similarity is traditionally handdesigned, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement. 1</p><p>2 0.17791952 <a title="155-tfidf-2" href="./nips-2011-Higher-Order_Correlation_Clustering_for_Image_Segmentation.html">119 nips-2011-Higher-Order Correlation Clustering for Image Segmentation</a></p>
<p>Author: Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, Chang D. Yoo</p><p>Abstract: For many of the state-of-the-art computer vision algorithms, image segmentation is an important preprocessing step. As such, several image segmentation algorithms have been proposed, however, with certain reservation due to high computational load and many hand-tuning parameters. Correlation clustering, a graphpartitioning algorithm often used in natural language processing and document clustering, has the potential to perform better than previously proposed image segmentation algorithms. We improve the basic correlation clustering formulation by taking into account higher-order cluster relationships. This improves clustering in the presence of local boundary ambiguities. We ﬁrst apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels. Fast inference is possible by linear programming relaxation, and also effective parameter learning framework by structured support vector machine is possible. Experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms.</p><p>3 0.13989884 <a title="155-tfidf-3" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>4 0.11186702 <a title="155-tfidf-4" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>Author: Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: Graph cut optimization is one of the standard workhorses of image segmentation since for binary random ﬁeld representations of the image, it gives globally optimal results and there are efﬁcient polynomial time implementations. Often, the random ﬁeld is applied over a ﬂat partitioning of the image into non-intersecting elements, such as pixels or super-pixels. In the paper we show that if, instead of a ﬂat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding beneﬁts of global optimality and efﬁciency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree. We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a ﬂat partitioning) and demonstrate that the optimization in the pylon model is able to ﬂexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches. 1</p><p>5 0.10718893 <a title="155-tfidf-5" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<p>Author: Abhishek Kumar, Piyush Rai, Hal Daume</p><p>Abstract: In many clustering problems, we have access to multiple views of the data each of which could be individually used for clustering. Exploiting information from multiple views, one can hope to ﬁnd a clustering that is more accurate than the ones obtained using the individual views. Often these different views admit same underlying clustering of the data, so we can approach this problem by looking for clusterings that are consistent across the views, i.e., corresponding data points in each view should have same cluster membership. We propose a spectral clustering framework that achieves this goal by co-regularizing the clustering hypotheses, and propose two co-regularization schemes to accomplish this. Experimental comparisons with a number of baselines on two synthetic and three real-world datasets establish the efﬁcacy of our proposed approaches.</p><p>6 0.095969789 <a title="155-tfidf-6" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>7 0.086741157 <a title="155-tfidf-7" href="./nips-2011-Efficient_Inference_in_Fully_Connected_CRFs_with_Gaussian_Edge_Potentials.html">76 nips-2011-Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</a></p>
<p>8 0.083874956 <a title="155-tfidf-8" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>9 0.074260876 <a title="155-tfidf-9" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>10 0.073651612 <a title="155-tfidf-10" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>11 0.072648801 <a title="155-tfidf-11" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>12 0.067323633 <a title="155-tfidf-12" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>13 0.064631335 <a title="155-tfidf-13" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>14 0.064567253 <a title="155-tfidf-14" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>15 0.063699424 <a title="155-tfidf-15" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<p>16 0.062327035 <a title="155-tfidf-16" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>17 0.058192547 <a title="155-tfidf-17" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>18 0.058014229 <a title="155-tfidf-18" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>19 0.057902813 <a title="155-tfidf-19" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>20 0.057372466 <a title="155-tfidf-20" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.178), (1, 0.037), (2, -0.03), (3, 0.152), (4, -0.028), (5, 0.037), (6, -0.063), (7, 0.005), (8, 0.083), (9, -0.026), (10, 0.054), (11, 0.156), (12, 0.105), (13, -0.196), (14, -0.06), (15, 0.003), (16, 0.008), (17, -0.038), (18, 0.004), (19, -0.039), (20, -0.049), (21, 0.016), (22, -0.042), (23, -0.01), (24, -0.016), (25, -0.003), (26, -0.023), (27, 0.015), (28, 0.033), (29, -0.008), (30, 0.01), (31, -0.042), (32, 0.065), (33, -0.065), (34, -0.033), (35, 0.023), (36, 0.051), (37, 0.009), (38, 0.042), (39, 0.021), (40, -0.004), (41, 0.044), (42, 0.021), (43, -0.005), (44, -0.013), (45, -0.087), (46, -0.023), (47, 0.024), (48, 0.032), (49, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93220705 <a title="155-lsi-1" href="./nips-2011-Learning_to_Agglomerate_Superpixel_Hierarchies.html">155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</a></p>
<p>Author: Viren Jain, Srinivas C. Turaga, K Briggman, Moritz N. Helmstaedter, Winfried Denk, H. S. Seung</p><p>Abstract: An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. The function that evaluates similarity is traditionally handdesigned, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement. 1</p><p>2 0.84661424 <a title="155-lsi-2" href="./nips-2011-Higher-Order_Correlation_Clustering_for_Image_Segmentation.html">119 nips-2011-Higher-Order Correlation Clustering for Image Segmentation</a></p>
<p>Author: Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, Chang D. Yoo</p><p>Abstract: For many of the state-of-the-art computer vision algorithms, image segmentation is an important preprocessing step. As such, several image segmentation algorithms have been proposed, however, with certain reservation due to high computational load and many hand-tuning parameters. Correlation clustering, a graphpartitioning algorithm often used in natural language processing and document clustering, has the potential to perform better than previously proposed image segmentation algorithms. We improve the basic correlation clustering formulation by taking into account higher-order cluster relationships. This improves clustering in the presence of local boundary ambiguities. We ﬁrst apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels. Fast inference is possible by linear programming relaxation, and also effective parameter learning framework by structured support vector machine is possible. Experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms.</p><p>3 0.64902747 <a title="155-lsi-3" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<p>Author: Abhishek Kumar, Piyush Rai, Hal Daume</p><p>Abstract: In many clustering problems, we have access to multiple views of the data each of which could be individually used for clustering. Exploiting information from multiple views, one can hope to ﬁnd a clustering that is more accurate than the ones obtained using the individual views. Often these different views admit same underlying clustering of the data, so we can approach this problem by looking for clusterings that are consistent across the views, i.e., corresponding data points in each view should have same cluster membership. We propose a spectral clustering framework that achieves this goal by co-regularizing the clustering hypotheses, and propose two co-regularization schemes to accomplish this. Experimental comparisons with a number of baselines on two synthetic and three real-world datasets establish the efﬁcacy of our proposed approaches.</p><p>4 0.64848 <a title="155-lsi-4" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>Author: Soumya Ghosh, Andrei B. Ungureanu, Erik B. Sudderth, David M. Blei</p><p>Abstract: The distance dependent Chinese restaurant process (ddCRP) was recently introduced to accommodate random partitions of non-exchangeable data [1]. The ddCRP clusters data in a biased way: each data point is more likely to be clustered with other data that are near it in an external sense. This paper examines the ddCRP in a spatial setting with the goal of natural image segmentation. We explore the biases of the spatial ddCRP model and propose a novel hierarchical extension better suited for producing “human-like” segmentations. We then study the sensitivity of the models to various distance and appearance hyperparameters, and provide the ﬁrst rigorous comparison of nonparametric Bayesian models in the image segmentation domain. On unsupervised image segmentation, we demonstrate that similar performance to existing nonparametric Bayesian models is possible with substantially simpler models and algorithms.</p><p>5 0.61829561 <a title="155-lsi-5" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>Author: Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: Graph cut optimization is one of the standard workhorses of image segmentation since for binary random ﬁeld representations of the image, it gives globally optimal results and there are efﬁcient polynomial time implementations. Often, the random ﬁeld is applied over a ﬂat partitioning of the image into non-intersecting elements, such as pixels or super-pixels. In the paper we show that if, instead of a ﬂat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding beneﬁts of global optimality and efﬁciency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree. We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a ﬂat partitioning) and demonstrate that the optimization in the pylon model is able to ﬂexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches. 1</p><p>6 0.60976881 <a title="155-lsi-6" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>7 0.60322714 <a title="155-lsi-7" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>8 0.57665902 <a title="155-lsi-8" href="./nips-2011-Efficient_Inference_in_Fully_Connected_CRFs_with_Gaussian_Edge_Potentials.html">76 nips-2011-Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</a></p>
<p>9 0.5548088 <a title="155-lsi-9" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>10 0.5184322 <a title="155-lsi-10" href="./nips-2011-Clustering_via_Dirichlet_Process_Mixture_Models_for_Portable_Skill_Discovery.html">52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</a></p>
<p>11 0.50983977 <a title="155-lsi-11" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>12 0.47244745 <a title="155-lsi-12" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>13 0.46765226 <a title="155-lsi-13" href="./nips-2011-History_distribution_matching_method_for_predicting_effectiveness_of_HIV_combination_therapies.html">120 nips-2011-History distribution matching method for predicting effectiveness of HIV combination therapies</a></p>
<p>14 0.44173497 <a title="155-lsi-14" href="./nips-2011-Simultaneous_Sampling_and_Multi-Structure_Fitting_with_Adaptive_Reversible_Jump_MCMC.html">255 nips-2011-Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC</a></p>
<p>15 0.43513522 <a title="155-lsi-15" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>16 0.42611617 <a title="155-lsi-16" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>17 0.41587713 <a title="155-lsi-17" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>18 0.41172379 <a title="155-lsi-18" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>19 0.40863526 <a title="155-lsi-19" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>20 0.40102395 <a title="155-lsi-20" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (4, 0.029), (20, 0.065), (26, 0.01), (31, 0.064), (33, 0.031), (43, 0.029), (45, 0.084), (57, 0.023), (65, 0.012), (74, 0.462), (83, 0.028), (84, 0.014), (99, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96285129 <a title="155-lda-1" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>Author: Olana Missura, Thomas Gärtner</p><p>Abstract: Motivated by applications in electronic games as well as teaching systems, we investigate the problem of dynamic difﬁculty adjustment. The task here is to repeatedly ﬁnd a game difﬁculty setting that is neither ‘too easy’ and bores the player, nor ‘too difﬁcult’ and overburdens the player. The contributions of this paper are (i) the formulation of difﬁculty adjustment as an online learning problem on partially ordered sets, (ii) an exponential update algorithm for dynamic difﬁculty adjustment, (iii) a bound on the number of wrong difﬁculty settings relative to the best static setting chosen in hindsight, and (iv) an empirical investigation of the algorithm when playing against adversaries. 1</p><p>2 0.94304579 <a title="155-lda-2" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>Author: Artin Armagan, Merlise Clyde, David B. Dunson</p><p>Abstract: In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We ﬁrst propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efﬁciently to the types of truly massive data sets that are now encountered routinely. 1</p><p>3 0.94129717 <a title="155-lda-3" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>Author: David P. Wipf</p><p>Abstract: In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications). In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of ﬁnding, to the extent possible, regimes of dictionary invariant performance. In particular, a Type II Bayesian estimator with a dictionarydependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees no longer hold. This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions. 1</p><p>same-paper 4 0.86651605 <a title="155-lda-4" href="./nips-2011-Learning_to_Agglomerate_Superpixel_Hierarchies.html">155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</a></p>
<p>Author: Viren Jain, Srinivas C. Turaga, K Briggman, Moritz N. Helmstaedter, Winfried Denk, H. S. Seung</p><p>Abstract: An agglomerative clustering algorithm merges the most similar pair of clusters at every iteration. The function that evaluates similarity is traditionally handdesigned, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training. Here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem. We apply this general method to segment images by clustering superpixels, an application that we call Learning to Agglomerate Superpixel Hierarchies (LASH). When applied to a challenging dataset of brain images from serial electron microscopy, LASH dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms. The naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement. 1</p><p>5 0.76950777 <a title="155-lda-5" href="./nips-2011-Demixed_Principal_Component_Analysis.html">68 nips-2011-Demixed Principal Component Analysis</a></p>
<p>Author: Wieland Brendel, Ranulfo Romo, Christian K. Machens</p><p>Abstract: In many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters. In electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters. The heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difﬁcult. Standard dimensionality reduction techniques such as principal component analysis (PCA) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret. Here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters. We show how to modify the loss function of PCA so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters. We call this method demixed principal component analysis (dPCA) as the principal components here segregate the parameter dependencies. We phrase the problem as a probabilistic graphical model, and present a fast Expectation-Maximization (EM) algorithm. We demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response. 1</p><p>6 0.5927074 <a title="155-lda-6" href="./nips-2011-On_Strategy_Stitching_in_Large_Extensive_Form_Multiplayer_Games.html">196 nips-2011-On Strategy Stitching in Large Extensive Form Multiplayer Games</a></p>
<p>7 0.58330393 <a title="155-lda-7" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>8 0.56727093 <a title="155-lda-8" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>9 0.54607749 <a title="155-lda-9" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>10 0.54264247 <a title="155-lda-10" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>11 0.54193383 <a title="155-lda-11" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>12 0.54058725 <a title="155-lda-12" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<p>13 0.53019518 <a title="155-lda-13" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>14 0.52705026 <a title="155-lda-14" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>15 0.52668852 <a title="155-lda-15" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>16 0.52628148 <a title="155-lda-16" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>17 0.52469736 <a title="155-lda-17" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>18 0.52370805 <a title="155-lda-18" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>19 0.52095163 <a title="155-lda-19" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>20 0.51441318 <a title="155-lda-20" href="./nips-2011-Continuous-Time_Regression_Models_for_Longitudinal_Networks.html">62 nips-2011-Continuous-Time Regression Models for Longitudinal Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
