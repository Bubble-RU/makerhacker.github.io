<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-77" href="#">nips2011-77</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</h1>
<br/><p>Source: <a title="nips-2011-77-pdf" href="http://papers.nips.cc/paper/4429-efficient-learning-of-generalized-linear-and-single-index-models-with-isotonic-regression.pdf">pdf</a></p><p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>Reference: <a title="nips-2011-77-reference" href="../nips2011_reference/nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. [sent-9, score-0.055]
</p><p>2 The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). [sent-10, score-0.279]
</p><p>3 However, to obtain provable performance, the method requires a fresh sample every iteration. [sent-11, score-0.055]
</p><p>4 We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. [sent-13, score-0.357]
</p><p>5 1  Introduction  The oft used linear regression paradigm models a dependent variable Y as a linear function of a vector-valued independent variable X. [sent-15, score-0.066]
</p><p>6 Generalized linear models (GLMs) provide a ﬂexible extension of linear regression, by assuming that the dependent variable Y is of the form, E[Y |X] = u(w · X); u is referred to as the inverse link function or transfer function (see [1] for a review). [sent-17, score-0.111]
</p><p>7 Generalized linear models include commonly used regression techniques such as logistic regression, where u(z) = 1/(1 + e−z ) is the logistic function. [sent-18, score-0.178]
</p><p>8 The class of perceptrons also falls in this category, where u is a simple piecewise linear function of the form /¯, with the slope of the middle piece being the inverse of the margin. [sent-19, score-0.112]
</p><p>9 Unfortunately, in the case of GLMs, even in the setting when u is known, the problem of ﬁtting a model that minimizes squared error is typically not convex. [sent-21, score-0.064]
</p><p>10 We are not aware of any classical estimation procedure for GLMs which is both computationally and statistically efﬁcient, and with provable guarantees. [sent-22, score-0.039]
</p><p>11 The case when both u and w are unknown (sometimes referred to as Single Index Models (SIMs)), involves the more challenging (and practically relevant) question of jointly estimating u and w, 1  where u may come from a large non-parametric family such as all monotonic functions. [sent-24, score-0.092]
</p><p>12 2) Is there a computationally efﬁcient algorithm for this joint estimation? [sent-26, score-0.04]
</p><p>13 The Isotron algorithm of Kalai and Sastry [7] provides the ﬁrst provably efﬁcient method for learning GLMs and SIMs, under the common assumption that u is monotonic and Lipschitz, and assuming that the data corresponds to the model. [sent-31, score-0.126]
</p><p>14 The algorithm is a variant of the “gradient-like” perceptron algorithm, where apart from the perceptronlike updates, an isotonic regression procedure is performed on the linear predictions using the Pool Adjacent Violators (PAV) algorithm, on every iteration. [sent-33, score-0.297]
</p><p>15 While the Isotron algorithm is appealing due to its ease of implementation (it has no parameters other than the number of iterations to run) and theoretical guarantees (it works for any u, w), there is one principal drawback. [sent-34, score-0.039]
</p><p>16 It is a batch algorithm, but the analysis given requires the algorithm to be run on fresh samples each batch. [sent-35, score-0.056]
</p><p>17 Our Contributions: We show that the overﬁtting problem in Isotron stems from the fact that although it uses a slope (Lipschitz) condition as an assumption in the analysis, it does not constrain the output hypothesis to be of this form. [sent-37, score-0.053]
</p><p>18 To address this issue, we introduce the S L I SOTRON algorithm (pronounced slice-o-tron, combining slope and Isotron). [sent-38, score-0.054]
</p><p>19 The algorithm replaces the isotonic regression step of the Isotron by ﬁnding the best non-decreasing function with a bounded Lipschitz parameter - this constraint plays here a similar role as the margin in classiﬁcation algorithms. [sent-39, score-0.265]
</p><p>20 We also note S L I SOTRON (like Isotron) has a signiﬁcant advantage over standard regression techniques, since it does not require knowing the transfer function. [sent-40, score-0.177]
</p><p>21 We provide an efﬁcient O(n log(n)) time algorithm for ﬁnding the best non-decreasing function with a bounded Lipschitz parameter, improving on the previous O(n2 ) algorithm [10]. [sent-44, score-0.036]
</p><p>22 We begin with a simple perceptron-like algorithm for ﬁtting GLMs, with a known transfer function u which is monotone and Lipschitz. [sent-46, score-0.129]
</p><p>23 Somewhat surprisingly, prior to this work (and Isotron [7]) a computationally efﬁcient procedure that guarantees to learn GLMs was not known. [sent-47, score-0.043]
</p><p>24 Section 4 contains the more challenging S L I SOTRON algorithm and also the efﬁcient O(n log(n)) algorithm for Lipschitz isotonic regression. [sent-48, score-0.217]
</p><p>25 Our algorithms and 1  In the more challenging agnostic setting, the data is not required to be distributed according to a true u and w, but it is required to ﬁnd the best u, w which minimize the empirical squared error. [sent-54, score-0.079]
</p><p>26 [8], it is straightforward to show that this problem is likely to be computationally intractable in the agnostic setting. [sent-56, score-0.046]
</p><p>27 [9] present a kernel-based algorithm for learning certain types of GLMs and SIMs in the agnostic setting. [sent-59, score-0.042]
</p><p>28 2  Algorithm 1 GLM- TRON Input: data (xi , yi ) m ∈ Rd × [0, 1], u : R → [0, 1], held-out data (xm+j , ym+j ) i=1 w1 := 0; for t = 1, 2, . [sent-61, score-0.096]
</p><p>29 do ht (x) := u(wt · x); m 1 wt+1 := wt + (yi − u(wt · xi ))xi ; m i=1 end for s Output: arg minht j=1 (ht (xm+j ) − ym+j )2  s j=1  analysis also apply to the case where Bd is the unit ball in some high (or inﬁnite)-dimensional kernel feature space. [sent-64, score-0.338]
</p><p>30 Our focus is on approximating the regression function well, as measured by the squared loss. [sent-67, score-0.105]
</p><p>31 , (xm , ym ), to be 1 err(h) = m  m 2  (h(xi ) − yi ) ; i=1  1 ε(h) = ˆ m  m  (h(xi ) − u(w · xi ))2 . [sent-73, score-0.247]
</p><p>32 ˆ Our algorithms work by iteratively constructing hypotheses ht of the form ht (x) = ut (wt ·x), where ut is a non-decreasing, 1-Lipschitz function, and wt is a linear predictor. [sent-75, score-0.565]
</p><p>33 3  The GLM- TRON algorithm  We begin with the simpler case, where the transfer function u is assumed to be known (e. [sent-77, score-0.129]
</p><p>34 We present a simple, parameter-free, perceptronlike algorithm, GLM- TRON (Alg. [sent-80, score-0.032]
</p><p>35 We note that the algorithm works for arbitrary non-decreasing, Lipschitz functions u, and thus covers most generalized linear models. [sent-82, score-0.041]
</p><p>36 To analyze the performance of the algorithm, we show that if we run the algorithm for sufﬁciently many iterations, one of the predictors ht obtained must be nearly-optimal, compared to the Bayesoptimal predictor. [sent-84, score-0.213]
</p><p>37 , (xm , ym ) are drawn independently from a distribution supported on Bd × [0, 1], such that E[y|x] = u(w · x), where w ≤ W , and u : R → [0, 1] is a known nondecreasing 1-Lipschitz function. [sent-89, score-0.135]
</p><p>38 Then for any δ ∈ (0, 1), the following holds with probability at least 1 − δ: there exists some iteration t < O(W m/ log(1/δ)) of GLM- TRON such that the hypothesis ht (x) = u(wt · x) satisﬁes max{ˆ(ht ), ε(ht )} ≤ O ε  3  W 2 log(m/δ) m  . [sent-90, score-0.238]
</p><p>39 Algorithm 2 S L I SOTRON Input: data (xi , yi ) m ∈ Rd × [0, 1], held-out data (xm+j , ym+j ) s i=1 j=1 w1 := 0; for t = 1, 2, . [sent-91, score-0.096]
</p><p>40 , (wt · xm , ym )) // Fit 1-d function along wt m 1 wt+1 := wt + (yi − ut (wt · xi ))xi m i=1 end for s Output: arg minht j=1 (ht (xm+j ) − ym+j )2 In particular, the theorem implies that some ht has small enough ε(ht ). [sent-97, score-0.667]
</p><p>41 Since ε(ht ) equals err(ht ) up to a constant, we can easily ﬁnd an appropriate ht by picking the one that has least err(ht ) on a held-out set. [sent-98, score-0.195]
</p><p>42 The main idea of the proof is showing that at each iteration, if ε(ht ) is not small, then the squared disˆ 2 2 tance wt+1 − w is substantially smaller than wt − w . [sent-99, score-0.134]
</p><p>43 Since the squared distance is bounded 2  below by 0, and w0 − w ≤ W 2 , there is an iteration (arrived at within reasonable time) such that the hypothesis ht at that iteration is highly accurate. [sent-100, score-0.303]
</p><p>44 Although the algorithm minimizes empirical squared error, we can bound the true error using a uniform convergence argument. [sent-101, score-0.098]
</p><p>45 2), which is applicable to the harder setting where the transfer function u is unknown, except for it being non-decreasing and 1-Lipschitz. [sent-104, score-0.111]
</p><p>46 The main difference between S L I SOTRON and GLM- TRON is that now the transfer function must also be learned, and the algorithm keeps track of a transfer function ut which changes from iteration to iteration. [sent-106, score-0.326]
</p><p>47 This key difference allows for an analysis that does not require a fresh sample each iteration. [sent-108, score-0.058]
</p><p>48 , (xm , ym ) are drawn independently from a distribution supported on Bd × [0, 1], such that E[y|x] = u(w · x), where w ≤ W , and u : R → [0, 1] is an unknown non-decreasing 1-Lipschitz function. [sent-118, score-0.135]
</p><p>49 (Dimension-dependent) With probability at least 1 − δ, there exists some iteration t < O  Wm d log(W m/δ)  1/3  of S L I SOTRON such that  max{ˆ(ht ), ε(ht )} ≤ O ε  dW 2 log(W m/δ) m  1/3  . [sent-120, score-0.026]
</p><p>50 1, one can easily ﬁnd ht which satisﬁes the theorem’s conditions, by running the S L I SOTRON algorithm for sufﬁciently many iterations, and choosing the hypothesis ht which minimizes err(ht ) on a held-out set. [sent-123, score-0.45]
</p><p>51 The algorithm minimizes empirical error and generalization bounds are obtained using a uniform convergence argument. [sent-124, score-0.059]
</p><p>52 1  Lipschitz isotonic regression  The S L I SOTRON algorithm (Alg. [sent-127, score-0.265]
</p><p>53 The goal is to ﬁnd the best ﬁt (least squared error) non-decreasing 1-Lipschitz function that ﬁts the data in one dimension. [sent-129, score-0.039]
</p><p>54 (zm , ym ) be such that zi ∈ R, yi ∈ [0, 1] and z1 ≤ z2 ≤ · · · ≤ zm . [sent-133, score-0.302]
</p><p>55 Algorithm Sketch: We deﬁne functions Gi (·), where Gi (s) is the minimum squared loss that can be attained if yi is ﬁxed to be s, and yi+1 , . [sent-141, score-0.135]
</p><p>56 ym are then chosen to be the best ﬁt 1-Lipschitz ˆ ˆ ˆ non-decreasing function to the points (zi , yi ), . [sent-144, score-0.231]
</p><p>57 ,ˆm ˆ y  1 1 (s − yi )2 + (ˆj − yj )2 y 2 2 j=i+1  (4)  subject to the constraints (where s = yi ), ˆ yj ≤ yj+1 ˆ ˆ yj+1 − yj ≤ zj+1 − zj ˆ ˆ  i ≤ j ≤ m − 1 (Monotonic) i ≤ j ≤ m − 1 (Lipschitz)  Furthermore, deﬁne: s∗ = mins Gi (s). [sent-154, score-0.305]
</p><p>58 The functions Gi are piecewise quadratic, differentiable i everywhere and strictly convex, a fact we prove in full paper [11]. [sent-155, score-0.113]
</p><p>59 Thus, Gi is minimized at s∗ i and it is strictly increasing on both sides of s∗ . [sent-156, score-0.051]
</p><p>60 Note that Gm (s) = (1/2)(s − ym )2 and hence i is piecewise quadratic, differentiable everywhere and strictly convex. [sent-157, score-0.248]
</p><p>61 i Since s∗ minimizes G1 (s), which is the same as the objective of (1), start with y1 = s∗ , and then ˆ 1 1 successively chose values for yi to be as close to s∗ as possible without violating the Lipschitz ˆ i or monotonicity constraints. [sent-162, score-0.18]
</p><p>62 This will produce an assignment for yi which achieves loss equal to ˆ G1 (s∗ ) and hence is optimal. [sent-163, score-0.096]
</p><p>63 m i We design a special data structure, called notable red-black trees, for representing piecewise linear, continuous, strictly increasing functions. [sent-168, score-0.112]
</p><p>64 We initialize such a tree T to represent Gm (s) = s − ym . [sent-169, score-0.135]
</p><p>65 Move the left half of the piecewise linear function Gi by δi−1 (Fig. [sent-177, score-0.062]
</p><p>66 Finally, we add the linear function s − yi−1 to every interval, to get Gi−1 , which is again piecewise linear, continuous and strictly increasing. [sent-179, score-0.097]
</p><p>67 To prevent a large number of such notes accumulating at any given node we show that these notes satisfy certain commutative and additive relations, thus requiring us to keep track of no more than 2 notes at any given node. [sent-182, score-0.072]
</p><p>68 The second one demonstrates the advantage of using S L I SOTRON over standard regression techniques, in the sense that S L I SOTRON can learn any monotonic Lipschitz function. [sent-188, score-0.158]
</p><p>69 All errors are reported in terms of average root mean squared error (RMSE) using 10 fold cross validation along with the standard deviation. [sent-190, score-0.142]
</p><p>70 1  Synthetic Experiments  Although, the theoretical guarantees for Isotron are under the assumption that we get a fresh sample each round, one may still attempt to run Isotron on the same sample each iteration and evaluate the 6  1 1 0. [sent-192, score-0.085]
</p><p>71 004  (b) Synthetic Experiment 2  Figure 2: (a) The ﬁgure shows the transfer functions as predicted by S L I SOTRON and Isotron. [sent-236, score-0.13]
</p><p>72 The table shows the average RMSE using 10 fold cross validation. [sent-237, score-0.105]
</p><p>73 The ∆ column shows the average difference between the RMSE values of the two algorithms across the folds. [sent-238, score-0.055]
</p><p>74 (b) The ﬁgure shows the transfer function as predicted by S L I SOTRON. [sent-239, score-0.13]
</p><p>75 Table shows the average RMSE using 10 fold cross validation for S L I SOTRON and Logistic Regression. [sent-240, score-0.103]
</p><p>76 The ∆ column shows the average difference between the RMSE values of the two algorithms across folds. [sent-241, score-0.055]
</p><p>77 Then, the main difference between S L I SOTRON and Isotron is that while S L I SOTRON ﬁts the best Lipschitz monotonic function using LIR each iteration, Isotron merely ﬁnds the best monotonic ﬁt using PAV. [sent-243, score-0.204]
</p><p>78 Our ﬁrst synthetic dataset is the following: The dataset is of size m = 1500 in d = 500 dimensions. [sent-247, score-0.056]
</p><p>79 , 0) and the transfer function is u(z) = (1 + z)/2. [sent-253, score-0.111]
</p><p>80 Figure 2(a) shows the transfer functions as predicted by the two algorithms, and the table below the plot shows the average RMSE using 10 fold cross validation. [sent-256, score-0.235]
</p><p>81 The ∆ column shows the average difference between the RMSE values of the two algorithms across the folds. [sent-257, score-0.055]
</p><p>82 A principle advantage of S L I SOTRON over standard regression techniques is that it is not necessary to know the transfer function in advance. [sent-258, score-0.177]
</p><p>83 We chose a random direction as the “true” w and used a piecewise linear function as the “true” u. [sent-261, score-0.079]
</p><p>84 2(b) shows the actual transfer function as predicted by S L I SOTRON, which is essentially the function we used. [sent-267, score-0.13]
</p><p>85 The table below the ﬁgure shows the performance comparison between S L I SOTRON and logistic regression. [sent-268, score-0.072]
</p><p>86 The SIM algorithm works by iteratively ﬁxing the direction w and ﬁnding the best transfer function u, and then ﬁxing u and 7  optimizing w via gradient descent. [sent-274, score-0.146]
</p><p>87 For each of the algorithms we performed 10-fold cross validation, using 1 fold each time as the test set, and we report averaged results across the folds. [sent-275, score-0.107]
</p><p>88 The ﬁrst column shows the mean Y value (with standard deviation) of the dataset for comparison. [sent-277, score-0.032]
</p><p>89 Table 2 shows the average difference between RMSE values of S L I SOTRON and the other algorithms across the folds. [sent-278, score-0.038]
</p><p>90 The results suggest that the performance of S L I SOTRON (and even Isotron) is comparable to other regression techniques and in many cases also slightly better. [sent-280, score-0.066]
</p><p>91 The performance of GLM- TRON is similar to standard implementations of logistic regression on these datasets. [sent-281, score-0.122]
</p><p>92 It is also illustrative to see how the transfer functions found by S L I SOTRON and Isotron compare. [sent-283, score-0.111]
</p><p>93 In Figure 3, we plot the transfer functions for concrete and communities. [sent-284, score-0.161]
</p><p>94 We also observe that concrete is the only dataset where S L I SOTRON performs noticeably better than logistic regression, and the transfer function is indeed somewhat far from the logistic function. [sent-286, score-0.308]
</p><p>95 ¯ Table 1: Average RMSE values using 10 fold cross validation. [sent-287, score-0.089]
</p><p>96 dataset communities concrete housing parkinsons winequality  ¯ Y 0. [sent-289, score-0.224]
</p><p>97 The values reported are the average difference between RMSE values of the algorithm and S L I SOTRON across the folds. [sent-359, score-0.056]
</p><p>98 dataset communities concrete housing parkinsons winequality  GLM-t 0. [sent-361, score-0.224]
</p><p>99 8  1  (b) communities  Figure 3: The transfer function u as predicted by S L I SOTRON (blue) and Isotron (red) for the concrete and communities datasets. [sent-445, score-0.278]
</p><p>100 Efﬁcient learning of generalized linear and single index models with isotonic regression. [sent-517, score-0.226]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sotron', 0.657), ('isotron', 0.449), ('ht', 0.195), ('gi', 0.188), ('isotonic', 0.181), ('lipschitz', 0.156), ('ym', 0.135), ('tron', 0.129), ('glms', 0.121), ('transfer', 0.111), ('err', 0.102), ('rmse', 0.1), ('lir', 0.096), ('sims', 0.096), ('yi', 0.096), ('wt', 0.095), ('monotonic', 0.092), ('kalai', 0.071), ('regression', 0.066), ('slisotron', 0.064), ('piecewise', 0.062), ('bd', 0.059), ('xm', 0.059), ('logistic', 0.056), ('sim', 0.056), ('fold', 0.053), ('concrete', 0.05), ('communities', 0.049), ('iso', 0.048), ('parkinsons', 0.042), ('zi', 0.04), ('ut', 0.04), ('monotonicity', 0.039), ('gm', 0.039), ('squared', 0.039), ('fresh', 0.038), ('housing', 0.036), ('slope', 0.036), ('cross', 0.036), ('strictly', 0.035), ('yj', 0.032), ('minht', 0.032), ('perceptronlike', 0.032), ('winequality', 0.032), ('yeganova', 0.032), ('zm', 0.031), ('bayesoptimal', 0.028), ('pav', 0.028), ('iteration', 0.026), ('synthetic', 0.026), ('minimizes', 0.025), ('sastry', 0.024), ('kanade', 0.024), ('agnostic', 0.024), ('notes', 0.024), ('appendix', 0.023), ('generalized', 0.023), ('index', 0.022), ('computationally', 0.022), ('guarantees', 0.021), ('wm', 0.021), ('violating', 0.02), ('somewhat', 0.02), ('difference', 0.02), ('microsoft', 0.02), ('predicted', 0.019), ('tting', 0.018), ('log', 0.018), ('across', 0.018), ('algorithm', 0.018), ('provable', 0.017), ('obey', 0.017), ('zj', 0.017), ('hypothesis', 0.017), ('direction', 0.017), ('column', 0.017), ('shamir', 0.017), ('empirical', 0.016), ('ts', 0.016), ('nding', 0.016), ('sides', 0.016), ('xing', 0.016), ('xi', 0.016), ('everywhere', 0.016), ('table', 0.016), ('nds', 0.016), ('heuristics', 0.016), ('provably', 0.016), ('uci', 0.015), ('dataset', 0.015), ('notable', 0.015), ('kakade', 0.015), ('hall', 0.014), ('validation', 0.014), ('snowbird', 0.014), ('perceptrons', 0.014), ('cryptographic', 0.014), ('interleaves', 0.014), ('seas', 0.014), ('horowitz', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="77-tfidf-1" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>2 0.061562695 <a title="77-tfidf-2" href="./nips-2011-Transfer_from_Multiple_MDPs.html">291 nips-2011-Transfer from Multiple MDPs</a></p>
<p>Author: Alessandro Lazaric, Marcello Restelli</p><p>Abstract: Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.</p><p>3 0.060237244 <a title="77-tfidf-3" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>Author: Elad Hazan, Satyen Kale</p><p>Abstract: We present an efﬁcient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss deﬁned in [AR09], which is parameterized by a scalar α. We prove that the regret of N EWTRON is O(log T ) when α is a constant that does not vary with horizon T , and at most O(T 2/3 ) if α is allowed to increase to inﬁnity √ with T . For α = O(log T ), the regret is bounded by O( T ), thus solving the open problem of [KSST08, AR09]. Our algorithm is based on a novel application of the online Newton method [HAK07]. We test our algorithm and show it to perform well in experiments, even when α is a small constant. 1</p><p>4 0.058820337 <a title="77-tfidf-4" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>Author: Yee W. Teh, Vinayak Rao</p><p>Abstract: Renewal processes are generalizations of the Poisson process on the real line whose intervals are drawn i.i.d. from some distribution. Modulated renewal processes allow these interevent distributions to vary with time, allowing the introduction of nonstationarity. In this work, we take a nonparametric Bayesian approach, modelling this nonstationarity with a Gaussian process. Our approach is based on the idea of uniformization, which allows us to draw exact samples from an otherwise intractable distribution. We develop a novel and efﬁcient MCMC sampler for posterior inference. In our experiments, we test these on a number of synthetic and real datasets. 1</p><p>5 0.052310135 <a title="77-tfidf-5" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>Author: Lei Yuan, Jun Liu, Jieping Ye</p><p>Abstract: The group Lasso is an extension of the Lasso for feature selection on (predeﬁned) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efﬁcient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efﬁcient than existing state-of-the-art algorithms. 1</p><p>6 0.049728218 <a title="77-tfidf-6" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>7 0.047671437 <a title="77-tfidf-7" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>8 0.046256315 <a title="77-tfidf-8" href="./nips-2011-Distributed_Delayed_Stochastic_Optimization.html">72 nips-2011-Distributed Delayed Stochastic Optimization</a></p>
<p>9 0.044853862 <a title="77-tfidf-9" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>10 0.044597913 <a title="77-tfidf-10" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<p>11 0.044432372 <a title="77-tfidf-11" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>12 0.042956609 <a title="77-tfidf-12" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>13 0.041618817 <a title="77-tfidf-13" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>14 0.041005965 <a title="77-tfidf-14" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>15 0.039135925 <a title="77-tfidf-15" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>16 0.035994973 <a title="77-tfidf-16" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>17 0.035929587 <a title="77-tfidf-17" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>18 0.035713676 <a title="77-tfidf-18" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>19 0.034452964 <a title="77-tfidf-19" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>20 0.032977063 <a title="77-tfidf-20" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, -0.025), (2, -0.014), (3, -0.038), (4, 0.01), (5, 0.006), (6, 0.011), (7, -0.049), (8, -0.052), (9, 0.008), (10, -0.006), (11, -0.041), (12, -0.009), (13, 0.014), (14, -0.005), (15, 0.013), (16, -0.012), (17, 0.004), (18, 0.046), (19, -0.019), (20, -0.075), (21, 0.028), (22, -0.043), (23, 0.027), (24, 0.017), (25, -0.021), (26, 0.017), (27, -0.043), (28, 0.015), (29, -0.024), (30, 0.046), (31, 0.009), (32, -0.02), (33, 0.029), (34, -0.016), (35, 0.002), (36, 0.096), (37, -0.022), (38, 0.045), (39, -0.023), (40, -0.005), (41, -0.041), (42, 0.076), (43, -0.125), (44, 0.107), (45, 0.033), (46, 0.071), (47, -0.121), (48, -0.04), (49, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87351263 <a title="77-lsi-1" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>2 0.58647394 <a title="77-lsi-2" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>Author: Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, Masashi Sugiyama</p><p>Abstract: Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test. However, since density-ratio functions often possess high ﬂuctuation, divergence estimation is still a challenging task in practice. In this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overﬁts even with complex models. Through experiments, we demonstrate the usefulness of the proposed approach. 1</p><p>3 0.57420325 <a title="77-lsi-3" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>Author: Jakob H. Macke, Iain Murray, Peter E. Latham</p><p>Abstract: Maximum entropy models have become popular statistical models in neuroscience and other areas in biology, and can be useful tools for obtaining estimates of mutual information in biological systems. However, maximum entropy models ﬁt to small data sets can be subject to sampling bias; i.e. the true entropy of the data can be severely underestimated. Here we study the sampling properties of estimates of the entropy obtained from maximum entropy models. We show that if the data is generated by a distribution that lies in the model class, the bias is equal to the number of parameters divided by twice the number of observations. However, in practice, the true distribution is usually outside the model class, and we show here that this misspeciﬁcation can lead to much larger bias. We provide a perturbative approximation of the maximally expected bias when the true model is out of model class, and we illustrate our results using numerical simulations of an Ising model; i.e. the second-order maximum entropy distribution on binary data. 1</p><p>4 0.54571241 <a title="77-lsi-4" href="./nips-2011-t-divergence_Based_Approximate_Inference.html">306 nips-2011-t-divergence Based Approximate Inference</a></p>
<p>Author: Nan Ding, Yuan Qi, S.v.n. Vishwanathan</p><p>Abstract: Approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions. We extend the idea of approximate inference to the t-exponential family by deﬁning a new t-divergence. This divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy. We illustrate our approach on the Bayes Point Machine with a Student’s t-prior. 1</p><p>5 0.49363035 <a title="77-lsi-5" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>Author: Daniel Hernández-lobato, Jose M. Hernández-lobato, Pierre Dupont</p><p>Abstract: Multi-class Gaussian Process Classiﬁers (MGPCs) are often affected by overﬁtting problems when labeling errors occur far from the decision boundaries. To prevent this, we investigate a robust MGPC (RMGPC) which considers labeling errors independently of their distance to the decision boundaries. Expectation propagation is used for approximate inference. Experiments with several datasets in which noise is injected in the labels illustrate the beneﬁts of RMGPC. This method performs better than other Gaussian process alternatives based on considering latent Gaussian noise or heavy-tailed processes. When no noise is injected in the labels, RMGPC still performs equal or better than the other methods. Finally, we show how RMGPC can be used for successfully identifying data instances which are difﬁcult to classify correctly in practice. 1</p><p>6 0.48672304 <a title="77-lsi-6" href="./nips-2011-Differentially_Private_M-Estimators.html">69 nips-2011-Differentially Private M-Estimators</a></p>
<p>7 0.48557785 <a title="77-lsi-7" href="./nips-2011-Thinning_Measurement_Models_and_Questionnaire_Design.html">288 nips-2011-Thinning Measurement Models and Questionnaire Design</a></p>
<p>8 0.48431179 <a title="77-lsi-8" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>9 0.46650389 <a title="77-lsi-9" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>10 0.44172966 <a title="77-lsi-10" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>11 0.43089995 <a title="77-lsi-11" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>12 0.42900079 <a title="77-lsi-12" href="./nips-2011-An_Exact_Algorithm_for_F-Measure_Maximization.html">33 nips-2011-An Exact Algorithm for F-Measure Maximization</a></p>
<p>13 0.42846176 <a title="77-lsi-13" href="./nips-2011-Unifying_Non-Maximum_Likelihood_Learning_Objectives_with_Minimum_KL_Contraction.html">295 nips-2011-Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction</a></p>
<p>14 0.42591602 <a title="77-lsi-14" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>15 0.41780594 <a title="77-lsi-15" href="./nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</a></p>
<p>16 0.3884894 <a title="77-lsi-16" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>17 0.38334161 <a title="77-lsi-17" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>18 0.37308401 <a title="77-lsi-18" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>19 0.37286967 <a title="77-lsi-19" href="./nips-2011-Ranking_annotators_for_crowdsourced_labeling_tasks.html">232 nips-2011-Ranking annotators for crowdsourced labeling tasks</a></p>
<p>20 0.37194902 <a title="77-lsi-20" href="./nips-2011-Bayesian_Bias_Mitigation_for_Crowdsourcing.html">42 nips-2011-Bayesian Bias Mitigation for Crowdsourcing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.022), (4, 0.027), (20, 0.021), (26, 0.02), (31, 0.072), (33, 0.011), (43, 0.089), (45, 0.118), (57, 0.051), (65, 0.362), (74, 0.037), (83, 0.039), (99, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95068341 <a title="77-lda-1" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>Author: Maneesh Bhand, Ritvik Mudur, Bipin Suresh, Andrew Saxe, Andrew Y. Ng</p><p>Abstract: The efﬁcient coding hypothesis holds that neural receptive ﬁelds are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism’s lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive ﬁeld properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive ﬁeld properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive ﬁeld plasticity during an organism’s lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, ﬁrst proposed by Mountcastle [1], that a qualitatively similar learning algorithm acts throughout primary sensory cortices. 1</p><p>2 0.93670684 <a title="77-lda-2" href="./nips-2011-Extracting_Speaker-Specific_Information_with_a_Regularized_Siamese_Deep_Network.html">93 nips-2011-Extracting Speaker-Specific Information with a Regularized Siamese Deep Network</a></p>
<p>Author: Ke Chen, Ahmad Salman</p><p>Abstract: Speech conveys different yet mixed information ranging from linguistic to speaker-speciﬁc components, and each of them should be exclusively used in a speciﬁc task. However, it is extremely difﬁcult to extract a speciﬁc information component given the fact that nearly all existing acoustic representations carry all types of speech information. Thus, the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information. In this paper, we present a deep neural architecture to extract speaker-speciﬁc information from MFCCs. As a result, a multi-objective loss function is proposed for learning speaker-speciﬁc characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss. With LDC benchmark corpora and a Chinese speech corpus, we demonstrate that a resultant speaker-speciﬁc representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms MFCCs and other state-of-the-art techniques in speaker recognition. We discuss relevant issues and relate our approach to previous work. 1</p><p>3 0.76375848 <a title="77-lda-3" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>Author: Vikas Sindhwani, Aurelie C. Lozano</p><p>Abstract: We consider regularized risk minimization in a large dictionary of Reproducing kernel Hilbert Spaces (RKHSs) over which the target function has a sparse representation. This setting, commonly referred to as Sparse Multiple Kernel Learning (MKL), may be viewed as the non-parametric extension of group sparsity in linear models. While the two dominant algorithmic strands of sparse learning, namely convex relaxations using l1 norm (e.g., Lasso) and greedy methods (e.g., OMP), have both been rigorously extended for group sparsity, the sparse MKL literature has so far mainly adopted the former with mild empirical success. In this paper, we close this gap by proposing a Group-OMP based framework for sparse MKL. Unlike l1 -MKL, our approach decouples the sparsity regularizer (via a direct l0 constraint) from the smoothness regularizer (via RKHS norms), which leads to better empirical performance and a simpler optimization procedure that only requires a black-box single-kernel solver. The algorithmic development and empirical studies are complemented by theoretical analyses in terms of Rademacher generalization bounds and sparse recovery conditions analogous to those for OMP [27] and Group-OMP [16]. 1</p><p>4 0.73968226 <a title="77-lda-4" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>Author: Quoc V. Le, Alexandre Karpenko, Jiquan Ngiam, Andrew Y. Ng</p><p>Abstract: Independent Components Analysis (ICA) and its variants have been successfully used for unsupervised feature learning. However, standard ICA requires an orthonoramlity constraint to be enforced, which makes it difﬁcult to learn overcomplete features. In addition, ICA is sensitive to whitening. These properties make it challenging to scale ICA to high dimensional data. In this paper, we propose a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse features even on unwhitened data. Our formulation reveals formal connections between ICA and sparse autoencoders, which have previously been observed only empirically. Our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers. We show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks. Using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks. We achieve state-of-the-art test accuracies on the STL-10 and Hollywood2 datasets. 1</p><p>same-paper 5 0.73715037 <a title="77-lda-5" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>Author: Sham M. Kakade, Varun Kanade, Ohad Shamir, Adam Kalai</p><p>Abstract: Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) provided the ﬁrst provably efﬁcient method, the Isotron algorithm, for learning SIMs and GLMs, under the assumption that the data is in fact generated under a GLM and under certain monotonicity and Lipschitz (bounded slope) constraints. The Isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (ﬁtting a one-dimensional non-decreasing function). However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efﬁcient. We modify the isotonic regression step in Isotron to ﬁt a Lipschitz monotonic function, and also provide an efﬁcient O(n log(n)) algorithm for this step, improving upon the previous O(n2 ) algorithm. We provide a brief empirical study, demonstrating the feasibility of our algorithms in practice. 1</p><p>6 0.72335106 <a title="77-lda-6" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>7 0.65888989 <a title="77-lda-7" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>8 0.64260763 <a title="77-lda-8" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>9 0.56398696 <a title="77-lda-9" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>10 0.54984719 <a title="77-lda-10" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>11 0.54203153 <a title="77-lda-11" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>12 0.53185356 <a title="77-lda-12" href="./nips-2011-Dynamic_Pooling_and_Unfolding_Recursive_Autoencoders_for_Paraphrase_Detection.html">74 nips-2011-Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</a></p>
<p>13 0.53033978 <a title="77-lda-13" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>14 0.52464586 <a title="77-lda-14" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>15 0.50945431 <a title="77-lda-15" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>16 0.5059182 <a title="77-lda-16" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>17 0.50582451 <a title="77-lda-17" href="./nips-2011-The_Local_Rademacher_Complexity_of_Lp-Norm_Multiple_Kernel_Learning.html">286 nips-2011-The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning</a></p>
<p>18 0.50157398 <a title="77-lda-18" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>19 0.49624151 <a title="77-lda-19" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>20 0.49360502 <a title="77-lda-20" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
