<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-149" href="#">nips2011-149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</h1>
<br/><p>Source: <a title="nips-2011-149-pdf" href="http://papers.nips.cc/paper/4400-learning-sparse-representations-of-high-dimensional-data-on-large-scale-dictionaries.pdf">pdf</a></p><p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>Reference: <a title="nips-2011-149-reference" href="../nips2011_reference/nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. [sent-3, score-0.268]
</p><p>2 But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. [sent-4, score-0.259]
</p><p>3 First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. [sent-6, score-0.942]
</p><p>4 Second, we study the properties of random projections in the context of learning sparse representations. [sent-7, score-0.337]
</p><p>5 Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. [sent-8, score-1.046]
</p><p>6 Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. [sent-9, score-0.359]
</p><p>7 1  Introduction  Consider approximating a p-dimensional data point x by a linear combination x ≈ Bw of m (possibly linearly dependent) codewords in a dictionary B = [b1 , b2 , . [sent-10, score-0.737]
</p><p>8 , x is approximated as a weighted sum of only a few codewords in the dictionary, has recently attracted much attention [1]. [sent-16, score-0.478]
</p><p>9 As a further reﬁnement, when there are many data points xj , the dictionary B can be optimized to make the representations wj as sparse as possible. [sent-17, score-0.63]
</p><p>10 , xn ] ∈ Rp×n , we want to learn a dictionary B = [b1 , b2 , . [sent-22, score-0.259]
</p><p>11 , bm ] ∈ Rp×m and sparse representation weights W = [w1 , w2 , . [sent-25, score-0.267]
</p><p>12 , wn ] ∈ Rm×n so that each data point xj is well approximated by Bwj with wj a sparse vector: 1 min X − BW 2 + λ W 1 F B,W 2 (1) s. [sent-28, score-0.32]
</p><p>13 Fourier, wavelet, DCT), problem (1) assumes minimal prior knowledge and uses sparsity as a cue to learn a dictionary adapted to the data. [sent-41, score-0.288]
</p><p>14 In many other approaches (including [2–4]), although the codewords in B are cleverly chosen, the new representation w is simply a linear mapping of x, e. [sent-45, score-0.565]
</p><p>15 As a ﬁnal point, we note that the human visual cortex uses similar mechanisms to encode visual scenes [7] and sparse representation has exhibited superior performance on difﬁcult computer vision problems such as face [8] and object [9] recognition. [sent-49, score-0.287]
</p><p>16 First (§2), we explore dictionary screening [13, 14], to select a subset of codewords to use in each Lasso optimization. [sent-57, score-1.037]
</p><p>17 We derive two new screening tests that are signiﬁcantly better than existing tests when the data points and codewords are highly correlated, a typical scenario in sparse representation applications [15]. [sent-58, score-1.359]
</p><p>18 We also provide simple geometric intuition for guiding the derivation of screening tests. [sent-59, score-0.329]
</p><p>19 Second (§3), we examine projecting data onto a lower dimensional space so that we can control information ﬂow in our hierarchical framework and solve sparse representations with smaller p. [sent-60, score-0.359]
</p><p>20 We identify an important property of the data that’s implicitly assumed in sparse representation problems (scale indifference) and study how random projection preserves this property. [sent-61, score-0.35]
</p><p>21 Finally (§4), we develop a framework for learning a hierarchical dictionary (similar in spirit to [17] and DBN [18]). [sent-63, score-0.38]
</p><p>22 To do so we exploit our results on screening and random projection and impose a zero-tree like structured sparsity constraint on the representation. [sent-64, score-0.488]
</p><p>23 2  Reducing the Dictionary By Screening  In this section we assume that all data points and codewords are normalized: xj 2 = bi 2 = 1, 1 ≤ j ≤ n, 1 ≤ i ≤ m (we discuss the implications of this assumption in §3). [sent-72, score-0.927]
</p><p>24 Each subproblem is then of the form: m  m  1 x− wi bi 2 i=1  min  w1 ,w2 ,. [sent-79, score-0.426]
</p><p>25 +λ  (2)  i=1  To address the challenge of solving (2) for large m, we ﬁrst explore simple screening tests that identify and discard codewords bi guaranteed to have optimal solution wi = 0. [sent-83, score-1.462]
</p><p>26 El Ghaoui’s SAFE ˜ rule [13] is an example of a simple screening test. [sent-84, score-0.329]
</p><p>27 We introduce some simple geometric intuition for screening and use this to derive new tests that are signiﬁcantly better than existing tests for the type of problems of interest here. [sent-85, score-0.599]
</p><p>28 λ2 x 2 1 x 2− θ− 2 2 2 λ 2 T |θ bi | ≤ 1 ∀i = 1, 2, . [sent-88, score-0.339]
</p><p>29 , wm ] and the optimal solution of the dual problem θ ˜ ˜ ˜ m  ˜ wi bi + λθ, ˜  x=  ˜T θ bi ∈  i=1  {sign wi } ˜ [−1, 1]  if wi = 0, ˜ if wi = 0. [sent-95, score-1.062]
</p><p>30 Since x 2 = bi 2 = 1, x and all bi lie on the unit sphere S p−1 (Fig. [sent-97, score-0.785]
</p><p>31 (c) The solid red, dotted blue and solid magenta circles leading to sphere tests ST1/SAFE, ST2, ST3, respectively. [sent-117, score-0.362]
</p><p>32 By (4), if θ is not on P (bi ) or P (−bi ), then wi = 0 and we can safely discard bi from problem (2). [sent-125, score-0.492]
</p><p>33 ˜ Let λmax = maxi |xT bi | and b∗ ∈ {±bi }m be selected so that λmax = xT b∗ . [sent-126, score-0.339]
</p><p>34 ˜ ˜ These observations can be used for screening as follows. [sent-135, score-0.329]
</p><p>35 If we know that θ is within a region R, then we can discard those bi for which the tangent hyperplanes P (bi ) and P (−bi ) don’t intersect R, since by (4) the corresponding wi will be 0. [sent-136, score-0.526]
</p><p>36 , {θ : θ − q 2 ≤ r}, then one can discard all bi for which |qT bi | is smaller than a threshold determined by the common tangent hyperplanes of the spheres θ − q 2 = r and S p−1 . [sent-142, score-0.778]
</p><p>37 If the solution θ of (3) satisﬁes θ − q 2 ≤ r, then |qT bi | < (1 − r) ⇒ wi = 0. [sent-145, score-0.426]
</p><p>38 Plugging in q = x/λ and r = 1/λ − 1/λmax into Lemma 1 yields El Ghaoui’s SAFE rule: Sphere Test # 1 (ST1/SAFE):  If |xT bi | < λ − 1 + λ/λmax , then wi = 0. [sent-150, score-0.426]
</p><p>39 , when the codewords are very similar to the data points, a frequent situation in applications [15]. [sent-153, score-0.507]
</p><p>40 Plugging q = x/λmax and r = 2 1/λ2 − 1(λmax /λ − 1) into Lemma 1 yields our ﬁrst new test: max 3  Sphere Test # 2 (ST2):  If |xT bi | < λmax (1 − 2  1/λ2 − 1(λmax /λ − 1)), then wi = 0. [sent-166, score-0.534]
</p><p>41 ˜ max  Since ST2 and ST1/SAFE both test |xT bi | against thresholds, we can compare the tests by plotting their thresholds. [sent-167, score-0.582]
</p><p>42 When λmax > 3/2, if ST1/SAFE discards bi , then ST2 also discards bi . [sent-175, score-0.898]
</p><p>43 Finally, to use the closed ball constraint (5), we plug in q = x/λ − (λmax /λ − 1)b∗ and r = 1/λ2 − 1(λmax /λ − 1) into Lemma 1 to obtain a second new test: max Sphere Test # 3 (ST3): If |xT bi − (λmax − λ)bT bi | < λ(1 − ∗  1/λ2 − 1(λmax /λ − 1)), then wi = 0. [sent-176, score-1.041]
</p><p>44 Given any x, b∗ and λ, if ST2 discards bi , then ST3 also discards bi . [sent-180, score-0.898]
</p><p>45 The ﬁrst pass holds x, u, bi ∈ Rp in memory at once and computes u(i) = xT bi . [sent-185, score-0.678]
</p><p>46 The second pass holds u, b∗ , bi in memory at once, computes bT bi and executes the test. [sent-187, score-0.678]
</p><p>47 ∗  3  Random Projections of the Data  In §4 we develop a framework for learning a hierarchical dictionary and this involves the use of random data projections to control information ﬂow to the levels of the hierarchy. [sent-188, score-0.613]
</p><p>48 Here we lay some groundwork by studying basic properties of random projections in learning sparse representations. [sent-190, score-0.337]
</p><p>49 We ﬁrst revisit the normalization assumption xj 2 = bi 2 = 1, 1 ≤ j ≤ n, 1 ≤ i ≤ m in §2. [sent-191, score-0.387]
</p><p>50 The assumption that all codewords are normalized: bi 2 = 1, is necessary for (1) to be meaningful, otherwise we can increase the scale of bi and inversely scale the ith row of W to lower the loss. [sent-192, score-1.185]
</p><p>51 To see this, assume that the data {xj }n are samples from an underlying low dimensional smooth j=1 manifold X and that one desires a correspondence between codewords and local regions on X . [sent-194, score-0.507]
</p><p>52 Intuitively, SI means that X doesn’t contain points differing only in scale and it implies that points x1 , x2 from distinct regions on X will use different codewords in their representation. [sent-197, score-0.544]
</p><p>53 Since a random projection of the original data doesn’t preserve the normalization xj 2 = 1, it’s important for the random projection to preserve the SI property so that it is reasonable to renormalize the projected data. [sent-202, score-0.266]
</p><p>54 If X satisﬁes SI and has a κ-sparse representation using dictionary B, then the projected data T (X ) satisﬁes SI if (2κ − 1)M (TB) < 1, where M (·) is matrix mutual coherence. [sent-228, score-0.413]
</p><p>55 Let wj be the new representation of xj and µi = Txj − Bwj 2 be the length of the residual (j = 1, 2). [sent-233, score-0.245]
</p><p>56 Therefore the distances between the sparse representation weights reﬂect the original data point distances. [sent-236, score-0.296]
</p><p>57 4  Learning a Hierarchical Dictionary  Our hierarchical framework decomposes a large dictionary learning problem into a sequence of smaller hierarchically structured dictionary learning problems. [sent-238, score-0.61]
</p><p>58 High levels of the tree give course representations, deeper levels give more detailed representations, and the codewords at the leaves form the ﬁnal dictionary. [sent-240, score-0.528]
</p><p>59 The tree is grown top-down in l levels by reﬁning the dictionary at the previous level to give the dictionary at the next level. [sent-241, score-0.538]
</p><p>60 We also enforce a zero-tree constraint on the sparse representation weights so that the zero weights in the previous level will force the corresponding weights in the next level to be zero. [sent-243, score-0.514]
</p><p>61 At each stage we combine this zero-tree constraint with our new screening tests to reduce the size of Lasso problems that must be solved. [sent-244, score-0.533]
</p><p>62 At level k we learn a dictionary Bk ∈ Rdk ×mk and weights Wk ∈ Rmk ×n by solving a small sparse representation problem similar to (1): min  Bk ,Wk  s. [sent-247, score-0.554]
</p><p>63 1 Tk X − Bk Wk 2 (k) 2 2  bi  ≤ 1,  2 F  + λk Wk  1  (9)  ∀i = 1, 2, . [sent-249, score-0.339]
</p><p>64 (k)  Here bi is the ith column of matrix Bk and mk is assumed to be a multiple of mk−1 , so the number of codewords mk increases with k. [sent-253, score-1.216]
</p><p>65 The ith group has mk /mk−1 weights: {wj (rmk−1 + i), 0 ≤ r < mk /mk−1 }, (k−1)  and has weight wj (i) as its parent weight. [sent-260, score-0.509]
</p><p>66 So for every level k ≥ 2, data point j (1 ≤ j ≤ n), group i (k) (1 ≤ i ≤ mk−1 ), and weight wj (rmk−1 + i) (0 ≤ r < mk /mk−1 ), we enforce: (k−1)  wj  (i) = 0  ⇒  (k)  wj (rmk−1 + i) = 0. [sent-262, score-0.572]
</p><p>67 In addition, (10) means that the weights of the previous layer select a small subset of codewords to k enter the Lasso optimization. [sent-264, score-0.652]
</p><p>68 When solving for wj , (10) reduces the number of codewords from (k−1)  (k−1)  mk to (mk /mk−1 ) wj is sparse. [sent-265, score-0.883]
</p><p>69 Thus the screening 0 , a considerable reduction since wj rules in §2 and the imposed screening rule (10) work together to reduce the effective dictionary size. [sent-266, score-1.038]
</p><p>70 The tree structure in the weights introduces a similar hierarchical tree structure in the dictionaries (k) (k−1) {Bk }l : the codewords {brmk−1 +i , 0 ≤ r < mk /mk−1 } are the children of codeword bi . [sent-267, score-1.357]
</p><p>71 When k > 1, the mk codewords in layer k are naturally divided into mk−1 groups, so we can solve Bk by optimizing each group sequentially. [sent-269, score-0.79]
</p><p>72 , mk−1 , let B = [brmk−1 +i ]r=0 k−1 denote the codewords in group i. [sent-274, score-0.478]
</p><p>73 Denote the remaining codewords and weights by B and W . [sent-280, score-0.525]
</p><p>74 5  Experiments  We tested our framework on: (a) the COIL rotational image data set [23], (b) the MNIST digit classiﬁcation data set [24], and (c) the extended Yale B face recognition data set [25] [26]. [sent-290, score-0.381]
</p><p>75 We ran the traditional sparse representation algorithm to compare the three screening tests in §2. [sent-297, score-0.741]
</p><p>76 2(c), ST3 discards a larger fraction of codewords than ST2 and ST2 discards a larger fraction than ST1/SAFE. [sent-300, score-0.698]
</p><p>77 5), helps the second layer discard more codewords when the tree constraint (10) is imposed. [sent-307, score-0.79]
</p><p>78 2(b) illustrates this constraint: the 16 second layer codewords are organized in 4 groups of 4 (only 2 groups shown). [sent-309, score-0.667]
</p><p>79 This imposed constraint discards many more codewords in the screening stage than any of the three tests in §2. [sent-311, score-1.161]
</p><p>80 2(b) are the actual values in 6  Learning non−hierarchical sparse representation Average % of codewords discarded  0(1& 0,1&  Average percentage of discarded codewords in the prescreening. [sent-315, score-1.308]
</p><p>81 ST3, original data ST3, projected data ST2, original data ST2, projected data ST1/SAFE, original data ST1/SAFE, projected data  100  80  80  021& 0/1&  60  Use our new bound on the origianl data Use our new bound on the projected data Use El Ghaoui et al. [sent-316, score-0.5]
</p><p>82 8  1  Learning the second layer sparse representation Average percentage of discarded codewords in the prescreening. [sent-326, score-0.907]
</p><p>83 Average % of codewords discarded  100 80  80 (10) + ST3  60 60  Use constraint (13) and our new bound ST3 only Use our new bound (10) + ST2 ST2 only Use constraint (13) and El Ghaoui et al. [sent-327, score-0.666]
</p><p>84 8%) # of random projections (percentage of image size) to use  Average encoding time (ms)  Classification accuracy (%) on testing set  97  Recognition rate (%) on testing set  tion. [sent-345, score-0.374]
</p><p>85 (c): Comparison of the three screening tests for sparse representation. [sent-346, score-0.597]
</p><p>86 (d): Screening performance in the second layer of our hierarchical framework using combinations of screening criteria. [sent-347, score-0.606]
</p><p>87 The imposed constraint (10) helps to discard signiﬁcantly more codewords when λ is small. [sent-348, score-0.653]
</p><p>88 80 60  Traditional sparse representation Our hierarchical framework Our framework with PCA projections Linear classifier  40 20  0 32(0. [sent-349, score-0.672]
</p><p>89 8%) # of random projections (percentage of image size) to use  Figure 3: Left: MNIST: The tradeoff between classiﬁcation accuracy and average encoding time for various sparse representation methods. [sent-353, score-0.524]
</p><p>90 The performance of traditional sparse representation is consistent with [9]. [sent-356, score-0.277]
</p><p>91 Traditional sparse representation has the best accuracy and is very close to a similar method SRC in [8] (SRC’s recognition rate is cited from [8] but data on encoding time is not available). [sent-358, score-0.345]
</p><p>92 Using PCA projections in our framework yields worse performance since these projections do not spread information across the layers. [sent-360, score-0.459]
</p><p>93 The sparse representation gives a multiresolution representation of the rotational pattern: the ﬁrst layer encodes rough orientation and the second layer reﬁnes it. [sent-364, score-0.623]
</p><p>94 We ran the traditional sparse representation algorithm for dictionary size m ∈ {64, 128, 192, 256} and λ ∈ Λ = {0. [sent-372, score-0.507]
</p><p>95 The plot shows that compared to the traditional sparse representation, our hierarchical framework achieves roughly a 1% accuracy improvement given the same encoding time and a roughly 2X speedup given the same accuracy. [sent-387, score-0.398]
</p><p>96 In this experiment we start with the random projected data (p ∈ {32, 64, 128, 256} random projections of the original 192x128 data) and use this data as follows: (a) learn a traditional non-hierarchical sparse representation, (b) our framework, i. [sent-391, score-0.548]
</p><p>97 , sample the data in two stages using orthogonal random projections and learn a 2 layer hierarchical sparse representation, (c) use PCA projections to replace random projections in (b), (d) directly apply a linear classiﬁer without ﬁrst learning a sparse representation. [sent-393, score-1.162]
</p><p>98 The PCA variant of our framework has worse performance because the ﬁrst 3 8 p projections contain too much information, leaving the second layer too little information (which also drags down the speed for lack of sparsity and structure). [sent-402, score-0.411]
</p><p>99 As expected, λmax is large, a situation that favors our new screening tests (ST2, ST3). [sent-410, score-0.464]
</p><p>100 We have shown that under certain conditions, random projection preserves the scale indifference (SI) property with high probability, thus providing an opportunity to learn informative sparse representations with data fewer dimensions. [sent-412, score-0.401]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('codewords', 0.478), ('bi', 0.339), ('screening', 0.329), ('dictionary', 0.23), ('projections', 0.204), ('mk', 0.185), ('tests', 0.135), ('sparse', 0.133), ('layer', 0.127), ('discards', 0.11), ('wj', 0.11), ('max', 0.108), ('sphere', 0.107), ('hierarchical', 0.099), ('bk', 0.093), ('rmk', 0.091), ('si', 0.091), ('ghaoui', 0.09), ('wi', 0.087), ('representation', 0.087), ('tk', 0.078), ('safe', 0.071), ('el', 0.071), ('constraint', 0.069), ('face', 0.067), ('projected', 0.067), ('discard', 0.066), ('ball', 0.065), ('submanifold', 0.063), ('indifference', 0.062), ('rotational', 0.062), ('projection', 0.061), ('tx', 0.06), ('layers', 0.059), ('dictionaries', 0.059), ('encoding', 0.058), ('doesn', 0.057), ('traditional', 0.057), ('pca', 0.055), ('src', 0.055), ('framework', 0.051), ('codeword', 0.05), ('tree', 0.05), ('discarded', 0.05), ('xj', 0.048), ('lemma', 0.048), ('classifier', 0.047), ('mq', 0.047), ('riemannian', 0.047), ('representations', 0.047), ('weights', 0.047), ('wk', 0.046), ('mnist', 0.046), ('solid', 0.045), ('lasso', 0.044), ('ow', 0.043), ('discarding', 0.043), ('brmk', 0.042), ('bwj', 0.042), ('coil', 0.042), ('rdk', 0.042), ('image', 0.042), ('preserves', 0.04), ('feasible', 0.04), ('rp', 0.04), ('xt', 0.04), ('imposed', 0.04), ('recognition', 0.038), ('wavelet', 0.037), ('dual', 0.036), ('testing', 0.035), ('satis', 0.035), ('closed', 0.034), ('tangent', 0.034), ('princeton', 0.034), ('digit', 0.034), ('bw', 0.034), ('nadler', 0.034), ('zhen', 0.034), ('lighting', 0.034), ('points', 0.033), ('shaded', 0.033), ('percentage', 0.032), ('wright', 0.032), ('rows', 0.032), ('sp', 0.031), ('mairal', 0.031), ('groups', 0.031), ('supplemental', 0.03), ('magenta', 0.03), ('sparsity', 0.029), ('learn', 0.029), ('ith', 0.029), ('data', 0.029), ('wakin', 0.029), ('xiang', 0.029), ('level', 0.028), ('thresholds', 0.028), ('enforce', 0.028), ('challenge', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="149-tfidf-1" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>2 0.23142651 <a title="149-tfidf-2" href="./nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">178 nips-2011-Multiclass Boosting: Theory and Algorithms</a></p>
<p>Author: Mohammad J. Saberian, Nuno Vasconcelos</p><p>Abstract: The problem of multi-class boosting is considered. A new framework, based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets. 1</p><p>3 0.214562 <a title="149-tfidf-3" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><p>4 0.17597553 <a title="149-tfidf-4" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>Author: David P. Wipf</p><p>Abstract: In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications). In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of ﬁnding, to the extent possible, regimes of dictionary invariant performance. In particular, a Type II Bayesian estimator with a dictionarydependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees no longer hold. This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions. 1</p><p>5 0.13871041 <a title="149-tfidf-5" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efﬁciently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). 1</p><p>6 0.13308486 <a title="149-tfidf-6" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>7 0.12774011 <a title="149-tfidf-7" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>8 0.12279493 <a title="149-tfidf-8" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>9 0.11185341 <a title="149-tfidf-9" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>10 0.11050227 <a title="149-tfidf-10" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>11 0.097573861 <a title="149-tfidf-11" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>12 0.093080603 <a title="149-tfidf-12" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>13 0.091448285 <a title="149-tfidf-13" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>14 0.089594699 <a title="149-tfidf-14" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>15 0.087140486 <a title="149-tfidf-15" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>16 0.087061115 <a title="149-tfidf-16" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>17 0.081071362 <a title="149-tfidf-17" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>18 0.079798512 <a title="149-tfidf-18" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>19 0.078795366 <a title="149-tfidf-19" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>20 0.076455928 <a title="149-tfidf-20" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.252), (1, 0.065), (2, -0.07), (3, -0.071), (4, -0.014), (5, 0.11), (6, 0.065), (7, 0.162), (8, -0.109), (9, -0.051), (10, -0.008), (11, 0.022), (12, -0.024), (13, -0.005), (14, -0.018), (15, -0.027), (16, -0.033), (17, -0.012), (18, -0.01), (19, -0.059), (20, 0.247), (21, -0.014), (22, 0.059), (23, 0.01), (24, 0.001), (25, -0.203), (26, -0.063), (27, 0.018), (28, -0.116), (29, 0.023), (30, -0.107), (31, -0.067), (32, -0.004), (33, -0.05), (34, 0.001), (35, 0.039), (36, 0.002), (37, -0.066), (38, -0.026), (39, 0.173), (40, 0.045), (41, -0.06), (42, 0.006), (43, -0.073), (44, -0.064), (45, -0.071), (46, 0.105), (47, 0.212), (48, -0.019), (49, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95253986 <a title="149-lsi-1" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>2 0.65300804 <a title="149-lsi-2" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>Author: David P. Wipf</p><p>Abstract: In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications). In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of ﬁnding, to the extent possible, regimes of dictionary invariant performance. In particular, a Type II Bayesian estimator with a dictionarydependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees no longer hold. This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions. 1</p><p>3 0.63256252 <a title="149-lsi-3" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>Author: Ioannis A. Gkioulekas, Todd Zickler</p><p>Abstract: We propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding. We formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain. We derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing. Our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals. 1</p><p>4 0.5688507 <a title="149-lsi-4" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>Author: Liefeng Bo, Xiaofeng Ren, Dieter Fox</p><p>Abstract: Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efﬁcient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efﬁciently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classiﬁcation problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports). 1</p><p>5 0.55521196 <a title="149-lsi-5" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>Author: Nobuyuki Morioka, Shin'ichi Satoh</p><p>Abstract: Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, 1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents “Generalized Lasso based Approximation of Sparse coding” (GLAS). By representing the distribution of sparse coefﬁcients with slice transform, we ﬁt a piece-wise linear mapping function with the generalized lasso. We also propose an efﬁcient post-reﬁnement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to 1 regularized sparse coding, yet achieves a signiﬁcant speed up demonstrating its effectiveness for large-scale visual recognition problems. 1</p><p>6 0.55139631 <a title="149-lsi-6" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>7 0.52508748 <a title="149-lsi-7" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>8 0.50074661 <a title="149-lsi-8" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>9 0.47521588 <a title="149-lsi-9" href="./nips-2011-Multiclass_Boosting%3A_Theory_and_Algorithms.html">178 nips-2011-Multiclass Boosting: Theory and Algorithms</a></p>
<p>10 0.45691997 <a title="149-lsi-10" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>11 0.45282936 <a title="149-lsi-11" href="./nips-2011-Advice_Refinement_in_Knowledge-Based_SVMs.html">27 nips-2011-Advice Refinement in Knowledge-Based SVMs</a></p>
<p>12 0.41524625 <a title="149-lsi-12" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>13 0.40123609 <a title="149-lsi-13" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>14 0.40115136 <a title="149-lsi-14" href="./nips-2011-Maximum_Covariance_Unfolding_%3A_Manifold_Learning_for_Bimodal_Data.html">167 nips-2011-Maximum Covariance Unfolding : Manifold Learning for Bimodal Data</a></p>
<p>15 0.3869366 <a title="149-lsi-15" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>16 0.38620955 <a title="149-lsi-16" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>17 0.37795013 <a title="149-lsi-17" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>18 0.3746964 <a title="149-lsi-18" href="./nips-2011-Extracting_Speaker-Specific_Information_with_a_Regularized_Siamese_Deep_Network.html">93 nips-2011-Extracting Speaker-Specific Information with a Regularized Siamese Deep Network</a></p>
<p>19 0.37312254 <a title="149-lsi-19" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>20 0.37243721 <a title="149-lsi-20" href="./nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization.html">51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.035), (4, 0.055), (20, 0.036), (26, 0.025), (31, 0.069), (33, 0.029), (43, 0.067), (45, 0.168), (57, 0.061), (65, 0.031), (74, 0.078), (83, 0.072), (92, 0.142), (99, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91095281 <a title="149-lda-1" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>2 0.90625167 <a title="149-lda-2" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>same-paper 3 0.89665198 <a title="149-lda-3" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>4 0.87401962 <a title="149-lda-4" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>5 0.83656102 <a title="149-lda-5" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>6 0.83025128 <a title="149-lda-6" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>7 0.82881546 <a title="149-lda-7" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>8 0.82877785 <a title="149-lda-8" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>9 0.82826781 <a title="149-lda-9" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>10 0.82622892 <a title="149-lda-10" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>11 0.82601607 <a title="149-lda-11" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>12 0.82481778 <a title="149-lda-12" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>13 0.82437867 <a title="149-lda-13" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>14 0.82349414 <a title="149-lda-14" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>15 0.82331842 <a title="149-lda-15" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>16 0.82028913 <a title="149-lda-16" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>17 0.81982207 <a title="149-lda-17" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>18 0.81879985 <a title="149-lda-18" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>19 0.81868124 <a title="149-lda-19" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>20 0.81865656 <a title="149-lda-20" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
