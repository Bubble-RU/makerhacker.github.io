<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2011-Algorithms for Hyper-Parameter Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-30" href="#">nips2011-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2011-Algorithms for Hyper-Parameter Optimization</h1>
<br/><p>Source: <a title="nips-2011-30-pdf" href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">pdf</a></p><p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>Reference: <a title="nips-2011-30-reference" href="../nips2011_reference/nips-2011-Algorithms_for_Hyper-Parameter_Optimization_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tpe', 0.672), ('gp', 0.374), ('dbn', 0.241), ('smbo', 0.231), ('search', 0.178), ('dbns', 0.143), ('tri', 0.129), ('ei', 0.114), ('mrbi', 0.105), ('parz', 0.102), ('bergstr', 0.095), ('gpu', 0.095), ('cd', 0.095), ('lay', 0.09), ('dy', 0.088), ('gps', 0.088), ('tness', 0.074), ('zca', 0.074), ('man', 0.072), ('surrog', 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="30-tfidf-1" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>2 0.22968166 <a title="30-tfidf-2" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>3 0.22406234 <a title="30-tfidf-3" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>4 0.11793629 <a title="30-tfidf-4" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>5 0.098190434 <a title="30-tfidf-5" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>6 0.096493401 <a title="30-tfidf-6" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>7 0.085670032 <a title="30-tfidf-7" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>8 0.079193518 <a title="30-tfidf-8" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>9 0.077735558 <a title="30-tfidf-9" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>10 0.076505885 <a title="30-tfidf-10" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>11 0.072565675 <a title="30-tfidf-11" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>12 0.070440583 <a title="30-tfidf-12" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>13 0.057366762 <a title="30-tfidf-13" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>14 0.053775962 <a title="30-tfidf-14" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>15 0.053175565 <a title="30-tfidf-15" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>16 0.052820899 <a title="30-tfidf-16" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>17 0.052504439 <a title="30-tfidf-17" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>18 0.048739541 <a title="30-tfidf-18" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>19 0.047110122 <a title="30-tfidf-19" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>20 0.046946928 <a title="30-tfidf-20" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, -0.006), (2, 0.011), (3, -0.01), (4, 0.026), (5, 0.052), (6, 0.041), (7, 0.034), (8, -0.156), (9, -0.019), (10, 0.024), (11, 0.132), (12, 0.034), (13, 0.083), (14, -0.013), (15, 0.124), (16, -0.066), (17, -0.035), (18, 0.161), (19, -0.02), (20, 0.157), (21, 0.115), (22, 0.118), (23, 0.128), (24, 0.069), (25, -0.002), (26, -0.128), (27, 0.11), (28, -0.104), (29, -0.018), (30, -0.038), (31, -0.002), (32, -0.081), (33, 0.105), (34, 0.036), (35, -0.032), (36, 0.053), (37, 0.066), (38, 0.009), (39, -0.15), (40, 0.026), (41, 0.072), (42, -0.057), (43, -0.041), (44, 0.15), (45, 0.006), (46, 0.098), (47, 0.014), (48, 0.03), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86314392 <a title="30-lsi-1" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>2 0.8025468 <a title="30-lsi-2" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>3 0.72268969 <a title="30-lsi-3" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>4 0.53223491 <a title="30-lsi-4" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>5 0.52464062 <a title="30-lsi-5" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>6 0.44456244 <a title="30-lsi-6" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>7 0.43341309 <a title="30-lsi-7" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>8 0.38803303 <a title="30-lsi-8" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>9 0.38491029 <a title="30-lsi-9" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>10 0.35426393 <a title="30-lsi-10" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>11 0.35140753 <a title="30-lsi-11" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>12 0.34146014 <a title="30-lsi-12" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>13 0.33982626 <a title="30-lsi-13" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>14 0.32262444 <a title="30-lsi-14" href="./nips-2011-Extracting_Speaker-Specific_Information_with_a_Regularized_Siamese_Deep_Network.html">93 nips-2011-Extracting Speaker-Specific Information with a Regularized Siamese Deep Network</a></p>
<p>15 0.32209501 <a title="30-lsi-15" href="./nips-2011-Dynamic_Pooling_and_Unfolding_Recursive_Autoencoders_for_Paraphrase_Detection.html">74 nips-2011-Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</a></p>
<p>16 0.31571421 <a title="30-lsi-16" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>17 0.31519544 <a title="30-lsi-17" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>18 0.3109493 <a title="30-lsi-18" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>19 0.30874938 <a title="30-lsi-19" href="./nips-2011-On_Causal_Discovery_with_Cyclic_Additive_Noise_Models.html">194 nips-2011-On Causal Discovery with Cyclic Additive Noise Models</a></p>
<p>20 0.30412689 <a title="30-lsi-20" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.011), (14, 0.046), (22, 0.074), (36, 0.038), (53, 0.017), (55, 0.156), (57, 0.256), (65, 0.069), (68, 0.223), (79, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82754761 <a title="30-lda-1" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>2 0.82682359 <a title="30-lda-2" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>3 0.81425905 <a title="30-lda-3" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>same-paper 4 0.80792302 <a title="30-lda-4" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>5 0.75799942 <a title="30-lda-5" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>6 0.75383025 <a title="30-lda-6" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>7 0.75087917 <a title="30-lda-7" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>8 0.75026453 <a title="30-lda-8" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>9 0.74984062 <a title="30-lda-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.74952942 <a title="30-lda-10" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>11 0.74931741 <a title="30-lda-11" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>12 0.74905705 <a title="30-lda-12" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>13 0.74889445 <a title="30-lda-13" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>14 0.7486192 <a title="30-lda-14" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>15 0.74801332 <a title="30-lda-15" href="./nips-2011-A_blind_sparse_deconvolution_method_for_neural_spike_identification.html">13 nips-2011-A blind sparse deconvolution method for neural spike identification</a></p>
<p>16 0.74796772 <a title="30-lda-16" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>17 0.74766099 <a title="30-lda-17" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>18 0.74709666 <a title="30-lda-18" href="./nips-2011-Efficient_Learning_of_Generalized_Linear_and_Single_Index_Models_with_Isotonic_Regression.html">77 nips-2011-Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression</a></p>
<p>19 0.74687612 <a title="30-lda-19" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>20 0.746723 <a title="30-lda-20" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
