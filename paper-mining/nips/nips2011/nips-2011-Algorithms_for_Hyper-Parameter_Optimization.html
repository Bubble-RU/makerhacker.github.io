<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2011-Algorithms for Hyper-Parameter Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-30" href="#">nips2011-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2011-Algorithms for Hyper-Parameter Optimization</h1>
<br/><p>Source: <a title="nips-2011-30-pdf" href="http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf">pdf</a></p><p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>Reference: <a title="nips-2011-30-reference" href="../nips2011_reference/nips-2011-Algorithms_for_Hyper-Parameter_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. [sent-10, score-0.257]
</p><p>2 Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. [sent-11, score-0.175]
</p><p>3 We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). [sent-12, score-0.214]
</p><p>4 We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. [sent-13, score-0.329]
</p><p>5 Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. [sent-14, score-0.197]
</p><p>6 The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. [sent-15, score-0.141]
</p><p>7 The difﬁculty of tuning these models makes published results difﬁcult to reproduce and extend, and makes even the original investigation of such methods more of an art than a science. [sent-18, score-0.068]
</p><p>8 Recent results such as [5], [6], and [7] demonstrate that the challenge of hyper-parameter optimization in large and multilayer models is a direct impediment to scientiﬁc progress. [sent-19, score-0.082]
</p><p>9 These works have advanced state of the art performance on image classiﬁcation problems by more concerted hyper-parameter optimization in simple algorithms, rather than by innovative modeling or machine learning strategies. [sent-20, score-0.116]
</p><p>10 Instead, hyper-parameter optimization should be regarded as a formal outer loop in the learning process. [sent-22, score-0.082]
</p><p>11 A learning algorithm, as a functional from data to classiﬁer (taking classiﬁcation problems as an example), includes a budgeting choice of how many CPU cycles are to be spent on hyper-parameter exploration, and how many CPU cycles are to be spent evaluating each hyperparameter choice (i. [sent-23, score-0.089]
</p><p>12 The results of [5] and [7] suggest that with current generation hardware such as large computer clusters and GPUs, the optimal alloca1  tion of CPU cycles includes more hyper-parameter exploration than has been typical in the machine learning literature. [sent-26, score-0.069]
</p><p>13 Hyper-parameter optimization is the problem of optimizing a loss function over a graph-structured conﬁguration space. [sent-27, score-0.12]
</p><p>14 the number of hidden units in the 2nd layer of a DBN) are only well-deﬁned when node variables (e. [sent-31, score-0.072]
</p><p>15 a discrete choice of how many layers to use) take particular values. [sent-33, score-0.079]
</p><p>16 Not only must a hyper-parameter optimization algorithm optimize over variables which are discrete, ordinal, and continuous, but it must simultaneously choose which variables to optimize. [sent-34, score-0.128]
</p><p>17 Random search is the algorithm of drawing hyper-parameter assignments from that process and evaluating them. [sent-36, score-0.17]
</p><p>18 This paper makes two contributions: 1) Random search is competitive with the manual optimization of DBNs in [1], and 2) Automatic sequential optimization outperforms both manual and random search. [sent-38, score-0.872]
</p><p>19 Section 2 covers sequential model-based optimization, and the expected improvement criterion. [sent-39, score-0.127]
</p><p>20 Section 3 introduces a Gaussian Process based hyper-parameter optimization algorithm. [sent-40, score-0.082]
</p><p>21 Section 6 shows the efﬁciency of sequential optimization on the two hardest datasets according to random search. [sent-43, score-0.233]
</p><p>22 In an application where the true ﬁtness function f : X → R is costly to evaluate, model-based algorithms approximate f with a surrogate that is cheaper to evaluate. [sent-46, score-0.082]
</p><p>23 Typically the inner loop in an SMBO algorithm is the numerical optimization of this surrogate, or some transformation of the surrogate. [sent-47, score-0.082]
</p><p>24 The point x∗ that maximizes the surrogate (or its transformation) becomes the proposal for where the true function f should be evaluated. [sent-48, score-0.094]
</p><p>25 We chose to use the EI criterion in our work because it is intuitive, and has been shown to work well in a variety of settings. [sent-55, score-0.06]
</p><p>26 We leave the systematic exploration of improvement criteria for future work. [sent-56, score-0.071]
</p><p>27 3  The Gaussian Process Approach (GP)  Gaussian Processes have long been recognized as a good method for modeling loss functions in model-based optimization literature [13]. [sent-60, score-0.082]
</p><p>28 The above mentioned closedness property, along with the fact that GPs provide an assessment of prediction uncertainty incorporating the effect of data scarcity, make the GP an elegant candidate for both ﬁnding candidate x∗ (Figure 1, step 3) and ﬁtting a model Mt (Figure 1, step 6). [sent-67, score-0.06]
</p><p>29 EI functions are usually optimized with an exhaustive grid search over the input space, or a Latin Hypercube search in higher dimensions. [sent-73, score-0.34]
</p><p>30 The authors of [16] used the preceding remarks on the landscape of EI to design an evolutionary algorithm with mixture search, speciﬁcally aimed at optimizing EI, that is shown to outperform exhaustive search for a given budget in EI evaluations. [sent-75, score-0.278]
</p><p>31 CMA-ES is a state-of-the-art gradient-free evolutionary algorithm for optimization on continuous domains, which has been shown to outperform the Gaussian search EDA. [sent-78, score-0.296]
</p><p>32 The use of tesselations suggested by [16] is prohibitive here, as our task often means working in more than 10 dimensions, thus we start each local search at the center of mass of a simplex with vertices randomly picked among the training points. [sent-81, score-0.17]
</p><p>33 3  4  Tree-structured Parzen Estimator Approach (TPE)  Anticipating that our hyper-parameter optimization tasks will mean high dimensions and small ﬁtness evaluation budgets, we now turn to another modeling strategy and EI optimization scheme for the SMBO algorithm. [sent-87, score-0.191]
</p><p>34 In the experimental section, we will see that the conﬁguation space is described using uniform, log-uniform, quantized log-uniform, and categorical variables. [sent-93, score-0.067]
</p><p>35 Whereas the GP-based approach favoured quite an aggressive y ∗ (typically less than the best observed loss), the TPE algorithm depends on a y ∗ that is larger than the best observed f (x) so that some points can be used to form (x). [sent-100, score-0.073]
</p><p>36 By maintaining sorted lists of observed variables in H, the runtime of each iteration of the TPE algorithm can scale linearly in |H| and linearly in the number of variables (dimensions) being optimized. [sent-102, score-0.073]
</p><p>37 1  Optimizing EI in the TPE algorithm  The parametrization of p(x, y) as p(y)p(x|y) in the TPE algorithm was chosen to facilitate the optimization of EI. [sent-104, score-0.082]
</p><p>38 y∗  y∗  (y ∗ − y)p(y|x)dy =  EIy∗ (x) = −∞ ∗  −∞  By construction, γ = p(y < y ) and p(x) = y∗ ∗  (y − y)p(x|y)p(y)dy  (y ∗ − y)  =  R y∗  p(x|y)p(y) dy p(x)  p(x|y)p(y)dy = γ (x) + (1 − γ)g(x). [sent-105, score-0.082]
</p><p>39 This last expression  shows that to maximize improvement we would like points x with high probability under (x) and low probability under g(x). [sent-107, score-0.06]
</p><p>40 5, 1) W init U (−a, a) or N (0, a2 ) random seed 5 choices a algo A or B (see text) classiﬁer learn rate log U (0. [sent-128, score-0.095]
</p><p>41 layers 1 to 3 CD anneal start log U (10, 104 ) batch size 20 or 100 CD sample data yes or no  5  Random Search for Hyper-Parameter Optimization in DBNs  One simple, but recent step toward formalizing hyper-parameter optimization is the use of random search [5]. [sent-131, score-0.379]
</p><p>42 [19] showed that random search was much more efﬁcient than grid search for optimizing the parameters of one-layer neural network classiﬁers. [sent-132, score-0.41]
</p><p>43 In this section, we evaluate random search for DBN optimization, compared with the sequential grid-assisted manual search carried out in [1]. [sent-133, score-0.672]
</p><p>44 We chose the prior listed in Table 1 to deﬁne the search space over DBN conﬁgurations. [sent-134, score-0.245]
</p><p>45 These changes expand the hyper-parameter search problem, while maintaining the original hyper-parameter search space as a subset of the expanded search space. [sent-137, score-0.533]
</p><p>46 The results of this preliminary random search are in Figure 2. [sent-138, score-0.202]
</p><p>47 Perhaps surprisingly, the result of manual search can be reliably matched with 32 random trials for several datasets. [sent-139, score-0.583]
</p><p>48 The efﬁciency of random search in this setting is explored further in [21]. [sent-140, score-0.202]
</p><p>49 Where random search results match human performance, it is not clear from Figure 2 whether the reason is that it searched the original space as efﬁciently, or that it searched a larger space where good performance is easier to ﬁnd. [sent-141, score-0.302]
</p><p>50 The results in Figure 2 indicate that hyper-parameter optimization is harder for some datasets. [sent-144, score-0.082]
</p><p>51 For example, in the case of the “MNIST rotated background images” dataset (MRBI), random sampling appears to converge to a maximum relatively quickly (best models among experiments of 32 trials show little variance in performance), but this plateau is lower than what was found by manual search. [sent-145, score-0.491]
</p><p>52 In another dataset (convex), the random sampling procedure exceeds the performance of manual search, but is slow to converge to any sort of plateau. [sent-146, score-0.238]
</p><p>53 This slow convergence indicates that better performance is probably available, but we need to search the conﬁguration space more efﬁciently to ﬁnd it. [sent-148, score-0.193]
</p><p>54 The remainder of this paper explores sequential optimization strategies for hyper-parameter optimization for these two datasets: convex and MRBI. [sent-149, score-0.291]
</p><p>55 1 by comparing with random sampling on the Boston Housing dataset, a regression task with 506 points made of 13 scaled input variables and a scalar 5  mnist basic  1. [sent-151, score-0.147]
</p><p>56 Random search is used to explore up to 32 hyper-parameters (see Table 1). [sent-199, score-0.17]
</p><p>57 Results found using a grid-search-assisted manual search over a similar domain with an average 41 trials are given in green (1-layer DBN) and red (3-layer DBN). [sent-200, score-0.551]
</p><p>58 ) shows the distribution of test set performance when the best model among N random trials is selected. [sent-204, score-0.23]
</p><p>59 The datasets “convex” and “mnist rotated background images” are used for more thorough hyper-parameter optimization. [sent-205, score-0.103]
</p><p>60 The ﬁrst 30 iterations were made using random sampling, while from the 30th on, we differentiated the random samples from the GP approach trained on the updated history. [sent-209, score-0.064]
</p><p>61 Although the number of points is particularly small compared to the dimensionality, the surrogate modelling approach ﬁnds noticeably better points than random, which supports the application of SMBO approaches to more ambitious tasks and datasets. [sent-211, score-0.14]
</p><p>62 Applying the GP to the problem of optimizing DBN performance, we allowed 3 random restarts to the CMA+ES algorithm per proposal x∗ , and up to 500 iterations of conjugate gradient method in ﬁtting the length scales of the GP. [sent-212, score-0.143]
</p><p>63 The CMA-ES part of GPs dealt with boundaries using a penalty method, the binomial sampling part dealt with it by nature. [sent-214, score-0.075]
</p><p>64 15 and picked the best among 100 candidates drawn from (x) on each iteration as the proposal x∗ . [sent-218, score-0.09]
</p><p>65 TPE was allowed to grow past the initial bounds used with for random sampling in the course of optimization, whereas the GP and random search were restricted to stay within the initial bounds throughout the course of optimization. [sent-220, score-0.271]
</p><p>66 The TPE algorithm was also initialized with the same 30 randomly sampled points as were used to seed the GP. [sent-221, score-0.064]
</p><p>67 For the GP approach, the so-called constant liar approach was used: each time a candidate point x∗ was proposed, a fake ﬁtness evaluation equal to the mean of the y’s within the training set D was assigned temporarily, until the evaluation completed and reported the actual loss f (x∗ ). [sent-224, score-0.084]
</p><p>68 This makes search less efﬁcient, though faster in terms of wall time. [sent-227, score-0.194]
</p><p>69 44%  50  Table 2: The test set classiﬁcation error of the best model found by each search algorithm on each problem. [sent-244, score-0.193]
</p><p>70 Each search algorithm was allowed up to 200 trials. [sent-245, score-0.207]
</p><p>71 The manual searches used 82 trials for convex and 27 trials MRBI. [sent-246, score-0.589]
</p><p>72 With the parallel evaluation of up to ﬁve proposals from the GP and TPE algorithms, each experiment took about 24 hours of wall time using ﬁve GPUs. [sent-253, score-0.101]
</p><p>73 7  Discussion  The trajectories (H) constructed by each algorithm up to 200 steps are illustrated in Figure 4, and compared with random search and the manual search carried out in [1]. [sent-254, score-0.578]
</p><p>74 On the convex dataset (2-way classiﬁcation), both algorithms converged to a validation score of 13% error. [sent-256, score-0.086]
</p><p>75 TPE’s best was signiﬁcantly better than both manual search (19%) and random search with 200 trials (17%). [sent-260, score-0.776]
</p><p>76 On the MRBI dataset (10-way classiﬁcation), random search was the worst performer (50% error), the GP approach and manual search approximately tied (47% error), while the TPE algorithm found a new best result (44% error). [sent-261, score-0.601]
</p><p>77 The GP and TPE algorithms were slightly less efﬁcient than manual search: GP and EI identiﬁed performance on par with manual search within 80 trials, the manual search of [1] used 82 trials for convex and 27 trials for MRBI. [sent-263, score-1.365]
</p><p>78 Perhaps, conversely, the exploration induced by the TPE’s lack of accuracy turned out to be a good heuristic for search. [sent-266, score-0.066]
</p><p>79 Critically though, all four SMBO runs matched or exceeded both random search and a careful human-guided search, which are currently the state of the art methods for hyper-parameter optimization. [sent-269, score-0.236]
</p><p>80 Sequential optimization algorithms work by leveraging structure in observed (x, y) pairs. [sent-271, score-0.106]
</p><p>81 8  Conclusion  This paper has introduced two sequential hyper-parameter optimization algorithms, and shown them to meet or exceed human performance and the performance of a brute-force random search in two difﬁcult hyper-parameter optimization tasks involving DBNs. [sent-274, score-0.46]
</p><p>82 equal layer sizes at all layers) on the search space, and fall back on a more natural hyperparameter space of 32 variables (including both discrete and continuous variables) in which many 7  Dataset: convex  Dataset: mnist rotated background images  0. [sent-277, score-0.515]
</p><p>83 The solid coloured lines are the validation set accuracy of the best trial found before each point in time. [sent-296, score-0.104]
</p><p>84 Both the TPE and GP algorithms make signiﬁcant advances from their random initial conditions, and substantially outperform the manual and random search methods. [sent-297, score-0.464]
</p><p>85 A 95% conﬁdence interval about the best validation means on the convex task extends 0. [sent-298, score-0.085]
</p><p>86 The solid black line is the test set accuracy obtained by domain experts using a combination of grid search and manual search [1]. [sent-301, score-0.574]
</p><p>87 5% quantile of validation performance found among trials sampled from our prior distribution (see Table 1), estimated from 457 and 361 random trials on the two datasets respectively. [sent-303, score-0.491]
</p><p>88 In this 32-dimensional search problem, the TPE algorithm presented here has uncovered new best results on both of these datasets that are signiﬁcantly better than what DBNs were previously believed to achieve. [sent-307, score-0.242]
</p><p>89 Moreover, the GP and TPE algorithms are practical: the optimization for each dataset was done in just 24 hours using ﬁve GPU processors. [sent-308, score-0.106]
</p><p>90 Although our results are only for DBNs, our methods are quite general, and extend naturally to any hyper-parameter optimization problem in which the hyper-parameters are drawn from a measurable set. [sent-309, score-0.082]
</p><p>91 We hope that our work may spur researchers in the machine learning community to treat the hyperparameter optimization strategy as an interesting and important component of all learning algorithms. [sent-310, score-0.109]
</p><p>92 ” is not a fully speciﬁed, empirically answerable question – different approaches to hyper-parameter optimization will give different answers. [sent-312, score-0.082]
</p><p>93 Algorithmic approaches to hyper-parameter optimization make machine learning results easier to disseminate, reproduce, and transfer to other domains. [sent-313, score-0.082]
</p><p>94 Finally, powerful hyper-parameter optimization algorithms broaden the horizon of models that can realistically be studied; researchers need not restrict themselves to systems of a few variables that can readily be tuned by hand. [sent-315, score-0.129]
</p><p>95 An empirical evaluation of deep architectures on problems with many factors of variation. [sent-326, score-0.082]
</p><p>96 Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. [sent-342, score-0.117]
</p><p>97 A taxonomy of global optimization methods based on response surfaces. [sent-382, score-0.108]
</p><p>98 An informational approach to the global optimization of expensive-to-evaluate functions. [sent-388, score-0.108]
</p><p>99 Gaussian process optimization in the bandit setting: No regret and experimental design. [sent-395, score-0.082]
</p><p>100 Surrogating the surrogate: accelerating Gaussian Process optimization with e mixtures. [sent-425, score-0.082]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tpe', 0.626), ('gp', 0.348), ('dbn', 0.224), ('smbo', 0.215), ('manual', 0.206), ('trials', 0.175), ('search', 0.17), ('dbns', 0.133), ('ei', 0.106), ('mrbi', 0.098), ('parzen', 0.095), ('sequential', 0.094), ('guration', 0.093), ('bergstra', 0.089), ('cd', 0.088), ('optimization', 0.082), ('dy', 0.082), ('gps', 0.082), ('gpu', 0.079), ('tness', 0.069), ('zca', 0.069), ('mnist', 0.065), ('bardenet', 0.059), ('eiy', 0.059), ('surrogate', 0.058), ('layers', 0.056), ('deep', 0.055), ('rotated', 0.052), ('layer', 0.049), ('evolutionary', 0.044), ('categorical', 0.044), ('con', 0.043), ('anneal', 0.039), ('cma', 0.039), ('exploration', 0.038), ('cpu', 0.038), ('optimizing', 0.038), ('seed', 0.037), ('allowed', 0.037), ('proposal', 0.036), ('experimenter', 0.034), ('mlp', 0.034), ('theano', 0.034), ('art', 0.034), ('reproduce', 0.034), ('convex', 0.033), ('improvement', 0.033), ('criterion', 0.033), ('universit', 0.032), ('random', 0.032), ('mt', 0.032), ('recherche', 0.032), ('candidates', 0.031), ('cycles', 0.031), ('denoising', 0.031), ('perhaps', 0.03), ('candidate', 0.03), ('gaussian', 0.03), ('quantile', 0.03), ('housing', 0.03), ('lozano', 0.03), ('validation', 0.029), ('noticeably', 0.028), ('informatique', 0.028), ('accuracy', 0.028), ('chose', 0.027), ('networks', 0.027), ('hyperparameter', 0.027), ('densities', 0.027), ('coates', 0.027), ('searched', 0.027), ('classi', 0.027), ('evaluation', 0.027), ('points', 0.027), ('runtime', 0.027), ('background', 0.026), ('global', 0.026), ('landscape', 0.026), ('boston', 0.026), ('algo', 0.026), ('autoencoders', 0.026), ('rectangles', 0.026), ('experiment', 0.025), ('took', 0.025), ('binomial', 0.025), ('dealt', 0.025), ('datasets', 0.025), ('prior', 0.025), ('algorithms', 0.024), ('wall', 0.024), ('believed', 0.024), ('images', 0.024), ('trial', 0.024), ('ciency', 0.024), ('stacked', 0.023), ('discrete', 0.023), ('belief', 0.023), ('space', 0.023), ('best', 0.023), ('variables', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="30-tfidf-1" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>2 0.20701273 <a title="30-tfidf-2" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>Author: Andrew Mchutchon, Carl E. Rasmussen</p><p>Abstract: In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods. 1</p><p>3 0.18009998 <a title="30-tfidf-3" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>4 0.10626417 <a title="30-tfidf-4" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>5 0.083362319 <a title="30-tfidf-5" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>Author: Yee W. Teh, Vinayak Rao</p><p>Abstract: Renewal processes are generalizations of the Poisson process on the real line whose intervals are drawn i.i.d. from some distribution. Modulated renewal processes allow these interevent distributions to vary with time, allowing the introduction of nonstationarity. In this work, we take a nonparametric Bayesian approach, modelling this nonstationarity with a Gaussian process. Our approach is based on the idea of uniformization, which allows us to draw exact samples from an otherwise intractable distribution. We develop a novel and efﬁcient MCMC sampler for posterior inference. In our experiments, we test these on a number of synthetic and real datasets. 1</p><p>6 0.078619555 <a title="30-tfidf-6" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>7 0.078298844 <a title="30-tfidf-7" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>8 0.075605266 <a title="30-tfidf-8" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>9 0.071542129 <a title="30-tfidf-9" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>10 0.06984967 <a title="30-tfidf-10" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>11 0.069505267 <a title="30-tfidf-11" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>12 0.068432949 <a title="30-tfidf-12" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>13 0.053126901 <a title="30-tfidf-13" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>14 0.0527073 <a title="30-tfidf-14" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>15 0.052654319 <a title="30-tfidf-15" href="./nips-2011-An_Unsupervised_Decontamination_Procedure_For_Improving_The_Reliability_Of_Human_Judgments.html">34 nips-2011-An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments</a></p>
<p>16 0.052134078 <a title="30-tfidf-16" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>17 0.052100301 <a title="30-tfidf-17" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>18 0.050081693 <a title="30-tfidf-18" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>19 0.046494678 <a title="30-tfidf-19" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>20 0.046353724 <a title="30-tfidf-20" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.007), (2, 0.026), (3, 0.018), (4, -0.028), (5, -0.059), (6, 0.058), (7, 0.006), (8, 0.036), (9, 0.022), (10, -0.15), (11, -0.047), (12, 0.053), (13, 0.012), (14, -0.038), (15, 0.165), (16, 0.052), (17, 0.128), (18, -0.16), (19, -0.157), (20, 0.008), (21, 0.1), (22, -0.114), (23, -0.169), (24, 0.02), (25, 0.081), (26, 0.011), (27, -0.026), (28, 0.003), (29, -0.147), (30, -0.017), (31, -0.038), (32, 0.109), (33, 0.089), (34, 0.086), (35, -0.008), (36, 0.028), (37, -0.07), (38, -0.066), (39, 0.015), (40, -0.01), (41, 0.025), (42, 0.112), (43, -0.017), (44, 0.002), (45, -0.031), (46, -0.044), (47, 0.086), (48, -0.077), (49, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91643524 <a title="30-lsi-1" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>2 0.82619077 <a title="30-lsi-2" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>Author: Andrew Mchutchon, Carl E. Rasmussen</p><p>Abstract: In standard Gaussian Process regression input locations are assumed to be noise free. We present a simple yet effective GP model for training on input points corrupted by i.i.d. Gaussian noise. To make computations tractable we use a local linear expansion about each input point. This allows the input noise to be recast as output noise proportional to the squared gradient of the GP posterior mean. The input noise variances are inferred from the data as extra hyperparameters. They are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood. Training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient. Analytic predictive moments can then be found for Gaussian distributed test points. We compare our model to others over a range of different regression problems and show that it improves over current methods. 1</p><p>3 0.80963367 <a title="30-lsi-3" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>Author: David K. Duvenaud, Hannes Nickisch, Carl E. Rasmussen</p><p>Abstract: We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efﬁcient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks. 1</p><p>4 0.55658871 <a title="30-lsi-4" href="./nips-2011-Contextual_Gaussian_Process_Bandit_Optimization.html">61 nips-2011-Contextual Gaussian Process Bandit Optimization</a></p>
<p>Author: Andreas Krause, Cheng S. Ong</p><p>Abstract: How should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions? How should we select relevant documents (ads) to display, given information about the user? These tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents). The key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data. We model the payoff function as a sample from a Gaussian process deﬁned over the joint context-action space, and develop CGP-UCB, an intuitive upper-conﬁdence style algorithm. We show that by mixing and matching kernels for contexts and actions, CGP-UCB can handle a variety of practical applications. We further provide generic tools for deriving regret bounds when using such composite kernel functions. Lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management. We show that context-sensitive optimization outperforms no or naive use of context. 1</p><p>5 0.53005308 <a title="30-lsi-5" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>Author: Yee W. Teh, Vinayak Rao</p><p>Abstract: Renewal processes are generalizations of the Poisson process on the real line whose intervals are drawn i.i.d. from some distribution. Modulated renewal processes allow these interevent distributions to vary with time, allowing the introduction of nonstationarity. In this work, we take a nonparametric Bayesian approach, modelling this nonstationarity with a Gaussian process. Our approach is based on the idea of uniformization, which allows us to draw exact samples from an otherwise intractable distribution. We develop a novel and efﬁcient MCMC sampler for posterior inference. In our experiments, we test these on a number of synthetic and real datasets. 1</p><p>6 0.45231926 <a title="30-lsi-6" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>7 0.44733062 <a title="30-lsi-7" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>8 0.40214679 <a title="30-lsi-8" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>9 0.37438396 <a title="30-lsi-9" href="./nips-2011-Shallow_vs._Deep_Sum-Product_Networks.html">250 nips-2011-Shallow vs. Deep Sum-Product Networks</a></p>
<p>10 0.3621273 <a title="30-lsi-10" href="./nips-2011-An_Unsupervised_Decontamination_Procedure_For_Improving_The_Reliability_Of_Human_Judgments.html">34 nips-2011-An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments</a></p>
<p>11 0.3607901 <a title="30-lsi-11" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>12 0.35382065 <a title="30-lsi-12" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>13 0.34541681 <a title="30-lsi-13" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>14 0.34467724 <a title="30-lsi-14" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>15 0.34317723 <a title="30-lsi-15" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>16 0.34193489 <a title="30-lsi-16" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>17 0.34028727 <a title="30-lsi-17" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>18 0.33959827 <a title="30-lsi-18" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>19 0.33941257 <a title="30-lsi-19" href="./nips-2011-Dynamic_Pooling_and_Unfolding_Recursive_Autoencoders_for_Paraphrase_Detection.html">74 nips-2011-Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</a></p>
<p>20 0.33792424 <a title="30-lsi-20" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (4, 0.042), (20, 0.034), (26, 0.018), (31, 0.097), (33, 0.021), (40, 0.236), (43, 0.082), (45, 0.111), (57, 0.076), (65, 0.031), (74, 0.046), (83, 0.037), (99, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86578971 <a title="30-lda-1" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. D. Babacan</p><p>Abstract: Variational Bayesian matrix factorization (VBMF) efﬁciently approximates the posterior distribution of factorized matrices by assuming matrix-wise independence of the two factors. A recent study on fully-observed VBMF showed that, under a stronger assumption that the two factorized matrices are column-wise independent, the global optimal solution can be analytically computed. However, it was not clear how restrictive the column-wise independence assumption is. In this paper, we prove that the global solution under matrix-wise independence is actually column-wise independent, implying that the column-wise independence assumption is harmless. A practical consequence of our theoretical ﬁnding is that the global solution under matrix-wise independence (which is a standard setup) can be obtained analytically in a computationally very efﬁcient way without any iterative algorithms. We experimentally illustrate advantages of using our analytic solution in probabilistic principal component analysis. 1</p><p>same-paper 2 0.77431911 <a title="30-lda-2" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<p>Author: James S. Bergstra, Rémi Bardenet, Yoshua Bengio, Balázs Kégl</p><p>Abstract: Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difﬁcult DBN learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements. 1</p><p>3 0.74393064 <a title="30-lda-3" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>Author: Shie Mannor, Ohad Shamir</p><p>Abstract: We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known “experts” setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds. 1</p><p>4 0.72149611 <a title="30-lda-4" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>Author: Kamil A. Wnuk, Stefano Soatto</p><p>Abstract: We propose a robust ﬁltering approach based on semi-supervised and multiple instance learning (MIL). We assume that the posterior density would be unimodal if not for the eﬀect of outliers that we do not wish to explicitly model. Therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior. Our approach can be thought of as a combination of standard ﬁnite-dimensional ﬁltering (Extended Kalman Filter, or Unscented Filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements. We show how both the state (regression) and the inlier set (classiﬁcation) can be estimated iteratively and causally by processing only the current measurement. We illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set). 1</p><p>5 0.6661976 <a title="30-lda-5" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>Author: Nobuyuki Morioka, Shin'ichi Satoh</p><p>Abstract: Sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision. For visual object category recognition, 1 regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance. However, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck. To overcome this computational challenge, this paper presents “Generalized Lasso based Approximation of Sparse coding” (GLAS). By representing the distribution of sparse coefﬁcients with slice transform, we ﬁt a piece-wise linear mapping function with the generalized lasso. We also propose an efﬁcient post-reﬁnement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting. The experiments show that GLAS obtains a comparable performance to 1 regularized sparse coding, yet achieves a signiﬁcant speed up demonstrating its effectiveness for large-scale visual recognition problems. 1</p><p>6 0.64335078 <a title="30-lda-6" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>7 0.63753253 <a title="30-lda-7" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>8 0.63689035 <a title="30-lda-8" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>9 0.63421339 <a title="30-lda-9" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>10 0.63316125 <a title="30-lda-10" href="./nips-2011-Hierarchically_Supervised_Latent_Dirichlet_Allocation.html">116 nips-2011-Hierarchically Supervised Latent Dirichlet Allocation</a></p>
<p>11 0.63084286 <a title="30-lda-11" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>12 0.63016754 <a title="30-lda-12" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>13 0.62951225 <a title="30-lda-13" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>14 0.62904406 <a title="30-lda-14" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>15 0.62893599 <a title="30-lda-15" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>16 0.62888259 <a title="30-lda-16" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>17 0.62879938 <a title="30-lda-17" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>18 0.62769729 <a title="30-lda-18" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>19 0.62624371 <a title="30-lda-19" href="./nips-2011-Spectral_Methods_for_Learning_Multivariate_Latent_Tree_Structure.html">267 nips-2011-Spectral Methods for Learning Multivariate Latent Tree Structure</a></p>
<p>20 0.62511808 <a title="30-lda-20" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
