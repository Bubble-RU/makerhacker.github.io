<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-16" href="#">nips2011-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</h1>
<br/><p>Source: <a title="nips-2011-16-pdf" href="http://papers.nips.cc/paper/4274-a-reinterpretation-of-the-policy-oscillation-phenomenon-in-approximate-policy-iteration.pdf">pdf</a></p><p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>Reference: <a title="nips-2011-16-reference" href="../nips2011_reference/nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A reinterpretation of the policy oscillation phenomenon in approximate policy iteration  Paul Wagner Department of Information and Computer Science Aalto University School of Science PO Box 15400, FI-00076 Aalto, Finland pwagner@cis. [sent-1, score-2.09]
</p><p>2 fi  Abstract A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. [sent-3, score-1.187]
</p><p>3 The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. [sent-4, score-1.19]
</p><p>4 We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. [sent-5, score-0.204]
</p><p>5 In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. [sent-8, score-0.714]
</p><p>6 1  Introduction  We consider the reinforcement learning problem in which one attempts to ﬁnd a good policy for controlling a stochastic nonlinear dynamical system. [sent-11, score-0.837]
</p><p>7 The majority of these methods can be categorized into greedy value function methods (critic-only) and value-based policy gradient methods (actor-critic) (e. [sent-17, score-0.95]
</p><p>8 The former approach, although fast, is susceptible to potentially severe policy oscillations in presence of approximations. [sent-20, score-0.874]
</p><p>9 This phenomenon is known as the policy oscillation (or policy chattering) phenomenon [7, 8]. [sent-21, score-2.082]
</p><p>10 Bertsekas has recently called attention to the currently not well understood policy oscillation phenomenon [7]. [sent-26, score-1.24]
</p><p>11 First, the policy oscillation phenomenon is intimately connected to some aspects of the learning dynamics at the very heart of approximate dynamic An extended version of this paper is available at http://users. [sent-31, score-1.34]
</p><p>12 The policy oscillation phenomenon is strongly associated in the literature with the popular Tetris benchmark problem. [sent-38, score-1.24]
</p><p>13 Impressively fast initial improvement followed by severe degradation was reported in [12] using a greedy approximate policy iteration method. [sent-42, score-1.103]
</p><p>14 This degradation has been taken in the literature as a manifestation of the policy oscillation phenomenon [12, 8]. [sent-43, score-1.298]
</p><p>15 Policy gradient and greedy approximate value iteration methods have shown much more stable behavior in the Tetris problem [13, 14], although it has seemed that this stability tends to come at the price of speed (see esp. [sent-44, score-0.409]
</p><p>16 The typical performance levels obtained with approximate dynamic programming methods have been around 5,000 points [12, 8, 13, 16], while an improvement to around 20,000 points has been obtained in [14] by considerably lowering the discount factor. [sent-47, score-0.251]
</p><p>17 It has been hypothesized in [7] that this grossly suboptimal performance of even the best-performing approximate dynamic programming methods might also have some subtle connection to the oscillation phenomenon. [sent-49, score-0.58]
</p><p>18 After providing background in Section 2, we discuss the policy oscillation phenomenon in Section 3 along with three examples, one of which is novel and generalizes the others. [sent-52, score-1.24]
</p><p>19 We develop a novel view to the policy oscillation phenomenon in Sections 4 and 5. [sent-53, score-1.24]
</p><p>20 We validate the view also empirically in Section 6 and proceed to looking for the suggested connection between the oscillation phenomenon and the convergence issues in the Tetris problem. [sent-54, score-0.57]
</p><p>21 We report empirical evidence that indeed suggests a shared explanation to the policy degradation observed in [12, 8] and the early stagnation of all the rest of the attempted approximate dynamic programming methods. [sent-55, score-0.975]
</p><p>22 However, it seems that this explanation is not primarily related to the oscillation phenomenon but to numerical instability. [sent-56, score-0.552]
</p><p>23 A (soft-)greedy policy π ∗ (a|s, Q) is a (stochastic) mapping from states to actions and is based on the value function Q. [sent-60, score-0.759]
</p><p>24 A parameterized policy π(a|s, θ) is a stochastic mapping from states to actions and is based on the parameter vector θ. [sent-61, score-0.781]
</p><p>25 In policy iteration, the current policy is fully evaluated, after which a policy improvement step is taken based on this evaluation. [sent-65, score-2.191]
</p><p>26 In optimistic policy iteration, policy improvement is based on an incomplete evaluation. [sent-66, score-1.483]
</p><p>27 , [2, 3]), the current policy on iteration k is usually implicit and is greedy (and thus deterministic) with respect to the value function Qk−1 of the previous policy: π ∗ (a|s, Qk−1 ) =  1 0  if a = arg maxb Qk−1 (s, b) otherwise. [sent-70, score-1.012]
</p><p>28 Soft-greedy iteration is obtained by slightly softening π ∗ in some way so that 2  π ∗ (a|s, Qk−1 ) > 0, ∀a, s, the Gibbs soft-greedy policy class with a temperature τ (Boltzmann exploration) being a common choice: π ∗ (a|s, Qk−1 ) ∝ eQk−1 (s,a)/τ . [sent-72, score-0.811]
</p><p>29 A common choice for approximating Q is to obtain a least-squares ﬁt using a linear-in-parameters ˜ approximator Q with the feature basis φ∗ : ˜ Qk (s, a, wk ) = wk φ∗ (s, a) ≈ Qk (s, a) . [sent-74, score-0.195]
</p><p>30 (3)  For the soft-greedy case, one option is to use an approximator that will obtain an approximation of an advantage function (see [9]):1 ˜ Ak (s, a, wk ) = wk  ˜ π ∗ (b|s, Ak−1 )φ∗ (s, b)  φ∗ (s, a) −  ≈ Ak (s, a) . [sent-75, score-0.195]
</p><p>31 For greedy approximate policy iteration in the general case, policy convergence is guaranteed only up to bounded sustained oscillation [2]. [sent-77, score-2.226]
</p><p>32 Optimistic variants can permit asymptotic convergence in parameters, although the corresponding policy can manifest sustained oscillation even then [8, 2, 7]. [sent-78, score-1.303]
</p><p>33 For the case of greedy approximate value iteration, a line of research has provided solid (although restrictive) conditions for the approximator class for having asymptotic parameter convergence (reviewed in, e. [sent-79, score-0.283]
</p><p>34 , [3]), whereas the question of policy convergence in these cases has been left quite open. [sent-81, score-0.746]
</p><p>35 In the rest of the paper, our focus will be on non-optimistic approximate policy iteration. [sent-82, score-0.747]
</p><p>36 , [9, 6, 4, 5]), the current policy on iteration k is explicitly represented using some differentiable stochastic policy class π(θ), the Gibbs policy with some basis φ being a common choice: π(a|s, θ) ∝ eθ  φ(s,a)  . [sent-85, score-2.274]
</p><p>37 In actorcritic (value-based policy gradient) methods that implement a policy gradient based approximate policy iteration scheme, the so-called ‘compatibility condition’ is fulﬁlled if the value function is ˜ approximated using (4) with φ∗ = φ and π(θk ) in place of π ∗ (Ak−1 ) (e. [sent-87, score-2.409]
</p><p>38 In this case, the value function parameter vector w becomes the natural gradient estimate η for the policy π(a|s, θ), leading to the natural actor-critic algorithm [13, 4]: η=w. [sent-90, score-0.908]
</p><p>39 3  The policy oscillation phenomenon  It is well known that greedy policy iteration can be non-convergent under approximations. [sent-95, score-2.173]
</p><p>40 The widely used projected equation approach can manifest convergence behavior that is complex and not well understood, including bounded but potentially severe sustained policy oscillations [7, 8, 18]. [sent-96, score-0.965]
</p><p>41 It is important to remember that sustained policy oscillation can take place even under (asymptotic) value function convergence (e. [sent-100, score-1.279]
</p><p>42 Continuously soft-greedy action selection (which is essentially a step toward the policy 1  The approach in [4] is needed to permit temporal difference evaluation in this case. [sent-104, score-0.883]
</p><p>43 A notable approach is introduced in [7] wherein it is also shown that the suboptimality bound for a converging policy sequence is much better. [sent-106, score-0.749]
</p><p>44 Interestingly, for the special case of Monte Carlo estimation of action values, it is also possible to establish convergence by solely modifying the exploration scheme, which is known as consistent exploration [23] or MCESP [24]. [sent-107, score-0.204]
</p><p>45 The setting likely to be the simplest possible in which oscillation occurs even with Monte Carlo policy evaluation is depicted in Figure 1. [sent-117, score-1.146]
</p><p>46 The actions al and ar are available in the decision states s1 and s2 . [sent-119, score-0.211]
</p><p>47 The only reward is obtained with the decision sequence (s1 , al ; s2 , ar ). [sent-121, score-0.227]
</p><p>48 Greedy value function methods that operate without state estimation will oscillate between the policies π(y1 ) = al and π(y1 ) = ar , excluding the exceptions mentioned above. [sent-122, score-0.259]
</p><p>49 The action values are approximated with Q(s1 , al ) = w1,l , Q(s2 , al ) = ˜ ˜ ˜ 0. [sent-131, score-0.203]
</p><p>50 5w2,l , Q(s3 , al ) = w2,l , Q(s1 , ar ) = w1,r , Q(s2 , ar ) = 0. [sent-133, score-0.254]
</p><p>51 The only reward is obtained with the decision sequence (s1 , al ; s2 , ar ; s3 , al ). [sent-136, score-0.293]
</p><p>52 A detailed description of the oscillation phenomenon can be found in [8, §6. [sent-141, score-0.532]
</p><p>53 4  Approximations and attractive stochastic policies  In this section, we brieﬂy and informally examine how policy oscillation arises in the examples in Section 3. [sent-145, score-1.261]
</p><p>54 In all cases, oscillation is caused by the presence of an attractive stochastic policy, these attractors being induced by approximations. [sent-146, score-0.561]
</p><p>55 1), the policy class is incapable of representing differing action distributions for the same observation with differing histories. [sent-148, score-0.82]
</p><p>56 This makes the optimal sequence (y1 , al ; y1 , ar ) inexpressible for deterministic policies, whereas a stochastic policy can still emit it every now and then by chance. [sent-149, score-0.948]
</p><p>57 3, the same situation is arrived at due to the insufﬁcient capacity of the approximator: the speciﬁed value function approximator cannot express such value estimates that 4  would lead to an implicit greedy policy that attains the optimal sequence (s1 , al ; s2 , ar ; s3 , al ). [sent-151, score-1.189]
</p><p>58 Generally speaking, in these cases, oscillation follows from a mismatch between the main policy class and the exploration policy class: stochastic exploration can occasionally reach the reward, but the deterministic main policy is incapable of exploiting this opportunity. [sent-152, score-2.764]
</p><p>59 , it gains variance reduction in policy evaluation by making the Markov assumption. [sent-157, score-0.748]
</p><p>60 Generally speaking, oscillation results in this case from perceived but non-existent improvement opportunities that vanish once an attempt is made to exploit them. [sent-163, score-0.466]
</p><p>61 In summary, stochastic policies can become attractive due to deterministically unreachable or completely non-existing improvement opportunities that appear in the value function. [sent-165, score-0.248]
</p><p>62 5  Policy oscillation as sustained overshooting  In this section, we focus more carefully on how attractive stochastic policies lead to sustained policy oscillation when viewed within the policy gradient framework. [sent-167, score-2.712]
</p><p>63 We begin by looking at a natural ˜ actor-critic algorithm that uses the Gibbs policy class (5). [sent-168, score-0.748]
</p><p>64 We iterate by fully estimating Ak in (4) for the current policy π(θk ), as shown in [4], and then a gradient update is performed using (6): θk+1 = θk + αηk . [sent-169, score-0.846]
</p><p>65 (7)  Now let us consider some policy π(θk ) from such a policy sequence generated by (7) and denote the ˜ corresponding value function estimate by Ak and the natural gradient estimate by ηk . [sent-170, score-1.576]
</p><p>66 It is shown in [13] that taking a very long step in the direction of the natural gradient ηk will approach in the limit ˜ a greedy update (1) for the value function Ak : lim  ˜ π(a|s, θk + αηk ) = π ∗ (a|s, Ak ) ,  α → ∞, θk → ∞, η = 0, ∀s, a . [sent-171, score-0.333]
</p><p>67 (8)  The resulting policy will have the form π(a|s, θk + αηk ) ∝ eθk φ(s,a)+αηk φ(s,a) . [sent-172, score-0.708]
</p><p>68 Thus, this type of a greedy update is a special case of a natural gradient update in which the step-size approaches inﬁnity. [sent-174, score-0.301]
</p><p>69 Thus, natural gradient based policy iteration using such a very large but constant step-size does not approach greedy value function based policy iteration after the ﬁrst such iteration. [sent-176, score-1.904]
</p><p>70 Little is needed, however, to make the equality apply in the case of full policy iteration. [sent-177, score-0.708]
</p><p>71 Let π(θk ) denote the kth policy obtained from (7) using the step-sizes α[0,k−1] and natural gradients η[0,k−1] . [sent-180, score-0.77]
</p><p>72 Let π ∗ (wk ) denote the kth policy obtained from (1) with inﬁnitely small ˜ added softness and using a value function (4), with φ∗ = φ and A(w0 ) being evaluated for π(θ0 ). [sent-181, score-0.755]
</p><p>73 In the following, a more practical alternative is discussed that both avoids the related numerical issues and that allows gradual interpolation back toward conventional policy gradient iteration. [sent-194, score-0.87]
</p><p>74 The constraint affects the policy π(θk+1 ) only when θk + αηk > c, in which case the magnitude of the parameter vector is scaled down with a factor τc so that it becomes equal to c. [sent-209, score-0.733]
</p><p>75 This has a diminishing effect on the resulting policy as c → ∞ because the Gibbs distribution becomes increasingly insensitive to scaling of the parameter vector when its magnitude approaches inﬁnity: lim  π(τc θ) = π(θ) ,  ∀τc , θ such that τc θ → ∞, θ → ∞ . [sent-210, score-0.762]
</p><p>76 If the soft-greedy method uses (4) for policy evaluation, then exact equivalence in the limit is obtained when c → ∞ while maintaining α/c → ∞. [sent-212, score-0.728]
</p><p>77 Lowering α interpolates toward a conventional natural gradient method. [sent-213, score-0.202]
</p><p>78 Greedy policy iteration searches in the space of deterministic policies. [sent-215, score-0.844]
</p><p>79 As noted, the sequence of greedy policies that is generated by such a process can be approximated arbitrarily closely with the Gibbs policy class (2) with τ → 0. [sent-216, score-0.927]
</p><p>80 For this class, the parameters of all deterministic policies lie at inﬁnity in different directions in the parameter space, whereas stochastic policies are obtained with ﬁnite parameter values (except for vanishingly narrow directions along diagonals). [sent-217, score-0.228]
</p><p>81 Based on Theorems 1 and 2, we observe that the policy sequence that results from these jumps can be approximated arbitrarily closely with a natural actor-critic method using very large step-sizes. [sent-219, score-0.771]
</p><p>82 Although we ignore the latter requirement, we note that the former requirement is satisﬁed when only deterministic attractors exist in the Gibbs policy space. [sent-223, score-0.881]
</p><p>83 However, when these conditions do not hold and there is an attractor in the policy space that corresponds to a stochastic policy, there is a ﬁnite attractor in the parameter space that resides inside the ∞-radius sphere. [sent-228, score-0.845]
</p><p>84 In essence, the sustained policy oscillation that results from using very large step-sizes or greedy updates in the latter two cases (2. [sent-246, score-1.36]
</p><p>85 2b) is caused by sustained overshooting over the ﬁnite attractor in the policy parameter space. [sent-248, score-0.915]
</p><p>86 This is because whatever initial improvement speed can be achieved with the latter due to greedy updates, the same speed can be also achieved with the former using the same basis together with very long steps and constraining. [sent-250, score-0.286]
</p><p>87 This effectively corresponds to an attempt to exploit whatever remains of the special structure of a Markovian problem, making the use of a very large α in constrained policy improvement analogous to using a small λ in policy evaluation. [sent-251, score-1.483]
</p><p>88 6  Empirical results  In this section, we apply several variants of the natural actor-critic algorithm and some greedy policy iteration algorithms to the Tetris problem using the standard features from [12]. [sent-253, score-0.973]
</p><p>89 For policy improvement, we use the original natural actor-critic (NAC) from [4], a constrained one (CNAC) that uses (11) and a very large α, and a soft-greedy policy iteration algorithm (SGPI) that uses (2). [sent-254, score-1.559]
</p><p>90 For policy evaluation, we use LSPE [28] and an SVD-based batch solver (pinv). [sent-255, score-0.708]
</p><p>91 5I and the policy was updated after every 100th episode. [sent-257, score-0.708]
</p><p>92 We used a simple initial policy (θmaxh = θholes = −3) that scores around 20 points. [sent-259, score-0.708]
</p><p>93 1 shows that with soft-greedy policy iteration (SGPI), it is in fact possible to avoid policy degradation by using a suitable amount of softening. [sent-281, score-1.577]
</p><p>94 The algorithm can indeed emulate greedy updates (SGPI) and the associated policy degradation. [sent-284, score-0.852]
</p><p>95 Currently, we expect that the policy oscillation or chattering phenomenon is not the main cause for neither policy degradation nor stagnation in this problem. [sent-303, score-2.091]
</p><p>96 Instead, it seems that, for both greedy and gradient approaches, the explanation is related to numerical instabilities that stem possibly both from the involved estimators and from insufﬁcient exploration. [sent-304, score-0.237]
</p><p>97 Approximate policy iteration: A survey and some new methods. [sent-346, score-0.708]
</p><p>98 Temporal differences-based policy iteration and applications in neurodynamic programming. [sent-377, score-0.811]
</p><p>99 Q-learning and enhanced policy iteration in discounted dynamic programming. [sent-468, score-0.872]
</p><p>100 Least squares policy evaluation algorithms with linear function approximac tion. [sent-475, score-0.748]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.708), ('oscillation', 0.398), ('tetris', 0.17), ('cnac', 0.136), ('nac', 0.134), ('phenomenon', 0.134), ('greedy', 0.122), ('sgpi', 0.119), ('sustained', 0.11), ('iteration', 0.103), ('gradient', 0.095), ('ar', 0.094), ('reinforcement', 0.082), ('policies', 0.074), ('qk', 0.07), ('wk', 0.068), ('al', 0.066), ('ak', 0.064), ('dynamic', 0.061), ('attractors', 0.06), ('exploration', 0.059), ('approximator', 0.059), ('degradation', 0.058), ('oscillations', 0.055), ('programming', 0.055), ('bertsekas', 0.052), ('chattering', 0.051), ('nity', 0.051), ('action', 0.048), ('former', 0.047), ('stochastic', 0.047), ('improvement', 0.046), ('attractor', 0.045), ('reward', 0.042), ('observability', 0.041), ('suboptimality', 0.041), ('carlo', 0.04), ('natural', 0.04), ('evaluation', 0.04), ('monte', 0.039), ('approximate', 0.039), ('convergence', 0.038), ('toward', 0.038), ('susceptible', 0.037), ('markovian', 0.036), ('gibbs', 0.035), ('lspe', 0.034), ('stagnation', 0.034), ('thiery', 0.034), ('landscape', 0.034), ('attractive', 0.034), ('deterministic', 0.033), ('requirement', 0.033), ('optima', 0.032), ('speaking', 0.032), ('aalto', 0.03), ('busoniu', 0.03), ('farias', 0.03), ('icga', 0.03), ('maxb', 0.03), ('overshooting', 0.03), ('lim', 0.029), ('conventional', 0.029), ('severe', 0.027), ('lowering', 0.027), ('grossly', 0.027), ('manifest', 0.027), ('temporal', 0.027), ('actions', 0.026), ('finland', 0.026), ('value', 0.025), ('speed', 0.025), ('decision', 0.025), ('keeps', 0.025), ('magnitude', 0.025), ('dominating', 0.024), ('incapable', 0.024), ('implicit', 0.024), ('discount', 0.023), ('casting', 0.023), ('massachusetts', 0.023), ('approximated', 0.023), ('update', 0.022), ('caused', 0.022), ('kth', 0.022), ('updates', 0.022), ('opportunities', 0.022), ('permit', 0.022), ('understanding', 0.021), ('established', 0.021), ('whatever', 0.021), ('optimistic', 0.021), ('score', 0.021), ('fully', 0.021), ('athena', 0.02), ('differing', 0.02), ('mismatch', 0.02), ('st', 0.02), ('equivalence', 0.02), ('explanation', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="16-tfidf-1" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>2 0.36744022 <a title="16-tfidf-2" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>3 0.33889592 <a title="16-tfidf-3" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>Author: Amir-massoud Farahmand</p><p>Abstract: Many practitioners of reinforcement learning problems have observed that oftentimes the performance of the agent reaches very close to the optimal performance even though the estimated (action-)value function is still far from the optimal one. The goal of this paper is to explain and formalize this phenomenon by introducing the concept of the action-gap regularity. As a typical result, we prove that for an ˆ agent following the greedy policy π with respect to an action-value function Q, the ˆ ˆ ˆ performance loss E V ∗ (X) − V π (X) is upper bounded by O( Q − Q∗ 1+ζ ), ∞ in which ζ ≥ 0 is the parameter quantifying the action-gap regularity. For ζ > 0, our results indicate smaller performance loss compared to what previous analyses had suggested. Finally, we show how this regularity affects the performance of the family of approximate value iteration algorithms. 1</p><p>4 0.27795753 <a title="16-tfidf-4" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an inﬁnite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms. 1</p><p>5 0.26604065 <a title="16-tfidf-5" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>Author: Zhan Lim, Lee Sun, Daniel J. Hsu</p><p>Abstract: POMDP planning faces two major computational challenges: large state spaces and long planning horizons. The recently introduced Monte Carlo Value Iteration (MCVI) can tackle POMDPs with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning horizons. This paper presents Macro-MCVI, which extends MCVI by exploiting macro-actions for temporal abstraction. We provide sufﬁcient conditions for Macro-MCVI to inherit the good theoretical properties of MCVI. Macro-MCVI does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice. Experiments show that Macro-MCVI substantially improves the performance of MCVI with suitable macro-actions. 1</p><p>6 0.20465659 <a title="16-tfidf-6" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>7 0.20337938 <a title="16-tfidf-7" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>8 0.19433522 <a title="16-tfidf-8" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>9 0.19224146 <a title="16-tfidf-9" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>10 0.16505949 <a title="16-tfidf-10" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>11 0.14272735 <a title="16-tfidf-11" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>12 0.1204536 <a title="16-tfidf-12" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>13 0.10773539 <a title="16-tfidf-13" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>14 0.10116088 <a title="16-tfidf-14" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>15 0.10024027 <a title="16-tfidf-15" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>16 0.09700314 <a title="16-tfidf-16" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>17 0.095986255 <a title="16-tfidf-17" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>18 0.092991635 <a title="16-tfidf-18" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>19 0.08675988 <a title="16-tfidf-19" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>20 0.082179412 <a title="16-tfidf-20" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, -0.243), (2, 0.127), (3, 0.297), (4, -0.387), (5, 0.11), (6, -0.004), (7, 0.047), (8, -0.079), (9, -0.015), (10, 0.038), (11, 0.041), (12, -0.085), (13, -0.081), (14, -0.15), (15, -0.093), (16, 0.086), (17, 0.122), (18, 0.039), (19, 0.035), (20, -0.073), (21, -0.071), (22, 0.061), (23, 0.014), (24, -0.109), (25, -0.062), (26, 0.028), (27, 0.026), (28, 0.042), (29, 0.036), (30, 0.078), (31, 0.049), (32, 0.083), (33, 0.024), (34, -0.005), (35, 0.017), (36, -0.037), (37, 0.066), (38, 0.0), (39, 0.031), (40, -0.048), (41, 0.019), (42, -0.022), (43, 0.003), (44, -0.017), (45, 0.067), (46, -0.068), (47, 0.003), (48, -0.01), (49, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98285937 <a title="16-lsi-1" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>2 0.89482611 <a title="16-lsi-2" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>3 0.84663063 <a title="16-lsi-3" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>Author: Tingting Zhao, Hirotaka Hachiya, Gang Niu, Masashi Sugiyama</p><p>Abstract: Policy gradient is a useful model-free reinforcement learning approach, but it tends to suffer from instability of gradient estimates. In this paper, we analyze and improve the stability of policy gradient methods. We ﬁrst prove that the variance of gradient estimates in the PGPE (policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption. We then derive the optimal baseline for PGPE, which contributes to further reducing the variance. We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates. Finally, we demonstrate the usefulness of the improved PGPE method through experiments. 1</p><p>4 0.79465467 <a title="16-lsi-4" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>Author: Oliver B. Kroemer, Jan R. Peters</p><p>Abstract: In this paper, we consider the problem of policy evaluation for continuousstate systems. We present a non-parametric approach to policy evaluation, which uses kernel density estimation to represent the system. The true form of the value function for this model can be determined, and can be computed using Galerkin’s method. Furthermore, we also present a uniﬁed view of several well-known policy evaluation methods. In particular, we show that the same Galerkin method can be used to derive Least-Squares Temporal Diﬀerence learning, Kernelized Temporal Diﬀerence learning, and a discrete-state Dynamic Programming solution, as well as our proposed method. In a numerical evaluation of these algorithms, the proposed approach performed better than the other methods. 1</p><p>5 0.79021937 <a title="16-lsi-5" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>Author: Javad Azimi, Alan Fern, Xiaoli Z. Fern</p><p>Abstract: Budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs. Typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments, which fail to capture important aspects of many real-world problems. This paper deﬁnes a novel problem formulation with the following important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic experiment durations; and 3) placing constraints on both the total number of experiments and the total experimental time. We develop both ofﬂine and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks. The results show that our algorithms produce highly effective schedules compared to natural baselines. 1</p><p>6 0.78958368 <a title="16-lsi-6" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>7 0.76377422 <a title="16-lsi-7" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>8 0.74138778 <a title="16-lsi-8" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>9 0.69982362 <a title="16-lsi-9" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>10 0.63924462 <a title="16-lsi-10" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>11 0.58742809 <a title="16-lsi-11" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>12 0.51993865 <a title="16-lsi-12" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>13 0.46193838 <a title="16-lsi-13" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>14 0.41077542 <a title="16-lsi-14" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>15 0.40225232 <a title="16-lsi-15" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>16 0.38222358 <a title="16-lsi-16" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>17 0.34341979 <a title="16-lsi-17" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>18 0.33394337 <a title="16-lsi-18" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>19 0.33257326 <a title="16-lsi-19" href="./nips-2011-Speedy_Q-Learning.html">268 nips-2011-Speedy Q-Learning</a></p>
<p>20 0.31711373 <a title="16-lsi-20" href="./nips-2011-Learning_to_Agglomerate_Superpixel_Hierarchies.html">155 nips-2011-Learning to Agglomerate Superpixel Hierarchies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.02), (4, 0.035), (11, 0.019), (20, 0.039), (23, 0.186), (26, 0.026), (31, 0.112), (33, 0.014), (43, 0.045), (45, 0.122), (57, 0.04), (65, 0.014), (74, 0.078), (83, 0.04), (99, 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92285681 <a title="16-lda-1" href="./nips-2011-On_Strategy_Stitching_in_Large_Extensive_Form_Multiplayer_Games.html">196 nips-2011-On Strategy Stitching in Large Extensive Form Multiplayer Games</a></p>
<p>Author: Richard G. Gibson, Duane Szafron</p><p>Abstract: Computing a good strategy in a large extensive form game often demands an extraordinary amount of computer memory, necessitating the use of abstraction to reduce the game size. Typically, strategies from abstract games perform better in the real game as the granularity of abstraction is increased. This paper investigates two techniques for stitching a base strategy in a coarse abstraction of the full game tree, to expert strategies in ﬁne abstractions of smaller subtrees. We provide a general framework for creating static experts, an approach that generalizes some previous strategy stitching efforts. In addition, we show that static experts can create strong agents for both 2-player and 3-player Leduc and Limit Texas Hold’em poker, and that a speciﬁc class of static experts can be preferred among a number of alternatives. Furthermore, we describe a poker agent that used static experts and won the 3-player events of the 2010 Annual Computer Poker Competition.</p><p>2 0.90383041 <a title="16-lda-2" href="./nips-2011-Continuous-Time_Regression_Models_for_Longitudinal_Networks.html">62 nips-2011-Continuous-Time Regression Models for Longitudinal Networks</a></p>
<p>Author: Duy Q. Vu, David Hunter, Padhraic Smyth, Arthur U. Asuncion</p><p>Abstract: The development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science. Leveraging ideas from survival and event history analysis, we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefﬁcients. We also develop an efﬁcient inference scheme that allows our approach to scale to large networks. On synthetic and real-world data, empirical results demonstrate that the proposed inference approach can accurately estimate the coefﬁcients of the regression model, which is useful for interpreting the evolution of the network; furthermore, the learned model has systematically better predictive performance compared to standard baseline methods.</p><p>same-paper 3 0.83320898 <a title="16-lda-3" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>4 0.73996818 <a title="16-lda-4" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>5 0.73398054 <a title="16-lda-5" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>Author: Alex K. Susemihl, Ron Meir, Manfred Opper</p><p>Abstract: Bayesian ﬁltering of stochastic stimuli has received a great deal of attention recently. It has been applied to describe the way in which biological systems dynamically represent and make decisions about the environment. There have been no exact results for the error in the biologically plausible setting of inference on point process, however. We present an exact analysis of the evolution of the meansquared error in a state estimation task using Gaussian-tuned point processes as sensors. This allows us to study the dynamics of the error of an optimal Bayesian decoder, providing insights into the limits obtainable in this task. This is done for Markovian and a class of non-Markovian Gaussian processes. We ﬁnd that there is an optimal tuning width for which the error is minimized. This leads to a characterization of the optimal encoding for the setting as a function of the statistics of the stimulus, providing a mathematically sound primer for an ecological theory of sensory processing. 1</p><p>6 0.73345149 <a title="16-lda-6" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>7 0.72936612 <a title="16-lda-7" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>8 0.7248376 <a title="16-lda-8" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>9 0.72401655 <a title="16-lda-9" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>10 0.72204947 <a title="16-lda-10" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>11 0.71955174 <a title="16-lda-11" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>12 0.71905756 <a title="16-lda-12" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>13 0.71873122 <a title="16-lda-13" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>14 0.71756589 <a title="16-lda-14" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>15 0.71612763 <a title="16-lda-15" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>16 0.71601284 <a title="16-lda-16" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>17 0.71503258 <a title="16-lda-17" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>18 0.71261424 <a title="16-lda-18" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>19 0.71188831 <a title="16-lda-19" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>20 0.71139306 <a title="16-lda-20" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
