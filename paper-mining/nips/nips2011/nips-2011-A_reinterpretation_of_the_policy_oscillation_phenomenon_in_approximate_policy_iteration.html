<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-16" href="#">nips2011-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</h1>
<br/><p>Source: <a title="nips-2011-16-pdf" href="http://papers.nips.cc/paper/4274-a-reinterpretation-of-the-policy-oscillation-phenomenon-in-approximate-policy-iteration.pdf">pdf</a></p><p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiÔ¨Åcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>Reference: <a title="nips-2011-16-reference" href="../nips2011_reference/nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('policy', 0.755), ('oscil', 0.392), ('tet', 0.182), ('cnac', 0.146), ('phenomenon', 0.143), ('nac', 0.132), ('sgpi', 0.127), ('greedy', 0.124), ('sustain', 0.096), ('grady', 0.095), ('reinforc', 0.085), ('attract', 0.077), ('qk', 0.075), ('wk', 0.073), ('ak', 0.066), ('it', 0.062), ('dynam', 0.061), ('al', 0.056), ('bertseka', 0.056), ('nity', 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="16-tfidf-1" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>2 0.42434558 <a title="16-tfidf-2" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>3 0.36553848 <a title="16-tfidf-3" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>4 0.29722515 <a title="16-tfidf-4" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>5 0.26450804 <a title="16-tfidf-5" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>6 0.26042411 <a title="16-tfidf-6" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>7 0.24183625 <a title="16-tfidf-7" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>8 0.21308164 <a title="16-tfidf-8" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>9 0.1987793 <a title="16-tfidf-9" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>10 0.19809657 <a title="16-tfidf-10" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>11 0.17933075 <a title="16-tfidf-11" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>12 0.13690101 <a title="16-tfidf-12" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>13 0.13560624 <a title="16-tfidf-13" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>14 0.12529686 <a title="16-tfidf-14" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>15 0.12476536 <a title="16-tfidf-15" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>16 0.11970967 <a title="16-tfidf-16" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>17 0.11690909 <a title="16-tfidf-17" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>18 0.11430388 <a title="16-tfidf-18" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>19 0.10800135 <a title="16-tfidf-19" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>20 0.10323198 <a title="16-tfidf-20" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.147), (2, -0.243), (3, -0.394), (4, 0.305), (5, -0.044), (6, 0.016), (7, -0.058), (8, 0.093), (9, -0.041), (10, 0.113), (11, -0.081), (12, -0.028), (13, 0.06), (14, 0.076), (15, 0.057), (16, -0.201), (17, 0.153), (18, -0.02), (19, 0.114), (20, -0.097), (21, 0.014), (22, -0.056), (23, -0.113), (24, 0.062), (25, 0.049), (26, -0.046), (27, 0.01), (28, -0.038), (29, -0.018), (30, 0.027), (31, 0.004), (32, -0.018), (33, 0.016), (34, 0.092), (35, 0.01), (36, -0.015), (37, 0.003), (38, -0.038), (39, -0.054), (40, -0.043), (41, 0.009), (42, 0.01), (43, -0.109), (44, 0.005), (45, 0.023), (46, -0.04), (47, -0.019), (48, -0.037), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97022361 <a title="16-lsi-1" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>2 0.87784338 <a title="16-lsi-2" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>3 0.86142761 <a title="16-lsi-3" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>4 0.85709333 <a title="16-lsi-4" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>5 0.80524307 <a title="16-lsi-5" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>6 0.78768027 <a title="16-lsi-6" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>7 0.75356245 <a title="16-lsi-7" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>8 0.74671352 <a title="16-lsi-8" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>9 0.73688239 <a title="16-lsi-9" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>10 0.65331799 <a title="16-lsi-10" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>11 0.55152941 <a title="16-lsi-11" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>12 0.51144016 <a title="16-lsi-12" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>13 0.48168442 <a title="16-lsi-13" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>14 0.48114455 <a title="16-lsi-14" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>15 0.40940025 <a title="16-lsi-15" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>16 0.39406496 <a title="16-lsi-16" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>17 0.3910785 <a title="16-lsi-17" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>18 0.38489622 <a title="16-lsi-18" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>19 0.38233823 <a title="16-lsi-19" href="./nips-2011-Clustering_via_Dirichlet_Process_Mixture_Models_for_Portable_Skill_Discovery.html">52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</a></p>
<p>20 0.31229565 <a title="16-lsi-20" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.047), (16, 0.179), (22, 0.054), (36, 0.039), (53, 0.015), (55, 0.273), (65, 0.096), (68, 0.158), (79, 0.02), (95, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90695053 <a title="16-lda-1" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>2 0.89910626 <a title="16-lda-2" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>3 0.87342966 <a title="16-lda-3" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>4 0.87208539 <a title="16-lda-4" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>5 0.87109917 <a title="16-lda-5" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>6 0.86845803 <a title="16-lda-6" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>7 0.86785805 <a title="16-lda-7" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>8 0.86719322 <a title="16-lda-8" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>9 0.86700135 <a title="16-lda-9" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>10 0.86700028 <a title="16-lda-10" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>11 0.86695576 <a title="16-lda-11" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>12 0.8665061 <a title="16-lda-12" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>13 0.86443782 <a title="16-lda-13" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>14 0.86368275 <a title="16-lda-14" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>15 0.86357278 <a title="16-lda-15" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>16 0.86240387 <a title="16-lda-16" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>17 0.86197072 <a title="16-lda-17" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>18 0.86171114 <a title="16-lda-18" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>19 0.86072832 <a title="16-lda-19" href="./nips-2011-Composite_Multiclass_Losses.html">59 nips-2011-Composite Multiclass Losses</a></p>
<p>20 0.86002964 <a title="16-lda-20" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
