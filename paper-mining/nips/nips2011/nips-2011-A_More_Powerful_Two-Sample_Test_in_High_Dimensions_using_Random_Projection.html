<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-9" href="#">nips2011-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</h1>
<br/><p>Source: <a title="nips-2011-9-pdf" href="http://papers.nips.cc/paper/4260-a-more-powerful-two-sample-test-in-high-dimensions-using-random-projection.pdf">pdf</a></p><p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>Reference: <a title="nips-2011-9-reference" href="../nips2011_reference/nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. [sent-5, score-0.366]
</p><p>2 Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. [sent-6, score-0.394]
</p><p>3 Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. [sent-7, score-0.569]
</p><p>4 Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. [sent-8, score-0.242]
</p><p>5 Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. [sent-9, score-0.28]
</p><p>6 1  Introduction  Two-sample hypothesis tests are concerned with the question of whether two samples of data are generated from the same distribution. [sent-10, score-0.154]
</p><p>7 Such tests are among the most widely used inference procedures in treatment-control studies in science and engineering [1]. [sent-11, score-0.156]
</p><p>8 In transcriptomics, for instance, p gene expression measures on the order of hundreds or thousands may be used to investigate differences between two biological conditions, and it is often difﬁcult to obtain sample sizes n1 and n2 larger than several dozen in each condition. [sent-21, score-0.188]
</p><p>9 Likewise, there has been growing interest in developing testing procedures that are better suited to deal with the effects of dimension [e. [sent-23, score-0.221]
</p><p>10 manner from p-dimensional multivariate normal distributions N (µ1 , Σ) and N (µ2 , Σ) respectively, where the mean vectors µ1 , µ2 ∈ Rp and covariance matrix Σ 0 are all ﬁxed and unknown. [sent-36, score-0.192]
</p><p>11 The hypothesis testing problem of interest is H0 : µ1 = µ2 versus H1 : µ1 = µ2 . [sent-37, score-0.195]
</p><p>12 (1)  The most well-known test statistic for this problem is the Hotelling T 2 statistic, deﬁned by T 2 :=  n1 n2 ¯ ¯ ¯ ¯ (X − Y ) Σ−1 (X − Y ), n1 + n2 1  (2)  n1 1 ¯ ¯ where X := n1 j=1 Xj and Y := 1 covariance matrix, given by Σ := n n := n1 + n2 − 2. [sent-38, score-0.313]
</p><p>13 1 n2  n2 j=1  n1 j=1 (Xj  Yj are the sample means, and Σ is the pooled sample 1 ¯ ¯ ¯ ¯ − X)(Xj − X) + n n2 (Yj − Y )(Yj − Y ) , with j=1  When p > n, the matrix Σ is singular, and the Hotelling test is not well-deﬁned. [sent-39, score-0.166]
</p><p>14 This behavior was demonstrated in a seminal paper of Bai and Saranadasa [6] (or BS for short), who studied the performance of the Hotelling test under (p, n) → ∞ with p/n → 1 − , and showed that the asymptotic power of the test suffers for small values of > 0. [sent-41, score-0.55]
</p><p>15 In this paper, we propose a new test statistic for the two-sample test of means with multivariate normal data, applicable when p ≥ n/2. [sent-45, score-0.433]
</p><p>16 We provide an explicit asymptotic power function for our test with (p, n) → ∞, and show that under certain conditions, our test has greater asymptotic power than other state-of-the-art tests. [sent-46, score-0.938]
</p><p>17 In addition to its advantage in terms of asymptotic power, our procedure speciﬁes exact level-α critical values for multivariate normal data, whereas competing procedures offer only approximate level-α critical values. [sent-48, score-0.507]
</p><p>18 Furthermore, our experiments in Section 4 suggest that the critical values of our test may also be more robust than those of competing tests. [sent-49, score-0.204]
</p><p>19 In Section 2, we provide background on hypothesis testing and describe our testing procedure. [sent-52, score-0.282]
</p><p>20 1 provides an asymptotic power function, and Theorems 2 and 3 in Sections 3. [sent-55, score-0.324]
</p><p>21 4 give sufﬁcient conditions for achieving greater power than state-of-the-art tests in the sense of asymptotic relative efﬁciency. [sent-57, score-0.544]
</p><p>22 Lastly, we study a high-dimensional gene expression dataset involving the discrimination of different cancer types, demonstrating that our test’s false positive rate is reliable in practice. [sent-59, score-0.439]
</p><p>23 For a positive-deﬁnite covariance matrix Σ, −1/2 −1/2 let Dσ := diag(Σ), and deﬁne the associated correlation matrix R := Dσ ΣDσ . [sent-66, score-0.199]
</p><p>24 2  Background and random projection method  For the remainder of the paper, we retain the set-up for the two-sample test of means (1) with Gaussian data, assuming throughout that p ≥ n/2, and n = n1 + n2 − 2. [sent-70, score-0.16]
</p><p>25 The primary focus of our results will be on the comparison of power between test statistics, and here we give precise meaning to this notion. [sent-72, score-0.26]
</p><p>26 When testing a null hypothesis H0 versus an alternative hypothesis H1 , a procedure based on a test statistic T speciﬁes a critical value, such that H0 is rejected if T exceeds that critical value, and H0 is accepted otherwise. [sent-73, score-0.624]
</p><p>27 A test is said to have level α if the probability of committing a type I error is at most α. [sent-76, score-0.155]
</p><p>28 Finally, at a given level α, the power of a test is the probability of rejecting H0 under H1 , i. [sent-77, score-0.375]
</p><p>29 When evaluating testing procedures at a given level α, we seek to identify the one with the greatest power. [sent-80, score-0.214]
</p><p>30 We shall see later that this limited use of covariance structure sacriﬁces power when the data exhibit non-trivial correlation. [sent-86, score-0.226]
</p><p>31 In this regard, our procedure is motivated by the idea that covariance structure may be used more effectively by testing with projected samples in a space of lower dimension. [sent-87, score-0.316]
</p><p>32 Second, the Hotelling T 2 test is applied to a new hypothesis testing problem, H0,proj versus H1,proj , in the projected space. [sent-101, score-0.384]
</p><p>33 A decision is then pulled back to the original problem by simply rejecting H0 whenever the Hotelling test rejects H0,proj . [sent-102, score-0.186]
</p><p>34 211] for applying the Hotelling T 2 procedure to the following new two-sample problem in the projected space Rk : H0,proj : Pk µ1 = Pk µ2 versus H1,proj : Pk µ1 = Pk µ2 . [sent-119, score-0.15]
</p><p>35 (3)  For this projected problem, the Hotelling test statistic takes the form2 2 Tk :=  n1 n2 ¯ n1 +n2 (X  ¯ ¯ ¯ − Y ) Pk (Pk ΣPk )−1 Pk (X − Y ),  ¯ ¯ where X, Y , and Σ are as deﬁned in Section 1. [sent-120, score-0.31]
</p><p>36 2 It is a basic fact about the classical Hotelling test that rejecting H0,proj when Tk ≥ tα is a level-α 2 test for the projected problem (3) (e. [sent-122, score-0.375]
</p><p>37 Therefore, rejecting the original 2 H0 when Tk ≥ tα is also a level α test for the original problem (1). [sent-127, score-0.228]
</p><p>38 Likewise, we deﬁne this as the condition for rejecting H0 at level α in our procedure for (1). [sent-128, score-0.195]
</p><p>39 Projected Hotelling test at level α for problem (1). [sent-144, score-0.155]
</p><p>40 1  Asymptotic power function  As is standard in high-dimensional asymptotics, we will consider a sequence of hypothesis testing problems indexed by n, allowing the dimension p, mean vectors µ1 and µ2 and covariance matrix Σ to implicitly vary as functions of n, with n → ∞. [sent-147, score-0.472]
</p><p>41 We also make another type of asymptotic assumption, known as a local alternative [18, p. [sent-148, score-0.177]
</p><p>42 The idea lying behind a local alternative assumption is that if the difﬁculty of discriminating between H0 and H1 is “held ﬁxed” with respect to n, then it is often the case that most testing procedures have power tending to 1 under H1 as n → ∞. [sent-150, score-0.349]
</p><p>43 In such a situation, it is not possible to tell if one test has greater asymptotic power than another. [sent-151, score-0.501]
</p><p>44 Consequently, it is standard to derive asymptotic power results under the extra condition that H0 and H1 become harder to distinguish as n grows. [sent-152, score-0.381]
</p><p>45 This theoretical device aids in identifying the conditions under which one test is more powerful than another. [sent-153, score-0.171]
</p><p>46 To set the notation for Theorem 1, it is important to notice that each time the procedure ( ) is im2 plemented, a draw of Pk induces a new test statistic Tk . [sent-158, score-0.283]
</p><p>47 To make this dependence clear, recall θ := (δ, Σ), and let β(θ; Pk ) denote the exact (non-asymptotic) power function of our level-α test for problem (1), induced by a draw of Pk , as in ( ). [sent-159, score-0.26]
</p><p>48 To the best of our knowledge, 4  these works represent the state of the art3 among tests for problem (1) with a known asymptotic power function under (p, n) → ∞. [sent-172, score-0.42]
</p><p>49 From Theorem 1, the asymptotic power function of our random projection-based test at level α is √ √ βRP (θ; Pk ) := Φ −z1−α + b(1−b) n ∆2 . [sent-173, score-0.479]
</p><p>50 (5) k 2 The asymptotic power functions for the CQ and SD testing procedures at level α are βCQ (θ) := Φ −z1−α +  b(1−b) n δ 2 2 √ | | 2 ||Σ||F  ,  and  βSD (θ) := Φ −z1−α +  −1 b(1−b) n δ Dσ δ √ ||R||F | | 2  . [sent-174, score-0.538]
</p><p>51 The functions βCQ and βSD are derived under local alternatives and asymptotic assumptions that are similar to the ones used here to obtain βRP . [sent-176, score-0.177]
</p><p>52 A standard method of comparing asymptotic power functions under local alternatives is through the concept of asymptotic relative efﬁciency (ARE) e. [sent-178, score-0.527]
</p><p>53 To compare power between tests, the ARE is simply deﬁned via the ratio of such terms. [sent-183, score-0.147]
</p><p>54 k k ||R|| | | F  F  Whenever the ARE is less than 1, our procedure is considered to have greater asymptotic power than the competing test—with our advantage being greater for smaller values of the ARE. [sent-185, score-0.548]
</p><p>55 3  Comparison with Chen and Qin [9]  The next result compares the asymptotic power of our projection-based test with that of Chen and Qin [9]. [sent-198, score-0.437]
</p><p>56 The choice of 1 = 1 below (and in Theorem 3) is the reference for equal asymptotic performance, with smaller values of 1 corresponding to better performance of random projection. [sent-199, score-0.177]
</p><p>57 As such, it is reasonable to interpret this ratio as a measure of 3 Two other high-dimensional tests have been proposed in older works [6, 19, 20] that lead to the asymptotic power function βCQ , but under more restrictive assumptions. [sent-206, score-0.42]
</p><p>58 The ratio tr(Σ)2 / |||Σ|||F can also be viewed as measuring the decay rate of the 2 spectrum of Σ, with tr(Σ)2 |||Σ|||F p indicating rapid decay. [sent-209, score-0.208]
</p><p>59 4  Comparison with Srivastava and Du [7, 8]  We now turn to comparison of asymptotic power with the test of Srivastava and Du (SD). [sent-212, score-0.437]
</p><p>60 Unlike the comparison with the CQ test, the correlation matrix R plays a large role in determining the relative efﬁciency between our procedure and the SD test. [sent-218, score-0.168]
</p><p>61 Inspection of the SD test 2 statistic in [7] shows that it does not make any essential use of correlation. [sent-222, score-0.234]
</p><p>62 By contrast, our Tk statistic does take correlation into account, and so it is understandable that correlated data enhance the performance of our test relative to SD. [sent-223, score-0.326]
</p><p>63 We could even let the correlation ρ decay at a rate of n−q with q ∈ (0, 1/2), and (7) would still be satisﬁed for large enough n. [sent-229, score-0.245]
</p><p>64 In order to validate the consequences of our theory and compare against other methods in a controlled fashion, we performed simulations in four settings: slow/fast spectrum decay, and diagonal/random covariance structure. [sent-239, score-0.164]
</p><p>65 01 and 1, and raised them to the power 20 for fast decay and the power 5 for slow decay. [sent-241, score-0.488]
</p><p>66 First notice that fast spectral decay improves the performance of our test relative to CQ, as expected from Theorem 2. [sent-253, score-0.286]
</p><p>67 Since the ROC curve of our method is roughly the same as that of CQ in panels (a) and (c) (where again n = 98), our condition (6) is somewhat conservative for slow decay at the ﬁnite sample level. [sent-257, score-0.277]
</p><p>68 8  False positive rate  (d) random Σ, fast decay  0. [sent-272, score-0.215]
</p><p>69 0  (b) diagonal Σ, fast decay  RP SD CQ BS KFDA MMD TreeRank  0. [sent-297, score-0.181]
</p><p>70 0  (a) diagonal Σ, slow decay  True positive rate  0. [sent-305, score-0.296]
</p><p>71 0  To study the consequences of Theorem 3, observe that when the covariance matrix Σ is generated randomly, the amount of correlation is much larger than in the idealized case that Σ is diagonal. [sent-324, score-0.201]
</p><p>72 Consequently, when comparing (a) with (c), and (b) with (d), we see that correlation improves the performance of our test relative to SD, as expected from the bound in Theorem 3. [sent-326, score-0.205]
</p><p>73 10  (f) FPR for genomic data (zoom)  Figure 1: Left and middle panels: ROC curves of several test statistics for two different choices of correlation structure and decay rate. [sent-336, score-0.411]
</p><p>74 (a) Diagonal covariance slow decay, (b) Diagonal covariance fast decay, (c) Random covariance slow decay, (d) Random covariance fast decay. [sent-337, score-0.41]
</p><p>75 Right panels: (e) False positive rate against p-value threshold on the gene expression experiment of Section 4 for RP ( ), BS, CQ, SD and enrichment test, (f) zoom on the p-value < 0. [sent-338, score-0.343]
</p><p>76 The ability to identify gene sets having different expression between two types of conditions, e. [sent-341, score-0.188]
</p><p>77 Likewise, there is considerable motivation to study our procedure in the context of detecting differential expression of p genes between two small groups of patients of sizes n1 and n2 . [sent-344, score-0.202]
</p><p>78 2 To compare the performance our Tk statistic against competitors CQ and SD in this type of application, we constructed a collection of 1680 distinct two-sample problems in the following manner, using data from three genomic studies of ovarian [21], myeloma [22] and colorectal [23] cancers. [sent-345, score-0.329]
</p><p>79 Next, we considered pairwise comparisons between all sets of patients on each of 14 biologically meaningful gene sets from the canonical pathways of MSigDB [24], with each gene set containing between 75 and 128 genes. [sent-347, score-0.376]
</p><p>80 4 A natural performance measure for comparing test statistics is the actual false positive rate (FPR) as a function of the nominal level α. [sent-350, score-0.361]
</p><p>81 When testing at level α, the actual FPR should be as close to α as possible, but differences may occur if the distribution of the test statistic under H0 is not known exactly (as is the case in practice). [sent-351, score-0.388]
</p><p>82 Figure 1 (f) is a zoomed plot of this region and shows that the SD and CQ tests commit too many false positives at low thresholds. [sent-354, score-0.192]
</p><p>83 Again, in this regime, our procedure is closer to the diagonal and safely commits fewer than the allowed number of false positives. [sent-355, score-0.179]
</p><p>84 The same thresholds on the p-values of our test lead to false positive rates of 0. [sent-362, score-0.245]
</p><p>85 5  Conclusion  We have proposed a novel testing procedure for the two-sample test of means in high dimensions. [sent-367, score-0.274]
</p><p>86 This procedure can be implemented in a simple manner by ﬁrst projecting a dataset with a single randomly drawn matrix, and then applying the standard Hotelling T 2 test in the projected space. [sent-368, score-0.238]
</p><p>87 In addition to obtaining the asymptotic power of this test, we have provided interpretable conditions on the covariance matrix Σ for achieving greater power than competing tests in the sense of asymptotic relative efﬁciency. [sent-369, score-1.021]
</p><p>88 Speciﬁcally, our theoretical comparisons show that our test is well suited to interesting regimes where most of the variance in the data can be captured in a relatively small number of variables, or where the variables are highly correlated. [sent-370, score-0.185]
</p><p>89 Furthermore, in the realistic case of (n, p) = (98, 200), these regimes were shown to correspond to favorable performance of our test against several competitors in ROC curve comparisons on simulated data. [sent-371, score-0.226]
</p><p>90 Finally, we showed on real gene expression data that our procedure was more reliable than competitors in terms of its false positive rate. [sent-372, score-0.41]
</p><p>91 Extensions of this work may include more reﬁned applications of random projection to high-dimensional testing problems. [sent-373, score-0.159]
</p><p>92 4 Although this assumption could be violated by the existence of various cancer subtypes, or technical differences between original tissue samples, our initial step of randomly splitting the three cancer datasets into subsets guards against these effects. [sent-378, score-0.174]
</p><p>93 Hotelling’s T2 multivariate proﬁling for detecting differential expression in microarrays. [sent-392, score-0.144]
</p><p>94 Analyzing gene expression data in terms of gene sets: methodological u issues. [sent-398, score-0.322]
</p><p>95 A test for the mean with fewer observations than the dimension under non-normality. [sent-426, score-0.162]
</p><p>96 Gains in power from structured two-sample tests of means on graphs. [sent-483, score-0.243]
</p><p>97 A signiﬁcance test for the separation of two highly multivariate small samples. [sent-513, score-0.169]
</p><p>98 Novel molecular subtypes of serous and endometrioid ovarian cancer linked to clinical outcome. [sent-518, score-0.183]
</p><p>99 A high-risk signature for patients with multiple myeloma established from the molecular classiﬁcation of human myeloma cell lines. [sent-522, score-0.214]
</p><p>100 Metastasis-associated gene expression changes predict poor outcomes in patients with dukes stage b and c colorectal cancer. [sent-527, score-0.285]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pk', 0.445), ('cq', 0.368), ('hotelling', 0.286), ('sd', 0.243), ('asymptotic', 0.177), ('bs', 0.176), ('rp', 0.159), ('kfda', 0.157), ('decay', 0.147), ('power', 0.147), ('treerank', 0.138), ('gene', 0.134), ('tr', 0.127), ('statistic', 0.121), ('test', 0.113), ('testing', 0.112), ('mmd', 0.111), ('roc', 0.104), ('tests', 0.096), ('false', 0.096), ('tk', 0.092), ('fpr', 0.089), ('cancer', 0.087), ('covariance', 0.079), ('projected', 0.076), ('rejecting', 0.073), ('srivastava', 0.071), ('correlation', 0.066), ('patients', 0.065), ('greater', 0.064), ('qin', 0.064), ('procedures', 0.06), ('myeloma', 0.059), ('hypothesis', 0.058), ('theorems', 0.057), ('multivariate', 0.056), ('expression', 0.054), ('enrichment', 0.052), ('panels', 0.052), ('dimension', 0.049), ('procedure', 0.049), ('projection', 0.047), ('slow', 0.047), ('competing', 0.047), ('lastly', 0.045), ('curves', 0.044), ('critical', 0.044), ('comparisons', 0.043), ('nominal', 0.042), ('level', 0.042), ('competitors', 0.041), ('bai', 0.041), ('genomic', 0.041), ('hg', 0.041), ('clin', 0.039), ('saranadasa', 0.039), ('du', 0.039), ('positive', 0.036), ('mencon', 0.035), ('ovarian', 0.035), ('zoom', 0.035), ('diagonal', 0.034), ('detecting', 0.034), ('conditions', 0.034), ('theorem', 0.033), ('rate', 0.032), ('jacob', 0.032), ('colorectal', 0.032), ('apr', 0.032), ('condition', 0.031), ('molecular', 0.031), ('ip', 0.03), ('sphere', 0.03), ('normal', 0.03), ('differentially', 0.03), ('quantile', 0.03), ('subtypes', 0.03), ('tending', 0.03), ('likewise', 0.03), ('outperforming', 0.029), ('regimes', 0.029), ('spectrum', 0.029), ('consequences', 0.029), ('consequently', 0.028), ('chen', 0.027), ('fmri', 0.027), ('matrix', 0.027), ('shift', 0.027), ('simulations', 0.027), ('relative', 0.026), ('pooled', 0.026), ('harder', 0.026), ('superior', 0.026), ('res', 0.025), ('bioinformatics', 0.025), ('versus', 0.025), ('blind', 0.024), ('device', 0.024), ('inequality', 0.024), ('yj', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="9-tfidf-1" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>2 0.19049312 <a title="9-tfidf-2" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>Author: Yoshinobu Kawahara, Takashi Washio</p><p>Abstract: In this paper, we propose the ﬁrst exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning. In fact, this generalizes any set-function optimization. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.</p><p>3 0.096799582 <a title="9-tfidf-3" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>4 0.089453325 <a title="9-tfidf-4" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>Author: Shengbo Guo, Onno Zoeter, Cédric Archambeau</p><p>Abstract: We propose a new sparse Bayesian model for multi-task regression and classiﬁcation. The model is able to capture correlations between tasks, or more speciﬁcally a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classiﬁcation tasks it achieves competitive predictive performance compared to previously proposed methods. 1</p><p>5 0.085340202 <a title="9-tfidf-5" href="./nips-2011-Phase_transition_in_the_family_of_p-resistances.html">213 nips-2011-Phase transition in the family of p-resistances</a></p>
<p>Author: Morteza Alamgir, Ulrike V. Luxburg</p><p>Abstract: We study the family of p-resistances on graphs for p 1. This family generalizes the standard resistance distance. We prove that for any ﬁxed graph, for p = 1 the p-resistance coincides with the shortest path distance, for p = 2 it coincides with the standard resistance distance, and for p ! 1 it converges to the inverse of the minimal s-t-cut in the graph. Secondly, we consider the special case of random geometric graphs (such as k-nearest neighbor graphs) when the number n of vertices in the graph tends to inﬁnity. We prove that an interesting phase transition takes place. There exist two critical thresholds p⇤ and p⇤⇤ such that if p < p⇤ , then the p-resistance depends on meaningful global properties of the graph, whereas if p > p⇤⇤ , it only depends on trivial local quantities and does not convey any useful information. We can explicitly compute the critical values: p⇤ = 1 + 1/(d 1) and p⇤⇤ = 1 + 1/(d 2) where d is the dimension of the underlying space (we believe that the fact that there is a small gap between p⇤ and p⇤⇤ is an artifact of our proofs). We also relate our ﬁndings to Laplacian regularization and suggest to use q-Laplacians as regularizers, where q satisﬁes 1/p⇤ + 1/q = 1. 1</p><p>6 0.079553083 <a title="9-tfidf-6" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>7 0.074054979 <a title="9-tfidf-7" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>8 0.067815006 <a title="9-tfidf-8" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>9 0.066892259 <a title="9-tfidf-9" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>10 0.065224737 <a title="9-tfidf-10" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>11 0.064956509 <a title="9-tfidf-11" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>12 0.063048713 <a title="9-tfidf-12" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>13 0.062029447 <a title="9-tfidf-13" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>14 0.061284449 <a title="9-tfidf-14" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>15 0.059838392 <a title="9-tfidf-15" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>16 0.057964563 <a title="9-tfidf-16" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>17 0.056826439 <a title="9-tfidf-17" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>18 0.05485101 <a title="9-tfidf-18" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>19 0.054067649 <a title="9-tfidf-19" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>20 0.054003112 <a title="9-tfidf-20" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.006), (2, -0.016), (3, -0.106), (4, -0.054), (5, 0.004), (6, -0.07), (7, 0.018), (8, 0.013), (9, 0.046), (10, 0.005), (11, -0.02), (12, 0.038), (13, 0.014), (14, 0.009), (15, 0.03), (16, 0.004), (17, 0.003), (18, 0.093), (19, -0.02), (20, 0.051), (21, 0.064), (22, 0.013), (23, 0.116), (24, -0.018), (25, -0.023), (26, -0.025), (27, 0.01), (28, 0.039), (29, -0.017), (30, -0.072), (31, 0.036), (32, -0.017), (33, 0.062), (34, -0.149), (35, -0.053), (36, 0.139), (37, 0.116), (38, -0.083), (39, 0.008), (40, -0.038), (41, -0.07), (42, 0.047), (43, 0.122), (44, -0.092), (45, -0.037), (46, 0.147), (47, 0.074), (48, 0.019), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94488066 <a title="9-lsi-1" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>2 0.5877735 <a title="9-lsi-2" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>Author: Tzu-kuo Huang, Jeff G. Schneider</p><p>Abstract: Vector Auto-regressive models (VAR) are useful tools for analyzing time series data. In quite a few modern time series modelling tasks, the collection of reliable time series turns out to be a major challenge, either due to the slow progression of the dynamic process of interest, or inaccessibility of repetitive measurements of the same dynamic process over time. In those situations, however, we observe that it is often easier to collect a large amount of non-sequence samples, or snapshots of the dynamic process of interest. In this work, we assume a small amount of time series data are available, and propose methods to incorporate non-sequence data into penalized least-square estimation of VAR models. We consider non-sequence data as samples drawn from the stationary distribution of the underlying VAR model, and devise a novel penalization scheme based on the Lyapunov equation concerning the covariance of the stationary distribution. Experiments on synthetic and video data demonstrate the effectiveness of the proposed methods. 1</p><p>3 0.57010067 <a title="9-lsi-3" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>Author: Yoshinobu Kawahara, Takashi Washio</p><p>Abstract: In this paper, we propose the ﬁrst exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning. In fact, this generalizes any set-function optimization. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.</p><p>4 0.56208354 <a title="9-lsi-4" href="./nips-2011-Regularized_Laplacian_Estimation_and_Fast_Eigenvector_Approximation.html">236 nips-2011-Regularized Laplacian Estimation and Fast Eigenvector Approximation</a></p>
<p>Author: Patrick O. Perry, Michael W. Mahoney</p><p>Abstract: Recently, Mahoney and Orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the ﬁrst nontrivial eigenvector of a data graph Laplacian exactly solve certain regularized Semi-Deﬁnite Programs (SDPs). In this paper, we extend that result by providing a statistical interpretation of their approximation procedure. Our interpretation will be analogous to the manner in which 2 -regularized or 1 -regularized 2 -regression (often called Ridge regression and Lasso regression, respectively) can be interpreted in terms of a Gaussian prior or a Laplace prior, respectively, on the coefﬁcient vector of the regression problem. Our framework will imply that the solutions to the MahoneyOrecchia regularized SDP can be interpreted as regularized estimates of the pseudoinverse of the graph Laplacian. Conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based PageRank procedure for computing an approximation to the ﬁrst nontrivial eigenvector of the graph Laplacian. Empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm. 1</p><p>5 0.54792655 <a title="9-lsi-5" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>6 0.51791966 <a title="9-lsi-6" href="./nips-2011-Phase_transition_in_the_family_of_p-resistances.html">213 nips-2011-Phase transition in the family of p-resistances</a></p>
<p>7 0.51249528 <a title="9-lsi-7" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>8 0.48660952 <a title="9-lsi-8" href="./nips-2011-A_Global_Structural_EM_Algorithm_for_a_Model_of_Cancer_Progression.html">6 nips-2011-A Global Structural EM Algorithm for a Model of Cancer Progression</a></p>
<p>9 0.47749749 <a title="9-lsi-9" href="./nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</a></p>
<p>10 0.45976734 <a title="9-lsi-10" href="./nips-2011-Learning_Patient-Specific_Cancer_Survival_Distributions_as_a_Sequence_of_Dependent_Regressors.html">147 nips-2011-Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors</a></p>
<p>11 0.45021403 <a title="9-lsi-11" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>12 0.45007619 <a title="9-lsi-12" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>13 0.44404227 <a title="9-lsi-13" href="./nips-2011-Clustered_Multi-Task_Learning_Via_Alternating_Structure_Optimization.html">51 nips-2011-Clustered Multi-Task Learning Via Alternating Structure Optimization</a></p>
<p>14 0.44111192 <a title="9-lsi-14" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>15 0.4384726 <a title="9-lsi-15" href="./nips-2011-Automated_Refinement_of_Bayes_Networks%27_Parameters_based_on_Test_Ordering_Constraints.html">40 nips-2011-Automated Refinement of Bayes Networks' Parameters based on Test Ordering Constraints</a></p>
<p>16 0.43780166 <a title="9-lsi-16" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>17 0.43529558 <a title="9-lsi-17" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>18 0.43164039 <a title="9-lsi-18" href="./nips-2011-Maximum_Covariance_Unfolding_%3A_Manifold_Learning_for_Bimodal_Data.html">167 nips-2011-Maximum Covariance Unfolding : Manifold Learning for Bimodal Data</a></p>
<p>19 0.42783058 <a title="9-lsi-19" href="./nips-2011-Continuous-Time_Regression_Models_for_Longitudinal_Networks.html">62 nips-2011-Continuous-Time Regression Models for Longitudinal Networks</a></p>
<p>20 0.41789791 <a title="9-lsi-20" href="./nips-2011-Advice_Refinement_in_Knowledge-Based_SVMs.html">27 nips-2011-Advice Refinement in Knowledge-Based SVMs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (4, 0.05), (20, 0.036), (26, 0.028), (31, 0.058), (33, 0.019), (43, 0.112), (45, 0.145), (57, 0.037), (65, 0.02), (67, 0.012), (74, 0.06), (83, 0.066), (84, 0.018), (92, 0.183), (99, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87271231 <a title="9-lda-1" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>2 0.87072676 <a title="9-lda-2" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>3 0.84355551 <a title="9-lda-3" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>4 0.80587339 <a title="9-lda-4" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>5 0.76894569 <a title="9-lda-5" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>6 0.75758868 <a title="9-lda-6" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>7 0.75528395 <a title="9-lda-7" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>8 0.75086749 <a title="9-lda-8" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>9 0.75054574 <a title="9-lda-9" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>10 0.74892777 <a title="9-lda-10" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>11 0.74613392 <a title="9-lda-11" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>12 0.74608058 <a title="9-lda-12" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>13 0.74491125 <a title="9-lda-13" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>14 0.74478102 <a title="9-lda-14" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>15 0.74308062 <a title="9-lda-15" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>16 0.74226665 <a title="9-lda-16" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>17 0.74091595 <a title="9-lda-17" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>18 0.74089569 <a title="9-lda-18" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>19 0.74038136 <a title="9-lda-19" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>20 0.73986709 <a title="9-lda-20" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
