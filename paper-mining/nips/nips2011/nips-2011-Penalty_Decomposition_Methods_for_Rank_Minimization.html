<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>211 nips-2011-Penalty Decomposition Methods for Rank Minimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-211" href="#">nips2011-211</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>211 nips-2011-Penalty Decomposition Methods for Rank Minimization</h1>
<br/><p>Source: <a title="nips-2011-211-pdf" href="http://papers.nips.cc/paper/4235-penalty-decomposition-methods-for-rank-minimization.pdf">pdf</a></p><p>Author: Yong Zhang, Zhaosong Lu</p><p>Abstract: In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We ﬁrst show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems. As a consequence, we establish that a class of rank minimization problems have closed form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems. The convergence results of the PD methods have been shown in the longer version of the paper [19]. Finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed. 1</p><p>Reference: <a title="nips-2011-211-reference" href="../nips2011_reference/nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pd', 0.544), ('rank', 0.305), ('bcd', 0.278), ('penal', 0.204), ('nuclear', 0.19), ('decomposit', 0.165), ('fea', 0.156), ('ylk', 0.156), ('subsect', 0.139), ('fras', 0.125), ('xlk', 0.125), ('inv', 0.121), ('matrix', 0.105), ('nearest', 0.102), ('singul', 0.099), ('subproblem', 0.095), ('im', 0.095), ('longcor', 0.094), ('mask', 0.082), ('correl', 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="211-tfidf-1" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>Author: Yong Zhang, Zhaosong Lu</p><p>Abstract: In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We ﬁrst show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems. As a consequence, we establish that a class of rank minimization problems have closed form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems. The convergence results of the PD methods have been shown in the longer version of the paper [19]. Finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed. 1</p><p>2 0.17109641 <a title="211-tfidf-2" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>Author: Kevin G. Jamieson, Robert Nowak</p><p>Abstract: This paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects). In general, the ranking of n objects can be identiﬁed by standard sorting methods using n log2 n pairwise comparisons. We are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons. Speciﬁcally, we assume that the objects can be embedded into a d-dimensional Euclidean space and that the rankings reﬂect their relative distances from a common reference point in Rd . We show that under this assumption the number of possible rankings grows like n2d and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than d log n adaptively selected pairwise comparisons, on average. If instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking. In addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct. Experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis. 1</p><p>3 0.12847756 <a title="211-tfidf-3" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>Author: David P. Wipf</p><p>Abstract: In the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random Gaussian or Fourier) characterized by unit ℓ2 norm, incoherent columns or features. But in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications). In contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows. Sparse penalized regression models are analyzed with the purpose of ﬁnding, to the extent possible, regimes of dictionary invariant performance. In particular, a Type II Bayesian estimator with a dictionarydependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the ℓ1 norm, especially in areas where existing theoretical recovery guarantees no longer hold. This can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions. 1</p><p>4 0.12675768 <a title="211-tfidf-4" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>Author: Ricardo S. Cabral, Fernando Torre, Joao P. Costeira, Alexandre Bernardino</p><p>Abstract: Recently, image categorization has been an active research topic due to the urgent need to retrieve and browse digital images via semantic keywords. This paper formulates image categorization as a multi-label classiﬁcation problem using recent advances in matrix completion. Under this setting, classiﬁcation of testing data is posed as a problem of completing unknown label entries on a data matrix that concatenates training and testing features with training labels. We propose two convex algorithms for matrix completion based on a Rank Minimization criterion speciﬁcally tailored to visual data, and prove its convergence properties. A major advantage of our approach w.r.t. standard discriminative classiﬁcation methods for image categorization is its robustness to outliers, background noise and partial occlusions both in the feature and label space. Experimental validation on several datasets shows how our method outperforms state-of-the-art algorithms, while effectively capturing semantic concepts of classes. 1</p><p>5 0.11632179 <a title="211-tfidf-5" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>Author: Mark Schmidt, Nicolas L. Roux, Francis R. Bach</p><p>Abstract: We consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the non-smooth term. We show that both the basic proximal-gradient method and the accelerated proximal-gradient method achieve the same convergence rate as in the error-free case, provided that the errors decrease at appropriate rates. Using these rates, we perform as well as or better than a carefully chosen ﬁxed error level on a set of structured sparsity problems. 1</p><p>6 0.11525351 <a title="211-tfidf-6" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<p>7 0.11090385 <a title="211-tfidf-7" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>8 0.10691929 <a title="211-tfidf-8" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>9 0.096210666 <a title="211-tfidf-9" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>10 0.092640959 <a title="211-tfidf-10" href="./nips-2011-Testing_a_Bayesian_Measure_of_Representativeness_Using_a_Large_Image_Database.html">280 nips-2011-Testing a Bayesian Measure of Representativeness Using a Large Image Database</a></p>
<p>11 0.090958312 <a title="211-tfidf-11" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>12 0.084886767 <a title="211-tfidf-12" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>13 0.083477713 <a title="211-tfidf-13" href="./nips-2011-RTRMC%3A_A_Riemannian_trust-region_method_for_low-rank_matrix_completion.html">230 nips-2011-RTRMC: A Riemannian trust-region method for low-rank matrix completion</a></p>
<p>14 0.081277169 <a title="211-tfidf-14" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>15 0.080893107 <a title="211-tfidf-15" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>16 0.075275399 <a title="211-tfidf-16" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>17 0.072810702 <a title="211-tfidf-17" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>18 0.072699785 <a title="211-tfidf-18" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>19 0.069830209 <a title="211-tfidf-19" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>20 0.069241926 <a title="211-tfidf-20" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, -0.034), (2, 0.092), (3, 0.069), (4, 0.095), (5, -0.002), (6, 0.051), (7, -0.014), (8, 0.091), (9, 0.128), (10, -0.141), (11, -0.115), (12, 0.019), (13, 0.008), (14, -0.009), (15, -0.043), (16, -0.116), (17, 0.003), (18, 0.036), (19, 0.031), (20, 0.001), (21, -0.051), (22, -0.005), (23, 0.009), (24, 0.04), (25, 0.104), (26, -0.058), (27, -0.036), (28, 0.087), (29, 0.004), (30, -0.048), (31, 0.043), (32, -0.108), (33, 0.13), (34, -0.018), (35, 0.047), (36, -0.034), (37, 0.056), (38, -0.021), (39, 0.053), (40, 0.026), (41, 0.007), (42, 0.055), (43, -0.098), (44, -0.01), (45, 0.006), (46, -0.02), (47, 0.119), (48, 0.086), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93713844 <a title="211-lsi-1" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>Author: Yong Zhang, Zhaosong Lu</p><p>Abstract: In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We ﬁrst show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems. As a consequence, we establish that a class of rank minimization problems have closed form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems. The convergence results of the PD methods have been shown in the longer version of the paper [19]. Finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed. 1</p><p>2 0.64152586 <a title="211-lsi-2" href="./nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</a></p>
<p>Author: Zhouchen Lin, Risheng Liu, Zhixun Su</p><p>Abstract: Many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efﬁciently solved by the alternating direction method (ADM). However, usually the subproblems in ADM are easily solvable only when the linear mappings in the constraints are identities. To address this issue, we propose a linearized ADM (LADM) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems. For fast convergence, we also allow the penalty to change adaptively according a novel update rule. We prove the global convergence of LADM with adaptive penalty (LADMAP). As an example, we apply LADMAP to solve lowrank representation (LRR), which is an important subspace clustering technique yet suffers from high computation cost. By combining LADMAP with a skinny SVD representation technique, we are able to reduce the complexity O(n3 ) of the original ADM based method to O(rn2 ), where r and n are the rank and size of the representation matrix, respectively, hence making LRR possible for large scale applications. Numerical experiments verify that for LRR our LADMAP based methods are much faster than state-of-the-art algorithms. 1</p><p>3 0.61027515 <a title="211-lsi-3" href="./nips-2011-RTRMC%3A_A_Riemannian_trust-region_method_for_low-rank_matrix_completion.html">230 nips-2011-RTRMC: A Riemannian trust-region method for low-rank matrix completion</a></p>
<p>Author: Nicolas Boumal, Pierre-antoine Absil</p><p>Abstract: We consider large matrices of low rank. We address the problem of recovering such matrices when most of the entries are unknown. Matrix completion ﬁnds applications in recommender systems. In this setting, the rows of the matrix may correspond to items and the columns may correspond to users. The known entries are the ratings given by users to some items. The aim is to predict the unobserved ratings. This problem is commonly stated in a constrained optimization framework. We follow an approach that exploits the geometry of the low-rank constraint to recast the problem as an unconstrained optimization problem on the Grassmann manifold. We then apply ﬁrst- and second-order Riemannian trust-region methods to solve it. The cost of each iteration is linear in the number of known entries. Our methods, RTRMC 1 and 2, outperform state-of-the-art algorithms on a wide range of problem instances. 1</p><p>4 0.608841 <a title="211-lsi-4" href="./nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">260 nips-2011-Sparse Features for PCA-Like Linear Regression</a></p>
<p>Author: Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail</p><p>Abstract: Principal Components Analysis (PCA) is often used as a feature extraction procedure. Given a matrix X ∈ Rn×d , whose rows represent n data points with respect to d features, the top k right singular vectors of X (the so-called eigenfeatures), are arbitrary linear combinations of all available features. The eigenfeatures are very useful in data analysis, including the regularization of linear regression. Enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a small number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures. We present deterministic and randomized algorithms that construct such sparse eigenfeatures while provably achieving in-sample performance comparable to regularized linear regression. Our algorithms are relatively simple and practically efﬁcient, and we demonstrate their performance on several data sets.</p><p>5 0.59693903 <a title="211-lsi-5" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>Author: Yi-kai Liu</p><p>Abstract: We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefﬁcients. We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rankr restricted isometry property (RIP). This implies that M can be recovered from a ﬁxed (“universal”) set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley’s inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality. 1</p><p>6 0.59599358 <a title="211-lsi-6" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>7 0.58574444 <a title="211-lsi-7" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>8 0.57416689 <a title="211-lsi-8" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>9 0.56583798 <a title="211-lsi-9" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>10 0.5460642 <a title="211-lsi-10" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>11 0.52860516 <a title="211-lsi-11" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>12 0.52077174 <a title="211-lsi-12" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>13 0.51078135 <a title="211-lsi-13" href="./nips-2011-Active_Learning_Ranking_from_Pairwise_Preferences_with_Almost_Optimal_Query_Complexity.html">20 nips-2011-Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity</a></p>
<p>14 0.50780064 <a title="211-lsi-14" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>15 0.5075953 <a title="211-lsi-15" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<p>16 0.50516641 <a title="211-lsi-16" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>17 0.49942926 <a title="211-lsi-17" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>18 0.49446011 <a title="211-lsi-18" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>19 0.46613526 <a title="211-lsi-19" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>20 0.45207417 <a title="211-lsi-20" href="./nips-2011-Maximum_Covariance_Unfolding_%3A_Manifold_Learning_for_Bimodal_Data.html">167 nips-2011-Maximum Covariance Unfolding : Manifold Learning for Bimodal Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.04), (22, 0.595), (36, 0.03), (55, 0.102), (65, 0.047), (68, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90015107 <a title="211-lda-1" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>Author: Yong Zhang, Zhaosong Lu</p><p>Abstract: In this paper we consider general rank minimization problems with rank appearing in either objective function or constraint. We ﬁrst show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems. As a consequence, we establish that a class of rank minimization problems have closed form solutions. Using this result, we then propose penalty decomposition methods for general rank minimization problems. The convergence results of the PD methods have been shown in the longer version of the paper [19]. Finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems. The computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed. 1</p><p>2 0.76930904 <a title="211-lda-2" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<p>Author: Ryota Tomioka, Taiji Suzuki, Kohei Hayashi, Hisashi Kashima</p><p>Abstract: We analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance. We show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor. The current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors. Furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.</p><p>3 0.69888741 <a title="211-lda-3" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>Author: Ambuj Tewari, Pradeep K. Ravikumar, Inderjit S. Dhillon</p><p>Abstract: A hallmark of modern machine learning is its ability to deal with high dimensional problems by exploiting structural assumptions that limit the degrees of freedom in the underlying model. A deep understanding of the capabilities and limits of high dimensional learning methods under specific assumptions such as sparsity, group sparsity, and low rank has been attsined. Efforts [1,2] are now underway to distill this valuable experience by proposing general unified frameworks that can achieve the twio goals of summarizing previous analyses and enabling their application to notions of structure hitherto unexplored. Inspired by these developments, we propose and analyze a general computational scheme based on a greedy strategy to solve convex optimization problems that arise when dealing with structurally constrained high-dimensional problems. Our framework not only unifies existing greedy algorithms by recovering them as special cases but also yields novel ones. Finally, we extend our results to infinite dimensional settings by using interesting connections between smoothness of norms and behavior of martingales in Banach spaces.</p><p>4 0.67327452 <a title="211-lda-4" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<p>Author: Salah Rifai, Yann N. Dauphin, Pascal Vincent, Yoshua Bengio, Xavier Muller</p><p>Abstract: We combine three important ideas present in previous work for building classiﬁers: the semi-supervised hypothesis (the input distribution contains information about the classiﬁer), the unsupervised manifold hypothesis (data density concentrates near low-dimensional manifolds), and the manifold hypothesis for classiﬁcation (different classes correspond to disjoint manifolds separated by low density). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classiﬁer to be insensitive to local directions changes along the manifold. Record-breaking classiﬁcation results are obtained. 1</p><p>5 0.67092472 <a title="211-lda-5" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>6 0.63621169 <a title="211-lda-6" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>7 0.62364441 <a title="211-lda-7" href="./nips-2011-Statistical_Tests_for_Optimization_Efficiency.html">271 nips-2011-Statistical Tests for Optimization Efficiency</a></p>
<p>8 0.60542268 <a title="211-lda-8" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>9 0.60441732 <a title="211-lda-9" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>10 0.59391826 <a title="211-lda-10" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>11 0.57336712 <a title="211-lda-11" href="./nips-2011-RTRMC%3A_A_Riemannian_trust-region_method_for_low-rank_matrix_completion.html">230 nips-2011-RTRMC: A Riemannian trust-region method for low-rank matrix completion</a></p>
<p>12 0.55268174 <a title="211-lda-12" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>13 0.55213314 <a title="211-lda-13" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>14 0.53875041 <a title="211-lda-14" href="./nips-2011-Linearized_Alternating_Direction_Method_with_Adaptive_Penalty_for_Low-Rank_Representation.html">161 nips-2011-Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation</a></p>
<p>15 0.52755564 <a title="211-lda-15" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>16 0.52189475 <a title="211-lda-16" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>17 0.52078605 <a title="211-lda-17" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>18 0.51958376 <a title="211-lda-18" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>19 0.50747317 <a title="211-lda-19" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>20 0.50607008 <a title="211-lda-20" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
