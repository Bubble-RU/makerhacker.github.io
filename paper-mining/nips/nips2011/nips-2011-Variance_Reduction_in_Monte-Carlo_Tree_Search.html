<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-300" href="#">nips2011-300</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</h1>
<br/><p>Source: <a title="nips-2011-300-pdf" href="http://papers.nips.cc/paper/4288-variance-reduction-in-monte-carlo-tree-search.pdf">pdf</a></p><p>Author: Joel Veness, Marc Lanctot, Michael Bowling</p><p>Abstract: Monte-Carlo Tree Search (MCTS) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments. The stochastic nature of the Monte-Carlo simulations introduces errors in the value estimates, both in terms of bias and variance. Whilst reducing bias (typically through the addition of domain knowledge) has been studied in the MCTS literature, comparatively little effort has focused on reducing variance. This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. We demonstrate how these techniques can be applied to MCTS and explore their efﬁcacy on three different stochastic, single-agent settings: Pig, Can’t Stop and Dominion. 1</p><p>Reference: <a title="nips-2011-300-reference" href="../nips2011_reference/nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. [sent-10, score-0.234]
</p><p>2 In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. [sent-11, score-0.965]
</p><p>3 These value estimates are used to direct the growth of the search tree and to estimate the value under the optimal policy from each internal node. [sent-15, score-0.252]
</p><p>4 Somewhat surprisingly however, the application of classical variance reduction techniques to MCTS has remained unexplored. [sent-19, score-0.234]
</p><p>5 In this paper we survey some common variance reduction ideas and show how they can be used to improve the efﬁciency of MCTS. [sent-20, score-0.256]
</p><p>6 We found that substantial increases in performance can be obtained by using the appropriate combination of variance reduction techniques. [sent-22, score-0.197]
</p><p>7 To the best of our knowledge, our work constitutes the ﬁrst investigation of classical variance reduction techniques in the context of MCTS. [sent-23, score-0.234]
</p><p>8 The transition probability kernel gives rise to the state transition kernel P(s, a, s ) := P0 ({s } × R | s, a), which gives the probability of transitioning from state s to state s if action a is taken in s. [sent-32, score-0.283]
</p><p>9 , An−1 , Sn , Rn describing the execution of the system up to n time n from a state st , the return from st is deﬁned as Xst := i=t+1 Ri . [sent-40, score-0.373]
</p><p>10 An optimal policy, denoted by π ∗ , is a policy that maximizes the expected return E [Xst ] for all states st ∈ S. [sent-42, score-0.279]
</p><p>11 2  Online Monte-Carlo Planning in MDPs  If the state space is small, an optimal action can be computed ofﬂine for each state using techniques such as exhaustive Expectimax Search [18] or Q-Learning [23]. [sent-45, score-0.296]
</p><p>12 This effectively amortizes the planning effort across multiple time steps, and implicitly focuses the approximation effort on the relevant parts of the state space. [sent-49, score-0.284]
</p><p>13 A popular way to construct an online planning algorithm is to use a depth-limited version of an exhaustive search technique (such as Expectimax Search) in conjunction with iterative deepening [18]. [sent-50, score-0.321]
</p><p>14 Depth-limited exhaustive search is generally outperformed by Monte-Carlo planning techniques in these situations. [sent-54, score-0.275]
</p><p>15 A canonical example of online Monte-Carlo planning is 1-ply rollout-based planning [3]. [sent-55, score-0.325]
</p><p>16 At each time t < n, given a starting state st , for each at ∈ A and with t < i < n, E [Xst ,at | Ai ∼ π(· | Si )] is estimated by generating trajectories St+1 , Rt+1 , . [sent-57, score-0.248]
</p><p>17 One of the main advantages of rollout based planning compared with exhaustive depth-limited search is that a much larger search horizon can be used. [sent-65, score-0.371]
</p><p>18 The disadvantage however is that if π is suboptimal, then E [Xst ,a | Ai ∼ π(· | Si )] < E [Xst ,a | Ai ∼ π ∗ (· | Si )] for at least one state-action pair (st , a) ∈ S × A, which implies that at least some value estimates constructed by 1-ply rollout-based planning are biased. [sent-66, score-0.215]
</p><p>19 Monte-Carlo Tree Search algorithms improve on this procedure, by providing a means to construct asymptotically consistent estimates of the return under the optimal policy from simulation trajectories. [sent-69, score-0.25]
</p><p>20 Like rolloutbased planning, it uses a default policy to generate trajectories of agent-system interaction. [sent-71, score-0.209]
</p><p>21 Initially, the search tree consists of a 2  single node, which represents the current state st at time t. [sent-73, score-0.31]
</p><p>22 Associated with each state-action pair (s, a) ∈ S × A is an estimate Xs,a of the return m under the optimal policy and a count Ts,a ∈ N representing the number of times this state-action 0 ¯0 pair has been visited after m simulations, with Ts,a := 0 and Xs,a := 0. [sent-76, score-0.224]
</p><p>23 Selection involves traversing a path from the root node to a leaf node in the following manner: for each non-leaf, internal node representing some state s on this path, the UCB [1] criterion is applied to select an action until a leaf node corresponding to state sl is reached. [sent-78, score-0.413]
</p><p>24 Provided sl is non-terminal, the expansion phase is then executed, by selecting an action Al ∼ π(· | sl ), observing a successor state Sl+1 = sl+1 , and then adding a node to the search tree so that Tm+1 = Tm ∪ {sl+1 }. [sent-82, score-0.445]
</p><p>25 Notice that for all (s, a) ∈ S × A, the ¯m value estimate Xs,a corresponds to the average return of the realized simulation trajectories passing through state-action pair (s, a). [sent-90, score-0.265]
</p><p>26 After the desired number of simulations k has been performed in ¯k state st , the action with the highest expected return at := argmaxa∈A Xst ,a is selected. [sent-91, score-0.415]
</p><p>27 The next section describes how the accuracy of UCT’s value estimates can be improved by adapting classical variance reduction techniques to MCTS. [sent-95, score-0.27]
</p><p>28 3  Variance Reduction in MCTS  This section describes how three variance reduction techniques — control variates, common random numbers and antithetic variates — can be applied to the UCT algorithm. [sent-96, score-0.998]
</p><p>29 Each subsection begins with a short overview of each variance reduction technique, followed by a description of how UCT can be modiﬁed to efﬁciently incorporate it. [sent-97, score-0.197]
</p><p>30 Whilst we restrict our attention to planning in MDPs using the UCT algorithm, the ideas and techniques we present are quite general. [sent-98, score-0.212]
</p><p>31 For example, similar modiﬁcations could be made to the Sparse Sampling [14] or AMS [5] algorithms for planning in MDPs, or to the POMCP algorithm [22] for planning in POMDPs. [sent-99, score-0.29]
</p><p>32 , (Xn , Yn ) and setting c = c∗ , the control variate enhanced estimator n 1 ¯ Xcv := [Xi + c∗ (Yi − µY )] (2) n i=1 3  is obtained, with variance 1 Cov[X, Y ]2 Var[X] − . [sent-113, score-0.262]
</p><p>33 n Var[Y ] Thus the total variance reduction is dependent on the strength of correlation between X and Y . [sent-114, score-0.197]
</p><p>34 For the optimal value of c, the variance reduction obtained by using Z in place of X is 100×Corr[X, Y ]2 percent. [sent-115, score-0.197]
</p><p>35 Control variates can be applied recursively, by redeﬁning the return Xs,a for every state-action pair (s, a) ∈ S × A to Zs,a := Xs,a + cs,a (Ys,a − E[Ys,a ]) , (3) provided E [Ys,a ] exists and is known for all (s, a) ∈ S × A, and Ys,a is a function of the random variables At , St+1 , Rt+1 , . [sent-123, score-0.436]
</p><p>36 Furthermore, as E [Zst ,at | Ai ∼ π(· | Si )] = E [Xst ,at | Ai ∼ π(· | Si )], for all policies π, for all (st , at ) ∈ S × A and for all t < i < n, the inductive argument [15] used to establish the asymptotic consistency of UCT still applies when control variates are introduced in this fashion. [sent-128, score-0.4]
</p><p>37 Finding appropriate control variates whose expectations are known in advance can prove difﬁcult. [sent-129, score-0.4]
</p><p>38 This situation is further complicated in UCT where we seek a set of control variates {Ys,a } for all (s, a) ∈ S × A. [sent-130, score-0.4]
</p><p>39 Drawing inspiration from advantage sum estimators [25], we now provide a general class of control variates designed for application in UCT. [sent-131, score-0.4]
</p><p>40 Given a realization of a random simulation trajectory St = st , At = at , St+1 = st+1 , At+1 = at+1 , . [sent-132, score-0.25]
</p><p>41 , Sn = sn , consider control variates of the form n−1 Yst ,at := i=t I[b(Si+1 )] − P[b(Si+1 ) | Si =si , Ai =ai ], (4) where b : S → {true, false} denotes a boolean function of state and I denotes the binary indicator function. [sent-135, score-0.547]
</p><p>42 Thus, using control variates of this form simpliﬁes the task to specifying a state property that is strongly correlated with the return, such that P[b(Si+1 ) | Si = si , Ai = ai ] is known for all (si , ai ) ∈ (S, A), for all t ≤ i < n. [sent-137, score-0.741]
</p><p>43 This considerably reduces the effort required to ﬁnd an appropriate set of control variates for UCT. [sent-138, score-0.439]
</p><p>44 Rather than directly reducing the variance of the individual return estimates, common random numbers can instead be applied to reduce the variance of the estimated differences 4  ¯m ¯m in return Xs,a − Xs,a , for each pair of distinct actions a, a ∈ A in a state s. [sent-148, score-0.561]
</p><p>45 This has the beneﬁt ¯m of reducing the effect of variance in both determining the action at := argmaxa∈A Xs,a selected m m ¯m by UCT in state st and the actions argmaxa∈A Xs,a + c log(Ts )/Ts,a selected by UCB as the search tree is constructed. [sent-149, score-0.573]
</p><p>46 Our approach is to use the same chance outcomes to determine the trajectories originating from state-action pairs (s, a) and (s, a ) if j i m Ts,a = Ts,a , for any a, a ∈ A and i, j ∈ N. [sent-151, score-0.266]
</p><p>47 This idea can be applied recursively, provided that the shared chance events from the current state do not conﬂict with those deﬁned at any possible ancestor state. [sent-154, score-0.239]
</p><p>48 (5) 4  2  The method of antithetic variates exploits this identity, by deliberately introducing a negative corˆ ˆ relation between h1 (X) and h2 (Y). [sent-164, score-0.638]
</p><p>49 Like the technique of common random numbers, antithetic variates can be applied to UCT by modifying the way simulation trajectories are sampled. [sent-168, score-0.837]
</p><p>50 Whenever a node representing (si , ai ) ∈ S × A is visited during the backup phase of UCT, the realized trajectory m si+1 , ri+1 , ai+1 , . [sent-169, score-0.279]
</p><p>51 The next time this node is visited during the selection phase, the previous trajectory is used to predetermine one or more antithetic events that will (partially) drive subsequent state transitions for the current simulation trajectory. [sent-173, score-0.605]
</p><p>52 This technique can be applied to all state-action pairs inside the tree, provided that the antithetic events determined by any state-action pair do not overlap with the antithetic events deﬁned by any possible ancestor. [sent-175, score-0.852]
</p><p>53 4  Empirical Results  This section begins with a description of our test domains, and how our various variance reduction ideas can be applied to them. [sent-176, score-0.227]
</p><p>54 1  Test Domains  Pig is a turn-based jeopardy dice game that can be played with one or more players [20]. [sent-179, score-0.343]
</p><p>55 Players roll two dice each turn and keep a turn total. [sent-180, score-0.507]
</p><p>56 At each decision point, they have two actions, roll and stop. [sent-181, score-0.3]
</p><p>57 Normally, dice rolls add to the players turn total, with the following exceptions: if a single is rolled the turn total will be reset and the turn ended; if a is rolled then the players turn will end along with their total score being reset to 0. [sent-183, score-0.527]
</p><p>58 Can’t Stop is a dice game where the goal is to obtain three complete columns by reaching the highest level in each of the 2-12 columns [19]. [sent-185, score-0.267]
</p><p>59 This done by repeatedly rolling 4 dice and playing zero or more pairing combinations. [sent-186, score-0.286]
</p><p>60 If the dice are rolled and no legal pairing combination can be made, the player loses all of the progress made towards completing columns on this turn. [sent-189, score-0.362]
</p><p>61 A key component of the game involves correctly assessing the risk associated with not being able to make a legal dice pairing given the current board conﬁguration. [sent-191, score-0.38]
</p><p>62 It involves acquiring cards by spending the money cards in your current deck. [sent-193, score-0.279]
</p><p>63 The control variates used for all domains were of the form speciﬁed by Equation 4 in Section 3. [sent-204, score-0.44]
</p><p>64 In Pig, we used a boolean function that returned true if we had just performed the roll action and obtained at least one . [sent-206, score-0.402]
</p><p>65 This control variate has an intuitive interpretation, since we would expect the return from a single trajectory to be an underestimate if it contained more rolls with a than expected, and an overestimate if it contained less rolls with a than expected. [sent-207, score-0.343]
</p><p>66 In Can’t Stop, we used similarly inspired boolean function that returned true if we could not make a legal pairing from our most recent roll of the 4 dice. [sent-208, score-0.415]
</p><p>67 In Dominion, we used a boolean function that returned whether we had just played an action that let us randomly draw a hand with 8 or more money to spend. [sent-209, score-0.219]
</p><p>68 2, we need to specify the future chance events to be shared across all of the trajectories originating from each state. [sent-222, score-0.286]
</p><p>69 Since a player’s ﬁnal score in Pig is strongly dependent on their dice rolls, it is natural to consider sharing one or more future dice roll outcomes. [sent-223, score-0.649]
</p><p>70 By exploiting the property in Pig that each roll event is independent of the current state, our implementation shares a batch of roll outcomes large enough to drive a complete simulation trajectory. [sent-224, score-0.653]
</p><p>71 So that these chance events don’t conﬂict, we limited the sharing of roll events to just the root node. [sent-225, score-0.56]
</p><p>72 We found this scheme to be superior to sharing a smaller number of future roll outcomes and applying the ideas in Section 3. [sent-227, score-0.392]
</p><p>73 Here we implemented common random numbers by recursively sharing preshufﬂed deck conﬁgurations across the actions at each state. [sent-230, score-0.205]
</p><p>74 The motivation for this kind of sharing is that it should reduce the chance of one action appearing better than another simply because of “luckier” shufﬂes. [sent-231, score-0.228]
</p><p>75 3, we need to describe how the antithetic events are constructed from previous simulation trajectories. [sent-234, score-0.443]
</p><p>76 In Pig, a negative correlation between the returns of pairs of simulation trajectories can be induced by forcing the roll outcomes in the second trajectory to oppose those occurring in the ﬁrst trajectory. [sent-235, score-0.52]
</p><p>77 Exploiting the property that the relative worth of each pair of dice outcomes is independent of state, a list of antithetic roll outcomes can be constructed by mapping each individual roll outcome in the ﬁrst trajectory to its antithetic partner. [sent-236, score-1.54]
</p><p>78 For example, a lucky roll of was paired with the unlucky roll of . [sent-237, score-0.532]
</p><p>79 Chance events that are favorable in the current state are assigned low indexes, while unfavorable events are assigned high index values. [sent-243, score-0.227]
</p><p>80 Later when the antithetic trajectory needs to be simulated, the previously recorded rank indexes are used to compute the relevant antithetic event for the current state. [sent-245, score-0.673]
</p><p>81 For Dominion, a number of antithetic mappings were tried, but none provided any substantial reduction in variance. [sent-249, score-0.39]
</p><p>82 The complexity of how cards can be played to draw more cards from one’s deck makes a good or bad shufﬂe intricately dependent on the exact composition of cards in one’s deck, of which there are intractably many possibilities with no obvious symmetries. [sent-250, score-0.43]
</p><p>83 3  Experimental Setup  Each variance reduction technique is evaluated in combination with the UCT algorithm, with varying levels of search effort. [sent-252, score-0.301]
</p><p>84 In Pig, the default (rollout) policy plays the roll and stop actions with probability 0. [sent-253, score-0.601]
</p><p>85 In Dominion, the default policy incorporates some simple domain knowledge that favors obtaining higher cost cards and avoiding redundant actions. [sent-258, score-0.259]
</p><p>86 The next set of results is used to assess the overall performance of UCT when augmented with our variance reduction techniques. [sent-265, score-0.197]
</p><p>87 Therefore, when assessing the potential impact of variance reduction, it is important to know just how much of the estimation error is caused by variance as opposed to bias. [sent-268, score-0.218]
</p><p>88 This allows us to compute the expected return E[Xs1 | π ∗ ] of the optimal action (roll) at the starting state s1 . [sent-271, score-0.227]
</p><p>89 We use this value to compute both the bias-squared and variance component of the MSE for the estimated return of the roll action at s1 when using UCT without variance reduction. [sent-272, score-0.65]
</p><p>90 As Pig has just two actions, we can also compute the MSE of the estimated difference in return between rolling and stopping using UCT without variance reduction. [sent-277, score-0.217]
</p><p>91 Here we see that variance is the dominating component (the bias is within ±2) when the number of simulations is less than 1024. [sent-281, score-0.219]
</p><p>92 The role of bias and variance will of course vary from domain to domain, but this result suggests that variance reduction may play an important role when trying to determine the best action. [sent-282, score-0.351]
</p><p>93 The results also show a clear beneﬁt to using variance reduction techniques in the challenging game of Dominion. [sent-290, score-0.326]
</p><p>94 Here the best combination of variance reduction techniques leads to an improvement roughly equivalent to using 25-40% more simulations. [sent-291, score-0.234]
</p><p>95 The use of antithetic variates in both Pig and Can’t Stop gave a measurable increase in performance, however the technique was less effective than either control variates or common random numbers. [sent-292, score-1.115]
</p><p>96 Control variates was particularly helpful across all domains, and even more effective when combined with common random numbers. [sent-293, score-0.365]
</p><p>97 Common random numbers and antithetic variates increase the space complexity of UCT by a multiplicative constant. [sent-295, score-0.671]
</p><p>98 Control variates typically increase the time complexity of each value backup by a constant. [sent-296, score-0.384]
</p><p>99 Note that surprising results are possible; for example, if generating the underlying chance events is expensive, using common random numbers or antithetic variates can even reduce the computational cost of each simulation. [sent-298, score-0.878]
</p><p>100 6  Conclusion  This paper describes how control variates, common random numbers and antithetic variates can be used to improve the performance of Monte-Carlo Tree Search by reducing variance. [sent-302, score-0.764]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('variates', 0.336), ('uct', 0.326), ('antithetic', 0.302), ('pig', 0.294), ('roll', 0.266), ('dominion', 0.191), ('mcts', 0.191), ('dice', 0.175), ('planning', 0.145), ('stop', 0.136), ('st', 0.123), ('var', 0.117), ('xst', 0.116), ('cards', 0.114), ('variance', 0.109), ('si', 0.106), ('action', 0.1), ('chance', 0.095), ('cov', 0.095), ('game', 0.092), ('policy', 0.09), ('reduction', 0.088), ('games', 0.088), ('ai', 0.087), ('events', 0.083), ('mse', 0.08), ('rollout', 0.077), ('tree', 0.07), ('pairing', 0.069), ('trajectory', 0.069), ('return', 0.066), ('simulations', 0.065), ('control', 0.064), ('trajectories', 0.064), ('xcv', 0.064), ('sl', 0.063), ('outcomes', 0.063), ('state', 0.061), ('simulation', 0.058), ('deck', 0.056), ('search', 0.056), ('default', 0.055), ('actions', 0.054), ('money', 0.051), ('sn', 0.05), ('technique', 0.048), ('rolls', 0.048), ('variate', 0.048), ('backup', 0.048), ('chaslot', 0.048), ('crn', 0.048), ('cvcrn', 0.048), ('expectimax', 0.048), ('ine', 0.045), ('bias', 0.045), ('rt', 0.044), ('bs', 0.044), ('originating', 0.044), ('players', 0.044), ('legal', 0.044), ('realized', 0.043), ('tm', 0.042), ('rolled', 0.042), ('rolling', 0.042), ('estimator', 0.041), ('domains', 0.04), ('effort', 0.039), ('xsk', 0.039), ('gelly', 0.039), ('exhaustive', 0.037), ('ucb', 0.037), ('techniques', 0.037), ('estimates', 0.036), ('shuf', 0.036), ('guillaume', 0.036), ('boolean', 0.036), ('mdps', 0.035), ('online', 0.035), ('pair', 0.034), ('decision', 0.034), ('sharing', 0.033), ('turn', 0.033), ('alberta', 0.033), ('numbers', 0.033), ('node', 0.032), ('played', 0.032), ('istvan', 0.032), ('lanctot', 0.032), ('tsk', 0.032), ('veness', 0.032), ('winands', 0.032), ('yizao', 0.032), ('yst', 0.032), ('argmaxa', 0.032), ('player', 0.032), ('stochastic', 0.031), ('cn', 0.031), ('cv', 0.03), ('ideas', 0.03), ('common', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="300-tfidf-1" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>Author: Joel Veness, Marc Lanctot, Michael Bowling</p><p>Abstract: Monte-Carlo Tree Search (MCTS) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments. The stochastic nature of the Monte-Carlo simulations introduces errors in the value estimates, both in terms of bias and variance. Whilst reducing bias (typically through the addition of domain knowledge) has been studied in the MCTS literature, comparatively little effort has focused on reducing variance. This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. We demonstrate how these techniques can be applied to MCTS and explore their efﬁcacy on three different stochastic, single-agent settings: Pig, Can’t Stop and Dominion. 1</p><p>2 0.12205539 <a title="300-tfidf-2" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>Author: Zhan Lim, Lee Sun, Daniel J. Hsu</p><p>Abstract: POMDP planning faces two major computational challenges: large state spaces and long planning horizons. The recently introduced Monte Carlo Value Iteration (MCVI) can tackle POMDPs with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning horizons. This paper presents Macro-MCVI, which extends MCVI by exploiting macro-actions for temporal abstraction. We provide sufﬁcient conditions for Macro-MCVI to inherit the good theoretical properties of MCVI. Macro-MCVI does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice. Experiments show that Macro-MCVI substantially improves the performance of MCVI with suitable macro-actions. 1</p><p>3 0.12138724 <a title="300-tfidf-3" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>4 0.09700314 <a title="300-tfidf-4" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>Author: Paul Wagner</p><p>Abstract: A majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods. The former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon. We take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter. We explain the phenomenon in terms of this view and illustrate the underlying mechanism with artiﬁcial examples. We also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches. In addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the Tetris benchmark problem of all attempted approximate dynamic programming methods. We report empirical evidence against such a connection and in favor of an alternative explanation. Finally, we report scores in the Tetris problem that improve on existing dynamic programming based results. 1</p><p>5 0.094394431 <a title="300-tfidf-5" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>6 0.089489445 <a title="300-tfidf-6" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>7 0.088371493 <a title="300-tfidf-7" href="./nips-2011-Autonomous_Learning_of_Action_Models_for_Planning.html">41 nips-2011-Autonomous Learning of Action Models for Planning</a></p>
<p>8 0.086932689 <a title="300-tfidf-8" href="./nips-2011-On_Strategy_Stitching_in_Large_Extensive_Form_Multiplayer_Games.html">196 nips-2011-On Strategy Stitching in Large Extensive Form Multiplayer Games</a></p>
<p>9 0.083431445 <a title="300-tfidf-9" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>10 0.082539529 <a title="300-tfidf-10" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>11 0.080717616 <a title="300-tfidf-11" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>12 0.076242447 <a title="300-tfidf-12" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>13 0.071789369 <a title="300-tfidf-13" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>14 0.068869166 <a title="300-tfidf-14" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>15 0.068118908 <a title="300-tfidf-15" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>16 0.067752898 <a title="300-tfidf-16" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>17 0.067506313 <a title="300-tfidf-17" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>18 0.064574242 <a title="300-tfidf-18" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>19 0.062752999 <a title="300-tfidf-19" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>20 0.060048949 <a title="300-tfidf-20" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.132), (2, 0.049), (3, 0.118), (4, -0.091), (5, -0.003), (6, -0.038), (7, -0.045), (8, -0.01), (9, -0.05), (10, 0.032), (11, -0.008), (12, -0.057), (13, 0.023), (14, 0.012), (15, -0.044), (16, 0.038), (17, 0.032), (18, 0.023), (19, -0.055), (20, 0.057), (21, -0.067), (22, -0.028), (23, 0.0), (24, -0.014), (25, 0.065), (26, -0.07), (27, 0.049), (28, 0.002), (29, -0.063), (30, -0.043), (31, -0.003), (32, -0.011), (33, 0.018), (34, -0.02), (35, -0.012), (36, 0.143), (37, 0.002), (38, -0.055), (39, 0.054), (40, -0.053), (41, 0.094), (42, 0.072), (43, -0.007), (44, 0.03), (45, 0.022), (46, -0.083), (47, -0.001), (48, -0.128), (49, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94420761 <a title="300-lsi-1" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>Author: Joel Veness, Marc Lanctot, Michael Bowling</p><p>Abstract: Monte-Carlo Tree Search (MCTS) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments. The stochastic nature of the Monte-Carlo simulations introduces errors in the value estimates, both in terms of bias and variance. Whilst reducing bias (typically through the addition of domain knowledge) has been studied in the MCTS literature, comparatively little effort has focused on reducing variance. This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. We demonstrate how these techniques can be applied to MCTS and explore their efﬁcacy on three different stochastic, single-agent settings: Pig, Can’t Stop and Dominion. 1</p><p>2 0.66053277 <a title="300-lsi-2" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>Author: Zhan Lim, Lee Sun, Daniel J. Hsu</p><p>Abstract: POMDP planning faces two major computational challenges: large state spaces and long planning horizons. The recently introduced Monte Carlo Value Iteration (MCVI) can tackle POMDPs with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning horizons. This paper presents Macro-MCVI, which extends MCVI by exploiting macro-actions for temporal abstraction. We provide sufﬁcient conditions for Macro-MCVI to inherit the good theoretical properties of MCVI. Macro-MCVI does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice. Experiments show that Macro-MCVI substantially improves the performance of MCVI with suitable macro-actions. 1</p><p>3 0.64697802 <a title="300-lsi-3" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>Author: Tingting Zhao, Hirotaka Hachiya, Gang Niu, Masashi Sugiyama</p><p>Abstract: Policy gradient is a useful model-free reinforcement learning approach, but it tends to suffer from instability of gradient estimates. In this paper, we analyze and improve the stability of policy gradient methods. We ﬁrst prove that the variance of gradient estimates in the PGPE (policy gradients with parameter-based exploration) method is smaller than that of the classical REINFORCE method under a mild assumption. We then derive the optimal baseline for PGPE, which contributes to further reducing the variance. We also theoretically show that PGPE with the optimal baseline is more preferable than REINFORCE with the optimal baseline in terms of the variance of gradient estimates. Finally, we demonstrate the usefulness of the improved PGPE method through experiments. 1</p><p>4 0.60429043 <a title="300-lsi-4" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>Author: Javad Azimi, Alan Fern, Xiaoli Z. Fern</p><p>Abstract: Budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs. Typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments, which fail to capture important aspects of many real-world problems. This paper deﬁnes a novel problem formulation with the following important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic experiment durations; and 3) placing constraints on both the total number of experiments and the total experimental time. We develop both ofﬂine and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks. The results show that our algorithms produce highly effective schedules compared to natural baselines. 1</p><p>5 0.5922401 <a title="300-lsi-5" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>Author: Andre S. Barreto, Doina Precup, Joelle Pineau</p><p>Abstract: Kernel-based reinforcement-learning (KBRL) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees. However, the size of the approximator grows with the number of transitions, which makes the approach impractical for large problems. In this paper we introduce a novel algorithm to improve the scalability of KBRL. We resort to a special decomposition of a transition matrix, called stochastic factorization, to ﬁx the size of the approximator while at the same time incorporating all the information contained in the data. The resulting algorithm, kernel-based stochastic factorization (KBSF), is much faster but still converges to a unique solution. We derive a theoretical upper bound for the distance between the value functions computed by KBRL and KBSF. The effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems, including a difﬁcult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains. We empirically demonstrate that the proposed approach is able to compress the information contained in KBRL’s model. Also, on the tasks studied, KBSF outperforms two of the most prominent reinforcement-learning algorithms, namely least-squares policy iteration and ﬁtted Q-iteration. 1</p><p>6 0.59073806 <a title="300-lsi-6" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>7 0.57395703 <a title="300-lsi-7" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>8 0.55055892 <a title="300-lsi-8" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>9 0.54299682 <a title="300-lsi-9" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>10 0.53728813 <a title="300-lsi-10" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>11 0.52761871 <a title="300-lsi-11" href="./nips-2011-On_Strategy_Stitching_in_Large_Extensive_Form_Multiplayer_Games.html">196 nips-2011-On Strategy Stitching in Large Extensive Form Multiplayer Games</a></p>
<p>12 0.52312225 <a title="300-lsi-12" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>13 0.50459516 <a title="300-lsi-13" href="./nips-2011-Predicting_Dynamic_Difficulty.html">218 nips-2011-Predicting Dynamic Difficulty</a></p>
<p>14 0.50184727 <a title="300-lsi-14" href="./nips-2011-Autonomous_Learning_of_Action_Models_for_Planning.html">41 nips-2011-Autonomous Learning of Action Models for Planning</a></p>
<p>15 0.48721808 <a title="300-lsi-15" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>16 0.48133627 <a title="300-lsi-16" href="./nips-2011-Blending_Autonomous_Exploration_and_Apprenticeship_Learning.html">48 nips-2011-Blending Autonomous Exploration and Apprenticeship Learning</a></p>
<p>17 0.4613356 <a title="300-lsi-17" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>18 0.45588112 <a title="300-lsi-18" href="./nips-2011-Clustering_via_Dirichlet_Process_Mixture_Models_for_Portable_Skill_Discovery.html">52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</a></p>
<p>19 0.4502286 <a title="300-lsi-19" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>20 0.43920788 <a title="300-lsi-20" href="./nips-2011-Reconstructing_Patterns_of_Information_Diffusion_from_Incomplete_Observations.html">234 nips-2011-Reconstructing Patterns of Information Diffusion from Incomplete Observations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (4, 0.038), (20, 0.028), (26, 0.034), (31, 0.129), (33, 0.025), (43, 0.045), (45, 0.078), (57, 0.047), (71, 0.3), (74, 0.061), (79, 0.012), (83, 0.032), (99, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75478339 <a title="300-lda-1" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>Author: Joel Veness, Marc Lanctot, Michael Bowling</p><p>Abstract: Monte-Carlo Tree Search (MCTS) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments. The stochastic nature of the Monte-Carlo simulations introduces errors in the value estimates, both in terms of bias and variance. Whilst reducing bias (typically through the addition of domain knowledge) has been studied in the MCTS literature, comparatively little effort has focused on reducing variance. This is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics. In this paper, we examine the application of some standard techniques for variance reduction in MCTS, including common random numbers, antithetic variates and control variates. We demonstrate how these techniques can be applied to MCTS and explore their efﬁcacy on three different stochastic, single-agent settings: Pig, Can’t Stop and Dominion. 1</p><p>2 0.613662 <a title="300-lda-2" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>Author: Jun-ichiro Hirayama, Aapo Hyvärinen</p><p>Abstract: Components estimated by independent component analysis and related methods are typically not independent in real data. A very common form of nonlinear dependency between the components is correlations in their variances or energies. Here, we propose a principled probabilistic model to model the energycorrelations between the latent variables. Our two-stage model includes a linear mixing of latent signals into the observed ones like in ICA. The main new feature is a model of the energy-correlations based on the structural equation model (SEM), in particular, a Linear Non-Gaussian SEM. The SEM is closely related to divisive normalization which effectively reduces energy correlation. Our new twostage model enables estimation of both the linear mixing and the interactions related to energy-correlations, without resorting to approximations of the likelihood function or other non-principled approaches. We demonstrate the applicability of our method with synthetic dataset, natural images and brain signals. 1</p><p>3 0.59882265 <a title="300-lda-3" href="./nips-2011-Transfer_from_Multiple_MDPs.html">291 nips-2011-Transfer from Multiple MDPs</a></p>
<p>Author: Alessandro Lazaric, Marcello Restelli</p><p>Abstract: Transfer reinforcement learning (RL) methods leverage on the experience collected on a set of source tasks to speed-up RL algorithms. A simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task. In this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks. Finally, we report illustrative experimental results in a continuous chain problem.</p><p>4 0.57949686 <a title="300-lda-4" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study multi-label prediction for structured output sets, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multilabel classiﬁcation techniques are typically not applicable in this situation, because they require explicit enumeration of the label set, which is infeasible in case of structured outputs. Relying on techniques originally designed for single-label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems. In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneﬁcial properties with single-label maximum-margin approaches, in particular formulation as a convex optimization problem, efﬁcient working set training, and PAC-Bayesian generalization bounds. 1</p><p>5 0.5298087 <a title="300-lda-5" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>Author: Michael L. Wick, Andrew McCallum</p><p>Abstract: Traditional approaches to probabilistic inference such as loopy belief propagation and Gibbs sampling typically compute marginals for all the unobserved variables in a graphical model. However, in many real-world applications the user’s interests are focused on a subset of the variables, speciﬁed by a query. In this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten. In this paper we propose a query-speciﬁc approach to MCMC that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efﬁciency. Surprisingly there has been almost no previous work on query-aware MCMC. We demonstrate the success of our approach with positive experimental results on a wide range of graphical models. 1</p><p>6 0.52487761 <a title="300-lda-6" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>7 0.524602 <a title="300-lda-7" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>8 0.52049583 <a title="300-lda-8" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>9 0.52036023 <a title="300-lda-9" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>10 0.51903605 <a title="300-lda-10" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>11 0.51835406 <a title="300-lda-11" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>12 0.51741105 <a title="300-lda-12" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>13 0.51669872 <a title="300-lda-13" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>14 0.51608193 <a title="300-lda-14" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>15 0.51602823 <a title="300-lda-15" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>16 0.51522684 <a title="300-lda-16" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>17 0.51415807 <a title="300-lda-17" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>18 0.51356107 <a title="300-lda-18" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>19 0.51299226 <a title="300-lda-19" href="./nips-2011-Generalised_Coupled_Tensor_Factorisation.html">102 nips-2011-Generalised Coupled Tensor Factorisation</a></p>
<p>20 0.51246822 <a title="300-lda-20" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
