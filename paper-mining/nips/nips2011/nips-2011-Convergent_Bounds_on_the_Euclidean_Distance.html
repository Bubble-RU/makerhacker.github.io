<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2011-Convergent Bounds on the Euclidean Distance</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-64" href="#">nips2011-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 nips-2011-Convergent Bounds on the Euclidean Distance</h1>
<br/><p>Source: <a title="nips-2011-64-pdf" href="http://papers.nips.cc/paper/4353-convergent-bounds-on-the-euclidean-distance.pdf">pdf</a></p><p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>Reference: <a title="nips-2011-64-reference" href="../nips2011_reference/nips-2011-Convergent_Bounds_on_the_Euclidean_Distance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 kr  Abstract Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . [sent-3, score-0.395]
</p><p>2 For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . [sent-4, score-0.208]
</p><p>3 Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. [sent-5, score-0.501]
</p><p>4 Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. [sent-6, score-0.249]
</p><p>5 An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. [sent-7, score-0.105]
</p><p>6 The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. [sent-8, score-0.236]
</p><p>7 We provide experimental results on the nearest and the farthest neighbor searches. [sent-9, score-0.399]
</p><p>8 1  Introduction  The Euclidean distance between two vectors x and y in d-dimensional space is a typical distance measure that reﬂects their proximity in the space. [sent-10, score-0.402]
</p><p>9 Measuring the Euclidean distance is a fundamental operation in computer science, including the areas of database, computational geometry, computer vision and computer graphics. [sent-11, score-0.099]
</p><p>10 In machine learning, the Euclidean distance, denoted by dist(x, y), or it’s variations(for example, e||x−y|| ) are widely used to measure data similarity for clustering [1], classiﬁcation [2] and so on. [sent-12, score-0.05]
</p><p>11 Given two sets X and Y of vectors in d-dimensional space, our goal is to ﬁnd a pair (x, y), for x ∈ X and y ∈ Y , such that dist(x, y) is the optimum (minimum or maximum) over all such pairs. [sent-14, score-0.113]
</p><p>12 For the nearest or farthest neighbor searches, X is the set consisting of a single query point while Y consists of all candidate data points. [sent-15, score-0.451]
</p><p>13 In d dimensional space, a single distance computation already takes O(d) time, thus the cost for ﬁnding the nearest or farthest neighbor becomes O(dnm) time, where n and m are the cardinalities of X and Y , respectively. [sent-18, score-0.515]
</p><p>14 1  If we restrict ourselves to the nearest neighbor search, some methods using space partitioning trees such as KD-tree [4], R-tree [5], or their variations have been widely used. [sent-23, score-0.291]
</p><p>15 Recently, cover tree [6] has been used for high dimensional nearest neighbor search, but its construction time increases drastically as the dimension increases [7]. [sent-25, score-0.383]
</p><p>16 One of such methods is to compute a distance bound using the inner product approximation [8]. [sent-27, score-0.153]
</p><p>17 Another method is to compute a distance bound using bitwise operations [9]. [sent-29, score-0.164]
</p><p>18 In this paper, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . [sent-33, score-0.208]
</p><p>19 Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides tight upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. [sent-34, score-0.534]
</p><p>20 Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. [sent-35, score-0.249]
</p><p>21 We provide an analysis on a random sequence of k reﬁnement steps for 0 ≤ k ≤ d, which shows a good expectation on the lower and upper bounds. [sent-37, score-0.091]
</p><p>22 This can justify that the MS-distance provides very tight bounds in a few reﬁnement steps of a typical sequence. [sent-38, score-0.146]
</p><p>23 The MS-distance can be used to various applications where the Euclidean distance is a measure for proximity or similarity between objects. [sent-41, score-0.236]
</p><p>24 Among them, we provide experimental results on the nearest and the farthest neighbor searches. [sent-42, score-0.399]
</p><p>25 , xd ], we denote its mean by µx = d i=1 xi and its d 1 2 2 variance by σx = d i=1 (xi − µx ) . [sent-46, score-0.057]
</p><p>26 For a pair of vectors x and y, we can reformulate the squared Euclidean distance between x and y as follows. [sent-47, score-0.273]
</p><p>27 d  dist(x, y)2 =  (xi − yi )2 i=1 d  ((µx + ai ) − (µy + bi ))2  = i=1 d  (µ2 + 2ai µx + a2 + µ2 + 2bi µy + b2 − 2(µx µy + ai µy + bi µx + ai bi )) (1) x i y i  = i=1 d  (µ2 − 2µx µy + µ2 + a2 + b2 − 2ai bi ) x y i i  =  (2)  i=1 d  = d (µx − µy )2 + (σx + σy )2 − 2dσx σy − 2  ai bi  (3)  ai bi . [sent-55, score-1.962]
</p><p>28 (4)  i=1 d  = d (µx − µy )2 + (σx − σy )2 + 2dσx σy − 2 i=1  2  d  d  d  1 2 By the deﬁnitions of ai and bi , we have i=1 ai = i=1 bi = 0, and d i=1 a2 = σx . [sent-56, score-0.704]
</p><p>29 By the ﬁrst i properties, equation (1) is simpliﬁed to (2), and by the second property, equations (2) becomes (3) and (4). [sent-57, score-0.078]
</p><p>30 Note that equations (3) and (4) are composed of the mean and variance values (their products and squared values, multiplied by d) of x and y, except the last summations. [sent-58, score-0.074]
</p><p>31 Thus, once we preprocess V of n vectors such that both µx and σx for all x ∈ V are computed in O(dn) time and stored in a table of size O(n), this sum can be computed in constant time for any pair of vectors, regardless of the dimension. [sent-59, score-0.17]
</p><p>32 d  The last summation, i ai bi , is the inner product a, b , and therefore by applying the CauchySchwarz inequality we get d  d  | a, b | = |  ai bi | ≤  d  a2 )( i  (  i=1  i=1  b2 ) = dσx σy . [sent-60, score-0.755]
</p><p>33 i  (5)  i=1  This gives us the following upper and lower bounds of the squared Euclidean distance from equations (3) and (4). [sent-61, score-0.313]
</p><p>34 Lemma 1 For two d-dimensional vectors x, y, the followings hold. [sent-62, score-0.078]
</p><p>35 However, in some applications these bounds may not be tight enough. [sent-64, score-0.105]
</p><p>36 In this section, we introduce the MSdistance which not only provides lower and upper bounds of the Euclidean distance in constant time, but also could be reﬁned further in such a way to converge to the exact Euclidean distance within d steps. [sent-65, score-0.43]
</p><p>37 To do this, we reformulate the last term of equations (3) and (4), that is, the inner product a, b . [sent-66, score-0.111]
</p><p>38 If d  d  d  2 2 the norms ||a|| = i=1 ai or ||b|| = i=1 bi are zero, then i=1 ai bi = 0, thus the upper and lower bounds become the same. [sent-67, score-0.867]
</p><p>39 This implies that we can compute the exact Euclidean distance in constant time. [sent-68, score-0.144]
</p><p>40 We can also get equation (10) by i i switching the roles of the term −dσx σy and the term dσx σy in the above equations. [sent-72, score-0.053]
</p><p>41 Now we deﬁne the MS-distance between x and y in its lower bound form, denoted by MSL (x, y, k), by replacing the last term of equation (3) with equation (9), and in its upper bound form, denoted by MSU(x, y, k) by replacing the last term of equation (4) with equation (10). [sent-74, score-0.437]
</p><p>42 The MS-distance makes use of the nonincreasing intermediate values for its upper bound and the nondecreasing intermediate values for its lower bound. [sent-75, score-0.21]
</p><p>43 k  MSL (x, y, k)  = d (µx − µy )2 + (σx − σy )2 + σx σy i=0 k  MSU (x, y, k)  = d (µx − µy )2 + (σx + σy )2 − σx σy i=0  bi ai − σy σx  2  bi ai + σy σx  2  (11)  (12)  Properties. [sent-77, score-0.704]
</p><p>44 Note that equation (11) is nondecreasing and equation (12) is nonincreasing while i ai bi ai bi increases from 0 to d, because d, σx , and σy are all nonnegative, and ( σy − σx )2 and ( σy + σx )2 are also nonnegative for all i. [sent-78, score-0.884]
</p><p>45 This is very useful because, in equation (11), the ﬁrst term, MSL(x, y, 0), is already a lower bound of dist(x, y)2 by inequality (6) , and the lower bound can be reﬁned further nondecreasingly over the summation in the second term. [sent-79, score-0.237]
</p><p>46 If we stop the summation at i = k, for k < d, the intermediate result is also a reﬁned lower bounds of dist(x, y)2 . [sent-80, score-0.204]
</p><p>47 Similarly, in equation (12), the ﬁrst term, MSU(x, y, 0), is already an upper bound of dist(x, y)2 by inequality (7) , and the upper bound can be reﬁned further nonincreasingly over the summation in the second term. [sent-81, score-0.259]
</p><p>48 This means we can stop the summation as soon as we ﬁnd a bound good enough for the application under consideration. [sent-82, score-0.097]
</p><p>49 Lemma 2 (Monotone Convergence) Let MSL(x, y, k) and MSU(x, y, k) be the lower and upper bounds of MS-distance as deﬁned above, respectively. [sent-85, score-0.163]
</p><p>50 Let φ denote a threshold for ﬁltering deﬁned in some proximity search problem under consideration. [sent-94, score-0.147]
</p><p>51 If φ < MSL(x, y, 0) in case of nearest search or φ > MSL(x, y, 0) in case of farthest search, we do not need to consider this pair (x, y) as a candidate, thus we can save time from computing their exact Euclidean distance. [sent-95, score-0.423]
</p><p>52 , xd ] into a pair of points, ˆ ˆ ˆ ˆ x+ and x− , in the 2-dimensional plane such that x+ = [µx , σx ] and x− = [µx , −σx ]. [sent-99, score-0.094]
</p><p>53 x ˆ  (13) (14)  To see why it is useful in fast ﬁltering, consider the case of ﬁnding the nearest vector. [sent-101, score-0.167]
</p><p>54 For ddimensional vectors in V of size n, we have n pairs of points in the plane as in Figure 1. [sent-102, score-0.13]
</p><p>55 Let q be a query vector, and let q+ denote the point mapped in the plane as deﬁned above. [sent-104, score-0.113]
</p><p>56 Among these n points lying on or below µ-axis, let ˆi ˆ x− be the point that is nearest to q+ . [sent-105, score-0.136]
</p><p>57 Note that the closest point from the query can be computed efﬁciently in 2-dimensional space, for example, after constructing some space partitioning structures such as kd-trees or R-trees, each query can be answered in poly-logarithmic search time. [sent-106, score-0.182]
</p><p>58 4  ˆ Then we can ignore all d-dimensional vectors x whose mapped point x+ lies outside the circle ˆ centered at q+ and of radius dist(ˆ + , x− ) in the plane, because they are strictly farther than xi q ˆi from q. [sent-107, score-0.225]
</p><p>59 All d-dimensional vectors x whose ˆ mapped point x+ lies outside the circle are strictly farther than xi from q. [sent-109, score-0.225]
</p><p>60 Observe that MSL(x, y, k) is almost the same as MSL(x, y, k − 1) if bk /σy ≈ ak /σx . [sent-111, score-0.076]
</p><p>61 Hence, in the worst case, MSL(x, y, 0) = MSL(x, y, d − 1) < MSL(x, y, d) = dist(x, y)2 when bk /σy = ak /σx for all k = 0, 1, . [sent-112, score-0.076]
</p><p>62 Therefore, if we need a lower bound strictly better than MSL (x, y, 0), then we need to go through all d reﬁnement steps, which takes O(d) time. [sent-116, score-0.085]
</p><p>63 Consider a random order for the last term in equation MSL (x, y, k) and for the last term in equation MSU (x, y, k). [sent-119, score-0.152]
</p><p>64 We measure the expected quality of the bounds by the difference between the bounds, that is, MSU(x, y, k) − MSL(x, y, k) as follows. [sent-122, score-0.092]
</p><p>65 k  MSU (x, y, k) − MSL (x, y, k)  = 4dσx σy − 2σx σy i=0  = 4dσx σy − 2σx σy  k d  d  i=0  aγ(i) σx  2  ai σx  2  +  +  bi σy  2  bγ(i) σy  = 4dσx σy − 4kσx σy = 4σx σy (d − k)  (15) 2  (16) (17) (18)  Let us explain how we get Equation (16) from (15). [sent-123, score-0.352]
</p><p>66 d  d  2 2 Equations (17) and (18) are because i=1 a2 = dσx and i=1 b2 = dσy by deﬁnitions of ai and bi . [sent-128, score-0.352]
</p><p>67 This shows a good theoretical expectation on the lower and upper bounds. [sent-133, score-0.091]
</p><p>68 This can justify that the MS-distance provides very tight bounds in a few reﬁnement steps of a typical sequence. [sent-134, score-0.146]
</p><p>69 5  Applications : Proximity Searches  The MS-distance can be used to application problems where the Euclidean distance is a measure for proximity or similarity of objects. [sent-135, score-0.236]
</p><p>70 As a case study, we implemented the nearest neighbor search (NNS) and the farthest neighbor search (FNS) using the MS-distance. [sent-136, score-0.638]
</p><p>71 Given a set X of d-dimensional vectors xi , for i = 1, . [sent-137, score-0.111]
</p><p>72 , n, and a d-dimensional query vector q, we use the following simple randomized algorithm for NNS. [sent-140, score-0.052]
</p><p>73 Consider the vectors in X one at a time according to this sequence. [sent-143, score-0.098]
</p><p>74 , d : if MSL(q, xi , j) > φ : break; if j = d: φ = MSL(q, xi , d); NN = i; 2. [sent-148, score-0.066]
</p><p>75 return NN as the nearest neighbor of q with the squared Euclidean distance φ. [sent-149, score-0.38]
</p><p>76 Note that the ﬁrst line of the pseudocodes ﬁlters out the vectors whose distance to q is larger than φ as in the fast ﬁltering in Section 3. [sent-150, score-0.247]
</p><p>77 In the for loop, we compute MSL(q, xi , j) from MSL(q, xi , j −1) in constant time. [sent-151, score-0.083]
</p><p>78 From the last two lines of the pseudocodes, we update φ to the exact Euclidean distance between q and xi and store the index as the current nearest neighbor (NN). [sent-152, score-0.438]
</p><p>79 The algorithm for the farthest neighbor search is similar to this one, except that it uses MSU(xi , y, j) and maintains the maximum distance. [sent-153, score-0.323]
</p><p>80 For empirical comparison, we implemented a linear search algorithm that simply computes distances from q to every xi and chooses the one with the minimum distance. [sent-154, score-0.134]
</p><p>81 We also used the implementation of the cover tree [6]. [sent-155, score-0.091]
</p><p>82 A cover tree is a data structure that supports fast nearest neighbor queries given a ﬁxed intrinsic dimensionality [7]. [sent-156, score-0.438]
</p><p>83 We selected data sets D from various dimensions (from 10 to 100, 000), and randomly selected 30 queries points Q ⊂ D, and queried them on D \ Q. [sent-158, score-0.063]
</p><p>84 Probably this is because the distances from queries to their nearest vectors tend to converge to the distances to their farthest vectors as described in [13]. [sent-166, score-0.586]
</p><p>85 However, on such high dimensions, both the linear search and the cover tree algorithm also show poor performance. [sent-168, score-0.151]
</p><p>86 Figure 3 shows the preprocessing time of the MS-distance and the cover tree for NNS. [sent-169, score-0.135]
</p><p>87 Cover  Figure 3: Preprocessing time for nearest neighbor search in log-scaled second. [sent-175, score-0.335]
</p><p>88 This is because for the MS-distance it requires only O(dn) time to compute the mean and the standard deviation values. [sent-177, score-0.051]
</p><p>89 Linear  Figure 5: Relative running time for the farthest neighbor search queries, normalized by linear search time. [sent-189, score-0.403]
</p><p>90 The graph shows the query time that is normalized by the linear search time. [sent-191, score-0.132]
</p><p>91 It is clear that the ﬁltering algorithm based on the MS-distance beats the linear search algorithm, even on high dimensional data in the results. [sent-192, score-0.077]
</p><p>92 The cover tree, which is designed exclusively for NNS, shows slightly better query performance than ours. [sent-193, score-0.117]
</p><p>93 However, the MS-distance is more general and ﬂexible: it supports addition of a new vector to the data set (our data structure) in O(d) time for computing the mean and the standard deviation values of the vector. [sent-194, score-0.068]
</p><p>94 This is outstanding compared to the linear search algorithm. [sent-198, score-0.06]
</p><p>95 6  Conclusion  We introduce a fast distance bounding technique, called the MS-distance, by using the mean and the standard deviation values. [sent-200, score-0.161]
</p><p>96 The MS-distance between two vectors provides upper and lower bounds of Euclidean distance between them in constant time, and these bounds converge monotonically to the exact Euclidean distance over iteration. [sent-201, score-0.606]
</p><p>97 The MS-distance can be used to application problems where the Euclidean distance is a measure for proximity or similarity of objects. [sent-202, score-0.236]
</p><p>98 The experimental results show that our method is efﬁcient enough even to replace the best known algorithms for proximity searches. [sent-203, score-0.087]
</p><p>99 Dimensionality reduction and similarity computation by inner g g product approximations. [sent-250, score-0.058]
</p><p>100 idistance: An adaptive b+-tree based indexing method for nearest neighbor search. [sent-270, score-0.255]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('msl', 0.671), ('msu', 0.454), ('dist', 0.23), ('bi', 0.202), ('ai', 0.15), ('farthest', 0.144), ('euclidean', 0.136), ('nearest', 0.136), ('neighbor', 0.119), ('nement', 0.1), ('distance', 0.099), ('proximity', 0.087), ('nns', 0.087), ('fns', 0.079), ('vectors', 0.078), ('bounds', 0.072), ('cover', 0.065), ('search', 0.06), ('equation', 0.053), ('summation', 0.052), ('query', 0.052), ('upper', 0.051), ('re', 0.047), ('ltering', 0.045), ('queries', 0.044), ('distances', 0.041), ('lower', 0.04), ('bitwise', 0.039), ('pohang', 0.039), ('pseudocodes', 0.039), ('bk', 0.039), ('dn', 0.038), ('ak', 0.037), ('reformulate', 0.035), ('pair', 0.035), ('plane', 0.035), ('tight', 0.033), ('xi', 0.033), ('nn', 0.033), ('korea', 0.032), ('sigmod', 0.032), ('deviation', 0.031), ('fast', 0.031), ('similarity', 0.03), ('inner', 0.028), ('exact', 0.028), ('ltered', 0.027), ('nonincreasing', 0.027), ('farther', 0.027), ('squared', 0.026), ('monotonically', 0.026), ('bound', 0.026), ('tree', 0.026), ('mapped', 0.026), ('equations', 0.025), ('preprocessing', 0.024), ('nondecreasing', 0.024), ('usa', 0.024), ('converge', 0.024), ('xd', 0.024), ('nonnegative', 0.023), ('spent', 0.023), ('circle', 0.023), ('ny', 0.023), ('last', 0.023), ('justify', 0.022), ('management', 0.021), ('searches', 0.021), ('intermediate', 0.021), ('time', 0.02), ('measure', 0.02), ('lies', 0.019), ('typical', 0.019), ('stop', 0.019), ('uci', 0.019), ('lter', 0.019), ('strictly', 0.019), ('database', 0.019), ('dimensions', 0.019), ('replacing', 0.018), ('trees', 0.018), ('acm', 0.018), ('partitioning', 0.018), ('york', 0.018), ('constant', 0.017), ('ramakrishnan', 0.017), ('arcene', 0.017), ('archive', 0.017), ('covertype', 0.017), ('ddimensional', 0.017), ('hwang', 0.017), ('isolet', 0.017), ('madelon', 0.017), ('mest', 0.017), ('parkinsons', 0.017), ('schek', 0.017), ('ubuntu', 0.017), ('weber', 0.017), ('supports', 0.017), ('dimensional', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="64-tfidf-1" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>2 0.10213019 <a title="64-tfidf-2" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>Author: Dominique Tschopp, Suhas Diggavi, Payam Delgosha, Soheil Mohajer</p><p>Abstract: This paper addresses the problem of ﬁnding the nearest neighbor (or one of the R-nearest neighbors) of a query object q in a database of n objects, when we can only use a comparison oracle. The comparison oracle, given two reference objects and a query object, returns the reference object most similar to the query object. The main problem we study is how to search the database for the nearest neighbor (NN) of a query, while minimizing the questions. The difﬁculty of this problem depends on properties of the underlying database. We show the importance of a characterization: combinatorial disorder D which deﬁnes approximate triangle n inequalities on ranks. We present a lower bound of Ω(D log D + D2 ) average number of questions in the search phase for any randomized algorithm, which demonstrates the fundamental role of D for worst case behavior. We develop 3 a randomized scheme for NN retrieval in O(D3 log2 n + D log2 n log log nD ) 3 questions. The learning requires asking O(nD3 log2 n + D log2 n log log nD ) questions and O(n log2 n/ log(2D)) bits to store.</p><p>3 0.089594699 <a title="64-tfidf-3" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>4 0.078577176 <a title="64-tfidf-4" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>Author: Dan Garber, Elad Hazan</p><p>Abstract: In recent years semideﬁnite optimization has become a tool of major importance in various optimization and machine learning problems. In many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms. In this work we present the ﬁrst sublinear time approximation algorithm for semideﬁnite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice. We present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric. 1</p><p>5 0.076214358 <a title="64-tfidf-5" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>Author: Carl Vondrick, Deva Ramanan</p><p>Abstract: We introduce a novel active learning framework for video annotation. By judiciously choosing which frames a user should annotate, we can obtain highly accurate tracks with minimal user effort. We cast this problem as one of active learning, and show that we can obtain excellent performance by querying frames that, if annotated, would produce a large expected change in the estimated object track. We implement a constrained tracker and compute the expected change for putative annotations with efﬁcient dynamic programming algorithms. We demonstrate our framework on four datasets, including two benchmark datasets constructed with key frame annotations obtained by Amazon Mechanical Turk. Our results indicate that we could obtain equivalent labels for a small fraction of the original cost. 1</p><p>6 0.074561507 <a title="64-tfidf-6" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>7 0.073496282 <a title="64-tfidf-7" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>8 0.063916348 <a title="64-tfidf-8" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>9 0.059274495 <a title="64-tfidf-9" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>10 0.053277746 <a title="64-tfidf-10" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>11 0.05322919 <a title="64-tfidf-11" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>12 0.053187292 <a title="64-tfidf-12" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>13 0.052865013 <a title="64-tfidf-13" href="./nips-2011-Convergence_Rates_of_Inexact_Proximal-Gradient_Methods_for_Convex_Optimization.html">63 nips-2011-Convergence Rates of Inexact Proximal-Gradient Methods for Convex Optimization</a></p>
<p>14 0.051741328 <a title="64-tfidf-14" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>15 0.047055662 <a title="64-tfidf-15" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>16 0.04493982 <a title="64-tfidf-16" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>17 0.044595338 <a title="64-tfidf-17" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>18 0.043518946 <a title="64-tfidf-18" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>19 0.04339309 <a title="64-tfidf-19" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>20 0.043148559 <a title="64-tfidf-20" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.001), (2, -0.035), (3, -0.008), (4, -0.002), (5, 0.029), (6, -0.026), (7, -0.033), (8, -0.038), (9, -0.065), (10, 0.014), (11, 0.023), (12, -0.047), (13, -0.003), (14, 0.081), (15, 0.036), (16, -0.014), (17, 0.135), (18, -0.058), (19, -0.039), (20, 0.089), (21, 0.002), (22, 0.138), (23, -0.015), (24, -0.088), (25, -0.069), (26, -0.022), (27, 0.069), (28, -0.011), (29, -0.044), (30, 0.008), (31, -0.097), (32, -0.034), (33, 0.038), (34, -0.031), (35, -0.053), (36, 0.034), (37, 0.08), (38, 0.03), (39, 0.055), (40, 0.063), (41, -0.032), (42, 0.037), (43, -0.082), (44, 0.034), (45, -0.058), (46, -0.013), (47, 0.157), (48, 0.088), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92735207 <a title="64-lsi-1" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>2 0.54863042 <a title="64-lsi-2" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>Author: Michael Shindler, Alex Wong, Adam W. Meyerson</p><p>Abstract: Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o( nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.</p><p>3 0.53780669 <a title="64-lsi-3" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>Author: Dominique Tschopp, Suhas Diggavi, Payam Delgosha, Soheil Mohajer</p><p>Abstract: This paper addresses the problem of ﬁnding the nearest neighbor (or one of the R-nearest neighbors) of a query object q in a database of n objects, when we can only use a comparison oracle. The comparison oracle, given two reference objects and a query object, returns the reference object most similar to the query object. The main problem we study is how to search the database for the nearest neighbor (NN) of a query, while minimizing the questions. The difﬁculty of this problem depends on properties of the underlying database. We show the importance of a characterization: combinatorial disorder D which deﬁnes approximate triangle n inequalities on ranks. We present a lower bound of Ω(D log D + D2 ) average number of questions in the search phase for any randomized algorithm, which demonstrates the fundamental role of D for worst case behavior. We develop 3 a randomized scheme for NN retrieval in O(D3 log2 n + D log2 n log log nD ) 3 questions. The learning requires asking O(nD3 log2 n + D log2 n log log nD ) questions and O(n log2 n/ log(2D)) bits to store.</p><p>4 0.50486034 <a title="64-lsi-4" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>5 0.44428059 <a title="64-lsi-5" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>6 0.44078216 <a title="64-lsi-6" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>7 0.42488742 <a title="64-lsi-7" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>8 0.41977969 <a title="64-lsi-8" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>9 0.41136649 <a title="64-lsi-9" href="./nips-2011-Active_Learning_Ranking_from_Pairwise_Preferences_with_Almost_Optimal_Query_Complexity.html">20 nips-2011-Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity</a></p>
<p>10 0.40948078 <a title="64-lsi-10" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>11 0.40489107 <a title="64-lsi-11" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>12 0.39374256 <a title="64-lsi-12" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>13 0.36802873 <a title="64-lsi-13" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>14 0.36612296 <a title="64-lsi-14" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>15 0.35255814 <a title="64-lsi-15" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>16 0.34446055 <a title="64-lsi-16" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>17 0.33475164 <a title="64-lsi-17" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>18 0.33271399 <a title="64-lsi-18" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>19 0.32769278 <a title="64-lsi-19" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>20 0.3264603 <a title="64-lsi-20" href="./nips-2011-The_Manifold_Tangent_Classifier.html">287 nips-2011-The Manifold Tangent Classifier</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.029), (4, 0.115), (20, 0.034), (25, 0.235), (26, 0.038), (31, 0.069), (33, 0.022), (43, 0.059), (45, 0.118), (57, 0.046), (65, 0.011), (74, 0.032), (83, 0.06), (99, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.7989639 <a title="64-lda-1" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>same-paper 2 0.77976203 <a title="64-lda-2" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>3 0.72337437 <a title="64-lda-3" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>4 0.64413232 <a title="64-lda-4" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A nonparametric kernel-based method for realizing Bayes’ rule is proposed, based on kernel representations of probabilities in reproducing kernel Hilbert spaces. The prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample. The kernel Bayes’ rule can be applied to a wide variety of Bayesian inference problems: we demonstrate Bayesian computation without likelihood, and ﬁltering with a nonparametric statespace model. A consistency rate for the posterior estimate is established. 1</p><p>5 0.64000607 <a title="64-lda-5" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>Author: Yibiao Zhao, Song-chun Zhu</p><p>Abstract: This paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3D scene layout, detecting 3D objects (e.g. furniture), detecting 2D faces (windows, doors etc.), and segmenting background. In contrast to previous scene labeling work that applied discriminative classiﬁers to pixels (or super-pixels), we use a generative Stochastic Scene Grammar (SSG). This grammar represents the compositional structures of visual entities from scene categories, 3D foreground/background, 2D faces, to 1D lines. The grammar includes three types of production rules and two types of contextual relations. Production rules: (i) AND rules represent the decomposition of an entity into sub-parts; (ii) OR rules represent the switching among sub-types of an entity; (iii) SET rules represent an ensemble of visual entities. Contextual relations: (i) Cooperative “+” relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) Competitive “-” relations represents negative links between competing entities, such as mutually exclusive boxes. We design an efﬁcient MCMC inference algorithm, namely Hierarchical cluster sampling, to search in the large solution space of scene conﬁgurations. The algorithm has two stages: (i) Clustering: It forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations. (ii) Sampling: It jumps between alternative structures (clusters) in each layer of the hierarchy to ﬁnd the most probable conﬁguration (represented by a parse tree). In our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset. In addition, our approach achieves richer structures in the parse tree. 1</p><p>6 0.63797075 <a title="64-lda-6" href="./nips-2011-Target_Neighbor_Consistent_Feature_Weighting_for_Nearest_Neighbor_Classification.html">279 nips-2011-Target Neighbor Consistent Feature Weighting for Nearest Neighbor Classification</a></p>
<p>7 0.63368261 <a title="64-lda-7" href="./nips-2011-Phase_transition_in_the_family_of_p-resistances.html">213 nips-2011-Phase transition in the family of p-resistances</a></p>
<p>8 0.63130599 <a title="64-lda-8" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>9 0.63056415 <a title="64-lda-9" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>10 0.62398684 <a title="64-lda-10" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>11 0.62233973 <a title="64-lda-11" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>12 0.61877751 <a title="64-lda-12" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>13 0.61760741 <a title="64-lda-13" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>14 0.61492896 <a title="64-lda-14" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>15 0.61426759 <a title="64-lda-15" href="./nips-2011-Newtron%3A_an_Efficient_Bandit_algorithm_for_Online_Multiclass_Prediction.html">185 nips-2011-Newtron: an Efficient Bandit algorithm for Online Multiclass Prediction</a></p>
<p>16 0.61149639 <a title="64-lda-16" href="./nips-2011-See_the_Tree_Through_the_Lines%3A_The_Shazoo_Algorithm.html">242 nips-2011-See the Tree Through the Lines: The Shazoo Algorithm</a></p>
<p>17 0.61143243 <a title="64-lda-17" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>18 0.61032051 <a title="64-lda-18" href="./nips-2011-Efficient_Online_Learning_via_Randomized_Rounding.html">80 nips-2011-Efficient Online Learning via Randomized Rounding</a></p>
<p>19 0.61005223 <a title="64-lda-19" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>20 0.60924864 <a title="64-lda-20" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
