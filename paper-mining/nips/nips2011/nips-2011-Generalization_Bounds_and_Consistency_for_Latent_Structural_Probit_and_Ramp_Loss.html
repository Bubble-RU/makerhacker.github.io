<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-103" href="#">nips2011-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</h1>
<br/><p>Source: <a title="nips-2011-103-pdf" href="http://papers.nips.cc/paper/4268-generalization-bounds-and-consistency-for-latent-structural-probit-and-ramp-loss.pdf">pdf</a></p><p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><p>Reference: <a title="nips-2011-103-reference" href="../nips2011_reference/nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider latent structural versions of probit loss and ramp loss. [sent-3, score-1.038]
</p><p>2 We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. [sent-4, score-0.592]
</p><p>3 We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. [sent-5, score-0.27]
</p><p>4 These bounds suggest that probit loss converges more rapidly. [sent-6, score-0.476]
</p><p>5 However, ramp loss is more easily optimized on a given sample. [sent-7, score-0.669]
</p><p>6 No metric is perfect and any metric is controversial, but quantitative metrics provide a basis for quantitative experimentation and quantitative experimentation has lead to real progress. [sent-14, score-0.12]
</p><p>7 Here we adopt the convention that a performance metric is given as a task loss — a measure of a quantity of error or cost such as the word error rate in speech recognition. [sent-15, score-0.262]
</p><p>8 We consider general methods for minimizing task loss at evaluation time. [sent-16, score-0.222]
</p><p>9 Although the goal is to minimize task loss, most systems are trained by minimizing a surrogate loss different from task loss. [sent-17, score-0.321]
</p><p>10 A surrogate loss is necessary when using scale-sensitive regularization in training a linear classiﬁer. [sent-18, score-0.303]
</p><p>11 So directly regularizing the task loss of a linear classiﬁer is meaningless. [sent-22, score-0.222]
</p><p>12 For binary classiﬁcation standard surrogate loss functions include log loss, hinge loss, probit loss, and ramp loss. [sent-23, score-1.06]
</p><p>13 The standard surrogate loss functions for binary classiﬁcation have generalizations to the structured output setting. [sent-25, score-0.304]
</p><p>14 Structural log loss is used in conditional random ﬁelds (CRFs) [7]. [sent-26, score-0.198]
</p><p>15 Structural hinge loss is used in structural SVMs [13, 14]. [sent-27, score-0.374]
</p><p>16 Structural probit loss is deﬁned and empirically evaluated in [6]. [sent-28, score-0.421]
</p><p>17 A version of structural ramp loss is deﬁned and empirically evaluated in [3] (but see also [12] for a treatment of the fundamental motivation for ramp loss). [sent-29, score-1.223]
</p><p>18 All four of these structural surrogate loss functions are deﬁned formally in section 2. [sent-30, score-0.356]
</p><p>19 1 1  The deﬁnition of ramp loss used here is slightly different from that in [3]. [sent-31, score-0.669]
</p><p>20 1  This paper is concerned with developing a better theoretical understanding of the relationship between surrogate loss training and task loss testing for structured labels. [sent-32, score-0.542]
</p><p>21 Structural ramp loss is justiﬁed in [3] as being a tight upper bound on task loss. [sent-33, score-0.742]
</p><p>22 But of course the tightest upper bound on task loss is the task loss itself. [sent-34, score-0.493]
</p><p>23 A ﬁnite sample generalization bound for probit loss was stated implicitly in [9] and an explicit probit loss bound is given in [6]. [sent-36, score-0.947]
</p><p>24 Here we review the ﬁnite sample bounds for probit loss and prove a ﬁnite sample bound for ramp loss. [sent-37, score-0.977]
</p><p>25 Both probit loss and ramp loss can be optimized in practice by stochastic gradient descent. [sent-40, score-1.09]
</p><p>26 The subgradient update for ramp loss is similar to a perceptron update — the update is a difference between a “good” feature vector and a “bad” feature vector. [sent-42, score-0.786]
</p><p>27 Ramp loss updates are closely related to updates derived from n-best lists in training machine translaiton systems [8, 2]. [sent-43, score-0.214]
</p><p>28 Ramp loss updates regularized by early stopping have been shown to be effective in phoneme alignment [10]. [sent-44, score-0.286]
</p><p>29 It is also shown in [10] that in the limit of large weight vectors the expected ramp loss update converges to the true gradient of task loss. [sent-45, score-0.774]
</p><p>30 This result suggests consistency for ramp loss, a suggestion conﬁrmed here. [sent-46, score-0.522]
</p><p>31 A practical stochastic gradient descent algorithm for structural probit loss is given in [6] where it is also shown that probit loss can be effective for phoneme recognition. [sent-47, score-0.965]
</p><p>32 Although the generalization bounds suggest that probit loss converges faster than ramp loss, ramp loss seems easier to optimize. [sent-48, score-1.671]
</p><p>33 We formulate all the notions of loss in the presence of latent structure as well as structured labels. [sent-49, score-0.292]
</p><p>34 For example, in natural language translation the alignment between the words in the source and the words in the target is not explicitly given in a translation pair. [sent-51, score-0.143]
</p><p>35 Grammatical structures are also not given in a translation pair but may be constructed as part of the translation process. [sent-52, score-0.11]
</p><p>36 Although the presence of latent structure makes log loss and hinge loss non-convex, latent strucure seems essential in many applications. [sent-54, score-0.615]
</p><p>37 Latent structural log loss, and the notion of a hidden CRF, is formulated in [11]. [sent-55, score-0.083]
</p><p>38 Latent structural hinge loss, and the notion of a latent structural SVM, is formulated in [15]. [sent-56, score-0.322]
</p><p>39 We assume a loss function L such that for any two labels y and y we have ˆ that L(y, y ) ∈ [0, 1] is the loss (or cost) when the true label is y and we predict y . [sent-61, score-0.449]
</p><p>40 We will be interested in linear predictors involving latent structure. [sent-64, score-0.089]
</p><p>41 For example, we might take Z to be the set of all parse trees of source and target sentences in a machien translation system. [sent-66, score-0.096]
</p><p>42 In machine translation the label y is typically a sentence with no parse tree speciﬁed. [sent-67, score-0.11]
</p><p>43 We can recover the pure structural case, with no latent information, by taking Z to be a singleton set. [sent-68, score-0.175]
</p><p>44 It will be convenient to deﬁne S to be the set of pairs of a label and a latent label. [sent-69, score-0.095]
</p><p>45 We assume a feature y ˆ map φ such that for an input x and augmented label s we have φ(x, s) ∈ 2 with ||φ(x, s)|| ≤ 1. [sent-71, score-0.09]
</p><p>46 2 Given an input x and a weight vector w ∈ sw (x) ˆ  2  =  we deﬁne the prediction sw (x) as follows. [sent-72, score-0.537]
</p><p>47 ˆ argmax w φ(x, s) s  2 We note that this setting covers the ﬁnite dimensional case because the range of the feature map can be taken to be a ﬁnite dimensional subset of 2 — we are not assuming a universal feature map. [sent-73, score-0.082]
</p><p>48 2  Our goal is to use the training data to learn a weight vector w so as to minimize the expected loss on newly drawn labeled data Ex,y [L(y, sw (x))]. [sent-74, score-0.508]
</p><p>49 L(w, x, y) = L(y, sw (x)) ˆ L(w) = Ex,y [L(w, x, y)] L∗ = inf w∈ 2 L(w)  ˆ Ln (w) =  n i=1  1 n  L(w, xi , yi )  We adopt the convention that in the deﬁnition of L(w, x, y) we break ties in deﬁnition of sw (x) in ˆ favor of augmented labels of larger loss. [sent-79, score-0.578]
</p><p>50 Here we deﬁne latent structural log loss, hinge loss, ramp loss and probit loss as follows. [sent-81, score-1.329]
</p><p>51 Llog (w, x, y) = ln (1 + e−m )  Lhinge (w, x, y) = max(0, 1 − m)  Lramp (w, x, y) = min(1, max(0, 1 − m)) Lprobit (w, x, y) = P  ∼N (0,1) [  ≥ m]  Returning to the general case we consider the relationship between hinge and ramp loss. [sent-87, score-0.743]
</p><p>52 First we consider the case where Z is a singleton set — the case of no latent structure. [sent-88, score-0.092]
</p><p>53 In this case hinge loss is convex in w — the hinge loss becomes a maximum of linear functions. [sent-89, score-0.582]
</p><p>54 Also, in the case where Z is singleton one can easily see that hinge loss is unbounded — wrong labels may score arbitrarily better than the given label. [sent-91, score-0.355]
</p><p>55 Hinge loss remains unbounded in case of non-singleton Z. [sent-92, score-0.236]
</p><p>56 =  max w Φ(x, s) + L(y, s) − w Φ(x, sw (x)) ˆ  ≤  Lramp (w, x, y)  max w Φ(x, s) + 1 − w Φ(x, sw (x)) = 1 ˆ  s  s  Next, as is emphasized in [3], we note that ramp loss is a tighter upper bound on task loss than is hinge loss. [sent-94, score-1.623]
</p><p>57 3  Furthermore, the following derivation shows Lramp (w, x, y) ≥ L(w, x, y) where we assume pessimistic tie breaking in the deﬁnition of sw (x). [sent-99, score-0.334]
</p><p>58 ˆ Lramp (w, x, y)  =  max w Φ(x, s) + L(y, s) − w Φ(x, sw (x)) ˆ s  ≥ w Φ(x, sw (x)) + L(y, sw (x)) − w Φ(x, sw (x)) = L(y, sw (x)) ˆ ˆ ˆ ˆ But perhaps the most important property of ramp loss is the following. [sent-100, score-1.982]
</p><p>59 lim Lramp (αw, x, y) = L(w, x, y)  α→∞  (1)  This can be veriﬁed by noting that as α goes to inﬁnity the maximum of the ﬁrst term in ramp loss must occur at s = sw (x). [sent-101, score-1.052]
</p><p>60 ∆w s+ (x, y) ˆw  ∝ φ(x, sw (x)) − φ(x, s+ (x, y)) ˆ ˆw =  (2)  argmax w φ(x, s) + L(y, s) s  We will refer to (2) as the ramp loss update rule. [sent-103, score-0.946]
</p><p>61 w L(w)  = lim αEx,y φ(x, s+ (x, y)) − φ(x, sw (x)) ˆαw ˆ α→∞  (3)  Equation (3) expresses a relationship between the expected ramp loss update and the gradient of generalization loss. [sent-105, score-1.098]
</p><p>62 Signiﬁcant empirical success has been achieved with the ramp loss update rule using early stopping regularization [10]. [sent-106, score-0.736]
</p><p>63 But both (1) and (3) suggests that regularized ramp loss should be consistent as is conﬁrmed here. [sent-107, score-0.669]
</p><p>64 Finally it is worth noting that Lramp and Lprobit are meaningful for an arbitrary prediction space S, label space Y, and loss function L(y, s) between a label and a prediction. [sent-108, score-0.277]
</p><p>65 Log loss and hinge loss can be generalized to arbitrary prediction and label spaces provided that we assume a compatibility relation between predictions and labels. [sent-109, score-0.521]
</p><p>66 The framework of independent prediction and label spaces is explored more fully in [5] where a notion of weak-label SVM is deﬁned subsuming both ramp and hinge loss as special cases. [sent-110, score-0.794]
</p><p>67 3  Consistency of Probit Loss  We start with the consistency of probit loss which is easier to prove. [sent-111, score-0.486]
</p><p>68 ˆ probit (w) + λn ||w||2 wn = argmin Ln ˆ 2n w  (4)  We now prove the following fairly straightforward consequence of a generalization bound appearing in [6]. [sent-113, score-0.498]
</p><p>69 For wn deﬁned by (4), if the sequence λn increases without ˆ bound, and λn ln n/n converges to zero, then with probability one over the draw of the inﬁnite sample we have limn→∞ Lprobit (wn ) = L∗ . [sent-115, score-0.43]
</p><p>70 However, the learning algorithm (4) achieves consistency in the sense that the stochastic predictor deﬁned by wn + where is Gaussian noise ˆ has a loss which converges to L∗ . [sent-120, score-0.477]
</p><p>71 To prove theorem 1 we start by reviewing the generalization bound of [6]. [sent-121, score-0.121]
</p><p>72 The departure point for this generalization bound is the following PAC-Bayesian theorem where P and Q range over ˆ probability measures on a given space of predictors and L(Q) and Ln (Q) are deﬁned as expectations over selecting a predictor from Q. [sent-122, score-0.178]
</p><p>73 L(Q) ≤  1 1 1 − 2λ  ˆ Ln (Q) + λ  KL(Q, P ) + ln 1 δ n  (5)  For the space of linear predictors we take the prior P to be the zero-mean unit-variance Gaussian distribution and for w ∈ 2 we deﬁne the distribution Qw to be the unit-variance Gaussian centered at w. [sent-125, score-0.205]
</p><p>74 Lprobit (w) ≤  1 1 1 − 2λn  ˆ probit (w) + λn Ln  + ln 1 δ n  1 2 2 ||w||  (6)  To prove theorem 1 from (6) we consider an arbitrary unit-norm weight vector w∗ and an arbitrary scalar α > 0. [sent-129, score-0.494]
</p><p>75 Setting δ to 1/n2 , and noting that wn is the minimizer of the right hand side of (6), ˆ we have the following with probability at least 1 − 1/n2 over the draw of the sample. [sent-130, score-0.242]
</p><p>76 Lprobit (wn ) ≤ ˆ  1 1 1 − 2λn  ∗ ˆ Ln probit (αw ) + λn  1 2 2α  + 2 ln n n  (7)  A standard Chernoff bound argument yields that for w∗ and α > 0 selected prior to drawing the sample, we have the following with probability at least 1 − 1/n2 over the choice of the sample. [sent-131, score-0.45]
</p><p>77 ln n (8) n Combining (7) and (8) with a union bound yields that with probability at least 1 − 2/n2 we have the following. [sent-132, score-0.227]
</p><p>78 ∗ ∗ ˆ Ln probit (αw ) ≤ Lprobit (αw ) +  Lprobit (wn ) ≤ ˆ  1 1 1 − 2λn  Lprobit (αw∗ ) +  ln n + λn n  1 2 2α  + 2 ln n n  Because the probability that the above inequality is violated goes as 1/n2 , with probability one over the draw of the sample we have the following. [sent-133, score-0.644]
</p><p>79 1 n→∞ 1 − 1 2λn  lim Lprobit (wn ) ≤ lim ˆ  n→∞  Lprobit (αw∗ ) +  ln n + λn n  1 2 2α  + 2 ln n n  Under the conditions on λn given in the statement of theorem 1 we then have lim Lprobit (wn ) ≤ Lprobit (αw∗ ). [sent-134, score-0.717]
</p><p>80 lim Lprobit (wn ) ≤ lim Lprobit (αw∗ ) ˆ  n→∞  α→∞  (9)  Now consider lim Lprobit (αw, x, y) = lim E [L(αw + , x, y)] = lim E [L(w + σ , x, y)] . [sent-136, score-0.555]
</p><p>81 Under the pessimistic tie breaking in the deﬁnition of L(w, x, y) we then get limα→∞ Lprobit (αw, x, y) ≤ L(w, x, y). [sent-139, score-0.077]
</p><p>82 lim Lprobit (αw) = Ex,y  α→∞  lim Lprobit (αw, x, y) ≤ Ex,y [L(w, x, y)] = L(w)  α→∞  (10)  Combining (9) and (10) yields limn→∞ Lprobit (wn ) ≤ L(w∗ ). [sent-141, score-0.222]
</p><p>83 5  4  Consistency of Ramp Loss  Now we consider the following ramp loss training equation. [sent-144, score-0.685]
</p><p>84 γn ˆ ramp wn = argmin Ln (w) + ˆ ||w||2 2n w  (11)  The main result of this paper is the following. [sent-145, score-0.651]
</p><p>85 For wn deﬁned by (11), if the sequence γn / ln2 n increases ˆ without bound, and the sequence γn /(n ln n) converges to zero, then with probability one over the draw of the inﬁnite sample we have limn→∞ Lprobit ((ln n)wn ) = L∗ . [sent-147, score-0.43]
</p><p>86 ˆ As with theorem 1, theorem 3 is derived from a ﬁnite sample generalization bound. [sent-148, score-0.093]
</p><p>87 The bound ˆ ˆn is derived from (6) by upper bounding Ln probit (w/σ) in terms of Lramp (w). [sent-149, score-0.272]
</p><p>88 w |S| Lprobit , x, y ≤ Lramp (w, x, y) + σ + σ 8 ln σ σ Proof. [sent-153, score-0.179]
</p><p>89 We ﬁrst prove that for any σ > 0 we have w Lprobit , x, y ≤ σ + max L(y, s) σ s: m(s)≤M  (12)  where ∆φ(s) = φ(x, sw (x)) − φ(x, s) ˆ  m(s) = w ∆φ(s)  M =σ  8 ln  |S| . [sent-154, score-0.486]
</p><p>90 w Lprobit , x, y ≤ σ + max L(y, s) σ s: m(s)≤M ≤ σ+  max  L(y, s) − m(s) + M  s: m(s)≤M  ≤ σ + max L(y, s) − m(s) + M s  = σ + Lramp (w, x, y) + M  Inserting lemma 1 into (6) we get the following. [sent-157, score-0.084]
</p><p>91 Lprobit  w σ  1 ≤ 1 1 − 2λn  ˆ ramp Ln (w) + σ + σ  6  |S| 8 ln + λn σ  ||w||2 2σ 2  + ln 1 δ n  (13)  To prove theorem 3 we now take σn = 1/ ln n and λn = γn / ln2 n. [sent-160, score-1.056]
</p><p>92 We then have that wn is the ˆ minimizer of the right hand side of (13). [sent-161, score-0.18]
</p><p>93 lim Lprobit ((ln n)wn ) ≤ Lramp (αw∗ ) ˆ  n→∞  The remainder of the proof is the same in section 3 but where we now use limα→∞ Lramp (αw∗ ) = L(w∗ ) whose proof we omit. [sent-164, score-0.111]
</p><p>94 Lprobit  w σ  ≤  1 1 1 − 2λn  ˆ ramp Ln (w) +  λn ||w||2 n  1/3  3 + 2  8 ln  |S| σ  +  λn ln 1 δ n  (15)  1/3 ˜ We have that (15) gives O ||wn ||2 /n ˆ as opposed to (6) which gives O ||wn ||2 /n . [sent-167, score-0.829]
</p><p>95 This ˆ suggests that while probit loss and ramp loss are both consistent, ramp loss may converge more slowly. [sent-168, score-1.759]
</p><p>96 6  Discussion and Open Problems  The contributions of this paper are a consistency theorem for latent structural probit loss and both a generalization bound and a consistency theorem for latent structural ramp loss. [sent-169, score-1.411]
</p><p>97 These bounds suggest that probit loss converges more rapidly. [sent-170, score-0.476]
</p><p>98 However, we have only proved upper bounds on generalization loss and it remains possible that these upper bounds, while sufﬁcient to show consistency, are not accurate characterizations of the actual generalization loss. [sent-171, score-0.369]
</p><p>99 The deﬁnition of ramp loss used here is not the only one possible. [sent-173, score-0.669]
</p><p>100 A uniformly tighter upper bound on generalization loss is achieved by optimizing the posterior in the PAC-Bayesian theorem. [sent-181, score-0.308]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lprobit', 0.53), ('ramp', 0.471), ('lramp', 0.409), ('sw', 0.257), ('probit', 0.223), ('loss', 0.198), ('wn', 0.18), ('ln', 0.179), ('lim', 0.111), ('hinge', 0.093), ('structural', 0.083), ('surrogate', 0.075), ('latent', 0.063), ('lhinge', 0.061), ('translation', 0.055), ('zw', 0.053), ('consistency', 0.051), ('llog', 0.045), ('limn', 0.044), ('generalization', 0.041), ('phoneme', 0.04), ('tie', 0.034), ('bound', 0.032), ('label', 0.032), ('mcallester', 0.031), ('draw', 0.031), ('bounds', 0.031), ('structured', 0.031), ('nite', 0.031), ('tamir', 0.03), ('singleton', 0.029), ('max', 0.028), ('keshet', 0.027), ('experimentation', 0.027), ('pessimistic', 0.026), ('augmented', 0.026), ('predictors', 0.026), ('theorem', 0.026), ('joseph', 0.025), ('converges', 0.024), ('task', 0.024), ('remains', 0.024), ('predictor', 0.024), ('speech', 0.023), ('parse', 0.023), ('weight', 0.023), ('quantitative', 0.022), ('prove', 0.022), ('labels', 0.021), ('subgradient', 0.021), ('scalar', 0.021), ('tighter', 0.02), ('object', 0.02), ('update', 0.02), ('answering', 0.02), ('david', 0.02), ('chernoff', 0.019), ('stopping', 0.019), ('svms', 0.019), ('olivier', 0.018), ('source', 0.018), ('feature', 0.018), ('linguistics', 0.018), ('classi', 0.017), ('upper', 0.017), ('breaking', 0.017), ('convention', 0.017), ('dimensional', 0.016), ('training', 0.016), ('tied', 0.016), ('probability', 0.016), ('finding', 0.016), ('taskar', 0.016), ('hofmann', 0.016), ('nition', 0.016), ('isotropic', 0.016), ('noting', 0.015), ('achievable', 0.015), ('open', 0.015), ('alignment', 0.015), ('rmed', 0.015), ('outliers', 0.015), ('unbounded', 0.014), ('pascal', 0.014), ('easier', 0.014), ('regularization', 0.014), ('labeled', 0.014), ('early', 0.014), ('gaussian', 0.014), ('limit', 0.014), ('map', 0.014), ('chiang', 0.013), ('grammatical', 0.013), ('abbreviates', 0.013), ('bakir', 0.013), ('bleu', 0.013), ('choon', 0.013), ('departure', 0.013), ('jeopardy', 0.013), ('laviolette', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="103-tfidf-1" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><p>2 0.078994475 <a title="103-tfidf-2" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>Author: Stéphan J. Clémençcon</p><p>Abstract: Many clustering techniques aim at optimizing empirical criteria that are of the form of a U -statistic of degree two. Given a measure of dissimilarity between pairs of observations, the goal is to minimize the within cluster point scatter over a class of partitions of the feature space. It is the purpose of this paper to deﬁne a general statistical framework, relying on the theory of U -processes, for studying the performance of such clustering methods. In this setup, under adequate assumptions on the complexity of the subsets forming the partition candidates, the √ excess of clustering risk is proved to be of the order OP (1/ n). Based on recent results related to the tail behavior of degenerate U -processes, it is also shown how to establish tighter rate bounds. Model selection issues, related to the number of clusters forming the data partition in particular, are also considered. 1</p><p>3 0.074867733 <a title="103-tfidf-3" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>Author: Alex Graves</p><p>Abstract: Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus. 1</p><p>4 0.072918057 <a title="103-tfidf-4" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>Author: Yevgeny Seldin, Peter Auer, John S. Shawe-taylor, Ronald Ortner, François Laviolette</p><p>Abstract: We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The p scaling of our regret bound with the number of states (contexts) N goes as N I⇢t (S; A), where I⇢t (S; A) is the mutual information between states and actions (the side information) used by the algorithm at round t. If the algorithm p uses all the side information, the regret bound scales as N ln K, where K is the number of actions (arms). However, if the side information I⇢t (S; A) is not fully used, the regret bound is signiﬁcantly tighter. In the extreme case, when I⇢t (S; A) = 0, the dependence on the number of states reduces from linear to logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round. 1</p><p>5 0.071615502 <a title="103-tfidf-5" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>Author: Ross B. Girshick, Pedro F. Felzenszwalb, David A. McAllester</p><p>Abstract: Compositional models provide an elegant formalism for representing the visual appearance of highly variable objects. While such models are appealing from a theoretical point of view, it has been difﬁcult to demonstrate that they lead to performance advantages on challenging datasets. Here we develop a grammar model for person detection and show that it outperforms previous high-performance systems on the PASCAL benchmark. Our model represents people using a hierarchy of deformable parts, variable structure and an explicit model of occlusion for partially visible objects. To train the model, we introduce a new discriminative framework for learning structured prediction models from weakly-labeled data. 1</p><p>6 0.069717951 <a title="103-tfidf-6" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>7 0.064200498 <a title="103-tfidf-7" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>8 0.063524023 <a title="103-tfidf-8" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>9 0.063234814 <a title="103-tfidf-9" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>10 0.057282519 <a title="103-tfidf-10" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>11 0.05716509 <a title="103-tfidf-11" href="./nips-2011-Better_Mini-Batch_Algorithms_via_Accelerated_Gradient_Methods.html">46 nips-2011-Better Mini-Batch Algorithms via Accelerated Gradient Methods</a></p>
<p>12 0.057006355 <a title="103-tfidf-12" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>13 0.053971626 <a title="103-tfidf-13" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>14 0.048402779 <a title="103-tfidf-14" href="./nips-2011-The_Fast_Convergence_of_Boosting.html">282 nips-2011-The Fast Convergence of Boosting</a></p>
<p>15 0.045161374 <a title="103-tfidf-15" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>16 0.042245984 <a title="103-tfidf-16" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>17 0.042232301 <a title="103-tfidf-17" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>18 0.041494936 <a title="103-tfidf-18" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>19 0.039318975 <a title="103-tfidf-19" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>20 0.038518421 <a title="103-tfidf-20" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, -0.029), (2, -0.03), (3, -0.014), (4, 0.034), (5, -0.006), (6, 0.009), (7, -0.036), (8, -0.043), (9, 0.04), (10, -0.044), (11, -0.023), (12, 0.009), (13, -0.051), (14, -0.019), (15, -0.026), (16, -0.06), (17, -0.045), (18, 0.051), (19, 0.055), (20, -0.017), (21, -0.066), (22, 0.044), (23, 0.123), (24, -0.0), (25, 0.11), (26, 0.03), (27, 0.061), (28, -0.043), (29, -0.046), (30, -0.023), (31, -0.033), (32, 0.112), (33, 0.087), (34, 0.113), (35, -0.015), (36, 0.016), (37, -0.072), (38, 0.088), (39, -0.007), (40, 0.044), (41, -0.04), (42, 0.027), (43, -0.067), (44, -0.026), (45, 0.013), (46, -0.063), (47, 0.062), (48, 0.021), (49, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92319119 <a title="103-lsi-1" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><p>2 0.68769085 <a title="103-lsi-2" href="./nips-2011-Greedy_Model_Averaging.html">109 nips-2011-Greedy Model Averaging</a></p>
<p>Author: Dong Dai, Tong Zhang</p><p>Abstract: This paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression. It is known that if the models are mis-speciﬁed, model averaging is superior to model selection. Speciﬁcally, let n be the sample size, then the worst case regret of the former decays at the rate of O(1/n) while the worst √ case regret of the latter decays at the rate of O(1/ n). In the literature, the most important and widely studied model averaging method that achieves the optimal O(1/n) average regret is the exponential weighted model averaging (EWMA) algorithm. However this method suffers from several limitations. The purpose of this paper is to present a new greedy model averaging procedure that improves EWMA. We prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples. 1</p><p>3 0.64791709 <a title="103-lsi-3" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>Author: Tim V. Erven, Wouter M. Koolen, Steven D. Rooij, Peter Grünwald</p><p>Abstract: Most methods for decision-theoretic online learning are based on the Hedge algorithm, which takes a parameter called the learning rate. In most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is signiﬁcantly better than all others. We propose a new way of setting the learning rate, which adapts to the difﬁculty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret. In particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions. We also provide a simulation study comparing our approach to existing methods. 1</p><p>4 0.54644036 <a title="103-lsi-4" href="./nips-2011-k-NN_Regression_Adapts_to_Local_Intrinsic_Dimension.html">305 nips-2011-k-NN Regression Adapts to Local Intrinsic Dimension</a></p>
<p>Author: Samory Kpotufe</p><p>Abstract: Many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data. These regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g. a manifold). We show that k-NN regression is also adaptive to intrinsic dimension. In particular our rates are local to a query x and depend only on the way masses of balls centered at x vary with radius. Furthermore, we show a simple way to choose k = k(x) locally at any x so as to nearly achieve the minimax rate at x in terms of the unknown intrinsic dimension in the vicinity of x. We also establish that the minimax rate does not depend on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure. 1</p><p>5 0.50588512 <a title="103-lsi-5" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study multi-label prediction for structured output sets, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries. Conventional multilabel classiﬁcation techniques are typically not applicable in this situation, because they require explicit enumeration of the label set, which is infeasible in case of structured outputs. Relying on techniques originally designed for single-label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems. In this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy. It also shares most beneﬁcial properties with single-label maximum-margin approaches, in particular formulation as a convex optimization problem, efﬁcient working set training, and PAC-Bayesian generalization bounds. 1</p><p>6 0.50464761 <a title="103-lsi-6" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>7 0.48278555 <a title="103-lsi-7" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>8 0.48211205 <a title="103-lsi-8" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>9 0.45559788 <a title="103-lsi-9" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>10 0.45016259 <a title="103-lsi-10" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>11 0.40629703 <a title="103-lsi-11" href="./nips-2011-Multiple_Instance_Learning_on_Structured_Data.html">181 nips-2011-Multiple Instance Learning on Structured Data</a></p>
<p>12 0.40550548 <a title="103-lsi-12" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>13 0.40039945 <a title="103-lsi-13" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>14 0.39024791 <a title="103-lsi-14" href="./nips-2011-Similarity-based_Learning_via_Data_Driven_Embeddings.html">254 nips-2011-Similarity-based Learning via Data Driven Embeddings</a></p>
<p>15 0.38681284 <a title="103-lsi-15" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>16 0.37977257 <a title="103-lsi-16" href="./nips-2011-Submodular_Multi-Label_Learning.html">277 nips-2011-Submodular Multi-Label Learning</a></p>
<p>17 0.37797338 <a title="103-lsi-17" href="./nips-2011-The_Impact_of_Unlabeled_Patterns_in_Rademacher_Complexity_Theory_for_Kernel_Classifiers.html">284 nips-2011-The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers</a></p>
<p>18 0.36855972 <a title="103-lsi-18" href="./nips-2011-Hierarchical_Multitask_Structured_Output_Learning_for_Large-scale_Sequence_Segmentation.html">114 nips-2011-Hierarchical Multitask Structured Output Learning for Large-scale Sequence Segmentation</a></p>
<p>19 0.35793898 <a title="103-lsi-19" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>20 0.35461652 <a title="103-lsi-20" href="./nips-2011-Variance_Penalizing_AdaBoost.html">299 nips-2011-Variance Penalizing AdaBoost</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.022), (4, 0.064), (20, 0.071), (26, 0.029), (31, 0.055), (33, 0.398), (43, 0.058), (45, 0.073), (57, 0.023), (74, 0.028), (83, 0.025), (99, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94979793 <a title="103-lda-1" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efﬁcient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efﬁciently solve tasks relevant to many computer vision applications, such as ﬁnding all pairs of windows between two images with distance smaller than a threshold, or ﬁnding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ﬁnds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>2 0.94238967 <a title="103-lda-2" href="./nips-2011-Modelling_Genetic_Variations_using_Fragmentation-Coagulation_Processes.html">173 nips-2011-Modelling Genetic Variations using Fragmentation-Coagulation Processes</a></p>
<p>Author: Yee W. Teh, Charles Blundell, Lloyd Elliott</p><p>Abstract: We propose a novel class of Bayesian nonparametric models for sequential data called fragmentation-coagulation processes (FCPs). FCPs model a set of sequences using a partition-valued Markov process which evolves by splitting and merging clusters. An FCP is exchangeable, projective, stationary and reversible, and its equilibrium distributions are given by the Chinese restaurant process. As opposed to hidden Markov models, FCPs allow for ﬂexible modelling of the number of clusters, and they avoid label switching non-identiﬁability problems. We develop an efﬁcient Gibbs sampler for FCPs which uses uniformization and the forward-backward algorithm. Our development of FCPs is motivated by applications in population genetics, and we demonstrate the utility of FCPs on problems of genotype imputation with phased and unphased SNP data. 1</p><p>3 0.82113218 <a title="103-lda-3" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>4 0.76348323 <a title="103-lda-4" href="./nips-2011-From_Stochastic_Nonlinear_Integrate-and-Fire_to_Generalized_Linear_Models.html">99 nips-2011-From Stochastic Nonlinear Integrate-and-Fire to Generalized Linear Models</a></p>
<p>Author: Skander Mensi, Richard Naud, Wulfram Gerstner</p><p>Abstract: Variability in single neuron models is typically implemented either by a stochastic Leaky-Integrate-and-Fire model or by a model of the Generalized Linear Model (GLM) family. We use analytical and numerical methods to relate state-of-theart models from both schools of thought. First we ﬁnd the analytical expressions relating the subthreshold voltage from the Adaptive Exponential Integrate-andFire model (AdEx) to the Spike-Response Model with escape noise (SRM as an example of a GLM). Then we calculate numerically the link-function that provides the ﬁring probability given a deterministic membrane potential. We ﬁnd a mathematical expression for this link-function and test the ability of the GLM to predict the ﬁring probability of a neuron receiving complex stimulation. Comparing the prediction performance of various link-functions, we ﬁnd that a GLM with an exponential link-function provides an excellent approximation to the Adaptive Exponential Integrate-and-Fire with colored-noise input. These results help to understand the relationship between the different approaches to stochastic neuron models. 1 Motivation When it comes to modeling the intrinsic variability in simple neuron models, we can distinguish two traditional approaches. One approach is inspired by the stochastic Leaky Integrate-and-Fire (LIF) hypothesis of Stein (1967) [1], where a noise term is added to the system of differential equations implementing the leaky integration to a threshold. There are multiple versions of such a stochastic LIF [2]. How the noise affects the ﬁring probability is also a function of the parameters of the neuron model. Therefore, it is important to take into account the reﬁnements of simple neuron models in terms of subthreshold resonance [3, 4], spike-triggered adaptation [5, 6] and non-linear spike 1 initiation [7, 5]. All these improvements are encompassed by the Adaptive Exponential Integrateand-Fire model (AdEx [8, 9]). The other approach is to start with some deterministic dynamics for the the state of the neuron (for instance the instantaneous distance from the membrane potential to the threshold) and link the probability intensity of emitting a spike with a non-linear function of the state variable. Under some conditions, this type of model is part of a greater class of statistical models called Generalized Linear Models (GLM [10]). As a single neuron model, the Spike Response Model (SRM) with escape noise is a GLM in which the state variable is explicitly the distance between a deterministic voltage and the threshold. The original SRM could account for subthreshold resonance, refractory effects and spike-frequency adaptation [11]. Mathematically similar models were developed independently in the study of the visual system [12] where spike-frequency adaptation has also been modeled [13]. Recently, this approach has retained increased attention since the probabilistic framework can be linked with the Bayesian theory of neural systems [14] and because Bayesian inference can be applied to the population of neurons [15]. In this paper, we investigate the similarity and differences between the state-of-the-art GLM and the stochastic AdEx. The motivation behind this work is to relate the traditional threshold neuron models to Bayesian theory. Our results extend the work of Plesser and Gerstner (2000) [16] since we include the non-linearity for spike initiation and spike-frequency adaptation. We also provide relationships between the parameters of the AdEx and the equivalent GLM. These precise relationships can be used to relate analog implementations of threshold models [17] to the probabilistic models used in the Bayesian approach. The paper is organized as follows: We ﬁrst describe the expressions relating the SRM state-variable to the parameters of the AdEx (Sect. 3.1) in the subthreshold regime. Then, we use numerical methods to ﬁnd the non-linear link-function that models the ﬁring probability (Sect. 3.2). We ﬁnd a functional form for the SRM link-function that best describes the ﬁring probability of a stochastic AdEx. We then compare the performance of this link-function with the often used exponential or linear-rectiﬁer link-functions (also called half-wave linear rectiﬁer) in terms of predicting the ﬁring probability of an AdEx under complex stimulus (Sect. 3.3). We ﬁnd that the exponential linkfunction yields almost perfect prediction. Finally, we explore the relations between the statistic of the noise and the sharpness of the non-linearity for spike initiation with the parameters of the SRM. 2 Presentation of the Models In this section we present the general formula for the stochastic AdEx model (Sect. 2.1) and the SRM (Sect 2.2). 2.1 The Stochastic Adaptive Exponential Integrate-and-Fire Model The voltage dynamics of the stochastic AdEx is given by: V −Θ ˙ τm V = El − V + ∆T exp − Rw + RI + R (1) ∆T τw w = a(V − El ) − w ˙ (2) where τm is the membrane time constant, El the reverse potential, R the membrane resistance, Θ is the threshold, ∆T is the shape factor and I(t) the input current which is chosen to be an Ornstein−Θ Uhlenbeck process with correlation time-constant of 5 ms. The exponential term ∆T exp( V∆T ) is a non-linear function responsible for the emission of spikes and is a diffusive white noise with standard deviation σ (i.e. ∼ N (0, σ)). Note that the diffusive white-noise does not imply white noise ﬂuctuations of the voltage V (t), the probability distribution of V (t) will depend on ∆T and Θ. The second variable, w, describes the subthreshold as well as the spike-triggered adaptation both ˆ parametrized by the coupling strength a and the time constant τw . Each time tj the voltage goes to inﬁnity, we assumed that a spike is emitted. Then the voltage is reset to a ﬁxed value Vr and w is increased by a constant value b. 2.2 The Generalized Linear Model In the SRM, The voltage V (t) is given by the convolution of the injected current I(t) with the membrane ﬁlter κ(t) plus the additional kernel η(t) that acts after each spikes (here we split the 2 spike-triggered kernel in two η(t) = ηv (t) + ηw (t) for reasons that will become clear later): V (t) = ˆ ˆ ηv (t − tj ) + ηw (t − tj ) El + [κ ∗ I](t) + (3) ˆ {tj } ˆ Then at each time tj a spike is emitted which results in a change of voltage described by η(t) = ηv (t) + ηw (t). Given the deterministic voltage, (Eq. 3) a spike is emitted according to the ﬁring intensity λ(V ): λ(t) = f (V (t)) (4) where f (·) is an arbitrary function called the link-function. Then the ﬁring behavior of the SRM depends on the choice of the link-function and its parameters. The most common link-function used to model single neuron activities are the linear-rectiﬁer and the exponential function. 3 Mapping In order to map the stochastic AdEx to the SRM we follow a two-step procedure. First we derive the ﬁlter κ(t) and the kernels ηv (t) and ηw (t) analytically as a function of AdEx parameters. Second, we derive the link-function of the SRM from the stochastic spike emission of the AdEx. Figure 1: Mapping of the subthreshold dynamics of an AdEx to an equivalent SRM. A. Membrane ﬁlter κ(t) for three different sets of parameters of the AdEx leading to over-damped, critically damped and under-damped cases (upper, middle and lower panel, respectively). B. Spike-Triggered η(t) (black), ηv (t) (light gray) and ηw (gray) for the three cases. C. Example of voltage trace produced when an AdEx is stimulated with a step of colored noise (black). The corresponding voltage from a SRM stimulated with the same current and where we forced the spikes to match those of the AdEx (red). D. Error in the subthreshold voltage (VAdEx − VGLM ) as a function of the mean voltage of the AdEx, for the three different cases: over-, critically and under-damped (light gray, gray and black, respectively) with ∆T = 1 mV. Red line represents the voltage threshold Θ. E. Root Mean Square Error (RMSE) ratio for the three cases with ∆T = 1 mV. The RMSE ratio is the RMSE between the deterministic VSRM and the stochastic VAdEx divided by the RMSE between repetitions of the stochastic AdEx voltage. The error bar shows a single standard deviation as the RMSE ratio is averaged accross multiple value of σ. 3.1 Subthreshold voltage dynamics We start by assuming that the non-linearity for spike initiation does not affect the mean subthreshold voltage of the stochastic AdEx (see Figure 1 D). This assumption is motivated by the small ∆T 3 observed in in-vitro recordings (from 0.5 to 2 mV [8, 9]) which suggest that the subthreshold dynamics are mainly linear except very close to Θ. Also, we expect that the non-linear link-function will capture some of the dynamics due to the non-linearity for spike initiation. Thus it is possible to rewrite the deterministic subthreshold part of the AdEx (Eq. 1-2 without and without ∆T exp((V − Θ)/∆T )) using matrices: ˙ x = Ax (5) with x = V w and A = − τ1 m a τw − gl1m τ − τ1 w (6) In this form, the dynamics of the deterministic AdEx voltage is a damped oscillator with a driving force. Depending on the eigenvalues of A the system could be over-damped, critically damped or under-damped. The ﬁlter κ(t) of the GLM is given by the impulse response of the system of coupled differential equations of the AdEx, described by Eq. 5 and 6. In other words, one has to derive the response of the system when stimulating with a Dirac-delta function. The type of damping gives three different qualitative shapes of the kernel κ(t), which are summarized in Table 3.1 and Figure 1 A. Since the three different ﬁlters also affect the nature of the stochastic voltage ﬂuctuations, we will keep the distinction between over-damped, critically damped and under-damped scenarios throughout the paper. This means that our approach is valid for at least 3 types of diffusive voltage-noise (i.e. the white noise in Eq. 1 ﬁltered by 3 different membrane ﬁlters κ(t)). To complete the description of the deterministic voltage, we need an expression for the spiketriggered kernels. The voltage reset at each spike brings a spike-triggered jump in voltage of magˆ nitude ∆ = Vr − V (t). This perturbation is superposed to the current ﬂuctuations due to I(t) and can be mediated by a Delta-diract pulse of current. Thus we can write the voltage reset kernel by: ηv (t) = ∆ ∆ [δ ∗ κ] (t) = κ(t) κ(0) κ(0) (7) where δ(t) is the Dirac-delta function. The shape of this kernel depends on κ(t) and can be computed from Table 3.1 (see Figure 1 B). Finally, the AdEx mediates spike-frequency adaptation by the jump of the second variables w. From Eq. 2 we can see that this produces a current wspike (t) = b exp (−t/τw ) that can cumulate over subsequent spikes. The effect of this current on voltage is then given by the convolution of wspike (t) with the membrane ﬁlter κ(t). Thus in the SRM framework the spike-frequency adaptation is taken into account by: ηw (t) = [wspike ∗ κ](t) (8) Again the precise form of ηw (t) depends on κ(t) and can be computed from Table 3.1 (see Figure 1 B). At this point, we would like to verify our assumption that the non-linearity for spike emission can be neglected. Fig. 1 C and D shows that the error between the voltage from Eq. 3 and the voltage from the stochastic AdEx is generally small. Moreover, we see that the main contribution to the voltage prediction error is due to the mismatch close to the spikes. However the non-linearity for spike initiation may change the probability distribution of the voltage ﬂuctuations, which in turn inﬂuences the probability of spiking. This will inﬂuence the choice of the link-function, as we will see in the next section. 3.2 Spike Generation Using κ(t), ηv (t) and ηw (t), we must relate the spiking probability of the stochastic AdEx as a function of its deterministic voltage. According to [2] the probability of spiking in time bin dt given the deterministic voltage V (t) is given by: p(V ) = prob{spike in [t, t + dt]} = 1 − exp (−f (V (t))dt) (9) where f (·) gives the ﬁring intensity as a function of the deterministic V (t) (Eq. 3). Thus to extract the link-function f we have to compute the probability of spiking given V (t) for our SRM. To do so we apply the method proposed by Jolivet et al. (2004) [18], where the probability of spiking is simply given by the distribution of the deterministic voltage estimated at the spike times divided by the distribution of the SRM voltage when there is no spike (see ﬁgure 2 A). One can numerically compute these two quantities for our models using N repetitions of the same stimulus. 4 Table 1: Analytical expressions for the membrane ﬁlter κ(t) in terms of the parameters of the AdEx for over-, critically-, and under-damped cases. Membrane Filter: κ(t) over-damped if: (τm + τw )2 > 4τm τw (gl +a) gl κ(t) = k1 eλ1 t + k2 eλ2 t λ1 = 1 2τm τw (−(τm + τw ) + critically-damped if: (τm + τw )2 = 4τm τw (gl +a) gl κ(t) = (αt + β)eλt λ= under-damped if: (τm + τw )2 < 4τm τw (gl +a) gl κ(t) = (k1 cos (ωt) + k2 sin (ωt)) eλt −(τm +τw ) 2τm τw λ= −(τm +τw ) 2τm τw (τm + τw )2 − 4 τm τw (gl + a) gl λ2 = 1 2τm τw (−(τm + τw ) − α= τm −τw 2Cτm τw ω= τw −τm 2τm τw 2 − a g l τm τw (τm + τw )2 − 4 τm τw (gl + a) gl k1 = −(1+(τm λ2 )) Cτm (λ1 −λ2 ) k2 = 1+(τm λ1 ) Cτm (λ1 −λ2 ) β= 1 C k1 = k2 = 1 C −(1+τm λ) Cωτm The standard deviation σ of the noise and the parameter ∆T of the AdEx non-linearity may affect the shape of the link-function. We thus extract p(V ) for different σ and ∆T (Fig. 2 B). Then using visual heuristics and previous knowledge about the potential analytical expression of the link-funtion, we try to ﬁnd a simple analytical function that captures p(V ) for a large range of combinations of σ and ∆T . We observed that the log(− log(p)) is close to linear in most studied conditions Fig. 2 B suggesting the following two distributions of p(V ): V − VT (10) p(V ) = 1 − exp − exp ∆V V − VT p(V ) = exp − exp − (11) ∆V Once we have p(V ), we can use Eq. 4 to obtain the equivalent SRM link-function, which leads to: −1 f (V ) = log (1 − p(V )) (12) dt Then the two potential link-functions of the SRM can be derived from Eq. 10 and Eq. 11 (respectively): V − VT f (V ) = λ0 exp (13) ∆V V − VT (14) f (V ) = −λ0 log 1 − exp − exp − ∆V 1 with λ0 = dt , VT the threshold of the SRM and ∆V the sharpness of the link-function (i.e. the parameters that governs the degree of the stochasticity). Note that the exact value of λ0 has no importance since it is redundant with VT . Eq. 13 is the standard exponential link-function, but we call Eq. 14 the log-exp-exp link-function. 3.3 Prediction The next point is to evaluate the ﬁt quality of each link-function. To do this, we ﬁrst estimate the parameters VT and ∆V of the GLM link-function that maximize the likelihood of observing a spike 5 Figure 2: SRM link-function. A. Histogram of the SRM voltage at the AdEx ﬁring times (red) and at non-ﬁring times (gray). The ratio of the two distributions gives p(V ) (Eq. 9, dashed lines). Inset, zoom to see the voltage histogram evaluated at the ﬁring time (red). B. log(− log(p)) as a function of the SRM voltage for three different noise levels σ = 0.07, 0.14, 0.18 nA (pale gray, gray, black dots, respectively) and ∆T = 1 mV. The line is a linear ﬁt corresponding to the log-exp-exp linkfunction and the dashed line corresponds to a ﬁt with the exponential link-function. C. Same data and labeling scheme as B, but plotting f (V ) according to Eq. 12. The lines are produced with Eq. 14 with parameters ﬁtted as described in B. and the dashed lines are produced with Eq. 13. Inset, same plot but on a semi-log(y) axis. train generated with an AdEx. Second we look at the predictive power of the resulting SRM in terms of Peri-Stimulus Time Histogram (PSTH). In other words we ask how close the spike trains generated with a GLM are from the spike train generated with a stochastic AdEx when both models are stimulated with the same input current. For any GLM with link-function f (V ) ≡ f (t|I, θ) and parameters θ regulating the shape of κ(t), ˆ ηv (t) and ηw (t), the Negative Log-Likelihood (NLL) of observing a spike-train {t} is given by:   NLL = − log(f (t|I, θ)) − f (t|I, θ) (15) t ˆ t It has been shown that the negative log-likelihood is convex in the parameters if f is convex and logconcave [19]. It is easy to show that a linear-rectiﬁer link-function, the exponential link-function and the log-exp-exp link-function all satisfy these conditions. This allows efﬁcient estimation of ˆ ˆ the optimal parameters VT and ∆V using a simple gradient descent. One can thus estimate from a single AdEx spike train the optimal parameters of a given link-function, which is more efﬁcient than the method used in Sect. 3.2. The minimal NLL resulting from the gradient descent gives an estimation of the ﬁt quality. A better estimate of the ﬁt quality is given by the distance between the PSTHs in response to stimuli not used for parameter ﬁtting . Let ν1 (t) be the PSTH of the AdEx, and ν2 (t) be the PSTH of the ﬁtted SRM, 6 Figure 3: PSTH prediction. A. Injected current. B. Voltage traces produced by an AdEx (black) and the equivalent SRM (red), when stimulated with the current in A. C. Raster plot for 20 realizations of AdEx (black tick marks) and equivalent SRM (red tick marks). D. PSTH of the AdEx (black) and the SRM (red) obtained by averaging 10,000 repetitions. E. Optimal log-likelihood for the three cases of the AdEx, using three different link-functions, a linear-rectiﬁer (light gray), an exponential link-function (gray) and the link-function deﬁned by Eq. 14 (dark gray), these values are obtained by averaging over 40 different combinations σ and ∆T (see Fig. 4). Error bars are one standard deviation, the stars denote a signiﬁcant difference, two-sample t-test with α = 0.01. F. same as E. but for Md (Eq. 16). then we use Md ∈ [0, 1] as a measure of match: Md = 2 2 (ν1 (t) − ν2 (t)) dt ν1 (t)2 dt + ν2 (t)2 dt (16) Md = 1 means that it is impossible to differentiate the SRM from the AdEx in terms of their PSTHs, whereas a Md of 0 means that the two PSTHs are completely different. Thus Md is a normalized similarity measure between two PSTHs. In practice, Md is estimated from the smoothed (boxcar average of 1 ms half-width) averaged spike train of 1 000 repetitions for each models. We use both the NLL and Md to quantify the ﬁt quality for each of the three damping cases and each of the three link-functions. Figure 3 shows the match between the stochastic AdEx used as a reference and the derived GLM when both are stimulated with the same input current (Fig. 3 A). The resulting voltage traces are almost identical (Fig. 3 B) and both models predict almost the same spike trains and so the same PSTHs (Fig. 3 C and D). More quantitalively, we see on Fig. 3 E and F, that the linear-rectiﬁer ﬁts signiﬁcantly worse than both the exponential and log-exp-exp link-functions, both in terms of NLL and of Md . The exponential link-function performs as well as the log-exp-exp link-function, with a spike train similarity measure Md being almost 1 for both. Finally the likelihood-based method described above gives us the opportunity to look at the relationship between the AdEx parameters σ and ∆T that governs its spike emission and the parameters VT and ∆V of the link-function (Fig. 4). We observe that an increase of the noise level produces a ﬂatter link-function (greater ∆V ) while an increase in ∆T also produces an increase in ∆V and VT (note that Fig. 4 shows ∆V and VT for the exponential link-function only, but equivalent results are obtained with the log-exp-exp link-function). 4 Discussion In Sect. 3.3 we have shown that it is possible to predict with almost perfect accuracy the PSTH of a stochastic AdEx model using an appropriate set of parameters in the SRM. Moreover, since 7 Figure 4: Inﬂuence of the AdEx parameters on the parameters of the exponential link-function. A. VT as a function of ∆T and σ. B. ∆V as a function of ∆T and σ. the subthreshold voltage of the AdEx also gives a good match with the deterministic voltage of the SRM, we expect that the AdEx and the SRM will not differ in higher moments of the spike train probability distributions beyond the PSTH. We therefore conclude that diffusive noise models of the type of Eq. 1-2 are equivalent to GLM of the type of Eq. 3-4. Once combined with similar results on other types of stochastic LIF (e.g. correlated noise), we could bridge the gap between the literature on GLM and the literature on diffusive noise models. Another noteworthy observation pertains to the nature of the link-function. The link-function has been hypothesized to be a linear-rectiﬁer, an exponential, a sigmoidal or a Gaussian [16]. We have observed that for the AdEx the link-function follows Eq. 14 that we called the log-exp-exp linkfunction. Although the link-function is log-exp-exp for most of the AdEx parameters, the exponential link-function gives an equivalently good prediction of the PSTH. This can be explained by the fact that the difference between log-exp-exp and exponential link-functions happens mainly at low voltage (i.e. far from the threshold), where the probability of emitting a spike is so low (Figure 2 C, until -50 mv). Therefore, even if the exponential link-function overestimates the ﬁring probability at these low voltages it rarely produces extra spikes. At voltages closer to the threshold, where most of the spikes are emitted, the two link-functions behave almost identically and hence produce the same PSTH. The Gaussian link-function can be seen as lying in-between the exponential link-function and the log-exp-exp link-function in Fig. 2. This means that the work of Plesser and Gerstner (2000) [16] is in agreement with the results presented here. The importance of the time-derivative of the ˙ voltage stressed by Plesser and Gerstner (leading to a two-dimensional link-function f (V, V )) was not studied here to remain consistent with the typical usage of GLM in neural systems [14]. Finally we restricted our study to exponential non-linearity for spike initiation and do not consider other cases such as the Quadratic Integrate-and-ﬁre (QIF, [5]) or other polynomial functional shapes. We overlooked these cases for two reasons. First, there are many evidences that the non-linearity in neurons (estimated from in-vitro recordings of Pyramidal neurons) is well approximated by a single exponential [9]. Second, the exponential non-linearity of the AdEx only affects the subthreshold voltage at high voltage (close to threshold) and thus can be neglected to derive the ﬁlters κ(t) and η(t). Polynomial non-linearities on the other hand affect a larger range of the subthreshold voltage so that it would be difﬁcult to justify the linearization of subthreshold dynamics essential to the method presented here. References [1] R. B. Stein, “Some models of neuronal variability,” Biophys J, vol. 7, no. 1, pp. 37–68, 1967. [2] W. Gerstner and W. Kistler, Spiking neuron models. Cambridge University Press New York, 2002. [3] E. Izhikevich, “Resonate-and-ﬁre neurons,” Neural Networks, vol. 14, no. 883-894, 2001. [4] M. J. E. Richardson, N. Brunel, and V. Hakim, “From subthreshold to ﬁring-rate resonance,” Journal of Neurophysiology, vol. 89, pp. 2538–2554, 2003. 8 [5] E. Izhikevich, “Simple model of spiking neurons,” IEEE Transactions on Neural Networks, vol. 14, pp. 1569–1572, 2003. [6] S. Mensi, R. Naud, M. Avermann, C. C. H. Petersen, and W. Gerstner, “Parameter extraction and classiﬁcation of three neuron types reveals two different adaptation mechanisms,” Under review. [7] N. Fourcaud-Trocme, D. Hansel, C. V. Vreeswijk, and N. Brunel, “How spike generation mechanisms determine the neuronal response to ﬂuctuating inputs,” Journal of Neuroscience, vol. 23, no. 37, pp. 11 628–11 640, 2003. [8] R. Brette and W. Gerstner, “Adaptive exponential integrate-and-ﬁre model as an effective description of neuronal activity,” Journal of Neurophysiology, vol. 94, pp. 3637–3642, 2005. [9] L. Badel, W. Gerstner, and M. Richardson, “Dependence of the spike-triggered average voltage on membrane response properties,” Neurocomputing, vol. 69, pp. 1062–1065, 2007. [10] P. McCullagh and J. A. Nelder, Generalized linear models, 2nd ed. Chapman & Hall/CRC, 1998, vol. 37. [11] W. Gerstner, J. van Hemmen, and J. Cowan, “What matters in neuronal locking?” Neural computation, vol. 8, pp. 1653–1676, 1996. [12] D. Hubel and T. Wiesel, “Receptive ﬁelds and functional architecture of monkey striate cortex,” Journal of Physiology, vol. 195, pp. 215–243, 1968. [13] J. Pillow, L. Paninski, V. Uzzell, E. Simoncelli, and E. Chichilnisky, “Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model,” Journal of Neuroscience, vol. 25, no. 47, pp. 11 003–11 013, 2005. [14] K. Doya, S. Ishii, A. Pouget, and R. P. N. Rao, Bayesian brain: Probabilistic approaches to neural coding. The MIT Press, 2007. [15] S. Gerwinn, J. H. Macke, M. Seeger, and M. Bethge, “Bayesian inference for spiking neuron models with a sparsity prior,” in Advances in Neural Information Processing Systems, 2007. [16] H. Plesser and W. Gerstner, “Noise in integrate-and-ﬁre neurons: From stochastic input to escape rates,” Neural Computation, vol. 12, pp. 367–384, 2000. [17] J. Schemmel, J. Fieres, and K. Meier, “Wafer-scale integration of analog neural networks,” in Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, june 2008, pp. 431 –438. [18] R. Jolivet, T. Lewis, and W. Gerstner, “Generalized integrate-and-ﬁre models of neuronal activity approximate spike trains of a detailed model to a high degree of accuracy,” Journal of Neurophysiology, vol. 92, pp. 959–976, 2004. [19] L. Paninski, “Maximum likelihood estimation of cascade point-process neural encoding models,” Network: Computation in Neural Systems, vol. 15, pp. 243–262, 2004. 9</p><p>same-paper 5 0.76127493 <a title="103-lda-5" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><p>6 0.68520808 <a title="103-lda-6" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>7 0.66693443 <a title="103-lda-7" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>8 0.66476744 <a title="103-lda-8" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>9 0.59303117 <a title="103-lda-9" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>10 0.59012967 <a title="103-lda-10" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>11 0.56982791 <a title="103-lda-11" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>12 0.56933951 <a title="103-lda-12" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>13 0.56862277 <a title="103-lda-13" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>14 0.55692339 <a title="103-lda-14" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>15 0.5559839 <a title="103-lda-15" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>16 0.54442775 <a title="103-lda-16" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>17 0.50952148 <a title="103-lda-17" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>18 0.50843495 <a title="103-lda-18" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>19 0.50430179 <a title="103-lda-19" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>20 0.5033049 <a title="103-lda-20" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
