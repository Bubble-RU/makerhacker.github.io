<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-79" href="#">nips2011-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</h1>
<br/><p>Source: <a title="nips-2011-79-pdf" href="http://papers.nips.cc/paper/4385-efficient-offline-communication-policies-for-factored-multiagent-pomdps.pdf">pdf</a></p><p>Author: João V. Messias, Matthijs Spaan, Pedro U. Lima</p><p>Abstract: Factored Decentralized Partially Observable Markov Decision Processes (DecPOMDPs) form a powerful framework for multiagent planning under uncertainty, but optimal solutions require a rigid history-based policy representation. In this paper we allow inter-agent communication which turns the problem in a centralized Multiagent POMDP (MPOMDP). We map belief distributions over state factors to an agent’s local actions by exploiting structure in the joint MPOMDP policy. The key point is that when sparse dependencies between the agents’ decisions exist, often the belief over its local state factors is sufﬁcient for an agent to unequivocally identify the optimal action, and communication can be avoided. We formalize these notions by casting the problem into convex optimization form, and present experimental results illustrating the savings in communication that we can obtain.</p><p>Reference: <a title="nips-2011-79-reference" href="../nips2011_reference/nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bl', 0.645), ('ag', 0.357), ('mpomdp', 0.271), ('multy', 0.24), ('pomdp', 0.212), ('commun', 0.194), ('believ', 0.184), ('nf', 0.092), ('matthid', 0.083), ('bt', 0.082), ('spa', 0.079), ('ml', 0.076), ('room', 0.075), ('ict', 0.074), ('decid', 0.072), ('dec', 0.071), ('pr', 0.068), ('bf', 0.067), ('policy', 0.064), ('fran', 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="79-tfidf-1" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>2 0.18696471 <a title="79-tfidf-2" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>3 0.17870101 <a title="79-tfidf-3" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>4 0.12805633 <a title="79-tfidf-4" href="./nips-2011-Better_Mini-Batch_Algorithms_via_Accelerated_Gradient_Methods.html">46 nips-2011-Better Mini-Batch Algorithms via Accelerated Gradient Methods</a></p>
<p>5 0.10226495 <a title="79-tfidf-5" href="./nips-2011-Blending_Autonomous_Exploration_and_Apprenticeship_Learning.html">48 nips-2011-Blending Autonomous Exploration and Apprenticeship Learning</a></p>
<p>6 0.10071271 <a title="79-tfidf-6" href="./nips-2011-Clustering_via_Dirichlet_Process_Mixture_Models_for_Portable_Skill_Discovery.html">52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</a></p>
<p>7 0.081815459 <a title="79-tfidf-7" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>8 0.070576854 <a title="79-tfidf-8" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>9 0.068202779 <a title="79-tfidf-9" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>10 0.067489401 <a title="79-tfidf-10" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>11 0.062248342 <a title="79-tfidf-11" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>12 0.061746623 <a title="79-tfidf-12" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>13 0.060893659 <a title="79-tfidf-13" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>14 0.059330545 <a title="79-tfidf-14" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>15 0.057739403 <a title="79-tfidf-15" href="./nips-2011-Autonomous_Learning_of_Action_Models_for_Planning.html">41 nips-2011-Autonomous Learning of Action Models for Planning</a></p>
<p>16 0.057565648 <a title="79-tfidf-16" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<p>17 0.054864831 <a title="79-tfidf-17" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>18 0.053832121 <a title="79-tfidf-18" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>19 0.053464241 <a title="79-tfidf-19" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>20 0.05339494 <a title="79-tfidf-20" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, -0.055), (2, -0.062), (3, -0.114), (4, 0.099), (5, -0.013), (6, -0.063), (7, -0.045), (8, -0.002), (9, -0.027), (10, 0.048), (11, -0.046), (12, -0.044), (13, -0.01), (14, 0.054), (15, 0.047), (16, -0.111), (17, 0.101), (18, -0.035), (19, 0.032), (20, 0.095), (21, -0.055), (22, -0.083), (23, -0.042), (24, -0.054), (25, -0.098), (26, 0.024), (27, -0.053), (28, -0.117), (29, -0.034), (30, 0.024), (31, -0.021), (32, 0.002), (33, 0.057), (34, -0.027), (35, 0.004), (36, -0.011), (37, -0.032), (38, 0.047), (39, -0.04), (40, 0.014), (41, 0.092), (42, 0.05), (43, 0.101), (44, -0.079), (45, -0.069), (46, -0.023), (47, -0.028), (48, 0.051), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92534351 <a title="79-lsi-1" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>2 0.74418676 <a title="79-lsi-2" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>3 0.73072529 <a title="79-lsi-3" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>4 0.55749965 <a title="79-lsi-4" href="./nips-2011-Blending_Autonomous_Exploration_and_Apprenticeship_Learning.html">48 nips-2011-Blending Autonomous Exploration and Apprenticeship Learning</a></p>
<p>5 0.55033845 <a title="79-lsi-5" href="./nips-2011-Budgeted_Optimization_with_Concurrent_Stochastic-Duration_Experiments.html">50 nips-2011-Budgeted Optimization with Concurrent Stochastic-Duration Experiments</a></p>
<p>6 0.52885908 <a title="79-lsi-6" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>7 0.52351981 <a title="79-lsi-7" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>8 0.52021348 <a title="79-lsi-8" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>9 0.51756638 <a title="79-lsi-9" href="./nips-2011-Clustering_via_Dirichlet_Process_Mixture_Models_for_Portable_Skill_Discovery.html">52 nips-2011-Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery</a></p>
<p>10 0.4864015 <a title="79-lsi-10" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>11 0.45391843 <a title="79-lsi-11" href="./nips-2011-Autonomous_Learning_of_Action_Models_for_Planning.html">41 nips-2011-Autonomous Learning of Action Models for Planning</a></p>
<p>12 0.44956332 <a title="79-lsi-12" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>13 0.4340494 <a title="79-lsi-13" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>14 0.42242926 <a title="79-lsi-14" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>15 0.41258922 <a title="79-lsi-15" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>16 0.39504954 <a title="79-lsi-16" href="./nips-2011-Better_Mini-Batch_Algorithms_via_Accelerated_Gradient_Methods.html">46 nips-2011-Better Mini-Batch Algorithms via Accelerated Gradient Methods</a></p>
<p>17 0.36697954 <a title="79-lsi-17" href="./nips-2011-How_Do_Humans_Teach%3A_On_Curriculum_Learning_and_Teaching_Dimension.html">122 nips-2011-How Do Humans Teach: On Curriculum Learning and Teaching Dimension</a></p>
<p>18 0.36286411 <a title="79-lsi-18" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<p>19 0.3505047 <a title="79-lsi-19" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>20 0.34520271 <a title="79-lsi-20" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.236), (1, 0.01), (11, 0.01), (14, 0.044), (22, 0.024), (36, 0.03), (37, 0.054), (47, 0.019), (53, 0.026), (55, 0.207), (65, 0.088), (68, 0.135), (79, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85155469 <a title="79-lda-1" href="./nips-2011-Solving_Decision_Problems_with_Limited_Information.html">256 nips-2011-Solving Decision Problems with Limited Information</a></p>
<p>same-paper 2 0.81842315 <a title="79-lda-2" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>3 0.80205083 <a title="79-lda-3" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>4 0.79948276 <a title="79-lda-4" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>5 0.74283767 <a title="79-lda-5" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>6 0.74105352 <a title="79-lda-6" href="./nips-2011-Periodic_Finite_State_Controllers_for_Efficient_POMDP_and_DEC-POMDP_Planning.html">212 nips-2011-Periodic Finite State Controllers for Efficient POMDP and DEC-POMDP Planning</a></p>
<p>7 0.73815626 <a title="79-lda-7" href="./nips-2011-Gaussian_process_modulated_renewal_processes.html">101 nips-2011-Gaussian process modulated renewal processes</a></p>
<p>8 0.73073983 <a title="79-lda-8" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>9 0.72991765 <a title="79-lda-9" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>10 0.72891581 <a title="79-lda-10" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>11 0.72871387 <a title="79-lda-11" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>12 0.72854483 <a title="79-lda-12" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>13 0.72634667 <a title="79-lda-13" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>14 0.72587603 <a title="79-lda-14" href="./nips-2011-Composite_Multiclass_Losses.html">59 nips-2011-Composite Multiclass Losses</a></p>
<p>15 0.72361183 <a title="79-lda-15" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>16 0.72318006 <a title="79-lda-16" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>17 0.72273254 <a title="79-lda-17" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>18 0.72268462 <a title="79-lda-18" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>19 0.72264409 <a title="79-lda-19" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>20 0.72252059 <a title="79-lda-20" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
