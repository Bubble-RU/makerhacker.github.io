<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-269" href="#">nips2011-269</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2011-269-pdf" href="http://papers.nips.cc/paper/4305-spike-and-slab-variational-inference-for-multi-task-and-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>Reference: <a title="nips-2011-269-reference" href="../nips2011_reference/nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 es  Abstract We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. [sent-7, score-0.263]
</p><p>2 The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. [sent-8, score-0.458]
</p><p>3 We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. [sent-9, score-0.126]
</p><p>4 This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. [sent-10, score-0.392]
</p><p>5 We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. [sent-11, score-0.08]
</p><p>6 It is a generic idea that can be combined with popular models, such as linear regression, factor analysis and more recently multi-task and multiple kernel learning models. [sent-13, score-0.088]
</p><p>7 In the regularization theory literature sparse inference is tackled via 1 regularization [2], which requires expensive cross-validation for model selection. [sent-14, score-0.076]
</p><p>8 From a Bayesian perspective, the spike and slab prior [1, 4, 5], also called twogroups prior [6], is the golden standard for sparse linear models. [sent-15, score-0.49]
</p><p>9 Speciﬁcally, for M linear weights, inference under a spike and slab prior distribution on those weights requires a combinatorial search over 2M possible models. [sent-17, score-0.451]
</p><p>10 The problems found when working with the spike and slab prior led several researchers to consider soft-sparse or shrinkage priors such as the Laplace and other related scale mixtures of normals [3, 7, 8, 9, 10]. [sent-18, score-0.403]
</p><p>11 In this paper, we introduce a simple and efﬁcient variational inference algorithm based on the spike and slab prior which can be widely applied to sparse linear models. [sent-20, score-0.65]
</p><p>12 The novel characteristic of this algorithm is that the variational distribution over sparse weights has a factorial nature, i. [sent-21, score-0.289]
</p><p>13 Unlike the standard mean ﬁeld approximation which uses a unimodal variational distribution, our variational algorithm can more precisely match the combinational nature of the posterior distribution over the weights. [sent-24, score-0.432]
</p><p>14 We will show that the proposed variational approach is more accurate and robust to unfavorable initializations than the standard mean ﬁeld variational approximation. [sent-25, score-0.429]
</p><p>15 We apply the variational method to a general multi-task and multiple kernel learning model that expresses the correlation between tasks by letting them share a common set of Gaussian process latent functions. [sent-26, score-0.346]
</p><p>16 Each task is modeled by linearly combining these latent functions with taskspeciﬁc weights which are given a spike and slab prior distribution. [sent-27, score-0.491]
</p><p>17 This model is a spike and slab Bayesian reformulation of previous Gaussian process-based single-task multiple kernel learning 1  methods [11, 12, 13] and multi-task Gaussian processes (GPs) [14, 15, 16, 17]. [sent-28, score-0.434]
</p><p>18 Further, this model uniﬁes several sparse linear models, such as generalized linear models, factor analysis, probabilistic PCA and matrix factorization with missing values. [sent-29, score-0.157]
</p><p>19 In the experiments, we apply the variational inference algorithms to all the above models and present results in multi-output regression, multi-class classiﬁcation, image denoising, image inpainting and collaborative ﬁltering. [sent-30, score-0.402]
</p><p>20 2  Spike and slab multi-task and multiple kernel learning  Section 2. [sent-31, score-0.308]
</p><p>21 1 discusses the spike and slab multi-task and multiple kernel learning (MTMKL) model that linearly combines Gaussian process latent functions. [sent-32, score-0.471]
</p><p>22 Spike and slab factor analysis and probabilistic PCA is discussed in Section 2. [sent-33, score-0.27]
</p><p>23 We use yq to refer to the q-th column of Y and ynq to the (n, q) entry. [sent-38, score-0.113]
</p><p>24 (1c) (1d)  M  wqm φm (x) = wq φ(x),  fq (x) = m=1  2 wqm ∼ πN (wqm |0, σw ) + (1 − π)δ0 (wqm ), φm (x) ∼ GP(µm (x), km (xi , xj )),  Here, each µm (x) is a mean function, km (xi , xj ) a covariance function, wq = [wq1 , . [sent-40, score-1.884]
</p><p>25 Since each of the Q tasks is a linear combination of the same set of latent functions {φm (x)}M (where m=1 typically M < Q ), correlation is induced in the outputs. [sent-47, score-0.111]
</p><p>26 Sharing a common set of features means that “knowledge transfer” between tasks can occur and latent functions are inferred more accurately, since data belonging to all tasks are used. [sent-48, score-0.182]
</p><p>27 For instance, a generalized linear model is obtained when the GPs are Dirac delta measures (with zero covariance functions) that deterministically assign each φm (x) to its mean function µm (x). [sent-50, score-0.075]
</p><p>28 Thus, a posterior distribution over the basis functions of the generalized linear model can be inferred from data. [sent-53, score-0.074]
</p><p>29 Secondly, a truly sparse prior, the spike and slab prior (1c), is placed over the weights of the model. [sent-54, score-0.487]
</p><p>30 Speciﬁcally, with probability 1−π, each wqm is zero, and with probability π, it is drawn from a Gaussian. [sent-55, score-0.69]
</p><p>31 Thirdly, the number of basis functions M can be inferred from data, since the sparse prior on the weights allows basis functions to be “switched off” as necessary by setting the corresponding weights to zero. [sent-58, score-0.21]
</p><p>32 Further, the model in (1) can be considered as a spike and slab Bayesian reformulation of multitask [14, 15] and multiple kernel learning previous methods [11, 12] that learn the weights using maximum likelihood. [sent-59, score-0.47]
</p><p>33 By assuming the weights wq are given, each output function yq (x) is a GP with covariance function M 2 wqm km (xi , xj ),  Cov[(yq (xi ), yq (xj )] = m=1  which clearly consists of a conic combination of kernel functions. [sent-60, score-1.114]
</p><p>34 Therefore, the proposed model can be reinterpreted as multiple kernel learning in which the weights of each kernel are assigned spike and slab priors in a full Bayesian formulation. [sent-61, score-0.513]
</p><p>35 2 Sparse factor and principal component analysis An interesting case arises when µm (x) = 0 and km (xi , xj ) = δij ∀m, where δij is the Kronecker delta. [sent-63, score-0.107]
</p><p>36 This says that each latent function is drawn from a white process so that it consists of independent values each following the standard normal distribution. [sent-64, score-0.076]
</p><p>37 We ﬁrst deﬁne matrices Φ ∈ RN ×M and W ∈ RQ×M , whose elements are, respectively, φnm = φm (xn ) and wqm . [sent-65, score-0.69]
</p><p>38 Then, the model in (1) reduces to Y = ΦW + ξ, 2 πN (wqm |0, σw )  wqm ∼ φnm ∼ N (φnm |0, 1),  (2a) + (1 − π)δ0 (wqm ),  ∀q,m ∀n,m ∀n,q ,  2 ξnq ∼ N (ξnq |0, σq ),  (2b) (2c) (2d)  where ξ is an N × Q noise matrix with entries ξnq . [sent-66, score-0.712]
</p><p>39 The resulting model thus corresponds to sparse 2 factor analysis or sparse probabilistic PCA (when the noise is homoscedastic, i. [sent-67, score-0.143]
</p><p>40 Observe that the sparse spike and slab prior is placed on the factor loadings W. [sent-70, score-0.476]
</p><p>41 3 Missing values The method can easily handle missing values and thus be applied to problems involving matrix completion and collaborative ﬁltering. [sent-72, score-0.103]
</p><p>42 In the experiments we consider missing values in applications such as image inpainting and collaborative ﬁltering. [sent-77, score-0.21]
</p><p>43 3  Efﬁcient variational inference  The presence of the Dirac delta mass function makes the application of variational approximate inference algorithms in spike and slab Bayesian models troublesome. [sent-78, score-0.805]
</p><p>44 However, there exists a simple reparameterization of the spike and slab prior that is more amenable to approximate inference 2 methods. [sent-79, score-0.438]
</p><p>45 Speciﬁcally, assume a Gaussian random variable wqm ∼ N (wqm |0, σw ) and a Bernoulli random variable sqm ∼ π sqm (1 − π)1−sqm . [sent-80, score-1.7]
</p><p>46 The product sqm wqm forms a new random variable that follows the probability distribution in eq. [sent-81, score-1.195]
</p><p>47 This allows to reparameterize wqm according to wqm = sqm wqm and assign the above prior distributions on sqm and wqm . [sent-83, score-3.819]
</p><p>48 Thus, the reparameterized spike and slab prior takes the form: 2 p(wqm , sqm ) = N (wqm |0, σw )π sqm (1 − π)1−sqm ,  ∀q,m . [sent-84, score-1.42]
</p><p>49 (3)  Notice that the presence of wqm in the likelihood function in (1a) is now replaced by the product sqm wqm . [sent-85, score-1.885]
</p><p>50 However, the above variational distribution leads to a very inefﬁcient approximation. [sent-88, score-0.187]
</p><p>51 This is because (4) is a unimodal distribution, and therefore has limited capacity when approximating the factorial true posterior distribution which can have exponentially many modes. [sent-89, score-0.076]
</p><p>52 Property 1: The true marginal posterior p(w|Y) can be written as a mixture distribution having 2M components. [sent-91, score-0.072]
</p><p>53 The standard variational distribution in (4) ignores these properties and approximates the marginal p(w|Y), which is a mixture with 2M components, with a single Gaussian distribution. [sent-99, score-0.228]
</p><p>54 Next we present an alternative variational approximation that takes into account the above properties. [sent-100, score-0.187]
</p><p>55 1  The proposed variational method  In the reparameterized spike and slab prior, each pair of variables {wqm , sqm } is strongly correlated since their product is the underlying variable that interacts with the data. [sent-102, score-1.07]
</p><p>56 Thus, a sensible approximation must treat each pair {wqm , sqm } as a unit so that {wqm , sqm } are placed in the same factor of the variational distribution. [sent-103, score-1.238]
</p><p>57 The simplest factorization that achieves this is: M  q(wq , sq ) =  q(wqm , sqm ). [sent-104, score-0.563]
</p><p>58 (5)  m=1  This variational distribution yields a marginal q(wq ) which has 2M components. [sent-105, score-0.212]
</p><p>59 This can be seen by M writing q(wq ) = m=1 [q(wqm , sqm = 1) + q(wqm , sqm = 0)] and then by multiplying the terms M a mixture of 2 components is obtained. [sent-106, score-1.026]
</p><p>60 , excluding {wqm , sqm }) and q(Θ) their variational distribution. [sent-111, score-0.692]
</p><p>61 The stationary condition for q(wqm , sqm ) is q(wqm , sqm ) =  1 e Z  log p(Y,wqm ,sqm ,Θ)  q(Θ)  2 N (wqm |0, σw )π sqm (1 − π)1−sqm ,  (7)  where Z is a normalizing constant that does not depend on {wqm , sqm }. [sent-112, score-2.02]
</p><p>62 Therefore, we C 2 have q(wqm |sqm = 0) ∝ q(wqm , sqm = 0) = Z N (wqm |0, σw )(1 − π), where C = e log p(Y,wqm ,sqm =0,Θ) q(Θ) is a constant that does not depend on wqm . [sent-113, score-1.195]
</p><p>63 The above remarks regarding variational distribution (5) are general and can hold for many spike and slab probability models as long as the weights w and binary variables s interact inside the likelihood function according to w ◦ s. [sent-115, score-0.578]
</p><p>64 2  Application to the multi-task and multiple kernel learning model  Here, we brieﬂy discuss the variational method applied to the multi-task and multiple kernel model described in Section 2. [sent-117, score-0.313]
</p><p>65 1 and refer to supplementary material for variational EM update equations. [sent-118, score-0.187]
</p><p>66 The marginal likelihood is analytically intractable, so we lower bound it using the following variational distribution Q  M  q(W, S, Φ) =  M  q(wqm , sqm ) q=1 m=1  q(φm ). [sent-120, score-0.717]
</p><p>67 More precisely, q(φm ) is an N -dimensional Gaussian distribution and each factor q(wqm , sqm ) leads to a marginal q(wqm ) which is a mixture of two Gaussians where one component is q(wqm |sqm = 2 0) = N (wqm |0, σw ), as shown in the previous section. [sent-122, score-0.571]
</p><p>68 The optimization proceeds using an EM algorithm that at the E-step updates the factors in (8) and at the M-step updates hyperparameters 2 {{σq }Q , σw , π, {θm }M } where θm parameterize kernel matrix Km . [sent-123, score-0.097]
</p><p>69 The GP hyperparameters θm are strongly dependent on the factor q(φm ) of the corresponding GP latent vector, so updating θm by keeping ﬁxed the factor q(φm ) exhibits slow convergence. [sent-125, score-0.123]
</p><p>70 4  Assessing the accuracy of the approximation  In this section we compare the proposed variational inference method, in the following called paired mean ﬁeld (PMF), against the standard mean ﬁeld (MF) approximation. [sent-128, score-0.234]
</p><p>71 The objective of the comparison is to measure the accuracy when approximating the true posterior mean value for the parameter vector wtr = E[w ◦ s] where the expectation is under the true posterior distribution. [sent-131, score-0.102]
</p><p>72 Since initialization is crucial for variational non-convex algorithms, the accuracy of PMF and MF is averaged over many random initializations of their respective variational distributions. [sent-134, score-0.426]
</p><p>73 Average values for the variational lower bound are also shown. [sent-161, score-0.187]
</p><p>74 This Gibbs sampler iteratively samples the pair (wm , sm ) from the conditional p(wm , sm |w\m , s\m , y) and has been observed to mix much faster than the standard Gibbs sampler that samples w and s separately. [sent-163, score-0.084]
</p><p>75 For each run PMF and MF are initialized to the same variational parameters. [sent-172, score-0.187]
</p><p>76 Table 1 reports average absolute errors and also average values of the variational lower bounds. [sent-173, score-0.187]
</p><p>77 To illustrate the capabilities of the proposed model, we ﬁrst apply it to a toy multi-output dataset with missing observations. [sent-179, score-0.085]
</p><p>78 5(xi − xj )) + cos(2(xi − xj ))), 20  k(xi , xj ) = exp  at 201 evenly spaced points in the interval x ∈ [−10, 10]. [sent-184, score-0.111]
</p><p>79 2 to those random latent functions, and two additional tasks consist only of Gaussian noise with standard deviations 0. [sent-186, score-0.135]
</p><p>80 Note that the above covariance function is rank-4, so ten out of the twelve tasks will be related, though we do not know how, or which ones. [sent-191, score-0.114]
</p><p>81 All tasks are then learned using both independent GPs with squared exponential (SE) covariance function kSE (xi , xj ) = exp(−(xi − xj )2 /(2 )) and the proposed MTMKL with M = 7 latent functions, each of them also using the SE prior. [sent-192, score-0.208]
</p><p>82 Hyperparameter , as well as noise levels are learned independently for each latent function. [sent-193, score-0.075]
</p><p>83 After learning, only 4 out of the 7 available latent functions remain active, while the other ones are pruned by setting the corresponding weights to zero. [sent-241, score-0.104]
</p><p>84 6  Inferred noise standard deviations for the noise-only tasks are 0. [sent-269, score-0.082]
</p><p>85 Here we illustrate denoising on the 256 × 256 “house” image used in [19]. [sent-311, score-0.071]
</p><p>86 For the covariance of the latent functions, we consider two possible choices: Either a white covariance function (as |xi −xj |  , where x are the pixel in [19]) or an exponential covariance of the form kEXP (xi , xj ) = e− coordinates within each block. [sent-315, score-0.227]
</p><p>87 We now address the inpainting problem in color images. [sent-323, score-0.079]
</p><p>88 A dictionary size of M = 100 and a white covariance function (which is used in [19]) are selected. [sent-326, score-0.092]
</p><p>89 The PSNR of the image 7  PSNR (dB) Noise std  Figure 2: Noisy “house” image with σ = 25 and restored version using Exponential cov. [sent-328, score-0.126]
</p><p>90 08  Table 4: PSNR for noisy and restored image using several noise levels and covariance functions. [sent-340, score-0.143]
</p><p>91 for white and Exponential  (b) Castle: Missing values, restored and original  Figure 3: Dictionaries inferred from noisy (σ = 25) “house” image; and “castle” inpainting results. [sent-347, score-0.155]
</p><p>92 sparse PCA but with heteroscedastic noise for the columns of the observation matrix Y which corresponds to ﬁlms) with M = 20 latent dimensions. [sent-358, score-0.143]
</p><p>93 6  Discussion  In this work we have proposed a spike and slab multi-task and multiple kernel learning model. [sent-365, score-0.418]
</p><p>94 A novel variational algorithm to perform inference in this model has been derived. [sent-366, score-0.215]
</p><p>95 The key contribution in this regard that explains the good performance of the algorithm is the choice of a joint distribution over wqm and sqm in the variational posterior, as opposed to the usual independence ˜ assumption. [sent-367, score-1.382]
</p><p>96 This has the effect of using exponentially many modes to approximate the posterior, thus rendering it more accurate and much more robust to poor initializations of the variational parameters. [sent-368, score-0.221]
</p><p>97 The relevance and wide applicability of the proposed model has been illustrated by using it on very diverse tasks: multi-output regression, multi-class classiﬁcation, image denoising, image inpainting and collaborative ﬁltering. [sent-369, score-0.187]
</p><p>98 Finally an interesting topic for future research is to optimize the variational distribution proposed here with alternative approximate inference frameworks such as belief propagation or expectation propagation. [sent-371, score-0.215]
</p><p>99 Bayesian learning in sparse graphical factor models via variational mean-ﬁeld annealing. [sent-488, score-0.26]
</p><p>100 Non-parametric Bayesian dictionary learning for sparse image represent ations. [sent-497, score-0.122]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wqm', 0.69), ('sqm', 0.505), ('slab', 0.245), ('variational', 0.187), ('mtmkl', 0.146), ('wq', 0.131), ('spike', 0.11), ('pmf', 0.091), ('ynq', 0.066), ('missing', 0.066), ('inpainting', 0.064), ('gps', 0.063), ('mf', 0.06), ('latent', 0.053), ('sparse', 0.048), ('nq', 0.047), ('yq', 0.047), ('psnr', 0.047), ('km', 0.045), ('kernel', 0.043), ('tasks', 0.043), ('image', 0.043), ('bayesian', 0.041), ('restored', 0.04), ('fq', 0.04), ('wtr', 0.04), ('sq', 0.04), ('covariance', 0.038), ('xj', 0.037), ('collaborative', 0.037), ('weights', 0.036), ('wm', 0.035), ('initializations', 0.034), ('house', 0.033), ('prior', 0.032), ('dictionary', 0.031), ('posterior', 0.031), ('owers', 0.03), ('denoising', 0.028), ('inferred', 0.028), ('inference', 0.028), ('unimodal', 0.027), ('gibbs', 0.027), ('ower', 0.026), ('gp', 0.026), ('dirac', 0.025), ('factor', 0.025), ('marginal', 0.025), ('gaussian', 0.024), ('golden', 0.023), ('reparameterization', 0.023), ('reparameterized', 0.023), ('castle', 0.023), ('db', 0.023), ('nm', 0.023), ('white', 0.023), ('dictionaries', 0.023), ('sampler', 0.023), ('ratings', 0.022), ('auc', 0.022), ('noise', 0.022), ('miguel', 0.021), ('unfavorable', 0.021), ('rr', 0.021), ('hern', 0.02), ('heteroscedastic', 0.02), ('delta', 0.02), ('multiple', 0.02), ('hyperparameters', 0.02), ('toy', 0.019), ('lms', 0.019), ('archambeau', 0.019), ('regression', 0.019), ('sm', 0.019), ('paired', 0.019), ('factorization', 0.018), ('qm', 0.018), ('marginalized', 0.018), ('classi', 0.018), ('initialization', 0.018), ('eld', 0.018), ('xi', 0.018), ('pca', 0.018), ('twelve', 0.018), ('factorial', 0.018), ('rb', 0.018), ('assign', 0.017), ('deviations', 0.017), ('updates', 0.017), ('se', 0.017), ('seeger', 0.017), ('mkl', 0.017), ('priors', 0.016), ('placed', 0.016), ('ra', 0.016), ('reformulation', 0.016), ('mixture', 0.016), ('ten', 0.015), ('color', 0.015), ('functions', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="269-tfidf-1" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>2 0.12083802 <a title="269-tfidf-2" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>Author: Shengbo Guo, Onno Zoeter, Cédric Archambeau</p><p>Abstract: We propose a new sparse Bayesian model for multi-task regression and classiﬁcation. The model is able to capture correlations between tasks, or more speciﬁcally a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classiﬁcation tasks it achieves competitive predictive performance compared to previously proposed methods. 1</p><p>3 0.11989678 <a title="269-tfidf-3" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>Author: Neil D. Lawrence, Michalis K. Titsias, Andreas Damianou</p><p>Abstract: High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences. 1</p><p>4 0.087250248 <a title="269-tfidf-4" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>Author: Danilo J. Rezende, Daan Wierstra, Wulfram Gerstner</p><p>Abstract: We derive a plausible learning rule for feedforward, feedback and lateral connections in a recurrent network of spiking neurons. Operating in the context of a generative model for distributions of spike sequences, the learning mechanism is derived from variational inference principles. The synaptic plasticity rules found are interesting in that they are strongly reminiscent of experimental Spike Time Dependent Plasticity, and in that they differ for excitatory and inhibitory neurons. A simulation conﬁrms the method’s applicability to learning both stationary and temporal spike patterns. 1</p><p>5 0.080192953 <a title="269-tfidf-5" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>Author: Artin Armagan, Merlise Clyde, David B. Dunson</p><p>Abstract: In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We ﬁrst propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efﬁciently to the types of truly massive data sets that are now encountered routinely. 1</p><p>6 0.075516149 <a title="269-tfidf-6" href="./nips-2011-On_the_Analysis_of_Multi-Channel_Neural_Spike_Data.html">200 nips-2011-On the Analysis of Multi-Channel Neural Spike Data</a></p>
<p>7 0.070730262 <a title="269-tfidf-7" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>8 0.064739943 <a title="269-tfidf-8" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>9 0.060238644 <a title="269-tfidf-9" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>10 0.058571659 <a title="269-tfidf-10" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>11 0.056788281 <a title="269-tfidf-11" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>12 0.055866383 <a title="269-tfidf-12" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>13 0.054320615 <a title="269-tfidf-13" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>14 0.050747264 <a title="269-tfidf-14" href="./nips-2011-Additive_Gaussian_Processes.html">26 nips-2011-Additive Gaussian Processes</a></p>
<p>15 0.05065015 <a title="269-tfidf-15" href="./nips-2011-A_blind_sparse_deconvolution_method_for_neural_spike_identification.html">13 nips-2011-A blind sparse deconvolution method for neural spike identification</a></p>
<p>16 0.049002122 <a title="269-tfidf-16" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>17 0.047837142 <a title="269-tfidf-17" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>18 0.045361511 <a title="269-tfidf-18" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>19 0.044478077 <a title="269-tfidf-19" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>20 0.041794427 <a title="269-tfidf-20" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.063), (2, 0.063), (3, -0.041), (4, -0.033), (5, -0.059), (6, 0.066), (7, -0.033), (8, 0.049), (9, 0.11), (10, -0.058), (11, -0.064), (12, 0.017), (13, -0.007), (14, -0.079), (15, 0.028), (16, -0.016), (17, 0.012), (18, 0.002), (19, -0.065), (20, 0.085), (21, -0.003), (22, 0.044), (23, -0.048), (24, 0.009), (25, -0.043), (26, 0.047), (27, 0.049), (28, -0.047), (29, 0.068), (30, 0.004), (31, -0.006), (32, -0.048), (33, -0.06), (34, 0.097), (35, 0.005), (36, -0.063), (37, 0.031), (38, -0.012), (39, -0.056), (40, -0.065), (41, 0.024), (42, -0.024), (43, -0.058), (44, -0.03), (45, -0.104), (46, 0.04), (47, -0.021), (48, -0.016), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90030384 <a title="269-lsi-1" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>2 0.76928473 <a title="269-lsi-2" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>3 0.6934396 <a title="269-lsi-3" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>Author: Neil D. Lawrence, Michalis K. Titsias, Andreas Damianou</p><p>Abstract: High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences. 1</p><p>4 0.67893118 <a title="269-lsi-4" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>Author: Artin Armagan, Merlise Clyde, David B. Dunson</p><p>Abstract: In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We ﬁrst propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efﬁciently to the types of truly massive data sets that are now encountered routinely. 1</p><p>5 0.66766852 <a title="269-lsi-5" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>Author: Shengbo Guo, Onno Zoeter, Cédric Archambeau</p><p>Abstract: We propose a new sparse Bayesian model for multi-task regression and classiﬁcation. The model is able to capture correlations between tasks, or more speciﬁcally a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classiﬁcation tasks it achieves competitive predictive performance compared to previously proposed methods. 1</p><p>6 0.65623116 <a title="269-lsi-6" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>7 0.62716764 <a title="269-lsi-7" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>8 0.60729229 <a title="269-lsi-8" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>9 0.57241976 <a title="269-lsi-9" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<p>10 0.54453689 <a title="269-lsi-10" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>11 0.51100904 <a title="269-lsi-11" href="./nips-2011-Global_Solution_of_Fully-Observed_Variational_Bayesian_Matrix_Factorization_is_Column-Wise_Independent.html">107 nips-2011-Global Solution of Fully-Observed Variational Bayesian Matrix Factorization is Column-Wise Independent</a></p>
<p>12 0.50548196 <a title="269-lsi-12" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>13 0.47482008 <a title="269-lsi-13" href="./nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</a></p>
<p>14 0.47120523 <a title="269-lsi-14" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>15 0.45996433 <a title="269-lsi-15" href="./nips-2011-Kernel_Bayes%27_Rule.html">139 nips-2011-Kernel Bayes' Rule</a></p>
<p>16 0.45281032 <a title="269-lsi-16" href="./nips-2011-Gaussian_Process_Training_with_Input_Noise.html">100 nips-2011-Gaussian Process Training with Input Noise</a></p>
<p>17 0.45133299 <a title="269-lsi-17" href="./nips-2011-Bayesian_Bias_Mitigation_for_Crowdsourcing.html">42 nips-2011-Bayesian Bias Mitigation for Crowdsourcing</a></p>
<p>18 0.44787255 <a title="269-lsi-18" href="./nips-2011-A_concave_regularization_technique_for_sparse_mixture_models.html">14 nips-2011-A concave regularization technique for sparse mixture models</a></p>
<p>19 0.43251985 <a title="269-lsi-19" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>20 0.42690572 <a title="269-lsi-20" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (4, 0.028), (20, 0.027), (25, 0.251), (26, 0.022), (31, 0.099), (33, 0.021), (43, 0.089), (45, 0.087), (57, 0.05), (65, 0.014), (66, 0.011), (74, 0.073), (83, 0.045), (84, 0.015), (99, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78718179 <a title="269-lda-1" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>Author: Philip S. Thomas</p><p>Abstract: We present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules. We present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module’s input, output, and the TD error broadcast by a critic. Such updates are necessary when computation of compatible features becomes prohibitively difﬁcult and are also desirable to increase the biological plausibility of reinforcement learning methods. 1</p><p>same-paper 2 0.743774 <a title="269-lda-2" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>3 0.67813516 <a title="269-lda-3" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efﬁcient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deﬁne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reﬁned further in such a way to converge monotonically to the exact Euclidean distance within d reﬁnement steps. An analysis on a random sequence of reﬁnement steps shows that the MS-distance provides very tight bounds in only a few reﬁnement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>4 0.60070783 <a title="269-lda-4" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>Author: Jun-ichiro Hirayama, Aapo Hyvärinen</p><p>Abstract: Components estimated by independent component analysis and related methods are typically not independent in real data. A very common form of nonlinear dependency between the components is correlations in their variances or energies. Here, we propose a principled probabilistic model to model the energycorrelations between the latent variables. Our two-stage model includes a linear mixing of latent signals into the observed ones like in ICA. The main new feature is a model of the energy-correlations based on the structural equation model (SEM), in particular, a Linear Non-Gaussian SEM. The SEM is closely related to divisive normalization which effectively reduces energy correlation. Our new twostage model enables estimation of both the linear mixing and the interactions related to energy-correlations, without resorting to approximations of the likelihood function or other non-principled approaches. We demonstrate the applicability of our method with synthetic dataset, natural images and brain signals. 1</p><p>5 0.59761167 <a title="269-lda-5" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>Author: Shengbo Guo, Onno Zoeter, Cédric Archambeau</p><p>Abstract: We propose a new sparse Bayesian model for multi-task regression and classiﬁcation. The model is able to capture correlations between tasks, or more speciﬁcally a low-rank approximation of the covariance matrix, while being sparse in the features. We introduce a general family of group sparsity inducing priors based on matrix-variate Gaussian scale mixtures. We show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type II maximum likelihood estimation of the hyperparameters. Empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classiﬁcation tasks it achieves competitive predictive performance compared to previously proposed methods. 1</p><p>6 0.59680194 <a title="269-lda-6" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>7 0.59592146 <a title="269-lda-7" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>8 0.59162301 <a title="269-lda-8" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>9 0.59039497 <a title="269-lda-9" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>10 0.58917236 <a title="269-lda-10" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>11 0.58793223 <a title="269-lda-11" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>12 0.58743507 <a title="269-lda-12" href="./nips-2011-Hierarchically_Supervised_Latent_Dirichlet_Allocation.html">116 nips-2011-Hierarchically Supervised Latent Dirichlet Allocation</a></p>
<p>13 0.58369964 <a title="269-lda-13" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>14 0.58341795 <a title="269-lda-14" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>15 0.58269274 <a title="269-lda-15" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>16 0.58246791 <a title="269-lda-16" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>17 0.58244103 <a title="269-lda-17" href="./nips-2011-Expressive_Power_and_Approximation_Errors_of_Restricted_Boltzmann_Machines.html">92 nips-2011-Expressive Power and Approximation Errors of Restricted Boltzmann Machines</a></p>
<p>18 0.58226931 <a title="269-lda-18" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>19 0.58222204 <a title="269-lda-19" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>20 0.58165747 <a title="269-lda-20" href="./nips-2011-The_Kernel_Beta_Process.html">285 nips-2011-The Kernel Beta Process</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
