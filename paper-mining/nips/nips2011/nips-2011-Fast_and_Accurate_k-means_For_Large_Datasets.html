<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2011-Fast and Accurate k-means For Large Datasets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-95" href="#">nips2011-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 nips-2011-Fast and Accurate k-means For Large Datasets</h1>
<br/><p>Source: <a title="nips-2011-95-pdf" href="http://papers.nips.cc/paper/4362-fast-and-accurate-k-means-for-large-datasets.pdf">pdf</a></p><p>Author: Michael Shindler, Alex Wong, Adam W. Meyerson</p><p>Abstract: Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o( nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.</p><p>Reference: <a title="nips-2011-95-reference" href="../nips2011_reference/nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. [sent-8, score-0.442]
</p><p>2 Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. [sent-10, score-0.365]
</p><p>3 1 Introduction We design improved algorithms for Euclidean k-means in the streaming model. [sent-13, score-0.297]
</p><p>4 Our goal is to select k points in this space to designate asfacilities (sometimes called centers or means); the overall cost of the solution is the sum of the squared distances from each point to its nearest facility. [sent-15, score-0.304]
</p><p>5 In the streaming model, we require that the point set be read sequentially, and that our algorithm stores very few points at any given time. [sent-17, score-0.412]
</p><p>6 Many problems which are easy to solve in the standard batch-processing model require more complex techniques in the streaming model (a survey of streaming results is available [3]); nonetheless there are a number of existing streaming approximations for Euclidean k-means. [sent-18, score-0.891]
</p><p>7 We present a new algorithm for the problem based on [9] with several significant improvements; we are able to prove a faster worst-case running time and a better approximation factor. [sent-19, score-0.228]
</p><p>8 Many of the applications for k-means have experienced a large growth in data that has overtaken the amount of memory typically available to a computer. [sent-24, score-0.221]
</p><p>9 This is expressed in the streaming model, where an algorithm must make one (or very few) passes through the data, reflecting cases where random access to the data is unavailable, such as a very large file onÂ· a hard disk. [sent-25, score-0.392]
</p><p>10 They "guess" the cost of the optimum, then run the online facility location algorithm of [24] until either the total cost of the solution exceeds a constant times the guess or the total number of facilities exceeds some computed value 1\,. [sent-28, score-1.364]
</p><p>11 They then declare the end of a phase, increase the guess, consolidate the facilities via matching, and continue with the next point. [sent-29, score-0.415]
</p><p>12 When the stream has been exhausted, the algorithm has some I\, facilities, which are then consolidated down to k. [sent-30, score-0.235]
</p><p>13 They then run a ball k-means step (similar to [25]) by maintaining samples of the points assigned to each facility and moving the facilities to the centers of mass of these samples. [sent-31, score-1.026]
</p><p>14 2 for the definition), they use ball k-means to improve the approximation factor to 1 + 0(0- 2 ). [sent-34, score-0.132]
</p><p>15 The approximation factor is in the hundreds, and the 0 (k log n) memory requirement has sufficiently high constants that there are actually more than n facilities for many of the data sets analyzed in previous papers. [sent-36, score-0.818]
</p><p>16 We improve the manner by which the algorithm determines better facility cost as the stream is processed, removing unnecessary checks and allowing the user to parametrize what remains. [sent-40, score-0.871]
</p><p>17 We remove the end-of-phase condition based on the total cost, ending phases only when the number of facilities exceeds 1\,. [sent-43, score-0.449]
</p><p>18 We also simplify the transition between phases, observing that it's quite simple to bound the number of phases by log 0 PT (where OPT is the optimum k-means cost), and that in practice this number of phases is usually quite a bit less than n. [sent-45, score-0.243]
</p><p>19 Our proof is based on a much tighter bound on the cost incurred per phase, along with a more flexible definition of the "critical phase" by which the algorithm should terminate. [sent-47, score-0.202]
</p><p>20 For appropriately chosen constants our approximation factor will be roughly 17, substantially less than the factor claimed in [9] prior to the ball k-means step. [sent-49, score-0.245]
</p><p>21 In addition, we apply approximate nearest-neighbor algorithms to compute the facility assignment of each point. [sent-50, score-0.5]
</p><p>22 The running time of our algorithm is dominated by repeated nearest-neighbor calculations, and an appropriate technique can change our running time from 8 (nk log n) to 8 (n(log k + loglogn)), an improvement for most values of k. [sent-51, score-0.231]
</p><p>23 Note that our final running time is actually faster than the 8 (nk) time needed to compute the k-means cost of a given set of facilities! [sent-53, score-0.319]
</p><p>24 This allows us to compare our algorithm to previous [4, 2] streaming k-means results. [sent-55, score-0.333]
</p><p>25 These become the new facilities for the next phase, and the process repeats until it is stable. [sent-60, score-0.38]
</p><p>26 Unfortunately, Lloyd's algorithm has no provable approximation bound, and arbitrarily bad examples exist. [sent-61, score-0.129]
</p><p>27 They defined the notion of (J"separability, where the input to k-means is said to be (J"-separable if reducing the number of facilities They designed an from k to k - 1 would incr~ase the cost of the optimum solution by a factor algorithm with approximation ratio 1 + O((J"2). [sent-68, score-0.794]
</p><p>28 ;2'  There are two basic approaches to the streaming version of the k-means problem. [sent-70, score-0.297]
</p><p>29 Our approach is based on solving k-means as we go (thus at each point in the algorithm, our memory contains a current set of facilities). [sent-71, score-0.221]
</p><p>30 Their work was based on guessing a lower bound on the optimum k-median cost and running O(log n) parallel versions of the online facility location algorithm of Meyerson [24] with facility cost based on the guessed lower bound. [sent-75, score-1.589]
</p><p>31 When these parallel calls exceeded the approximation bounds, they would be terminated and the guessed lower bound on the optimum k-median cost would increase. [sent-76, score-0.312]
</p><p>32 The recent paper of Braverman, Meyerson, Ostrovsky, Roytman, Shindler, and Tagiku [9] extended the result of [11] to k-means and improved the space bound to 0 ( k log n) by proving high-probability bounds on the performance of online facility location. [sent-77, score-0.564]
</p><p>33 This result also added a ball k-means step (as in [25]) to substantially improve the approximation factor under the assumption that the original data was (J"-separable. [sent-78, score-0.177]
</p><p>34 Another recent result for streaming k-means, due to Ailon, Jaiswal, and Monteleoni [4], is based on a divide and conquer approach, similar to the k-median algorithm of Guha, Meyerson Mishra, Motwani, and O'Callaghan [16]. [sent-79, score-0.385]
</p><p>35 Their experiment showed that this algorithm is an improvement over an online variant of Lloyd's algorithm and was comparable to the batch version of Lloyd's. [sent-81, score-0.167]
</p><p>36 The other approach to streaming k-means is based on coresets: selecting a weighted subset of the original input points such that any k-means solution on the subset has roughly the same cost as on the original point set. [sent-82, score-0.513]
</p><p>37 At any point in the algorithm, the memory should contain a weighted representative sample of the points. [sent-83, score-0.221]
</p><p>38 2 Algorithm and Theory Both our algorithm and that of [9] are based on the online facility location algorithm of [24]. [sent-87, score-0.655]
</p><p>39 For the facility location problem, the number of clusters is not part of the input (as it is for k-means), but rather a facility cost is given; an algorithm to solve this problem may have as many clusters as it desires in its output, simply by denoting some point as a facility. [sent-88, score-1.22]
</p><p>40 The solution cost is then the sum of the resulting k-means cost ("service cost") and the total paid for facilities. [sent-89, score-0.305]
</p><p>41 Our algorithm runs the online facility location algorithm of [24] with a small facility cost until we have more than ~ E e (k log n) facilities. [sent-90, score-1.32]
</p><p>42 It then increases the facility cost, re-evaluates the current facilities, and continues with the stream. [sent-91, score-0.5]
</p><p>43 We ignore the overall service cost in determining when to end a phase and raise our facility cost f. [sent-95, score-0.909]
</p><p>44 Further, the number of facilities which must open to end a phase can be any ~ E (k log n), the constants do not depend directly on the competitive ratio of online facility location (as they did in [9]). [sent-96, score-1.136]
</p><p>45 Our constant is substantially smaller than those implicit in [9], with most of the loss occurring in the final non-streaming k-means algorithm to consolidate ~ means down to k. [sent-99, score-0.216]
</p><p>46 Suppose that our algorithm completes the data stream when the facility cost is f. [sent-102, score-0.871]
</p><p>47 Then the overall solution prior to the final re-clustering has expected service cost at most ~~, and the probability of being within 1 + E of the expected service cost is at least 1 - pol~( n) . [sent-103, score-0.551]
</p><p>48 With probability at least 1 - pol~(n)' the algorithm will either halt with 1 :::; e(~*){3, where C* is the optimum k-means cost, or it will halt within one phase of exceeding this value. [sent-105, score-0.246]
</p><p>49 In practice, we would expect the performance of online facility location to be substantially better than worst-case (in fact, if the ordering of points in the stream is non-adversarial there is a proof to this effect in [24]); in addition the assumption was made that distances add (i. [sent-108, score-0.874]
</p><p>50 We also assumed that using more than k facilities does not substantially help the optimum service cost (also unlikely to be true for real data). [sent-111, score-0.71]
</p><p>51 Combining these, it would be unsurprising if our service cost was actually better than optimum at the end of the data stream (of course, we used many more facilities than optimum, so it is not precisely a fair comparison). [sent-112, score-0.864]
</p><p>52 4  We note that if we run the streaming part of the algorithm NI times in parallel, we can take the solution with the smallest final facility cost. [sent-116, score-0.994]
</p><p>53 memory requirement and increasing NI can increase both memory and running time requirements. [sent-122, score-0.525]
</p><p>54 A theoretically sound approach involves mapping these means back to randomly selected points from the original set (these can be maintained in a streaming manner) and then approximating k-means on ~ points using a non-streaming algorithm. [sent-124, score-0.391]
</p><p>55 The overall approximation ratio will be twice the ratio established by our algorithm (we lose a factor of two by mapping back to the original points) plus the approximation ratio for the non-streaming algorithm. [sent-125, score-0.314]
</p><p>56 This requires as many as ~ distance computations; there are a number of results enabling fast computation of approximate nearest neighbors and applying these results will improve our running time. [sent-129, score-0.141]
</p><p>57 We store our facilities sorted by their inner product with w. [sent-137, score-0.38]
</p><p>58 When a new point x arrives, instead of taking O(~) to determine its (exact) nearest neighbor, we instead use O(log~) to find the two facilities that x . [sent-138, score-0.438]
</p><p>59 We determine the (exact) closer of these two facilities; this determines the value of 6 in lines 5 and 17 and the "closest" facility in lines 9 and 21. [sent-140, score-0.5]
</p><p>60 If our approximate nearest neighbor computation finds a facility with distance at most v times the distance to the closest facility in expectation, then the approximation ratio increase by a constant factor. [sent-142, score-1.241]
</p><p>61 We selected data sets which have been used previously to demonstrate streaming algorithms. [sent-147, score-0.297]
</p><p>62 The main motivation for streaming is very large data sets, so we are more interested in sets that might be difficult to fit in a main memory and focused on the largest examples. [sent-149, score-0.553]
</p><p>63 We took the best observed cost for each value of k, and found the four values of k minimizing the ratio of k-means cost to (k - I)-means cost. [sent-159, score-0.314]
</p><p>64 Instead, we ran a modified version of our algorithm; at the end of a phase, it adjusts the facility cost and restarts the stream. [sent-161, score-0.675]
</p><p>65 First, the memory is not configurable, making it not fit into the common baselme that we will define shortly. [sent-166, score-0.221]
</p><p>66 Second, the memory requirements and runtime, while asymptotically nice, have large leading constants that cause it to be impracticaL In fact, it was an attempt to implement this algorithm that initially motivated the work on this paper. [sent-167, score-0.295]
</p><p>67 1  Implementation Discussion  The divide and conquer ("D&C;") algorithm [4] can use its available memory in two possible ways. [sent-169, score-0.309]
</p><p>68 First, it can use the entire amount to read from the stream, writing the results of computing their 3k log k means to disk; when the stream is exhausted, this file is treated as a stream, until an iteration produces a file that fits entirely into main memory. [sent-170, score-0.378]
</p><p>69 Alternately, the available memory could be partitioned into layers; the first layer would be filled by reading from the stream, and the weighted facilities produced would be stored in the second. [sent-171, score-0.601]
</p><p>70 Upon completion of the stream, any remaining points are gathered and clustered to produce k final means. [sent-173, score-0.147]
</p><p>71 When larger amounts of memory are available, the latter method is preferred. [sent-174, score-0.221]
</p><p>72 As our goal is to judge streaming algorithms under low memory conditions, we used the first approach, which is more fitting to such a constraint. [sent-176, score-0.518]
</p><p>73 9 GhZ and with 6 GB main memory (although nowhere near the entirety of this was used by any algorithm). [sent-179, score-0.221]
</p><p>74 With all algorithms, the reported cost is determined by taking the resulting k facilities and computing the k-means cost across the entire dataset. [sent-181, score-0.652]
</p><p>75 The time to compute this cost is not included in the reported running times of the algorithms. [sent-182, score-0.219]
</p><p>76 Instead of just comparing for the same dataset and cluster count, we further constrained each to use the same amount of memory (in terms of number of points stored in random access). [sent-186, score-0.268]
</p><p>77 The memory constraints were chosen to reflect the usage of small amounts of memory that are close to the algorithms' designers' specifications, where possible. [sent-187, score-0.442]
</p><p>78 Ailon et al [4] suggest -v:n:k memory for the batch process; this memory availability is marked in the charts by an asterisk. [sent-188, score-0.502]
</p><p>79 The suggestion from [2] for a coreset of size 200k was not run for all algorithms, as the amount of memory necessary for computing a coreset of this size is much larger than the other cases, and our goal is to compare the algorithms at a small memory limit. [sent-189, score-0.712]
</p><p>80 This does produce a drop in solution quality compared to running the algorithm at their suggested parameters, although their approach remains competitive. [sent-190, score-0.152]
</p><p>81 Finally, our algorithm suggests memory of ~ = k log n or a small constant times the same. [sent-191, score-0.286]
</p><p>82 In each case, the memory constraint dictates the parameters; for the divide and conquer algorithm, this is simply the batch size. [sent-192, score-0.333]
</p><p>83 Our algorithm is a little more parametrizable; when M memory is available, we allowed ~ = M /5 and each facility to have four samples. [sent-194, score-0.757]
</p><p>84 edu/ - shindler / to access code for our algorithms  6  JIIIOurs+ANN IIIlD&C;  JIIID&C;  2520  3350  MemoryAvailable  MemoryAvailaible  Figure 1: Census Data, k=8, cost  ~  Figure 2: Census Data, k=8, time  1I0ur,,+ANN  4DDE+13  1I0&C;  . [sent-198, score-0.254]
</p><p>85 D&C;  3780  5040  Memory Availahle  MemOfyAvaUahle  Figure 3: Census Data, k= 12, cost  ~  Figure 4: Census Data, k=12, time  1. [sent-199, score-0.136]
</p><p>86 l+ANN  MemoryAvailahle  MemoryAvaiEable  Figure 5: BigCross Data, k=13, cost  Figure 6: BigCross Data, k=13, time  ; Ours 1lI0urs-:-ANN IlIStreamKMH  MemoryAvaifable  MemoryAVlliiable  Figure 7: BigCross Data, k=24, cost  Figure 8: BigCross Data, k=24, time  7  4. [sent-202, score-0.272]
</p><p>87 Furthermore, our algorithm stands to gain the most by improved solutions to batch k-means, due to the better representative sample present after the stream is processed. [sent-205, score-0.295]
</p><p>88 The prohibitively high running time of the divide-and-conquer algorithm [4] is due to the many repeated instances of running their k-means# algorithm on each batch of the given size. [sent-206, score-0.298]
</p><p>89 The decline in accuracy for StreamKM++ at very low memory can be partially explained by the 8(k 2 1og8 n) points' worth of memory needed for a strong guarantee in previous theory work [12]. [sent-213, score-0.442]
</p><p>90 However, the fact that the algorithm is able to achieve a good approximation in practice while using far less than that amount of memory suggests that improved provable bounds for coreset algorithms may be on the horizon. [sent-214, score-0.471]
</p><p>91 We should note that the performance of the algorithm declines sharply as the memory difference with the authors' specification grows, but gains accuracy as the memory grows. [sent-215, score-0.478]
</p><p>92 The final approximation ratios can be described as a(l + E) where a is the loss from the final batch algorithm. [sent-217, score-0.321]
</p><p>93 The coreset E is a direct function of the memory allowed to the algorithm, and can be made arbitrarily small. [sent-218, score-0.342]
</p><p>94 However, the memory needed to provably reduce E to a small constant is quite substantial, and while StreamKM++ does produce a good resulting clustering, it is not immediately clear the the discovery of better batch k-means algorithms would improve their solution quality. [sent-219, score-0.314]
</p><p>95 Our algorithm's E represents the ratio of the cost of our f1:-mean solution to the cost of the optimum k-means solution. [sent-220, score-0.423]
</p><p>96 For our algorithm, the observed value of 1 + E has been typically between 1 and 3, whereas the D&C; approach did not yield one better than 24, and was high (low thousands) for the very low memory conditions. [sent-222, score-0.221]
</p><p>97 The coreset algorithm was the worst, with even the best values in the mid ten figures (tens to hundreds of billions). [sent-223, score-0.157]
</p><p>98 The low ratio for our algorithm also suggests that our ~ facilities are a good sketch of the overall data, and thus our observed accuracy can be expected to improve as more accurate batch k-means algorithms are discovered. [sent-224, score-0.518]
</p><p>99 Local search heuristic for k-median and facility location problems. [sent-254, score-0.548]
</p><p>100 On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications. [sent-266, score-0.129]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facility', 0.5), ('facilities', 0.38), ('streaming', 0.297), ('memory', 0.221), ('stream', 0.199), ('meyerson', 0.157), ('bigcross', 0.137), ('cost', 0.136), ('coreset', 0.121), ('lloyd', 0.121), ('shindler', 0.118), ('streamkm', 0.118), ('census', 0.111), ('final', 0.1), ('running', 0.083), ('mishra', 0.079), ('ostrovsky', 0.079), ('sohler', 0.079), ('optimum', 0.076), ('christian', 0.073), ('service', 0.073), ('phases', 0.069), ('guha', 0.069), ('motwani', 0.069), ('adam', 0.067), ('nk', 0.067), ('clustering', 0.066), ('phase', 0.064), ('coresets', 0.063), ('approximation', 0.061), ('batch', 0.06), ('file', 0.059), ('liadan', 0.059), ('sufficiently', 0.059), ('nearest', 0.058), ('ailon', 0.052), ('conquer', 0.052), ('exhausted', 0.052), ('location', 0.048), ('stoc', 0.048), ('pol', 0.048), ('significant', 0.048), ('points', 0.047), ('neighbor', 0.046), ('substantially', 0.045), ('ann', 0.042), ('ratio', 0.042), ('ball', 0.041), ('ackermann', 0.039), ('alenex', 0.039), ('badoiu', 0.039), ('guessed', 0.039), ('iki', 0.039), ('jaiswal', 0.039), ('lammersen', 0.039), ('martens', 0.039), ('minyek', 0.039), ('modified', 0.039), ('rabani', 0.039), ('rafail', 0.039), ('raupach', 0.039), ('roytman', 0.039), ('sariel', 0.039), ('schulman', 0.039), ('setk', 0.039), ('sudipto', 0.039), ('unread', 0.039), ('vassilvitskii', 0.039), ('constants', 0.038), ('arthur', 0.037), ('algorithm', 0.036), ('online', 0.035), ('scg', 0.035), ('consolidate', 0.035), ('difficult', 0.035), ('halt', 0.035), ('kanungo', 0.035), ('netanyahu', 0.035), ('nina', 0.035), ('piatko', 0.035), ('rajeev', 0.035), ('silverman', 0.035), ('soda', 0.034), ('closest', 0.034), ('solution', 0.033), ('guess', 0.032), ('read', 0.032), ('provable', 0.032), ('focs', 0.032), ('braverman', 0.032), ('charikar', 0.032), ('mount', 0.032), ('logn', 0.032), ('piotr', 0.032), ('factor', 0.03), ('definition', 0.03), ('centers', 0.03), ('log', 0.029), ('run', 0.028), ('streams', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="95-tfidf-1" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>Author: Michael Shindler, Alex Wong, Adam W. Meyerson</p><p>Abstract: Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o( nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.</p><p>2 0.2073745 <a title="95-tfidf-2" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>Author: Dan Feldman, Matthew Faulkner, Andreas Krause</p><p>Abstract: How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models ï¬tting the coreset will also provide a good ï¬t for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3 /Îµ2 ) data points sufï¬ces for computing a (1 + Îµ)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efï¬ciently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones. 1</p><p>3 0.068067014 <a title="95-tfidf-3" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>Author: Dominique Tschopp, Suhas Diggavi, Payam Delgosha, Soheil Mohajer</p><p>Abstract: This paper addresses the problem of ï¬nding the nearest neighbor (or one of the R-nearest neighbors) of a query object q in a database of n objects, when we can only use a comparison oracle. The comparison oracle, given two reference objects and a query object, returns the reference object most similar to the query object. The main problem we study is how to search the database for the nearest neighbor (NN) of a query, while minimizing the questions. The difï¬culty of this problem depends on properties of the underlying database. We show the importance of a characterization: combinatorial disorder D which deï¬nes approximate triangle n inequalities on ranks. We present a lower bound of â¦(D log D + D2 ) average number of questions in the search phase for any randomized algorithm, which demonstrates the fundamental role of D for worst case behavior. We develop 3 a randomized scheme for NN retrieval in O(D3 log2 n + D log2 n log log nD ) 3 questions. The learning requires asking O(nD3 log2 n + D log2 n log log nD ) questions and O(n log2 n/ log(2D)) bits to store.</p><p>4 0.056444407 <a title="95-tfidf-4" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>5 0.04579027 <a title="95-tfidf-5" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<p>Author: Abhishek Kumar, Piyush Rai, Hal Daume</p><p>Abstract: In many clustering problems, we have access to multiple views of the data each of which could be individually used for clustering. Exploiting information from multiple views, one can hope to ï¬nd a clustering that is more accurate than the ones obtained using the individual views. Often these different views admit same underlying clustering of the data, so we can approach this problem by looking for clusterings that are consistent across the views, i.e., corresponding data points in each view should have same cluster membership. We propose a spectral clustering framework that achieves this goal by co-regularizing the clustering hypotheses, and propose two co-regularization schemes to accomplish this. Experimental comparisons with a number of baselines on two synthetic and three real-world datasets establish the efï¬cacy of our proposed approaches.</p><p>6 0.045277052 <a title="95-tfidf-6" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>7 0.045054857 <a title="95-tfidf-7" href="./nips-2011-Hogwild%3A_A_Lock-Free_Approach_to_Parallelizing_Stochastic_Gradient_Descent.html">121 nips-2011-Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</a></p>
<p>8 0.043952897 <a title="95-tfidf-8" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>9 0.043421425 <a title="95-tfidf-9" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>10 0.043262489 <a title="95-tfidf-10" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>11 0.042475782 <a title="95-tfidf-11" href="./nips-2011-Active_Learning_Ranking_from_Pairwise_Preferences_with_Almost_Optimal_Query_Complexity.html">20 nips-2011-Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity</a></p>
<p>12 0.041677542 <a title="95-tfidf-12" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>13 0.041619502 <a title="95-tfidf-13" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>14 0.041337717 <a title="95-tfidf-14" href="./nips-2011-A_Collaborative_Mechanism_for_Crowdsourcing_Prediction_Problems.html">3 nips-2011-A Collaborative Mechanism for Crowdsourcing Prediction Problems</a></p>
<p>15 0.038711213 <a title="95-tfidf-15" href="./nips-2011-Approximating_Semidefinite_Programs_in_Sublinear_Time.html">39 nips-2011-Approximating Semidefinite Programs in Sublinear Time</a></p>
<p>16 0.037564643 <a title="95-tfidf-16" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>17 0.036556214 <a title="95-tfidf-17" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>18 0.036508992 <a title="95-tfidf-18" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>19 0.035690676 <a title="95-tfidf-19" href="./nips-2011-Statistical_Performance_of_Convex_Tensor_Decomposition.html">270 nips-2011-Statistical Performance of Convex Tensor Decomposition</a></p>
<p>20 0.034760311 <a title="95-tfidf-20" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, -0.007), (2, -0.023), (3, -0.014), (4, -0.002), (5, 0.01), (6, -0.026), (7, -0.006), (8, -0.027), (9, -0.039), (10, 0.018), (11, 0.054), (12, -0.007), (13, -0.093), (14, 0.108), (15, 0.044), (16, -0.041), (17, 0.037), (18, -0.023), (19, -0.01), (20, 0.037), (21, 0.017), (22, 0.021), (23, -0.024), (24, -0.013), (25, 0.014), (26, 0.039), (27, 0.096), (28, 0.088), (29, 0.008), (30, -0.046), (31, -0.035), (32, 0.005), (33, 0.061), (34, -0.053), (35, 0.054), (36, 0.068), (37, -0.006), (38, 0.014), (39, -0.015), (40, -0.04), (41, -0.059), (42, -0.041), (43, 0.03), (44, 0.072), (45, -0.027), (46, -0.032), (47, 0.107), (48, 0.092), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89641428 <a title="95-lsi-1" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>Author: Michael Shindler, Alex Wong, Adam W. Meyerson</p><p>Abstract: Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o( nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.</p><p>2 0.79829937 <a title="95-lsi-2" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>Author: Dan Feldman, Matthew Faulkner, Andreas Krause</p><p>Abstract: How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models ï¬tting the coreset will also provide a good ï¬t for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size independent of the size of the data set. More precisely, we prove that a weighted set of O(dk3 /Îµ2 ) data points sufï¬ces for computing a (1 + Îµ)-approximation for the optimal model on the original n data points. Moreover, such coresets can be efï¬ciently constructed in a map-reduce style computation, as well as in a streaming setting. Our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of Gaussians. We empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in mobile phones. 1</p><p>3 0.61116964 <a title="95-lsi-3" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>Author: Yoonho Hwang, Hee-kap Ahn</p><p>Abstract: Given a set V of n vectors in d-dimensional space, we provide an efï¬cient method for computing quality upper and lower bounds of the Euclidean distances between a pair of vectors in V . For this purpose, we deï¬ne a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance provides upper and lower bounds of Euclidean distance between any pair of vectors in V in constant time. Furthermore, these bounds can be reï¬ned further in such a way to converge monotonically to the exact Euclidean distance within d reï¬nement steps. An analysis on a random sequence of reï¬nement steps shows that the MS-distance provides very tight bounds in only a few reï¬nement steps. The MS-distance can be used to various applications where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches. 1</p><p>4 0.52961403 <a title="95-lsi-4" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>Author: Bogdan Alexe, Viviana Petrescu, Vittorio Ferrari</p><p>Abstract: We present a computationally efï¬cient technique to compute the distance of highdimensional appearance descriptor vectors between image windows. The method exploits the relation between appearance distance and spatial overlap. We derive an upper bound on appearance distance given the spatial overlap of two windows in an image, and use it to bound the distances of many pairs between two images. We propose algorithms that build on these basic operations to efï¬ciently solve tasks relevant to many computer vision applications, such as ï¬nding all pairs of windows between two images with distance smaller than a threshold, or ï¬nding the single pair with the smallest distance. In experiments on the PASCAL VOC 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18] and on hashing [21]. For example, our algorithm ï¬nds the most similar pair of windows between two images while computing only 1% of all distances on average. 1</p><p>5 0.51296741 <a title="95-lsi-5" href="./nips-2011-Efficient_anomaly_detection_using_bipartite_k-NN_graphs.html">81 nips-2011-Efficient anomaly detection using bipartite k-NN graphs</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection. Several approaches to learning minimum volume sets have been proposed in the literature, including the K-point nearest neighbor graph (K-kNNG) algorithm based on the geometric entropy minimization (GEM) principle [4]. The K-kNNG detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out kNNG (L1O-kNNG) was proposed. In this paper, we propose a novel bipartite k-nearest neighbor graph (BPkNNG) anomaly detection scheme for estimating minimum volume sets. Our bipartite estimator retains all the desirable theoretical properties of the K-kNNG, while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors. We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point. Experimental results are given that illustrate the superior performance of BP-kNNG as compared to the L1O-kNNG and other state of the art anomaly detection schemes.</p><p>6 0.48101076 <a title="95-lsi-6" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>7 0.47273663 <a title="95-lsi-7" href="./nips-2011-On_U-processes_and_clustering_performance.html">198 nips-2011-On U-processes and clustering performance</a></p>
<p>8 0.43519107 <a title="95-lsi-8" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>9 0.43456945 <a title="95-lsi-9" href="./nips-2011-Simultaneous_Sampling_and_Multi-Structure_Fitting_with_Adaptive_Reversible_Jump_MCMC.html">255 nips-2011-Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC</a></p>
<p>10 0.43401706 <a title="95-lsi-10" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>11 0.43229377 <a title="95-lsi-11" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>12 0.4293901 <a title="95-lsi-12" href="./nips-2011-Bayesian_Partitioning_of_Large-Scale_Distance_Data.html">43 nips-2011-Bayesian Partitioning of Large-Scale Distance Data</a></p>
<p>13 0.4292219 <a title="95-lsi-13" href="./nips-2011-History_distribution_matching_method_for_predicting_effectiveness_of_HIV_combination_therapies.html">120 nips-2011-History distribution matching method for predicting effectiveness of HIV combination therapies</a></p>
<p>14 0.40651762 <a title="95-lsi-14" href="./nips-2011-Heavy-tailed_Distances_for_Gradient_Based_Image_Descriptors.html">112 nips-2011-Heavy-tailed Distances for Gradient Based Image Descriptors</a></p>
<p>15 0.40406778 <a title="95-lsi-15" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>16 0.4034273 <a title="95-lsi-16" href="./nips-2011-Variance_Penalizing_AdaBoost.html">299 nips-2011-Variance Penalizing AdaBoost</a></p>
<p>17 0.40113002 <a title="95-lsi-17" href="./nips-2011-Sparse_Manifold_Clustering_and_Embedding.html">263 nips-2011-Sparse Manifold Clustering and Embedding</a></p>
<p>18 0.39877281 <a title="95-lsi-18" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>19 0.39220074 <a title="95-lsi-19" href="./nips-2011-Active_Learning_Ranking_from_Pairwise_Preferences_with_Almost_Optimal_Query_Complexity.html">20 nips-2011-Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity</a></p>
<p>20 0.38895255 <a title="95-lsi-20" href="./nips-2011-Hogwild%3A_A_Lock-Free_Approach_to_Parallelizing_Stochastic_Gradient_Descent.html">121 nips-2011-Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (4, 0.055), (20, 0.022), (26, 0.035), (28, 0.342), (31, 0.092), (33, 0.02), (43, 0.046), (45, 0.101), (57, 0.066), (74, 0.035), (83, 0.029), (99, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.76627922 <a title="95-lda-1" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, BarnabÃ¡s PÃ³czos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>same-paper 2 0.70518655 <a title="95-lda-2" href="./nips-2011-Fast_and_Accurate_k-means_For_Large_Datasets.html">95 nips-2011-Fast and Accurate k-means For Large Datasets</a></p>
<p>Author: Michael Shindler, Alex Wong, Adam W. Meyerson</p><p>Abstract: Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o( nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms - both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.</p><p>3 0.63718116 <a title="95-lda-3" href="./nips-2011-Projection_onto_A_Nonnegative_Max-Heap.html">226 nips-2011-Projection onto A Nonnegative Max-Heap</a></p>
<p>Author: Jun Liu, Liang Sun, Jieping Ye</p><p>Abstract: We consider the problem of computing the Euclidean projection of a vector of length p onto a non-negative max-heapâan ordered tree where the values of the nodes are all nonnegative and the value of any parent node is no less than the value(s) of its child node(s). This Euclidean projection plays a building block role in the optimization problem with a non-negative maxheap constraint. Such a constraint is desirable when the features follow an ordered tree structure, that is, a given feature is selected for the given regression/classiï¬cation task only if its parent node is selected. In this paper, we show that such Euclidean projection problem admits an analytical solution and we develop a top-down algorithm where the key operation is to ï¬nd the so-called maximal root-tree of the subtree rooted at each node. A naive approach for ï¬nding the maximal root-tree is to enumerate all the possible root-trees, which, however, does not scale well. We reveal several important properties of the maximal root-tree, based on which we design a bottom-up algorithm with merge for eï¬ciently ï¬nding the maximal roottree. The proposed algorithm has a (worst-case) linear time complexity for a sequential list, and O(p2 ) for a general tree. We report simulation results showing the eï¬ectiveness of the max-heap for regression with an ordered tree structure. Empirical results show that the proposed algorithm has an expected linear time complexity for many special cases including a sequential list, a full binary tree, and a tree with depth 1. 1</p><p>4 0.63509184 <a title="95-lda-4" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>Author: Antonio Torralba, Joshua B. Tenenbaum, Ruslan Salakhutdinov</p><p>Abstract: We introduce HD (or âHierarchical-Deepâ) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian models. Speciï¬cally we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a Deep Boltzmann Machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efï¬cient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets. 1</p><p>5 0.52226716 <a title="95-lda-5" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>Author: Shai Shalev-shwartz, Yonatan Wexler, Amnon Shashua</p><p>Abstract: Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sublinearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efï¬ciently ï¬nds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the beneï¬ts of ShareBoost and evaluate its success relatively to other state-of-the-art approaches. 1</p><p>6 0.45936885 <a title="95-lda-6" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>7 0.45718923 <a title="95-lda-7" href="./nips-2011-Convergent_Bounds_on_the_Euclidean_Distance.html">64 nips-2011-Convergent Bounds on the Euclidean Distance</a></p>
<p>8 0.45680201 <a title="95-lda-8" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>9 0.45577666 <a title="95-lda-9" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>10 0.45513472 <a title="95-lda-10" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>11 0.45510191 <a title="95-lda-11" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>12 0.45504987 <a title="95-lda-12" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>13 0.45412719 <a title="95-lda-13" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>14 0.45358446 <a title="95-lda-14" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>15 0.45235828 <a title="95-lda-15" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>16 0.45183963 <a title="95-lda-16" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>17 0.45161551 <a title="95-lda-17" href="./nips-2011-Crowdclustering.html">66 nips-2011-Crowdclustering</a></p>
<p>18 0.45154944 <a title="95-lda-18" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>19 0.45049012 <a title="95-lda-19" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>20 0.44956499 <a title="95-lda-20" href="./nips-2011-Algorithms_for_Hyper-Parameter_Optimization.html">30 nips-2011-Algorithms for Hyper-Parameter Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
