<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-82" href="#">nips2011-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</h1>
<br/><p>Source: <a title="nips-2011-82-pdf" href="http://papers.nips.cc/paper/4384-efficient-coding-of-natural-images-with-a-population-of-noisy-linear-nonlinear-neurons.pdf">pdf</a></p><p>Author: Yan Karklin, Eero P. Simoncelli</p><p>Abstract: Efﬁcient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. The ﬁlters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal ﬁlters are localized and oriented.</p><p>Reference: <a title="nips-2011-82-reference" href="../nips2011_reference/nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Efﬁcient coding of natural images with a population of noisy Linear-Nonlinear neurons  Yan Karklin and Eero P. [sent-1, score-0.582]
</p><p>2 edu  Abstract Efﬁcient coding provides a powerful principle for explaining early sensory coding. [sent-5, score-0.251]
</p><p>3 Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. [sent-6, score-0.306]
</p><p>4 Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. [sent-7, score-1.44]
</p><p>5 Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. [sent-8, score-0.867]
</p><p>6 When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. [sent-9, score-0.348]
</p><p>7 As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. [sent-11, score-0.303]
</p><p>8 A substantial body of work has examined coding efﬁciency of early visual representations. [sent-17, score-0.216]
</p><p>9 For example, the receptive ﬁelds of retinal neurons have been shown to be consistent with efﬁcient coding principles [3, 4, 5, 6]. [sent-18, score-0.779]
</p><p>10 Such approaches have been used to make predictions about neural coding in general [7, 8], and, when combined with a constraint on the mean response level, to derive oriented receptive ﬁelds similar to those found in primary visual cortex [9, 10]. [sent-22, score-0.756]
</p><p>11 , retinal) stages of visual processing, in which receptive ﬁelds are center-surround. [sent-26, score-0.239]
</p><p>12 A number of authors have studied coding efﬁciency of scalar nonlinear functions in the presence of noise and compared them to neural responses to variables such as contrast [11, 12, 13, 14, 15]. [sent-27, score-0.503]
</p><p>13 Others have veriﬁed that the distributions of neural responses are in accordance with predictions of coding efﬁciency [16, 17, 18, 19]. [sent-28, score-0.323]
</p><p>14 To our knowledge, however, no previous result has attempted to jointly optimize the linear receptive ﬁeld and the nonlinear response properties in the presence of realistic levels of input and output noise, and realistic constraints on response levels. [sent-29, score-0.859]
</p><p>15 Here, we develop methods to optimize a full population of linear-nonlinear (LN) model neurons for transmitting information in natural images. [sent-30, score-0.44]
</p><p>16 We include a term in the objective function that captures metabolic costs associated with ﬁring spikes [20, 21, 22]. [sent-31, score-0.286]
</p><p>17 We implement an algorithm for jointly optimizing the population of linear receptive ﬁelds and their associated nonlinearities. [sent-33, score-0.288]
</p><p>18 We ﬁnd that, in the regime of signiﬁcant noise, the optimal ﬁlters have a center-surround form, and the optimal nonlinearities are rectifying, consistent with response properties of retinal ganglion cells. [sent-34, score-0.86]
</p><p>19 We also observe asymmetries between the On- and the Off-center types similar to those measured in retinal populations. [sent-35, score-0.308]
</p><p>20 When both the input and the output noise are sufﬁciently small, our learning algorithm reduces to a generalized form of independent component analysis (ICA), yielding optimal ﬁlters that are localized and oriented, with corresponding smooth nonlinearities. [sent-36, score-0.379]
</p><p>21 2 A model for noisy nonlinear efﬁcient coding We assume a neural model in the form of an LN cascade (Fig. [sent-37, score-0.274]
</p><p>22 1a), which has been successfully ﬁt to neural responses in retina, lateral geniculate nucleus, and primary visual cortex of primate visual systems [e. [sent-38, score-0.459]
</p><p>23 We develop a numerical method to optimize both the linear receptive ﬁelds and the corresponding point nonlinearities so as to maximize the information transmitted about natural images in the presence of input and output noise, as well as metabolic constraints on neural processing. [sent-41, score-1.134]
</p><p>24 The response of a neuron rj is computed by taking an inner product of the (noise-corrupted) input with a linear ﬁlter wj to obtain a generator signal yj (e. [sent-45, score-0.504]
</p><p>25 membrane voltage), which is then passed through neural nonlinearity fj (corresponding to the spike-generating process) and corrupted with additional neural noise, rj = fj (yj ) + nr yj =  T wj  (x + nx ) ,  (1) (2)  (Fig. [sent-47, score-0.723]
</p><p>26 Note that we did not constrain the model to be “complete” (the number of neurons can be smaller or larger than the input dimensionality) and that each neuron can have a different nonlinearity. [sent-49, score-0.412]
</p><p>27 We aim to optimize an objective function that includes the mutual information between the input signal and the population responses, denoted I(X; R), as well as an approximate measure of the metabolic operating cost of the system. [sent-50, score-0.529]
</p><p>28 It has been estimated that most of the energy expended by spiking neurons is associated with the cost of generating (and recovering from) spikes and that this cost is roughly proportional to the neural ﬁring rate [22]. [sent-51, score-0.494]
</p><p>29 The goal is to maximize information transfer between images x and the neural response r, subject to metabolic cost of ﬁring spikes. [sent-55, score-0.479]
</p><p>30 two pixels of an image, x1 and x2 ) with linear ﬁlters (black lines) whose output is passed through scalar nonlinear functions (thick color lines; thin color lines show isoresponse contours at evenly spaced output levels). [sent-60, score-0.345]
</p><p>31 The steepness of the nonlinearities speciﬁes the precision with which each projection is represented: regions of steep slope correspond to ﬁner partitioning of the input space, reducing the uncertainty about the input. [sent-61, score-0.446]
</p><p>32 Efﬁcient codes optimize this binning, subject to input distribution, noise levels, and metabolic costs on the outputs. [sent-64, score-0.472]
</p><p>33 Our goal is to adjust both the ﬁlters and the nonlinearities of the neural population so as to maximize the expectation of (3) under the joint distribution of inputs and outputs, p(x, r). [sent-68, score-0.595]
</p><p>34 We assume the ﬁlters are unit norm ( wj = 1) to avoid an underdetermined model in which the nonlinearity scales along its input dimension to compensate for ﬁlter ampliﬁcation. [sent-69, score-0.234]
</p><p>35 The nonlinearities fj are assumed to be monotonically increasing. [sent-70, score-0.356]
</p><p>36 We parameterized the slope of the nonlinearity gj = dfj /dyj using a weighted sum of Gaussian kernels, K  gj (yj |cjk , µjk , σj ) =  cjk exp − k=1  (yj − µjk )2 2 2σj  ,  (4)  with coefﬁcients cjk ≥ 0. [sent-71, score-0.378]
</p><p>37 1 Computing mutual information How can we compute the information transmitted by the nonlinear network of neurons? [sent-75, score-0.21]
</p><p>38 The second term is the conditional differential entropy and represents the uncertainty in the input after observing the neural response. [sent-80, score-0.266]
</p><p>39 First, we assume the nonlinearity is smooth enough that, at the level of the noise (both input and output), fj can be linearized using ﬁrst-order Taylor series expansion. [sent-86, score-0.385]
</p><p>40 (Similar approximations have been used to minimize reconstruction error in neural nonlinearities [27] and maximize information in networks of interacting genes [28]. [sent-89, score-0.42]
</p><p>41 )  If input and output noises are assumed to be constant and Gaussian, with covariances Cnx and Cnr , respectively, we obtain a Gaussian likelihood p(r|x), with covariance Ci = Gi WT Cnx WGi + Cnr . [sent-90, score-0.213]
</p><p>42 r|x  (6)  We emphasize that although the likelihood locally takes the form of a Gaussian distribution, its covariance is not ﬁxed but depends on the input, leading to different values for the entropy of the posterior across the input space. [sent-91, score-0.236]
</p><p>43 1b illustrates schematically how the organization of the ﬁlters and the nonlinearities affects the entropy and thus determines the precision with which neurons encode the inputs. [sent-93, score-0.674]
</p><p>44 x|r 2  (8)  We obtain Monte Carlo estimates of this conditional entropy by averaging the term in the brackets over a large ensemble of patches drawn from natural images and input/output noise sampled from assumed noise distributions. [sent-104, score-0.519]
</p><p>45 One important special case is derived when the number of inputs equals the number of outputs, and both noise levels approach zero. [sent-108, score-0.265]
</p><p>46 In this setting, the update rule for the ﬁlters reduces to the ICA learning rule [8], with the gradient updates maximizing the entropy of the output distributions. [sent-109, score-0.21]
</p><p>47 Because our response constraint effectively limits the mean ﬁring rate and not the maximum, the anti-Hebbian term is different from that found in standard ICA, and the optimal (maximum entropy) response distributions are exponential, rather than uniform. [sent-110, score-0.248]
</p><p>48 Note also that our method is more general than standard ICA: it adaptively adjusts the nonlinearities to match the input distribution, whereas standard ICA relies on a ﬁxed nonlinear “contrast” function. [sent-111, score-0.477]
</p><p>49 To ensure all nonlinearities were monotonically increasing, the coefﬁcients cjk were adapted in log-space. [sent-112, score-0.407]
</p><p>50 It was also 4  necessary to adjust the sampling of the nonlinearities (location of µjk ’s) because, as the ﬁxed-norm ﬁlters rotated through input space, the variance of the projections can change drastically. [sent-114, score-0.4]
</p><p>51 2 2 We assumed that the input and output noises were i. [sent-123, score-0.213]
</p><p>52 Output noise levels were set to -6dB (computed as 20 log10 ( rj /σnr ); σnr = 2) in order to match the high variability observed in retinal ganglion cells (see below). [sent-130, score-0.774]
</p><p>53 Parameter λj was adjusted to attain an average rate of one spike per neuron per input image, rj = 1. [sent-131, score-0.307]
</p><p>54 In the retina, the ratio of inputs (cones) to outputs (retinal ganglion cells) varies greatly, from almost 1:3 in central fovea to more than 10:1 in the periphery [30]. [sent-134, score-0.216]
</p><p>55 2 Optimal ﬁlters and nonlinearities We found that, in the presence of signiﬁcant input and output noise, the optimal ﬁlters have centersurround structure, rather than the previously reported oriented shapes (Fig. [sent-142, score-0.642]
</p><p>56 The population contains fewer On-center neurons (41 of 100) and their ﬁlters are spatially larger (Fig. [sent-145, score-0.374]
</p><p>57 These results are consistent with measurements of receptive ﬁeld structure in retinal ganglion cells [31] (Fig. [sent-147, score-0.643]
</p><p>58 Measured neural nonlinearities are typically softer, but when rectiﬁed noise is taken into account, a hard-rectiﬁed model has been shown to be a good description of neural variability [32]. [sent-151, score-0.588]
</p><p>59 The combination of hard-rectifying nonlinearities and On/Off ﬁlter organization means that the subspace encoded by model neurons is approximately half the dimensionality of the output. [sent-152, score-0.557]
</p><p>60 For substantial levels of noise, we ﬁnd that even a “complete” network (in which the number of outputs equals the number of inputs) does not span the input space and instead encodes the subspace with highest signal power. [sent-153, score-0.199]
</p><p>61 The metabolic cost parameters λj that yielded the target output rate were close to 0. [sent-154, score-0.301]
</p><p>62 This means that increasing the ﬁring rate of each neuron by one spike per image leads to an information gain of 20 bits for the entire population. [sent-156, score-0.225]
</p><p>63 To examine the effect of noise on optimal representations, we trained the model under different regimes of noise (Fig. [sent-158, score-0.29]
</p><p>64 The learned nonlinearities for the ﬁrst four model neurons, superimposed on distributions of ﬁlter outputs. [sent-166, score-0.315]
</p><p>65 A characterization of two retinal ganglion cells obtained with white noise stimulus [31]. [sent-168, score-0.673]
</p><p>66 in the number of On-center neurons (bottom left panel). [sent-173, score-0.242]
</p><p>67 In this case, increasing the number of neurons restored the balance of On- and Off-center ﬁlters (not shown). [sent-174, score-0.242]
</p><p>68 In the case of vanishing input and output noise, we obtain localized oriented ﬁlters (top left panel), and the nonlinearities are smoothly accelerating functions that map inputs to an exponential output distribution (not shown). [sent-175, score-0.755]
</p><p>69 These results are consistent with previous theoretical work showing that optimal nonlinearity in the low noise regime maximizes the entropy of the output subject to response constraints [11, 7, 17]. [sent-176, score-0.593]
</p><p>70 As a rough estimate of input entropy H(X), we used an upper bound – a Gaussian distribution with the covariance of natural images. [sent-181, score-0.235]
</p><p>71 First, we ﬁxed the allotted population spike budget to 100 (per input), ﬁxed the absolute output noise, and 6  σnx = 0. [sent-185, score-0.363]
</p><p>72 40 (8dB)  σnr = 2 (−6dB)  ← output noise  input noise →  Figure 4: Each panel shows a subset of ﬁlters (20 of 100) obtained under different levels of input and output noise, as well as the nonlinearity for a typical neuron in each model. [sent-189, score-0.922]
</p><p>73 45 W2  W3  40 MI (bits)  W1  35 30  1  25 20 2  3  15  80  100 120 total spikes  Figure 5: Information transmitted as a function of spike rate, under noisy conditions (8dB SNRin , −6dB SNRout ). [sent-190, score-0.248]
</p><p>74 varied the number of neurons from 1 (very precise) neuron to 150 (fairly noisy) neurons (Fig. [sent-194, score-0.569]
</p><p>75 In this regime of noise and spiking budget, the optimal population size was around 100 neurons. [sent-197, score-0.318]
</p><p>76 Next, we repeated the analysis but used neurons with ﬁxed precision, i. [sent-198, score-0.242]
</p><p>77 , the spike budget was scaled with the population to give 1 noisy neuron or 150 equally noisy neurons (Fig. [sent-200, score-0.597]
</p><p>78 This suggests that incorporating an additional penalty, such as a ﬁxed metabolic cost per neuron, would allow us to predict the optimal number of canonical noisy neurons. [sent-203, score-0.208]
</p><p>79 4 Discussion We have described an efﬁcient coding model that incorporates ingredients essential for computation in sensory systems: non-Gaussian signal distributions, realistic levels of input and output noise, metabolic costs, nonlinear responses, and a large population of neurons. [sent-204, score-1.014]
</p><p>80 The resulting optimal solution mimics neural behaviors observed in the retina: a combination of On and Off center-surround receptive ﬁelds, halfwave-rectiﬁed nonlinear responses, and pronounced asymmetries between the On- and the Off- populations. [sent-205, score-0.357]
</p><p>81 Our results bear some resemblance to previous attempts to derive retinal properties as optimal solutions. [sent-215, score-0.248]
</p><p>82 Most notably, optimal linear transforms that optimize information transmission under a constraint on total response power have been shown to be consistent with center-surround [4] and more detailed [34] shapes of retinal receptive ﬁelds. [sent-216, score-0.676]
</p><p>83 In contrast, neural systems must deal with changing levels of noise and signal, and must estimate them based only on their inputs. [sent-222, score-0.286]
</p><p>84 Instead, we intend to extend this framework to cortical representations that must deal with accumulated nonlinearity and noise arising from previous stages of the processing hierarchy. [sent-228, score-0.296]
</p><p>85 Lewicki, “A theory of retinal population coding,” in Advances in Neural Information Processing Systems 19 (B. [sent-267, score-0.38]
</p><p>86 Parga, “Nonlinear neurons in the low-noise limit: a factorial code maximizes information transfer,” Network: Computation in Neural Systems, vol. [sent-275, score-0.242]
</p><p>87 Wakeman, “Firing rate distributions and efﬁciency of information transmission of inferior temporal cortex neurons to natural visual stimuli,” Neural Computation, vol. [sent-313, score-0.488]
</p><p>88 Stocks, “Maximally informative stimuli and tuning curves for sigmoidal Rate-Coding neurons and populations,” Physical Review Letters, vol. [sent-340, score-0.242]
</p><p>89 Rolls, “Responses of neurons in primary and inferior temporal visual cortices to natural scenes. [sent-363, score-0.394]
</p><p>90 Jerez, “Neuronal selectivity, population sparseness, and ergodicity in the inferior temporal visual cortex,” Biol. [sent-382, score-0.215]
</p><p>91 Meister, “Decoding visual information from a population of retinal ganglion cells,” Journal of Neurophysiology, vol. [sent-415, score-0.636]
</p><p>92 Chichilnisky, “Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model,” The Journal of Neuroscience, vol. [sent-428, score-0.546]
</p><p>93 Tishby, “Efﬁcient representation as a design principle for neural coding and computation,” in IEEE International Symposium on Information Theory, pp. [sent-441, score-0.197]
</p><p>94 Sejnowski, “Spatiochromatic receptive ﬁeld properties derived from information-theoretic analyses of cone mosaic responses to natural scenes,” Neural Computation, vol. [sent-468, score-0.273]
</p><p>95 Boycott, “Retinal ganglion cell density and cortical a u o magniﬁcation factor in the primate,” Vision Research, vol. [sent-476, score-0.21]
</p><p>96 Kalmar, “Functional asymmetries in ON and OFF ganglion cells of primate retina,” The Journal of Neuroscience, vol. [sent-484, score-0.36]
</p><p>97 Carandini, “Ampliﬁcation of Trial-to-Trial response variability by neurons in visual cortex,” PLoS Biol, vol. [sent-489, score-0.449]
</p><p>98 Troy, “Information transmission rates of cat retinal ganglion cells,” Journal of Neurophysiology, vol. [sent-497, score-0.503]
</p><p>99 Simoncelli, “Redundant representations in macaque retinal populations are consistent with efﬁcient coding,” in Computational and Systems Neuroscience (CoSyNe), February 2011. [sent-519, score-0.293]
</p><p>100 Baddeley, “Synaptic energy efﬁciency in retinal processing,” Vision Research, vol. [sent-524, score-0.248]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lters', 0.376), ('nonlinearities', 0.315), ('retinal', 0.248), ('neurons', 0.242), ('ganglion', 0.173), ('metabolic', 0.173), ('receptive', 0.156), ('noise', 0.145), ('coding', 0.133), ('population', 0.132), ('response', 0.124), ('sensory', 0.118), ('entropy', 0.117), ('nonlinearity', 0.114), ('transmitted', 0.099), ('nx', 0.099), ('retina', 0.099), ('ica', 0.099), ('output', 0.093), ('cjk', 0.092), ('cnr', 0.092), ('cnx', 0.092), ('nr', 0.091), ('ring', 0.085), ('input', 0.085), ('neuron', 0.085), ('responses', 0.084), ('visual', 0.083), ('transmission', 0.082), ('laughlin', 0.081), ('nonlinear', 0.077), ('levels', 0.077), ('spikes', 0.077), ('steveninck', 0.074), ('yj', 0.073), ('spike', 0.072), ('oriented', 0.07), ('wgi', 0.069), ('bits', 0.068), ('cells', 0.066), ('budget', 0.066), ('rj', 0.065), ('neural', 0.064), ('bialek', 0.063), ('primate', 0.061), ('asymmetries', 0.06), ('elds', 0.059), ('localized', 0.056), ('rolls', 0.052), ('gi', 0.052), ('ciency', 0.05), ('curvature', 0.048), ('cortex', 0.048), ('sejnowski', 0.047), ('attneave', 0.046), ('baddeley', 0.046), ('binning', 0.046), ('booth', 0.046), ('centersurround', 0.046), ('isoresponse', 0.046), ('snrin', 0.046), ('snrout', 0.046), ('steepness', 0.046), ('wakeman', 0.046), ('chichilnisky', 0.045), ('doi', 0.045), ('jk', 0.045), ('realistic', 0.045), ('populations', 0.045), ('ingredients', 0.044), ('inputs', 0.043), ('simoncelli', 0.042), ('images', 0.042), ('predictions', 0.042), ('ln', 0.042), ('stimulus', 0.041), ('fj', 0.041), ('spiking', 0.041), ('maximize', 0.041), ('atick', 0.04), ('conveyed', 0.04), ('barlow', 0.04), ('gj', 0.04), ('cortical', 0.037), ('signal', 0.037), ('patches', 0.037), ('lter', 0.036), ('passed', 0.036), ('biologically', 0.036), ('primary', 0.036), ('costs', 0.036), ('cost', 0.035), ('noises', 0.035), ('wj', 0.035), ('mutual', 0.034), ('ascent', 0.034), ('posterior', 0.034), ('optimize', 0.033), ('shapes', 0.033), ('natural', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="82-tfidf-1" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>Author: Yan Karklin, Eero P. Simoncelli</p><p>Abstract: Efﬁcient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. The ﬁlters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal ﬁlters are localized and oriented.</p><p>2 0.24409874 <a title="82-tfidf-2" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>Author: Maneesh Bhand, Ritvik Mudur, Bipin Suresh, Andrew Saxe, Andrew Y. Ng</p><p>Abstract: The efﬁcient coding hypothesis holds that neural receptive ﬁelds are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism’s lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive ﬁeld properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive ﬁeld properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive ﬁeld plasticity during an organism’s lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, ﬁrst proposed by Mountcastle [1], that a qualitatively similar learning algorithm acts throughout primary sensory cortices. 1</p><p>3 0.21472606 <a title="82-tfidf-3" href="./nips-2011-Bayesian_Spike-Triggered_Covariance_Analysis.html">44 nips-2011-Bayesian Spike-Triggered Covariance Analysis</a></p>
<p>Author: Jonathan W. Pillow, Il M. Park</p><p>Abstract: Neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli. Here we describe an explicit modelbased interpretation of traditional estimators for a neuron’s multi-dimensional feature space, which allows for several important generalizations and extensions. First, we show that traditional estimators based on the spike-triggered average (STA) and spike-triggered covariance (STC) can be formalized in terms of the “expected log-likelihood” of a Linear-Nonlinear-Poisson (LNP) model with Gaussian stimuli. This model-based formulation allows us to deﬁne maximum-likelihood and Bayesian estimators that are statistically consistent and efﬁcient in a wider variety of settings, such as with naturalistic (non-Gaussian) stimuli. It also allows us to employ Bayesian methods for regularization, smoothing, sparsiﬁcation, and model comparison, and provides Bayesian conﬁdence intervals on model parameters. We describe an empirical Bayes method for selecting the number of features, and extend the model to accommodate an arbitrary elliptical nonlinear response function, which results in a more powerful and more ﬂexible model for feature space inference. We validate these methods using neural data recorded extracellularly from macaque primary visual cortex. 1</p><p>4 0.21213241 <a title="82-tfidf-4" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>Author: Kamiar R. Rad, Liam Paninski</p><p>Abstract: Many fundamental questions in theoretical neuroscience involve optimal decoding and the computation of Shannon information rates in populations of spiking neurons. In this paper, we apply methods from the asymptotic theory of statistical inference to obtain a clearer analytical understanding of these quantities. We ﬁnd that for large neural populations carrying a ﬁnite total amount of information, the full spiking population response is asymptotically as informative as a single observation from a Gaussian process whose mean and covariance can be characterized explicitly in terms of network and single neuron properties. The Gaussian form of this asymptotic sufﬁcient statistic allows us in certain cases to perform optimal Bayesian decoding by simple linear transformations, and to obtain closed-form expressions of the Shannon information carried by the network. One technical advantage of the theory is that it may be applied easily even to non-Poisson point process network models; for example, we ﬁnd that under some conditions, neural populations with strong history-dependent (non-Poisson) effects carry exactly the same information as do simpler equivalent populations of non-interacting Poisson neurons with matched ﬁring rates. We argue that our ﬁndings help to clarify some results from the recent literature on neural decoding and neuroprosthetic design.</p><p>5 0.20763142 <a title="82-tfidf-5" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>Author: Adam Coates, Andrew Y. Ng</p><p>Abstract: Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded “local receptive ﬁelds” that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Speciﬁcally, we choose local receptive ﬁelds that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive ﬁelds (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive ﬁelds by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered networks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively. 1</p><p>6 0.20259073 <a title="82-tfidf-6" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>7 0.18688433 <a title="82-tfidf-7" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>8 0.18360548 <a title="82-tfidf-8" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>9 0.1791485 <a title="82-tfidf-9" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>10 0.17475 <a title="82-tfidf-10" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>11 0.16881472 <a title="82-tfidf-11" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>12 0.15231669 <a title="82-tfidf-12" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>13 0.15140504 <a title="82-tfidf-13" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>14 0.15047453 <a title="82-tfidf-14" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>15 0.14643173 <a title="82-tfidf-15" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>16 0.14156607 <a title="82-tfidf-16" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>17 0.1364769 <a title="82-tfidf-17" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>18 0.13306648 <a title="82-tfidf-18" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>19 0.13000272 <a title="82-tfidf-19" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>20 0.12855658 <a title="82-tfidf-20" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.258), (1, 0.193), (2, 0.343), (3, -0.002), (4, 0.127), (5, 0.152), (6, 0.033), (7, 0.152), (8, -0.023), (9, -0.087), (10, -0.055), (11, -0.015), (12, 0.038), (13, -0.037), (14, 0.073), (15, 0.09), (16, 0.061), (17, -0.052), (18, 0.024), (19, -0.012), (20, -0.135), (21, -0.117), (22, 0.069), (23, -0.025), (24, 0.004), (25, 0.043), (26, -0.011), (27, 0.061), (28, 0.035), (29, -0.056), (30, 0.036), (31, 0.044), (32, 0.011), (33, -0.015), (34, -0.032), (35, -0.066), (36, 0.069), (37, -0.005), (38, -0.006), (39, -0.037), (40, -0.03), (41, -0.041), (42, 0.015), (43, -0.057), (44, -0.001), (45, -0.028), (46, -0.03), (47, -0.032), (48, -0.005), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96172792 <a title="82-lsi-1" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>Author: Yan Karklin, Eero P. Simoncelli</p><p>Abstract: Efﬁcient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. The ﬁlters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal ﬁlters are localized and oriented.</p><p>2 0.82963574 <a title="82-lsi-2" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>Author: Maneesh Bhand, Ritvik Mudur, Bipin Suresh, Andrew Saxe, Andrew Y. Ng</p><p>Abstract: The efﬁcient coding hypothesis holds that neural receptive ﬁelds are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales. In this work we focus on that component of adaptation which occurs during an organism’s lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive ﬁeld properties across multiple primary sensory cortices. Furthermore, we show that the same algorithms account for altered receptive ﬁeld properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as phenomenological models of receptive ﬁeld plasticity during an organism’s lifetime. Finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, ﬁrst proposed by Mountcastle [1], that a qualitatively similar learning algorithm acts throughout primary sensory cortices. 1</p><p>3 0.77203953 <a title="82-lsi-3" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>Author: Alyson K. Fletcher, Sundeep Rangan, Lav R. Varshney, Aniruddha Bhargava</p><p>Abstract: Many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear ﬁltering stage that produces a lowdimensional signal that drives subsequent nonlinear stages. This paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive ﬁelds in sensory neurons. The proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (GAMP) method. The GAMP method is based on Gaussian approximations of loopy belief propagation. In the neural connectivity problem, the GAMP-based method is shown to be computational efﬁcient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and signiﬁcantly outperforms previous compressed-sensing methods. For the receptive ﬁeld estimation, the GAMP method can also exploit inherent structured sparsity in the linear weights. The method is validated on estimation of linear nonlinear Poisson (LNP) cascade models for receptive ﬁelds of salamander retinal ganglion cells. 1</p><p>4 0.7526713 <a title="82-lsi-4" href="./nips-2011-Bayesian_Spike-Triggered_Covariance_Analysis.html">44 nips-2011-Bayesian Spike-Triggered Covariance Analysis</a></p>
<p>Author: Jonathan W. Pillow, Il M. Park</p><p>Abstract: Neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli. Here we describe an explicit modelbased interpretation of traditional estimators for a neuron’s multi-dimensional feature space, which allows for several important generalizations and extensions. First, we show that traditional estimators based on the spike-triggered average (STA) and spike-triggered covariance (STC) can be formalized in terms of the “expected log-likelihood” of a Linear-Nonlinear-Poisson (LNP) model with Gaussian stimuli. This model-based formulation allows us to deﬁne maximum-likelihood and Bayesian estimators that are statistically consistent and efﬁcient in a wider variety of settings, such as with naturalistic (non-Gaussian) stimuli. It also allows us to employ Bayesian methods for regularization, smoothing, sparsiﬁcation, and model comparison, and provides Bayesian conﬁdence intervals on model parameters. We describe an empirical Bayes method for selecting the number of features, and extend the model to accommodate an arbitrary elliptical nonlinear response function, which results in a more powerful and more ﬂexible model for feature space inference. We validate these methods using neural data recorded extracellularly from macaque primary visual cortex. 1</p><p>5 0.70673805 <a title="82-lsi-5" href="./nips-2011-Probabilistic_Modeling_of_Dependencies_Among_Visual_Short-Term_Memory_Representations.html">224 nips-2011-Probabilistic Modeling of Dependencies Among Visual Short-Term Memory Representations</a></p>
<p>Author: Emin Orhan, Robert A. Jacobs</p><p>Abstract: Extensive evidence suggests that items are not encoded independently in visual short-term memory (VSTM). However, previous research has not quantitatively considered how the encoding of an item inﬂuences the encoding of other items. Here, we model the dependencies among VSTM representations using a multivariate Gaussian distribution with a stimulus-dependent mean and covariance matrix. We report the results of an experiment designed to determine the speciﬁc form of the stimulus-dependence of the mean and the covariance matrix. We ﬁnd that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items’ feature values, similar to a Gaussian process with a distance-dependent, stationary kernel function. We further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses. 1</p><p>6 0.68042004 <a title="82-lsi-6" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>7 0.63194567 <a title="82-lsi-7" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>8 0.62776285 <a title="82-lsi-8" href="./nips-2011-Emergence_of_Multiplication_in_a_Biophysical_Model_of_a_Wide-Field_Visual_Neuron_for_Computing_Object_Approaches%3A_Dynamics%2C_Peaks%2C_%26_Fits.html">85 nips-2011-Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches: Dynamics, Peaks, & Fits</a></p>
<p>9 0.62381655 <a title="82-lsi-9" href="./nips-2011-A_Brain-Machine_Interface_Operating_with_a_Real-Time_Spiking_Neural_Network_Control_Algorithm.html">2 nips-2011-A Brain-Machine Interface Operating with a Real-Time Spiking Neural Network Control Algorithm</a></p>
<p>10 0.6021986 <a title="82-lsi-10" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>11 0.59347069 <a title="82-lsi-11" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>12 0.58976912 <a title="82-lsi-12" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>13 0.5861395 <a title="82-lsi-13" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>14 0.58486629 <a title="82-lsi-14" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>15 0.58387905 <a title="82-lsi-15" href="./nips-2011-ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.html">124 nips-2011-ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning</a></p>
<p>16 0.55522829 <a title="82-lsi-16" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>17 0.54232025 <a title="82-lsi-17" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>18 0.51176822 <a title="82-lsi-18" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>19 0.509655 <a title="82-lsi-19" href="./nips-2011-How_biased_are_maximum_entropy_models%3F.html">123 nips-2011-How biased are maximum entropy models?</a></p>
<p>20 0.49655369 <a title="82-lsi-20" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.02), (4, 0.031), (20, 0.027), (26, 0.023), (31, 0.092), (33, 0.017), (39, 0.224), (43, 0.116), (45, 0.077), (57, 0.064), (65, 0.066), (74, 0.049), (83, 0.084), (84, 0.021), (99, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83134675 <a title="82-lda-1" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>Author: Yan Karklin, Eero P. Simoncelli</p><p>Abstract: Efﬁcient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. The ﬁlters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal ﬁlters are localized and oriented.</p><p>2 0.79411322 <a title="82-lda-2" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>3 0.7880863 <a title="82-lda-3" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>Author: Yi-kai Liu</p><p>Abstract: We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefﬁcients. We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rankr restricted isometry property (RIP). This implies that M can be recovered from a ﬁxed (“universal”) set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley’s inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality. 1</p><p>4 0.72367352 <a title="82-lda-4" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>Author: Kristen Grauman, Fei Sha, Sung J. Hwang</p><p>Abstract: We introduce an approach to learn discriminative visual representations while exploiting external semantic knowledge about object category relationships. Given a hierarchical taxonomy that captures semantic similarity between the objects, we learn a corresponding tree of metrics (ToM). In this tree, we have one metric for each non-leaf node of the object hierarchy, and each metric is responsible for discriminating among its immediate subcategory children. Speciﬁcally, a Mahalanobis metric learned for a given node must satisfy the appropriate (dis)similarity constraints generated only among its subtree members’ training instances. To further exploit the semantics, we introduce a novel regularizer coupling the metrics that prefers a sparse disjoint set of features to be selected for each metric relative to its ancestor (supercategory) nodes’ metrics. Intuitively, this reﬂects that visual cues most useful to distinguish the generic classes (e.g., feline vs. canine) should be different than those cues most useful to distinguish their component ﬁne-grained classes (e.g., Persian cat vs. Siamese cat). We validate our approach with multiple image datasets using the WordNet taxonomy, show its advantages over alternative metric learning approaches, and analyze the meaning of attribute features selected by our algorithm. 1</p><p>5 0.67253149 <a title="82-lda-5" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>Author: Alyson K. Fletcher, Sundeep Rangan, Lav R. Varshney, Aniruddha Bhargava</p><p>Abstract: Many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear ﬁltering stage that produces a lowdimensional signal that drives subsequent nonlinear stages. This paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive ﬁelds in sensory neurons. The proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (GAMP) method. The GAMP method is based on Gaussian approximations of loopy belief propagation. In the neural connectivity problem, the GAMP-based method is shown to be computational efﬁcient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and signiﬁcantly outperforms previous compressed-sensing methods. For the receptive ﬁeld estimation, the GAMP method can also exploit inherent structured sparsity in the linear weights. The method is validated on estimation of linear nonlinear Poisson (LNP) cascade models for receptive ﬁelds of salamander retinal ganglion cells. 1</p><p>6 0.66572195 <a title="82-lda-6" href="./nips-2011-Information_Rates_and_Optimal_Decoding_in_Large_Neural_Populations.html">135 nips-2011-Information Rates and Optimal Decoding in Large Neural Populations</a></p>
<p>7 0.6516611 <a title="82-lda-7" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>8 0.64972234 <a title="82-lda-8" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>9 0.63409841 <a title="82-lda-9" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>10 0.63393342 <a title="82-lda-10" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>11 0.63299924 <a title="82-lda-11" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>12 0.63259953 <a title="82-lda-12" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>13 0.62619758 <a title="82-lda-13" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>14 0.62526655 <a title="82-lda-14" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>15 0.62335056 <a title="82-lda-15" href="./nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</a></p>
<p>16 0.62233591 <a title="82-lda-16" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>17 0.62028396 <a title="82-lda-17" href="./nips-2011-Evaluating_the_inverse_decision-making_approach_to_preference_learning.html">90 nips-2011-Evaluating the inverse decision-making approach to preference learning</a></p>
<p>18 0.61715102 <a title="82-lda-18" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>19 0.61630297 <a title="82-lda-19" href="./nips-2011-Kernel_Embeddings_of_Latent_Tree_Graphical_Models.html">140 nips-2011-Kernel Embeddings of Latent Tree Graphical Models</a></p>
<p>20 0.61402178 <a title="82-lda-20" href="./nips-2011-Spectral_Methods_for_Learning_Multivariate_Latent_Tree_Structure.html">267 nips-2011-Spectral Methods for Learning Multivariate Latent Tree Structure</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
