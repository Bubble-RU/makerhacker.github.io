<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2011-Improving Topic Coherence with Regularized Topic Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-129" href="#">nips2011-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2011-Improving Topic Coherence with Regularized Topic Models</h1>
<br/><p>Source: <a title="nips-2011-129-pdf" href="http://papers.nips.cc/paper/4291-improving-topic-coherence-with-regularized-topic-models.pdf">pdf</a></p><p>Author: David Newman, Edwin V. Bonilla, Wray Buntine</p><p>Abstract: Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reﬂect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data. 1</p><p>Reference: <a title="nips-2011-129-reference" href="../nips2011_reference/nips-2011-Improving_Topic_Coherence_with_Regularized_Topic_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. [sent-7, score-0.796]
</p><p>2 When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. [sent-8, score-0.908]
</p><p>3 However, when dealing with small collections or noisy text (e. [sent-9, score-0.393]
</p><p>4 web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. [sent-11, score-0.845]
</p><p>5 To overcome this, we propose two methods to regularize the learning of topic models. [sent-12, score-0.634]
</p><p>6 Our regularizers work by creating a structured prior over words that reﬂect broad patterns in the external data. [sent-13, score-0.329]
</p><p>7 Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. [sent-14, score-1.358]
</p><p>8 Overall, this work makes topic models more useful across a broader range of text data. [sent-15, score-0.756]
</p><p>9 1  Introduction  Topic modeling holds much promise for improving the ways users search, discover, and organize online content by automatically extracting semantic themes from collections of text documents. [sent-16, score-0.754]
</p><p>10 Learned topics can be useful in user interfaces for ad-hoc document retrieval [18]; driving faceted browsing [14]; clustering search results [19]; or improving display of search results by increasing result diversity [10]. [sent-17, score-1.215]
</p><p>11 When the text being modeled is plentiful, clear and well written (e. [sent-18, score-0.18]
</p><p>12 large collections of abstracts from scientiﬁc literature), learned topics are usually coherent, easily understood, and ﬁt for use in user interfaces. [sent-20, score-0.64]
</p><p>13 However, topics are not always consistently coherent, and even with relatively well written text, one can learn topics that are a mix of concepts or hard to understand [1, 6]. [sent-21, score-0.775]
</p><p>14 This problem is exacerbated for content that is sparse or noisy, such as blog posts, tweets, or web search result snippets. [sent-22, score-0.436]
</p><p>15 Take for instance the task of learning categories in clustering search engine results. [sent-23, score-0.169]
</p><p>16 A few searches with Carrot2, Yippee, or WebClust quickly demonstrate that consistently learning meaningful topic facets is a difﬁcult task [5]. [sent-24, score-0.642]
</p><p>17 Our goal in this paper is to improve the coherence, interpretability and ultimate usability of learned topics. [sent-25, score-0.348]
</p><p>18 To achieve this we propose Q UAD -R EG and C ONV-R EG, two new methods for regularizing topic models, which produce more coherent and interpretable topics. [sent-26, score-0.918]
</p><p>19 Our work is predicated on recent evidence that a pointwise mutual information-based score (PMI-Score) is highly correlated with human-judged topic coherence [15, 16]. [sent-27, score-1.136]
</p><p>20 We develop two Bayesian regularization formulations that are designed to improve PMI-Score. [sent-28, score-0.094]
</p><p>21 We experiment with ﬁve search result datasets from 7M Blog posts, four search result datasets from 1M News articles, and four datasets of Google search results. [sent-29, score-0.462]
</p><p>22 Using these thirteen datasets, our experiments demonstrate that both regularizers consistently improve topic coherence and interpretability, as measured separately by PMI-Score and human judgements. [sent-30, score-1.307]
</p><p>23 To the best of our knowledge, our models are the ﬁrst to address the problem of learning topics when dealing with limited and/or noisy text content. [sent-31, score-0.601]
</p><p>24 This work opens up new application areas for topic modeling. [sent-32, score-0.584]
</p><p>25 1  2  Topic Coherence and PMI-Score  Topics learned from a statistical topic model are formally a multinomial distribution over words, and are often displayed by printing the 10 most probable words in the topic. [sent-33, score-0.874]
</p><p>26 These top-10 words usually provide sufﬁcient information to determine the subject area and interpretation of a topic, and distinguish one topic from another. [sent-34, score-0.682]
</p><p>27 However, topics learned on sparse or noisy text data are often less coherent, difﬁcult to interpret, and not particularly useful. [sent-35, score-0.676]
</p><p>28 Some of these noisy topics can be vaguely interpretable, but contain (in the top-10 words) one or two unrelated words – while other topics can be practically incoherent. [sent-36, score-0.898]
</p><p>29 In this paper we wish to improve topic models learned on document collections where the text data is sparse and/or noisy. [sent-37, score-1.06]
</p><p>30 We postulate that using additional (possibly external) data will regularize the learning of the topic models. [sent-38, score-0.658]
</p><p>31 Topic coherence – meaning semantic coherence – is a human judged quality that depends on the semantics of the words, and cannot be measured by model-based statistical measures that treat the words as exchangeable tokens. [sent-40, score-1.092]
</p><p>32 Fortunately, recent work has demonstrated that it is possible to automatically measure topic coherence with near-human accuracy [16, 15] using a score based on pointwise mutual information (PMI). [sent-41, score-1.117]
</p><p>33 In that work they showed (using 6000 human evaluations) that the PMI-Score broadly agrees with human-judged topic coherence. [sent-42, score-0.657]
</p><p>34 The PMI-Score is motivated by measuring word association between all pairs of words in the top-10 topic words. [sent-43, score-0.705]
</p><p>35 PMI-Score is deﬁned as follows: 1 PMI-Score(w) = PMI(wi , wj ), ij ∈ {1 . [sent-44, score-0.024]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('topic', 0.546), ('coherence', 0.364), ('topics', 0.327), ('coherent', 0.21), ('posts', 0.168), ('browsing', 0.154), ('text', 0.153), ('blog', 0.144), ('faceted', 0.127), ('pmi', 0.127), ('interpretable', 0.126), ('collections', 0.119), ('words', 0.112), ('themes', 0.112), ('thirteen', 0.112), ('regularizers', 0.111), ('interpretability', 0.1), ('search', 0.097), ('pointwise', 0.091), ('eg', 0.091), ('newman', 0.087), ('learned', 0.082), ('web', 0.069), ('improve', 0.068), ('document', 0.068), ('diversity', 0.067), ('semantic', 0.067), ('noisy', 0.065), ('regularize', 0.064), ('consistently', 0.063), ('datasets', 0.057), ('usability', 0.056), ('edwin', 0.056), ('buntine', 0.056), ('predicated', 0.056), ('dealing', 0.056), ('improving', 0.055), ('content', 0.054), ('external', 0.052), ('extracting', 0.052), ('snippets', 0.051), ('printing', 0.048), ('abstracts', 0.048), ('nicta', 0.048), ('exacerbated', 0.048), ('postulate', 0.048), ('mutual', 0.048), ('judged', 0.046), ('bonilla', 0.046), ('interfaces', 0.044), ('engine', 0.044), ('human', 0.043), ('ultimate', 0.042), ('organize', 0.04), ('user', 0.04), ('articles', 0.039), ('promise', 0.038), ('unrelated', 0.038), ('irvine', 0.038), ('australian', 0.038), ('opens', 0.038), ('automatically', 0.037), ('exchangeable', 0.037), ('regularizing', 0.036), ('broadly', 0.036), ('semantics', 0.035), ('probable', 0.034), ('driving', 0.034), ('searches', 0.033), ('broader', 0.033), ('agrees', 0.032), ('fortunately', 0.031), ('score', 0.031), ('mix', 0.031), ('discover', 0.03), ('creating', 0.029), ('practically', 0.029), ('news', 0.029), ('clustering', 0.028), ('google', 0.028), ('display', 0.028), ('multinomial', 0.028), ('written', 0.027), ('users', 0.027), ('valuable', 0.027), ('interpret', 0.027), ('formulations', 0.026), ('less', 0.025), ('evaluations', 0.025), ('broad', 0.025), ('retrieval', 0.025), ('useful', 0.024), ('displayed', 0.024), ('overcome', 0.024), ('word', 0.024), ('wj', 0.024), ('treat', 0.024), ('usually', 0.024), ('sparse', 0.024), ('measuring', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="129-tfidf-1" href="./nips-2011-Improving_Topic_Coherence_with_Regularized_Topic_Models.html">129 nips-2011-Improving Topic Coherence with Regularized Topic Models</a></p>
<p>Author: David Newman, Edwin V. Bonilla, Wray Buntine</p><p>Abstract: Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reﬂect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data. 1</p><p>2 0.45488369 <a title="129-tfidf-2" href="./nips-2011-Complexity_of_Inference_in_Latent_Dirichlet_Allocation.html">58 nips-2011-Complexity of Inference in Latent Dirichlet Allocation</a></p>
<p>Author: David Sontag, Dan Roy</p><p>Abstract: We consider the computational complexity of probabilistic inference in Latent Dirichlet Allocation (LDA). First, we study the problem of ﬁnding the maximum a posteriori (MAP) assignment of topics to words, where the document’s topic distribution is integrated out. We show that, when the e↵ective number of topics per document is small, exact inference takes polynomial time. In contrast, we show that, when a document has a large number of topics, ﬁnding the MAP assignment of topics to words in LDA is NP-hard. Next, we consider the problem of ﬁnding the MAP topic distribution for a document, where the topic-word assignments are integrated out. We show that this problem is also NP-hard. Finally, we brieﬂy discuss the problem of sampling from the posterior, showing that this is NP-hard in one restricted setting, but leaving open the general question. 1</p><p>3 0.31940514 <a title="129-tfidf-3" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>Author: Dae I. Kim, Erik B. Sudderth</p><p>Abstract: Topic models are learned via a statistical model of variation within document collections, but designed to extract meaningful semantic structure. Desirable traits include the ability to incorporate annotations or metadata associated with documents; the discovery of correlated patterns of topic usage; and the avoidance of parametric assumptions, such as manual speciﬁcation of the number of topics. We propose a doubly correlated nonparametric topic (DCNT) model, the ﬁrst model to simultaneously capture all three of these properties. The DCNT models metadata via a ﬂexible, Gaussian regression on arbitrary input features; correlations via a scalable square-root covariance representation; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction. We validate the semantic structure and predictive performance of the DCNT using a corpus of NIPS documents annotated by various metadata. 1</p><p>4 0.21570884 <a title="129-tfidf-4" href="./nips-2011-Hierarchical_Topic_Modeling_for_Analysis_of_Time-Evolving_Personal_Choices.html">115 nips-2011-Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin, David B. Dunson</p><p>Abstract: The nested Chinese restaurant process is extended to design a nonparametric topic-model tree for representation of human choices. Each tree path corresponds to a type of person, and each node (topic) has a corresponding probability vector over items that may be selected. The observed data are assumed to have associated temporal covariates (corresponding to the time at which choices are made), and we wish to impose that with increasing time it is more probable that topics deeper in the tree are utilized. This structure is imposed by developing a new “change point</p><p>5 0.21079685 <a title="129-tfidf-5" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>6 0.11876939 <a title="129-tfidf-6" href="./nips-2011-Hierarchically_Supervised_Latent_Dirichlet_Allocation.html">116 nips-2011-Hierarchically Supervised Latent Dirichlet Allocation</a></p>
<p>7 0.1093654 <a title="129-tfidf-7" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>8 0.09334261 <a title="129-tfidf-8" href="./nips-2011-Linear_Submodular_Bandits_and_their_Application_to_Diversified_Retrieval.html">160 nips-2011-Linear Submodular Bandits and their Application to Diversified Retrieval</a></p>
<p>9 0.064462572 <a title="129-tfidf-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.063555807 <a title="129-tfidf-10" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>11 0.053556085 <a title="129-tfidf-11" href="./nips-2011-A_concave_regularization_technique_for_sparse_mixture_models.html">14 nips-2011-A concave regularization technique for sparse mixture models</a></p>
<p>12 0.050395671 <a title="129-tfidf-12" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>13 0.048098836 <a title="129-tfidf-13" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>14 0.046698555 <a title="129-tfidf-14" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>15 0.033123888 <a title="129-tfidf-15" href="./nips-2011-Co-regularized_Multi-view_Spectral_Clustering.html">54 nips-2011-Co-regularized Multi-view Spectral Clustering</a></p>
<p>16 0.032202154 <a title="129-tfidf-16" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>17 0.031505138 <a title="129-tfidf-17" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>18 0.030584818 <a title="129-tfidf-18" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>19 0.030499235 <a title="129-tfidf-19" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>20 0.030453164 <a title="129-tfidf-20" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, 0.059), (2, -0.039), (3, 0.025), (4, -0.042), (5, -0.454), (6, 0.16), (7, 0.196), (8, -0.234), (9, 0.068), (10, 0.155), (11, 0.218), (12, -0.027), (13, 0.072), (14, 0.07), (15, -0.032), (16, 0.084), (17, 0.056), (18, -0.03), (19, 0.057), (20, -0.12), (21, 0.019), (22, 0.054), (23, 0.027), (24, 0.044), (25, -0.015), (26, -0.063), (27, -0.033), (28, 0.024), (29, -0.058), (30, -0.041), (31, -0.016), (32, 0.048), (33, -0.056), (34, -0.007), (35, -0.012), (36, 0.005), (37, -0.019), (38, 0.032), (39, -0.002), (40, -0.027), (41, 0.023), (42, -0.016), (43, 0.006), (44, -0.014), (45, -0.011), (46, -0.048), (47, -0.059), (48, -0.021), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99235094 <a title="129-lsi-1" href="./nips-2011-Improving_Topic_Coherence_with_Regularized_Topic_Models.html">129 nips-2011-Improving Topic Coherence with Regularized Topic Models</a></p>
<p>Author: David Newman, Edwin V. Bonilla, Wray Buntine</p><p>Abstract: Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reﬂect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data. 1</p><p>2 0.90763837 <a title="129-lsi-2" href="./nips-2011-The_Doubly_Correlated_Nonparametric_Topic_Model.html">281 nips-2011-The Doubly Correlated Nonparametric Topic Model</a></p>
<p>Author: Dae I. Kim, Erik B. Sudderth</p><p>Abstract: Topic models are learned via a statistical model of variation within document collections, but designed to extract meaningful semantic structure. Desirable traits include the ability to incorporate annotations or metadata associated with documents; the discovery of correlated patterns of topic usage; and the avoidance of parametric assumptions, such as manual speciﬁcation of the number of topics. We propose a doubly correlated nonparametric topic (DCNT) model, the ﬁrst model to simultaneously capture all three of these properties. The DCNT models metadata via a ﬂexible, Gaussian regression on arbitrary input features; correlations via a scalable square-root covariance representation; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction. We validate the semantic structure and predictive performance of the DCNT using a corpus of NIPS documents annotated by various metadata. 1</p><p>3 0.88968706 <a title="129-lsi-3" href="./nips-2011-Complexity_of_Inference_in_Latent_Dirichlet_Allocation.html">58 nips-2011-Complexity of Inference in Latent Dirichlet Allocation</a></p>
<p>Author: David Sontag, Dan Roy</p><p>Abstract: We consider the computational complexity of probabilistic inference in Latent Dirichlet Allocation (LDA). First, we study the problem of ﬁnding the maximum a posteriori (MAP) assignment of topics to words, where the document’s topic distribution is integrated out. We show that, when the e↵ective number of topics per document is small, exact inference takes polynomial time. In contrast, we show that, when a document has a large number of topics, ﬁnding the MAP assignment of topics to words in LDA is NP-hard. Next, we consider the problem of ﬁnding the MAP topic distribution for a document, where the topic-word assignments are integrated out. We show that this problem is also NP-hard. Finally, we brieﬂy discuss the problem of sampling from the posterior, showing that this is NP-hard in one restricted setting, but leaving open the general question. 1</p><p>4 0.77496815 <a title="129-lsi-4" href="./nips-2011-Group_Anomaly_Detection_using_Flexible_Genre_Models.html">110 nips-2011-Group Anomaly Detection using Flexible Genre Models</a></p>
<p>Author: Liang Xiong, Barnabás Póczos, Jeff G. Schneider</p><p>Abstract: An important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena. In this paper, we study the group anomaly detection problem. Unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points. For this purpose, we propose the Flexible Genre Model (FGM). FGM is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies. We evaluate the effectiveness of FGM on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies. 1</p><p>5 0.64476657 <a title="129-lsi-5" href="./nips-2011-Hierarchical_Topic_Modeling_for_Analysis_of_Time-Evolving_Personal_Choices.html">115 nips-2011-Hierarchical Topic Modeling for Analysis of Time-Evolving Personal Choices</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin, David B. Dunson</p><p>Abstract: The nested Chinese restaurant process is extended to design a nonparametric topic-model tree for representation of human choices. Each tree path corresponds to a type of person, and each node (topic) has a corresponding probability vector over items that may be selected. The observed data are assumed to have associated temporal covariates (corresponding to the time at which choices are made), and we wish to impose that with increasing time it is more probable that topics deeper in the tree are utilized. This structure is imposed by developing a new “change point</p><p>6 0.608441 <a title="129-lsi-6" href="./nips-2011-Hierarchically_Supervised_Latent_Dirichlet_Allocation.html">116 nips-2011-Hierarchically Supervised Latent Dirichlet Allocation</a></p>
<p>7 0.45161313 <a title="129-lsi-7" href="./nips-2011-A_concave_regularization_technique_for_sparse_mixture_models.html">14 nips-2011-A concave regularization technique for sparse mixture models</a></p>
<p>8 0.36709559 <a title="129-lsi-8" href="./nips-2011-Linear_Submodular_Bandits_and_their_Application_to_Diversified_Retrieval.html">160 nips-2011-Linear Submodular Bandits and their Application to Diversified Retrieval</a></p>
<p>9 0.3616572 <a title="129-lsi-9" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>10 0.22251956 <a title="129-lsi-10" href="./nips-2011-Multi-View_Learning_of_Word_Embeddings_via_CCA.html">176 nips-2011-Multi-View Learning of Word Embeddings via CCA</a></p>
<p>11 0.21242757 <a title="129-lsi-11" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>12 0.18901265 <a title="129-lsi-12" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>13 0.15998252 <a title="129-lsi-13" href="./nips-2011-Dynamic_Pooling_and_Unfolding_Recursive_Autoencoders_for_Paraphrase_Detection.html">74 nips-2011-Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</a></p>
<p>14 0.15805906 <a title="129-lsi-14" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>15 0.15450026 <a title="129-lsi-15" href="./nips-2011-Structure_Learning_for_Optimization.html">274 nips-2011-Structure Learning for Optimization</a></p>
<p>16 0.14555529 <a title="129-lsi-16" href="./nips-2011-Maximum_Covariance_Unfolding_%3A_Manifold_Learning_for_Bimodal_Data.html">167 nips-2011-Maximum Covariance Unfolding : Manifold Learning for Bimodal Data</a></p>
<p>17 0.14300381 <a title="129-lsi-17" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>18 0.14198093 <a title="129-lsi-18" href="./nips-2011-Large-Scale_Sparse_Principal_Component_Analysis_with_Application_to_Text_Data.html">142 nips-2011-Large-Scale Sparse Principal Component Analysis with Application to Text Data</a></p>
<p>19 0.1351601 <a title="129-lsi-19" href="./nips-2011-Learning_a_Distance_Metric_from_a_Network.html">150 nips-2011-Learning a Distance Metric from a Network</a></p>
<p>20 0.13504961 <a title="129-lsi-20" href="./nips-2011-Advice_Refinement_in_Knowledge-Based_SVMs.html">27 nips-2011-Advice Refinement in Knowledge-Based SVMs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.01), (4, 0.017), (20, 0.026), (31, 0.031), (33, 0.042), (43, 0.054), (45, 0.178), (57, 0.039), (65, 0.023), (67, 0.366), (74, 0.049), (83, 0.02), (99, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81026495 <a title="129-lda-1" href="./nips-2011-Learning_Patient-Specific_Cancer_Survival_Distributions_as_a_Sequence_of_Dependent_Regressors.html">147 nips-2011-Learning Patient-Specific Cancer Survival Distributions as a Sequence of Dependent Regressors</a></p>
<p>Author: Hsiu-chin Lin, Vickie Baracos, Russell Greiner, Chun-nam J. Yu</p><p>Abstract: An accurate model of patient survival time can help in the treatment and care of cancer patients. The common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients. In this paper, we propose a local regression method for learning patient-speciﬁc survival time distribution based on patient attributes such as blood tests and clinical assessments. When tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the Cox and Aalen regression models. Our results also show that using patient-speciﬁc attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only. 1</p><p>same-paper 2 0.78872663 <a title="129-lda-2" href="./nips-2011-Improving_Topic_Coherence_with_Regularized_Topic_Models.html">129 nips-2011-Improving Topic Coherence with Regularized Topic Models</a></p>
<p>Author: David Newman, Edwin V. Bonilla, Wray Buntine</p><p>Abstract: Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval. However, when dealing with small collections or noisy text (e.g. web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful. To overcome this, we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reﬂect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall, this work makes topic models more useful across a broader range of text data. 1</p><p>3 0.63312167 <a title="129-lda-3" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>Author: Loc X. Bui, Ramesh Johari, Shie Mannor</p><p>Abstract: We consider a multi-armed bandit problem where there are two phases. The ﬁrst phase is an experimentation phase where the decision maker is free to explore multiple options. In the second phase the decision maker has to commit to one of the arms and stick with it. Cost is incurred during both phases with a higher cost during the experimentation phase. We analyze the regret in this setup, and both propose algorithms and provide upper and lower bounds that depend on the ratio of the duration of the experimentation phase to the duration of the commitment phase. Our analysis reveals that if given the choice, it is optimal to experiment Θ(ln T ) steps and then commit, where T is the time horizon.</p><p>4 0.58092445 <a title="129-lda-4" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>Author: Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: Graph cut optimization is one of the standard workhorses of image segmentation since for binary random ﬁeld representations of the image, it gives globally optimal results and there are efﬁcient polynomial time implementations. Often, the random ﬁeld is applied over a ﬂat partitioning of the image into non-intersecting elements, such as pixels or super-pixels. In the paper we show that if, instead of a ﬂat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding beneﬁts of global optimality and efﬁciency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree. We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a ﬂat partitioning) and demonstrate that the optimization in the pylon model is able to ﬂexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches. 1</p><p>5 0.5405612 <a title="129-lda-5" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>Author: Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, Aarti Singh</p><p>Abstract: Although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed. We analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability. We additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes. Further, using minimax analysis, we derive tight upper and lower bounds for the clustering problem and compare the performance of spectral clustering to these information theoretic limits. We also present experiments on simulated and real world data illustrating our results. 1</p><p>6 0.48710606 <a title="129-lda-6" href="./nips-2011-Generalized_Lasso_based_Approximation_of_Sparse_Coding_for_Visual_Recognition.html">105 nips-2011-Generalized Lasso based Approximation of Sparse Coding for Visual Recognition</a></p>
<p>7 0.48583403 <a title="129-lda-7" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>8 0.48343074 <a title="129-lda-8" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>9 0.47877121 <a title="129-lda-9" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>10 0.47797358 <a title="129-lda-10" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>11 0.47759309 <a title="129-lda-11" href="./nips-2011-Selecting_Receptive_Fields_in_Deep_Networks.html">244 nips-2011-Selecting Receptive Fields in Deep Networks</a></p>
<p>12 0.47653225 <a title="129-lda-12" href="./nips-2011-ShareBoost%3A_Efficient_multiclass_learning_with_feature_sharing.html">252 nips-2011-ShareBoost: Efficient multiclass learning with feature sharing</a></p>
<p>13 0.47605193 <a title="129-lda-13" href="./nips-2011-Maximum_Margin_Multi-Label_Structured_Prediction.html">169 nips-2011-Maximum Margin Multi-Label Structured Prediction</a></p>
<p>14 0.47441414 <a title="129-lda-14" href="./nips-2011-Relative_Density-Ratio_Estimation_for_Robust_Distribution_Comparison.html">238 nips-2011-Relative Density-Ratio Estimation for Robust Distribution Comparison</a></p>
<p>15 0.47345188 <a title="129-lda-15" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>16 0.47330406 <a title="129-lda-16" href="./nips-2011-Prediction_strategies_without_loss.html">220 nips-2011-Prediction strategies without loss</a></p>
<p>17 0.47319433 <a title="129-lda-17" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>18 0.47238418 <a title="129-lda-18" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>19 0.47227898 <a title="129-lda-19" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>20 0.47225362 <a title="129-lda-20" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
