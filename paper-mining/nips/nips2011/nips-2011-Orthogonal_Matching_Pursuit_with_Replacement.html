<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>209 nips-2011-Orthogonal Matching Pursuit with Replacement</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-209" href="#">nips2011-209</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>209 nips-2011-Orthogonal Matching Pursuit with Replacement</h1>
<br/><p>Source: <a title="nips-2011-209-pdf" href="http://papers.nips.cc/paper/4462-orthogonal-matching-pursuit-with-replacement.pdf">pdf</a></p><p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>Reference: <a title="nips-2011-209-reference" href="../nips2011_reference/nips-2011-Orthogonal_Matching_Pursuit_with_Replacement_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. [sent-7, score-0.168]
</p><p>2 For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. [sent-8, score-0.168]
</p><p>3 While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). [sent-9, score-0.267]
</p><p>4 OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. [sent-10, score-0.114]
</p><p>5 This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). [sent-12, score-0.284]
</p><p>6 Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. [sent-14, score-0.199]
</p><p>7 Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. [sent-15, score-0.088]
</p><p>8 We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. [sent-16, score-0.155]
</p><p>9 Compressed sensing, a new and active branch of modem signal processing, deals with the problem of designing measurement matrices and recovery algorithms, such that almost all sparse signals can be recovered from a smalI number of measurements. [sent-22, score-0.229]
</p><p>10 In this paper, we focus on the compressed sensing setting [3, 7] where we want to design a measurement matrix A E R=xn such that a sparse vector x* E R n with Ilx*llo := IBUpp(X*)I ::; k < n can be efficiently recovered from the measurements b = Ax* E R=. [sent-24, score-0.277]
</p><p>11 Initial work focused on various random ensembles of matrices A such that, if A was chosen randomly from that ensemble, one would be able to recover all or almost all sparse vectors x* from Ax*. [sent-25, score-0.115]
</p><p>12 Candes and Tao[3] isolated a key property called the restricted Isometry property (RIP) and proved that, as long as the measurement matrix A satisfies RIP, the true sparse vector can be obtained by solving an i,-optimization problem,  min Ilxll, S. [sent-26, score-0.205]
</p><p>13 It was shown in [2] that i,-minimization recovers all k-sparse vectors provided A satisfies t. [sent-31, score-0.204]
</p><p>14 Note that, in compressed sensing, the goal is to recover all, or most, k-sparse signals using the same measurement matrix A. [sent-35, score-0.123]
</p><p>15 Hence, weaker cooditioos such as restricted coovexity [20] studied in the statistical literature (where the aint is to recover a single sparse vector from noisy linear measurements) typically do not suffice. [sent-36, score-0.134]
</p><p>16 In fact, if RIP is not satisfied then multiple sparse vectors x can lead to the sante observatioo b, hence making recovery of the true sparse vector intpossible. [sent-37, score-0.308]
</p><p>17 Based on its RIP guarantees, i,-minimizatioo can guarantee recovery using just O(k log(n/ k») measurements, but it  has been observed in practice that i,-minimization is too expensive in large scale applications [8], for example, when the dimensionality is in the millions. [sent-38, score-0.13]
</p><p>18 This has sparked a huge interest in other iterative methods for sparse recovery. [sent-39, score-0.099]
</p><p>19 Indeed, it is known that, if run for k iterations, OMP cannot uoiformly recover all k-sparse vectors assumiug RIP cooditioo of the form 02k :'0 IJ [22, 18]. [sent-42, score-0.109]
</p><p>20 However, Zhang [26] showed that OMP, if run for 30k iterations, recovers the optimal solution when 03'k :'0 1/3; a significantly more restrictive cooditioo than the ones required by other methods like i,-minimization. [sent-43, score-0.187]
</p><p>21 In the family ofiterative hard thresholding algorithms, we can identifY two major subfamilies [17]: one- and two-stage algorithms. [sent-47, score-0.267]
</p><p>22 One-stage algorithms such as IHT, m and HTP, decide on the choice of the next support set and then usually solve a least squares problem on the updated support. [sent-49, score-0.14]
</p><p>23 On the other hand, two-stage algorithms, notable examples being CoSaMP and SP, first enlarge the support set, solve a least squares 00 it, and then reduce the support set back again to the desired size. [sent-51, score-0.227]
</p><p>24 However, it differs from our proposed methods as its analysis requires very restrictive RIP cooditioos (08k < 0. [sent-55, score-0.1]
</p><p>25 1 as quoted in [14]) and the connection to locality sensitive hashing (see below) is not made. [sent-56, score-0.131]
</p><p>26 In this paper, we present and provide a unified analysis for a family of one-stage iterative hard thresholding algorithms. [sent-60, score-0.352]
</p><p>27 OMPR can be thought of as a sintple modification of the classic greedy algorithm OMP: instead of sintply adding an element to the existiug support, it replaces an existiug support element with a new one. [sent-64, score-0.167]
</p><p>28 Surprisingly, this change allows us to prove sparse recovery under the condition 02k < 0. [sent-65, score-0.193]
</p><p>29 This is the best 02k based RIP condition under which any method, including i, -minimization, is (currently) known to provably perform sparse recovery. [sent-67, score-0.091]
</p><p>30 OMPR also lends itself to a faster intplententatioo using locality sensitive hashing (LSH). [sent-68, score-0.131]
</p><p>31 This allows us to provide recovery guarantees using an algorithm whose run-time is provably sub-linear in n, the number of dimensions. [sent-69, score-0.19]
</p><p>32 As a result, we are able to prove better recovery guarantees for these algorithms: 04k < 0. [sent-73, score-0.162]
</p><p>33 We hope that this unified analysis sheds more light on the interrelationships between the various kinds of iterative hard thresholding algorithms. [sent-76, score-0.303]
</p><p>34 • We present a family of iterative hard thresholding algorithms that on one end of the spectrum includes existing methods such as ITIIHTP while on the other end gives OMPR. [sent-78, score-0.326]
</p><p>35 • Unlike other intprovements over OMP, such as CoSaMP or SP, OMPR changes ouly ooe elentent of the support at a tinte. [sent-80, score-0.113]
</p><p>36 This allows us to use Locality Sensitive Hashing (LSH) to speed it up resultiug in the first provably sub-linear (in the ambient dimensionality n) time sparse recovery algorithm. [sent-81, score-0.198]
</p><p>37 +1 \b 1t+l  <-  0  • We provide a general proof for all the algorithms in our partial hard thresholding based family. [sent-86, score-0.27]
</p><p>38 In particular, we can guarantee recovery using OMPR, under both noiseless and noisy settings, provided 02' < 0. [sent-87, score-0.18]
</p><p>39 cooditioo under which any efficient sparse recovery method is known to work. [sent-90, score-0.228]
</p><p>40 Furthermore, our proof technique can be used to provide a general theorem that provides the least restrictive known guarantees for all the two-stage algorithms such as CoSaMP and SP (see Appendix D). [sent-91, score-0.175]
</p><p>41 2 Orthogonal Matching PUl"lIuit with Replacement Orthogonal matching pursuit (OMP), is a classic iterative algorithm for sparse recovery. [sent-93, score-0.266]
</p><p>42 At every stage, it selecta a coordinate to include in the current support set by maximizing the inner product between columns of the measurement matrix A and the current residnal b - Ax'. [sent-94, score-0.174]
</p><p>43 Doce the new coordinate has been added, it solves a least squares problem to fully miuimize the error on the current support set As a result, the residnal becomes orthogonal to the columos of A that correspond to the current support set. [sent-95, score-0.282]
</p><p>44 AB notation:  A\b:= argmin IIAx - bl1 2  •  z  The hard thresholding operator H. [sent-100, score-0.255]
</p><p>45 Doce the support IHI of the next iterate has been determined, the actna1 iterate X HI is obtained by solving the least squares problem: HI = X argmin IIAx - bli2 . [sent-112, score-0.252]
</p><p>46 x: supp(z)=It+l  Note that if the matrix A satisfies RIP of order k or larger, the above problem will be well conditioned and can be solved quickly and reliably using an iterative least squares solver. [sent-113, score-0.247]
</p><p>47 We will show that OMPR, uulike OMP, recovers any k-sparse vector under the RIP based cooditioo 02. [sent-114, score-0.131]
</p><p>48 This appears to be the least restrictive recovery condition (i. [sent-117, score-0.238]
</p><p>49 , best known coodition) under which any method, be it basis pursuit (ll-minimizatioo) or some iterative algorithm, is guaranteed to recover all k-sparse vectors. [sent-119, score-0.195]
</p><p>50 4992 "" 8 which makes it heuristically the least restrictive RIP condition for sparse recovery. [sent-126, score-0.148]
</p><p>51 Suppose the vector x* E IRn is k-sparse and the matrix A satisfies 1i2• < 0. [sent-129, score-0.106]
</p><p>52 Suppose the vector x* E IRn is k-sparse and the matrix A satisfies 1i2 • < 0. [sent-136, score-0.106]
</p><p>53 The I-th member of this family, OMPR (I), showo in Algorithm 2, replaces at most 1 elements of the curreot support with new elements. [sent-150, score-0.125]
</p><p>54 Our first result in this section conoects the OMPR family to hard thresholding. [sent-153, score-0.122]
</p><p>55 Given a set I of cardinality k, define the partial hard thresholding operator  Hk (z; I, I):~  argmin  (I)  Ily - zll . [sent-154, score-0.307]
</p><p>56 The name partial hard thresholding operator is justified because of the following reasoning. [sent-156, score-0.278]
</p><p>57 In fact, itbecomes identical to the standard hard thresholding operator  H. [sent-159, score-0.255]
</p><p>58 The following result shows that even the partial hard thresholding operator is easy to compute. [sent-164, score-0.278]
</p><p>59 In each iteration (with current iterate x' having support It ~ supp(xt», we do the following:  1. [sent-173, score-0.114]
</p><p>60 (partial Hard Thresholding) Form VH1 by partially hard thresholding zHI using the operator H. [sent-177, score-0.255]
</p><p>61 (Least Squares) Form the next iterate X HI by solving a least squares problem on the support IHI ofyHI. [sent-181, score-0.196]
</p><p>62 A nice property enjoyed by the entire OMPR family is guaranteed sparse recovery under RIP based conditions. [sent-182, score-0.219]
</p><p>63 Note from below that the condition under which OMPR (I) recovers sparse vectors becomes more restrictive as I increases. [sent-183, score-0.217]
</p><p>64 This could be an artifact of our analysis, as in experiments, we do not see any degradation in recovery ability as I is  increased. [sent-184, score-0.13]
</p><p>65 1/211Ax - bl1 2 :5 <)from measurements b = Ax* in O( ~ log(k/<» iterations provided we choose a step size 1'/ that satisfies 1'/(1 + 02. [sent-189, score-0.172]
</p><p>66 , 1/211Ax - bl1 2 :5 IIell 2 + <) from measurements b = Ax' + e in O( log«k + IleI1 2)1<) iterations provided we choose a step size 1'/ that satisfies 1'/(1 + 02,) < 1 and 1'/(1 - 02. [sent-196, score-0.172]
</p><p>67 Due to the least squares step of the previous iteration, the curreot residual Ax' - b is orthogoual to columns of AI,. [sent-202, score-0.126]
</p><p>68 " then our method moootonically decreases the objective function and converges to a local optimum even if RIP is not satisfied (note that upper RIP bound is indepeodeot oflower RIP bound, and can always be satisfied by nurma1izing the matrix appropriately). [sent-211, score-0.088]
</p><p>69 The above geoera1 result for the OMPR family immediately implies that it recovers sparse vectors as soon as the measuremeot matrix A satisfies 02, < 1/3. [sent-232, score-0.293]
</p><p>70 Suppose the vector x' E an is k-sparse and the matrix A satisfies 02k < 1/3. [sent-234, score-0.106]
</p><p>71 Then IlIT-Newton recovers x* from measurements b = Ax' in O(1og(k» iterations. [sent-235, score-0.139]
</p><p>72 5  4 Tighter Analysis of Two Stage Hard Thresholding Algorithms Recently, Maleki and Donoho [17] proposed a novel family of algorithms, namely two-stage hard thresholding algorithms. [sent-236, score-0.267]
</p><p>73 Doring each iteration, these algorithms add a fixed nwnber (say l) of elements to the current iterate's support set. [sent-237, score-0.103]
</p><p>74 A least squares problem is solved over the larger support set and then I elements with smallest magnitude are dropped to form next iterate's support set. [sent-238, score-0.198]
</p><p>75 Next iterate is then obtained by agaiu solviug the least squares over next iterate's support set. [sent-239, score-0.196]
</p><p>76 Usiug proof techniques developed for our proof of Theorem 4, we can obtain a simple proof for the entire spectrum of algorithms iu the two-stage hard thresholding family. [sent-241, score-0.388]
</p><p>77 Then the 7Wo-stage Hard Thresholding algorithm with replacement size I recovers x* from measurements b = Ax* in O(k) iterations provided: 6. [sent-244, score-0.19]
</p><p>78 CoSaMP[l9] recovers k-sparse x* E {-1,0, l}n from measurements b = Ax* provided 64k :::; 0. [sent-250, score-0.139]
</p><p>79 Subspace Pursuit[4] recovers k-sparse x* E {-I, 0, I}n from measurements b = Ax* provided 63k :::; 0. [sent-253, score-0.139]
</p><p>80 Note that while LSH is designed for nearest neighbor search (iu terms of Euclidean distances) and iu general might not have any guarantees for the similar neighbor search task, we are still able to apply it to our task because we can lower-hound the similarity of the most similar neighbor. [sent-264, score-0.115]
</p><p>81 LSH generates hash bits for a vector usiug randoruized hash functions that have the property that the probability of collision between two vectors is proportional to the similarity between them. [sent-266, score-0.267]
</p><p>82 I af a2 ) 1-;;: cos - I ( Iladlla211'  created by randoruly sampling hash functions h,. [sent-270, score-0.106]
</p><p>83 Next, q hash tables are constructed doring the pre-processiug stage usiug iudependently constructed hash key functions gl, 92, . [sent-274, score-0.286]
</p><p>84 , gq' Doring the query stage, a query is iudexed iuto each hash table usiug hash-key functions 91, 92, . [sent-277, score-0.165]
</p><p>85 However, we cannot directly use the above theorem to guarantee convergence of our hashing based OMPR algorithm as our algorithm requires finding the most similar poiut iu terms of magnitude of the iuner product. [sent-292, score-0.18]
</p><p>86 A detailed proof of the theorem below can be found iu Appendix B. [sent-294, score-0.141]
</p><p>87 The above theorem shows that the time complexity is sub-liuear iu n. [sent-297, score-0.112]
</p><p>88 (Best viewed in color)  6 Experimental Results In this section we present empirical results to demonstrate accurate and fast recovery by our OMPR method. [sent-303, score-0.13]
</p><p>89 In the first set of experiments, we present a phase transition diagram for OMPR and compare it to the phase transition diagrams of OMP and nIT-Newton with step size 1. [sent-304, score-0.251]
</p><p>90 For the second set of experiments, we demonstrate robostoess of OMPR compared to many existiog methods when measurements are noisy or smaller in number than what is required for exact recovery. [sent-305, score-0.119]
</p><p>91 For the third set of experiments, we demonstrate efficiency of our LSH based implementation by comparing recovery error and time required for our method with OMP and nIT-Newtoo (with step-size 1 and 1/2). [sent-306, score-0.13]
</p><p>92 We do not present results for the i,ibasis pursuit methods, as it has a1readybeen shown in several recent papers [10, 17] that the i, relaxation based methods are relatively inefficient for very large scale recovery problems. [sent-307, score-0.24]
</p><p>93 The underlying k-sparse vectors are generated by randomly selecting a support set of size k and then each entry in the support set is sampled uuiformiy from { +1, -I}. [sent-309, score-0.141]
</p><p>94 1  Phase Transition Diagrams  We first compare different methods using phase transition diagrams which are commouly used in compressed sensing literatore to compare different methods [17]. [sent-313, score-0.213]
</p><p>95 In Figure 1, we show the phase transition diagram of our OMPR method as well as that ofOMP and nIT-Newtoo (with step size 1). [sent-318, score-0.115]
</p><p>96 The plots shows probability of successful recovery as a function of p = min and 6 = kim. [sent-319, score-0.13]
</p><p>97 On the other hand, as expected, the phase transition diagram of OMP has a negligible fraction of the plot that shows high recovery probability. [sent-323, score-0.245]
</p><p>98 As shown in the phase transition diagrams in Figure 1, OMPR provides comparable recovery to the nIT-Newton method for noiseless cases. [sent-325, score-0.292]
</p><p>99 We then generate random binary vector x of sparsity k aod add Gaussian noise to it Figure 2 (a) shows recovery error (1iAx - bll) incurred by various methods for increasing k and noise level of 10%. [sent-328, score-0.13]
</p><p>100 Similarly, Figure 2 (b) shows recovery error incurred by various methods for fixed k = 50 and varying noise level. [sent-330, score-0.175]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ompr', 0.773), ('rip', 0.231), ('omp', 0.189), ('ax', 0.147), ('thresholding', 0.145), ('cosamp', 0.13), ('recovery', 0.13), ('lsh', 0.111), ('pursuit', 0.11), ('satisfies', 0.106), ('supp', 0.093), ('usiug', 0.088), ('iu', 0.083), ('hash', 0.077), ('hard', 0.073), ('recovers', 0.073), ('hashing', 0.068), ('measurements', 0.066), ('iterative', 0.059), ('measurement', 0.059), ('support', 0.058), ('cooditioo', 0.058), ('ihi', 0.058), ('zhi', 0.058), ('restrictive', 0.056), ('iterate', 0.056), ('squares', 0.053), ('diagrams', 0.052), ('sp', 0.052), ('htp', 0.051), ('irn', 0.051), ('replacement', 0.051), ('family', 0.049), ('phase', 0.047), ('fixed', 0.045), ('appeodix', 0.044), ('bll', 0.044), ('cooditioos', 0.044), ('corouary', 0.044), ('curreot', 0.044), ('doring', 0.044), ('iiax', 0.044), ('iiy', 0.044), ('satisfied', 0.044), ('sparse', 0.04), ('locality', 0.039), ('sensing', 0.039), ('compressed', 0.038), ('operator', 0.037), ('transition', 0.037), ('efficiently', 0.035), ('hk', 0.033), ('guarantees', 0.032), ('austin', 0.031), ('diagram', 0.031), ('hi', 0.031), ('xl', 0.029), ('proof', 0.029), ('theorem', 0.029), ('briefly', 0.029), ('choseo', 0.029), ('doce', 0.029), ('enlarge', 0.029), ('eotries', 0.029), ('existiog', 0.029), ('existiug', 0.029), ('extrente', 0.029), ('giveo', 0.029), ('heoce', 0.029), ('ily', 0.029), ('ilz', 0.029), ('isupp', 0.029), ('observatioo', 0.029), ('ooe', 0.029), ('orthogooal', 0.029), ('randoruly', 0.029), ('residnal', 0.029), ('tophi', 0.029), ('zll', 0.029), ('least', 0.029), ('matching', 0.029), ('classic', 0.028), ('provably', 0.028), ('coordinate', 0.028), ('orthogonal', 0.027), ('noiseless', 0.026), ('recover', 0.026), ('axt', 0.026), ('unified', 0.026), ('ouly', 0.026), ('india', 0.026), ('theo', 0.026), ('vectors', 0.025), ('sensitive', 0.024), ('noisy', 0.024), ('subspace', 0.024), ('ensembles', 0.024), ('condition', 0.023), ('replaces', 0.023), ('partial', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="209-tfidf-1" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>2 0.11011837 <a title="209-tfidf-2" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>Author: Yi-kai Liu</p><p>Abstract: We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefﬁcients. We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rankr restricted isometry property (RIP). This implies that M can be recovered from a ﬁxed (“universal”) set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley’s inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality. 1</p><p>3 0.098850533 <a title="209-tfidf-3" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>Author: Andrew E. Waters, Aswin C. Sankaranarayanan, Richard Baraniuk</p><p>Abstract: We consider the problem of recovering a matrix M that is the sum of a low-rank matrix L and a sparse matrix S from a small set of linear measurements of the form y = A(M) = A(L + S). This model subsumes three important classes of signal recovery problems: compressive sensing, afﬁne rank minimization, and robust principal component analysis. We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it. Empirically, SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efﬁcient implementation. Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efﬁcacy of the algorithm. 1</p><p>4 0.09353637 <a title="209-tfidf-4" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>Author: Alexandra Carpentier, Odalric-ambrym Maillard, Rémi Munos</p><p>Abstract: We consider the problem of recovering the parameter α ∈ RK of a sparse function f (i.e. the number of non-zero entries of α is small compared to the number K of features) given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomization process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven, independently on the number of sampling points N , even when the features are arbitrarily non-orthogonal. Under the assumption that f is H¨ lder continuous with exponent at least √ we proo 1/2, vide an estimate α of the parameter such that �α − α�2 = O(�η�2 / N ), where � � η is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method. 1</p><p>5 0.092041582 <a title="209-tfidf-5" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>Author: Zhen Li, Huazhong Ning, Liangliang Cao, Tong Zhang, Yihong Gong, Thomas S. Huang</p><p>Abstract: High dimensional similarity search in large scale databases becomes an important challenge due to the advent of Internet. For such applications, specialized data structures are required to achieve computational efﬁciency. Traditional approaches relied on algorithmic constructions that are often data independent (such as Locality Sensitive Hashing) or weakly dependent (such as kd-trees, k-means trees). While supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efﬁciency. Consequently such an embedding has to be used with linear scan or another search algorithm. Hence learning to hash does not directly address the search efﬁciency issue. This paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efﬁcient large scale search. Our approach takes both search quality and computational cost into consideration. Speciﬁcally, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples. The output of this search forest can be efﬁciently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efﬁciency. Experimental results show that our approach signiﬁcantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as LSH and k-means trees).</p><p>6 0.089647926 <a title="209-tfidf-6" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>7 0.084884144 <a title="209-tfidf-7" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>8 0.077346131 <a title="209-tfidf-8" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>9 0.074323311 <a title="209-tfidf-9" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>10 0.069758847 <a title="209-tfidf-10" href="./nips-2011-Hierarchical_Matching_Pursuit_for_Image_Classification%3A_Architecture_and_Fast_Algorithms.html">113 nips-2011-Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms</a></p>
<p>11 0.063852109 <a title="209-tfidf-11" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>12 0.060602881 <a title="209-tfidf-12" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>13 0.059495546 <a title="209-tfidf-13" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>14 0.055290826 <a title="209-tfidf-14" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>15 0.045917515 <a title="209-tfidf-15" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>16 0.045345932 <a title="209-tfidf-16" href="./nips-2011-Exploiting_spatial_overlap_to_efficiently_compute_appearance_distances_between_image_windows.html">91 nips-2011-Exploiting spatial overlap to efficiently compute appearance distances between image windows</a></p>
<p>17 0.043451846 <a title="209-tfidf-17" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>18 0.040982816 <a title="209-tfidf-18" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>19 0.037418444 <a title="209-tfidf-19" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>20 0.037032533 <a title="209-tfidf-20" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.003), (2, -0.038), (3, -0.086), (4, -0.042), (5, 0.062), (6, 0.016), (7, 0.061), (8, 0.033), (9, -0.012), (10, 0.043), (11, 0.032), (12, -0.06), (13, 0.042), (14, 0.076), (15, -0.065), (16, 0.076), (17, -0.023), (18, -0.048), (19, 0.054), (20, 0.036), (21, 0.034), (22, 0.014), (23, -0.143), (24, -0.043), (25, -0.005), (26, -0.004), (27, 0.014), (28, 0.083), (29, 0.097), (30, -0.065), (31, 0.002), (32, 0.109), (33, -0.058), (34, 0.052), (35, -0.179), (36, -0.009), (37, -0.044), (38, 0.068), (39, -0.076), (40, 0.022), (41, -0.007), (42, 0.133), (43, 0.039), (44, -0.067), (45, 0.04), (46, -0.081), (47, 0.08), (48, 0.005), (49, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90502274 <a title="209-lsi-1" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>2 0.73592198 <a title="209-lsi-2" href="./nips-2011-SpaRCS%3A_Recovering_low-rank_and_sparse_matrices_from_compressive_measurements.html">257 nips-2011-SpaRCS: Recovering low-rank and sparse matrices from compressive measurements</a></p>
<p>Author: Andrew E. Waters, Aswin C. Sankaranarayanan, Richard Baraniuk</p><p>Abstract: We consider the problem of recovering a matrix M that is the sum of a low-rank matrix L and a sparse matrix S from a small set of linear measurements of the form y = A(M) = A(L + S). This model subsumes three important classes of signal recovery problems: compressive sensing, afﬁne rank minimization, and robust principal component analysis. We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it. Empirically, SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efﬁcient implementation. Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efﬁcacy of the algorithm. 1</p><p>3 0.70861441 <a title="209-lsi-3" href="./nips-2011-Sparse_Recovery_with_Brownian_Sensing.html">264 nips-2011-Sparse Recovery with Brownian Sensing</a></p>
<p>Author: Alexandra Carpentier, Odalric-ambrym Maillard, Rémi Munos</p><p>Abstract: We consider the problem of recovering the parameter α ∈ RK of a sparse function f (i.e. the number of non-zero entries of α is small compared to the number K of features) given noisy evaluations of f at a set of well-chosen sampling points. We introduce an additional randomization process, called Brownian sensing, based on the computation of stochastic integrals, which produces a Gaussian sensing matrix, for which good recovery properties are proven, independently on the number of sampling points N , even when the features are arbitrarily non-orthogonal. Under the assumption that f is H¨ lder continuous with exponent at least √ we proo 1/2, vide an estimate α of the parameter such that �α − α�2 = O(�η�2 / N ), where � � η is the observation noise. The method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features. We report numerical experiments illustrating our method. 1</p><p>4 0.68642014 <a title="209-lsi-4" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>5 0.63649732 <a title="209-lsi-5" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>Author: Yi-kai Liu</p><p>Abstract: We study the problem of reconstructing an unknown matrix M of rank r and dimension d using O(rd poly log d) Pauli measurements. This has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing: recovering a sparse vector from a few of its Fourier coefﬁcients. We show that almost all sets of O(rd log6 d) Pauli measurements satisfy the rankr restricted isometry property (RIP). This implies that M can be recovered from a ﬁxed (“universal”) set of Pauli measurements, using nuclear-norm minimization (e.g., the matrix Lasso), with nearly-optimal bounds on the error. A similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm. Our proof uses Dudley’s inequality for Gaussian processes, together with bounds on covering numbers obtained via entropy duality. 1</p><p>6 0.5403074 <a title="209-lsi-6" href="./nips-2011-Divide-and-Conquer_Matrix_Factorization.html">73 nips-2011-Divide-and-Conquer Matrix Factorization</a></p>
<p>7 0.4752312 <a title="209-lsi-7" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>8 0.46657988 <a title="209-lsi-8" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>9 0.44830006 <a title="209-lsi-9" href="./nips-2011-Hashing_Algorithms_for_Large-Scale_Learning.html">111 nips-2011-Hashing Algorithms for Large-Scale Learning</a></p>
<p>10 0.42034709 <a title="209-lsi-10" href="./nips-2011-Minimax_Localization_of_Structural_Information_in_Large_Noisy_Matrices.html">172 nips-2011-Minimax Localization of Structural Information in Large Noisy Matrices</a></p>
<p>11 0.38528717 <a title="209-lsi-11" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>12 0.36853999 <a title="209-lsi-12" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>13 0.34979543 <a title="209-lsi-13" href="./nips-2011-Learning_to_Search_Efficiently_in_High_Dimensions.html">157 nips-2011-Learning to Search Efficiently in High Dimensions</a></p>
<p>14 0.32372382 <a title="209-lsi-14" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>15 0.31870559 <a title="209-lsi-15" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>16 0.30992854 <a title="209-lsi-16" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>17 0.30339977 <a title="209-lsi-17" href="./nips-2011-Learning_Anchor_Planes_for_Classification.html">143 nips-2011-Learning Anchor Planes for Classification</a></p>
<p>18 0.28944269 <a title="209-lsi-18" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>19 0.27951977 <a title="209-lsi-19" href="./nips-2011-Neural_Reconstruction_with_Approximate_Message_Passing_%28NeuRAMP%29.html">183 nips-2011-Neural Reconstruction with Approximate Message Passing (NeuRAMP)</a></p>
<p>20 0.27611884 <a title="209-lsi-20" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (4, 0.028), (20, 0.03), (26, 0.03), (31, 0.057), (33, 0.033), (39, 0.07), (43, 0.088), (45, 0.113), (57, 0.032), (65, 0.021), (74, 0.037), (83, 0.023), (92, 0.285), (99, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70966166 <a title="209-lda-1" href="./nips-2011-Orthogonal_Matching_Pursuit_with_Replacement.html">209 nips-2011-Orthogonal Matching Pursuit with Replacement</a></p>
<p>Author: Prateek Jain, Ambuj Tewari, Inderjit S. Dhillon</p><p>Abstract: In this paper, we consider the problem of compressed sensing where the goal is to recover all sparse vectors using a small number offixed linear measurements. For this problem, we propose a novel partial hard-thresholding operator that leads to a general family of iterative algorithms. While one extreme of the family yields well known hard thresholding algorithms like ITI and HTP[17, 10], the other end of the spectrum leads to a novel algorithm that we call Orthogonal Matching Pursnit with Replacement (OMPR). OMPR, like the classic greedy algorithm OMP, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residnal. However, unlike OMP, OMPR also removes one coordinate from the support. This simple change allows us to prove that OMPR has the best known guarantees for sparse recovery in terms of the Restricted Isometry Property (a condition on the measurement matrix). In contrast, OMP is known to have very weak performance guarantees under RIP. Given its simple structore, we are able to extend OMPR using locality sensitive hashing to get OMPR-Hasb, the first provably sub-linear (in dimensionality) algorithm for sparse recovery. Our proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as CoSaMP and Subspace Pursnit. We provide experimental results on large problems providing recovery for vectors of size up to million dimensions. We demonstrste that for large-scale problems our proposed methods are more robust and faster than existing methods.</p><p>2 0.70913053 <a title="209-lda-2" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>Author: Zuoguan Wang, Gerwin Schalk, Qiang Ji</p><p>Abstract: Brain-computer interfaces (BCIs) use brain signals to convey a user’s intent. Some BCI approaches begin by decoding kinematic parameters of movements from brain signals, and then proceed to using these signals, in absence of movements, to allow a user to control an output. Recent results have shown that electrocorticographic (ECoG) recordings from the surface of the brain in humans can give information about kinematic parameters (e.g., hand velocity or ﬁnger ﬂexion). The decoding approaches in these demonstrations usually employed classical classiﬁcation/regression algorithms that derive a linear mapping between brain signals and outputs. However, they typically only incorporate little prior information about the target kinematic parameter. In this paper, we show that different types of anatomical constraints that govern ﬁnger ﬂexion can be exploited in this context. Speciﬁcally, we incorporate these constraints in the construction, structure, and the probabilistic functions of a switched non-parametric dynamic system (SNDS) model. We then apply the resulting SNDS decoder to infer the ﬂexion of individual ﬁngers from the same ECoG dataset used in a recent study. Our results show that the application of the proposed model, which incorporates anatomical constraints, improves decoding performance compared to the results in the previous work. Thus, the results presented in this paper may ultimately lead to neurally controlled hand prostheses with full ﬁne-grained ﬁnger articulation. 1</p><p>3 0.65888053 <a title="209-lda-3" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>Author: Miles Lopes, Laurent Jacob, Martin J. Wainwright</p><p>Abstract: We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T 2 statistic. Working within a high-dimensional framework that allows (p, n) → ∞, we ﬁrst derive an asymptotic power function for our test, and then provide sufﬁcient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer. 1</p><p>4 0.58417028 <a title="209-lda-4" href="./nips-2011-Learning_Sparse_Representations_of_High_Dimensional_Data_on_Large_Scale_Dictionaries.html">149 nips-2011-Learning Sparse Representations of High Dimensional Data on Large Scale Dictionaries</a></p>
<p>Author: Zhen J. Xiang, Hao Xu, Peter J. Ramadge</p><p>Abstract: Learning sparse representations on data adaptive dictionaries is a state-of-the-art method for modeling data. But when the dictionary is large and the data dimension is high, it is a computationally challenging problem. We explore three aspects of the problem. First, we derive new, greatly improved screening tests that quickly identify codewords that are guaranteed to have zero weights. Second, we study the properties of random projections in the context of learning sparse representations. Finally, we develop a hierarchical framework that uses incremental random projections and screening to learn, in small stages, a hierarchically structured dictionary for sparse representations. Empirical results show that our framework can learn informative hierarchical sparse representations more efﬁciently. 1</p><p>5 0.54221827 <a title="209-lda-5" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>Author: Fatma K. Karzan, Arkadi S. Nemirovski, Boris T. Polyak, Anatoli Juditsky</p><p>Abstract: We discuss new methods for the recovery of signals with block-sparse structure, based on 1 -minimization. Our emphasis is on the efﬁciently computable error bounds for the recovery routines. We optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties. We justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance. 1</p><p>6 0.53150421 <a title="209-lda-6" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>7 0.52522421 <a title="209-lda-7" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>8 0.52435642 <a title="209-lda-8" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>9 0.50385696 <a title="209-lda-9" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>10 0.5036422 <a title="209-lda-10" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>11 0.50316411 <a title="209-lda-11" href="./nips-2011-Nearest_Neighbor_based_Greedy_Coordinate_Descent.html">182 nips-2011-Nearest Neighbor based Greedy Coordinate Descent</a></p>
<p>12 0.50214648 <a title="209-lda-12" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>13 0.50190431 <a title="209-lda-13" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>14 0.50134701 <a title="209-lda-14" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>15 0.50130415 <a title="209-lda-15" href="./nips-2011-On_Learning_Discrete_Graphical_Models_using_Greedy_Methods.html">195 nips-2011-On Learning Discrete Graphical Models using Greedy Methods</a></p>
<p>16 0.50057614 <a title="209-lda-16" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>17 0.49959618 <a title="209-lda-17" href="./nips-2011-Learning_with_the_weighted_trace-norm_under_arbitrary_sampling_distributions.html">159 nips-2011-Learning with the weighted trace-norm under arbitrary sampling distributions</a></p>
<p>18 0.49859846 <a title="209-lda-18" href="./nips-2011-A_Convergence_Analysis_of_Log-Linear_Training.html">4 nips-2011-A Convergence Analysis of Log-Linear Training</a></p>
<p>19 0.49834615 <a title="209-lda-19" href="./nips-2011-Noise_Thresholds_for_Spectral_Clustering.html">186 nips-2011-Noise Thresholds for Spectral Clustering</a></p>
<p>20 0.49804291 <a title="209-lda-20" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
