<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-1" href="#">nips2011-1</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</h1>
<br/><p>Source: <a title="nips-2011-1-pdf" href="http://papers.nips.cc/paper/4418-theta-mrf-capturing-spatial-and-semantic-structure-in-the-parameters-for-scene-understanding.pdf">pdf</a></p><p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>Reference: <a title="nips-2011-1-reference" href="../nips2011_reference/nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. [sent-6, score-1.267]
</p><p>2 We can capture such contextual information by taking as input the features/attributes from all the regions in the image. [sent-7, score-0.333]
</p><p>3 However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. [sent-8, score-0.879]
</p><p>4 In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. [sent-13, score-1.119]
</p><p>5 1  Introduction  Most scene understanding tasks (e. [sent-14, score-0.394]
</p><p>6 , when the airplane is on the ground)—here the contextual dependence of the airplane classiﬁer changes based on object’s location in the image. [sent-25, score-0.463]
</p><p>7 We can capture such contextual information by using features from all the regions in the image, and then also train a speciﬁc classiﬁer of each spatial location for each object category. [sent-26, score-1.071]
</p><p>8 Even if we group the regions into 64 (8 × 8) spatial locations, the total number of parameters will be 107 ∗ 64 ∗ K (for K features each). [sent-35, score-0.388]
</p><p>9 , in our multi-class object detection task this number would be about 47. [sent-38, score-0.516]
</p><p>10 The edges in our θ-MRF not only connect spatial neighbors but also semantic neighbors. [sent-47, score-0.569]
</p><p>11 For example, oven is often next to the dishwasher (in a kitchen scene), therefore they should share similar context, indicating that they can share their parameters. [sent-49, score-0.32]
</p><p>12 These semantic interactions between the parameters from different tasks also follow the undirected graph. [sent-50, score-0.552]
</p><p>13 Just like object labels are often modeled as conditionally independent of other non-contextual objects given the important context, the corresponding parameters can also be modeled similarly. [sent-51, score-0.633]
</p><p>14 These methods range from capturing the correlation between labels using a graphical model to introduce different types of priors on the labels (based on location, prior knowledge, etc. [sent-53, score-0.352]
</p><p>15 First, we consider the task of labeling 107 object categories in the SUN09 dataset, and show that our method gets better performance than the state-of-the-art methods even when with simple regression as the learning model. [sent-61, score-0.609]
</p><p>16 Second, we consider the multiple tasks of scene categorization, depth estimation and geometry labeling, and again show that our method gets comparable or better performance than the state-of-the-art methods when we use our method with simple regression. [sent-62, score-0.511]
</p><p>17 Various sources of context have been explored, ranging from the global scene layout, interactions between regions to local features. [sent-66, score-0.525]
</p><p>18 [47] use the statistics of low-level features across the entire scene to prime object detection. [sent-68, score-0.668]
</p><p>19 [45] use 3D scene information to provide priors on potential object locations. [sent-71, score-0.762]
</p><p>20 Many works also model context to capture the local interactions between neighboring regions [23, 35, 28], objects [48, 14], or both [16, 10, 2]. [sent-75, score-0.495]
</p><p>21 [9] combine individual classiﬁers by considering spatial interactions between the object detections, and solve a uniﬁed multi-class object detection problem through a structured discriminative approach. [sent-80, score-1.265]
</p><p>22 Other ways to share information across categories include sharing representations [12, 30], sharing training examples between categories [36, 15], sharing parameters [26, 27], and so on. [sent-81, score-0.887]
</p><p>23 Our work lies in the category of sharing parameters, aiming at capturing the dependencies in the parameters for relevant vision applications. [sent-82, score-0.339]
</p><p>24 learn a hierarchy to share the hierarchical parameters for the object appearance models. [sent-94, score-0.582]
</p><p>25 Our work is motivated by this direction of work, and our focus is to capture spatial and semantic sharing in parameters using undirected graphical models that have appropriate independence properties. [sent-95, score-0.857]
</p><p>26 "  39:" 79"  Figure 1: The proposed θ-MRF graph with spatial and semantic interaction structure. [sent-194, score-0.531]
</p><p>27 In applications, [46] considers capturing relationships between the object categories using a Dirichlet prior on the parameters. [sent-199, score-0.628]
</p><p>28 3  Our Approach: θ-MRF  In order to give better intuition, we use the multi-class object detection task as an illustrative example. [sent-205, score-0.516]
</p><p>29 We then have a binary classi(n) ﬁer, whose output is yk, ∈ {0, 1} that indicates the presence of the k th object at the th grid in the nth image. [sent-209, score-0.555]
</p><p>30 Intuitively the parameters of the classiﬁers at neighboring spatial regions (for the same object category) should share their parameters. [sent-242, score-0.995]
</p><p>31 To model this type of interactions between parameters, we introduce edges on the θ-MRF that connect the spatially neighboring nodes, as shown in Figure 1-left. [sent-243, score-0.338]
</p><p>32 Note that the spatial edges only couple the parameters of the same task together. [sent-244, score-0.348]
</p><p>33 λspt θi − θj 0  R(θi , θj ) =  p  if θi and θj are spatial neighbors for a task otherwise  where λspt is a tuning factor for the spatial interactions. [sent-247, score-0.505]
</p><p>34 In addition to connecting the parameters for neighboring locations, we also encourage the sharing between the elements of a parameter vector that correspond to spatially neighboring inputs. [sent-255, score-0.477]
</p><p>35 Assume we have the presence of the object “road” at the different regions of an image as attributes. [sent-257, score-0.595]
</p><p>36 In order to learn a car detector with these attributes as inputs, we would like to give similar high-weights to the neighboring regions in the car detector output. [sent-258, score-0.399]
</p><p>37 We call this source-based spatial grouping, as compared to target-based spatial grouping that we described in the previous paragraph. [sent-259, score-0.506]
</p><p>38 We not only connect the parameters for spatial neighbors of the same task, but also consider the semantic neighbors across tasks. [sent-269, score-0.62]
</p><p>39 Motivated by the conditional independency in the object labels which suggests that given the important context the presence of an object is independent of other non-contextual objects, we can encode such properties in our θ-MRF. [sent-270, score-1.005]
</p><p>40 R(θi , θj ) =  λsmn wij θi − θj 0  p  if θi and θj are semantic neighbors otherwise  where wij indicates the strength of the semantic dependency between these two parameters and λsmn is a tuning factor for the semantic interactions. [sent-276, score-0.833]
</p><p>41 In order to model how they share parameters, we model the relative spatial relationship between the positive outputs of the two tasks. [sent-281, score-0.377]
</p><p>42 For example, assume we have two highly co-occuring object categories, indexed by k1 and k2 . [sent-282, score-0.413]
</p><p>43 From the training data, we learn the relative th th spatial distribution map of the presence of the k2 object, given the k1 object in the center. [sent-283, score-0.824]
</p><p>44 " /0102345"  Figure 2: An instantiation of the proposed algorithm for the object recognition tasks on SUN09 dataset. [sent-335, score-0.508]
</p><p>45 Therefore, the parameters of the k2 object that satisfy these relative locations, have semantic edges with θk1 ,l1 . [sent-337, score-0.772]
</p><p>46 4  Applications  We apply our θ-MRF on two different settings: 1) object detection on the SUN09 dataset [7]; 2) multiple scene understanding tasks (scene categorization, geometric labeling, depth estimation), comparing to the cascaded classiﬁcation models (CCM) [22, 31]. [sent-348, score-1.212]
</p><p>47 The task of object detection is to recognize and localize objects of interest in an image. [sent-350, score-0.609]
</p><p>48 [7] use an additional set of 26,000 images to training baseline detectors [13], and select 107 object categories to evaluate their contextual model. [sent-353, score-0.889]
</p><p>49 , we use the same baseline object detector outputs as the attribute inputs for our algorithm, the same training/testing data, and the same evaluation metrics. [sent-356, score-0.668]
</p><p>50 We use each of the baseline object detectors to produce a 8 × 8 detection map, with each element indicating the conﬁdence (between 0 and 1) of the object’s presence at the respective region. [sent-359, score-0.719]
</p><p>51 , 107) scene category indicates the type of scene containing the ith object category. [sent-363, score-1.008]
</p><p>52 The 107 8 × 8 object maps and the 107 scene classiﬁer outputs together form a 6955-dimension feature vector, as the attribute inputs for our algorithm. [sent-365, score-0.814]
</p><p>53 Our algorithm learns a region-speciﬁc contextual model for each object category, resulting in a speciﬁc classiﬁer of each region for each category. [sent-368, score-0.672]
</p><p>54 For the lth region, it is labeled as positive for the k th object category if it satisﬁes: overlap(Ok , Rl )/ min area(Rl ), area(Ok ) > 0. [sent-373, score-0.586]
</p><p>55 3, where Ok means a bounding-box instantiation of the k th object category and Rl means the lth grid cell. [sent-374, score-0.586]
</p><p>56 We apply the trained classiﬁers to the test images, and gain the object detection maps. [sent-376, score-0.516]
</p><p>57 We consider the task of estimating different types of labels in a scene: scene categorization, geometry labeling, and depth estimation. [sent-379, score-0.48]
</p><p>58 The previous CCM algorithms [22, 31] consider sharing information across tasks, but do not consider the sharing between categories or between 5  Table 1: Performance of object recognition and detection on SUN09 dataset. [sent-382, score-0.929]
</p><p>59 Model Chance Baseline (w/o context) Single model per object Independent model State-of-the-art [7] θ-MRF (l2 -regularized) θ-MRF (l1 -regularized)  Object Recognition (% AP) 5. [sent-383, score-0.413]
</p><p>60 93  Table 2: Performance of scene categorization, geometric labeling, and depth estimation in CCM. [sent-396, score-0.464]
</p><p>61 Here we introduce the semantically-grouped regularization to scene categorization, and the spatially-grouped regularization to depth and geometry estimation. [sent-416, score-0.522]
</p><p>62 For scene categorization, we classify 8 different categories on the MIT outdoor scene dataset [39]. [sent-418, score-0.633]
</p><p>63 This gives us a total of 8 parameter vectors for scene categorization task. [sent-422, score-0.356]
</p><p>64 For depth estimation, we train a speciﬁc linear regression model for every region of the image (with uniformly divided 11 × 10 regions), and incorporate the spatial grouping on both the second-layer inputs and outputs. [sent-424, score-0.606]
</p><p>65 We evaluate the performance by computing the root mean square error of the estimated depth with respect to ground truth laser scan depth using the Make3D Range Image dataset [44]. [sent-426, score-0.322]
</p><p>66 On the secondlayer, we train a logistic regression classiﬁer for every region of the image (with uniformly divided 16 × 16 regions), and incorporate the spatial grouping on both the second-layer inputs and outputs. [sent-428, score-0.445]
</p><p>67 The training of our algorithm takes 6-7 hours for object detection/recognition and 3-4 hours for multi-task cascade. [sent-433, score-0.45]
</p><p>68 For example, a base object detector [13] takes about 1. [sent-437, score-0.464]
</p><p>69 With the semantic and spatial regularization, our proposed θ-MRF algorithm improves signiﬁcantly over the CCM algorithm that also uses the same set of tasks for prediction. [sent-446, score-0.57]
</p><p>70 Table 1 gives the performance of different methods on SUN09 dataset, for both object recognition (predicting the object presence) and object detection (predicting the object location). [sent-457, score-1.755]
</p><p>71 C/,+  Figure 3: Examples showing that infrequent object categories share parameters with frequent object categories. [sent-499, score-1.175]
</p><p>72 - Baseline (w/o context): the baseline object detectors trained by [13], which are also used to generate the initial detection results used as inputs for our algorithm and the state-of-the-art algorithm. [sent-500, score-0.697]
</p><p>73 - Single model: a single classiﬁer is trained for each object category, not varying across different locations. [sent-501, score-0.413]
</p><p>74 - Independent model: this means an independent classiﬁer is trained for the presence of an object for each region. [sent-503, score-0.455]
</p><p>75 - State-of-the-art: This is the tree-based graphical model proposed in [7], which explicitly models the object dependencies based on labels and detector outputs. [sent-505, score-0.566]
</p><p>76 2 - The proposed θ-MRF algorithm, which shares the models spatially within an object category and semantically across various objects. [sent-506, score-0.629]
</p><p>77 We study the relative improvement of the proposed parameter sharing algorithm over the nonparameter-sharing algorithm (Independent model in Table 1) on object categories with different number of training samples in the SUN09 object recognition task. [sent-511, score-1.131]
</p><p>78 The relative improvement on object categories with less than 200 training samples is 34. [sent-512, score-0.573]
</p><p>79 Our parameter sharing algorithm helps the infrequent objects implicitly make use of the data of frequent objects to learn better models. [sent-515, score-0.388]
</p><p>80 3, focusing on two infrequent object categories: van and awning, respectively. [sent-517, score-0.47]
</p><p>81 The histogram in the ﬁgures shows the number of training instances for each object category. [sent-518, score-0.45]
</p><p>82 The color bar shows the correlation between the learned parameter of the object with the parameters for other objects. [sent-519, score-0.476]
</p><p>83 Figure 3-left shows that the van category has few training instances, turn out to share the parameters strongly with the categories of car, building and road. [sent-521, score-0.414]
</p><p>84 We note that in the dataset, awning and streetlight are not highly co-occuring, thus initially when we create the semantic groups, these two objects do not appear simultaneously in any group. [sent-523, score-0.466]
</p><p>85 However, the semantic groups containing streetlight and the semantic groups containing awning both contain objects like road, building, and car. [sent-524, score-0.709]
</p><p>86 The following three ﬁgures shows the contextual inputs (showing the spatial map) which have the top ranked weights (highest positive elements of the parameters) . [sent-590, score-0.489]
</p><p>87 4-middle): the classiﬁers for the same object OkP different locations share a prior at ` ´ P θk ,i. [sent-595, score-0.61]
</p><p>88 4-right): the classiﬁers from the same semantic group share a prior βGi , and the parameter for a classiﬁer is ` P P deﬁned as: θk,l = βGi +´βk,l , where θk,l ∈ Gi . [sent-600, score-0.395]
</p><p>89 Table 3: Results for different Table 3 shows that the proposed θ-MRF algorithms outperform the parameter sharing methods on object recognition task. [sent-605, score-0.558]
</p><p>90 Modeling the spatial and semantic interactions by both methods (adding priors or adding edges) improve the performance, while the θ-MRF based approach is more effective, especially with l1 norm. [sent-608, score-0.673]
</p><p>91 For each object in a location, we show the top three contextual inputs learned by our approach. [sent-620, score-0.67]
</p><p>92 We note that to detect shoes in topper part of the image, shelves, closet and box are the most important contextual inputs; while ﬂoor and wall play a more important role in detecting shoes at the bottom of the image. [sent-622, score-0.555]
</p><p>93 In Figure 5-right, we show a group of parameters (corresponding to different regions for oven, refrigerator, and sink) that share semantic edges with each other on the parameter-MRF. [sent-625, score-0.558]
</p><p>94 Besides we also note the spatially smooth effect on the ﬁgures, which is resulted from our source-based spatial interactions. [sent-629, score-0.325]
</p><p>95 Exploiting hierarchical context on a large database of object categories. [sent-686, score-0.486]
</p><p>96 A bayesian approach to unsupervised one-shot learning of object categories. [sent-720, score-0.413]
</p><p>97 Learning to detect unseen object classes by between-class attribute transfer. [sent-835, score-0.459]
</p><p>98 Object bank: A high-level image representation for scene classiﬁcation and semantic feature sparsiﬁcation. [sent-858, score-0.545]
</p><p>99 Learning to share visual appearance for multiclass object detection. [sent-915, score-0.519]
</p><p>100 Modeling mutual context of object and human pose in human-object interaction activities. [sent-949, score-0.542]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('object', 0.413), ('scene', 0.255), ('semantic', 0.243), ('spatial', 0.232), ('ccm', 0.216), ('contextual', 0.196), ('depth', 0.161), ('sharing', 0.145), ('mrf', 0.138), ('categories', 0.123), ('shoes', 0.114), ('classi', 0.106), ('share', 0.106), ('interactions', 0.104), ('detection', 0.103), ('categorization', 0.101), ('saxena', 0.098), ('tasks', 0.095), ('priors', 0.094), ('cascaded', 0.093), ('spatially', 0.093), ('location', 0.093), ('objects', 0.093), ('regions', 0.093), ('hyper', 0.089), ('neighboring', 0.088), ('er', 0.087), ('airplane', 0.087), ('awning', 0.087), ('stove', 0.087), ('category', 0.085), ('microwave', 0.076), ('ap', 0.075), ('labeling', 0.073), ('context', 0.073), ('ers', 0.07), ('oven', 0.065), ('yk', 0.064), ('complementary', 0.064), ('labels', 0.064), ('parameters', 0.063), ('region', 0.063), ('cvpr', 0.062), ('contexts', 0.062), ('detectors', 0.062), ('inputs', 0.061), ('baseline', 0.058), ('car', 0.058), ('infrequent', 0.057), ('src', 0.057), ('road', 0.057), ('interaction', 0.056), ('edges', 0.053), ('regularization', 0.053), ('detector', 0.051), ('th', 0.05), ('gi', 0.049), ('geometric', 0.048), ('box', 0.048), ('undirected', 0.047), ('image', 0.047), ('prior', 0.046), ('attribute', 0.046), ('capturing', 0.046), ('locations', 0.045), ('ok', 0.045), ('independence', 0.045), ('understanding', 0.044), ('capture', 0.044), ('cabinet', 0.043), ('closet', 0.043), ('dishwasher', 0.043), ('jnew', 0.043), ('refrigerator', 0.043), ('smn', 0.043), ('spt', 0.043), ('streetlight', 0.043), ('tsuhan', 0.043), ('directed', 0.042), ('grouping', 0.042), ('presence', 0.042), ('iccv', 0.042), ('torralba', 0.041), ('neighbors', 0.041), ('respective', 0.041), ('wall', 0.04), ('ijcv', 0.04), ('outputs', 0.039), ('hoiem', 0.039), ('graphical', 0.038), ('galleguillos', 0.038), ('heitz', 0.038), ('lth', 0.038), ('shelves', 0.038), ('semantically', 0.038), ('training', 0.037), ('bounding', 0.036), ('desai', 0.035), ('floor', 0.035), ('rabinovich', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="1-tfidf-1" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>2 0.28806949 <a title="1-tfidf-2" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>Author: Vincent Delaitre, Josef Sivic, Ivan Laptev</p><p>Abstract: We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-speciﬁc body part and object interactions bypasses the difﬁcult problem of estimating the complete human body pose conﬁguration. Beneﬁts of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline. 1</p><p>3 0.27184787 <a title="1-tfidf-3" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>Author: Hema S. Koppula, Abhishek Anand, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an ofﬁce and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model’s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efﬁcient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and ofﬁces (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for ofﬁces, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of ﬁnding objects in large cluttered rooms.1 1</p><p>4 0.25498071 <a title="1-tfidf-4" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>5 0.24221818 <a title="1-tfidf-5" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>Author: Bin Zhao, Fei Li, Eric P. Xing</p><p>Abstract: Most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge. With the emergence of structured large-scale dataset such as the ImageNet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available. As human cognition of complex visual world beneﬁts from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance. In this paper, we employ such semantic relatedness among image categories for large-scale image categorization. Speciﬁcally, a category hierarchy is utilized to properly deﬁne loss function and select common set of features for related categories. An efﬁcient optimization method based on proximal approximation and accelerated parallel gradient method is introduced. Experimental results on a subset of ImageNet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach. 1</p><p>6 0.17928195 <a title="1-tfidf-6" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>7 0.17559336 <a title="1-tfidf-7" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>8 0.17270271 <a title="1-tfidf-8" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>9 0.17177454 <a title="1-tfidf-9" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>10 0.16138963 <a title="1-tfidf-10" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>11 0.15826787 <a title="1-tfidf-11" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>12 0.15629028 <a title="1-tfidf-12" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>13 0.14747305 <a title="1-tfidf-13" href="./nips-2011-Fast_and_Balanced%3A_Efficient_Label_Tree_Learning_for_Large_Scale_Object_Recognition.html">96 nips-2011-Fast and Balanced: Efficient Label Tree Learning for Large Scale Object Recognition</a></p>
<p>14 0.14514107 <a title="1-tfidf-14" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>15 0.14166975 <a title="1-tfidf-15" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>16 0.13952786 <a title="1-tfidf-16" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>17 0.13895975 <a title="1-tfidf-17" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>18 0.13523716 <a title="1-tfidf-18" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>19 0.12654389 <a title="1-tfidf-19" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>20 0.12632306 <a title="1-tfidf-20" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, 0.221), (2, -0.21), (3, 0.315), (4, 0.139), (5, 0.048), (6, 0.023), (7, -0.027), (8, 0.057), (9, 0.138), (10, 0.055), (11, -0.037), (12, -0.064), (13, 0.117), (14, -0.038), (15, 0.033), (16, 0.038), (17, -0.026), (18, 0.016), (19, -0.003), (20, -0.058), (21, 0.04), (22, -0.063), (23, 0.091), (24, 0.058), (25, -0.035), (26, 0.049), (27, -0.008), (28, 0.026), (29, -0.025), (30, 0.038), (31, 0.145), (32, -0.05), (33, 0.016), (34, 0.071), (35, -0.05), (36, -0.051), (37, 0.001), (38, -0.12), (39, 0.021), (40, 0.027), (41, 0.012), (42, -0.079), (43, -0.001), (44, 0.018), (45, 0.058), (46, 0.017), (47, -0.039), (48, 0.031), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97653687 <a title="1-lsi-1" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>2 0.87695038 <a title="1-lsi-2" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>Author: Vincent Delaitre, Josef Sivic, Ivan Laptev</p><p>Abstract: We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-speciﬁc body part and object interactions bypasses the difﬁcult problem of estimating the complete human body pose conﬁguration. Beneﬁts of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline. 1</p><p>3 0.86373162 <a title="1-lsi-3" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>Author: Hema S. Koppula, Abhishek Anand, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: Inexpensive RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an ofﬁce and address the task of semantic labeling of these 3D point clouds. We propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model’s parsimony becomes important and we address that by using multiple types of edge potentials. The model admits efﬁcient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and ofﬁces (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for ofﬁces, and 73.38% in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of ﬁnding objects in large cluttered rooms.1 1</p><p>4 0.86045498 <a title="1-lsi-4" href="./nips-2011-Joint_3D_Estimation_of_Objects_and_Scene_Layout.html">138 nips-2011-Joint 3D Estimation of Objects and Scene Layout</a></p>
<p>Author: Andreas Geiger, Christian Wojek, Raquel Urtasun</p><p>Abstract: We propose a novel generative model that is able to reason jointly about the 3D scene layout as well as the 3D location and orientation of objects in the scene. In particular, we infer the scene topology, geometry as well as trafﬁc activities from a short video sequence acquired with a single camera mounted on a moving car. Our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in 3D, we are able to signiﬁcantly increase the performance of state-of-the-art object detectors in their ability to estimate object orientation. 1</p><p>5 0.78678524 <a title="1-lsi-5" href="./nips-2011-Transfer_Learning_by_Borrowing_Examples_for_Multiclass_Object_Detection.html">290 nips-2011-Transfer Learning by Borrowing Examples for Multiclass Object Detection</a></p>
<p>Author: Joseph J. Lim, Antonio Torralba, Ruslan Salakhutdinov</p><p>Abstract: Despite the recent trend of increasingly large datasets for object detection, there still exist many classes with few training examples. To overcome this lack of training data for certain classes, we propose a novel way of augmenting the training data for each class by borrowing and transforming examples from other classes. Our model learns which training instances from other classes to borrow and how to transform the borrowed examples so that they become more similar to instances from the target class. Our experimental results demonstrate that our new object detector, with borrowed and transformed examples, improves upon the current state-of-the-art detector on the challenging SUN09 object detection dataset. 1</p><p>6 0.78232318 <a title="1-lsi-6" href="./nips-2011-Why_The_Brain_Separates_Face_Recognition_From_Object_Recognition.html">304 nips-2011-Why The Brain Separates Face Recognition From Object Recognition</a></p>
<p>7 0.77842015 <a title="1-lsi-7" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>8 0.76965928 <a title="1-lsi-8" href="./nips-2011-Understanding_the_Intrinsic_Memorability_of_Images.html">293 nips-2011-Understanding the Intrinsic Memorability of Images</a></p>
<p>9 0.73845893 <a title="1-lsi-9" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>10 0.73387325 <a title="1-lsi-10" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>11 0.72071105 <a title="1-lsi-11" href="./nips-2011-Object_Detection_with_Grammar_Models.html">193 nips-2011-Object Detection with Grammar Models</a></p>
<p>12 0.70107466 <a title="1-lsi-12" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>13 0.63146698 <a title="1-lsi-13" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>14 0.61427528 <a title="1-lsi-14" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>15 0.6060288 <a title="1-lsi-15" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>16 0.5768252 <a title="1-lsi-16" href="./nips-2011-PiCoDes%3A_Learning_a_Compact_Code_for_Novel-Category_Recognition.html">214 nips-2011-PiCoDes: Learning a Compact Code for Novel-Category Recognition</a></p>
<p>17 0.56717449 <a title="1-lsi-17" href="./nips-2011-Learning_a_Tree_of_Metrics_with_Disjoint_Visual_Features.html">151 nips-2011-Learning a Tree of Metrics with Disjoint Visual Features</a></p>
<p>18 0.55702722 <a title="1-lsi-18" href="./nips-2011-Recovering_Intrinsic_Images_with_a_Global_Sparsity_Prior_on_Reflectance.html">235 nips-2011-Recovering Intrinsic Images with a Global Sparsity Prior on Reflectance</a></p>
<p>19 0.53639066 <a title="1-lsi-19" href="./nips-2011-Rapid_Deformable_Object_Detection_using_Dual-Tree_Branch-and-Bound.html">233 nips-2011-Rapid Deformable Object Detection using Dual-Tree Branch-and-Bound</a></p>
<p>20 0.53435189 <a title="1-lsi-20" href="./nips-2011-Structured_Learning_for_Cell_Tracking.html">275 nips-2011-Structured Learning for Cell Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (4, 0.071), (20, 0.087), (26, 0.016), (31, 0.078), (33, 0.105), (36, 0.141), (43, 0.072), (45, 0.135), (57, 0.043), (60, 0.037), (74, 0.051), (83, 0.021), (99, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88625354 <a title="1-lda-1" href="./nips-2011-%24%5Ctheta%24-MRF%3A_Capturing_Spatial_and_Semantic_Structure_in_the_Parameters_for_Scene_Understanding.html">1 nips-2011-$\theta$-MRF: Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding</a></p>
<p>Author: Congcong Li, Ashutosh Saxena, Tsuhan Chen</p><p>Abstract: For most scene understanding tasks (such as object detection or depth estimation), the classiﬁers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by deﬁning a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks. 1</p><p>2 0.82386655 <a title="1-lda-2" href="./nips-2011-Generalization_Bounds_and_Consistency_for_Latent_Structural_Probit_and_Ramp_Loss.html">103 nips-2011-Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss</a></p>
<p>Author: Joseph Keshet, David A. McAllester</p><p>Abstract: We consider latent structural versions of probit loss and ramp loss. We show that these surrogate loss functions are consistent in the strong sense that for any feature map (ﬁnite or inﬁnite dimensional) they yield predictors approaching the inﬁmum task loss achievable by any linear predictor over the given features. We also give ﬁnite sample generalization bounds (convergence rates) for these loss functions. These bounds suggest that probit loss converges more rapidly. However, ramp loss is more easily optimized on a given sample. 1</p><p>3 0.81960487 <a title="1-lda-3" href="./nips-2011-Probabilistic_Joint_Image_Segmentation_and_Labeling.html">223 nips-2011-Probabilistic Joint Image Segmentation and Labeling</a></p>
<p>Author: Adrian Ion, Joao Carreira, Cristian Sminchisescu</p><p>Abstract: We present a joint image segmentation and labeling model (JSL) which, given a bag of ﬁgure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories. The process of drawing samples from the joint distribution can be interpreted as ﬁrst sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag [1], followed by sampling labels for those segments, conditioned on the choice of a particular tiling. We learn the segmentation and labeling parameters jointly, based on Maximum Likelihood with a novel Incremental Saddle Point estimation procedure. The partition function over tilings and labelings is increasingly more accurately approximated by including incorrect conﬁgurations that a not-yet-competent model rates probable during learning. We show that the proposed methodology matches the current state of the art in the Stanford dataset [2], as well as in VOC2010, where 41.7% accuracy on the test set is achieved.</p><p>4 0.81835651 <a title="1-lda-4" href="./nips-2011-Learning_person-object_interactions_for_action_recognition_in_still_images.html">154 nips-2011-Learning person-object interactions for action recognition in still images</a></p>
<p>Author: Vincent Delaitre, Josef Sivic, Ivan Laptev</p><p>Abstract: We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-speciﬁc body part and object interactions bypasses the difﬁcult problem of estimating the complete human body pose conﬁguration. Beneﬁts of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline. 1</p><p>5 0.81808776 <a title="1-lda-5" href="./nips-2011-Pylon_Model_for_Semantic_Segmentation.html">227 nips-2011-Pylon Model for Semantic Segmentation</a></p>
<p>Author: Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman</p><p>Abstract: Graph cut optimization is one of the standard workhorses of image segmentation since for binary random ﬁeld representations of the image, it gives globally optimal results and there are efﬁcient polynomial time implementations. Often, the random ﬁeld is applied over a ﬂat partitioning of the image into non-intersecting elements, such as pixels or super-pixels. In the paper we show that if, instead of a ﬂat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding beneﬁts of global optimality and efﬁciency). As a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree. We apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes. The experiments highlight the advantage of inference on a segmentation tree (over a ﬂat partitioning) and demonstrate that the optimization in the pylon model is able to ﬂexibly choose the level of segmentation across the image. Overall, the proposed system has superior segmentation accuracy on several datasets (Graz-02, Stanford background) compared to previously suggested approaches. 1</p><p>6 0.81741393 <a title="1-lda-6" href="./nips-2011-Spatial_distance_dependent_Chinese_restaurant_processes_for_image_segmentation.html">266 nips-2011-Spatial distance dependent Chinese restaurant processes for image segmentation</a></p>
<p>7 0.80738562 <a title="1-lda-7" href="./nips-2011-Semantic_Labeling_of_3D_Point_Clouds_for_Indoor_Scenes.html">247 nips-2011-Semantic Labeling of 3D Point Clouds for Indoor Scenes</a></p>
<p>8 0.80230892 <a title="1-lda-8" href="./nips-2011-Maximum_Margin_Multi-Instance_Learning.html">168 nips-2011-Maximum Margin Multi-Instance Learning</a></p>
<p>9 0.79691195 <a title="1-lda-9" href="./nips-2011-Image_Parsing_with_Stochastic_Scene_Grammar.html">127 nips-2011-Image Parsing with Stochastic Scene Grammar</a></p>
<p>10 0.79524469 <a title="1-lda-10" href="./nips-2011-Matrix_Completion_for_Multi-label_Image_Classification.html">165 nips-2011-Matrix Completion for Multi-label Image Classification</a></p>
<p>11 0.79198498 <a title="1-lda-11" href="./nips-2011-Large-Scale_Category_Structure_Aware_Image_Categorization.html">141 nips-2011-Large-Scale Category Structure Aware Image Categorization</a></p>
<p>12 0.7824257 <a title="1-lda-12" href="./nips-2011-Non-conjugate_Variational_Message_Passing_for_Multinomial_and_Binary_Regression.html">188 nips-2011-Non-conjugate Variational Message Passing for Multinomial and Binary Regression</a></p>
<p>13 0.77821624 <a title="1-lda-13" href="./nips-2011-Video_Annotation_and_Tracking_with_Active_Learning.html">303 nips-2011-Video Annotation and Tracking with Active Learning</a></p>
<p>14 0.77796102 <a title="1-lda-14" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>15 0.77715111 <a title="1-lda-15" href="./nips-2011-Maximal_Cliques_that_Satisfy_Hard_Constraints_with_Application_to_Deformable_Object_Model_Learning.html">166 nips-2011-Maximal Cliques that Satisfy Hard Constraints with Application to Deformable Object Model Learning</a></p>
<p>16 0.7767759 <a title="1-lda-16" href="./nips-2011-Multiple_Instance_Filtering.html">180 nips-2011-Multiple Instance Filtering</a></p>
<p>17 0.77095723 <a title="1-lda-17" href="./nips-2011-Im2Text%3A_Describing_Images_Using_1_Million_Captioned_Photographs.html">126 nips-2011-Im2Text: Describing Images Using 1 Million Captioned Photographs</a></p>
<p>18 0.76883209 <a title="1-lda-18" href="./nips-2011-An_ideal_observer_model_for_identifying_the_reference_frame_of_objects.html">35 nips-2011-An ideal observer model for identifying the reference frame of objects</a></p>
<p>19 0.76722437 <a title="1-lda-19" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<p>20 0.76693678 <a title="1-lda-20" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
