<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-289" href="#">nips2011-289</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</h1>
<br/><p>Source: <a title="nips-2011-289-pdf" href="http://papers.nips.cc/paper/4277-trace-lasso-a-trace-norm-regularization-for-correlated-designs.pdf">pdf</a></p><p>Author: Edouard Grave, Guillaume R. Obozinski, Francis R. Bach</p><p>Abstract: Using the 1 -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net. 1</p><p>Reference: <a title="nips-2011-289-reference" href="../nips2011_reference/nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Trace Lasso: a trace norm regularization for correlated designs  ´ Edouard Grave INRIA, Sierra Project-team ´ Ecole Normale Sup´ rieure, Paris e edouard. [sent-1, score-0.635]
</p><p>2 fr  Abstract Using the 1 -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. [sent-7, score-0.183]
</p><p>3 In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. [sent-8, score-0.301]
</p><p>4 This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. [sent-9, score-0.86]
</p><p>5 We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net. [sent-10, score-0.679]
</p><p>6 The simplest example of such norms is the 1 -norm, leading to the Lasso, when used within a least-squares framework. [sent-18, score-0.176]
</p><p>7 , close to block-diagonal covariance matrices) or sparse graphs, such as for example problems involving sequences (in which case, the covariance matrix is close to a Toeplitz matrix [9]). [sent-22, score-0.116]
</p><p>8 In these situations, the Lasso is known to have stability problems: although its predictive performance is not disastrous, the selected predictor may vary a lot (typically, given two correlated variables, the Lasso will only select one of the two, at random). [sent-23, score-0.181]
</p><p>9 First, the elastic net [10] adds a strongly convex penalty term (the squared 2 -norm) that will stabilize selection (typically, given two correlated variables, the elastic net will select the two variables). [sent-25, score-1.303]
</p><p>10 However, it is blind to the exact 1  correlation structure, and while strong convexity is required for some variables, it is not for other variables. [sent-26, score-0.113]
</p><p>11 Another solution is to consider the group Lasso, which will divide the predictors into groups and penalize the sum of the 2 -norm of these groups [11]. [sent-27, score-0.326]
</p><p>12 This is known to accomodate strong correlations within groups [12]; however it requires to know the groups in advance, which is not always possible. [sent-28, score-0.159]
</p><p>13 An ideal regularizer should thus take into account the design (like the group Lasso, with oracle groups), but without requiring human intervention (like the elastic net); it should thus add strong convexity only where needed, and not modifying variables where things behave correctly. [sent-30, score-0.602]
</p><p>14 In this paper, we propose a new norm towards this end. [sent-31, score-0.166]
</p><p>15 More precisely we make the following contributions: • We propose in Section 2 a new norm based on the trace norm (a. [sent-32, score-0.64]
</p><p>16 • We show that there is a unique minimum when penalizing with this norm in Section 2. [sent-36, score-0.197]
</p><p>17 • We perform synthetic experiments in Section 5, where we show that the trace Lasso outperforms existing norms in strong-correlation regimes. [sent-40, score-0.477]
</p><p>18 For M ∈ Rp×p , diag(M) ∈ Rp is the diagonal of the matrix M, while for u ∈ Rp , Diag(u) ∈ Rp×p is the diagonal matrix whose diagonal elements are the ui . [sent-48, score-0.088]
</p><p>19 We will use various matrix norms, here are the notations we use: M ∗ is the trace norm, i. [sent-54, score-0.352]
</p><p>20 , the sum of the singular values of the matrix M, M op is the operator norm, i. [sent-56, score-0.212]
</p><p>21 , the maximum singular value of the matrix M, M F is the Frobenius norm, i. [sent-58, score-0.086]
</p><p>22 , the 2 -norm of the singular values, which is also equal to tr(M M) and M 2,1 is the sum of the 2 -norm of the columns p  of M: M  2,1  M(i) 2 . [sent-60, score-0.126]
</p><p>23 = i=1  2  Deﬁnition and properties of the trace Lasso  We consider the problem of predicting y ∈ R, given a vector x ∈ Rp , assuming a linear model y = w x + ε, where ε is an additive (typically Gaussian) noise with mean 0 and variance σ 2 . [sent-61, score-0.308]
</p><p>24 1  Related work  We will now present some classical penalty functions for linear models which are widely used in the machine learning and statistics community. [sent-73, score-0.117]
</p><p>25 The ﬁrst one, known as Tikhonov regularization [16] or ridge [17], is the squared 2 -norm. [sent-74, score-0.17]
</p><p>26 One of the main drawbacks of this penalty function is the fact 2  that it does not perform variable selection and thus does not behave well in sparse high-dimensional settings. [sent-76, score-0.254]
</p><p>27 When two predictors are highly correlated, the Lasso has a very unstable behavior: it often only selects the variable that is the most correlated with the residual. [sent-83, score-0.247]
</p><p>28 On the other hand, Tikhonov regularization tends to shrink coefﬁcients of correlated variables together, leading to a very stable behavior. [sent-84, score-0.255]
</p><p>29 In order to get the best of both worlds, stability and variable selection, Zou and Hastie introduced the elastic net [10], which is the sum of the 1 -norm and squared 2 -norm. [sent-85, score-0.533]
</p><p>30 Unfortunately, this estimator needs two regularization parameters and is not adaptive to the precise correlation structure of the data. [sent-86, score-0.181]
</p><p>31 Some authors also proposed to use pairwise correlations between predictors to interpolate more adaptively between the 1 -norm and squared 2 -norm, by introducing a method called pairwise elastic net [20] (see comparisons with our approach in Section 5). [sent-87, score-0.812]
</p><p>32 Finally, when one has more knowledge about the data, for example clusters of variables that should be selected together, one can use the group Lasso [11]. [sent-88, score-0.129]
</p><p>33 i=1  The effect of this penalty function is to introduce sparsity at the group level: variables in a group are selected all together. [sent-90, score-0.337]
</p><p>34 2  The ridge, the Lasso and the trace Lasso  In this section, we show that Tikhonov regularization and the Lasso penalty can be viewed as norms of the matrix X Diag(w). [sent-93, score-0.692]
</p><p>35 We then introduce a new norm involving this matrix. [sent-94, score-0.193]
</p><p>36 The solution of empirical risk minimization penalized by the 1 -norm or 2 -norm is not equivariant to rescaling of the predictors X(i) , so it is common to normalize the predictors. [sent-95, score-0.193]
</p><p>37 When normalizing the predictors X(i) , and penalizing by Tikhonov regularization or by the Lasso, people are implicitly using a regularization term that depends on the data or design matrix X. [sent-96, score-0.46]
</p><p>38 In fact, there is an equivalence between normalizing the predictors and not normalizing them, using the two following reweighted 2 and 1 -norms instead of Tikhonov regularization and the Lasso: p  w  2 2  p  X(i)  =  2 2  2 wi  and  w  1  X(i)  =  i=1  2  |wi |. [sent-97, score-0.502]
</p><p>39 (2)  i=1  These two norms can be expressed using the matrix X Diag(w): w  2  = X Diag(w)  F  and  w  1  = X Diag(w)  2,1 ,  and a natural question arises: are there other relevant choices of functions or matrix norms? [sent-98, score-0.23]
</p><p>40 A classical measure of the complexity of a model is the number of predictors used by this model, which is equal to the size of the support of w. [sent-99, score-0.218]
</p><p>41 This penalty being non-convex, people use its convex relaxation, which is the 1 -norm, leading to the Lasso. [sent-100, score-0.198]
</p><p>42 This is equal to the rank of the selected predictors, or also to the rank of the matrix X Diag(w). [sent-102, score-0.123]
</p><p>43 Like the size of the support, this function is non-convex, and we propose to replace it by a convex surrogate, the trace norm, leading to the following penalty that we call “trace Lasso”: Ω(w) = X Diag(w) ∗ . [sent-103, score-0.506]
</p><p>44 3  The trace Lasso has some interesting properties: if all the predictors are orthogonal, then, it is equal to the 1 -norm. [sent-104, score-0.496]
</p><p>45 Indeed, we have the decomposition: p  X(i) e , X(i) 2 i  X(i) 2 wi  X Diag(w) = i=1  where ei are the vectors of the canonical basis. [sent-105, score-0.107]
</p><p>46 Since the predictors are orthogonal and the ei are orthogonal too, this gives the singular value decomposition of X Diag(w) and we get p  X Diag(w)  ∗  X(i) 2 |wi | = X Diag(w)  =  2,1 . [sent-106, score-0.274]
</p><p>47 i=1  On the other hand, if all the predictors are equal to X(1) , then X Diag(w) = X(1) w , and we get X Diag(w) ∗ = X(1) 2 w 2 = X Diag(w) F , which is equivalent to Tikhonov regularization. [sent-107, score-0.188]
</p><p>48 Thus when two predictors are strongly correlated, our norm will behave like Tikhonov regularization, while for almost uncorrelated predictors, it will behave like the Lasso. [sent-108, score-0.541]
</p><p>49 The trace Lasso, by adding strong convexity exactly in the direction of highly correlated covariates, always has a unique minimum, and is thus much more stable than the Lasso. [sent-110, score-0.459]
</p><p>50 If the loss function is strongly convex with respect to its second argument, then the solution of the empirical risk minimization penalized by the trace Lasso, i. [sent-112, score-0.453]
</p><p>51 The technical proof of this proposition can be found in [22], and consists in showing that in the ﬂat directions of the loss function, the trace Lasso is strongly convex. [sent-116, score-0.424]
</p><p>52 3  A new family of penalty functions  In this section, we introduce a new family of penalties, inspired by the trace Lasso, allowing us to write the 1 -norm, the 2 -norm and the newly introduced trace Lasso as special cases. [sent-118, score-0.838]
</p><p>53 In other words, we can express the 1 and 2 -norms of w using the trace norm of a given matrix times the matrix Diag(w). [sent-120, score-0.562]
</p><p>54 Therefore, we introduce the following family of penalty functions: Deﬁnition 1. [sent-122, score-0.183]
</p><p>55 We introduce the norm ΩP as ΩP (w) = P Diag(w) ∗ . [sent-124, score-0.193]
</p><p>56 As stated before, the 1 and 2 -norms are special cases of the family of norms we just introduced. [sent-128, score-0.181]
</p><p>57 Another important penalty that can be expressed as a special case is the group Lasso, with nonoverlapping groups. [sent-129, score-0.181]
</p><p>58 Sj  We deﬁne the matrix PGL by PGL = ij  1/ 0  |Sk |  if i and j are in the same group Sk , otherwise. [sent-134, score-0.15]
</p><p>59 (3) gives a singular value decomposition of PGL Diag(w) and so the group Lasso penalty can be expressed as a special case of our family of norms: PGL Diag(w) ∗ = wSj 2 = w GL . [sent-144, score-0.292]
</p><p>60 Sj  In the following proposition, we show that our norm only depends on the value of P P. [sent-145, score-0.166]
</p><p>61 This is an important property for the trace Lasso, where P = X, since it underlies the fact that this penalty only depends on the correlation matrix X X of the covariates. [sent-146, score-0.511]
</p><p>62 We plot the unit ball of our norm for the following value of P P (see ﬁgure 1): 1 0. [sent-150, score-0.191]
</p><p>63 7 1  1 1 0  1 1 0  0 0 1  We can lower bound and upper bound our norms by the 2 -norm and 1 -norm respectively. [sent-162, score-0.142]
</p><p>64 This shows that, as for the elastic net, our norms interpolate between the 1 -norm and the 2 -norm. [sent-163, score-0.474]
</p><p>65 But the main difference between the elastic net and our norms is the fact that our norms are adaptive, and require a single regularization parameter to tune. [sent-164, score-0.82]
</p><p>66 In particular for the trace Lasso, when two covariates are strongly correlated, it will be close to the 2 -norm, while when two covariates are almost uncorrelated, it will behave like the 1 -norm. [sent-165, score-0.674]
</p><p>67 This is a behavior close to the one of the pairwise elastic net [20]. [sent-166, score-0.532]
</p><p>68 Dual norm  The dual norm is an important quantity for both optimization and theoretical analysis of the estimator. [sent-171, score-0.402]
</p><p>69 Unfortunately, we are not able in general to obtain a closed form expression of the dual norm for the family of norms we just introduced. [sent-172, score-0.417]
</p><p>70 The dual norm, deﬁned by Ω∗ (u) = max u v, can be bounded by: P ΩP (v)≤1  Ω∗ (u) P  ≤ P Diag(u) 5  op . [sent-174, score-0.196]
</p><p>71 Using the fact that diag(P P) = 1, we have u v = tr Diag(u)P P Diag(v) ≤ P Diag(u) op P Diag(v) ∗ , where the inequality comes from the fact that the operator norm · op is the dual norm of the trace norm. [sent-176, score-1.011]
</p><p>72 The deﬁnition of the dual norm then gives the result. [sent-177, score-0.236]
</p><p>73 As a corollary, we can bound the dual norm by a constant times the ∞ -norm: Ω∗ (u) ≤ P Diag(u) op ≤ P op Diag(u) op = P op u P Using proposition (3), we also have the inequality Ω∗ (u) ≥ u ∞ . [sent-178, score-0.811]
</p><p>74 Optimization algorithm  In this section, we introduce an algorithm to estimate the parameter vector w when the loss function is equal to the square loss: (y, w x) = 1 (y − w x)2 and the penalty is the trace Lasso. [sent-180, score-0.5]
</p><p>75 It is 2 straightforward to extend this algorithm to the family of norms indexed by P. [sent-181, score-0.181]
</p><p>76 2 w 2 We could optimize this cost function by subgradient descent, but this is quite inefﬁcient: computing the subgradient of the trace Lasso is expensive and the rate of convergence of subgradient descent is quite slow. [sent-183, score-0.41]
</p><p>77 First, we need to introduce a well-known variational formulation for the trace norm [23]: Proposition 5. [sent-185, score-0.501]
</p><p>78 The trace norm of M is equal to: 1 M ∗ = inf tr M S−1 M + tr (S) , 2S 0 1/2  and the inﬁmum is attained for S = MM  . [sent-187, score-0.657]
</p><p>79 Optimizing over w is a least-squares problem penalized by a reweighted 2 -norm equal to w Dw, where D = Diag diag(X S−1 X) . [sent-193, score-0.179]
</p><p>80 • Set wi by solving the system (X X + λD)w = X y. [sent-206, score-0.107]
</p><p>81 Using the inequalities on the dual norm we obtained in the previous section, we get X y  ∞  ≤ Ω∗ (X y) ≤ X  Therefore, starting the path at λ = X  4  op  X y  ∞  op  X y  ∞. [sent-212, score-0.488]
</p><p>82 Approximation around the Lasso  We recall that when P = I ∈ Rp×p , our norm is equal to the 1 -norm, and we want to understand its behavior when P departs from the identity. [sent-214, score-0.242]
</p><p>83 Thus, we compute a second order approximation of our norm around the Lasso: we add a small perturbation ∆ ∈ Sp to the identity matrix, and using Prop. [sent-215, score-0.192]
</p><p>84 2|wj |  We can rewrite this approximation as (I + ∆) Diag(w)  ∗  = w  1  + diag(∆) |w| + i,j  ∆2 (|wi | − |wj |)2 ij + o( ∆ 2 ), 4(|wi | + |wj |)  using a slight abuse of notation, considering that the last term is equal to 0 when wi = wj = 0. [sent-217, score-0.284]
</p><p>85 The second order term is quite interesting: it shows that when two covariates are correlated, the effect of the trace Lasso is to shrink the corresponding coefﬁcients toward each other. [sent-218, score-0.457]
</p><p>86 We also note that this term is very similar to pairwise elastic net penalties, which are of the form |w| P|w|, where Pij is a decreasing function of ∆ij . [sent-219, score-0.504]
</p><p>87 5  Experiments  In this section, we perform experiments on synthetic data to illustrate the behavior of the trace Lasso and other classical penalties when there are highly correlated covariates in the design matrix. [sent-220, score-0.64]
</p><p>88 For i in the support of w, wi is independently drawn from a uniform distribution over [−1, 1]. [sent-225, score-0.137]
</p><p>89 In all six graphs of Figure 2, we observe behaviors that are typical of Lasso, ridge and elastic net: the Lasso performs very well on very sparse models but its performance degrades for denser models. [sent-234, score-0.38]
</p><p>90 The elastic net performs better than the Lasso for settings where there are strongly correlated covariates, thanks to its strongly convex 2 term. [sent-235, score-0.672]
</p><p>91 The trace Lasso approaches the Lasso when n is much larger than p, but the weak coupling induced by empirical correlations is sufﬁcient to slightly decrease its performance compared to that of the Lasso. [sent-238, score-0.358]
</p><p>92 By contrast, in settings 2 and 3, the trace Lasso outperforms other methods (including the pairwise elastic net) since variables that should be selected together are indeed correlated. [sent-239, score-0.719]
</p><p>93 As for the penalized elastic net, since it takes into account the correlations between variables, it is not surprising that in experiments 2 and 3 it performs better than methods that do not. [sent-240, score-0.4]
</p><p>94 e-net stands for elastic net, pen stands for pairwise elastic net and trace stands for trace Lasso. [sent-244, score-1.519]
</p><p>95 )  6  Conclusion  We introduce a new penalty function, the trace Lasso, which takes advantage of the correlation between covariates to add strong convexity exactly in the directions where needed, unlike the elastic net for example, which blindly adds a squared 2 -norm term in every directions. [sent-246, score-1.203]
</p><p>96 In the future, we want to show that if a dedicated norm using prior knowledge such as the group Lasso can be used, the trace Lasso will behave similarly and its performance will not degrade too much, providing theoretical guarantees to such adaptivity. [sent-248, score-0.613]
</p><p>97 Finally, we will seek applications of this estimator in inverse problems such as deblurring, where the design matrix exhibits strong correlation structure. [sent-249, score-0.187]
</p><p>98 Stability approach to regularization selection (stars) for high dimensional graphical models. [sent-328, score-0.115]
</p><p>99 Exploiting covariate similarity in sparse regression via the pairwise elastic net. [sent-368, score-0.374]
</p><p>100 Trace lasso: a trace norm regularization for correlated designs. [sent-380, score-0.635]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lasso', 0.446), ('diag', 0.439), ('trace', 0.308), ('elastic', 0.297), ('norm', 0.166), ('net', 0.158), ('norms', 0.142), ('predictors', 0.14), ('tikhonov', 0.128), ('pgl', 0.127), ('op', 0.126), ('covariates', 0.123), ('penalty', 0.117), ('wi', 0.107), ('wsj', 0.102), ('wj', 0.087), ('regularization', 0.081), ('correlated', 0.08), ('reweighted', 0.078), ('sierra', 0.077), ('behave', 0.075), ('proposition', 0.071), ('dual', 0.07), ('group', 0.064), ('sj', 0.063), ('rp', 0.058), ('ridge', 0.055), ('penalized', 0.053), ('normale', 0.052), ('rieure', 0.052), ('grave', 0.051), ('wsi', 0.051), ('correlations', 0.05), ('tr', 0.049), ('pairwise', 0.049), ('mum', 0.048), ('equal', 0.048), ('normalizing', 0.048), ('gl', 0.047), ('convex', 0.047), ('penalize', 0.046), ('obozinski', 0.046), ('strongly', 0.045), ('toeplitz', 0.045), ('matrix', 0.044), ('sk', 0.044), ('stability', 0.044), ('ecole', 0.043), ('ij', 0.042), ('correlation', 0.042), ('singular', 0.042), ('mallat', 0.041), ('uncorrelated', 0.04), ('paris', 0.04), ('family', 0.039), ('penalties', 0.039), ('inria', 0.039), ('guillaume', 0.038), ('convexity', 0.038), ('groups', 0.038), ('attained', 0.037), ('stabilize', 0.036), ('columns', 0.036), ('design', 0.035), ('interpolate', 0.035), ('francis', 0.035), ('stands', 0.034), ('subgradient', 0.034), ('leading', 0.034), ('variables', 0.034), ('squared', 0.034), ('selection', 0.034), ('univ', 0.034), ('strong', 0.033), ('estimator', 0.033), ('penalizing', 0.031), ('selected', 0.031), ('royal', 0.031), ('orthogonal', 0.031), ('zou', 0.03), ('decomposition', 0.03), ('support', 0.03), ('rk', 0.03), ('xw', 0.03), ('bach', 0.029), ('sparse', 0.028), ('behavior', 0.028), ('synthetic', 0.027), ('unstable', 0.027), ('partition', 0.027), ('introduce', 0.027), ('series', 0.027), ('add', 0.026), ('shrink', 0.026), ('lot', 0.026), ('sp', 0.026), ('methodology', 0.026), ('sup', 0.026), ('adaptive', 0.025), ('unit', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="289-tfidf-1" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>Author: Edouard Grave, Guillaume R. Obozinski, Francis R. Bach</p><p>Abstract: Using the 1 -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net. 1</p><p>2 0.34129313 <a title="289-tfidf-2" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>Author: Feng Yan, Yuan Qi</p><p>Abstract: For many real-world applications, we often need to select correlated variables— such as genetic variations and imaging features associated with Alzheimer’s disease—in a high dimensional space. The correlation between variables presents a challenge to classical variable selection methods. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not exploit the correlation information embedded in the data to select correlated variables. To overcome this limitation, we present a novel hybrid model, EigenNet, that uses the eigenstructures of data to guide variable selection. Speciﬁcally, it integrates a sparse conditional classiﬁcation model with a generative model capturing variable correlations in a principled Bayesian framework. We develop an efﬁcient active-set algorithm to estimate the model via evidence maximization. Experimental results on synthetic data and imaging genetics data demonstrate the superior predictive performance of the EigenNet over the lasso, the elastic net, and the automatic relevance determination. 1</p><p>3 0.23881856 <a title="289-tfidf-3" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><p>4 0.18344879 <a title="289-tfidf-4" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>5 0.16147396 <a title="289-tfidf-5" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>Author: Lei Yuan, Jun Liu, Jieping Ye</p><p>Abstract: The group Lasso is an extension of the Lasso for feature selection on (predeﬁned) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efﬁcient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efﬁcient than existing state-of-the-art algorithms. 1</p><p>6 0.13273986 <a title="289-tfidf-6" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>7 0.12967826 <a title="289-tfidf-7" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>8 0.12631878 <a title="289-tfidf-8" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>9 0.11497479 <a title="289-tfidf-9" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>10 0.10691441 <a title="289-tfidf-10" href="./nips-2011-Penalty_Decomposition_Methods_for_Rank_Minimization.html">211 nips-2011-Penalty Decomposition Methods for Rank Minimization</a></p>
<p>11 0.097635165 <a title="289-tfidf-11" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<p>12 0.092886321 <a title="289-tfidf-12" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>13 0.091432825 <a title="289-tfidf-13" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>14 0.09090998 <a title="289-tfidf-14" href="./nips-2011-Shaping_Level_Sets_with_Submodular_Functions.html">251 nips-2011-Shaping Level Sets with Submodular Functions</a></p>
<p>15 0.089463688 <a title="289-tfidf-15" href="./nips-2011-Dimensionality_Reduction_Using_the_Sparse_Linear_Model.html">70 nips-2011-Dimensionality Reduction Using the Sparse Linear Model</a></p>
<p>16 0.086788379 <a title="289-tfidf-16" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>17 0.084245801 <a title="289-tfidf-17" href="./nips-2011-On_the_Universality_of_Online_Mirror_Descent.html">202 nips-2011-On the Universality of Online Mirror Descent</a></p>
<p>18 0.081980146 <a title="289-tfidf-18" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>19 0.081966221 <a title="289-tfidf-19" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>20 0.081750222 <a title="289-tfidf-20" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.216), (1, 0.018), (2, -0.065), (3, -0.232), (4, -0.128), (5, 0.101), (6, 0.009), (7, 0.151), (8, 0.064), (9, 0.161), (10, 0.09), (11, -0.023), (12, -0.133), (13, 0.076), (14, -0.057), (15, 0.043), (16, 0.112), (17, -0.097), (18, 0.152), (19, -0.062), (20, 0.079), (21, 0.067), (22, -0.053), (23, 0.131), (24, 0.007), (25, -0.022), (26, 0.016), (27, -0.076), (28, 0.113), (29, -0.175), (30, 0.1), (31, -0.012), (32, -0.032), (33, -0.065), (34, -0.009), (35, 0.261), (36, 0.08), (37, -0.002), (38, 0.151), (39, 0.166), (40, 0.03), (41, -0.01), (42, 0.033), (43, 0.098), (44, -0.023), (45, 0.104), (46, -0.016), (47, -0.03), (48, -0.118), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97591937 <a title="289-lsi-1" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>Author: Edouard Grave, Guillaume R. Obozinski, Francis R. Bach</p><p>Abstract: Using the 1 -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net. 1</p><p>2 0.83436954 <a title="289-lsi-2" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>Author: Feng Yan, Yuan Qi</p><p>Abstract: For many real-world applications, we often need to select correlated variables— such as genetic variations and imaging features associated with Alzheimer’s disease—in a high dimensional space. The correlation between variables presents a challenge to classical variable selection methods. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not exploit the correlation information embedded in the data to select correlated variables. To overcome this limitation, we present a novel hybrid model, EigenNet, that uses the eigenstructures of data to guide variable selection. Speciﬁcally, it integrates a sparse conditional classiﬁcation model with a generative model capturing variable correlations in a principled Bayesian framework. We develop an efﬁcient active-set algorithm to estimate the model via evidence maximization. Experimental results on synthetic data and imaging genetics data demonstrate the superior predictive performance of the EigenNet over the lasso, the elastic net, and the automatic relevance determination. 1</p><p>3 0.67554289 <a title="289-lsi-3" href="./nips-2011-Efficient_Methods_for_Overlapping_Group_Lasso.html">78 nips-2011-Efficient Methods for Overlapping Group Lasso</a></p>
<p>Author: Lei Yuan, Jun Liu, Jieping Ye</p><p>Abstract: The group Lasso is an extension of the Lasso for feature selection on (predeﬁned) non-overlapping groups of features. The non-overlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efﬁcient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. We have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efﬁcient than existing state-of-the-art algorithms. 1</p><p>4 0.66437787 <a title="289-lsi-4" href="./nips-2011-Robust_Lasso_with_missing_and_grossly_corrupted_observations.html">239 nips-2011-Robust Lasso with missing and grossly corrupted observations</a></p>
<p>Author: Nasser M. Nasrabadi, Trac D. Tran, Nam Nguyen</p><p>Abstract: This paper studies the problem of accurately recovering a sparse vector β from highly corrupted linear measurements y = Xβ + e + w where e is a sparse error vector whose nonzero entries may be unbounded and w is a bounded noise. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both β and e . Our ﬁrst result shows that the extended Lasso can faithfully recover both the regression and the corruption vectors. Our analysis is relied on a notion of extended restricted eigenvalue for the design matrix X. Our second set of results applies to a general class of Gaussian design matrix X with i.i.d rows N (0, Σ), for which we provide a surprising phenomenon: the extended Lasso can recover exact signed supports of both β and e from only Ω(k log p log n) observations, even the fraction of corruption is arbitrarily close to one. Our analysis also shows that this amount of observations required to achieve exact signed support is optimal. 1</p><p>5 0.51438695 <a title="289-lsi-5" href="./nips-2011-High-dimensional_regression_with_noisy_and_missing_data%3A_Provable_guarantees_with_non-convexity.html">118 nips-2011-High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difﬁcult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings. 1</p><p>6 0.49838108 <a title="289-lsi-6" href="./nips-2011-Sparse_recovery_by_thresholded_non-negative_least_squares.html">265 nips-2011-Sparse recovery by thresholded non-negative least squares</a></p>
<p>7 0.43375972 <a title="289-lsi-7" href="./nips-2011-Learning_Auto-regressive_Models_from_Sequence_and_Non-sequence_Data.html">144 nips-2011-Learning Auto-regressive Models from Sequence and Non-sequence Data</a></p>
<p>8 0.42511564 <a title="289-lsi-8" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>9 0.41950989 <a title="289-lsi-9" href="./nips-2011-Sparse_Estimation_with_Structured_Dictionaries.html">259 nips-2011-Sparse Estimation with Structured Dictionaries</a></p>
<p>10 0.41758138 <a title="289-lsi-10" href="./nips-2011-Greedy_Algorithms_for_Structurally_Constrained_High_Dimensional_Problems.html">108 nips-2011-Greedy Algorithms for Structurally Constrained High Dimensional Problems</a></p>
<p>11 0.40588132 <a title="289-lsi-11" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>12 0.39498466 <a title="289-lsi-12" href="./nips-2011-Learning_Higher-Order_Graph_Structure_with_Features_by_Structure_Penalty.html">146 nips-2011-Learning Higher-Order Graph Structure with Features by Structure Penalty</a></p>
<p>13 0.39454645 <a title="289-lsi-13" href="./nips-2011-On_the_accuracy_of_l1-filtering_of_signals_with_block-sparse_structure.html">203 nips-2011-On the accuracy of l1-filtering of signals with block-sparse structure</a></p>
<p>14 0.38726267 <a title="289-lsi-14" href="./nips-2011-Efficient_inference_in_matrix-variate_Gaussian_models_with_%5Ciid_observation_noise.html">83 nips-2011-Efficient inference in matrix-variate Gaussian models with \iid observation noise</a></p>
<p>15 0.38525099 <a title="289-lsi-15" href="./nips-2011-Non-parametric_Group_Orthogonal_Matching_Pursuit_for_Sparse_Learning_with_Multiple_Kernels.html">189 nips-2011-Non-parametric Group Orthogonal Matching Pursuit for Sparse Learning with Multiple Kernels</a></p>
<p>16 0.3843306 <a title="289-lsi-16" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>17 0.37005979 <a title="289-lsi-17" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>18 0.3557786 <a title="289-lsi-18" href="./nips-2011-A_More_Powerful_Two-Sample_Test_in_High_Dimensions_using_Random_Projection.html">9 nips-2011-A More Powerful Two-Sample Test in High Dimensions using Random Projection</a></p>
<p>19 0.34578857 <a title="289-lsi-19" href="./nips-2011-Sparse_Features_for_PCA-Like_Linear_Regression.html">260 nips-2011-Sparse Features for PCA-Like Linear Regression</a></p>
<p>20 0.34331036 <a title="289-lsi-20" href="./nips-2011-Sparse_Inverse_Covariance_Matrix_Estimation_Using_Quadratic_Approximation.html">262 nips-2011-Sparse Inverse Covariance Matrix Estimation Using Quadratic Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (4, 0.027), (20, 0.037), (26, 0.382), (31, 0.047), (33, 0.027), (43, 0.075), (45, 0.109), (57, 0.029), (74, 0.09), (83, 0.033), (99, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95756632 <a title="289-lda-1" href="./nips-2011-Active_Learning_Ranking_from_Pairwise_Preferences_with_Almost_Optimal_Query_Complexity.html">20 nips-2011-Active Learning Ranking from Pairwise Preferences with Almost Optimal Query Complexity</a></p>
<p>Author: Nir Ailon</p><p>Abstract: Given a set V of n elements we wish to linearly order them using pairwise preference labels which may be non-transitive (due to irrationality or arbitrary noise). The goal is to linearly order the elements while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The number of disagreements (loss) and the query complexity (number of pairwise preference labels). Our algorithm adaptively queries at most O(n poly(log n, ε−1 )) preference labels for a regret of ε times the optimal loss. This is strictly better, and often signiﬁcantly better than what non-adaptive sampling could achieve. Our main result helps settle an open problem posed by learning-to-rank (from pairwise information) theoreticians and practitioners: What is a provably correct way to sample preference labels? 1</p><p>2 0.93056929 <a title="289-lda-2" href="./nips-2011-Lower_Bounds_for_Passive_and_Active_Learning.html">162 nips-2011-Lower Bounds for Passive and Active Learning</a></p>
<p>Author: Maxim Raginsky, Alexander Rakhlin</p><p>Abstract: We develop uniﬁed information-theoretic machinery for deriving lower bounds for passive and active learning schemes. Our bounds involve the so-called Alexander’s capacity function. The supremum of this function has been recently rediscovered by Hanneke in the context of active learning under the name of “disagreement coefﬁcient.” For passive learning, our lower bounds match the upper bounds of Gin´ and Koltchinskii up to constants and generalize analogous results of Mase sart and N´ d´ lec. For active learning, we provide ﬁrst known lower bounds based e e on the capacity function rather than the disagreement coefﬁcient. 1</p><p>3 0.90123111 <a title="289-lda-3" href="./nips-2011-Simultaneous_Sampling_and_Multi-Structure_Fitting_with_Adaptive_Reversible_Jump_MCMC.html">255 nips-2011-Simultaneous Sampling and Multi-Structure Fitting with Adaptive Reversible Jump MCMC</a></p>
<p>Author: Trung T. Pham, Tat-jun Chin, Jin Yu, David Suter</p><p>Abstract: Multi-structure model ﬁtting has traditionally taken a two-stage approach: First, sample a (large) number of model hypotheses, then select the subset of hypotheses that optimise a joint ﬁtting and model selection criterion. This disjoint two-stage approach is arguably suboptimal and inefﬁcient — if the random sampling did not retrieve a good set of hypotheses, the optimised outcome will not represent a good ﬁt. To overcome this weakness we propose a new multi-structure ﬁtting approach based on Reversible Jump MCMC. Instrumental in raising the effectiveness of our method is an adaptive hypothesis generator, whose proposal distribution is learned incrementally and online. We prove that this adaptive proposal satisﬁes the diminishing adaptation property crucial for ensuring ergodicity in MCMC. Our method effectively conducts hypothesis sampling and optimisation simultaneously, and yields superior computational efﬁciency over previous two-stage methods. 1</p><p>same-paper 4 0.88928604 <a title="289-lda-4" href="./nips-2011-Trace_Lasso%3A_a_trace_norm_regularization_for_correlated_designs.html">289 nips-2011-Trace Lasso: a trace norm regularization for correlated designs</a></p>
<p>Author: Edouard Grave, Guillaume R. Obozinski, Francis R. Bach</p><p>Abstract: Using the 1 -norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net. 1</p><p>5 0.87550122 <a title="289-lda-5" href="./nips-2011-Multilinear_Subspace_Regression%3A_An_Orthogonal_Tensor_Decomposition_Approach.html">179 nips-2011-Multilinear Subspace Regression: An Orthogonal Tensor Decomposition Approach</a></p>
<p>Author: Qibin Zhao, Cesar F. Caiafa, Danilo P. Mandic, Liqing Zhang, Tonio Ball, Andreas Schulze-bonhage, Andrzej S. Cichocki</p><p>Abstract: A multilinear subspace regression model based on so called latent variable decomposition is introduced. Unlike standard regression methods which typically employ matrix (2D) data representations followed by vector subspace transformations, the proposed approach uses tensor subspace transformations to model common latent variables across both the independent and dependent data. The proposed approach aims to maximize the correlation between the so derived latent variables and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data, where for the estimation of the latent variables we introduce an algorithm based on Multilinear Singular Value Decomposition (MSVD) on a specially deﬁned cross-covariance tensor. It is next shown that in this way we are also able to unify the existing Partial Least Squares (PLS) and N-way PLS regression algorithms within the same framework. Simulations on benchmark synthetic data conﬁrm the advantages of the proposed approach, in terms of its predictive ability and robustness, especially for small sample sizes. The potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram (ECoG) from a simultaneously recorded scalp electroencephalograph (EEG). 1</p><p>6 0.71837842 <a title="289-lda-6" href="./nips-2011-Active_Ranking_using_Pairwise_Comparisons.html">22 nips-2011-Active Ranking using Pairwise Comparisons</a></p>
<p>7 0.64891732 <a title="289-lda-7" href="./nips-2011-Active_Learning_with_a_Drifting_Distribution.html">21 nips-2011-Active Learning with a Drifting Distribution</a></p>
<p>8 0.63346946 <a title="289-lda-8" href="./nips-2011-EigenNet%3A_A_Bayesian_hybrid_of_generative_and_conditional_models_for_sparse_learning.html">84 nips-2011-EigenNet: A Bayesian hybrid of generative and conditional models for sparse learning</a></p>
<p>9 0.61538023 <a title="289-lda-9" href="./nips-2011-Algorithms_and_hardness_results_for_parallel_large_margin_learning.html">29 nips-2011-Algorithms and hardness results for parallel large margin learning</a></p>
<p>10 0.61343753 <a title="289-lda-10" href="./nips-2011-Universal_low-rank_matrix_recovery_from_Pauli_measurements.html">297 nips-2011-Universal low-rank matrix recovery from Pauli measurements</a></p>
<p>11 0.60525763 <a title="289-lda-11" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>12 0.59857279 <a title="289-lda-12" href="./nips-2011-Unifying_Framework_for_Fast_Learning_Rate_of_Non-Sparse_Multiple_Kernel_Learning.html">294 nips-2011-Unifying Framework for Fast Learning Rate of Non-Sparse Multiple Kernel Learning</a></p>
<p>13 0.59597391 <a title="289-lda-13" href="./nips-2011-Projection_onto_A_Nonnegative_Max-Heap.html">226 nips-2011-Projection onto A Nonnegative Max-Heap</a></p>
<p>14 0.59579939 <a title="289-lda-14" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>15 0.59350383 <a title="289-lda-15" href="./nips-2011-Accelerated_Adaptive_Markov_Chain_for_Partition_Function_Computation.html">17 nips-2011-Accelerated Adaptive Markov Chain for Partition Function Computation</a></p>
<p>16 0.58866292 <a title="289-lda-16" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>17 0.5799666 <a title="289-lda-17" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>18 0.57800251 <a title="289-lda-18" href="./nips-2011-Online_Submodular_Set_Cover%2C_Ranking%2C_and_Repeated_Active_Learning.html">205 nips-2011-Online Submodular Set Cover, Ranking, and Repeated Active Learning</a></p>
<p>19 0.5735141 <a title="289-lda-19" href="./nips-2011-Adaptive_Hedge.html">25 nips-2011-Adaptive Hedge</a></p>
<p>20 0.57107496 <a title="289-lda-20" href="./nips-2011-Learning_unbelievable_probabilities.html">158 nips-2011-Learning unbelievable probabilities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
