<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-243" href="#">nips2011-243</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</h1>
<br/><p>Source: <a title="nips-2011-243-pdf" href="http://papers.nips.cc/paper/4346-select-and-sample-a-model-of-efficient-neural-inference-and-learning.pdf">pdf</a></p><p>Author: Jacquelyn A. Shelton, Abdul S. Sheikh, Pietro Berkes, Joerg Bornschein, Joerg Luecke</p><p>Abstract: An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. 1</p><p>Reference: <a title="nips-2011-243-reference" href="../nips2011_reference/nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. [sent-7, score-0.31]
</p><p>2 One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. [sent-8, score-0.397]
</p><p>3 Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. [sent-9, score-0.526]
</p><p>4 Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. [sent-11, score-0.587]
</p><p>5 We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. [sent-12, score-0.134]
</p><p>6 In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. [sent-13, score-0.397]
</p><p>7 The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. [sent-14, score-0.326]
</p><p>8 For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. [sent-15, score-0.326]
</p><p>9 One approach to address this problem in neural circuits is to let neuronal activity represent the parameters of a variational approximation of the real posterior [10, 11]. [sent-19, score-0.393]
</p><p>10 Another approach is to identify neurons with variables and interpret neural activity as samples from their posterior [12, 13, 3]. [sent-21, score-0.396]
</p><p>11 This interpretation is consistent with a range of experimental observations, including neural variability (which would result from the uncertainty in the posterior) and spontaneous activity (corresponding to samples from the prior in the absence of a stimulus) [3, 9]. [sent-22, score-0.21]
</p><p>12 The advantage of using sampling is that the number of neurons scales linearly with the number of variables, and it can represent arbitrarily complex posterior distributons given enough samples. [sent-23, score-0.404]
</p><p>13 Modeling studies have shown that a small number of samples are sufﬁcient to perform well on low-dimensional tasks (intuitively, this is because taking a lowdimensional marginal of the posterior accumulates samples over all dimensions) [14, 15]. [sent-25, score-0.338]
</p><p>14 As such, in order to faithfully represent visual scenes containing potentially many objects and object parts, one requires a high-dimensional latent space to represent the high number of potential causes, which returns to the problem sampling approaches face in high dimensions. [sent-27, score-0.238]
</p><p>15 The goal of the line of research pursued here is to address the following questions: 1) can we ﬁnd a sophisticated representation of the posterior for very high-dimensional hidden spaces? [sent-28, score-0.3]
</p><p>16 In this paper we propose a novel approach to approximate inference and learning that addresses the drawbacks of sampling as a neural processing model, yet maintains its beneﬁcial posterior representation and neural plausibility. [sent-30, score-0.397]
</p><p>17 We show that sampling can be combined with a preselection of candidate units. [sent-31, score-0.469]
</p><p>18 Our combined approach emerges naturally by interpreting feedforward selection and sampling as approximations to exact inference in a probabilistic framework for perception. [sent-33, score-0.357]
</p><p>19 2  A Select and Sample Approach to Approximate Inference  Inference and learning in neural circuits can be regarded as the task of inferring the true hidden causes of a stimulus. [sent-34, score-0.25]
</p><p>20 , yD ), and we will refer to the hidden causes (the objects) as s = (s1 , . [sent-39, score-0.175]
</p><p>21 , sH ) with sh denoting hidden variable or hidden unit h. [sent-42, score-0.589]
</p><p>22 If we assume that the data distribution can be optimally modeled by the generative distribution for optimal parameters Θ∗ , then the posterior probability p(s | y, Θ∗ ) represents optimal inference given a data point y. [sent-44, score-0.255]
</p><p>23 EM iteratively optimizes a lower bound of the data likelihood by inferring the posterior distribution over hidden variables given the current parameters (the E-step), and then adjusting the parameters to maximize the likelihood of the data averaged over this posterior (the M-step). [sent-50, score-0.579]
</p><p>24 The M-step updates typically depend only on a small number of expectation values of the posterior as given by g(s)  p(s | y (n) ,Θ)  =  s  p(s | y (n) , Θ) g(s) ,  (1)  where g(s) is usually an elementary function of the hidden variables (e. [sent-51, score-0.378]
</p><p>25 The EM iterations can be associated with neural processing by the assumption that neural activity represents the posterior over hidden variables (E-step), and that synaptic plasticity implements changes to model parameters (M-step). [sent-60, score-0.439]
</p><p>26 Here we will consider two prominent models of neural processing on the ground of approximations to the expectation values (1) and show how they can be combined. [sent-61, score-0.16]
</p><p>27 One perspective on this early component of neural activity is as a preselection of candidate units or hypotheses for a given sensory stimulus ([18, 21, 26, 19] and many more), with the goal of reducing the computational demand of an otherwise too complex computation. [sent-64, score-0.6]
</p><p>28 In the context of probabilistic approaches, it has recently been shown that preselection can be formulated as a variational approximation to exact inference [27]. [sent-65, score-0.545]
</p><p>29 The variational distribution in this case is given by a truncated sum over possible hidden states: p(s | y (n) , Θ) ≈ qn (s; Θ) =  p(s | y (n) , Θ) p(s | y (n) , Θ)  s ∈Kn  δ(s ∈ Kn ) =  p(s, y (n) | Θ) p(s , y (n) | Θ)  δ(s ∈ Kn ) (2)  s ∈Kn  where δ(s ∈ Kn ) = 1 if s ∈ Kn and zero otherwise. [sent-66, score-0.464]
</p><p>30 2 results in good approximations to the posterior if Kn contains most posterior mass. [sent-69, score-0.364]
</p><p>31 Since for many applications the posterior mass is concentrated in small volumes of the state space, the approximation quality can stay high even for relatively small sets Kn . [sent-70, score-0.293]
</p><p>32 (3) g(s) p(s | y (n) ,Θ) ≈ g(s) qn (s;Θ) = s∈Kn (n) | Θ) s ∈Kn p(s , y Eqn. [sent-72, score-0.258]
</p><p>33 As such, a selection function Sh (y, Θ) needs to be carefully chosen in order to deﬁne Kn ; Sh (y, Θ) efﬁciently selects the candidate units sh that are most likely to have contributed to a data point y (n) . [sent-75, score-0.442]
</p><p>34 Kn can then be deﬁned by: Kn = {s | for all h ∈ I : sh = 0} ,  (4)  where I contains the H indices h with the highest values of Sh (y, Θ) (compare Fig. [sent-76, score-0.317]
</p><p>35 Often the precise form of Sh (y, Θ) has limited inﬂuence on the ﬁnal approximation accuracy because a) its values are not used for the approximation (3) itself and b) the size of sets Kn can often be chosen generously to easily contain the regions with large posterior mass. [sent-82, score-0.23]
</p><p>36 For Kn equal to the entire state space, no selection is required and the approximations (2) and (3) fall back to the case of exact inference. [sent-84, score-0.176]
</p><p>37 1 is by sampling from the posterior distribution, and using the samples to compute the average: M 1 g(s) p(s | y (n) ,Θ) ≈ M m=1 g(s(m) ) with s(m) ∼ p(s | y, Θ). [sent-87, score-0.343]
</p><p>38 This class of methods draws samples from the posterior distribution such that each subsequent sample is drawn relative to the current state, and the resulting sequence of samples form a Markov chain. [sent-90, score-0.381]
</p><p>39 B Graphical model showing each connection Wdh between the observed variables y and hidden variables s, and how H = 2 hidden variables/units are selected to form a set Kn . [sent-95, score-0.307]
</p><p>40 C Graphical model resulting from the selection of hidden variables and associated weights Wdh (black). [sent-96, score-0.193]
</p><p>41 The combined approach is thus given by: g(s)  p(s | y (n) ,Θ)  ≈ g(s)  qn (s;Θ)  ≈  1 M  M m=1  g(s(m) )  with  s(m) ∼ qn (s; Θ),  (6)  where s(m) denote samples from the truncated distribution qn . [sent-103, score-0.899]
</p><p>42 Instead of drawing from a distribution over the entire state space, approximation (6) requires only samples from a potentially very small subspace Kn (Fig. [sent-104, score-0.151]
</p><p>43 Compared to selection alone, the select and sample approach will represent an increase in efﬁciency as soon as the number of samples required for a good approximation is less then the number of states in Kn . [sent-107, score-0.374]
</p><p>44 3  Sparse Coding: An Example Application  We systematically investigate the computational efﬁciency, performance, and biological plausibility of the select and sample approach in comparison with selection and sampling alone using a sparse coding model of images. [sent-108, score-0.493]
</p><p>45 The choice of a sparse coding model has numerous advantages. [sent-109, score-0.134]
</p><p>46 Here we consider a discrete variant of this model known as Binary Sparse Coding (BSC; [29, 27], also compare [33]), which has binary hidden variables but otherwise the same features as standard sparse coding versions. [sent-114, score-0.27]
</p><p>47 The generative model for BSC is expressed by 1−sh H p(s|π) = h=1 π sh 1 − π , p(y|s, W, σ) = N (y; W s, σ 2 1) , (7) where W ∈ RD×H denotes the basis vectors and π parameterizes the sparsity (s and y as above). [sent-115, score-0.41]
</p><p>48 An EM algorithm without approximations is obtained if we use the exact posterior for the expectations: qn = p(s | y (n) , Θ). [sent-121, score-0.51]
</p><p>49 An algorithm that more efﬁciently scales with the number of hidden dimensions is obtained by applying preselection. [sent-128, score-0.225]
</p><p>50 For the BSC model we use qn as given in (3) and Kn = {s | (for all h ∈ I : sh = 0) or h sh = 1}. [sent-129, score-0.892]
</p><p>51 Note that (10) can be related to a deterministic ICA-like selection of a hidden state s(n) in the limit case of no noise (compare [27]). [sent-135, score-0.251]
</p><p>52 Gibbs sampling is an MCMC algorithm which systematically explores the sample space by repeatedly drawing samples from the conditional distributions of the individual hidden dimensions. [sent-139, score-0.358]
</p><p>53 Speciﬁcally, the posterior probability computed for each candidate sample is expressed by: p(sh = 1, s\h , y)β , (11) p(sh = 1 | s\h , y) = p(sh = 0, s\h , y)β + p(sh = 1, s\h , y)β where we have introduced a parameter β that allows for smoothing of the posterior distribution. [sent-145, score-0.402]
</p><p>54 To ensure an appropriate mixing behavior of the MCMC chains over a wide range of σ (note that T σ is a model parameter that changes with learning), we deﬁne β = σ2 , where T is a temperature parameter that is set manually and selected such that good mixing is achieved. [sent-146, score-0.135]
</p><p>55 The samples drawn in this manner can then be used to approximate the expectation values in (8) to (9) using (5). [sent-147, score-0.165]
</p><p>56 The EM learning algorithm given by combining selection and sampling is obtained by applying (6). [sent-149, score-0.149]
</p><p>57 First note that inserting the BSC generative model into (2) results in: qn (s; Θ)  = s  N (y; W s, σ 2 1) BernoulliKn (s; π) δ(s ∈ Kn ) 2 ∈Kn N (y; W s , σ 1) BernoulliKn (s ; π)  (12)  where BernoulliKn (s; π) = h∈I π sh (1 − π)1−sh . [sent-150, score-0.617]
</p><p>58 If we deﬁne s to be the binary vector consisting of all entries of s of the selected ˜ ∈ RD×H contains all basis functions of those selected, we observe that the dimensions, and if W distribution is equal to the posterior w. [sent-152, score-0.25]
</p><p>59 a BSC model with H instead of H hidden dimensions: ˜ ˜˜ N (y; W s, σ 2 1 H ) Bernoulli(s; π) ˜ p(s | y, Θ) = ˜ ˜˜ 2 ˜ s N (y; W s , σ 1 H ) Bernoulli(s ; π) Instead of drawing samples from qn (s; Θ) we can thus draw samples from the exact posterior w. [sent-155, score-0.784]
</p><p>60 The sampling procedure for BSCsample can thus be applied simply by ignoring the non-selected dimensions and their associated parameters. [sent-159, score-0.15]
</p><p>61 For different data points, different latent dimensions will be selected such that averaging over data points can update all model parameters. [sent-160, score-0.129]
</p><p>62 For selection we again use Sh (y, Θ) (10), deﬁning Kn as in (4), where I now contains the H –2 indices h with the highest values of Sh (y, Θ) and two randomly selected dimensions (drawn from a uniform distribution over all non-selected dimensions). [sent-161, score-0.15]
</p><p>63 The two randomly selected dimensions fulﬁll the same purpose as the inclusion of singleton states for BSCselect . [sent-162, score-0.149]
</p><p>64 Preselection and Gibbs sampling on the selected dimensions deﬁne an approximation to the required expectation values (3) and result in an EM algorithm referred to as BSCs+s . [sent-163, score-0.296]
</p><p>65 Collecting the number of operations necessary to compute the expectation values for all four BSC cases, we arrive at O N S( D + 1 + H ) (13) p(s,y)  s  ssT  where S denotes the number of hidden states that contribute to the calculation of the expectation values. [sent-165, score-0.348]
</p><p>66 For the approaches with preselection (BSCselect , BSCs+s ), all the calculations of the expectation values can be performed on the reduced latent space; therefore the H is replaced by H . [sent-166, score-0.422]
</p><p>67 For BSCexact this number scales exponentially in H: S exact = 2H , and in in the BSCselect case, it scales exponentially in the number of preselected hidden variables: S select = 2H . [sent-167, score-0.356]
</p><p>68 However, for the sampling based approaches (BSCsample and BSCs+s ), the number S directly corresponds to the number of samples to be evaluated and is obtained empirically. [sent-168, score-0.179]
</p><p>69 4  Numerical Experiments  We compare the select and sample approach with selection and sampling applied individually on different data sets: artiﬁcal images and natural image patches. [sent-170, score-0.298]
</p><p>70 For all experiments using the two sampling approaches, we draw 20 independent chains that are initialized at random states in order to increase the mixing of the samples. [sent-171, score-0.181]
</p><p>71 Our ﬁrst set of experiments investigate the select and sample approach’s convergence properties on artiﬁcial data sets where ground truth is available. [sent-174, score-0.134]
</p><p>72 For models using preselection (BSCselect and BSCs+s ), we set H to 6, effectively halving the number of hidden variables participating in the calculation of the expectation values. [sent-197, score-0.522]
</p><p>73 For BSCsample and BSCs+s we drew 200 samples from the posterior p(s | y (n) ) of each data point, as such the number of states evaluated totaled S sample = 200 × H = 2400 and S s+s = 200 × H = 1200, respectively. [sent-198, score-0.35]
</p><p>74 Comparing the computational costs of algorithms shows the beneﬁts of preselection already for this small scale problem: while BSCexact evaluates the expectation values using the full set of 2H = 4096 hidden 6  states, BSCselect only considers 2H + (H − H ) = 70 states. [sent-202, score-0.522]
</p><p>75 We test the select and sample approach on natural image data at a more challenging scale, to include biological plausibility in the demonstration of its applicability to larger scale problems. [sent-205, score-0.176]
</p><p>76 47e7  B  C  am  B  # of states  A  Figure 3: Experiments on image patches with D = 26 × 26, H = 800 and H = 20. [sent-214, score-0.146]
</p><p>77 B Random selection of learned basis functions (number of samples set to 200). [sent-216, score-0.195]
</p><p>78 We observe with BSCs+s that 200 samples per hidden dimension (total states = 200 × H ) are sufﬁcient: the ﬁnal value of the likelihood after 100 EM steps begins to saturate. [sent-222, score-0.322]
</p><p>79 Particularly, increasing the number of samples does not increase the likelihood by more than 1%. [sent-223, score-0.13]
</p><p>80 In another set of experiments, we used this number of samples (200 × H) in the pure sampling case (BSCsample ) in order to monitor the likelihood behavior. [sent-226, score-0.222]
</p><p>81 Comparison of the above results shows that the most efﬁcient algorithm is obtained by a combination of preselection and sampling, our select and sample approach (BSCs+s ), with no or only minimal effect on the performance of the algorithm – as depicted in Fig. [sent-231, score-0.416]
</p><p>82 To demonstrate the efﬁciency of the combined approach we applied BSCs+s to the same image dataset, but with a very high number of observed and hidden dimensions. [sent-234, score-0.215]
</p><p>83 BSCs+s was applied with the number of hidden units set to H = 1, 600 and with H = 34. [sent-236, score-0.173]
</p><p>84 Using the same conditions as in the previous experiments (notably S = 200 × H = 64, 000 samples and 100 EM iterations) we again obtain a set of Gabor-like basis functions (see Fig. [sent-237, score-0.138]
</p><p>85 To our knowledge, the presented results illustrate the largest application of sparse coding with a reasonably complete representation of the posterior. [sent-240, score-0.134]
</p><p>86 5  Discussion  We have introduced a novel and efﬁcient method for unsupervised learning in probabilistic models – one which maintains a complex representation of the posterior for problems consistent with 2 We restricted the set of images to 900 images without man-made structures (see Fig 3A). [sent-241, score-0.229]
</p><p>87 A random selection of the inferred basis functions is shown (see Suppl for all basis functions and model parameters). [sent-243, score-0.159]
</p><p>88 Furthermore, our approach is biologically plausible and models how the brain can make sense of its environment for large-scale sensory inputs. [sent-247, score-0.129]
</p><p>89 Speciﬁcally, the method could be implemented in neural networks using two mechanisms, both of which have been independently suggested in the context of a statistical framework for perception: feed-forward preselection [27], and sampling [12, 13, 3]. [sent-248, score-0.446]
</p><p>90 We used a sparse coding model of natural images – a standard model for neural response properties in V1 [22, 31] – in order to investigate, both numerically and analytically, the applicability and efﬁciency of the method. [sent-253, score-0.18]
</p><p>91 Comparisons of our approach with exact inference, selection alone, and sampling alone showed a very favorable scaling with the number of observed and hidden dimensions. [sent-254, score-0.386]
</p><p>92 To the best of our knowledge, the only other sparse coding implementation that reached a comparable problem size (D = 20 × 20, H = 2 000) assumed a Laplace prior and used a MAP estimation of the posterior [23]. [sent-255, score-0.298]
</p><p>93 Our method does not require any of these artiﬁcial mechanisms because of its rich posterior representation. [sent-257, score-0.201]
</p><p>94 Concretely, we used a sparse coding model with binary latent variables. [sent-259, score-0.17]
</p><p>95 In the model, the selection step results in a simple, local and neurally plausible integration of input data, given by (10). [sent-261, score-0.155]
</p><p>96 We used this in combination with Gibbs sampling, which is also neurally plausible because neurons can individually sample their next state based on the current state of the other neurons, as transmitted through recurrent connections [15]. [sent-262, score-0.287]
</p><p>97 Both approaches are different from selecting subspaces prior to sampling and are more difﬁcult to link to neural feed-forward sweeps [18, 21]. [sent-266, score-0.138]
</p><p>98 We expect the select and sample strategy to be widely applicable to machine learning models whenever the posterior probability masses can be expected to be concentrated in a small sub-space of the whole latent space. [sent-267, score-0.338]
</p><p>99 Using more sophisticated preselection mechanisms and sampling schemes could lead to a further reduction in computational efforts, although the details will depend in general on the particular model and input data. [sent-268, score-0.437]
</p><p>100 Interpreting neural response variability as Monte Carlo sampling from the posterior. [sent-379, score-0.138]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bscs', 0.349), ('kn', 0.339), ('sh', 0.317), ('preselection', 0.308), ('qn', 0.258), ('bscselect', 0.205), ('bsc', 0.164), ('bscexact', 0.164), ('bscsample', 0.164), ('posterior', 0.164), ('wdh', 0.144), ('em', 0.136), ('hidden', 0.136), ('frankfurt', 0.103), ('wh', 0.093), ('berkes', 0.093), ('sampling', 0.092), ('coding', 0.088), ('samples', 0.087), ('sst', 0.082), ('expectation', 0.078), ('variational', 0.07), ('select', 0.065), ('bernoullikn', 0.062), ('gt', 0.061), ('dimensions', 0.058), ('selection', 0.057), ('neurally', 0.056), ('states', 0.056), ('bornschein', 0.054), ('perception', 0.054), ('neuroscience', 0.053), ('mcmc', 0.053), ('sensory', 0.053), ('neurons', 0.052), ('exact', 0.052), ('basis', 0.051), ('alone', 0.049), ('patches', 0.049), ('inference', 0.049), ('activity', 0.047), ('stimulus', 0.046), ('neural', 0.046), ('sparse', 0.046), ('yd', 0.044), ('visual', 0.044), ('sample', 0.043), ('likelihood', 0.043), ('generative', 0.042), ('plausible', 0.042), ('preselected', 0.041), ('rner', 0.041), ('image', 0.041), ('ciency', 0.04), ('causes', 0.039), ('uential', 0.039), ('combined', 0.038), ('mechanisms', 0.037), ('receptive', 0.037), ('units', 0.037), ('approximations', 0.036), ('latent', 0.036), ('puertas', 0.036), ('trends', 0.036), ('mass', 0.035), ('selected', 0.035), ('temperature', 0.034), ('brain', 0.034), ('monte', 0.034), ('mixing', 0.033), ('smax', 0.033), ('riesenhuber', 0.033), ('turner', 0.033), ('orban', 0.033), ('approximation', 0.033), ('cognitive', 0.033), ('probabilistic', 0.033), ('represent', 0.033), ('recurrent', 0.032), ('complex', 0.032), ('candidate', 0.031), ('state', 0.031), ('outstanding', 0.031), ('latham', 0.031), ('hateren', 0.031), ('sc', 0.031), ('scales', 0.031), ('concentrated', 0.03), ('spontaneous', 0.03), ('sci', 0.03), ('inferring', 0.029), ('thousand', 0.028), ('carlo', 0.028), ('gibbs', 0.028), ('plausibility', 0.027), ('nature', 0.027), ('noise', 0.027), ('investigate', 0.026), ('dog', 0.026), ('annealing', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="243-tfidf-1" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>Author: Jacquelyn A. Shelton, Abdul S. Sheikh, Pietro Berkes, Joerg Bornschein, Joerg Luecke</p><p>Abstract: An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. 1</p><p>2 0.11901204 <a title="243-tfidf-2" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>3 0.11683421 <a title="243-tfidf-3" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>4 0.10333376 <a title="243-tfidf-4" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>Author: Yan Karklin, Eero P. Simoncelli</p><p>Abstract: Efﬁcient coding provides a powerful principle for explaining early sensory coding. Most attempts to test this principle have been limited to linear, noiseless models, and when applied to natural images, have yielded oriented ﬁlters consistent with responses in primary visual cortex. Here we show that an efﬁcient coding model that incorporates biologically realistic ingredients – input and output noise, nonlinear response functions, and a metabolic cost on the ﬁring rate – predicts receptive ﬁelds and response nonlinearities similar to those observed in the retina. Speciﬁcally, we develop numerical methods for simultaneously learning the linear ﬁlters and response nonlinearities of a population of model neurons, so as to maximize information transmission subject to metabolic costs. When applied to an ensemble of natural images, the method yields ﬁlters that are center-surround and nonlinearities that are rectifying. The ﬁlters are organized into two populations, with On- and Off-centers, which independently tile the visual space. As observed in the primate retina, the Off-center neurons are more numerous and have ﬁlters with smaller spatial extent. In the absence of noise, our method reduces to a generalized version of independent components analysis, with an adapted nonlinear “contrast” function; in this case, the optimal ﬁlters are localized and oriented.</p><p>5 0.10232388 <a title="243-tfidf-5" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>Author: Neil D. Lawrence, Michalis K. Titsias, Andreas Damianou</p><p>Abstract: High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences. 1</p><p>6 0.098517865 <a title="243-tfidf-6" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>7 0.098386645 <a title="243-tfidf-7" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>8 0.097947553 <a title="243-tfidf-8" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>9 0.087640889 <a title="243-tfidf-9" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>10 0.086189307 <a title="243-tfidf-10" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>11 0.085631944 <a title="243-tfidf-11" href="./nips-2011-Structured_sparse_coding_via_lateral_inhibition.html">276 nips-2011-Structured sparse coding via lateral inhibition</a></p>
<p>12 0.082036793 <a title="243-tfidf-12" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>13 0.081925817 <a title="243-tfidf-13" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>14 0.076999925 <a title="243-tfidf-14" href="./nips-2011-Sparse_Filtering.html">261 nips-2011-Sparse Filtering</a></p>
<p>15 0.076738179 <a title="243-tfidf-15" href="./nips-2011-Learning_in_Hilbert_vs._Banach_Spaces%3A_A_Measure_Embedding_Viewpoint.html">152 nips-2011-Learning in Hilbert vs. Banach Spaces: A Measure Embedding Viewpoint</a></p>
<p>16 0.074162595 <a title="243-tfidf-16" href="./nips-2011-Active_learning_of_neural_response_functions_with_Gaussian_processes.html">24 nips-2011-Active learning of neural response functions with Gaussian processes</a></p>
<p>17 0.072111487 <a title="243-tfidf-17" href="./nips-2011-Infinite_Latent_SVM_for_Classification_and_Multi-task_Learning.html">134 nips-2011-Infinite Latent SVM for Classification and Multi-task Learning</a></p>
<p>18 0.07179451 <a title="243-tfidf-18" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>19 0.071009926 <a title="243-tfidf-19" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>20 0.070465364 <a title="243-tfidf-20" href="./nips-2011-Learning_to_Learn_with_Compound_HD_Models.html">156 nips-2011-Learning to Learn with Compound HD Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, 0.094), (2, 0.123), (3, 0.023), (4, -0.007), (5, -0.051), (6, 0.072), (7, 0.003), (8, 0.027), (9, -0.024), (10, -0.074), (11, -0.108), (12, 0.033), (13, -0.038), (14, 0.003), (15, -0.001), (16, -0.016), (17, -0.054), (18, -0.035), (19, 0.035), (20, 0.077), (21, -0.004), (22, -0.003), (23, 0.014), (24, -0.041), (25, 0.01), (26, 0.068), (27, 0.009), (28, 0.077), (29, 0.035), (30, -0.009), (31, -0.003), (32, -0.079), (33, -0.153), (34, -0.047), (35, -0.018), (36, 0.019), (37, -0.018), (38, -0.057), (39, 0.0), (40, -0.064), (41, -0.005), (42, -0.011), (43, 0.011), (44, -0.127), (45, -0.035), (46, 0.088), (47, -0.013), (48, -0.003), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93174946 <a title="243-lsi-1" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>Author: Jacquelyn A. Shelton, Abdul S. Sheikh, Pietro Berkes, Joerg Bornschein, Joerg Luecke</p><p>Abstract: An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. 1</p><p>2 0.69212461 <a title="243-lsi-2" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>Author: David P. Reichert, Peggy Series, Amos J. Storkey</p><p>Abstract: It has been argued that perceptual multistability reﬂects probabilistic inference performed by the brain when sensory input is ambiguous. Alternatively, more traditional explanations of multistability refer to low-level mechanisms such as neuronal adaptation. We employ a Deep Boltzmann Machine (DBM) model of cortical processing to demonstrate that these two different approaches can be combined in the same framework. Based on recent developments in machine learning, we show how neuronal adaptation can be understood as a mechanism that improves probabilistic, sampling-based inference. Using the ambiguous Necker cube image, we analyze the perceptual switching exhibited by the model. We also examine the inﬂuence of spatial attention, and explore how binocular rivalry can be modeled with the same approach. Our work joins earlier studies in demonstrating how the principles underlying DBMs relate to cortical processing, and offers novel perspectives on the neural implementation of approximate probabilistic inference in the brain. 1</p><p>3 0.63379592 <a title="243-lsi-3" href="./nips-2011-Nonnegative_dictionary_learning_in_the_exponential_noise_model_for_adaptive_music_signal_representation.html">191 nips-2011-Nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</a></p>
<p>Author: Onur Dikmen, Cédric Févotte</p><p>Abstract: In this paper we describe a maximum likelihood approach for dictionary learning in the multiplicative exponential noise model. This model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram. Maximum joint likelihood estimation of the dictionary and expansion coefﬁcients leads to a nonnegative matrix factorization problem where the Itakura-Saito divergence is used. The optimality of this approach is in question because the number of parameters (which include the expansion coefﬁcients) grows with the number of observations. In this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefﬁcients have been integrated out (given a speciﬁc prior). We compare the output of both maximum joint likelihood estimation (i.e., standard Itakura-Saito NMF) and maximum marginal likelihood estimation (MMLE) on real and synthetical datasets. The MMLE approach is shown to embed automatic model order selection, akin to automatic relevance determination.</p><p>4 0.62221628 <a title="243-lsi-4" href="./nips-2011-Generalized_Beta_Mixtures_of_Gaussians.html">104 nips-2011-Generalized Beta Mixtures of Gaussians</a></p>
<p>Author: Artin Armagan, Merlise Clyde, David B. Dunson</p><p>Abstract: In recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems. In general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional Cauchy and double exponential priors. We ﬁrst propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases. This encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections. We then develop a class of variational Bayes approximations through the new hierarchy presented that will scale more efﬁciently to the types of truly massive data sets that are now encountered routinely. 1</p><p>5 0.62106526 <a title="243-lsi-5" href="./nips-2011-Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning.html">269 nips-2011-Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</a></p>
<p>Author: Miguel Lázaro-gredilla, Michalis K. Titsias</p><p>Abstract: We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-speciﬁc sparse weights, thus inducing relation between tasks. This model uniﬁes several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multioutput Gaussian process regression, multi-class classiﬁcation, image processing applications and collaborative ﬁltering. 1</p><p>6 0.61960012 <a title="243-lsi-6" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>7 0.61142796 <a title="243-lsi-7" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>8 0.59027362 <a title="243-lsi-8" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>9 0.56872076 <a title="243-lsi-9" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>10 0.56024814 <a title="243-lsi-10" href="./nips-2011-Practical_Variational_Inference_for_Neural_Networks.html">217 nips-2011-Practical Variational Inference for Neural Networks</a></p>
<p>11 0.55445963 <a title="243-lsi-11" href="./nips-2011-Identifying_Alzheimer%27s_Disease-Related_Brain_Regions_from_Multi-Modality_Neuroimaging_Data_using_Sparse_Composite_Linear_Discrimination_Analysis.html">125 nips-2011-Identifying Alzheimer's Disease-Related Brain Regions from Multi-Modality Neuroimaging Data using Sparse Composite Linear Discrimination Analysis</a></p>
<p>12 0.54580098 <a title="243-lsi-12" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>13 0.53994012 <a title="243-lsi-13" href="./nips-2011-Automated_Refinement_of_Bayes_Networks%27_Parameters_based_on_Test_Ordering_Constraints.html">40 nips-2011-Automated Refinement of Bayes Networks' Parameters based on Test Ordering Constraints</a></p>
<p>14 0.5372805 <a title="243-lsi-14" href="./nips-2011-Nonstandard_Interpretations_of_Probabilistic_Programs_for_Efficient_Inference.html">192 nips-2011-Nonstandard Interpretations of Probabilistic Programs for Efficient Inference</a></p>
<p>15 0.53330338 <a title="243-lsi-15" href="./nips-2011-Collective_Graphical_Models.html">55 nips-2011-Collective Graphical Models</a></p>
<p>16 0.52318114 <a title="243-lsi-16" href="./nips-2011-Sparse_Bayesian_Multi-Task_Learning.html">258 nips-2011-Sparse Bayesian Multi-Task Learning</a></p>
<p>17 0.52283865 <a title="243-lsi-17" href="./nips-2011-Efficient_coding_of_natural_images_with_a_population_of_noisy_Linear-Nonlinear_neurons.html">82 nips-2011-Efficient coding of natural images with a population of noisy Linear-Nonlinear neurons</a></p>
<p>18 0.51716161 <a title="243-lsi-18" href="./nips-2011-Variational_Learning_for_Recurrent_Spiking_Networks.html">302 nips-2011-Variational Learning for Recurrent Spiking Networks</a></p>
<p>19 0.50593245 <a title="243-lsi-19" href="./nips-2011-Unsupervised_learning_models_of_primary_cortical_receptive_fields_and_receptive_field_plasticity.html">298 nips-2011-Unsupervised learning models of primary cortical receptive fields and receptive field plasticity</a></p>
<p>20 0.50556493 <a title="243-lsi-20" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (4, 0.025), (20, 0.032), (26, 0.022), (31, 0.156), (33, 0.012), (43, 0.064), (45, 0.087), (57, 0.036), (65, 0.037), (74, 0.063), (76, 0.244), (83, 0.07), (84, 0.02), (99, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77662158 <a title="243-lda-1" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>Author: Jacquelyn A. Shelton, Abdul S. Sheikh, Pietro Berkes, Joerg Bornschein, Joerg Luecke</p><p>Abstract: An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difﬁculty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two inﬂuential proposals of efﬁcient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efﬁciency of this approach on a sparse coding model. In numerical experiments on artiﬁcial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efﬁcient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions. 1</p><p>2 0.76163465 <a title="243-lda-2" href="./nips-2011-Speedy_Q-Learning.html">268 nips-2011-Speedy Q-Learning</a></p>
<p>Author: Mohammad Ghavamzadeh, Hilbert J. Kappen, Mohammad G. Azar, Rémi Munos</p><p>Abstract: We introduce a new convergent variant of Q-learning, called speedy Q-learning (SQL), to address the problem of slow convergence in the standard form of the Q-learning algorithm. We prove a PAC bound on the performance of SQL, which shows that for an MDP with n state-action pairs and the discount factor γ only T = O log(n)/(ǫ2 (1 − γ)4 ) steps are required for the SQL algorithm to converge to an ǫ-optimal action-value function with high probability. This bound has a better dependency on 1/ǫ and 1/(1 − γ), and thus, is tighter than the best available result for Q-learning. Our bound is also superior to the existing results for both modelfree and model-based instances of batch Q-value iteration that are considered to be more efﬁcient than the incremental methods like Q-learning. 1</p><p>3 0.74256945 <a title="243-lda-3" href="./nips-2011-Hierarchically_Supervised_Latent_Dirichlet_Allocation.html">116 nips-2011-Hierarchically Supervised Latent Dirichlet Allocation</a></p>
<p>Author: Adler J. Perotte, Frank Wood, Noemie Elhadad, Nicholas Bartlett</p><p>Abstract: We introduce hierarchically supervised latent Dirichlet allocation (HSLDA), a model for hierarchically and multiply labeled bag-of-word data. Examples of such data include web pages and their placement in directories, product descriptions and associated categories from product hierarchies, and free-text clinical records and their assigned diagnosis codes. Out-of-sample label prediction is the primary goal of this work, but improved lower-dimensional representations of the bagof-word data are also of interest. We demonstrate HSLDA on large-scale data from clinical document labeling and retail product categorization tasks. We show that leveraging the structure from hierarchical labels improves out-of-sample label prediction substantially when compared to models that do not. 1</p><p>4 0.65276301 <a title="243-lda-4" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>Author: Johanni Brea, Walter Senn, Jean-pascal Pfister</p><p>Abstract: We consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns. Given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences. We show that learning synaptic weights towards hidden neurons signiﬁcantly improves the storing capacity of the network. Furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with Spike-Timing Dependent Plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</p><p>5 0.64902139 <a title="243-lda-5" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>Author: Biljana Petreska, Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: Simultaneous recordings of many neurons embedded within a recurrentlyconnected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function. In principle, these dynamics might be identiﬁed by purely unsupervised, statistical means. Here, we show that a Hidden Switching Linear Dynamical Systems (HSLDS) model— in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process—is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements. The regimes are identiﬁed without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial. The HSLDS model also performs better than recent comparable models in predicting the ﬁring rate of an isolated neuron based on the ﬁring rates of others, suggesting that it captures more of the “shared variance” of the data. Thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reﬂect its computational role. 1</p><p>6 0.6404798 <a title="243-lda-6" href="./nips-2011-Structural_equations_and_divisive_normalization_for_energy-dependent_component_analysis.html">273 nips-2011-Structural equations and divisive normalization for energy-dependent component analysis</a></p>
<p>7 0.63721979 <a title="243-lda-7" href="./nips-2011-Inferring_spike-timing-dependent_plasticity_from_spike_train_data.html">133 nips-2011-Inferring spike-timing-dependent plasticity from spike train data</a></p>
<p>8 0.63665682 <a title="243-lda-8" href="./nips-2011-Empirical_models_of_spiking_in_neural_populations.html">86 nips-2011-Empirical models of spiking in neural populations</a></p>
<p>9 0.63599825 <a title="243-lda-9" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>10 0.63540334 <a title="243-lda-10" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>11 0.63506973 <a title="243-lda-11" href="./nips-2011-Comparative_Analysis_of_Viterbi_Training_and_Maximum_Likelihood_Estimation_for_HMMs.html">57 nips-2011-Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs</a></p>
<p>12 0.63297379 <a title="243-lda-12" href="./nips-2011-Variational_Gaussian_Process_Dynamical_Systems.html">301 nips-2011-Variational Gaussian Process Dynamical Systems</a></p>
<p>13 0.62904823 <a title="243-lda-13" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>14 0.62853879 <a title="243-lda-14" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>15 0.62843883 <a title="243-lda-15" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>16 0.62789869 <a title="243-lda-16" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>17 0.62725514 <a title="243-lda-17" href="./nips-2011-Analytical_Results_for_the_Error_in_Filtering_of_Gaussian_Processes.html">37 nips-2011-Analytical Results for the Error in Filtering of Gaussian Processes</a></p>
<p>18 0.62676895 <a title="243-lda-18" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>19 0.62381577 <a title="243-lda-19" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>20 0.62334597 <a title="243-lda-20" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
