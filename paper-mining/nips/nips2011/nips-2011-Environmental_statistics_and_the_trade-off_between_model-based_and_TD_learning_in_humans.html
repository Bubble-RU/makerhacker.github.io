<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-88" href="#">nips2011-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</h1>
<br/><p>Source: <a title="nips-2011-88-pdf" href="http://papers.nips.cc/paper/4243-environmental-statistics-and-the-trade-off-between-model-based-and-td-learning-in-humans.pdf">pdf</a></p><p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>Reference: <a title="nips-2011-88-reference" href="../nips2011_reference/nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Environmental statistics and the trade-off between model-based and TD learning in humans Dylan A. [sent-1, score-0.087]
</p><p>2 edu  Abstract There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. [sent-6, score-0.175]
</p><p>3 Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. [sent-7, score-0.181]
</p><p>4 Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. [sent-8, score-0.489]
</p><p>5 Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. [sent-9, score-0.169]
</p><p>6 We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. [sent-10, score-0.145]
</p><p>7 The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. [sent-11, score-0.27]
</p><p>8 1  Introduction  There are many suggestions that humans and other animals employ multiple approaches to learned decision making [1]. [sent-13, score-0.232]
</p><p>9 Psychologists primarily distinguish between declarative rule learning and more incremental learning of stimulus-response (S–R) habits across a broad range of tasks [3, 4]. [sent-16, score-0.121]
</p><p>10 They have shown that large problem spaces, probabilistic feedback (as in the weather prediction task), and difﬁcult to verbalize rules (as in information integration tasks from category learning) all seem to promote the use of a habit learning system [5, 6, 7, 8, 9]. [sent-17, score-0.127]
</p><p>11 For instance, under different task circumstances or following different brain lesions, rats are more or less willing to continue working for a devalued food reward [11]. [sent-20, score-0.649]
</p><p>12 model-free RL [12, 13]: a world model permits updating a policy following a change in food value, while model-free methods preclude this. [sent-22, score-0.154]
</p><p>13 1  Intuitively, S–R habits correspond well to the policies learned by TD methods such as actor/critic [14, 15], and rule-based cognitive planning strategies seem to mirror model-based algorithms. [sent-23, score-0.11]
</p><p>14 , verbalizable rules) posited in the declarative system seem obviously related to the model-use distinction. [sent-30, score-0.194]
</p><p>15 Here we suggest that an explanation for this mismatch may follow from the circumstances under which each RL approach dominates. [sent-33, score-0.116]
</p><p>16 It has previously been proposed that model-free and modelbased reasoning should be traded off according to their relative statistical efﬁciency (proxied by uncertainty) in different circumstances [13]. [sent-34, score-0.229]
</p><p>17 In fact, what ultimately matters to a decision-maker is relative advantage in terms of reward [18]. [sent-35, score-0.502]
</p><p>18 Focusing speciﬁcally on task statistics, we extend the uncertainty framework to investigate under what circumstances the performance of a model-based system excels sufﬁciently to make it worthwhile. [sent-36, score-0.292]
</p><p>19 When the environment is completely static, TD is well known to converge to the optimal policy almost as quickly as model-based approaches [19], and so environmental change must be key to understanding its computational disadvantages. [sent-37, score-0.175]
</p><p>20 This process introduces additional noise to the sampling process which must be averaged over, as observational deviations resulting from the learner’s own choice variability or transition stochasticity in the environment are confounded with variability in immediate rewards. [sent-40, score-0.255]
</p><p>21 We provide a more formal argument of this observation in MDPs with dynamic rewards and static transitions, and ﬁnd that the environments in which TD is most impaired are those with frequent changes and little noise. [sent-43, score-0.153]
</p><p>22 This suggests a strategy by which these two approaches should optimally trade-off, which we test empirically using a decision task in humans while manipulating reward statistics. [sent-44, score-0.598]
</p><p>23 This may explain why a model-based system is associated with or perhaps specialized for rapid, declarative rule learning. [sent-46, score-0.142]
</p><p>24 2  Theory  Model-free and model-based methods differ in their strategies for estimating action values from samples. [sent-47, score-0.189]
</p><p>25 One key disadvantage of Monte Carlo sampling of long-run values in an MDP, relative to model-based RL (in which immediate rewards are sampled and aggregated according to the sampled transition dynamics), is the need to average samples over both reward and state transition stochasticity. [sent-48, score-0.909]
</p><p>26 This impairs its ability to track changes in the underlying MDP, with the disadvantage most pronounced in situations of high volatility and low noise. [sent-49, score-0.218]
</p><p>27 Each action leads to one of the two states with equal probability, and each of the four terminal states is associated with a reward. [sent-52, score-0.089]
</p><p>28 The rewards are stochastic and diffusing, according to a Gaussian process, and the transitions are ﬁxed. [sent-53, score-0.143]
</p><p>29 We consider the uncertainty and reward achievable as a function of the volatility and observation noise. [sent-54, score-0.62]
</p><p>30 We have here made some simpliﬁcations in order to make the intuition as clear as possible: 2  that each trajectory has only a single state transition and reward; that in the steady state the static transition matrix has been fully learned; and that all analyzed distributions are Gaussian. [sent-55, score-0.33]
</p><p>31 , reward or value) at ¯ ˆ ¯ time t, X refers to the (latent) true mean of X, and X refers to estimates of X made by the learning process. [sent-60, score-0.399]
</p><p>32 2  Value estimation  Consider the value of one of the actions in our two-action MDP which leads to state A or B. [sent-80, score-0.11]
</p><p>33 If each reward is changing according to 2 the Gaussian diffusion process described above, this will induce a change process on Q. [sent-82, score-0.522]
</p><p>34 A modelˆ ˆ based system that has fully learned the transition dynamics will be able to estimate R(A) and R(B) ˆ separately, and thus take the expectation to produce Q. [sent-83, score-0.168]
</p><p>35 By assuming each reward is sampled equally often and adopting the appropriate effective , the resulting uncertainty of this expectation, UMB , follows Equation 4, with X = Q. [sent-84, score-0.451]
</p><p>36 On the other hand, a Monte Carlo system that must take samples over transitions will observe Q = 2 ¯ ¯ R(A) or Q = R(B). [sent-85, score-0.115]
</p><p>37 If R(A) R(B) = d, it will observe an additional variance of d from the 4 mixture of the two reward distributions. [sent-86, score-0.399]
</p><p>38 The relative advantage of the model-based (MB) approach, c(UMB ) 3  MB–TD advantage (probability) 0. [sent-91, score-0.156]
</p><p>39 5  "  Figure 1: Difference in theoretical success rate between MB and MC  c(UMC ), is plotted in Figure 1 for an arbitrary reward deviation d = 1. [sent-117, score-0.399]
</p><p>40 As expected, as either the volatility or noise parameter gets very large and the task gets harder, the uncertainty increases, performance approaches chance, and the relative advantage vanishes. [sent-118, score-0.443]
</p><p>41 We compared the performance of a model-based approach using value iteration with a ﬁxed, optimal reward learning rate and transition counting (MB) against various model-free algorithms including Q(0), SARSA(0), and SARSA(1) (with ﬁxed optimal learning rates), all using a discount factor of = 0. [sent-128, score-0.494]
</p><p>42 As expected, all learners showed a decrement in reward as increased. [sent-130, score-0.468]
</p><p>43 Figure 2 shows the difference in mean reward obtained between MB and SARSA(0). [sent-131, score-0.399]
</p><p>44 The correspondence between the theoretical results and the simulation conﬁrms that the theoretical ﬁndings do hold more generally, and we claim that the same underlying effects drive these results. [sent-133, score-0.1]
</p><p>45 6  Figure 2: Difference in reward obtained between MB and SARSA(0)  4  Human behavior  Human subjects performed a decision task that represented an MDP with 4 states and 2 actions. [sent-158, score-0.622]
</p><p>46 The rewards followed the same contractive Gaussian diffusion process used in section 3, with and " parameters varied across subjects. [sent-159, score-0.199]
</p><p>47 We sought changes in the reliance on model-based and model-free strategies via regressions of past events onto current choices [21]. [sent-160, score-0.211]
</p><p>48 All participants gave informed consent and the study was approved by the human subjects ethics board of the institute. [sent-167, score-0.111]
</p><p>49 2  Task  Subjects viewed a graphical representation of a rotating disc with four pairs of colored squares equally spaced around the edge. [sent-170, score-0.115]
</p><p>50 Each of the two squares in a state represented an action (a 2 A = {L, R}), and had a left- or right-directed icon. [sent-172, score-0.159]
</p><p>51 During the task, only the top quadrant of the disc was visible at any time, and at decision time subjects could select the left or right action by pressing the left or right arrow button on a keyboard. [sent-173, score-0.422]
</p><p>52 Immediately after selecting an action, between zero and ﬁve coins (including a pie-fraction of a coin) appeared under the selected action square, representing a reward (R 2 [0, 5]). [sent-174, score-0.488]
</p><p>53 After 600 ms, the disc began rotating and the reward became slowly obscured over the next 1150 ms until a new pair of squares was at the top of the disc and the next decision could be entered, as seen in Figure 3. [sent-175, score-0.69]
</p><p>54 The state dynamics were determined by a ﬁxed transition function (T : S ⇥ A ! [sent-176, score-0.165]
</p><p>55 A) such that each action was most likely to lead to the next adjacent state along the edge of the disc (e. [sent-177, score-0.274]
</p><p>56 The reward distribution followed the same Gaussian process given in the previous sections, except shifted and trimmed. [sent-182, score-0.399]
</p><p>57 5, 0), 5) 5  Figure 3: Abstract task layout and screen shot shortly after a choice is made (yellow box indicates visible display): Each state has two actions, right (red) and left (blue), which lead to the indicated state with 70% probability, and otherwise to another state at random. [sent-189, score-0.261]
</p><p>58 Each action also results in a reward of 0–5 coins. [sent-190, score-0.488]
</p><p>59 Each subject was ﬁrst trained on the transition and reward dynamics of the task, including 16 ob¯ servations of reward samples where the latent value R was shown so as to get a feeling for both the change and noise processes. [sent-191, score-1.026]
</p><p>60 3  Analysis  Because they use different sampling strategies to estimate action values, TD and model-based RL differ in their predictions of how experience with states and rewards should affect subsequent choices. [sent-207, score-0.29]
</p><p>61 By conﬁning our analyses to the most recent samples we remain agnostic about free parameters with non-linear effects such as learning rates and discount factors, but rather measure the relative strength of reliance on either sort of evidence directly using a general linear model. [sent-210, score-0.299]
</p><p>62 Accordingly, below we deﬁne explanatory variables that capture the most recently experienced reward sample that would be relevant to a choice under either Q(1) TD or model-based planning. [sent-212, score-0.446]
</p><p>63 The data for each subject were considered to be the sequence of states visited, St , actions taken, At , and rewards received, Rt . [sent-213, score-0.141]
</p><p>64 We deﬁne additional vector time sequences a, j, r, q, and p, each indexed by time and state and referred to generally as xt (s), with all x0 initially undeﬁned. [sent-214, score-0.156]
</p><p>65 Note that these vectors are step functions, such that each value is updated (xt 6= xt 1 ) only when a relevant observation is made. [sent-217, score-0.086]
</p><p>66 6  Given the task dynamics, we can consider how a TD-based Q-learning system and a model-based planning system would compute values. [sent-219, score-0.197]
</p><p>67 Both take into account the last sample of the immediate reward, rt . [sent-220, score-0.235]
</p><p>68 They differ in how they account for the reward from the “next state”: either, for Q(1), as qt (the last reward received from the state visited after the last visit to St ) or, for model-based RL, as pt (the last sample of the reward at the true successor state). [sent-221, score-1.59]
</p><p>69 That is, while TD(1) will incorporate the reward observed following Rt , regardless of the state, a model-based system will instead consider the expected successor state [21]. [sent-222, score-0.634]
</p><p>70 Also, we required that subjects have at least 50 (10%) switch trials to be included. [sent-230, score-0.243]
</p><p>71 First we examined the main effects with a regression including ﬁxed effects of interest for r, r0 , q ⇤ , p⇤ , and random effects of no interest for r, q, and p (without covariances). [sent-231, score-0.3]
</p><p>72 Next, we ran a regression adding all the interactions between the condition variables ( , ") and the speciﬁc reward effects (q ⇤ , p⇤ ). [sent-232, score-0.499]
</p><p>73 Finally, we additionally included the interaction between change in reward on the previous trial (d) and the speciﬁc reward effects. [sent-233, score-0.905]
</p><p>74 2  Results  A total of 5 subjects failed to meet the inclusion criterion of 50 switch trials (in each case because they pressed the same button on almost all trials), leaving 500 decision trials from each of 50 subjects. [sent-235, score-0.428]
</p><p>75 Subjects were observed to switch on 143 ± 55 trials (mean ± 1 SD). [sent-236, score-0.132]
</p><p>76 The number of trials in which TD and model-based disagreed as to the most recent relevant sample of the next-state reward (r0 = 0) was 243 ± 26, and for 181 ± 19 of these, it was due to a more recent visit to the next state. [sent-238, score-0.522]
</p><p>77 Beyond the trivial effects of perseveration and reward, subjects showed a substantial amount of TDtype learning (q ⇤ > 0), and a smaller but signiﬁcant amount of model-based lookahead (p⇤ > 0). [sent-240, score-0.263]
</p><p>78 The interactions of these effects by condition demonstrated that subjects in higher drift conditions showed signiﬁcantly less TD ( ⇥q ⇤ < 0) but unreduced model-based learning ( ⇥p⇤ ), possibly due to the relative disadvantage of TD with increased drift. [sent-241, score-0.31]
</p><p>79 Similarly, higher noise conditions showed decreased model-based effects (" ⇥ p⇤ < 0) and no change in TD, which may be driven by the decreasing advantage of MB. [sent-242, score-0.286]
</p><p>80 Note that, since the (nonsigniﬁcant) trend on the unaffected variable is positive, it is unlikely that either interaction effect results from a nonspeciﬁc change in performance or the “noisiness” of choices. [sent-243, score-0.111]
</p><p>81 Both of these effects are consistent with the pattern of differential reliance predicted by the theoretical analysis. [sent-244, score-0.205]
</p><p>82 The effect of change on the previous trial (d) provides one hint as to how subjects may adjust their reliance on either system dynamically: higher changes are indicative of noisier environments which are thus expected to promote TD learning. [sent-245, score-0.545]
</p><p>83 0022  description perseveration last immediate r common next r TD(1) next-step r model predicted r TD with change model with change TD with noise model with noise TD after change model after change  presented. [sent-264, score-0.498]
</p><p>84 Model-based methods, while always superior to TD in terms of performance, have the largest advantage in the presence of change paired with low environmental noise, because the Monte Carlo sampling strategy of TD interferes with tracking fast change. [sent-265, score-0.179]
</p><p>85 If the additional costs of modelbased computation are ﬁxed, this would motivate employing the system only in the regime where its advantage was most pronounced [18]. [sent-266, score-0.189]
</p><p>86 Consistent with this, human behavior exhibited relatively larger use of model-based RL with increased reward volatility and lesser use of it with increased observation noise. [sent-267, score-0.568]
</p><p>87 Of course, increasing either the volatility or noise parameters makes the task harder, and a decline in the marker for either sort of learning, as we observed, implies an overall decrement in performance. [sent-268, score-0.357]
</p><p>88 However, as the decrement was speciﬁc to one or the other explanatory variable, this may also be interpreted as a relative increase in use of the unaffected strategy. [sent-269, score-0.212]
</p><p>89 Thus a decrease in learning rate for either system may be confounded with a decrease in the strength of its effect in our analysis. [sent-271, score-0.115]
</p><p>90 However, while the optimal learning rates are also predicted to differ between conditions, these predictions are common to both systems, and it seems unlikely that each would differentially adjust its learning rate in response to a different manipulation. [sent-272, score-0.125]
</p><p>91 The characteristics associated with these learning systems in psychology can be seen as consequences of the relative strengths of model-based and model-free learning. [sent-273, score-0.097]
</p><p>92 If the model-based system is most useful in conditions of low noise and high volatility, then the appropriate learning rates for such a system are large: there is less need and utility to take multiple samples for the purpose of averaging. [sent-274, score-0.214]
</p><p>93 In this case of a high learning rate, model-based learning is closely aligned with singleshot episodic encoding, possibly subsuming such a system [17], as well as with learning categorical, verbalizable rules in the psychological sense, rather than averages. [sent-275, score-0.2]
</p><p>94 This may also explain the selective engagement of putatively model-based brain regions such as the dorsolateral prefrontal cortex in tasks with less stochastic outcomes [24]. [sent-276, score-0.129]
</p><p>95 The speciﬁc advantage of high learning rates may well motivate the brain to use a restricted modelbased system, such as one with learning rate ﬁxed to 1. [sent-278, score-0.159]
</p><p>96 Indeed (see Supplemental materials), this restriction has little detriment on the system’s advantage over TD in the circumstances where it would be expected to be used, but causes drastic performance problems as observation noise increases, since averaging over samples is then required. [sent-279, score-0.237]
</p><p>97 Conversely, evaluations in a model based system are extremely costly when transitions are highly stochastic, since averages must be computed over exponentially many paths, while they add no cost to model-free learning. [sent-285, score-0.115]
</p><p>98 Category number impacts rule-based but not information-integration category learning: Further evidence for dissociable categorylearning systems. [sent-326, score-0.122]
</p><p>99 Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. [sent-350, score-0.132]
</p><p>100 Brain mechanism of reward prediction under predictable and unpredictable environmental dynamics. [sent-410, score-0.46]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reward', 0.399), ('td', 0.39), ('rt', 0.185), ('volatility', 0.169), ('rl', 0.144), ('mb', 0.138), ('circumstances', 0.116), ('disc', 0.115), ('subjects', 0.111), ('reliance', 0.105), ('nathaniel', 0.104), ('netw', 0.104), ('rewards', 0.101), ('st', 0.101), ('effects', 0.1), ('transition', 0.095), ('sarsa', 0.094), ('successor', 0.092), ('action', 0.089), ('humans', 0.087), ('xt', 0.086), ('daw', 0.084), ('dissociable', 0.078), ('umb', 0.078), ('umc', 0.078), ('trials', 0.078), ('episodic', 0.075), ('system', 0.073), ('mixed', 0.071), ('state', 0.07), ('behav', 0.069), ('decrement', 0.069), ('declarative', 0.069), ('noise', 0.068), ('change', 0.065), ('modelbased', 0.063), ('qt', 0.061), ('mdp', 0.061), ('decision', 0.061), ('environmental', 0.061), ('strategies', 0.058), ('diffusion', 0.058), ('mdps', 0.058), ('switch', 0.054), ('promote', 0.054), ('advantage', 0.053), ('balleine', 0.052), ('doherty', 0.052), ('habits', 0.052), ('hum', 0.052), ('maddox', 0.052), ('perseveration', 0.052), ('poldrack', 0.052), ('raymond', 0.052), ('seymour', 0.052), ('unde', 0.052), ('verbalizable', 0.052), ('environments', 0.052), ('uncertainty', 0.052), ('task', 0.051), ('relative', 0.05), ('peter', 0.05), ('immediate', 0.05), ('policy', 0.049), ('disadvantage', 0.049), ('regressions', 0.048), ('psychology', 0.047), ('explanatory', 0.047), ('dorsolateral', 0.046), ('striatal', 0.046), ('bernard', 0.046), ('button', 0.046), ('mem', 0.046), ('organ', 0.046), ('unaffected', 0.046), ('visit', 0.045), ('animals', 0.044), ('evidence', 0.044), ('adjust', 0.043), ('mc', 0.043), ('dayan', 0.043), ('brain', 0.043), ('transitions', 0.042), ('trial', 0.042), ('differ', 0.042), ('visited', 0.042), ('instrumental', 0.042), ('ben', 0.042), ('conceived', 0.042), ('confounded', 0.042), ('pt', 0.041), ('actions', 0.04), ('jump', 0.04), ('suggestions', 0.04), ('differentially', 0.04), ('food', 0.04), ('nat', 0.04), ('prefrontal', 0.04), ('contractive', 0.04), ('friston', 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="88-tfidf-1" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>2 0.41166645 <a title="88-tfidf-2" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>Author: George Konidaris, Scott Niekum, Philip S. Thomas</p><p>Abstract: We show that the λ-return target used in the TD(λ) family of algorithms is the maximum likelihood estimator for a speciﬁc model of how the variance of an nstep return estimate increases with n. We introduce the γ-return estimator, an alternative target based on a more accurate model of variance, which deﬁnes the TDγ family of complex-backup temporal difference learning algorithms. We derive TDγ , the γ-return equivalent of the original TD(λ) algorithm, which eliminates the λ parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length. We then derive a second algorithm, TDγ (C), with a capacity parameter C. TDγ (C) requires C times more time and memory than TD(λ) and is incremental and online. We show that TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains, and that TDγ (C) performs as well as or better than TDγ for intermediate settings of C. 1</p><p>3 0.3742058 <a title="88-tfidf-3" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>Author: J. Z. Kolter</p><p>Abstract: Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling. It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation. In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well. In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case. Furthermore, we show that we can efﬁciently project on to this convex set using only samples generated from the system. The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods. 1</p><p>4 0.25070056 <a title="88-tfidf-4" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an inﬁnite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms. 1</p><p>5 0.19678469 <a title="88-tfidf-5" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>Author: Sergey Levine, Zoran Popovic, Vladlen Koltun</p><p>Abstract: We present a probabilistic algorithm for nonlinear inverse reinforcement learning. The goal of inverse reinforcement learning is to learn the reward function in a Markov decision process from expert demonstrations. While most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use Gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert’s policy. Our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions. 1</p><p>6 0.19350421 <a title="88-tfidf-6" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>7 0.18279879 <a title="88-tfidf-7" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>8 0.13665599 <a title="88-tfidf-8" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>9 0.13559282 <a title="88-tfidf-9" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>10 0.13341366 <a title="88-tfidf-10" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>11 0.13340464 <a title="88-tfidf-11" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>12 0.12509312 <a title="88-tfidf-12" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>13 0.12380784 <a title="88-tfidf-13" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>14 0.11073456 <a title="88-tfidf-14" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>15 0.10024027 <a title="88-tfidf-15" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>16 0.099032521 <a title="88-tfidf-16" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>17 0.097558193 <a title="88-tfidf-17" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>18 0.094621144 <a title="88-tfidf-18" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>19 0.094394431 <a title="88-tfidf-19" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>20 0.088562235 <a title="88-tfidf-20" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, -0.234), (2, 0.187), (3, 0.255), (4, -0.242), (5, 0.048), (6, 0.034), (7, -0.048), (8, 0.037), (9, 0.006), (10, 0.176), (11, -0.0), (12, 0.177), (13, 0.158), (14, 0.171), (15, 0.188), (16, -0.105), (17, -0.145), (18, -0.036), (19, -0.022), (20, 0.04), (21, 0.038), (22, -0.085), (23, 0.056), (24, 0.089), (25, 0.02), (26, 0.018), (27, -0.074), (28, -0.011), (29, 0.012), (30, -0.012), (31, -0.05), (32, -0.037), (33, -0.031), (34, 0.025), (35, -0.048), (36, -0.028), (37, -0.047), (38, 0.018), (39, -0.016), (40, 0.059), (41, -0.074), (42, -0.063), (43, -0.036), (44, 0.031), (45, -0.138), (46, 0.078), (47, 0.023), (48, -0.024), (49, -0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94770044 <a title="88-lsi-1" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>2 0.82157207 <a title="88-lsi-2" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>Author: George Konidaris, Scott Niekum, Philip S. Thomas</p><p>Abstract: We show that the λ-return target used in the TD(λ) family of algorithms is the maximum likelihood estimator for a speciﬁc model of how the variance of an nstep return estimate increases with n. We introduce the γ-return estimator, an alternative target based on a more accurate model of variance, which deﬁnes the TDγ family of complex-backup temporal difference learning algorithms. We derive TDγ , the γ-return equivalent of the original TD(λ) algorithm, which eliminates the λ parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length. We then derive a second algorithm, TDγ (C), with a capacity parameter C. TDγ (C) requires C times more time and memory than TD(λ) and is incremental and online. We show that TDγ outperforms TD(λ) for any setting of λ on 4 out of 5 benchmark domains, and that TDγ (C) performs as well as or better than TDγ for intermediate settings of C. 1</p><p>3 0.76454479 <a title="88-lsi-3" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>Author: J. Z. Kolter</p><p>Abstract: Off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of Reinforcement Learning, and in recent years there has been much work on developing Temporal Different (TD) algorithms that are guaranteed to converge under off-policy sampling. It has remained an open question, however, whether anything can be said a priori about the quality of the TD solution when off-policy sampling is employed with function approximation. In general the answer is no: for arbitrary off-policy sampling the error of the TD solution can be unboundedly large, even when the approximator can represent the true value function well. In this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case. Furthermore, we show that we can efﬁciently project on to this convex set using only samples generated from the system. The end result is a novel TD algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing TD methods. 1</p><p>4 0.68828052 <a title="88-lsi-4" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>Author: Mehdi Keramati, Boris S. Gutkin</p><p>Abstract: Reinforcement learning models address animal’s behavioral adaptation to its changing “external” environment, and are based on the assumption that Pavlovian, habitual and goal-directed responses seek to maximize reward acquisition. Negative-feedback models of homeostatic regulation, on the other hand, are concerned with behavioral adaptation in response to the “internal” state of the animal, and assume that animals’ behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints. Building upon the drive-reduction theory of reward, we propose a new analytical framework that integrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identical. The proposed theory shows behavioral adaptation to both internal and external states in a disciplined way. We further show that the proposed framework allows for a uniﬁed explanation of some behavioral pattern like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.</p><p>5 0.56350571 <a title="88-lsi-5" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: The difﬁculty in inverse reinforcement learning (IRL) arises in choosing the best reward function since there are typically an inﬁnite number of reward functions that yield the given behaviour data as optimal. Using a Bayesian framework, we address this challenge by using the maximum a posteriori (MAP) estimation for the reward function, and show that most of the previous IRL algorithms can be modeled into our framework. We also present a gradient method for the MAP estimation based on the (sub)differentiability of the posterior distribution. We show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms. 1</p><p>6 0.53501111 <a title="88-lsi-6" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>7 0.49948028 <a title="88-lsi-7" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>8 0.45052147 <a title="88-lsi-8" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>9 0.41734588 <a title="88-lsi-9" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>10 0.4156833 <a title="88-lsi-10" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>11 0.3953602 <a title="88-lsi-11" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>12 0.38818839 <a title="88-lsi-12" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>13 0.38067135 <a title="88-lsi-13" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>14 0.37637964 <a title="88-lsi-14" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>15 0.37076464 <a title="88-lsi-15" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>16 0.37060508 <a title="88-lsi-16" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>17 0.35985252 <a title="88-lsi-17" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>18 0.3465161 <a title="88-lsi-18" href="./nips-2011-Blending_Autonomous_Exploration_and_Apprenticeship_Learning.html">48 nips-2011-Blending Autonomous Exploration and Apprenticeship Learning</a></p>
<p>19 0.34112522 <a title="88-lsi-19" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>20 0.34009665 <a title="88-lsi-20" href="./nips-2011-An_Unsupervised_Decontamination_Procedure_For_Improving_The_Reliability_Of_Human_Judgments.html">34 nips-2011-An Unsupervised Decontamination Procedure For Improving The Reliability Of Human Judgments</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.014), (4, 0.03), (20, 0.014), (26, 0.01), (31, 0.608), (33, 0.011), (43, 0.036), (45, 0.057), (57, 0.034), (65, 0.014), (74, 0.042), (83, 0.027), (99, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97753066 <a title="88-lda-1" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>2 0.97616202 <a title="88-lda-2" href="./nips-2011-Facial_Expression_Transfer_with_Input-Output_Temporal_Restricted_Boltzmann_Machines.html">94 nips-2011-Facial Expression Transfer with Input-Output Temporal Restricted Boltzmann Machines</a></p>
<p>Author: Matthew D. Zeiler, Graham W. Taylor, Leonid Sigal, Iain Matthews, Rob Fergus</p><p>Abstract: We present a type of Temporal Restricted Boltzmann Machine that deﬁnes a probability distribution over an output sequence conditional on an input sequence. It shares the desirable properties of RBMs: efﬁcient exact inference, an exponentially more expressive latent state than HMMs, and the ability to model nonlinear structure and dynamics. We apply our model to a challenging real-world graphics problem: facial expression transfer. Our results demonstrate improved performance over several baselines modeling high-dimensional 2D and 3D data. 1</p><p>3 0.9712826 <a title="88-lda-3" href="./nips-2011-Probabilistic_amplitude_and_frequency_demodulation.html">225 nips-2011-Probabilistic amplitude and frequency demodulation</a></p>
<p>Author: Richard Turner, Maneesh Sahani</p><p>Abstract: A number of recent scientiﬁc and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time. Although signal processing provides algorithms for so-called amplitude- and frequencydemodulation (AFD), there are well known problems with all of the existing methods. Motivated by the fact that AFD is ill-posed, we approach the problem using probabilistic inference. The new approach, called probabilistic amplitude and frequency demodulation (PAFD), models instantaneous frequency using an auto-regressive generalization of the von Mises distribution, and the envelopes using Gaussian auto-regressive dynamics with a positivity constraint. A novel form of expectation propagation is used for inference. We demonstrate that although PAFD is computationally demanding, it outperforms previous approaches on synthetic and real signals in clean, noisy and missing data settings. 1</p><p>4 0.9464339 <a title="88-lda-4" href="./nips-2011-Active_dendrites%3A_adaptation_to_spike-based_communication.html">23 nips-2011-Active dendrites: adaptation to spike-based communication</a></p>
<p>Author: Balazs B. Ujfalussy, Máté Lengyel</p><p>Abstract: Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment ﬂuctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity. 1</p><p>5 0.91185361 <a title="88-lda-5" href="./nips-2011-Iterative_Learning_for_Reliable_Crowdsourcing_Systems.html">137 nips-2011-Iterative Learning for Reliable Crowdsourcing Systems</a></p>
<p>Author: David R. Karger, Sewoong Oh, Devavrat Shah</p><p>Abstract: Crowdsourcing systems, in which tasks are electronically distributed to numerous “information piece-workers”, have emerged as an effective paradigm for humanpowered solving of large scale problems in domains such as image classiﬁcation, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase conﬁdence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting. In this paper, we consider a general model of such crowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers’ answers. We show that our algorithm signiﬁcantly outperforms majority voting and, in fact, is asymptotically optimal through comparison to an oracle that knows the reliability of every worker. 1</p><p>6 0.90263814 <a title="88-lda-6" href="./nips-2011-Robust_Multi-Class_Gaussian_Process_Classification.html">240 nips-2011-Robust Multi-Class Gaussian Process Classification</a></p>
<p>7 0.81136364 <a title="88-lda-7" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>8 0.80023938 <a title="88-lda-8" href="./nips-2011-Sequence_learning_with_hidden_units_in_spiking_neural_networks.html">249 nips-2011-Sequence learning with hidden units in spiking neural networks</a></p>
<p>9 0.78544807 <a title="88-lda-9" href="./nips-2011-Inference_in_continuous-time_change-point_models.html">131 nips-2011-Inference in continuous-time change-point models</a></p>
<p>10 0.7674765 <a title="88-lda-10" href="./nips-2011-Dynamical_segmentation_of_single_trials_from_population_neural_data.html">75 nips-2011-Dynamical segmentation of single trials from population neural data</a></p>
<p>11 0.76294917 <a title="88-lda-11" href="./nips-2011-Energetically_Optimal_Action_Potentials.html">87 nips-2011-Energetically Optimal Action Potentials</a></p>
<p>12 0.76128715 <a title="88-lda-12" href="./nips-2011-Query-Aware_MCMC.html">229 nips-2011-Query-Aware MCMC</a></p>
<p>13 0.75670964 <a title="88-lda-13" href="./nips-2011-Priors_over_Recurrent_Continuous_Time_Processes.html">221 nips-2011-Priors over Recurrent Continuous Time Processes</a></p>
<p>14 0.75450331 <a title="88-lda-14" href="./nips-2011-Select_and_Sample_-_A_Model_of_Efficient_Neural_Inference_and_Learning.html">243 nips-2011-Select and Sample - A Model of Efficient Neural Inference and Learning</a></p>
<p>15 0.748757 <a title="88-lda-15" href="./nips-2011-Two_is_better_than_one%3A_distinct_roles_for_familiarity_and_recollection_in_retrieving_palimpsest_memories.html">292 nips-2011-Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</a></p>
<p>16 0.74788713 <a title="88-lda-16" href="./nips-2011-Anatomically_Constrained_Decoding_of_Finger_Flexion_from_Electrocorticographic_Signals.html">38 nips-2011-Anatomically Constrained Decoding of Finger Flexion from Electrocorticographic Signals</a></p>
<p>17 0.73492521 <a title="88-lda-17" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>18 0.7340166 <a title="88-lda-18" href="./nips-2011-Neuronal_Adaptation_for_Sampling-Based_Probabilistic_Inference_in_Perceptual_Bistability.html">184 nips-2011-Neuronal Adaptation for Sampling-Based Probabilistic Inference in Perceptual Bistability</a></p>
<p>19 0.72107267 <a title="88-lda-19" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>20 0.71590769 <a title="88-lda-20" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
