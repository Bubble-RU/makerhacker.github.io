<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2011" href="../home/nips2011_home.html">nips2011</a> <a title="nips-2011-88" href="#">nips2011-88</a> knowledge-graph by maker-knowledge-mining</p><h1>88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</h1>
<br/><p>Source: <a title="nips-2011-88-pdf" href="http://papers.nips.cc/paper/4243-environmental-statistics-and-the-trade-off-between-model-based-and-td-learning-in-humans.pdf">pdf</a></p><p>Author: Dylan A. Simon, Nathaniel D. Daw</p><p>Abstract: There is much evidence that humans and other animals utilize a combination of model-based and model-free RL methods. Although it has been proposed that these systems may dominate according to their relative statistical efﬁciency in different circumstances, there is little speciﬁc evidence — especially in humans — as to the details of this trade-off. Accordingly, we examine the relative performance of different RL approaches under situations in which the statistics of reward are differentially noisy and volatile. Using theory and simulation, we show that model-free TD learning is relatively most disadvantaged in cases of high volatility and low noise. We present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions. The statistical circumstances favoring model-based RL are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rulebased vs. incremental learning. 1</p><p>Reference: <a title="nips-2011-88-reference" href="../nips2011_reference/nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reward', 0.496), ('td', 0.418), ('rt', 0.199), ('volatil', 0.17), ('rl', 0.154), ('mb', 0.148), ('circumst', 0.125), ('transit', 0.117), ('disc', 0.113), ('dissocy', 0.113), ('nathaniel', 0.112), ('netw', 0.112), ('st', 0.108), ('sars', 0.101), ('habit', 0.098), ('hum', 0.095), ('environ', 0.093), ('xt', 0.092), ('daw', 0.09), ('umb', 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="88-tfidf-1" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>2 0.46472296 <a title="88-tfidf-2" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>3 0.42668134 <a title="88-tfidf-3" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>4 0.32310611 <a title="88-tfidf-4" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>5 0.31259763 <a title="88-tfidf-5" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>6 0.26223052 <a title="88-tfidf-6" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>7 0.24321224 <a title="88-tfidf-7" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>8 0.22961318 <a title="88-tfidf-8" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>9 0.16705883 <a title="88-tfidf-9" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>10 0.15851042 <a title="88-tfidf-10" href="./nips-2011-Predicting_response_time_and_error_rates_in_visual_search.html">219 nips-2011-Predicting response time and error rates in visual search</a></p>
<p>11 0.14625892 <a title="88-tfidf-11" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>12 0.1382152 <a title="88-tfidf-12" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<p>13 0.13339968 <a title="88-tfidf-13" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>14 0.11538025 <a title="88-tfidf-14" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>15 0.11430388 <a title="88-tfidf-15" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>16 0.11347233 <a title="88-tfidf-16" href="./nips-2011-Online_Learning%3A_Stochastic%2C_Constrained%2C_and_Smoothed_Adversaries.html">204 nips-2011-Online Learning: Stochastic, Constrained, and Smoothed Adversaries</a></p>
<p>17 0.11034556 <a title="88-tfidf-17" href="./nips-2011-Committing_Bandits.html">56 nips-2011-Committing Bandits</a></p>
<p>18 0.10791631 <a title="88-tfidf-18" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>19 0.09574388 <a title="88-tfidf-19" href="./nips-2011-Action-Gap_Phenomenon_in_Reinforcement_Learning.html">18 nips-2011-Action-Gap Phenomenon in Reinforcement Learning</a></p>
<p>20 0.094311073 <a title="88-tfidf-20" href="./nips-2011-Transfer_from_Multiple_MDPs.html">291 nips-2011-Transfer from Multiple MDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, -0.121), (2, -0.302), (3, -0.352), (4, 0.208), (5, 0.025), (6, -0.068), (7, 0.046), (8, 0.101), (9, 0.021), (10, -0.221), (11, 0.084), (12, 0.103), (13, -0.08), (14, -0.132), (15, -0.079), (16, 0.284), (17, -0.164), (18, 0.047), (19, -0.107), (20, 0.057), (21, -0.007), (22, 0.073), (23, 0.036), (24, -0.08), (25, -0.008), (26, 0.049), (27, -0.017), (28, 0.075), (29, 0.07), (30, 0.0), (31, -0.001), (32, 0.038), (33, -0.055), (34, -0.1), (35, 0.035), (36, 0.048), (37, 0.02), (38, -0.018), (39, 0.08), (40, 0.044), (41, 0.043), (42, -0.019), (43, 0.039), (44, 0.013), (45, -0.02), (46, 0.066), (47, -0.027), (48, 0.018), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92526788 <a title="88-lsi-1" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>2 0.77942526 <a title="88-lsi-2" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>3 0.73054647 <a title="88-lsi-3" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>4 0.72929043 <a title="88-lsi-4" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>5 0.63338244 <a title="88-lsi-5" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>6 0.61216789 <a title="88-lsi-6" href="./nips-2011-Nonlinear_Inverse_Reinforcement_Learning_with_Gaussian_Processes.html">190 nips-2011-Nonlinear Inverse Reinforcement Learning with Gaussian Processes</a></p>
<p>7 0.50957268 <a title="88-lsi-7" href="./nips-2011-Selecting_the_State-Representation_in_Reinforcement_Learning.html">245 nips-2011-Selecting the State-Representation in Reinforcement Learning</a></p>
<p>8 0.48446545 <a title="88-lsi-8" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>9 0.46558449 <a title="88-lsi-9" href="./nips-2011-From_Bandits_to_Experts%3A_On_the_Value_of_Side-Observations.html">98 nips-2011-From Bandits to Experts: On the Value of Side-Observations</a></p>
<p>10 0.42763826 <a title="88-lsi-10" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>11 0.42653236 <a title="88-lsi-11" href="./nips-2011-A_Non-Parametric_Approach_to_Dynamic_Programming.html">10 nips-2011-A Non-Parametric Approach to Dynamic Programming</a></p>
<p>12 0.40433362 <a title="88-lsi-12" href="./nips-2011-Optimal_Reinforcement_Learning_for_Gaussian_Systems.html">206 nips-2011-Optimal Reinforcement Learning for Gaussian Systems</a></p>
<p>13 0.36342037 <a title="88-lsi-13" href="./nips-2011-Convergent_Fitted_Value_Iteration_with_Linear_Function_Approximation.html">65 nips-2011-Convergent Fitted Value Iteration with Linear Function Approximation</a></p>
<p>14 0.34841835 <a title="88-lsi-14" href="./nips-2011-Inductive_reasoning_about_chimeric_creatures.html">130 nips-2011-Inductive reasoning about chimeric creatures</a></p>
<p>15 0.34449166 <a title="88-lsi-15" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<p>16 0.33734542 <a title="88-lsi-16" href="./nips-2011-PAC-Bayesian_Analysis_of_Contextual_Bandits.html">210 nips-2011-PAC-Bayesian Analysis of Contextual Bandits</a></p>
<p>17 0.32264227 <a title="88-lsi-17" href="./nips-2011-Reinforcement_Learning_using_Kernel-Based_Stochastic_Factorization.html">237 nips-2011-Reinforcement Learning using Kernel-Based Stochastic Factorization</a></p>
<p>18 0.31997421 <a title="88-lsi-18" href="./nips-2011-Variance_Reduction_in_Monte-Carlo_Tree_Search.html">300 nips-2011-Variance Reduction in Monte-Carlo Tree Search</a></p>
<p>19 0.31963831 <a title="88-lsi-19" href="./nips-2011-An_Empirical_Evaluation_of_Thompson_Sampling.html">32 nips-2011-An Empirical Evaluation of Thompson Sampling</a></p>
<p>20 0.31889546 <a title="88-lsi-20" href="./nips-2011-Improved_Algorithms_for_Linear_Stochastic_Bandits.html">128 nips-2011-Improved Algorithms for Linear Stochastic Bandits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.098), (22, 0.016), (36, 0.028), (53, 0.017), (55, 0.277), (63, 0.198), (65, 0.064), (68, 0.17), (79, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89896733 <a title="88-lda-1" href="./nips-2011-Environmental_statistics_and_the_trade-off_between_model-based_and_TD_learning_in_humans.html">88 nips-2011-Environmental statistics and the trade-off between model-based and TD learning in humans</a></p>
<p>2 0.85477448 <a title="88-lda-2" href="./nips-2011-A_Reinforcement_Learning_Theory_for_Homeostatic_Regulation.html">11 nips-2011-A Reinforcement Learning Theory for Homeostatic Regulation</a></p>
<p>3 0.84821171 <a title="88-lda-3" href="./nips-2011-Portmanteau_Vocabularies_for_Multi-Cue_Image_Representation.html">216 nips-2011-Portmanteau Vocabularies for Multi-Cue Image Representation</a></p>
<p>4 0.84723568 <a title="88-lda-4" href="./nips-2011-The_Fixed_Points_of_Off-Policy_TD.html">283 nips-2011-The Fixed Points of Off-Policy TD</a></p>
<p>5 0.84694684 <a title="88-lda-5" href="./nips-2011-Scalable_Training_of_Mixture_Models_via_Coresets.html">241 nips-2011-Scalable Training of Mixture Models via Coresets</a></p>
<p>6 0.84666234 <a title="88-lda-6" href="./nips-2011-TD_gamma%3A_Re-evaluating_Complex_Backups_in_Temporal_Difference_Learning.html">278 nips-2011-TD gamma: Re-evaluating Complex Backups in Temporal Difference Learning</a></p>
<p>7 0.84579867 <a title="88-lda-7" href="./nips-2011-Randomized_Algorithms_for_Comparison-based_Search.html">231 nips-2011-Randomized Algorithms for Comparison-based Search</a></p>
<p>8 0.84553742 <a title="88-lda-8" href="./nips-2011-Analysis_and_Improvement_of_Policy_Gradient_Estimation.html">36 nips-2011-Analysis and Improvement of Policy Gradient Estimation</a></p>
<p>9 0.84492236 <a title="88-lda-9" href="./nips-2011-MAP_Inference_for_Bayesian_Inverse_Reinforcement_Learning.html">163 nips-2011-MAP Inference for Bayesian Inverse Reinforcement Learning</a></p>
<p>10 0.84447199 <a title="88-lda-10" href="./nips-2011-Manifold_Precis%3A_An_Annealing_Technique_for_Diverse_Sampling_of_Manifolds.html">164 nips-2011-Manifold Precis: An Annealing Technique for Diverse Sampling of Manifolds</a></p>
<p>11 0.84402436 <a title="88-lda-11" href="./nips-2011-Prismatic_Algorithm_for_Discrete_D.C._Programming_Problem.html">222 nips-2011-Prismatic Algorithm for Discrete D.C. Programming Problem</a></p>
<p>12 0.84162486 <a title="88-lda-12" href="./nips-2011-Efficient_Offline_Communication_Policies_for_Factored_Multiagent_POMDPs.html">79 nips-2011-Efficient Offline Communication Policies for Factored Multiagent POMDPs</a></p>
<p>13 0.84137678 <a title="88-lda-13" href="./nips-2011-On_Tracking_The_Partition_Function.html">197 nips-2011-On Tracking The Partition Function</a></p>
<p>14 0.84055233 <a title="88-lda-14" href="./nips-2011-An_Application_of_Tree-Structured_Expectation_Propagation_for_Channel_Decoding.html">31 nips-2011-An Application of Tree-Structured Expectation Propagation for Channel Decoding</a></p>
<p>15 0.84047848 <a title="88-lda-15" href="./nips-2011-Composite_Multiclass_Losses.html">59 nips-2011-Composite Multiclass Losses</a></p>
<p>16 0.84011376 <a title="88-lda-16" href="./nips-2011-Beating_SGD%3A_Learning_SVMs_in_Sublinear_Time.html">45 nips-2011-Beating SGD: Learning SVMs in Sublinear Time</a></p>
<p>17 0.8374688 <a title="88-lda-17" href="./nips-2011-Learning_Eigenvectors_for_Free.html">145 nips-2011-Learning Eigenvectors for Free</a></p>
<p>18 0.83691573 <a title="88-lda-18" href="./nips-2011-Policy_Gradient_Coagent_Networks.html">215 nips-2011-Policy Gradient Coagent Networks</a></p>
<p>19 0.83668184 <a title="88-lda-19" href="./nips-2011-A_reinterpretation_of_the_policy_oscillation_phenomenon_in_approximate_policy_iteration.html">16 nips-2011-A reinterpretation of the policy oscillation phenomenon in approximate policy iteration</a></p>
<p>20 0.83642703 <a title="88-lda-20" href="./nips-2011-Monte_Carlo_Value_Iteration_with_Macro-Actions.html">174 nips-2011-Monte Carlo Value Iteration with Macro-Actions</a></p>
<br/><br/><br/></body>
</html>
