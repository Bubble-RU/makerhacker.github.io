<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-166" href="#">nips2012-166</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</h1>
<br/><p>Source: <a title="nips-2012-166-pdf" href="http://papers.nips.cc/paper/4788-joint-modeling-of-a-matrix-with-associated-text-via-latent-binary-features.pdf">pdf</a></p><p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>Reference: <a title="nips-2012-166-reference" href="../nips2012_reference/nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. [sent-4, score-0.225]
</p><p>2 The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. [sent-5, score-0.604]
</p><p>3 A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. [sent-6, score-0.403]
</p><p>4 The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. [sent-7, score-0.547]
</p><p>5 Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. [sent-9, score-0.349]
</p><p>6 The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. [sent-10, score-0.505]
</p><p>7 1  Introduction  The analysis of legislative roll-call data provides an interesting setting for recent developments in the joint analysis of matrices and text [23, 8]. [sent-11, score-0.2]
</p><p>8 The problem is made interesting because, in addition to the matrix of votes, we have access to the text of the legislation (e. [sent-13, score-0.466]
</p><p>9 , characteristic of the columns of the matrix, with each column representing a piece of legislation and each row a legislator). [sent-15, score-0.481]
</p><p>10 , the text may correspond to content on a website; each column of the matrix may represent a website, and each row an individual, with the matrix representing number of visits). [sent-18, score-0.228]
</p><p>11 In most such research the binary data are typically analyzed with a probit or logistic link function, and the underlying real matrix is assumed to have rank one. [sent-20, score-0.198]
</p><p>12 Each legislator and piece of legislation exists at a point along this one dimension, which is interpreted as characterizing a (one-dimensional) political philosophy (e. [sent-21, score-0.484]
</p><p>13 As in much matrix-completion research [17, 18], one typically can only infer votes that are missing at random. [sent-25, score-0.242]
</p><p>14 It is not possible to predict the votes of legislators on a new piece of legislation (for which, for example, an entire column of votes is missing). [sent-26, score-0.807]
</p><p>15 In [23, 8] a latent Dirichlet allocation (LDA) [5] topic model was employed for the text. [sent-29, score-0.342]
</p><p>16 It has been demonstrated that LDA yields inferior perplexity scores when compared to modern Bayesian topic models, such as the focused topic model (FTM) [24]. [sent-30, score-0.468]
</p><p>17 Another signiﬁcant issue with [23, 8] concerns how the topic (text) and matrix models are coupled. [sent-31, score-0.269]
</p><p>18 In [23, 8] the frequency with which a given topic is utilized in the text legislation is used to infer the associated matrix parameters (e. [sent-32, score-0.77]
</p><p>19 , to infer the latent feature vector associated with the respective column of the matrix). [sent-34, score-0.215]
</p><p>20 Motivated by these limitations, in this paper the FTM is employed to model the text of legislation, with each piece of legislation characterized by a latent binary vector that deﬁnes the sparse set of associated topics. [sent-37, score-0.743]
</p><p>21 A new probabilistic low-rank matrix decomposition is developed for the votes, utilizing latent binary features; this leverages the merits of what were previously two distinct lines of matrix factorization methods [13, 17]. [sent-38, score-0.39]
</p><p>22 For a piece of legislation, the latent binary feature vectors for the FTM and matrix decomposition are shared, yielding a new means of jointly modeling text and matrices. [sent-40, score-0.507]
</p><p>23 This linkage between text and matrices is innovative as: (i) it’s based on whether a topic is relevant to a document/legislation, not based on the frequency with which the topic is used in the document (i. [sent-41, score-0.713]
</p><p>24 , not based on the style of writing); (ii) it enables interpretation of the underlying latent binary features [13, 9] based upon available text data. [sent-43, score-0.37]
</p><p>25 Section 2 ﬁrst reviews the focused topic model, then introduces a new lowrank matrix decomposition method, and the joint model of the two. [sent-45, score-0.403]
</p><p>26 In Section 4 quantitative results are presented for prediction of columns of roll-call votes based on the associated text legislation, and the joint model is demonstrated qualitatively to infer meaning/insight for the characteristics of legislation and voting patterns, and Section 5 concludes. [sent-47, score-0.711]
</p><p>27 It is desirable to share a set of topics across all documents, but with the additional constraint that a given document only utilize a small subset of the topics; this tends to yield more descriptive/focused topics, characteristic of detailed properties of the documents. [sent-51, score-0.241]
</p><p>28 A FTM is manifested as a compound linkage of the Indian buffet process (IBP) [10] and the Dirichlet process (DP). [sent-52, score-0.168]
</p><p>29 Each document draws latent binary features from an IBP to select a ﬁnite subset of atoms/topics from the DP. [sent-53, score-0.326]
</p><p>30 In the model details, the DP is represented in terms of a normalized gamma process [7] with weighting by the binary feature vector, constituting a document-speciﬁc topic distribution in which only a subset of topics are manifested with non-zero probability. [sent-54, score-0.53]
</p><p>31 2  bj: and λ, thereby selecting a subset of topics for document j (those for which the corresponding components of bj: are non-zero). [sent-58, score-0.206]
</p><p>32 The rest of the FTM is constructed similar to LDA [5], where for each token n in document j, a topic indicator is drawn as zjn |θj ∼ Mult(zjn |1, θj ). [sent-59, score-0.35]
</p><p>33 Conditional on zjn and the topics {βk }Kr , a word is drawn as wjn |zjn , {βk }Kr ∼ Mult(wjn |1, βzjn ), where k=1 k=1 βk |η ∼ Dirichlet(βk |η). [sent-60, score-0.221]
</p><p>34 Although in (1) bj: is mainly designed to map the global prevalence of topics across the corpus, λ, to a within-document proportion of topic usage, θj , latent features bj: are informative in their own right, as they indicate which subset of topics is relevant to a given document. [sent-61, score-0.657]
</p><p>35 We therefore make the linkage between documents and an associated matrix via the bj: , not based on θj (where [23, 8] base the document-matrix linkage via θj or it’s empirical estimate). [sent-63, score-0.259]
</p><p>36 2  Matrix factorization with binary latent factors and a low-rank assumption  Binary matrix factorization (BMF) [13, 14] is a general framework in which real latent matrix X ∈ RP ×N is decomposed as X = LHRT , where L ∈ {0, 1}P ×Kl , R ∈ {0, 1}N ×Kr are binary, and H ∈ RKl ×Kr is real. [sent-65, score-0.515]
</p><p>37 We focus on binary observed matrices, Y ∈ {0, 1}P ×N , and utilize f (·) as a probit model [2]: yij = with xij = xij + ˆ  ij ,  where  ij  if xij ≥ 0 ˆ if xij < 0 ˆ  1 0  (2)  ∼ N (0, 1). [sent-69, score-0.335]
</p><p>38 Thus the deﬁnition of Ψ and Φ via the binary matrices L and R and the linkage matrix H merges previously two distinct lines of matrix factorization methods. [sent-77, score-0.313]
</p><p>39 In the context of the application considered here, the decomposition X = LHRT will prove convenient, as we may share the binary matrices L or R among the topic usage of available documents. [sent-78, score-0.436]
</p><p>40 The binary features in L and R are therefore characteristic of the presence/absence of underlying topics, or related latent processes, and the matrix H provides the mapping of how these binary features map to observed data. [sent-79, score-0.47]
</p><p>41 We model the “signiﬁcance” of each rank-1 term in the expansion explicitly, using a stochastic Kc T process {sk }Kc , therefore H can be decomposed as H = k=1 sk u:k v:k , Kc can be inﬁnity in k=1 principle. [sent-82, score-0.16]
</p><p>42 As a result, the hierarchical representation in modeling the latent matrix X in probit model can be summarized as: xij | li: , rj: , {u:k , v:k , sk }Kc ∼ N xij | ˆ ˆ k=1  Kc T k=1 sk (li: u:k )(rj: v:k ) , 1  (4)  Note that sk in (4) is similar to the singular value of SVD in spirit. [sent-83, score-0.809]
</p><p>43 Theorem 1 below formally states that if sk is modeled by MGP as in (5), the rank-1 expansion in (4) will converge when Kc → ∞. [sent-87, score-0.202]
</p><p>44 When αc > 1, the sequence  Kc T k=1 sk (li: u:k )(rj: v:k )  converges in  2,  as Kc → ∞. [sent-89, score-0.16]
</p><p>45 ∞ T k=Kc +1 sk (li: u:k )(rj: v:k ) , then ∀ maxk E(li: u:k )2 , b = maxk E(rj: v:k )2 . [sent-92, score-0.16]
</p><p>46 3  Joint learning of FTM and BMF  Via the FTM and BMF framework of the previous subsections, each piece of legislation j is represented as two latent binary feature vectors bj: and rj: . [sent-98, score-0.621]
</p><p>47 To jointly model the matrix of votes with associated text of legislation, a natural choice is to impose bj: = rj: . [sent-99, score-0.343]
</p><p>48 As a result, the full joint model can be speciﬁed by equations (1) - (5), with bjt in (1) replaced by rjt . [sent-100, score-0.171]
</p><p>49 In the context of the model for Y = f (X), with X = LHRT , if one were to learn L and H based upon available training data, then a new legislation y:N +1 could be predicted if we had access to r:N +1 . [sent-103, score-0.323]
</p><p>50 Via the construction above, not only do we gain a predictive advantage, because the new legislation’s latent binary features r:N +1 can be obtained from modeling its document as in (1), but also the model provides powerful interpretative insights. [sent-104, score-0.353]
</p><p>51 Speciﬁcally the topics inferred from the documents may be used to interpret the latent binary features associated with the matrix factorization. [sent-105, score-0.56]
</p><p>52 4  Related work  The ideal point topic model (IPTM) was developed in [8], where the supervised Latent Dirichlet Allocation (sLDA) [4] model was used to link empirical topic-usage frequencies to the latent factors via regression. [sent-108, score-0.342]
</p><p>53 In [23] the authors proposed to jointly analyze the voting matrix and the associated text through a mixture model, where each legislation’s latent feature factor is clustered to a mixture component in coupled with that legislation’s document topic distribution θ. [sent-112, score-0.631]
</p><p>54 Note that in their case each piece of legislation can only belong to one cluster, while in our case the latent binary features for each document can be effectively treated as being grouped to multiple clusters [13] (a mixedmembership model, manifested in terms of the binary feature vectors). [sent-113, score-0.878]
</p><p>55 Similar research in linking collaborative ﬁltering and topic models can also be found in web content recommendation [1], movie recommendation[19], and scientiﬁc paper recommendation [22]. [sent-114, score-0.3]
</p><p>56 None of these methods makes use of the binary indicators as the characterization of associated documents, but perform linking via the topic distribution θ and the latent (real) features in different ways. [sent-115, score-0.54]
</p><p>57 Sampling {v:k , u:k }k=1:Kc Based on (3) and (4) the conditional posterior of v:k can be writN Kc ˆ ten as p(v:k |−) ∝ j=1 N (x:j | k=1 sk (Lu:k )(rj: v:k ), 1)N (v:k |0, IKr ). [sent-119, score-0.188]
</p><p>58 It can be shown that N T ˜ −k j=1 (Lu:k rj: ) x:j and covariN T −1 T ˜ :j ˆ , where x−k = x:j − LUVT rj: + j=1 (Lu:k rj: ) (Lu:k rj: )]  p(v:k |−) = N (v:k |µv:k , Σv:k ), with mean µv:k = sk Σv:k  ance matrix Σv:k = [IKr + s2 k Lu:k rj: v:k . [sent-120, score-0.213]
</p><p>59 Sampling {sk }k=1:Kc Based on (4) and (5) the conditional posterior of sk can be written N Kc −1 ˆ as p(sk |−) ∝ It can be shown that k=1 sk (Lu:k )(rj: v:k ), 1)N (sk |0, τk ). [sent-122, score-0.348]
</p><p>60 Sampling {rjt }j=1:N,t=1:Kr Similar to the derivation in [24], p(rjt = 1|−) = 1 if Njt > 0, where Njt denotes the number of times document j used topic t. [sent-126, score-0.288]
</p><p>61 When Njt = 0, based on (1) and (4) the conditional posterior of rjt can be written as p(rjt = 1|−) ∝ πt ˜ :j exp{− 1 [(LhT )T (LhT ) − 2(LhT )T x−k ]}, where ht: represents the tth row of H = t: t: t: 2 πt +2λt (1−πt ) Kc T k=1 sk u:k v:k ;  and p(rjt = 0|−) ∝  2λt (1−πt ) . [sent-127, score-0.303]
</p><p>62 1  Experiment setting  We have performed joint matrix and text analysis, considering the House of Representatives (House), sessions 106 - 111 2 ; we model each session’s roll-call votes separately as binary matrix Y. [sent-135, score-0.53]
</p><p>63 Entry yij = 1 denotes that the ith legislator’s response to legislation j is either “Yea” or “Yes” , and yij = 0 denotes that the corresponding response is either “Nay” or “No”. [sent-136, score-0.393]
</p><p>64 We recommend to set the IBP hyperparameters αl = αr = 1, MGP hyperparameters αc = 3, FTM hyperparameters γ = 5 and topic model hyperparameter η = 0. [sent-138, score-0.216]
</p><p>65 2  Predicting random missing votes  In this section we study the classical problem of estimating the values of matrix data that are missing uniformly at random (in-matrix missing votes), without the use of associated documents. [sent-147, score-0.4]
</p><p>66 This is done by decomposing the latent matrix X = ΨΦT , where each row of Ψ and ΦT are drawn from a Gaussian distribution with mean and covariance matrix modeled by a Gaussian-Wishart distribution. [sent-149, score-0.274]
</p><p>67 In Figure 1 each panel corresponds to a certain percentage of missingness; the horizontal axis is the number of columns (rank), which varies as a free parameter of PMF, while the vertical axis is the prediction accuracy. [sent-156, score-0.266]
</p><p>68 3  Predicting new bills based on text  We study the predictive power of the proposed model when the legislative roll-call votes and the associated bill documents are modeled jointly, as described in Section 2. [sent-165, score-0.699]
</p><p>69 We also compare our model with that in [23], where the authors proposed to combine the factor analysis model and topic model via a compounded mixture model, with all sessions of roll-call data are modeled jointly via a Markov process. [sent-169, score-0.312]
</p><p>70 Since our main goal is to predict new bills but not modeling the matrices dynamically, in the following experiments we remove the Markov process but model each session of House data separately; we call this model FATM. [sent-170, score-0.344]
</p><p>71 In [23] the authors proposed to use a beta-Bernoulli distributed binary variable bk to model if the kth rank-1 matrix is used in matrix decomposition. [sent-171, score-0.224]
</p><p>72 When performing posterior inference we ﬁnd that bk tends to be easily trapped in local maxima, while MGP, which models the signiﬁcance of usage (but not the binary usage) of each kth rank-1 matrix via sk , smoother estimates and better mixing were observed. [sent-172, score-0.43]
</p><p>73 For each session the bills are partitioned into 6-folds, and we iteratively remove a fold, and train the model with the remaining folds; predictions are then performed on the bills in the removed fold. [sent-173, score-0.549]
</p><p>74 This may lead to the undesirable consequence that the latent features learned from text are not discriminative in predicting a new piece of legislation. [sent-176, score-0.404]
</p><p>75 To reduce such risk, in practice we could either set αr such that it strongly favor fewer latent binary features, or we can truncate the stick breaking construction at a pre-deﬁned level Kr . [sent-177, score-0.277]
</p><p>76 91  50  1  5 10 BMF‐Original  Proposed  20 50 PMF PMF+MGP  1  5  10  20  50  Figure 1: Comparison of prediction accuracy for votes missing uniformly at random, for the 110th House data. [sent-201, score-0.217]
</p><p>77 Different panels corresponds to different percentage of missingness, for each panel the vertical axis represents accuracy and horizontal axis represents the rank set for PMF. [sent-202, score-0.303]
</p><p>78 81 30  50  100 150 200 300  FATM  IPTM  30  50  100  IPTM(Kc = 1)  Figure 2: Prediction accuracy for held-out legislation across 106th - 111th House data; prediction of an entire column of missing votes based on text. [sent-251, score-0.572]
</p><p>79 In each panel the vertical axis represents accuracy and the horizontal axis represents the number of topics used for each model. [sent-252, score-0.356]
</p><p>80 tage of our proposed model when the truncation on the number of topics Kr (horizontal axis) is small (e. [sent-254, score-0.186]
</p><p>81 4  Latent binary feature interpretation  In this study we partition all the bills into two groups: (i) bills for which there is near-unanimous agreement, with “Yea” or “Yes” more than 90%; (ii) contentious bills with percentage of votes received as “Yea” or “Yes” less than 60%. [sent-260, score-1.118]
</p><p>82 By linking the inferred binary latent features to the topics for those two groups, we can get insight into the characteristics of legislation and voting patterns, e. [sent-261, score-0.822]
</p><p>83 Figure 3 compares the latent feature usage pattern of those two groups; the horizontal axis represents the latent features, where we set Kr = 100 for illustration purpose, and the vertical axis is the aggregated frequency that a feature/topic is used by all the bills in each of those two groups. [sent-264, score-0.814]
</p><p>84 For example, in the left panel the features highlighted in blue are widely used by bills in the left group, but rarely used by bills in the right group. [sent-267, score-0.615]
</p><p>85 As observed 7  Binary feature usage pattern for unanimous agreed bills  Binary feature usage pattern for highly debated bills  0. [sent-268, score-0.734]
</p><p>86 01 0  0  10  20  30  40  50  60  70  80  90  0  100  0  10  20  30  40  50  60  70  80  90  100  Figure 3: Comparison of the frequencies of binary features usage between two groups of bills, left: nearunanimous afﬁrmative bills (e. [sent-281, score-0.46]
</p><p>87 , bills with percentage of votes received as “Yes” or “Yea” is more than 90%). [sent-283, score-0.473]
</p><p>88 , bills with percentage of votes received as “Yes” or “Yea” is less than 60%). [sent-286, score-0.473]
</p><p>89 The six most discriminative features/topics (labeled in the ﬁgure) are shown in Table 1  Table 1: Six discriminative topics of unanimous agreed/highly debated bills learned from the 110th house of representatives, with top-ten most probable words shown. [sent-289, score-0.617]
</p><p>90 We also study the interpretation of those latent features by linking them to the topics inferred from the texts. [sent-292, score-0.376]
</p><p>91 As an example, those six highlighted features are linked to their corresponding topics and depicted in Table 1, with the top-ten most probable words within each topic shown. [sent-293, score-0.42]
</p><p>92 For the unanimous agreed bills, we can read from Table 1 that they are highly probable to be related to topics about the education of youth (Topic 22), or the prevention of terrorist (Topic 73). [sent-294, score-0.224]
</p><p>93 While the bills from the contentious group tend to more related to making amendments to an existing piece of legislation (Topic 83) or discussing taxation (Topic 38). [sent-295, score-0.717]
</p><p>94 Note that compared to conventional topic modeling, these inferred topics are not only informative in semantic meaning of the bills, but also discriminative in predicting the outcome of the bills. [sent-296, score-0.431]
</p><p>95 5  Conclusion  A new methodology has been developed for the joint analysis of a matrix with associated text, based on sharing latent binary features modeled via the Indian buffet process. [sent-297, score-0.464]
</p><p>96 Imposition of a lowrank representation for the latent real matrix has proven important, with this done in a new manner via the multiplicative gamma process. [sent-299, score-0.249]
</p><p>97 The sharing of latent binary features provides a general joint learning framework for Indian buffet process based models [9], where focused topic model and binary matrix factorization are two examples, exploring other possibilities in different scenarios could be an interesting direction. [sent-301, score-0.761]
</p><p>98 Inﬁnite latent feature models and the Indian buffet process. [sent-363, score-0.178]
</p><p>99 The IBP compound Dirichlet process and its application to focused topic modeling. [sent-462, score-0.252]
</p><p>100 Hierarchical topic modeling for analysis of time-evolving personal choices. [sent-468, score-0.243]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kc', 0.582), ('legislation', 0.323), ('bills', 0.261), ('topic', 0.216), ('mgp', 0.186), ('votes', 0.168), ('sk', 0.16), ('kr', 0.139), ('topics', 0.134), ('latent', 0.126), ('ftm', 0.124), ('rj', 0.123), ('bmf', 0.115), ('iptm', 0.099), ('house', 0.098), ('piece', 0.091), ('text', 0.09), ('bj', 0.086), ('opic', 0.084), ('rjt', 0.084), ('binary', 0.081), ('pmf', 0.08), ('lu', 0.078), ('document', 0.072), ('usage', 0.071), ('yea', 0.07), ('ibp', 0.066), ('axis', 0.065), ('zjn', 0.062), ('linkage', 0.059), ('manifested', 0.057), ('bjt', 0.056), ('missingness', 0.056), ('documents', 0.056), ('yes', 0.055), ('sessions', 0.054), ('matrix', 0.053), ('truncation', 0.052), ('buffet', 0.052), ('legislative', 0.05), ('missing', 0.049), ('xij', 0.048), ('features', 0.047), ('indian', 0.047), ('dirichlet', 0.045), ('percentage', 0.044), ('voting', 0.042), ('gamma', 0.042), ('contentious', 0.042), ('ikr', 0.042), ('lhrt', 0.042), ('lht', 0.042), ('njt', 0.042), ('unanimous', 0.042), ('modeled', 0.042), ('decomposition', 0.039), ('vertical', 0.038), ('linking', 0.038), ('stick', 0.038), ('factorization', 0.038), ('legislator', 0.037), ('rank', 0.037), ('kth', 0.037), ('focused', 0.036), ('yij', 0.035), ('characteristic', 0.035), ('political', 0.033), ('column', 0.032), ('associated', 0.032), ('breaking', 0.032), ('frequency', 0.031), ('sampler', 0.031), ('joint', 0.031), ('tth', 0.031), ('horizontal', 0.031), ('inferred', 0.031), ('matrices', 0.029), ('hdp', 0.028), ('debated', 0.028), ('ibps', 0.028), ('lowrank', 0.028), ('posterior', 0.028), ('session', 0.027), ('probit', 0.027), ('modeling', 0.027), ('discriminative', 0.027), ('style', 0.026), ('infer', 0.025), ('dp', 0.025), ('legislators', 0.025), ('imposition', 0.025), ('wjn', 0.025), ('youth', 0.025), ('dunson', 0.024), ('vote', 0.024), ('predicting', 0.023), ('highlighted', 0.023), ('panel', 0.023), ('terrorist', 0.023), ('recommendation', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="166-tfidf-1" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>2 0.21180925 <a title="166-tfidf-2" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>3 0.15456791 <a title="166-tfidf-3" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>4 0.14235859 <a title="166-tfidf-4" href="./nips-2012-Multiresolution_Gaussian_Processes.html">233 nips-2012-Multiresolution Gaussian Processes</a></p>
<p>Author: David B. Dunson, Emily B. Fox</p><p>Abstract: We propose a multiresolution Gaussian process to capture long-range, nonMarkovian dependencies while allowing for abrupt changes and non-stationarity. The multiresolution GP hierarchically couples a collection of smooth GPs, each deﬁned over an element of a random nested partition. Long-range dependencies are captured by the top-level GP while the partition points deﬁne the abrupt changes. Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the marginal likelihood of the observations given the partition tree. This property allows for efﬁcient inference of the partition itself, for which we employ graph-theoretic techniques. We apply the multiresolution GP to the analysis of magnetoencephalography (MEG) recordings of brain activity.</p><p>5 0.13248257 <a title="166-tfidf-5" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><p>6 0.12129839 <a title="166-tfidf-6" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>7 0.10362002 <a title="166-tfidf-7" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>8 0.10010615 <a title="166-tfidf-8" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>9 0.097220913 <a title="166-tfidf-9" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>10 0.094814375 <a title="166-tfidf-10" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>11 0.093566887 <a title="166-tfidf-11" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>12 0.092027672 <a title="166-tfidf-12" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>13 0.082363285 <a title="166-tfidf-13" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>14 0.081606217 <a title="166-tfidf-14" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>15 0.080327667 <a title="166-tfidf-15" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>16 0.077918015 <a title="166-tfidf-16" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>17 0.077842377 <a title="166-tfidf-17" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>18 0.074804813 <a title="166-tfidf-18" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>19 0.072407685 <a title="166-tfidf-19" href="./nips-2012-A_latent_factor_model_for_highly_multi-relational_data.html">22 nips-2012-A latent factor model for highly multi-relational data</a></p>
<p>20 0.07054887 <a title="166-tfidf-20" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.072), (2, -0.025), (3, -0.017), (4, -0.187), (5, -0.039), (6, -0.011), (7, 0.024), (8, 0.127), (9, -0.045), (10, 0.121), (11, 0.162), (12, 0.0), (13, -0.046), (14, 0.033), (15, 0.065), (16, 0.075), (17, 0.103), (18, -0.0), (19, 0.053), (20, -0.014), (21, 0.04), (22, -0.02), (23, -0.088), (24, 0.007), (25, -0.016), (26, 0.04), (27, 0.096), (28, -0.034), (29, -0.013), (30, 0.04), (31, 0.085), (32, 0.012), (33, 0.054), (34, -0.009), (35, -0.013), (36, -0.099), (37, 0.021), (38, 0.02), (39, -0.053), (40, -0.069), (41, 0.029), (42, 0.07), (43, 0.055), (44, 0.018), (45, 0.19), (46, 0.0), (47, 0.014), (48, 0.108), (49, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94583738 <a title="166-lsi-1" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>2 0.77244318 <a title="166-lsi-2" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>3 0.73496461 <a title="166-lsi-3" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>4 0.68291223 <a title="166-lsi-4" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>Author: Kosuke Fukumasu, Koji Eguchi, Eric P. Xing</p><p>Abstract: Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. 1</p><p>5 0.67469335 <a title="166-lsi-5" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><p>6 0.65988076 <a title="166-lsi-6" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>7 0.63520163 <a title="166-lsi-7" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>8 0.63029456 <a title="166-lsi-8" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>9 0.59369326 <a title="166-lsi-9" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>10 0.58631456 <a title="166-lsi-10" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>11 0.57155603 <a title="166-lsi-11" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>12 0.52912885 <a title="166-lsi-12" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>13 0.51564682 <a title="166-lsi-13" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>14 0.51460057 <a title="166-lsi-14" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>15 0.47475958 <a title="166-lsi-15" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>16 0.47280905 <a title="166-lsi-16" href="./nips-2012-A_latent_factor_model_for_highly_multi-relational_data.html">22 nips-2012-A latent factor model for highly multi-relational data</a></p>
<p>17 0.4504123 <a title="166-lsi-17" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>18 0.41741025 <a title="166-lsi-18" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>19 0.38312677 <a title="166-lsi-19" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>20 0.38203961 <a title="166-lsi-20" href="./nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">59 nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.101), (9, 0.013), (21, 0.018), (38, 0.118), (39, 0.349), (42, 0.016), (53, 0.011), (54, 0.023), (55, 0.028), (74, 0.03), (76, 0.089), (80, 0.067), (92, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.77940071 <a title="166-lda-1" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family, even if the distributions are very heavy-tailed, have inﬁnite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness. Both theories and experiments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and robustness at almost no cost. 1</p><p>same-paper 2 0.7735182 <a title="166-lda-2" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>3 0.76776719 <a title="166-lda-3" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>Author: Gal Elidan, Cobi Cario</p><p>Abstract: The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models. In this work we present Nonparanormal BP for performing efﬁcient inference on distributions parameterized by a Gaussian copulas network and any univariate marginals. For tree structured networks, our approach is guaranteed to be exact for this powerful class of non-Gaussian models. Importantly, the method is as efﬁcient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used. 1</p><p>4 0.76025021 <a title="166-lda-4" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>5 0.67994142 <a title="166-lda-5" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><p>6 0.64808083 <a title="166-lda-6" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>7 0.64735281 <a title="166-lda-7" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>8 0.5489912 <a title="166-lda-8" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>9 0.53904313 <a title="166-lda-9" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>10 0.53203279 <a title="166-lda-10" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>11 0.51031357 <a title="166-lda-11" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>12 0.50585246 <a title="166-lda-12" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>13 0.49507833 <a title="166-lda-13" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>14 0.49116984 <a title="166-lda-14" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>15 0.49013868 <a title="166-lda-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.48952579 <a title="166-lda-16" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>17 0.4856185 <a title="166-lda-17" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>18 0.48550048 <a title="166-lda-18" href="./nips-2012-Isotropic_Hashing.html">163 nips-2012-Isotropic Hashing</a></p>
<p>19 0.4838706 <a title="166-lda-19" href="./nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">317 nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>20 0.48372221 <a title="166-lda-20" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
