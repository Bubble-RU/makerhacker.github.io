<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-316" href="#">nips2012-316</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</h1>
<br/><p>Source: <a title="nips-2012-316-pdf" href="http://papers.nips.cc/paper/4853-small-variance-asymptotics-for-exponential-family-dirichlet-process-mixture-models.pdf">pdf</a></p><p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>Reference: <a title="nips-2012-316-reference" href="../nips2012_reference/nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. [sent-12, score-0.739]
</p><p>2 We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. [sent-14, score-0.187]
</p><p>3 While probabilistic approaches—particularly Bayesian models—are ﬂexible from a modeling perspective, lack of scalable inference methods can limit applicability on some data. [sent-16, score-0.146]
</p><p>4 In some cases, links between probabilistic and non-probabilistic models can be made by applying asymptotics to the variance (or covariance) of distributions within the model. [sent-18, score-0.299]
</p><p>5 Besides providing a conceptual link between seemingly quite different approaches, small-variance asymptotics can yield useful alternatives to probabilistic models when the data size becomes large, as the non-probabilistic models often exhibit more favorable scaling properties. [sent-20, score-0.299]
</p><p>6 The use of such techniques to derive scalable algorithms from rich probabilistic models is still emerging, but provides a promising approach to developing scalable learning algorithms. [sent-21, score-0.173]
</p><p>7 This paper explores such small-variance asymptotics for clustering, focusing on the DP mixture. [sent-22, score-0.25]
</p><p>8 Existing work has considered asymptotics over the Gaussian DP mixture [3], leading to k-meanslike algorithms that do not ﬁx the number of clusters upfront. [sent-23, score-0.577]
</p><p>9 This approach, while an important ﬁrst step, raises the question of whether we can perform similar asymptotics over distributions other 1  than the Gaussian. [sent-24, score-0.25]
</p><p>10 We answer in the afﬁrmative by showing how such asymptotics may be applied to the exponential family distributions for DP mixtures; such analysis opens the door to a new class of scalable clustering algorithms and utilizes connections between Bregman divergences and exponential families. [sent-25, score-1.31]
</p><p>11 We further extend our approach to hierarchical nonparametric models (speciﬁcally, the hierarchical Dirichlet process (HDP) [4]), and we view a major contribution of our analysis to be the development of a general hard clustering algorithm for grouped data. [sent-26, score-0.515]
</p><p>12 One of the primary advantages of generalizing beyond the Gaussian case is that it opens the door to novel scalable algorithms for discrete-data problems. [sent-27, score-0.25]
</p><p>13 For instance, visual bag-of-words [5] have become a standard representation for images in a variety of computer vision tasks, but many existing probabilistic models in vision cannot scale to the size of data sets now commonly available. [sent-28, score-0.119]
</p><p>14 Our analysis covers such problems; for instance, a particular special case of our analysis is a hard version of HDP topic modeling. [sent-32, score-0.253]
</p><p>15 Related Work: In the non-Bayesian setting, asymptotics for the expectation-maximization algorithm for exponential family distributions were studied in [7]. [sent-34, score-0.535]
</p><p>16 The authors showed a connection between EM and a general k-means-like algorithm, where the squared Euclidean distance is replaced by the Bregman divergence corresponding to exponential family distribution of interest. [sent-35, score-0.446]
</p><p>17 Our results may be viewed as generalizing this approach to the Bayesian nonparametric setting. [sent-36, score-0.147]
</p><p>18 As discussed above, our results may also be viewed as generalizing the approach of [3], where the asymptotics were performed for the DP mixture with a Gaussian likelihood, leading to a k-means-like algorithm where the number of clusters is not ﬁxed upfront. [sent-37, score-0.662]
</p><p>19 Note that our setting is considerably more involved than either of these previous works, particularly since we will require an appropriate technique for computing an asymptotic marginal likelihood. [sent-38, score-0.128]
</p><p>20 Other connections between hard clustering and probabilistic models were explored in [8], which proposes a “Bayesian k-means” algorithm by performing a maximization-expectation algorithm. [sent-39, score-0.432]
</p><p>21 2  Background  In this section, we brieﬂy review exponential family distributions, Bregman divergences, and the Dirichlet process mixture model. [sent-40, score-0.411]
</p><p>22 1  The Exponential Family  Consider the exponential family with natural parameter θ = {θj }d ∈ Rd ; then the exponential j=1 family probability density function can be written as [9]: p(x | θ) = exp  x, θ − ψ(θ) − h(x) ,  where ψ(θ) = log exp( x, θ − h(x))dx is the log-partition function. [sent-42, score-0.662]
</p><p>23 ψ(θ) can be utilized to compute the mean and covariance of p(x | θ); in particular, the expected value is given by ψ(θ), and the covariance is 2 ψ(θ). [sent-44, score-0.199]
</p><p>24 A convenient property of the exponential family is that a conjugate prior distribution of θ exists; in particular, given any speciﬁc distribution in the exponential family, the conjugate prior can be parametrized as [11]: p(θ | τ, η) = exp  θ, τ − ηψ(θ) − m(τ, η) . [sent-46, score-0.726]
</p><p>25 Given a data point xi , the posterior distribution of θ has the same form as the prior, with τ → τ + xi and η → η + 1. [sent-48, score-0.228]
</p><p>26 The Bregman divergence for any pair of points x, y ∈ S is deﬁned as Dφ (x, y) = φ(x) − φ(y) − x − y, φ(y) , and can be viewed as a generalized distortion measure. [sent-50, score-0.225]
</p><p>27 An important result connecting Bregman divergences and exponential families was discussed in [7] (see also [10, 11]), where a bijection between the two was established. [sent-51, score-0.346]
</p><p>28 The Bregman divergence representation provides a natural way to parametrize the exponential family distributions with its expectation parameter and, as we will see, we will ﬁnd it convenient to work with this form. [sent-53, score-0.414]
</p><p>29 2  Dirichlet Process Mixture Models  The Dirichlet Process (DP) mixture model is a Bayesian nonparametric mixture model [12]; unlike most parametric mixture models (Bayesian or otherwise), the number of clusters in a DP mixture is not ﬁxed upfront. [sent-55, score-0.767]
</p><p>30 Using the exponential family parameterized by the expectation µc , the likelihood for a data point can be expressed as the following inﬁnite mixture: ∞  ∞  πc p(x | µc ) =  p(x) = c=1  πc exp(−Dφ (x, µc ))fφ (x). [sent-56, score-0.322]
</p><p>31 Moreover, a simple collapsed Gibbs sampler can be employed for performing inference in this model [13]; this Gibbs sampler will form the basis of our asymptotic analysis. [sent-58, score-0.392]
</p><p>32 , xn }, the state of the Markov chain is the set of cluster indicators {z1 , . [sent-62, score-0.252]
</p><p>33 , zn } as well as the cluster means of the currently-occupied clusters (the mixing weights have been integrated out). [sent-65, score-0.408]
</p><p>34 If we choose to start a new cluster during the Gibbs update, we sample its mean from the posterior distribution obtained from the prior distribution G0 and the single observation xi . [sent-70, score-0.411]
</p><p>35 After performing Gibbs moves on the cluster indicators, we update the cluster means µc by sampling from the posterior of µc given the data points assigned to cluster c. [sent-71, score-0.831]
</p><p>36 Given an exponential family distribution p(x | θ) with natural parameter θ and log-partition function ψ(θ), consider a ˜ scaled exponential family distribution whose natural parameter is θ = βθ and log-partition function ˜ θ) = βψ(θ/β), where β > 0. [sent-73, score-0.641]
</p><p>37 The following result characterizes the relationship between the ˜ ˜ is ψ( mean and covariance of the original and scaled exponential family distributions. [sent-74, score-0.444]
</p><p>38 Given a scaled exponential family with θ = βθ and ψ(θ) = βψ(θ/β), the mean µ(θ) of the ˜ is cov(θ)/β. [sent-78, score-0.389]
</p><p>39 It is perhaps intuitively simpler to observe what happens to the distribution using the 3  Bregman divergence representation. [sent-80, score-0.129]
</p><p>40 Recall that the generating function φ for the Bregman divergence is given by the Legendre-conjugate of ψ. [sent-81, score-0.129]
</p><p>41 The Bregman divergence representation for the scaled distribution is given by ˜ ˜ ˜ ˜ p(x | θ) = p(x | µ) = exp(−Dφ (x, µ))fφ (x) = exp(−βDφ (x, µ))fβφ (x), ˜ where the last equality follows from Lemma 3. [sent-83, score-0.2]
</p><p>42 Next we consider the prior distribution under the scaled exponential family. [sent-86, score-0.258]
</p><p>43 This gives the following prior written using the Bregman divergence, where we are now explicitly conditioning on β: ˜ p(θ | τ, η, β) = exp  −  η τ /β Dφ ,µ ˜ β η/β  τ η , β β  gφ ˜  = exp  − ηDφ  τ ,µ η  gφ ˜  τ η , . [sent-88, score-0.224]
</p><p>44 Standard algebraic manipulations yield the following: ˜ ˜ ˜ p(x | θ) × p(θ | τ, η, β)dθ  p(x | τ, η, β) =  = fφ (x) · gφ ˜ ˜ = fφ (x) · gφ ˜ ˜  βx + τ τ η ˜ ˜ ˜ , A(φ,τ,η,β) (x) exp − (β + η)Dφ , µ(θ) dθ ˜ β β β+η τ η βx + τ , A(φ,τ,η,β) (x) · β d · exp − (β + η)Dφ , µ(θ) ˜ β β β+η  dθ. [sent-90, score-0.184]
</p><p>45 (1)  Here, A(φ,τ,η,β) (x) = exp − (βφ(x) + ηφ( τ ) − (β + η)φ( βx+τ )) , which arises when combining ˜ η β+η the Bregman divergences from the likelihood and the prior. [sent-91, score-0.296]
</p><p>46 Note that the exponential term equals one since the divergence inside is 0. [sent-95, score-0.276]
</p><p>47 1 Asymptotic Behavior of the Gibbs Sampler We now have the tools to consider the Gibbs sampler for the exponential family DP mixture as we let β → ∞. [sent-97, score-0.528]
</p><p>48 As we will see, we will obtain a general k-means-like hard clustering algorithm which utilizes the appropriate Bregman divergence in place of the squared Euclidean distance, and also can vary the number of clusters. [sent-98, score-0.498]
</p><p>49 Now, we consider the asymptotic behavior of these probabilities as β → ∞. [sent-100, score-0.132]
</p><p>50 We 4  note that βxi + τ = xi , β→∞ β + η lim  and  lim A(φ,τ,η,β) (xi ) = exp(−η(φ(τ /η) − φ(xi ))), ˜  β→∞  and that the Laplace approximation error term goes to zero as β → ∞. [sent-101, score-0.128]
</p><p>51 That is, the data point xi will be assigned to the nearest cluster with a divergence at most λ. [sent-110, score-0.468]
</p><p>52 If the closest mean has a divergence greater than λ, we start a new cluster containing only xi . [sent-111, score-0.466]
</p><p>53 Next, we show that sampling µc from the posterior distribution is achieved by simply computing the empirical mean of a cluster in the limit. [sent-112, score-0.274]
</p><p>54 During Gibbs sampling, once we have performed one complete set of Gibbs moves on the cluster assignments, we need to sample the µc conditioned on all assignments and observations. [sent-113, score-0.3]
</p><p>55 If we let nc be the number of points assigned to cluster c, then the posterior distribution (parameterized by the expectation parameter) for cluster c is p(µc | X, z, τ, η, β) ∝ p(Xc | µc , β)×p(µc | τ, η, β) ∝ exp −(βnc +η)Dφ  nc i=1  βxc + τ i ,µ βnc + η  where X is all the data, Xc = {xc , . [sent-114, score-0.819]
</p><p>56 , xc c } is the set of points currently assigned to cluster c, and z n 1 is the set of all current assignments. [sent-117, score-0.361]
</p><p>57 We can see that the mass of the posterior distribution becomes nc x concentrated around the sample mean i=1 i as β → ∞. [sent-118, score-0.172]
</p><p>58 In other words, after we determine the nc assignments of data points to clusters, we update the means as the sample mean of the data points in each cluster. [sent-119, score-0.281]
</p><p>59 This is equivalent to the standard k-means cluster mean update step. [sent-120, score-0.268]
</p><p>60 2 Objective function and algorithm From the above asymptotic analysis of the Gibbs sampler, we observe a new algorithm which can be utilized for hard clustering. [sent-122, score-0.279]
</p><p>61 It is as simple as the popular k-means algorithm, but also provides the ability to adapt the number of clusters depending on the data as well as incorporate different distortion measures. [sent-123, score-0.229]
</p><p>62 , xn , λ > 0, and µ1 =  1 n  n i=1  xn  • Assignment: for each data point xi , compute the Bregman divergence Dφ (xi , µc ) to all existing clusters. [sent-127, score-0.226]
</p><p>63 If minc Dφ (xi , µc ) ≤ λ, then zi,c0 = 1 where c0 = argminc Dφ (xi , µc ); otherwise, start a new cluster and set zi,cnew = 1; • Mean Update: compute the cluster mean for each cluster, µj = the set of points in the j-th cluster. [sent-128, score-0.481]
</p><p>64 1 |lj |  x∈lj  x, where lj is  We iterate between the assignment and mean update steps until local convergence. [sent-129, score-0.199]
</p><p>65 Recall that the squared Euclidean distance is the Bregman divergence corresponding to the Gaussian distribution. [sent-133, score-0.161]
</p><p>66 In the context of clustering, a hierarchical mixture allows one to cluster multiple groups of data—each group is clustered into a set of local clusters, but these local clusters are shared among the groups (i. [sent-143, score-0.798]
</p><p>67 , sets of local clusters across groups form global clusters, with a shared global mean). [sent-145, score-0.402]
</p><p>68 For Bayesian nonparametric mixture models, one way of achieving such hierarchies arises via the hierarchical Dirichlet Process (HDP) [4], which provides a nonparametric approach to allow sharing of clusters among a set of DP mixtures. [sent-146, score-0.576]
</p><p>69 In particular, our approach opens the door to hard hierarchical algorithms over discrete data, such as text, and we brieﬂy discuss an application of our derived algorithm to topic modeling. [sent-150, score-0.5]
</p><p>70 The HDP model can be viewed as clustering each data set into local clusters, but where each local cluster is associated to a global mean. [sent-156, score-0.574]
</p><p>71 , µg ), the associations of data points to local clusters, zij , and the associations of local clusters to global means, vjt , where t indexes the local clusters for a data set. [sent-161, score-0.758]
</p><p>72 A standard Gibbs sampler considers updates on all of these variables, and in the nonparametric setting does not ﬁx the number of local or global clusters. [sent-162, score-0.316]
</p><p>73 As opposed to the ﬂat model, the hard HDP requires two parameters: a value λtop which is utilized when starting a global (top-level) cluster, and a value λbottom which is utilized when starting a local cluster. [sent-164, score-0.349]
</p><p>74 The resulting hard clustering algorithm ﬁrst performs local assignment moves on the zij , then updates the local cluster assignments, and ﬁnally updates all global means. [sent-165, score-0.832]
</p><p>75 The resulting objective function that is monotonically minimized by our algorithm is given as follows: k  min  {lc }k c=1  Dφ (xij , µc ) + λbottom t + λtop k,  (4)  c=1 xij ∈lc  where k is the total number of global clusters and t is the total number of local clusters. [sent-166, score-0.352]
</p><p>76 The bottomlevel penalty term λbottom controls both the number of local and top-level clusters, where larger λbottom tends to give fewer local clusters and more top-level clusters. [sent-167, score-0.339]
</p><p>77 Clustering via an asymptotic multinomial DP mixture considerably outperforms the asymptotic Gaussian DP mixture; see text for details. [sent-171, score-0.492]
</p><p>78 (Right) Elapsed time per iteration in seconds of our topic modeling algorithm when running on the NIPS data, as a function of the number of topics. [sent-172, score-0.16]
</p><p>79 5  Experiments  We conclude with a brief set of experiments highlighting applications of our analysis to discrete-data problems, namely image clustering and topic modeling. [sent-173, score-0.337]
</p><p>80 Each image is processed via standard visual-bag-of-words: SIFT is densely applied on top of image patches in image, and the resulting SIFT vectors are quantized into 1000 visual words. [sent-178, score-0.115]
</p><p>81 We explore whether the discrete version of our hard clustering algorithm based on a multinomial DP mixture outperforms the Gaussian mixture version (i. [sent-181, score-0.686]
</p><p>82 For both the Gaussian and multinomial cases, we utilize a farthest-ﬁrst approach for both selecting λ as well as initializing the clusters (see [3] for a discussion of farthest-ﬁrst for selecting λ). [sent-184, score-0.337]
</p><p>83 We compute the normalized mutual information (NMI) between the true clusters and the results of the two algorithms on this difﬁcult data set. [sent-185, score-0.201]
</p><p>84 06 on this data, whereas the hard multinomial version achieves a score of . [sent-187, score-0.233]
</p><p>85 Note that comparisons between the Gibbs sampler and the corresponding hard clustering algorithm for the Gaussian case were considered in [3], where experiments on several data sets showed comparable clustering accuracy results between the sampler and the hard clustering method. [sent-191, score-0.997]
</p><p>86 Furthermore, for a fully Bayesian model that places a prior on the concentration parameter, the sampler was shown to be considerably slower than the corresponding hard clustering method. [sent-192, score-0.487]
</p><p>87 Given the similarity of the sampler for the Gaussian and multinomial case, we expect similar behavior with the multinomial Gibbs sampler. [sent-193, score-0.327]
</p><p>88 We also highlight an application to topic modeling, by providing some qualitative results over two common document collections. [sent-195, score-0.152]
</p><p>89 Utilizing our general algorithm for a hard version of the multinomial HDP is straightforward. [sent-196, score-0.233]
</p><p>90 In order to apply the hard hierarchical algorithm to topic modeling, we simply utilize the discrete KL-divergence in the hard exponential family HDP, since topic modeling for text uses a multinomial distribution for the data likelihood. [sent-197, score-1.11]
</p><p>91 To test topic modeling using our asymptotic approach, we performed analyses using the NIPS 1-121 and the NYTimes [15] datasets. [sent-198, score-0.255]
</p><p>92 The prevailing metric to measure the goodness of topic models is perplexity; however, this is based on the predictive probability, which has no counterpart in the hard clustering case. [sent-206, score-0.422]
</p><p>93 Furthermore, ground truth for topic models is difﬁcult to obtain. [sent-207, score-0.125]
</p><p>94 This makes quantitative comparisons difﬁcult for topic modeling, and so we therefore focus on qualitative results. [sent-208, score-0.155]
</p><p>95 6  Conclusion  We considered a general small-variance asymptotic analysis for the exponential family DP and HDP mixture model. [sent-213, score-0.506]
</p><p>96 Crucially, this analysis allows us to move beyond the Gaussian distribution in such models, and opens the door to new clustering applications, such as those involving discrete data. [sent-214, score-0.338]
</p><p>97 Our analysis utilizes connections between Bregman divergences and exponential families, and results in a simple and scalable hard clustering algorithm which may be viewed as generalizing existing non-Bayesian Bregman clustering algorithms [7] as well as the DP-means algorithm [3]. [sent-215, score-1.02]
</p><p>98 We plan to continue to focus on the difﬁcult problem of quantitative evaluations comparing probabilistic and non-probabilistic methods for clustering, particularly for topic models. [sent-217, score-0.204]
</p><p>99 We also plan to compare our algorithms with recent online inference schemes for topic modeling, particularly the online LDA [16] and online HDP [17] algorithms. [sent-218, score-0.125]
</p><p>100 Markov chain sampling methods for Dirichlet process mixture models. [sent-299, score-0.126]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bregman', 0.392), ('asymptotics', 0.25), ('hdp', 0.25), ('dp', 0.215), ('cluster', 0.207), ('clusters', 0.201), ('clustering', 0.169), ('divergences', 0.167), ('gibbs', 0.149), ('exponential', 0.147), ('nytimes', 0.143), ('family', 0.138), ('divergence', 0.129), ('dirichlet', 0.129), ('hard', 0.128), ('mixture', 0.126), ('topic', 0.125), ('sampler', 0.117), ('cxi', 0.115), ('multinomial', 0.105), ('nc', 0.105), ('cov', 0.104), ('xi', 0.097), ('asymptotic', 0.095), ('exp', 0.092), ('cnew', 0.086), ('xc', 0.085), ('hierarchical', 0.078), ('lc', 0.074), ('door', 0.072), ('scaled', 0.071), ('zi', 0.069), ('opens', 0.065), ('scalable', 0.062), ('nonparametric', 0.062), ('conjugate', 0.061), ('bayesian', 0.06), ('elephant', 0.057), ('utilized', 0.056), ('covariance', 0.055), ('local', 0.055), ('lj', 0.054), ('global', 0.054), ('connections', 0.053), ('mixtures', 0.053), ('imagenet', 0.051), ('generalizing', 0.051), ('probabilistic', 0.049), ('assignments', 0.047), ('hierarchies', 0.047), ('moves', 0.046), ('indicators', 0.045), ('persian', 0.044), ('gaussian', 0.043), ('image', 0.043), ('country', 0.042), ('nmi', 0.042), ('xij', 0.042), ('prior', 0.04), ('euclidean', 0.04), ('utilizes', 0.04), ('won', 0.038), ('groups', 0.038), ('text', 0.038), ('brie', 0.038), ('likelihood', 0.037), ('probabilities', 0.037), ('modeling', 0.035), ('vision', 0.035), ('associations', 0.035), ('scalability', 0.035), ('assigned', 0.035), ('em', 0.034), ('viewed', 0.034), ('posterior', 0.034), ('points', 0.034), ('public', 0.033), ('zij', 0.033), ('considerably', 0.033), ('performing', 0.033), ('mean', 0.033), ('squared', 0.032), ('kulis', 0.032), ('favors', 0.032), ('families', 0.032), ('discrete', 0.032), ('utilize', 0.031), ('goes', 0.031), ('collapsed', 0.03), ('jordan', 0.03), ('quantitative', 0.03), ('pca', 0.029), ('topics', 0.029), ('processed', 0.029), ('assignment', 0.029), ('update', 0.028), ('controls', 0.028), ('distortion', 0.028), ('updates', 0.028), ('document', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="316-tfidf-1" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>2 0.2781415 <a title="316-tfidf-2" href="./nips-2012-Submodular-Bregman_and_the_Lov%C3%A1sz-Bregman_Divergences_with_Applications.html">328 nips-2012-Submodular-Bregman and the Lovász-Bregman Divergences with Applications</a></p>
<p>Author: Rishabh Iyer, Jeff A. Bilmes</p><p>Abstract: We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular-Bregman divergences. We consider two kinds, deﬁned either from tight modular upper or tight modular lower bounds of a submodular function. We show that the properties of these divergences are analogous to the (standard continuous) Bregman divergence. We demonstrate how they generalize many useful divergences, including the weighted Hamming distance, squared weighted Hamming, weighted precision, recall, conditional mutual information, and a generalized KL-divergence on sets. We also show that the generalized Bregman divergence on the Lov´ sz extension of a submodular function, which we a call the Lov´ sz-Bregman divergence, is a continuous extension of a submodular a Bregman divergence. We point out a number of applications, and in particular show that a proximal algorithm deﬁned through the submodular Bregman divergence provides a framework for many mirror-descent style algorithms related to submodular function optimization. We also show that a generalization of the k-means algorithm using the Lov´ sz Bregman divergence is natural in clustering scenarios where a ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efﬁcient unlike other order based distance measures. 1</p><p>3 0.27225909 <a title="316-tfidf-3" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>4 0.25134066 <a title="316-tfidf-4" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: We present a truncation-free stochastic variational inference algorithm for Bayesian nonparametric models. While traditional variational inference algorithms require truncations for the model or the variational distribution, our method adapts model complexity on the ﬂy. We studied our method with Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large data sets. Our method performs better than previous stochastic variational inference algorithms. 1</p><p>5 0.1754355 <a title="316-tfidf-5" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>Author: Yudong Chen, Sujay Sanghavi, Huan Xu</p><p>Abstract: We develop a new algorithm to cluster sparse unweighted graphs – i.e. partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph. Sparsity makes the problem noisier, and hence more difﬁcult to solve. Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case, these must be penalized differently. We analyze our algorithm’s performance on the natural, classical and widely studied “planted partition” model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods. This is seen empirically as well. 1</p><p>6 0.17354874 <a title="316-tfidf-6" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>7 0.16629681 <a title="316-tfidf-7" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>8 0.16126262 <a title="316-tfidf-8" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>9 0.13660081 <a title="316-tfidf-9" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>10 0.12581603 <a title="316-tfidf-10" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>11 0.12373737 <a title="316-tfidf-11" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>12 0.12331355 <a title="316-tfidf-12" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>13 0.12244976 <a title="316-tfidf-13" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>14 0.11995809 <a title="316-tfidf-14" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>15 0.11969399 <a title="316-tfidf-15" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>16 0.11889536 <a title="316-tfidf-16" href="./nips-2012-A_nonparametric_variable_clustering_model.html">26 nips-2012-A nonparametric variable clustering model</a></p>
<p>17 0.11392244 <a title="316-tfidf-17" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>18 0.11378959 <a title="316-tfidf-18" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>19 0.11181091 <a title="316-tfidf-19" href="./nips-2012-Bayesian_estimation_of_discrete_entropy_with_mixtures_of_stick-breaking_priors.html">57 nips-2012-Bayesian estimation of discrete entropy with mixtures of stick-breaking priors</a></p>
<p>20 0.10855456 <a title="316-tfidf-20" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.282), (1, 0.117), (2, -0.045), (3, -0.024), (4, -0.275), (5, -0.074), (6, -0.031), (7, -0.071), (8, 0.161), (9, -0.019), (10, 0.23), (11, -0.116), (12, -0.023), (13, -0.061), (14, 0.142), (15, -0.103), (16, -0.15), (17, -0.044), (18, -0.14), (19, -0.008), (20, -0.03), (21, -0.059), (22, 0.038), (23, 0.083), (24, 0.059), (25, -0.01), (26, -0.065), (27, -0.018), (28, 0.04), (29, 0.063), (30, -0.018), (31, 0.005), (32, -0.025), (33, -0.035), (34, 0.013), (35, 0.06), (36, 0.051), (37, -0.042), (38, 0.015), (39, -0.075), (40, 0.004), (41, -0.046), (42, -0.001), (43, 0.131), (44, 0.038), (45, -0.06), (46, 0.009), (47, 0.063), (48, 0.022), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95730859 <a title="316-lsi-1" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>2 0.79858261 <a title="316-lsi-2" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>3 0.76380819 <a title="316-lsi-3" href="./nips-2012-A_nonparametric_variable_clustering_model.html">26 nips-2012-A nonparametric variable clustering model</a></p>
<p>Author: Konstantina Palla, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to ﬁnd a disjoint partition, i.e. a simple clustering, of observed variables into highly correlated subsets. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. Our Dirichlet process variable clustering (DPVC) model can discover blockdiagonal covariance structures in data. We evaluate our method on both synthetic and gene expression analysis problems. 1</p><p>4 0.73138893 <a title="316-lsi-4" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>Author: Argyris Kalogeratos, Aristidis Likas</p><p>Abstract: Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that can be used as a wrapper around any iterative clustering algorithm of k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as an individual ‘viewer’ and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of distances between the viewer and the cluster members. Important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artiﬁcial and real datasets indicate the eﬀectiveness of our method and its superiority over analogous approaches.</p><p>5 0.70552397 <a title="316-lsi-5" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>Author: Deepak Venugopal, Vibhav Gogate</p><p>Abstract: First-order probabilistic models combine the power of ﬁrst-order logic, the de facto tool for handling relational structure, with probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the accuracy and scalability of existing graphical models’ inference algorithms by exploiting symmetry in the ﬁrst-order representation. In this paper, we consider blocked Gibbs sampling, an advanced MCMC scheme, and lift it to the ﬁrst-order level. We propose to achieve this by partitioning the ﬁrst-order atoms in the model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing the clusters and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy, scalability and convergence.</p><p>6 0.66814804 <a title="316-lsi-6" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>7 0.65788704 <a title="316-lsi-7" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>8 0.65520561 <a title="316-lsi-8" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>9 0.60465616 <a title="316-lsi-9" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>10 0.59321564 <a title="316-lsi-10" href="./nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">299 nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>11 0.59035569 <a title="316-lsi-11" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>12 0.58116883 <a title="316-lsi-12" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>13 0.5696106 <a title="316-lsi-13" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>14 0.54928827 <a title="316-lsi-14" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>15 0.54829389 <a title="316-lsi-15" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>16 0.5435366 <a title="316-lsi-16" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>17 0.53086174 <a title="316-lsi-17" href="./nips-2012-Variational_Inference_for_Crowdsourcing.html">359 nips-2012-Variational Inference for Crowdsourcing</a></p>
<p>18 0.51415503 <a title="316-lsi-18" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>19 0.49792403 <a title="316-lsi-19" href="./nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</a></p>
<p>20 0.4958474 <a title="316-lsi-20" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.074), (17, 0.016), (21, 0.043), (38, 0.126), (39, 0.01), (42, 0.052), (54, 0.025), (55, 0.027), (63, 0.169), (74, 0.067), (76, 0.144), (80, 0.102), (92, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92698961 <a title="316-lda-1" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>Author: Dahua Lin, John W. Fisher</p><p>Abstract: Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Speciﬁcally, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from previous ones that require the model structure to be a tree or a chain, allowing more ﬂexible designs. We also derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling. 1</p><p>2 0.89102435 <a title="316-lda-2" href="./nips-2012-Compressive_Sensing_MRI_with_Wavelet_Tree_Sparsity.html">78 nips-2012-Compressive Sensing MRI with Wavelet Tree Sparsity</a></p>
<p>Author: Chen Chen, Junzhou Huang</p><p>Abstract: In Compressive Sensing Magnetic Resonance Imaging (CS-MRI), one can reconstruct a MR image with good quality from only a small number of measurements. This can signiﬁcantly reduce MR scanning time. According to structured sparsity theory, the measurements can be further reduced to O(K + log n) for tree-sparse data instead of O(K + K log n) for standard K-sparse data with length n. However, few of existing algorithms have utilized this for CS-MRI, while most of them model the problem with total variation and wavelet sparse regularization. On the other side, some algorithms have been proposed for tree sparse regularization, but few of them have validated the beneﬁt of wavelet tree structure in CS-MRI. In this paper, we propose a fast convex optimization algorithm to improve CS-MRI. Wavelet sparsity, gradient sparsity and tree sparsity are all considered in our model for real MR images. The original complex problem is decomposed into three simpler subproblems then each of the subproblems can be efﬁciently solved with an iterative scheme. Numerous experiments have been conducted and show that the proposed algorithm outperforms the state-of-the-art CS-MRI algorithms, and gain better reconstructions results on real MR images than general tree based solvers or algorithms. 1</p><p>same-paper 3 0.86217368 <a title="316-lda-3" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>4 0.80090505 <a title="316-lda-4" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: We present a truncation-free stochastic variational inference algorithm for Bayesian nonparametric models. While traditional variational inference algorithms require truncations for the model or the variational distribution, our method adapts model complexity on the ﬂy. We studied our method with Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large data sets. Our method performs better than previous stochastic variational inference algorithms. 1</p><p>5 0.80049574 <a title="316-lda-5" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>Author: Vasiliy Karasev, Alessandro Chiuso, Stefano Soatto</p><p>Abstract: We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of “visual search” of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a “passive” agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an “omnipotent” agent, capable of inﬁnite control authority, can achieve arbitrarily good performance (asymptotically). In between these limiting cases, the tradeoff can be characterized empirically. 1</p><p>6 0.79876429 <a title="316-lda-6" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>7 0.79814065 <a title="316-lda-7" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>8 0.79766017 <a title="316-lda-8" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>9 0.79741257 <a title="316-lda-9" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>10 0.79668087 <a title="316-lda-10" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>11 0.79642773 <a title="316-lda-11" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>12 0.7961489 <a title="316-lda-12" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>13 0.79593176 <a title="316-lda-13" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>14 0.79545248 <a title="316-lda-14" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>15 0.79488933 <a title="316-lda-15" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>16 0.79463601 <a title="316-lda-16" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>17 0.79268485 <a title="316-lda-17" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>18 0.7926048 <a title="316-lda-18" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>19 0.79023629 <a title="316-lda-19" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>20 0.78977787 <a title="316-lda-20" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
