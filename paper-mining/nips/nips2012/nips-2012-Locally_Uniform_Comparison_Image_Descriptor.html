<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 nips-2012-Locally Uniform Comparison Image Descriptor</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-202" href="#">nips2012-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 nips-2012-Locally Uniform Comparison Image Descriptor</h1>
<br/><p>Source: <a title="nips-2012-202-pdf" href="http://papers.nips.cc/paper/4706-locally-uniform-comparison-image-descriptor.pdf">pdf</a></p><p>Author: Andrew Ziegler, Eric Christiansen, David Kriegman, Serge J. Belongie</p><p>Abstract: Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. However, SIFT and SURF do not perform well for real-time or mobile applications. As an alternative very fast binary descriptors like BRIEF and related methods use pairwise comparisons of pixel intensities in an image patch. We present an analysis of BRIEF and related approaches revealing that they are hashing schemes on the ordinal correlation metric Kendall’s tau. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on linear time permutation distances between the ordering of RGB values of two image patches. LUCID is computable in linear time with respect to the number of pixels and does not require ﬂoating point computation. 1</p><p>Reference: <a title="nips-2012-202-reference" href="../nips2012_reference/nips-2012-Locally_Uniform_Comparison_Image_Descriptor_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lucid', 0.495), ('brief', 0.356), ('kendal', 0.28), ('tau', 0.243), ('cayley', 0.209), ('permut', 0.204), ('surf', 0.178), ('patch', 0.164), ('ham', 0.164), ('bik', 0.154), ('im', 0.141), ('pixel', 0.138), ('keypoint', 0.134), ('intens', 0.132), ('impuls', 0.123), ('dist', 0.112), ('fkd', 0.11), ('skd', 0.11), ('sift', 0.102), ('calond', 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="202-tfidf-1" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>Author: Andrew Ziegler, Eric Christiansen, David Kriegman, Serge J. Belongie</p><p>Abstract: Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. However, SIFT and SURF do not perform well for real-time or mobile applications. As an alternative very fast binary descriptors like BRIEF and related methods use pairwise comparisons of pixel intensities in an image patch. We present an analysis of BRIEF and related approaches revealing that they are hashing schemes on the ordinal correlation metric Kendall’s tau. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on linear time permutation distances between the ordering of RGB values of two image patches. LUCID is computable in linear time with respect to the number of pixels and does not require ﬂoating point computation. 1</p><p>2 0.16063912 <a title="202-tfidf-2" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>Author: Tomasz Trzcinski, Mario Christoudias, Vincent Lepetit, Pascal Fua</p><p>Abstract: In this paper we apply boosting to learn complex non-linear local visual feature representations, drawing inspiration from its successful application to visual object detection. The main goal of local feature descriptors is to distinctively represent a salient image region while remaining invariant to viewpoint and illumination changes. This representation can be improved using machine learning, however, past approaches have been mostly limited to learning linear feature mappings in either the original input or a kernelized input feature space. While kernelized methods have proven somewhat effective for learning non-linear local feature descriptors, they rely heavily on the choice of an appropriate kernel function whose selection is often difﬁcult and non-intuitive. We propose to use the boosting-trick to obtain a non-linear mapping of the input to a high-dimensional feature space. The non-linear feature mapping obtained with the boosting-trick is highly intuitive. We employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known SIFT. As demonstrated in our experiments, the resulting descriptor can be learned directly from intensity patches achieving state-of-the-art performance. 1</p><p>3 0.15404835 <a title="202-tfidf-3" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family, even if the distributions are very heavy-tailed, have inﬁnite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness. Both theories and experiments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and robustness at almost no cost. 1</p><p>4 0.13974182 <a title="202-tfidf-4" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>Author: Mohammad Norouzi, David M. Blei, Ruslan Salakhutdinov</p><p>Abstract: Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efﬁcient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a ﬂexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST, with promising classiﬁcation results using no more than kNN on the binary codes. 1</p><p>5 0.11084903 <a title="202-tfidf-5" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>6 0.10586489 <a title="202-tfidf-6" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>7 0.096066885 <a title="202-tfidf-7" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>8 0.094833419 <a title="202-tfidf-8" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>9 0.091195144 <a title="202-tfidf-9" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>10 0.091098197 <a title="202-tfidf-10" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>11 0.0831898 <a title="202-tfidf-11" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>12 0.08280509 <a title="202-tfidf-12" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>13 0.079831496 <a title="202-tfidf-13" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>14 0.078473613 <a title="202-tfidf-14" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>15 0.077014752 <a title="202-tfidf-15" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>16 0.075876489 <a title="202-tfidf-16" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>17 0.073748849 <a title="202-tfidf-17" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>18 0.073748849 <a title="202-tfidf-18" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>19 0.066574074 <a title="202-tfidf-19" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>20 0.065207414 <a title="202-tfidf-20" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.059), (2, -0.017), (3, 0.136), (4, -0.102), (5, 0.01), (6, -0.003), (7, -0.044), (8, -0.071), (9, 0.027), (10, -0.024), (11, -0.063), (12, 0.028), (13, -0.023), (14, -0.038), (15, 0.04), (16, -0.029), (17, -0.0), (18, 0.038), (19, 0.075), (20, -0.064), (21, -0.022), (22, -0.021), (23, -0.103), (24, 0.031), (25, -0.191), (26, -0.082), (27, 0.089), (28, 0.034), (29, 0.053), (30, 0.011), (31, -0.113), (32, 0.055), (33, 0.032), (34, -0.048), (35, 0.005), (36, 0.038), (37, 0.011), (38, -0.034), (39, 0.064), (40, 0.027), (41, 0.017), (42, 0.061), (43, -0.069), (44, 0.016), (45, 0.06), (46, -0.066), (47, -0.028), (48, -0.016), (49, -0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89996672 <a title="202-lsi-1" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>Author: Andrew Ziegler, Eric Christiansen, David Kriegman, Serge J. Belongie</p><p>Abstract: Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. However, SIFT and SURF do not perform well for real-time or mobile applications. As an alternative very fast binary descriptors like BRIEF and related methods use pairwise comparisons of pixel intensities in an image patch. We present an analysis of BRIEF and related approaches revealing that they are hashing schemes on the ordinal correlation metric Kendall’s tau. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on linear time permutation distances between the ordering of RGB values of two image patches. LUCID is computable in linear time with respect to the number of pixels and does not require ﬂoating point computation. 1</p><p>2 0.66058844 <a title="202-lsi-2" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>Author: Aditya Khosla, Jianxiong Xiao, Antonio Torralba, Aude Oliva</p><p>Abstract: While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. 1</p><p>3 0.61997628 <a title="202-lsi-3" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>Author: Daniel Zoran, Yair Weiss</p><p>Abstract: Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models. 1 GMMs and natural image statistics models Many models for the statistics of natural image patches have been suggested in recent years. Finding good models for natural images is important to many different research areas - computer vision, biological vision and neuroscience among others. Recently, there has been a growing interest in comparing different aspects of models for natural images such as log-likelihood and multi-information reduction performance, and much progress has been achieved [1,2, 3,4,5, 6]. Out of these results there is one which is particularly interesting: simple, unconstrained Gaussian Mixture Models (GMMs) with a relatively small number of mixture components learned from image patches are extraordinarily good in modeling image statistics [6, 4]. This is a surprising result due to the simplicity of GMMs and their ubiquity. Another surprising aspect of this result is that many of the current models may be thought of as GMMs with an exponential or infinite number of components, having different constraints on the covariance structure of the mixture components. In this work we study the nature of GMMs learned from natural image patches. We start with a thorough comparison to some popular and cutting edge image models. We show that indeed, GMMs are excellent performers in modeling natural image patches. We then analyze what properties of natural images these GMMs capture, their dependence on the number of components in the mixture and their relation to the structure of the world around us. Finally, we show that the learned GMM suggests a strong connection between natural image statistics and a simple variant of the dead leaves model [7, 8] , explicitly modeling occlusions and explaining some of the success of GMMs in modeling natural images. 1 3.5 .,...- ••.......-.-.. -..---'-. 1 ~~6\8161·· -.. .-.. --...--.-- ---..-.- -. --------------MII+··+ilIl ..... .. . . ~ '[25 . . . ---- ] B'II 1_ -- ~2 ;t:: fI 1 - --- ,---- ._.. : 61.5 ..... '</p><p>4 0.60806054 <a title="202-lsi-4" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>5 0.5843327 <a title="202-lsi-5" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>Author: Tatsuya Harada, Yasuo Kuniyoshi</p><p>Abstract: This paper proposes a novel image representation called a Graphical Gaussian Vector (GGV), which is a counterpart of the codebook and local feature matching approaches. We model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efﬁciently represent the spatial relationship among local features. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Then we deﬁne a new image feature by embedding the proper metric into the parameters, which can be directly applied to scalable linear classiﬁers. We show that the GGV obtains better performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. 1</p><p>6 0.57825589 <a title="202-lsi-6" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>7 0.57346135 <a title="202-lsi-7" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>8 0.55678809 <a title="202-lsi-8" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>9 0.54820257 <a title="202-lsi-9" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>10 0.53957129 <a title="202-lsi-10" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>11 0.51934236 <a title="202-lsi-11" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>12 0.51837194 <a title="202-lsi-12" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>13 0.51649022 <a title="202-lsi-13" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>14 0.51532555 <a title="202-lsi-14" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>15 0.50976539 <a title="202-lsi-15" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>16 0.500184 <a title="202-lsi-16" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>17 0.49869373 <a title="202-lsi-17" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>18 0.47552681 <a title="202-lsi-18" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>19 0.4598147 <a title="202-lsi-19" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>20 0.45144293 <a title="202-lsi-20" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.338), (11, 0.109), (37, 0.023), (47, 0.075), (64, 0.024), (67, 0.078), (70, 0.049), (85, 0.054), (94, 0.061), (96, 0.012), (99, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68376416 <a title="202-lda-1" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>Author: Andrew Ziegler, Eric Christiansen, David Kriegman, Serge J. Belongie</p><p>Abstract: Keypoint matching between pairs of images using popular descriptors like SIFT or a faster variant called SURF is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion. However, SIFT and SURF do not perform well for real-time or mobile applications. As an alternative very fast binary descriptors like BRIEF and related methods use pairwise comparisons of pixel intensities in an image patch. We present an analysis of BRIEF and related approaches revealing that they are hashing schemes on the ordinal correlation metric Kendall’s tau. Here, we introduce Locally Uniform Comparison Image Descriptor (LUCID), a simple description method based on linear time permutation distances between the ordering of RGB values of two image patches. LUCID is computable in linear time with respect to the number of pixels and does not require ﬂoating point computation. 1</p><p>2 0.58509195 <a title="202-lda-2" href="./nips-2012-Variational_Inference_for_Crowdsourcing.html">359 nips-2012-Variational Inference for Crowdsourcing</a></p>
<p>Author: Qiang Liu, Jian Peng, Alex Ihler</p><p>Abstract: Crowdsourcing has become a popular paradigm for labeling large datasets. However, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators. We approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (BP) and mean ﬁeld (MF). We show that our BP algorithm generalizes both majority voting and a recent algorithm by Karger et al. [1], while our MF method is closely related to a commonly used EM algorithm. In both cases, we ﬁnd that the performance of the algorithms critically depends on the choice of a prior distribution on the workers’ reliability; by choosing the prior properly, both BP and MF (and EM) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions. 1</p><p>3 0.5551151 <a title="202-lda-3" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>Author: Brett Vintch, Andrew Zaharia, J Movshon, Hhmi) Hhmi), Eero P. Simoncelli</p><p>Abstract: Many visual and auditory neurons have response properties that are well explained by pooling the rectiﬁed responses of a set of spatially shifted linear ﬁlters. These ﬁlters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple ﬁlters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the ﬁlters reside rather than the ﬁlters themselves. Here, we assume a linear-nonlinear–linear-nonlinear (LN-LN) cascade model in which the ﬁrst linear stage is a set of shifted (‘convolutional’) copies of a common ﬁlter, and the ﬁrst nonlinear stage consists of rectifying scalar nonlinearities that are identical for all ﬁlter outputs. We refer to these initial LN elements as the ‘subunits’ of the receptive ﬁeld. The second linear stage then computes a weighted sum of the responses of the rectiﬁed subunits. We present a method for directly ﬁtting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model signiﬁcantly outperforms STA and STC in terms of cross-validated accuracy and efﬁciency. 1</p><p>4 0.55479473 <a title="202-lda-4" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>Author: Ping Li, Art Owen, Cun-hui Zhang</p><p>Abstract: Minwise hashing is a standard procedure in the context of search, for efﬁciently estimating set similarities in massive binary data such as text. Recently, b-bit minwise hashing has been applied to large-scale learning and sublinear time nearneighbor search. The major drawback of minwise hashing is the expensive preprocessing, as the method requires applying (e.g.,) k = 200 to 500 permutations on the data. This paper presents a simple solution called one permutation hashing. Conceptually, given a binary data matrix, we permute the columns once and divide the permuted columns evenly into k bins; and we store, for each data vector, the smallest nonzero location in each bin. The probability analysis illustrates that this one permutation scheme should perform similarly to the original (k-permutation) minwise hashing. Our experiments with training SVM and logistic regression conﬁrm that one permutation hashing can achieve similar (or even better) accuracies compared to the k-permutation scheme. See more details in arXiv:1208.1259.</p><p>5 0.48740655 <a title="202-lda-5" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>Author: Bogdan Alexe, Nicolas Heess, Yee W. Teh, Vittorio Ferrari</p><p>Abstract: The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired. We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image, exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while achieving higher object detection performance. 1</p><p>6 0.48594642 <a title="202-lda-6" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>7 0.48336264 <a title="202-lda-7" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>8 0.48099664 <a title="202-lda-8" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>9 0.48094389 <a title="202-lda-9" href="./nips-2012-Diffusion_Decision_Making_for_Adaptive_k-Nearest_Neighbor_Classification.html">97 nips-2012-Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification</a></p>
<p>10 0.47959974 <a title="202-lda-10" href="./nips-2012-Learning_Multiple_Tasks_using_Shared_Hypotheses.html">181 nips-2012-Learning Multiple Tasks using Shared Hypotheses</a></p>
<p>11 0.47675982 <a title="202-lda-11" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>12 0.47652292 <a title="202-lda-12" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>13 0.4735066 <a title="202-lda-13" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>14 0.47345158 <a title="202-lda-14" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>15 0.47042236 <a title="202-lda-15" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>16 0.46963257 <a title="202-lda-16" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>17 0.46904328 <a title="202-lda-17" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>18 0.46883383 <a title="202-lda-18" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>19 0.46862209 <a title="202-lda-19" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>20 0.46817702 <a title="202-lda-20" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
