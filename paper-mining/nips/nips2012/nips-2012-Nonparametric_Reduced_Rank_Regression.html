<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>247 nips-2012-Nonparametric Reduced Rank Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-247" href="#">nips2012-247</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>247 nips-2012-Nonparametric Reduced Rank Regression</h1>
<br/><p>Source: <a title="nips-2012-247-pdf" href="http://papers.nips.cc/paper/4843-nonparametric-reduced-rank-regression.pdf">pdf</a></p><p>Author: Rina Foygel, Michael Horrell, Mathias Drton, John D. Lafferty</p><p>Abstract: We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models. An additive model is estimated for each dimension of a q-dimensional response, with a shared p-dimensional predictor variable. To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank. Backﬁtting algorithms are derived and justiﬁed using a nonparametric form of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting. The methods are illustrated on gene expression data. 1</p><p>Reference: <a title="nips-2012-247-reference" href="../nips2012_reference/nips-2012-Nonparametric_Reduced_Rank_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An additive model is estimated for each dimension of a q-dimensional response, with a shared p-dimensional predictor variable. [sent-2, score-0.12]
</p><p>2 To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank. [sent-3, score-0.242]
</p><p>3 Backﬁtting algorithms are derived and justiﬁed using a nonparametric form of the nuclear norm subdifferential. [sent-4, score-0.323]
</p><p>4 Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting. [sent-5, score-0.18]
</p><p>5 1  Introduction  In the multivariate regression problem the objective is to estimate the conditional mean E(Y ∣ X) = m(X) = (m1 (X), . [sent-7, score-0.142]
</p><p>6 , mq (X))⊺ where Y is a q-dimensional response vector and X is a pdimensional covariate vector. [sent-10, score-0.229]
</p><p>7 Under a linear model, the mean is estimated as m(X) = BX where B ∈ Rq×p is a q × p matrix of regression coefﬁcients. [sent-13, score-0.147]
</p><p>8 In reduced rank regression the matrix B is estimated under a rank constraint r = rank(B) ≤ C, so that the rows or columns of B lie in an r-dimensional subspace of Rq or Rp . [sent-15, score-0.703]
</p><p>9 In low dimensions, the constrained rank model can be computed as an orthogonal projection of the least squares solution; but in high dimensions this is not well deﬁned. [sent-17, score-0.35]
</p><p>10 Recent research has studied the use of the nuclear norm as a convex surrogate for the rank constraint. [sent-18, score-0.475]
</p><p>11 The nuclear norm ∥B∥∗ , also known as the trace or Ky-Fan norm, is the sum of the singular vectors of B. [sent-19, score-0.308]
</p><p>12 A rank constraint can be thought of as imposing sparsity, but in an unknown basis; the nuclear norm plays the role of the 1 norm in sparse estimation. [sent-20, score-0.557]
</p><p>13 Its use for low rank estimation problems was proposed by Fazel in [2]. [sent-21, score-0.294]
</p><p>14 More recently, nuclear norm regularization in multivariate linear regression has been studied by Yuan et al. [sent-22, score-0.403]
</p><p>15 In this paper we study nonparametric parallels of reduced rank linear models. [sent-24, score-0.445]
</p><p>16 We focus our attention on additive models, so that the regression function m(X) = (m1 (X), . [sent-25, score-0.216]
</p><p>17 , mq (X))⊺ has each component mk (X) = ∑p mk (Xj ) equal to a sum of p functions, one for each covariate. [sent-28, score-0.454]
</p><p>18 The objective j j=1 is then to estimate the q × p matrix of functions M (X) = [mk (Xj )]. [sent-29, score-0.104]
</p><p>19 j The ﬁrst problem we address, in Section 2, is to determine a replacement for the regularization penalty ∥B∥∗ in the linear model. [sent-30, score-0.261]
</p><p>20 Because we must estimate a matrix of functions, the analogue of the nuclear norm is not immediately apparent. [sent-31, score-0.28]
</p><p>21 We propose two related regularization penalties for 1  nonparametric low rank regression, and show how they specialize to the linear case. [sent-32, score-0.597]
</p><p>22 We then study, in Section 4, the (inﬁnite dimensional) subdifferential of these penalties. [sent-33, score-0.11]
</p><p>23 In the population setting, this leads to stationary conditions for the minimizer of the regularized mean squared error. [sent-34, score-0.142]
</p><p>24 This subdifferential calculus then justiﬁes penalized backﬁtting algorithms for carrying out the optimization for a ﬁnite sample. [sent-35, score-0.14]
</p><p>25 A uniform bound on the excess risk of the estimator relative to an oracle is given Section 6. [sent-38, score-0.148]
</p><p>26 The analysis requires a concentration result for nonparametric covariance matrices in the spectral norm. [sent-40, score-0.167]
</p><p>27 Experiments with gene data are given in Section 7, which are used to illustrate different facets of the proposed nonparametric reduced rank regression techniques. [sent-41, score-0.585]
</p><p>28 2  Nonparametric Nuclear Norm Penalization  We begin by presenting the penalty that we will use to induce nonparametric regression estimates to be low rank. [sent-42, score-0.467]
</p><p>29 To motivate our choice of penalty and provide some intuition, suppose that f 1 (x), . [sent-43, score-0.229]
</p><p>30 What does it mean for this collection of functions to be low rank? [sent-47, score-0.101]
</p><p>31 We require that the n × q matrix of function values F(x1∶n ) = [f k (xi )] is low rank. [sent-52, score-0.099]
</p><p>32 This matrix is of rank at most r < q for every set {xi } of arbitrary size n if and only if the functions {f k } are r-linearly independent—each function can be written as a linear combination of r of the other functions. [sent-53, score-0.35]
</p><p>33 In the multivariate regression setting, but still assuming the domain is one-dimensional for simplicity (q > 1 and p = 1), we have a random sample X1 , . [sent-54, score-0.172]
</p><p>34 , mq ) of q smooth (regression) functions, and suppose that n > q. [sent-61, score-0.132]
</p><p>35 This suggests the penalty √ ∥M∥∗ = ∑q σs (M) = ∑q λs (M⊺ M), where {λs (A)} denotes the eigenvalues of a symmetric s=1 s=1 matrix A and {σs (B)} denotes the singular values of a matrix B. [sent-63, score-0.41]
</p><p>36 Now, assuming the columns of M 1 ̂ are centered, and E[mk (X)] = 0 for each k, we recognize n M⊺ M as the sample covariance Σ(M ) k l of the population covariance Σ(M ) ∶= Cov(M (X)) = [E(m (X)m (X))]. [sent-64, score-0.207]
</p><p>37 This motivates the following sample and population penalties, where A1/2 denotes the matrix square root: population penalty: ∥Σ(M )1/2 ∥∗ = ∥ Cov(M (X))1/2 ∥∗ 1 ̂ sample penalty: ∥Σ(M )1/2 ∥∗ = √ ∥M∥∗ . [sent-65, score-0.325]
</p><p>38 We consider the family of additive models, with regression functions of the form m(X) = (m1 (X), . [sent-74, score-0.269]
</p><p>39 , mq (X))⊺ = ∑p Mj (Xj ), where each term Mj (Xj ) = (m1 (Xj ), . [sent-77, score-0.132]
</p><p>40 , mq (Xj ))⊺ is j j=1 j a q-vector of functions evaluated at Xj . [sent-80, score-0.185]
</p><p>41 Assume that the functions mk (Xj ) j j j all have mean zero; this is required for identiﬁability in the additive model. [sent-86, score-0.334]
</p><p>42 As a shorthand, let Σj = Σ(Mj ) = Cov(Mj (Xj )) denote the covariance matrix of the j-th component functions, with ̂ sample version Σj . [sent-87, score-0.116]
</p><p>43 The population and sample versions of the ﬁrst penalty are then given by 1/2  1/2  ∥Σ1 ∥∗ + ∥Σ2 ∥∗ + ⋯ + ∥Σ1/2 ∥∗ p  (3. [sent-88, score-0.366]
</p><p>44 2)  The second penalty, intuitively, encourages the set of q vector-valued functions (mk , mk , . [sent-91, score-0.214]
</p><p>45 This penalty is given by 1/2  ∥(Σ1 ⋯Σ1/2 )∥ p  (3. [sent-95, score-0.229]
</p><p>46 The corresponding p 1 population and empirical risk functionals, for the ﬁrst penalty, are then p p 2 1 1/2 E∥Y − ∑ Mj (X)∥ + λ ∑ ∥Σj ∥∗ 2 2 j=1 j=1  (3. [sent-98, score-0.21]
</p><p>47 Some straightforward calculation shows that 1/2 1/2 1/2 the penalties reduce to ∑p ∥Σj ∥∗ = ∑p ∥Bj ∥2 for the ﬁrst penalty, and ∥Σ1 ⋯Σp ∥∗ = ∥B∥∗ j=1 j=1 for the second. [sent-104, score-0.138]
</p><p>48 Thus, in the linear case the ﬁrst penalty is encouraging B to be column-wise sparse, so that many of the Bj s are zero, meaning that Xj doesn’t appear in the ﬁt. [sent-105, score-0.229]
</p><p>49 The second penalty reduces to the nuclear norm regularization ∥B∥∗ used for high-dimensional reduced-rank regression. [sent-107, score-0.49]
</p><p>50 4  Subdifferentials for Functional Matrix Norms  A key to deriving algorithms for functional low-rank regression is computation of the subdifferenk tials of the penalties. [sent-108, score-0.143]
</p><p>51 For k each column index j and row index k, fj is a function of a random variable Xj , and we will take expectations with respect to Xj implicitly. [sent-110, score-0.28]
</p><p>52 We deﬁne the inner product between two matrices of functions as p  q  p  k k ⟪F, G⟫ ∶= ∑ ∑ E(fj gj ) = ∑ E(Fj⊺ Gj ) = tr (E(F G⊺ )) , j=1 k=1  (4. [sent-112, score-0.179]
</p><p>53 In fact, these two norms are dual—for any F , ∣∣∣F ∣∣∣∗ = sup ⟪G, F ⟫ , ∣∣∣G∣∣∣sp ≤1  3  (4. [sent-117, score-0.114]
</p><p>54 The subdifferential of ∣∣∣F ∣∣∣∗ is the set √ −1 S(F ) ∶= {( E(F F ⊺ )) F + H ∶ ∣∣∣H∣∣∣sp ≤ 1, E(F H ⊺ ) = 0q×q , E(F F ⊺ )H = 0q×p a. [sent-121, score-0.11]
</p><p>55 The fact that S(F ) contains the subdifferential ∂∣∣∣F ∣∣∣∗ can be proved by comparing our setting (matrices of functions) to the ordinary matrix case; see [9, 7]. [sent-126, score-0.161]
</p><p>56 Next, let E(F F ⊺ ) = V DV ⊺ be a reduced singular value decomposition, where D is a positive diagonal matrix of size q′ ≤ q. [sent-137, score-0.194]
</p><p>57 ̃̃ This implies that E(F F ⊺ ) ⋅ E(HH ⊺ ) = 0q×q and so these two symmetric matrices have orthogonal row spans and orthogonal column spans. [sent-145, score-0.126]
</p><p>58 This gives the subdifferential of penalty 2, deﬁned in (3. [sent-148, score-0.339]
</p><p>59 We can view the ﬁrst penalty update as just a special case of the second penalty update. [sent-150, score-0.458]
</p><p>60 5) ∗  which is clearly just a special case of penalty 2 with a single q-vector of functions instead of p different q-vectors of functions. [sent-153, score-0.282]
</p><p>61 6)  5  Stationary Conditions and Backﬁtting Algorithms  Returning to the base case of p = 1 covariate, consider the population regularized risk optimization 1 (5. [sent-158, score-0.21]
</p><p>62 Let E(P P ⊺ ) = U diag(τ )U ⊺ be the singular value decomposition and deﬁne √ M = U diag([1 − λ/ τ ]+ )U ⊺ P (5. [sent-178, score-0.106]
</p><p>63 √ It remains to show that H satisﬁes the conditions of the subdifferential in (4. [sent-199, score-0.11]
</p><p>64 The analysis above justiﬁes a backﬁtting algorithm for estimating a constrained rank additive model with the ﬁrst penalty, where the objective is p p 2 1 min{ E∥Y − ∑ Mj (Xj )∥ + λ ∑ ∣∣∣Mj ∣∣∣∗ }. [sent-210, score-0.391]
</p><p>65 9)  For a given coordinate j, we form the residual Zj = Y − ∑k≠j Mk , and then compute the projection Pj = E(Zj ∣ Xj ), with singular value decomposition E(Pj Pj⊺ ) = U diag(τ )U ⊺ . [sent-212, score-0.138]
</p><p>66 This is a Gauss-Seidel procedure that parallels the population backﬁtting algorithm for S PAM [6]. [sent-215, score-0.148]
</p><p>67 In the sample version we replace the conditional expectation ̂ Pj = E(Zj ∣ Xj ) by a nonparametric linear smoother, Pj = Sj Zj . [sent-216, score-0.124]
</p><p>68 6  Excess Risk Bounds  The population risk of a q × p regression matrix M (X) = [M1 (X1 )⋯Mp (Xp )] is R(M ) = E∥Y − M (X)1p ∥2 , 2 ̂ with sample version denoted R(M ). [sent-223, score-0.387]
</p><p>69 The population risk can be reexpressed as ⊺  ⊺  −I Y Y −I R(M ) = tr {( q⊺ ) E [( )( ) ] ( q⊺ )} V (X)⊺ V (X)⊺ DU DU ⊺  ΣY −I = tr {( q⊺ ) ( ⊺ Y DU ΣY V  ΣY V −I ) ( q )} ΣV V DU ⊺  ̂ and similarly for the sample risk, with Σn (V ) replacing Σ(V ) ∶= Cov((Y, V (X)⊺ )) above. [sent-225, score-0.354]
</p><p>70 We can express the remaining “controllable” risk as ⊺  −2Iq 0 Rc (M ) = R(M ) − Ru = tr {( ) Σ(V ) ( q ⊺ )} . [sent-227, score-0.16]
</p><p>71 1), it holds that ̂ ̂ sup ∥Σ(V ) − Σn (V )∥sp ≤ C sup sup w⊺ (Σ(V ) − Σn (V )) w V  V  w∈N  where N is a 1/2-covering of the unit (q + r)-sphere, which has size ∣N ∣ ≤ 6q+r ≤ 36q ; see [8]. [sent-231, score-0.219]
</p><p>72 We now assume that the functions vsj (xj ) are uniformly bounded from a Sobolev space of order two. [sent-232, score-0.134]
</p><p>73 } denote a uniformly bounded, orthonormal basis with respect to L2 [0, 1], and assume that vsj ∈ Hj where ∞  ∞  k=0  k=0  Hj = {fj ∶ fj (xj ) = ∑ ajk ψjk (xj ), ∑ a2 k 4 ≤ K 2 } jk √ for some 0 < K < ∞. [sent-236, score-0.377]
</p><p>74 ̂ Subtracting Ru − Ru from each of the bracketed differences, we obtain that ̂ ̂ ̂ ̂ ̂ R(M ) − inf R(M ) ≤ [Rc (M ) − Rc (M )] − [Rc (M∗ ) − Rc (M∗ )] M ∈Mn  ̂ ≤ 2 sup {Rc (M ) − Rc (M )} M ∈Mn  by (6. [sent-247, score-0.105]
</p><p>75 Let M minimize the empirical risk n ∑i ∥Yi − ∑j Mj (Xij )∥2 over the class Mn . [sent-253, score-0.103]
</p><p>76 M ∈Mn  7  Application to Gene Expression Data  To illustrate the proposed nonparametric reduced rank regression techniques, we consider data on gene expression in E. [sent-255, score-0.624]
</p><p>77 In this challenge genes were classiﬁed as transcription factors (TFs) or target genes (TGs). [sent-257, score-0.158]
</p><p>78 Our motivation for analyzing these 33 genes is that, according to the gold standard gene regulatory network used for the DREAM 5 challenge, the 6 TFs form the parent set common to two additional TFs, which have the 27 TGs as their child nodes. [sent-260, score-0.214]
</p><p>79 This means that if we treat the gold standard as a causal network, then up to noise, the functional relationship between X and Y is given by the composition of a map g ∶ R6 → R2 and a map h ∶ R2 → R27 . [sent-262, score-0.166]
</p><p>80 If g and h are both linear, their composition h ○ g is a linear map of rank no more than 2. [sent-263, score-0.289]
</p><p>81 As observed in Section 2, such a reduced rank linear model is a special case of an additive model with reduced rank in the sense of penalty 2. [sent-264, score-0.969]
</p><p>82 More generally, if g is an additive function and h is linear, then h ○ g has rank at most 2 in the sense of penalty 2. [sent-265, score-0.595]
</p><p>83 Higher rank can in principle occur 1  http://wiki. [sent-266, score-0.246]
</p><p>84 under functional composition, since even a univariate additive map h ∶ R → Rq may have rank up to q under our penalties (recall that penalty 1 and 2 coincide for univariate maps). [sent-273, score-0.852]
</p><p>85 The backﬁtting algorithm of Figure 1 with penalty 1 and a rather aggressive choice of the tuning parameter λ produces the estimates shown in Figure 2, for which we have selected three of the 27 TGs. [sent-274, score-0.255]
</p><p>86 Under such strong regularization, the 5th column of functions is rank zero and, thus, identically zero. [sent-275, score-0.299]
</p><p>87 The remaining columns have rank one; the estimated ﬁtted values are scalar multiples of one another. [sent-276, score-0.246]
</p><p>88 This property characterizes the difference between penalties 1 and 2; in an application of penalty 2, the scalings would have been the same across all functions in a given row. [sent-280, score-0.459]
</p><p>89 Next, we illustrate a higher-rank solution for penalty 2. [sent-281, score-0.229]
</p><p>90 Choosing the regularization parameter λ by ten-fold cross-validation gives a ﬁt of rank 5, considerably lower than 27, the maximum possible rank. [sent-282, score-0.278]
</p><p>91 Under rank ﬁve, each row of functions is a linear combination of up to ﬁve other, linearly independent rows. [sent-284, score-0.325]
</p><p>92 Hence, if the causal relationships for these data were indeed additive and low rank, then the true low rank might well be smaller than ﬁve. [sent-286, score-0.49]
</p><p>93 8  Summary  This paper introduced two penalties that induce reduced rank ﬁts in multivariate additive nonparametric regression. [sent-287, score-0.708]
</p><p>94 Under linearity, the penalties specialize to group lasso and nuclear norm penalties for classical reduced rank regression. [sent-288, score-0.854]
</p><p>95 Examining the subdifferentials of each of these penalties, we developed backﬁtting algorithms for the two resulting optimization problems that are based on softthresholding of singular values of smoothed residual matrices. [sent-289, score-0.165]
</p><p>96 The algorithms were demonstrated on a gene expression data set constructed to have a naturally low-rank structure. [sent-290, score-0.124]
</p><p>97 We also provided a persistence analysis that shows error tending to zero under a scaling assumption on the sample size n and the dimensions q and p of the regression problem. [sent-291, score-0.158]
</p><p>98 Minimax-optimal rates for sparse additive models over kernel classes via convex programming. [sent-325, score-0.12]
</p><p>99 Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization. [sent-333, score-0.526]
</p><p>100 How close is the sample covariance matrix to the actual covariance matrix? [sent-336, score-0.151]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sp', 0.396), ('mj', 0.29), ('fj', 0.254), ('rank', 0.246), ('penalty', 0.229), ('rc', 0.187), ('pj', 0.169), ('xj', 0.167), ('mk', 0.161), ('nuclear', 0.147), ('diag', 0.145), ('penalties', 0.138), ('mq', 0.132), ('hj', 0.127), ('hh', 0.124), ('additive', 0.12), ('du', 0.12), ('subdifferential', 0.11), ('tfs', 0.108), ('tgs', 0.108), ('population', 0.107), ('risk', 0.103), ('zj', 0.103), ('regression', 0.096), ('cram', 0.096), ('nonparametric', 0.094), ('gene', 0.085), ('mn', 0.084), ('ru', 0.083), ('norm', 0.082), ('pam', 0.081), ('vsj', 0.081), ('singular', 0.079), ('tting', 0.077), ('back', 0.074), ('sup', 0.073), ('pq', 0.073), ('reduced', 0.064), ('genes', 0.057), ('tr', 0.057), ('cov', 0.056), ('dream', 0.054), ('subdifferentials', 0.054), ('functions', 0.053), ('rq', 0.053), ('matrix', 0.051), ('bj', 0.051), ('gold', 0.048), ('low', 0.048), ('maryam', 0.048), ('functional', 0.047), ('multivariate', 0.046), ('excess', 0.045), ('transcription', 0.044), ('composition', 0.043), ('jk', 0.042), ('response', 0.042), ('op', 0.041), ('parallels', 0.041), ('norms', 0.041), ('fazel', 0.039), ('scalings', 0.039), ('specialize', 0.039), ('expression', 0.039), ('yuan', 0.039), ('tted', 0.038), ('matrices', 0.038), ('sj', 0.036), ('ming', 0.036), ('univariate', 0.036), ('stationary', 0.035), ('dv', 0.035), ('covariance', 0.035), ('regularization', 0.032), ('residual', 0.032), ('dd', 0.032), ('functionals', 0.032), ('scaling', 0.032), ('inf', 0.032), ('covariate', 0.031), ('gj', 0.031), ('orthogonal', 0.031), ('sample', 0.03), ('penalized', 0.03), ('soft', 0.03), ('smoother', 0.029), ('returning', 0.029), ('causal', 0.028), ('decomposition', 0.027), ('justi', 0.027), ('xij', 0.026), ('tuning', 0.026), ('row', 0.026), ('constrained', 0.025), ('yi', 0.025), ('network', 0.024), ('pdimensional', 0.024), ('marbach', 0.024), ('dissertation', 0.024), ('lgorithm', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="247-tfidf-1" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>Author: Rina Foygel, Michael Horrell, Mathias Drton, John D. Lafferty</p><p>Abstract: We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models. An additive model is estimated for each dimension of a q-dimensional response, with a shared p-dimensional predictor variable. To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank. Backﬁtting algorithms are derived and justiﬁed using a nonparametric form of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting. The methods are illustrated on gene expression data. 1</p><p>2 0.15250382 <a title="247-tfidf-2" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>Author: Andreas Argyriou, Rina Foygel, Nathan Srebro</p><p>Abstract: We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an 2 penalty. We show that this new k-support norm provides a tighter relaxation than the elastic net and can thus be advantageous in in sparse prediction problems. We also bound the looseness of the elastic net, thus shedding new light on it and providing justiﬁcation for its use. 1</p><p>3 0.14462809 <a title="247-tfidf-3" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>Author: Benjamin Rolfs, Bala Rajaratnam, Dominique Guillot, Ian Wong, Arian Maleki</p><p>Abstract: The 1 -regularized maximum likelihood estimation problem has recently become a topic of great interest within the machine learning, statistics, and optimization communities as a method for producing sparse inverse covariance estimators. In this paper, a proximal gradient method (G-ISTA) for performing 1 -regularized covariance matrix estimation is presented. Although numerous algorithms have been proposed for solving this problem, this simple proximal gradient method is found to have attractive theoretical and numerical properties. G-ISTA has a linear rate of convergence, resulting in an O(log ε) iteration complexity to reach a tolerance of ε. This paper gives eigenvalue bounds for the G-ISTA iterates, providing a closed-form linear convergence rate. The rate is shown to be closely related to the condition number of the optimal point. Numerical convergence results and timing comparisons for the proposed method are presented. G-ISTA is shown to perform very well, especially when the optimal point is well-conditioned. 1</p><p>4 0.1408436 <a title="247-tfidf-4" href="./nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>Author: Tingni Sun, Cun-hui Zhang</p><p>Abstract: This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a uniﬁed analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. 1</p><p>5 0.14016154 <a title="247-tfidf-5" href="./nips-2012-Matrix_reconstruction_with_the_local_max_norm.html">208 nips-2012-Matrix reconstruction with the local max norm</a></p>
<p>Author: Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We introduce a new family of matrix norms, the “local max” norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netﬂix and MovieLens ratings data, and ﬁnd improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms. 1</p><p>6 0.1373938 <a title="247-tfidf-6" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>7 0.10709526 <a title="247-tfidf-7" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>8 0.10465504 <a title="247-tfidf-8" href="./nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>9 0.10148942 <a title="247-tfidf-9" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>10 0.096216843 <a title="247-tfidf-10" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>11 0.095096998 <a title="247-tfidf-11" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>12 0.087661222 <a title="247-tfidf-12" href="./nips-2012-Convex_Multi-view_Subspace_Learning.html">86 nips-2012-Convex Multi-view Subspace Learning</a></p>
<p>13 0.083584219 <a title="247-tfidf-13" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>14 0.080573373 <a title="247-tfidf-14" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>15 0.077346116 <a title="247-tfidf-15" href="./nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">327 nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<p>16 0.077287711 <a title="247-tfidf-16" href="./nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">206 nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>17 0.077146336 <a title="247-tfidf-17" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>18 0.07291729 <a title="247-tfidf-18" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>19 0.072534218 <a title="247-tfidf-19" href="./nips-2012-Active_Comparison_of_Prediction_Models.html">32 nips-2012-Active Comparison of Prediction Models</a></p>
<p>20 0.071517758 <a title="247-tfidf-20" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.199), (1, 0.056), (2, 0.109), (3, -0.088), (4, 0.027), (5, 0.092), (6, -0.005), (7, 0.022), (8, -0.022), (9, -0.033), (10, -0.004), (11, 0.008), (12, -0.057), (13, -0.026), (14, 0.084), (15, 0.002), (16, 0.13), (17, 0.031), (18, 0.033), (19, -0.096), (20, 0.005), (21, 0.026), (22, -0.04), (23, 0.022), (24, -0.086), (25, -0.038), (26, 0.064), (27, -0.062), (28, -0.001), (29, 0.018), (30, -0.102), (31, 0.032), (32, -0.047), (33, 0.105), (34, 0.012), (35, 0.225), (36, -0.02), (37, -0.157), (38, -0.042), (39, 0.053), (40, -0.12), (41, 0.062), (42, -0.066), (43, 0.085), (44, 0.05), (45, 0.01), (46, 0.055), (47, -0.048), (48, 0.059), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9627589 <a title="247-lsi-1" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>Author: Rina Foygel, Michael Horrell, Mathias Drton, John D. Lafferty</p><p>Abstract: We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models. An additive model is estimated for each dimension of a q-dimensional response, with a shared p-dimensional predictor variable. To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank. Backﬁtting algorithms are derived and justiﬁed using a nonparametric form of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting. The methods are illustrated on gene expression data. 1</p><p>2 0.7795673 <a title="247-lsi-2" href="./nips-2012-Matrix_reconstruction_with_the_local_max_norm.html">208 nips-2012-Matrix reconstruction with the local max norm</a></p>
<p>Author: Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov</p><p>Abstract: We introduce a new family of matrix norms, the “local max” norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems. We show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm. We test this interpolation on simulated data and on the large-scale Netﬂix and MovieLens ratings data, and ﬁnd improved accuracy relative to the existing matrix norms. We also provide theoretical results showing learning guarantees for some of the new norms. 1</p><p>3 0.75241297 <a title="247-lsi-3" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>Author: Andreas Argyriou, Rina Foygel, Nathan Srebro</p><p>Abstract: We derive a novel norm that corresponds to the tightest convex relaxation of sparsity combined with an 2 penalty. We show that this new k-support norm provides a tighter relaxation than the elastic net and can thus be advantageous in in sparse prediction problems. We also bound the looseness of the elastic net, thus shedding new light on it and providing justiﬁcation for its use. 1</p><p>4 0.62402689 <a title="247-lsi-4" href="./nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>Author: Tingni Sun, Cun-hui Zhang</p><p>Abstract: This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a uniﬁed analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. 1</p><p>5 0.62023181 <a title="247-lsi-5" href="./nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>Author: Zhihua Zhang, Bojun Tu</p><p>Abstract: In this paper we study sparsity-inducing nonconvex penalty functions using L´ vy e processes. We deﬁne such a penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsityinducing nonconvex penalties. Particularly, we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionally, we explore the concave conjugate of nonconvex penalties. We ﬁnd that the LOG and EXP penalties are the concave conjugates of negative Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance. 1</p><p>6 0.61340612 <a title="247-lsi-6" href="./nips-2012-Convex_Multi-view_Subspace_Learning.html">86 nips-2012-Convex Multi-view Subspace Learning</a></p>
<p>7 0.6070767 <a title="247-lsi-7" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>8 0.5769949 <a title="247-lsi-8" href="./nips-2012-Spectral_Learning_of_General_Weighted_Automata_via_Constrained_Matrix_Completion.html">320 nips-2012-Spectral Learning of General Weighted Automata via Constrained Matrix Completion</a></p>
<p>9 0.5716275 <a title="247-lsi-9" href="./nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">221 nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>10 0.55699992 <a title="247-lsi-10" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>11 0.53681171 <a title="247-lsi-11" href="./nips-2012-Minimax_Multi-Task_Learning_and_a_Generalized_Loss-Compositional_Paradigm_for_MTL.html">212 nips-2012-Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL</a></p>
<p>12 0.51602906 <a title="247-lsi-12" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>13 0.51388115 <a title="247-lsi-13" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>14 0.49960762 <a title="247-lsi-14" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>15 0.49959734 <a title="247-lsi-15" href="./nips-2012-A_Scalable_CUR_Matrix_Decomposition_Algorithm%3A_Lower_Time_Complexity_and_Tighter_Bound.html">17 nips-2012-A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound</a></p>
<p>16 0.49450934 <a title="247-lsi-16" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>17 0.49414074 <a title="247-lsi-17" href="./nips-2012-On_the_Sample_Complexity_of_Robust_PCA.html">254 nips-2012-On the Sample Complexity of Robust PCA</a></p>
<p>18 0.48029479 <a title="247-lsi-18" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>19 0.46197382 <a title="247-lsi-19" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>20 0.45957074 <a title="247-lsi-20" href="./nips-2012-Factoring_nonnegative_matrices_with_linear_programs.html">125 nips-2012-Factoring nonnegative matrices with linear programs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.031), (21, 0.011), (36, 0.01), (38, 0.122), (39, 0.015), (42, 0.011), (54, 0.023), (55, 0.041), (56, 0.036), (68, 0.013), (74, 0.03), (76, 0.47), (80, 0.072), (92, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99371296 <a title="247-lda-1" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>Author: Francisco Pereira, Matthew Botvinick</p><p>Abstract: This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure. 1</p><p>2 0.99231821 <a title="247-lda-2" href="./nips-2012-Learning_High-Density_Regions_for_a_Generalized_Kolmogorov-Smirnov_Test_in_High-Dimensional_Data.html">175 nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</a></p>
<p>Author: Assaf Glazer, Michael Lindenbaum, Shaul Markovitch</p><p>Abstract: We propose an efﬁcient, generalized, nonparametric, statistical KolmogorovSmirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efﬁcient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods. 1</p><p>3 0.9912622 <a title="247-lda-3" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>Author: Kevin Tang, Vignesh Ramanathan, Li Fei-fei, Daphne Koller</p><p>Abstract: Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest ﬁrst. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features speciﬁc to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video [2] datasets that illustrate the beneﬁt of our approach to adapt object detectors to video. 1</p><p>4 0.99049801 <a title="247-lda-4" href="./nips-2012-Random_Utility_Theory_for_Social_Choice.html">286 nips-2012-Random Utility Theory for Social Choice</a></p>
<p>Author: Hossein Azari, David Parks, Lirong Xia</p><p>Abstract: Random utility theory models an agent’s preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received signiﬁcant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce. 1</p><p>5 0.98996991 <a title="247-lda-5" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>Author: Weiwei Cheng, Willem Waegeman, Volkmar Welker, Eyke Hüllermeier</p><p>Abstract: Several machine learning methods allow for abstaining from uncertain predictions. While being common for settings like conventional classiﬁcation, abstention has been studied much less in learning to rank. We address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders. In our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings. We formally analyze this approach for the Mallows and the Plackett-Luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders. These theoretical results are complemented by experiments demonstrating the practical usefulness of the approach. 1</p><p>6 0.98944736 <a title="247-lda-6" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>7 0.98806643 <a title="247-lda-7" href="./nips-2012-Active_Learning_of_Model_Evidence_Using_Bayesian_Quadrature.html">33 nips-2012-Active Learning of Model Evidence Using Bayesian Quadrature</a></p>
<p>same-paper 8 0.97591299 <a title="247-lda-8" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>9 0.97291827 <a title="247-lda-9" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>10 0.96829885 <a title="247-lda-10" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>11 0.93444467 <a title="247-lda-11" href="./nips-2012-The_Perturbed_Variation.html">338 nips-2012-The Perturbed Variation</a></p>
<p>12 0.927109 <a title="247-lda-12" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>13 0.925147 <a title="247-lda-13" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>14 0.91552079 <a title="247-lda-14" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>15 0.91449291 <a title="247-lda-15" href="./nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">291 nips-2012-Reducing statistical time-series problems to binary classification</a></p>
<p>16 0.91299337 <a title="247-lda-16" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>17 0.9128961 <a title="247-lda-17" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>18 0.91018337 <a title="247-lda-18" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>19 0.90941107 <a title="247-lda-19" href="./nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">327 nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<p>20 0.90682328 <a title="247-lda-20" href="./nips-2012-Multi-Task_Averaging.html">222 nips-2012-Multi-Task Averaging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
