<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-284" href="#">nips2012-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</h1>
<br/><p>Source: <a title="nips-2012-284-pdf" href="http://papers.nips.cc/paper/4710-q-mkl-matrix-induced-regularization-in-multi-kernel-learning-with-applications-to-neuroimaging.pdf">pdf</a></p><p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>Reference: <a title="nips-2012-284-reference" href="../nips2012_reference/nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. [sent-7, score-0.174]
</p><p>2 Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. [sent-8, score-0.382]
</p><p>3 We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. [sent-11, score-0.167]
</p><p>4 This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. [sent-12, score-0.045]
</p><p>5 We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. [sent-13, score-0.057]
</p><p>6 The design of a kernel (feature pre-processing) may involve using different sets of extracted features, dimensionality reductions, or parameterizations of the kernel functions. [sent-19, score-0.33]
</p><p>7 Each of these alternatives produces a distinct kernel matrix. [sent-20, score-0.165]
</p><p>8 Rather than selecting a single kernel, MKL offers the ﬂexibility of specifying a large set of kernels corresponding to the many options (i. [sent-25, score-0.321]
</p><p>9 In the context of our primary motivating application, the current state of the art in multi-modality neuroimaging-based Alzheimer’s Disease (AD) prediction [10] is achieved by multi-kernel methods [3, 4], where each imaging modality spawns a kernel, or set of kernels. [sent-31, score-0.118]
</p><p>10 In allowing the user to specify an arbitrary number of base kernels for combination MKL provides more expressive power, but this comes with the responsibility to regularize the kernel mixing coefﬁcients so that the classiﬁer generalizes well. [sent-32, score-0.689]
</p><p>11 While the importance of this regularization cannot be overstated, it is also a fact that commonly used p norm regularizers operate on kernels separately, without explicitly acknowledging dependencies and interactions among them. [sent-33, score-0.447]
</p><p>12 To see how such dependencies can arise in practice, consider our neuroimaging learning problem of interest: the task of learning to predict the onset of AD. [sent-34, score-0.148]
</p><p>13 , KM are derived from several different medical imaging modalities (MRI; PET), image processing methods (morphometric; anatomical modelling), and kernel functions (linear; RBF). [sent-38, score-0.275]
</p><p>14 Some features may be shared between kernels, or kernel functions may use similar parameters. [sent-39, score-0.165]
</p><p>15 As a result we expect the kernels’ behaviors to exhibit some correlational, or other cluster structure according to how they were constructed. [sent-40, score-0.059]
</p><p>16 Ideally, the regularizer should reﬂect these dependencies encoded by Q, as they can signiﬁcantly impact the learning characteristics of a linearly combined kernel. [sent-44, score-0.094]
</p><p>17 Instead, rather than penalizing covariances or inducing sparsity among groups of kernels, it may be beneﬁcial to reward such covariances, so as to better reﬂect a latent cluster structure between kernels. [sent-48, score-0.053]
</p><p>18 The development of MKL methods began with [5], which showed that the problem of learning the right kernel for an input problem instance could be formulated as a Semi-Deﬁnite Program (SDP). [sent-53, score-0.165]
</p><p>19 To this end, the model in [5] was shown to be solvable as a Second Order Cone Program [12], a Semi-Inﬁnite Linear Program [6], and via gradient descent methods in the dual and primal [7, 13]. [sent-55, score-0.062]
</p><p>20 A non-linear “hyperkernel” method was proposed [15] which implicitly maps the kernels themselves to an implicit RKHS, however this method is computationally very demanding, (it has 4th order interactions among training examples). [sent-58, score-0.345]
</p><p>21 The authors of [16] proposed to ﬁrst select the sub-kernel weights by minimizing an objective function derived from Normalized Cuts, and subsequently train an SVM on the combined kernel. [sent-59, score-0.052]
</p><p>22 Contemporary to these results, [18] showed that if a large number of kernels had a desirable shared structure (e. [sent-61, score-0.321]
</p><p>23 Recently in [8], a set of base classiﬁers were ﬁrst trained using each kernel and were then boosted to produce a strong multi-class classiﬁer. [sent-64, score-0.263]
</p><p>24 Adding kernels corresponds to taking a direct sum of Reproducing Kernel Hilbert √ spaces (RKHS), and scaling a kernel by a constant c scales the axes of it’s RKHS by c. [sent-72, score-0.486]
</p><p>25 In the wm  M  2  Hm 1 MKL setting, the SVM margin regularizer 2 w 2 becomes a weighted sum 1 m=1 over 2 βm contributions from RKHS’s H1 , . [sent-73, score-0.274]
</p><p>26 A norm penalty on β ensures that the units in which the margin is measured are meaningful (provided the base kernels are normalized). [sent-77, score-0.561]
</p><p>27 The MKL primal problem is given as  min  w,b,β≥0,ξ≥0  1 2  M  m  wm 2 m H +C βm  n  M  ξi + β  2 p  s. [sent-78, score-0.213]
</p><p>28 yi  wm , φm (xi )  Hm  +b  ≥ 1 − ξi , (1)  m  i  where φm (x) is the (potentially unknown) transformation from the original data space to the mth RKHS Hm . [sent-80, score-0.216]
</p><p>29 As in SVMs, we turn to the dual problem to see the role of kernels: max  0≤α≤C  αT 1 −  1 G q, 2  G ∈ RM ; Gm = (α ◦ y)T Km (α ◦ y),  (2)  1 where ◦ denotes element-wise multiplication, and the dual q-norm follows the identity p + 1 = 1. [sent-81, score-0.07]
</p><p>30 q 2 Note that the primal norm penalty β p becomes a dual-norm on the vector G. [sent-82, score-0.121]
</p><p>31 At optimality, wm  2  Hm wm = βm (α ◦ y)T φm (X), and so the term Gm = (α ◦ y)T Km (α ◦ y) = is the vector 2 βm of scaled classiﬁer norms. [sent-83, score-0.372]
</p><p>32 This shows that the dual norm is tied to how MKL measures the margin in each RKHS. [sent-84, score-0.132]
</p><p>33 The key characteristic of Q-MKL is that the standard (squared) p -norm penalty on β, along with the corresponding dual-norm penalty in (2), is substituted with a more general class of quadratic penalty functions, expressed as β T Qβ = β 2 . [sent-86, score-0.179]
</p><p>34 β Q = β T Qβ is a Q Mahalanobis (matrix-induced) norm so long as Q 0. [sent-87, score-0.049]
</p><p>35 In this framework, the burden of choosing a kernel is deferred to a choice of Q-function. [sent-88, score-0.19]
</p><p>36 yi  wm , φm (xi )  Hm  +b  ≥ 1 − ξi , (3)  m  i  where the last objective term provides a bias relative to β T Qβ. [sent-92, score-0.186]
</p><p>37 [19] derived a theoretical generalization error bound on kernel combinations which depends on the degree of redundancy between support vectors in SVMs trained on base kernels individually. [sent-100, score-0.584]
</p><p>38 Using this type of correlational structure, we can derive a Q function between kernels to automatically select a combination of kernels which will maximize this bound. [sent-101, score-0.711]
</p><p>39 We ﬁrst show that as Q−1 ’s eigen-values decay, so do the traces of the virtual kernels. [sent-106, score-0.335]
</p><p>40 Assuming Q−1 has a bounded, non-uniform spectrum, this property can then be used to analyze, (and bound), Q-MKL’s Rademacher complexity, which has been shown to depend on the traces of the base kernels. [sent-107, score-0.204]
</p><p>41 We then offer a few observations on how Q−1 ’s Renyi entropy [20] relates to these learning theoretic bounds. [sent-108, score-0.125]
</p><p>42 First, observe that because Q’s eigen-vectors provide an orthonormal basis of RM , β ∈ RM can be expressed as a linear combination in this basis with γ as its coefﬁcients: β = i γi vi = V γ. [sent-111, score-0.091]
</p><p>43 Each eigen-vector vi of Q can be used to deﬁne a linear combination of kernels, which we will refer to as virtual kernel Ki = m vi (m)Km . [sent-113, score-0.545]
</p><p>44 If Ki 0, ∀i, then Q-MKL is equivalent to 2-norm MKL using virtual kernels instead of base kernels. [sent-117, score-0.648]
</p><p>45 4) and K ∗ = 2 m βm Km 1 M M M M M −2 = = = = m i γi vi (m)Km i µi λ m vi (m)Km i µi Ki , where Ki 1 M λ− 2 m vi (m)Km is the ith virtual kernel. [sent-121, score-0.409]
</p><p>46 The learned kernel K ∗ is a weighted combination of virtual kernels, and the coefﬁcients are regularized under a squared 2-norm. [sent-122, score-0.425]
</p><p>47 We ﬁrst state a theorem from [21], which relates the Rademacher complexity of MKL to the traces of its base kernels. [sent-125, score-0.286]
</p><p>48 ([21]) The empirical Rademacher complexity on a sample set S of size n, with M base 23 kernels is given as follows (with η0 = 22 ), RS (HM p ) ≤ T  where u = [Tr(K1 ), · · · , Tr(KM )] and  1 p  +  1 q  η0 q u n  q  (5)  = 1. [sent-127, score-0.46]
</p><p>49 The bound in (5) shows that the Rademacher complexity RS (·) depends on u q , a norm on the base kernels’ traces. [sent-128, score-0.188]
</p><p>50 However, in Q-MKL the virtual kernels traces are not equal, T √v and are in fact given by Tr(Ki ) = 1 λ i . [sent-130, score-0.656]
</p><p>51 With this expression for the traces of the virtual kernels, i we can now prove that the bound given in (5) is strictly decreased as long as the eigen-values ψi of Q−1 are in the range (0, 1]. [sent-131, score-0.335]
</p><p>52 By Lemma 1, we have that the bound in (5) will decrease if u 2 , the norm on the virtual √ kernel traces, decreases. [sent-136, score-0.443]
</p><p>53 As shown above, the virtual kernel traces are given as Tr(Ki ) = ψi 1T vi , N N T meaning that u 2 = i ψi (1T vi )2 = i ψi 1T vi vi 1 = 1T Q−1 1. [sent-137, score-0.74]
</p><p>54 Q-MKL is equivalent to the following model: min  w,b,µ,ξ≥0  1 2  M  wm 2 m V +C µm  m  n  ξi + µ  2 2  M  s. [sent-147, score-0.186]
</p><p>55 yi  wm , φm (xi )  (6)  i  Vm  +b  ≥ 1 − ξi ,  1  Q− 2 µ ≥ 0,  m  where φm () is the feature transform mapping data space to the mth virtual kernel, denoted as Vm . [sent-149, score-0.445]
</p><p>56 4  1  While the virtual kernels themselves may be indeﬁnite, recall that µ = Q 2 β, and so the constraint 1 Q− 2 µ ≥ 0 is equivalent to β ≥ 0, guaranteeing that the combined kernel will be p. [sent-150, score-0.74]
</p><p>57 Renyi entropy [20] signiﬁcantly generalizes the usual notion of Shannon entropy [22, 23, 24], has applications in Statistics and many other ﬁelds, and has recently been proposed as an alternative to PCA [22]. [sent-155, score-0.213]
</p><p>58 2 points to an intuitive explanation of where the beneﬁt from a Q regularizer comes from as well, if we analyze the Renyi entropy of the distribution on kernels deﬁned by Q−1 , if we treat Q−1 as a kernel density estimator. [sent-157, score-0.61]
</p><p>59 The quadratic Renyi entropy of a probability measure is given as, H(p) = − log  p2 (x)dx. [sent-158, score-0.128]
</p><p>60 [15],) then with some normalization we can derive an estimate of the underlying probability p, which is a distribution over base kernels. [sent-164, score-0.098]
</p><p>61 We can then interpret its Renyi ˆ entropy as a complexity measure on the space of combined kernels. [sent-165, score-0.15]
</p><p>62 2) in [23] relates the 1 virtual kernel traces to the Renyi entropy estimator of Q−1 as p2 (x)dx = N 2 1T Q−1 1,1 which ˆ leads to a nice connection to Thm. [sent-168, score-0.625]
</p><p>63 , 2-norm MKL), has maximal Renyi entropy because it is maximally uninformative; adding structure to Q−1 concentrates p, reducing both its Renyi entropy, and Rademacher complexity together. [sent-172, score-0.125]
</p><p>64 2 relies on decreasing a norm on the virtual kernel traces, which we now see directly relates to the Renyi entropy of Q−1 , as well as with decreasing the Rademacher complexity of the search space of combined kernels. [sent-175, score-0.634]
</p><p>65 It is even possible that by directly analyzing Renyi entropy in a multi-kernel setting, this conjecture may be useful in deriving analogous bounds in, e. [sent-176, score-0.084]
</p><p>66 , Indeﬁnite Kernel Learning [25], because the virtual kernels are indeﬁnite in general. [sent-178, score-0.55]
</p><p>67 2 Special Cases: Q-SVM and relative margin Before describing our optimization strategy, we discuss several variations on the Q-MKL model. [sent-180, score-0.048]
</p><p>68 , singleton features,) then each coefﬁcient βm effectively becomes a feature weight, and a 2-norm penalty on β is a penalty on weights. [sent-185, score-0.09]
</p><p>69 Several interesting extensions to the SVM and MKL frameworks have been proposed which focus on the relative margin methods [28, 29] which maximize the margin relative to the spread of the data. [sent-190, score-0.12]
</p><p>70 Q-MKL generalizes β 2 to arbitrary convex quadratic functions, while the feasible set p is the same as for MKL. [sent-196, score-0.116]
</p><p>71 We may consider this process as a composition of two modules: one which solves for SVM dual parameters (α) with ﬁxed β, and the other for solving for β with ﬁxed α: 1  Note that this involves a Gaussian assumption, but [24] provides extensions to non-Gauss kernels. [sent-199, score-0.059]
</p><p>72 Notice that (8) has a sum of ratios with optimization variables in the denominator, while the constraint is quadratic – this means that standard convex optimization toolkits may not be able to solve this problem without signiﬁcant reformulation from its canonical form in (8). [sent-205, score-0.071]
</p><p>73 Writing the gradient in terms of the Lagrange multiplier δ, and setting it equal to 0 gives: wm 2 m H − δ(Qβ)m = 0, ∀m ∈ {1, · · · , M }. [sent-207, score-0.186]
</p><p>74 Left (a-b): weights given by a standard SVM; Right (c-d): weights given by Q-SVM . [sent-232, score-0.054]
</p><p>75 5  Experiments  We performed extensive experiments to validate Q-MKL, examine the effect it has on β, and to assess its advantages in the context of our motivating neuroimaging application. [sent-236, score-0.143]
</p><p>76 Our focus on a practical application is intended as a demonstration of how domain knowledge can be seamlessly incorporated into a learning model, giving signiﬁcant gains in accuracy. [sent-238, score-0.051]
</p><p>77 In out main experiments we used brain scans of AD patients and Cognitively Normal healthy controls (CN) from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [30] in a set of cross-validation experiments. [sent-242, score-0.088]
</p><p>78 ADNI is a landmark study sponsored by the NIH, major pharmaceuticals and others to determine the extent to which multi-modal brain imaging can help predict on-set, and monitor progression of, AD. [sent-243, score-0.116]
</p><p>79 For our experiments, 48 AD subjects and 66 controls were chosen who had both T1 -weighted MR scans and Fluoro-Deoxy-Glucose PET (FDG-PET) scans at two time-points two years apart. [sent-245, score-0.118]
</p><p>80 uk/spm/) were used to register scans to a common template and calculate Gray Matter (GM) densities at each voxel in the MR scans. [sent-251, score-0.059]
</p><p>81 Feature selection was performed separately in each set of images by sorting voxels by t-statistic (calculated using training data), and choosing the highest 2000, 5000, 10000,. [sent-253, score-0.046]
</p><p>82 We used linear, quadratic, and Gaussian kernels: a total of 24 kernels per set, (GM maps, TBM maps, baseline FDG-PET, FDG-PET at 2-year follow up) for a total of 96 kernels. [sent-257, score-0.321]
</p><p>83 For Q-matrix we used the Laplacian of covariance between single-kernel α parameters, (recall the motivation from [19] in Section 3,) plus a block-diagonal representing clusters of kernels derived from the same imaging modalities. [sent-258, score-0.378]
</p><p>84 To demonstrate that Q-regularizers indeed inﬂuence the learned classiﬁer, we performed classiﬁcation experiments with the Laplacian of the inverse distance between voxels as a Q matrix, and voxel-wise GM density (VBM) as features. [sent-262, score-0.046]
</p><p>85 2  Multi-modality Alzheimer’s disease (AD) prediction  Next, we performed multi-modality AD prediction experiments using all 96 kernels across two modalRegularizer Acc. [sent-272, score-0.405]
</p><p>86 have emerged as the state of the art in this domain [3, 4], and have performed favorably in extensive evaluations against various baselines such as single-kernel methods, and na¨ve combinations, ı we therefore focus our analysis on comparison with existing MKL methods. [sent-308, score-0.064]
</p><p>87 The primary beneﬁt of current sparse MKL methods is that they effectively ﬁlter out uninformative or noisy kernels, however, the kernels used in these experiments are all derived from clinically relevant neuroimaging data, and are thus highly reliable. [sent-316, score-0.464]
</p><p>88 We next turn to an analysis of the covariance structures found in the data empirically as a concrete demonstration of the type of patterns towards which the Q-MKL regularizer biases β. [sent-319, score-0.064]
</p><p>89 The FDG-PET kernels (last 48 kernels) are much more strongly interrelated. [sent-324, score-0.321]
</p><p>90 Interestingly, the ﬁrst eigenvector is almost entirely devoted to two large blocks of kernels – those which come from MRI data, and those which come from FDG-PET data. [sent-325, score-0.373]
</p><p>91 Q-MKL extends this framework to exploit higher order interactions between kernels using supervised, unsupervised, or domain-knowledge driven measures. [sent-331, score-0.345]
</p><p>92 This ﬂexibility can impart greater control over how the model utilizes cluster structure among kernels, and effectively encourage cancellation of errors wherever possible. [sent-332, score-0.053]
</p><p>93 We have presented a convex optimization model to efﬁciently solve the resultant model, and shown experiments on a challenging problem of identifying AD based on multi modal brain imaging data (obtaining statistically signiﬁcant improvements). [sent-333, score-0.139]
</p><p>94 Note the block structure in (a–d) relating to the imaging modalities and kernel functions. [sent-339, score-0.247]
</p><p>95 Let the kernel ﬁgure it out; principled learning of pre-processing for kernel classiﬁers. [sent-349, score-0.33]
</p><p>96 Alzheimer’s disease diagnosis in individual subjects using structural MR images: validation studies. [sent-416, score-0.084]
</p><p>97 Multiple kernel learning, conic duality, and the SMO algorithm. [sent-430, score-0.165]
</p><p>98 Exploring large feature spaces with hierarchical multiple kernel learning. [sent-468, score-0.165]
</p><p>99 Orthogonal series density estimation and the kernel eigenvalue problem. [sent-493, score-0.165]
</p><p>100 Spatial and anatomical regularization of SVM for brain image analysis. [sent-519, score-0.057]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mkl', 0.573), ('kernels', 0.321), ('virtual', 0.229), ('renyi', 0.225), ('wm', 0.186), ('km', 0.165), ('kernel', 0.165), ('rademacher', 0.158), ('alzheimer', 0.124), ('neuroimaging', 0.119), ('ad', 0.111), ('tbm', 0.106), ('traces', 0.106), ('module', 0.104), ('base', 0.098), ('hm', 0.094), ('svm', 0.091), ('vbm', 0.087), ('rkhs', 0.084), ('entropy', 0.084), ('disease', 0.084), ('gm', 0.08), ('ki', 0.074), ('inde', 0.065), ('adni', 0.065), ('vi', 0.06), ('scans', 0.059), ('imaging', 0.057), ('im', 0.056), ('mr', 0.053), ('hinrichs', 0.052), ('sdp', 0.051), ('norm', 0.049), ('margin', 0.048), ('voxels', 0.046), ('laplacian', 0.046), ('penalty', 0.045), ('generalizes', 0.045), ('gehler', 0.044), ('quadratic', 0.044), ('jmlr', 0.044), ('rmm', 0.043), ('shogun', 0.043), ('szafranski', 0.043), ('complexity', 0.041), ('relates', 0.041), ('regularizer', 0.04), ('tr', 0.038), ('neuroimage', 0.038), ('qz', 0.038), ('pet', 0.038), ('correlational', 0.038), ('sonnenburg', 0.038), ('uw', 0.038), ('art', 0.037), ('dual', 0.035), ('behaviors', 0.035), ('supplements', 0.035), ('morphometry', 0.035), ('singh', 0.034), ('rm', 0.032), ('optimized', 0.032), ('svms', 0.032), ('vm', 0.031), ('combination', 0.031), ('mth', 0.03), ('progression', 0.03), ('brain', 0.029), ('encourage', 0.029), ('covariances', 0.029), ('mixing', 0.029), ('dependencies', 0.029), ('cristianini', 0.029), ('brie', 0.028), ('program', 0.028), ('anatomical', 0.028), ('mri', 0.028), ('madison', 0.028), ('weights', 0.027), ('domain', 0.027), ('primal', 0.027), ('convex', 0.027), ('come', 0.026), ('psd', 0.026), ('multi', 0.026), ('lanckriet', 0.026), ('peng', 0.025), ('modalities', 0.025), ('burden', 0.025), ('combined', 0.025), ('wisconsin', 0.025), ('coef', 0.024), ('toolbox', 0.024), ('uninformative', 0.024), ('extensions', 0.024), ('interactions', 0.024), ('cluster', 0.024), ('regularizers', 0.024), ('demonstration', 0.024), ('motivating', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="284-tfidf-1" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>2 0.21166502 <a title="284-tfidf-2" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>3 0.19076546 <a title="284-tfidf-3" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>Author: Sung J. Hwang, Kristen Grauman, Fei Sha</p><p>Abstract: When learning features for complex visual recognition problems, labeled image exemplars alone can be insufﬁcient. While an object taxonomy specifying the categories’ semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classiﬁcation task, nor does a single taxonomy capture all ties that are relevant. In light of these issues, we propose a discriminative feature learning approach that leverages multiple hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reﬂect their phylogenic ties, while another could reﬂect their habitats). For each taxonomy, we ﬁrst learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes. Then, using the resulting semantic kernel forest, we learn class-speciﬁc kernel combinations to select only those relationships relevant to recognize each object class. To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies’ structure. We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields signiﬁcant accuracy improvements.</p><p>4 0.1717836 <a title="284-tfidf-4" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>Author: Hachem Kadri, Alain Rakotomamonjy, Philippe Preux, Francis R. Bach</p><p>Abstract: Positive deﬁnite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a ﬁnite linear combination of inﬁnite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an r -norm constraint on the combination coefﬁcients (r ≥ 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of ﬁnger movement prediction in brain-computer interfaces. 1</p><p>5 0.14541832 <a title="284-tfidf-5" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>Author: Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, Bharath K. Sriperumbudur</p><p>Abstract: Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.</p><p>6 0.13783975 <a title="284-tfidf-6" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>7 0.11314036 <a title="284-tfidf-7" href="./nips-2012-The_Lov%C3%A1sz_%CF%91_function%2C_SVMs_and_finding_large_dense_subgraphs.html">337 nips-2012-The Lovász ϑ function, SVMs and finding large dense subgraphs</a></p>
<p>8 0.10872429 <a title="284-tfidf-8" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>9 0.092912547 <a title="284-tfidf-9" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>10 0.092724457 <a title="284-tfidf-10" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>11 0.085303135 <a title="284-tfidf-11" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<p>12 0.083590977 <a title="284-tfidf-12" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>13 0.08271753 <a title="284-tfidf-13" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>14 0.082389683 <a title="284-tfidf-14" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>15 0.081138767 <a title="284-tfidf-15" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>16 0.07614512 <a title="284-tfidf-16" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>17 0.073390096 <a title="284-tfidf-17" href="./nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">230 nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<p>18 0.073161863 <a title="284-tfidf-18" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>19 0.071947865 <a title="284-tfidf-19" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>20 0.070970342 <a title="284-tfidf-20" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, 0.047), (2, -0.028), (3, -0.059), (4, 0.108), (5, 0.009), (6, -0.012), (7, 0.108), (8, -0.062), (9, -0.095), (10, 0.005), (11, 0.006), (12, 0.119), (13, -0.01), (14, 0.098), (15, -0.149), (16, -0.013), (17, 0.069), (18, 0.022), (19, -0.141), (20, 0.065), (21, -0.053), (22, 0.028), (23, -0.276), (24, -0.003), (25, 0.059), (26, -0.019), (27, -0.051), (28, -0.016), (29, 0.057), (30, -0.003), (31, -0.115), (32, -0.043), (33, -0.003), (34, 0.1), (35, -0.058), (36, 0.03), (37, -0.081), (38, -0.078), (39, -0.017), (40, 0.054), (41, 0.072), (42, 0.087), (43, 0.041), (44, 0.054), (45, -0.101), (46, 0.074), (47, -0.022), (48, 0.006), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95134282 <a title="284-lsi-1" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>2 0.81363082 <a title="284-lsi-2" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>3 0.79424065 <a title="284-lsi-3" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>Author: Hachem Kadri, Alain Rakotomamonjy, Philippe Preux, Francis R. Bach</p><p>Abstract: Positive deﬁnite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a ﬁnite linear combination of inﬁnite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an r -norm constraint on the combination coefﬁcients (r ≥ 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of ﬁnger movement prediction in brain-computer interfaces. 1</p><p>4 0.76482755 <a title="284-lsi-4" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>Author: Alexander Lorbert, Peter J. Ramadge</p><p>Abstract: We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We report experiments using real-world, multi-subject fMRI data. 1</p><p>5 0.73523337 <a title="284-lsi-5" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>Author: Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, Bharath K. Sriperumbudur</p><p>Abstract: Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.</p><p>6 0.72447497 <a title="284-lsi-6" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>7 0.67722642 <a title="284-lsi-7" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>8 0.59972721 <a title="284-lsi-8" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>9 0.55312449 <a title="284-lsi-9" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>10 0.5267185 <a title="284-lsi-10" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>11 0.50513601 <a title="284-lsi-11" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>12 0.48700255 <a title="284-lsi-12" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>13 0.4568283 <a title="284-lsi-13" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>14 0.44552571 <a title="284-lsi-14" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>15 0.44289407 <a title="284-lsi-15" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>16 0.43388486 <a title="284-lsi-16" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>17 0.42840105 <a title="284-lsi-17" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>18 0.4279609 <a title="284-lsi-18" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>19 0.4231644 <a title="284-lsi-19" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>20 0.41902635 <a title="284-lsi-20" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.052), (11, 0.031), (21, 0.031), (36, 0.011), (38, 0.115), (39, 0.011), (42, 0.031), (54, 0.027), (55, 0.046), (64, 0.219), (74, 0.058), (76, 0.135), (80, 0.075), (92, 0.045), (94, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81520569 <a title="284-lda-1" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>2 0.78049791 <a title="284-lda-2" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>Author: Arthur Guez, David Silver, Peter Dayan</p><p>Abstract: Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, ﬁnding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayesoptimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a signiﬁcant margin on several well-known benchmark problems – because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an inﬁnite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration. 1</p><p>3 0.76306206 <a title="284-lda-3" href="./nips-2012-Graphical_Models_via_Generalized_Linear_Models.html">147 nips-2012-Graphical Models via Generalized Linear Models</a></p>
<p>Author: Eunho Yang, Genevera Allen, Zhandong Liu, Pradeep K. Ravikumar</p><p>Abstract: Undirected graphical models, also known as Markov networks, enjoy popularity in a variety of applications. The popular instances of these models such as Gaussian Markov Random Fields (GMRFs), Ising models, and multinomial discrete models, however do not capture the characteristics of data in many settings. We introduce a new class of graphical models based on generalized linear models (GLMs) by assuming that node-wise conditional distributions arise from exponential families. Our models allow one to estimate multivariate Markov networks given any univariate exponential distribution, such as Poisson, negative binomial, and exponential, by ﬁtting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We also provide examples of non-Gaussian high-throughput genomic networks learned via our GLM graphical models. 1</p><p>4 0.74154592 <a title="284-lda-4" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>5 0.73040056 <a title="284-lda-5" href="./nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">221 nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>Author: Pinghua Gong, Jieping Ye, Chang-shui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem, we propose a MultiStage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. 1</p><p>6 0.68427235 <a title="284-lda-6" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>7 0.68378854 <a title="284-lda-7" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>8 0.68319398 <a title="284-lda-8" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>9 0.68198699 <a title="284-lda-9" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>10 0.68151784 <a title="284-lda-10" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>11 0.68118423 <a title="284-lda-11" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>12 0.68092918 <a title="284-lda-12" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>13 0.68091702 <a title="284-lda-13" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>14 0.67884791 <a title="284-lda-14" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>15 0.6786378 <a title="284-lda-15" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>16 0.67724675 <a title="284-lda-16" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>17 0.67651457 <a title="284-lda-17" href="./nips-2012-Multi-task_Vector_Field_Learning.html">225 nips-2012-Multi-task Vector Field Learning</a></p>
<p>18 0.6763339 <a title="284-lda-18" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>19 0.67616451 <a title="284-lda-19" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>20 0.67609882 <a title="284-lda-20" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
