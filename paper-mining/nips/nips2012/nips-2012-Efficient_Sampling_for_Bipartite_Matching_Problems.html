<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 nips-2012-Efficient Sampling for Bipartite Matching Problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-111" href="#">nips2012-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 nips-2012-Efficient Sampling for Bipartite Matching Problems</h1>
<br/><p>Source: <a title="nips-2012-111-pdf" href="http://papers.nips.cc/paper/4491-efficient-sampling-for-bipartite-matching-problems.pdf">pdf</a></p><p>Author: Maksims Volkovs, Richard S. Zemel</p><p>Abstract: Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in realworld applications of these problems is intractable, making efﬁcient approximation methods essential for learning and inference. In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difﬁcult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efﬁciently approximates the target distribution, signiﬁcantly outperforming other sampling approaches. 1</p><p>Reference: <a title="nips-2012-111-reference" href="../nips2012_reference/nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. [sent-7, score-0.435]
</p><p>2 In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. [sent-9, score-0.589]
</p><p>3 This allows the sampler to match the difﬁcult target distributions common in these problems: highly multimodal distributions with well separated modes. [sent-10, score-0.51]
</p><p>4 We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efﬁciently approximates the target distribution, signiﬁcantly outperforming other sampling approaches. [sent-11, score-1.083]
</p><p>5 1  Introduction  Bipartite matching problems (BMPs), which involve mapping one set of items to another, are ubiquitous, with applications ranging from computational biology to information retrieval to computer vision. [sent-12, score-0.636]
</p><p>6 The features for any two items do not provide a natural measure of compatibility between the items, i. [sent-15, score-0.326]
</p><p>7 Consequently the goal of learning is to create a mapping from the item features to the target matches such that when an unlabeled instance is presented the same mapping can be applied to accurately infer the matches. [sent-18, score-0.294]
</p><p>8 Recently there has been a ﬂurry of new methods for sampling for bipartite matching problems. [sent-26, score-0.428]
</p><p>9 However, to the best of our knowledge, even for simple versions of bipartite matching problems, no efﬁcient sampler exists. [sent-28, score-0.631]
</p><p>10 We compare the efﬁciency and performance of our sampler to others on two applications. [sent-30, score-0.25]
</p><p>11 1  2  Problem Formulation  A standard BMP consists of the two sets of N items U = {u1 , . [sent-31, score-0.298]
</p><p>12 The goal is to ﬁnd an assignment of the items so that every item in U is matched to exactly one item in V and no two items share the same match. [sent-38, score-1.171]
</p><p>13 In this problem an assignment corresponds to a permutation π where π is a bijection {1, . [sent-39, score-0.366]
</p><p>14 , N }, mapping each item in U to its match in V ; we use the terms assignment and permutation interchangeably. [sent-45, score-0.637]
</p><p>15 We deﬁne π(i) = j to denote the index of a match vπ(i) = vj for item ui in π and use π −1 (j) = i to denote the reverse. [sent-46, score-0.663]
</p><p>16 Permutations have a useful property that any subset of the permutation also constitutes a valid permutation with respect to the items in the subset. [sent-47, score-0.778]
</p><p>17 Given a full permutation π we deﬁne π1:t (π1:0 = ∅) as a partial permutation of only the ﬁrst t items in U . [sent-49, score-0.759]
</p><p>18 The energy of a given assignment is typically formulated as a combination of ranks and the model’s output from the query-document features. [sent-55, score-0.416]
</p><p>19 For example in [12] the energy is deﬁned as: E(π, θ) = −  1 N  N  θi (N − π(i) + 1)  (2)  i=1  where θi is a score assigned by the model to ui . [sent-56, score-0.278]
</p><p>20 For example in [17] the energy is given by: E(π, θ) =  1 |ψ|  N u v θ, (ψi − ψπ(i) )2  (3)  i=1  u v where ψi and ψπ(i) are feature descriptors for points ui and vπ(i) . [sent-59, score-0.307]
</p><p>21 It is important to note here that for all models where the energy is additive we can compute the energy E(π1:t , θ) for any partial permutation π1:t by summing the potentials only over the t assignments in π1:t . [sent-61, score-0.74]
</p><p>22 A particular instance of BMP that has been studied extensively is the maximum weight bipartite matching problem (WBMP). [sent-69, score-0.381]
</p><p>23 Finding the assignment with the maximum energy is tractable and can be solved in O(N 3 ) [16]. [sent-71, score-0.343]
</p><p>24 The majority of the proposed samplers are designed for 2  WBMPs and cannot be applied to the more general BMPs where the energy includes higher order potentials. [sent-73, score-0.315]
</p><p>25 There is thus an evident need to develop an effective sampler applicable to any BMP distribution. [sent-75, score-0.25]
</p><p>26 3  Related Approaches  In this section we brieﬂy describe existing sampling approaches, some of which have been developed speciﬁcally for bipartite matching problems while others come from matrix permanent research. [sent-76, score-0.573]
</p><p>27 To do that we start with some initial assignment π and consider a subset of items in U ; for illustration purposes we will use two items ui and uj . [sent-79, score-0.882]
</p><p>28 Given the selected subset of items the Gibbs sampler considers all possible assignment swaps within this subset. [sent-80, score-0.804]
</p><p>29 In our example there are only two possibilities: leave π unchanged or swap π(i) with π(j) to produce a new permutation π . [sent-81, score-0.276]
</p><p>30 Conditioned on the assignment of all the other items in U that were not selected, the probability of each permutation is: p(π |π\{i,j} ) =  exp(−E(π , θ)) exp(−E(π, θ)) + exp(−E(π , θ))  p(π|π\{i,j} ) = 1 − p(π |π\{i,j} )  where π\{i,j} is permutation π with ui and uj removed. [sent-82, score-0.986]
</p><p>31 The main reason for this is that the path from one probable assignment to another using only pairwise swaps is likely to go through regions that have very low probability [5]. [sent-86, score-0.258]
</p><p>32 This makes it very unlikely that those moves will be accepted, which typically traps the sampler in one mode. [sent-87, score-0.301]
</p><p>33 Thus, the local structure of the Gibbs sampler is likely to be inadequate for problems of the type considered here, in which several probable assignments will produce well-separated modes. [sent-88, score-0.416]
</p><p>34 2  Chain-Based Approaches  Chain-based methods extend the assignment swap idea behind the Gibbs sampler to generate samples more efﬁciently from WBMP distributions. [sent-90, score-0.537]
</p><p>35 Instead of randomly choosing subsets of items to swap, chain-based method generate a sequence (chain) of interdependent swaps. [sent-91, score-0.333]
</p><p>36 Given a (random) starting permutation π, an item ui (currently matched with vπ(i) ) is selected at random and a new match vj is proposed with probability p(ui , vj |θ) where p depends on the unary potential φ(ui , vj , θ) in the WBMP energy (see Equation 4). [sent-92, score-1.781]
</p><p>37 Now, assuming that the match {ui , vj }, is selected, matches {ui , vπ(i) } and {uπ−1 (j) , vj } are removed from π and {ui , vj } is added to make π . [sent-93, score-1.046]
</p><p>38 After this change uπ−1 (j) and vπ(i) are no longer matched to any item so π is a partial assignment. [sent-94, score-0.325]
</p><p>39 This chain-like match sampling is repeated either until π is a complete assignment or a termination criteria is reached. [sent-96, score-0.317]
</p><p>40 , [5] empirically demonstrated that the chain ﬂipping sampler can mix better than the Gibbs sampler when applied to multimodal distributions. [sent-99, score-0.618]
</p><p>41 First, unlike the Gibbs sampler which always maintains a valid assignment, the intermediate assignments π in chain-based methods are incomplete. [sent-101, score-0.421]
</p><p>42 This means that the chain either has to be run until a valid assignment is generated [5] or terminated early and produce an incomplete assignment [11]. [sent-102, score-0.528]
</p><p>43 In the ﬁrst case the sampler has a non-deterministic run-time whereas in the second case the incomplete assignment can not be taken as a valid sample from the model. [sent-103, score-0.493]
</p><p>44 Items are U = {u1 , u2 , u3 } and V = {v1 , v2 , v3 }; the reference permutation is σ = {2, 3, 1}. [sent-107, score-0.311]
</p><p>45 Under this model a permutation π is generated by ﬁrst selecting item vπ(1) from the set of N items and placing it in the ﬁrst position, then selecting vπ(2) from the remaining N − 1 items and placing it the second position, and so on until all N items are placed. [sent-133, score-1.321]
</p><p>46 Our approach is based on the observation that the sequential procedure behind the Plackett-Luce model can also be extended to generate matches between item sets. [sent-142, score-0.341]
</p><p>47 Instead of placing items into ranked positions we can think of the Plackett-Luce generative process as sequentially matching ranks to the items in V , as illustrated in the top row of Figure 1. [sent-143, score-0.933]
</p><p>48 To generate the permutation π = {3, 1, 2} the Plackett-Luce model ﬁrst matches rank 1 with vπ(1) = v2 then rank 2 with vπ(2) = v3 and ﬁnally rank 3 with vπ(3) = v1 . [sent-144, score-0.511]
</p><p>49 Unlike ranks, items in U do not have a natural order so we use a reference permutation σ, which speciﬁes the order in which items in U are matched. [sent-146, score-0.907]
</p><p>50 Formally the sequential matching process proceeds as follows: given some reference permutation σ, we start with an empty assignment π1:0 = ∅. [sent-149, score-0.788]
</p><p>51 , vjN } denotes the set of items not matched in π1:t−1 . [sent-156, score-0.376]
</p><p>52 Note that similarly to the Plackett-Luce model, |V \ π1:t−1 | = N − t + 1 so at each iteration, uσ(t) will have N − t + 1 left over items in V \ π1:t−1 to match with. [sent-157, score-0.403]
</p><p>53 We deﬁne the conditional probability of each such match to be p(vj |uσ(t) , π1:t−1 ), vj ∈V \π1:t−1 p(vj |uσ(t) , π1:t−1 ) = 1. [sent-158, score-0.397]
</p><p>54 After N iterations the permutation π1:N = π is produced with probability: N p(vπ(σ(t)) |uσ(t) , π1:t−1 )  Q(π|σ) =  (6)  t=1  where vπ(σ(t)) is a match for uσ(t) in π. [sent-159, score-0.339]
</p><p>55 The conditional match probabilities depend on both the current item uσ(t) and on the partial assignment π1:t−1 . [sent-160, score-0.558]
</p><p>56 Introducing this dependency generalizes the Plackett-Luce model which only takes into account that the items in π1:t−1 are already matched but does not take into account how these items are matched. [sent-161, score-0.697]
</p><p>57 This dependency becomes very important when the energy contains pairwise and/or higher order potentials as it allows us to compute the change in energy for each new match, in turn allowing for close approximations to the target BMP distribution. [sent-162, score-0.516]
</p><p>58 1  The important consequence of this proposition is that it allows us to work with a very rich class of matching probabilities with arbitrary dependencies and still obtain a valid distribution over assignments with a simple way to generate exact samples from it. [sent-164, score-0.564]
</p><p>59 1  Proposal Distribution  Given the general matching probabilities the goal is to deﬁne them so that the resulting proposal distribution Q matches the target distribution as closely as possible. [sent-168, score-0.586]
</p><p>60 The partial energy ignores all the items that are not matched in π1:t and thus provides an estimate of the ”current” energy at each iteration t. [sent-170, score-0.791]
</p><p>61 Using partial energies we can also ﬁnd the changes in energy when a given item is matched. [sent-171, score-0.403]
</p><p>62 However, in this form we see that p(vj |uσ(t) , π1:t−1 ) is directly related to the change in the partial energy 1  The proof is in the supplementary material. [sent-175, score-0.259]
</p><p>63 Thus, the matching choices will be made solely based on the changes in the partial energy. [sent-177, score-0.296]
</p><p>64 × = ∗ ∗ Z1 (uσ(1) , π1:0 ) ZN (uσ(N ) , π1:N −1 ) Z ∗ (π, σ)  Here Z ∗ (π, σ) is the normalization factor which depends both on the reference permutation σ and the generated assignment π. [sent-181, score-0.476]
</p><p>65 The numerator remains the exponent of the energy but the denominator is no longer a constant; rather it is a function which depends on the generated assignment and the reference permutation. [sent-183, score-0.453]
</p><p>66 Note that the proposal distribution deﬁned above can be used to generate samples for any target distribution with arbitrary energy consisting of single and/or higher order potentials. [sent-184, score-0.49]
</p><p>67 To the best of our knowledge aside from the Gibbs sampler this is the only sampling procedure that can be applied to arbitrary BMP distributions. [sent-185, score-0.297]
</p><p>68 To achieve this effect with the sequential matching model we introduce an additional parameter ρ which we refer to as temperature: p(vj |uσ(t) , π1:t−1 , ρ) ∝ exp(−E(H(vj , uσ(t) , π1:t−1 ), θ)/ρ). [sent-188, score-0.312]
</p><p>69 Decreasing ρ leads to sharp proposal distributions typically highly skewed towards one speciﬁc assignment, while increasing ρ makes the proposal distribution approach the uniform distribution. [sent-189, score-0.306]
</p><p>70 To ensure that the SM sampler converges to the required distribution we demonstrate that it satisﬁes the three requisite properties: detailed balance, ergodicity, and aperiodicity [15]. [sent-191, score-0.318]
</p><p>71 3  Reference Permutation  Fixing the reference permutation σ yields a Algorithm 1 Sequential Matching (SM) state independent sampler. [sent-197, score-0.311]
</p><p>72 Initialize π1:0 = ∅ However, for the general energy based distrifor t = 1 to N do {generate sample from Q(·|σ)} butions considered here ﬁnding the MAP state Find a match vj for uσ(t) using: can be very expensive and in many cases inp(vj |uσ(t) , π1:t−1 , ρ) tractable. [sent-199, score-0.575]
</p><p>73 Moreover, even if MAP can be found Add {uσ(t) , vj } to π1:t−1 to get π1:t efﬁciently there is still no guarantee that using it end for as the reference permutation will lead to a good Calculate forward probability: sampler. [sent-200, score-0.603]
</p><p>74 To avoid these problems we use a state Q(π|σ) = N p(vπ(σ(t)) |uσ(t) , π1:t−1 , ρ) t=1 Calculate backward probability: dependent sampler where the reference permuQ(σ|π) = N p(vσ(π(t)) |uπ(t) , σ1:t−1 , ρ) tation σ is updated every time a sample gets act=1 cepted. [sent-201, score-0.386]
</p><p>75 In the matching example (bottom row if U nif orm(0, 1) < exp(−E(π,θ))Q(σ|π) then exp(−E(σ,θ))Q(π|σ) of Figure 1) if the new match at t = 3 is acσ←π end if cepted then σ would be updated to {3, 1, 2}. [sent-202, score-0.363]
</p><p>76 Algorithm 1 summarizes the Metropolis-Hastings procedure for the state dependent sequential matching sampler. [sent-204, score-0.312]
</p><p>77 5  Experiments  To test the sequential matching sampling approach we conducted extensive experiments. [sent-205, score-0.359]
</p><p>78 We considered document ranking and image matching, two popular applications of BMP; and for the sake of 6  Table 1: Average Hellinger distances for learning to rank (left half) and image matching (right half) problems. [sent-206, score-0.449]
</p><p>79 For N = 50 we were unable to get a single sample from the RP sampler for any c in the allocated time limit (over 5 minutes). [sent-209, score-0.335]
</p><p>80 When comparing the samplers we concentrated on evaluating how well the Monte Carlo estimates of probabilities produced by the samplers approximate the true distribution P . [sent-325, score-0.351]
</p><p>81 When target probabilities are known this method of evaluation provides a good estimate of performance since the ultimate goal of any sampler is to approximate P as closely as possible. [sent-326, score-0.376]
</p><p>82 Computing D exactly quickly becomes intractable as the number of items grows. [sent-332, score-0.298]
</p><p>83 To overcome this problem we note that if a given permutation π is not generated by any of the samplers then the term P (π)Q(π) is 0 and does not affect the resulting estimate of D for any sampler. [sent-333, score-0.317]
</p><p>84 Since any valid sampler will eventually produce samples from the target distribution, we tested the methods with short chain lengths. [sent-341, score-0.519]
</p><p>85 Furthermore, to make comparisons fair we used the block GB sampler with the block size of 7 (the largest computationally feasible size) as the reference point. [sent-344, score-0.36]
</p><p>86 For each query the distribution over assignments was parametrized by the energy given in Equation 2. [sent-353, score-0.325]
</p><p>87 2 From the table it is seen that all the samplers perform equally well when the number of items is small (N = 8). [sent-359, score-0.414]
</p><p>88 However, as the number of items increases SM signiﬁcantly outperforms all other samplers. [sent-360, score-0.298]
</p><p>89 For N = 50 we were unable to get a single sample from the RP sampler after running it for over 5 minutes. [sent-362, score-0.291]
</p><p>90 Consequently the total rejection probability increases linearly with the number of items N . [sent-367, score-0.325]
</p><p>91 2  Image Matching  For an image matching task we followed the framework of Petterson et al. [sent-372, score-0.267]
</p><p>92 The target distribution over matchings was parametrized by the energy given by Equation 3 where ψ’s are the SIFT feature descriptors. [sent-377, score-0.295]
</p><p>93 This is the likely cause of the poor performance of the GB and CF samplers as both samplers propose new assignments through local moves. [sent-392, score-0.325]
</p><p>94 As in the learning to rank experiments, we found the rejection rate for the RP sampler to increase signiﬁcantly for N ≥ 25. [sent-393, score-0.347]
</p><p>95 We were unable to obtain any samples in the allocated time (over 5 mins) from the RP sampler for N = 50. [sent-394, score-0.37]
</p><p>96 6  Conclusion  In this paper we introduced a new sampling approach for bipartite matching problems based on a generalization of the Plackett-Luce model. [sent-396, score-0.454]
</p><p>97 In this approach the matching probabilities at each stage are conditioned on the partial assignment made to that point. [sent-397, score-0.524]
</p><p>98 We also plan to investigate the relationship between the proposal distribution produced by sequential matching and the target one. [sent-401, score-0.543]
</p><p>99 Protein structure comparison using bipartite graph matching and its application to protein structure classiﬁcation. [sent-561, score-0.417]
</p><p>100 A bipartite graph matching framework for ﬁnding correspondences between structural elements in two proteins. [sent-579, score-0.381]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bmp', 0.337), ('items', 0.298), ('vj', 0.292), ('sampler', 0.25), ('wbmp', 0.247), ('matching', 0.237), ('permutation', 0.201), ('energy', 0.178), ('item', 0.166), ('assignment', 0.165), ('sm', 0.155), ('bipartite', 0.144), ('hellinger', 0.139), ('gb', 0.137), ('rp', 0.126), ('permanent', 0.119), ('samplers', 0.116), ('proposal', 0.112), ('cf', 0.112), ('reference', 0.11), ('match', 0.105), ('ui', 0.1), ('assignments', 0.093), ('matched', 0.078), ('valid', 0.078), ('gibbs', 0.076), ('exp', 0.075), ('sequential', 0.075), ('chain', 0.07), ('rank', 0.07), ('swaps', 0.069), ('bmps', 0.067), ('matches', 0.065), ('target', 0.063), ('probabilities', 0.063), ('ranking', 0.06), ('partial', 0.059), ('zt', 0.056), ('unary', 0.055), ('ipping', 0.053), ('partitioning', 0.052), ('swap', 0.052), ('ranks', 0.049), ('sampling', 0.047), ('aperiodicity', 0.045), ('dellaert', 0.045), ('insertion', 0.045), ('ramos', 0.045), ('allocated', 0.044), ('unable', 0.041), ('recursive', 0.041), ('retrieval', 0.041), ('modes', 0.04), ('volkovs', 0.04), ('acceptance', 0.037), ('correspondence', 0.037), ('protein', 0.036), ('temperature', 0.036), ('samples', 0.035), ('distributions', 0.035), ('generate', 0.035), ('ranging', 0.034), ('petterson', 0.034), ('produced', 0.033), ('caetano', 0.033), ('parametrized', 0.031), ('potentials', 0.031), ('placing', 0.03), ('image', 0.03), ('descriptors', 0.029), ('compatibility', 0.028), ('ergodicity', 0.028), ('toronto', 0.028), ('half', 0.028), ('rejection', 0.027), ('moves', 0.027), ('run', 0.027), ('problems', 0.026), ('mix', 0.026), ('combinatorial', 0.024), ('typically', 0.024), ('yahoo', 0.024), ('vn', 0.024), ('zemel', 0.024), ('aggregation', 0.024), ('probable', 0.024), ('accepted', 0.024), ('distribution', 0.023), ('dependency', 0.023), ('equation', 0.023), ('produce', 0.023), ('mcmc', 0.022), ('distances', 0.022), ('selected', 0.022), ('change', 0.022), ('multimodal', 0.022), ('higher', 0.021), ('row', 0.021), ('robotics', 0.021), ('uj', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="111-tfidf-1" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>Author: Maksims Volkovs, Richard S. Zemel</p><p>Abstract: Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in realworld applications of these problems is intractable, making efﬁcient approximation methods essential for learning and inference. In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difﬁcult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efﬁciently approximates the target distribution, signiﬁcantly outperforming other sampling approaches. 1</p><p>2 0.20044534 <a title="111-tfidf-2" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>Author: Andriy Mnih, Yee W. Teh</p><p>Abstract: User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative ﬁltering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difﬁcult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative ﬁltering with implicit feedback based on modelling the user’s item selection process. In the interests of scalability, we restrict our attention to treestructured distributions over items and develop a principled and efﬁcient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data. 1</p><p>3 0.18126838 <a title="111-tfidf-3" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>Author: Michael C. Hughes, Erik B. Sudderth, Emily B. Fox</p><p>Abstract: Applications of Bayesian nonparametric methods require learning and inference algorithms which efﬁciently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and lengthy burn-in periods for just six sequences. 1</p><p>4 0.15809371 <a title="111-tfidf-4" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>Author: Francois Caron, Yee W. Teh</p><p>Abstract: We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an inﬁnite number of choice items. Our framework is based on the theory of random atomic measures, with the prior speciﬁed by a gamma process. We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation. We develop a time-varying extension of our model, and apply it to the New York Times lists of weekly bestselling books. 1</p><p>5 0.15750103 <a title="111-tfidf-5" href="./nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">165 nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>Author: Sahand Negahban, Sewoong Oh, Devavrat Shah</p><p>Abstract: The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR’s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, ﬁnding ‘scores’ for each object (e.g. player’s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efﬁcacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the ﬁnite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]. 1</p><p>6 0.15095222 <a title="111-tfidf-6" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>7 0.12886012 <a title="111-tfidf-7" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>8 0.1239037 <a title="111-tfidf-8" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>9 0.11997414 <a title="111-tfidf-9" href="./nips-2012-Accuracy_at_the_Top.html">30 nips-2012-Accuracy at the Top</a></p>
<p>10 0.11706315 <a title="111-tfidf-10" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>11 0.11385672 <a title="111-tfidf-11" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>12 0.11185487 <a title="111-tfidf-12" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>13 0.1102744 <a title="111-tfidf-13" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>14 0.10839583 <a title="111-tfidf-14" href="./nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</a></p>
<p>15 0.098345503 <a title="111-tfidf-15" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>16 0.094267823 <a title="111-tfidf-16" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>17 0.093233608 <a title="111-tfidf-17" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>18 0.091293335 <a title="111-tfidf-18" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>19 0.079220161 <a title="111-tfidf-19" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>20 0.075783253 <a title="111-tfidf-20" href="./nips-2012-Proper_losses_for_learning_from_partial_labels.html">280 nips-2012-Proper losses for learning from partial labels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.207), (1, 0.048), (2, -0.025), (3, -0.032), (4, -0.13), (5, -0.127), (6, -0.009), (7, 0.089), (8, 0.101), (9, 0.209), (10, -0.161), (11, 0.033), (12, -0.157), (13, 0.004), (14, 0.015), (15, -0.078), (16, -0.038), (17, -0.104), (18, 0.042), (19, -0.006), (20, 0.15), (21, -0.058), (22, -0.069), (23, -0.062), (24, -0.081), (25, 0.019), (26, -0.061), (27, 0.027), (28, 0.036), (29, -0.007), (30, -0.053), (31, 0.098), (32, -0.024), (33, 0.005), (34, 0.014), (35, 0.024), (36, -0.104), (37, 0.013), (38, -0.044), (39, -0.04), (40, 0.097), (41, -0.033), (42, -0.154), (43, -0.047), (44, 0.033), (45, -0.017), (46, -0.013), (47, 0.019), (48, 0.015), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95554644 <a title="111-lsi-1" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>Author: Maksims Volkovs, Richard S. Zemel</p><p>Abstract: Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in realworld applications of these problems is intractable, making efﬁcient approximation methods essential for learning and inference. In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difﬁcult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efﬁciently approximates the target distribution, signiﬁcantly outperforming other sampling approaches. 1</p><p>2 0.74137032 <a title="111-lsi-2" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>Author: Michael C. Hughes, Erik B. Sudderth, Emily B. Fox</p><p>Abstract: Applications of Bayesian nonparametric methods require learning and inference algorithms which efﬁciently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM), enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, Gaussian, and autoregressive emission models. Together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and lengthy burn-in periods for just six sequences. 1</p><p>3 0.66935289 <a title="111-lsi-3" href="./nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">165 nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>Author: Sahand Negahban, Sewoong Oh, Devavrat Shah</p><p>Abstract: The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR’s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, ﬁnding ‘scores’ for each object (e.g. player’s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efﬁcacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the ﬁnite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]. 1</p><p>4 0.66681373 <a title="111-lsi-4" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>Author: Andriy Mnih, Yee W. Teh</p><p>Abstract: User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative ﬁltering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difﬁcult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative ﬁltering with implicit feedback based on modelling the user’s item selection process. In the interests of scalability, we restrict our attention to treestructured distributions over items and develop a principled and efﬁcient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data. 1</p><p>5 0.63088107 <a title="111-lsi-5" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon, Seung-jean Kim</p><p>Abstract: Recent approaches to collaborative ﬁltering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative ﬁltering algorithms, with non-constant combination coefﬁcients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative ﬁltering algorithms. 1</p><p>6 0.63072646 <a title="111-lsi-6" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>7 0.58053142 <a title="111-lsi-7" href="./nips-2012-Accuracy_at_the_Top.html">30 nips-2012-Accuracy at the Top</a></p>
<p>8 0.55531055 <a title="111-lsi-8" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>9 0.54852664 <a title="111-lsi-9" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>10 0.54489583 <a title="111-lsi-10" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>11 0.51389921 <a title="111-lsi-11" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>12 0.5026772 <a title="111-lsi-12" href="./nips-2012-Learning_from_the_Wisdom_of_Crowds_by_Minimax_Entropy.html">189 nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</a></p>
<p>13 0.48526067 <a title="111-lsi-13" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>14 0.48237506 <a title="111-lsi-14" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>15 0.47211993 <a title="111-lsi-15" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>16 0.46927628 <a title="111-lsi-16" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>17 0.46522292 <a title="111-lsi-17" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>18 0.46016324 <a title="111-lsi-18" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>19 0.45635805 <a title="111-lsi-19" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>20 0.44755888 <a title="111-lsi-20" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (21, 0.024), (38, 0.161), (39, 0.013), (42, 0.016), (54, 0.027), (55, 0.016), (74, 0.1), (76, 0.191), (80, 0.103), (85, 0.163), (92, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91644514 <a title="111-lda-1" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>Author: Brett Vintch, Andrew Zaharia, J Movshon, Hhmi) Hhmi), Eero P. Simoncelli</p><p>Abstract: Many visual and auditory neurons have response properties that are well explained by pooling the rectiﬁed responses of a set of spatially shifted linear ﬁlters. These ﬁlters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple ﬁlters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the ﬁlters reside rather than the ﬁlters themselves. Here, we assume a linear-nonlinear–linear-nonlinear (LN-LN) cascade model in which the ﬁrst linear stage is a set of shifted (‘convolutional’) copies of a common ﬁlter, and the ﬁrst nonlinear stage consists of rectifying scalar nonlinearities that are identical for all ﬁlter outputs. We refer to these initial LN elements as the ‘subunits’ of the receptive ﬁeld. The second linear stage then computes a weighted sum of the responses of the rectiﬁed subunits. We present a method for directly ﬁtting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model signiﬁcantly outperforms STA and STC in terms of cross-validated accuracy and efﬁciency. 1</p><p>same-paper 2 0.90615773 <a title="111-lda-2" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>Author: Maksims Volkovs, Richard S. Zemel</p><p>Abstract: Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in realworld applications of these problems is intractable, making efﬁcient approximation methods essential for learning and inference. In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difﬁcult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efﬁciently approximates the target distribution, signiﬁcantly outperforming other sampling approaches. 1</p><p>3 0.86588353 <a title="111-lda-3" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>Author: Mark Herbster, Stephen Pasteris, Fabio Vitale</p><p>Abstract: We consider the problem of performing efﬁcient sum-product computations in an online setting over a tree. A natural application of our methods is to compute the marginal distribution at a vertex in a tree-structured Markov random ﬁeld. Belief propagation can be used to solve this problem, but requires time linear in the size of the tree, and is therefore too slow in an online setting where we are continuously receiving new data and computing individual marginals. With our method we aim to update the data and compute marginals in time that is no more than logarithmic in the size of the tree, and is often signiﬁcantly less. We accomplish this via a hierarchical covering structure that caches previous local sum-product computations. Our contribution is three-fold: we i) give a linear time algorithm to ﬁnd an optimal hierarchical cover of a tree; ii) give a sum-productlike algorithm to efﬁciently compute marginals with respect to this cover; and iii) apply “i” and “ii” to ﬁnd an efﬁcient algorithm with a regret bound for the online allocation problem in a multi-task setting. 1</p><p>4 0.86063844 <a title="111-lda-4" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>Author: Vasiliy Karasev, Alessandro Chiuso, Stefano Soatto</p><p>Abstract: We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of “visual search” of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a “passive” agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an “omnipotent” agent, capable of inﬁnite control authority, can achieve arbitrarily good performance (asymptotically). In between these limiting cases, the tradeoff can be characterized empirically. 1</p><p>5 0.85969198 <a title="111-lda-5" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>6 0.85699987 <a title="111-lda-6" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>7 0.85695589 <a title="111-lda-7" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>8 0.85575557 <a title="111-lda-8" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>9 0.85573727 <a title="111-lda-9" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>10 0.85476983 <a title="111-lda-10" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>11 0.8538028 <a title="111-lda-11" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>12 0.85243315 <a title="111-lda-12" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>13 0.85225368 <a title="111-lda-13" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>14 0.8518855 <a title="111-lda-14" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>15 0.85177267 <a title="111-lda-15" href="./nips-2012-Learning_Mixtures_of_Tree_Graphical_Models.html">180 nips-2012-Learning Mixtures of Tree Graphical Models</a></p>
<p>16 0.85152221 <a title="111-lda-16" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>17 0.85088003 <a title="111-lda-17" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>18 0.85086679 <a title="111-lda-18" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>19 0.85071105 <a title="111-lda-19" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>20 0.85059142 <a title="111-lda-20" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
