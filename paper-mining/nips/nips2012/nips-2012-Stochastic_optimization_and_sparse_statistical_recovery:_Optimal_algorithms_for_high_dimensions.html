<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-325" href="#">nips2012-325</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</h1>
<br/><p>Source: <a title="nips-2012-325-pdf" href="http://papers.nips.cc/paper/4645-stochastic-optimization-and-sparse-statistical-recovery-optimal-algorithms-for-high-dimensions.pdf">pdf</a></p><p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a O(d/T ) convergence rate for strongly convex objectives in d dimensions and O( s(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm is based on successively solving a series of ℓ1 -regularized optimization problems using Nesterov’s dual averaging algorithm. We establish that the error of our solution after T iterations is at most O(s(log d)/T ), with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also conﬁrmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</p><p>Reference: <a title="nips-2012-325-reference" href="../nips2012_reference/nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('epoch', 0.635), ('ti', 0.209), ('ri', 0.196), ('lsc', 0.167), ('rad', 0.16), ('kt', 0.16), ('stochast', 0.157), ('log', 0.148), ('length', 0.14), ('rq', 0.133), ('lipschitz', 0.122), ('yi', 0.114), ('juditsky', 0.104), ('strongly', 0.104), ('rda', 0.1), ('rat', 0.096), ('bq', 0.095), ('nesterov', 0.095), ('convex', 0.094), ('loss', 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="325-tfidf-1" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a O(d/T ) convergence rate for strongly convex objectives in d dimensions and O( s(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm is based on successively solving a series of ℓ1 -regularized optimization problems using Nesterov’s dual averaging algorithm. We establish that the error of our solution after T iterations is at most O(s(log d)/T ), with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also conﬁrmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</p><p>2 0.29914126 <a title="325-tfidf-2" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>Author: Xi Chen, Qihang Lin, Javier Pena</p><p>Abstract: This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth. We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the opti1 1 mal rate of O( N + N 2 ) for N iterations, which improves the rate O( log N ) for preN vious regularized dual averaging algorithms. In addition, our method constructs the ﬁnal solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., 1 -norm), it has the advantage of encouraging sparser solutions. We further develop a multistage extension using the proposed algorithm as a subroutine, which achieves the 1 uniformly-optimal rate O( N + exp{−N }) for strongly convex loss. 1</p><p>3 0.21084037 <a title="325-tfidf-3" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>Author: Andre Wibisono, Martin J. Wainwright, Michael I. Jordan, John C. Duchi</p><p>Abstract: We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their ﬁnite-sample convergence rates. We show that if pairs of function values are available, algorithms that √ gradient estimates based on random perturbations suffer a factor use of at most d in convergence rate over traditional stochastic gradient methods, where d is the problem dimension. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problemdependent quantities: they cannot be improved by more than constant factors. 1</p><p>4 0.17389312 <a title="325-tfidf-4" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, Jinfeng Yi</p><p>Abstract: Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. 1</p><p>5 0.14794305 <a title="325-tfidf-5" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>Author: Pieter-jan Kindermans, Hannes Verschore, David Verstraeten, Benjamin Schrauwen</p><p>Abstract: The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session. 1</p><p>6 0.13853873 <a title="325-tfidf-6" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>7 0.12506603 <a title="325-tfidf-7" href="./nips-2012-Convergence_Rate_Analysis_of_MAP_Coordinate_Minimization_Algorithms.html">84 nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</a></p>
<p>8 0.12099563 <a title="325-tfidf-8" href="./nips-2012-Coding_efficiency_and_detectability_of_rate_fluctuations_with_non-Poisson_neuronal_firing.html">73 nips-2012-Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing</a></p>
<p>9 0.11955772 <a title="325-tfidf-9" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>10 0.11891779 <a title="325-tfidf-10" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>11 0.1121875 <a title="325-tfidf-11" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>12 0.10994501 <a title="325-tfidf-12" href="./nips-2012-Globally_Convergent_Dual_MAP_LP_Relaxation_Solvers_using_Fenchel-Young_Margins.html">143 nips-2012-Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins</a></p>
<p>13 0.10393459 <a title="325-tfidf-13" href="./nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">206 nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>14 0.10075757 <a title="325-tfidf-14" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>15 0.099930435 <a title="325-tfidf-15" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>16 0.098256677 <a title="325-tfidf-16" href="./nips-2012-Mixability_in_Statistical_Learning.html">217 nips-2012-Mixability in Statistical Learning</a></p>
<p>17 0.096379817 <a title="325-tfidf-17" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>18 0.094839089 <a title="325-tfidf-18" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>19 0.093015999 <a title="325-tfidf-19" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>20 0.090598255 <a title="325-tfidf-20" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.262), (1, 0.002), (2, 0.094), (3, -0.166), (4, -0.01), (5, 0.114), (6, -0.044), (7, -0.03), (8, -0.021), (9, 0.005), (10, -0.06), (11, 0.016), (12, 0.072), (13, 0.014), (14, 0.115), (15, 0.015), (16, 0.005), (17, 0.075), (18, -0.115), (19, -0.058), (20, 0.061), (21, 0.048), (22, -0.091), (23, 0.115), (24, 0.06), (25, -0.06), (26, -0.115), (27, -0.053), (28, 0.028), (29, -0.001), (30, -0.016), (31, -0.007), (32, 0.056), (33, -0.0), (34, 0.109), (35, 0.05), (36, 0.196), (37, -0.004), (38, -0.147), (39, 0.018), (40, 0.092), (41, -0.032), (42, -0.111), (43, 0.042), (44, -0.042), (45, 0.119), (46, 0.045), (47, -0.006), (48, 0.081), (49, 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93343753 <a title="325-lsi-1" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a O(d/T ) convergence rate for strongly convex objectives in d dimensions and O( s(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm is based on successively solving a series of ℓ1 -regularized optimization problems using Nesterov’s dual averaging algorithm. We establish that the error of our solution after T iterations is at most O(s(log d)/T ), with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also conﬁrmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</p><p>2 0.87410653 <a title="325-lsi-2" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>Author: Xi Chen, Qihang Lin, Javier Pena</p><p>Abstract: This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth. We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the opti1 1 mal rate of O( N + N 2 ) for N iterations, which improves the rate O( log N ) for preN vious regularized dual averaging algorithms. In addition, our method constructs the ﬁnal solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., 1 -norm), it has the advantage of encouraging sparser solutions. We further develop a multistage extension using the proposed algorithm as a subroutine, which achieves the 1 uniformly-optimal rate O( N + exp{−N }) for strongly convex loss. 1</p><p>3 0.83057749 <a title="325-lsi-3" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>Author: Andre Wibisono, Martin J. Wainwright, Michael I. Jordan, John C. Duchi</p><p>Abstract: We consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their ﬁnite-sample convergence rates. We show that if pairs of function values are available, algorithms that √ gradient estimates based on random perturbations suffer a factor use of at most d in convergence rate over traditional stochastic gradient methods, where d is the problem dimension. We complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problemdependent quantities: they cannot be improved by more than constant factors. 1</p><p>4 0.77813423 <a title="325-lsi-4" href="./nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">20 nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<p>Author: Nicolas L. Roux, Mark Schmidt, Francis R. Bach</p><p>Abstract: We propose a new stochastic gradient method for optimizing the sum of a ﬁnite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly. 1</p><p>5 0.70043504 <a title="325-lsi-5" href="./nips-2012-Interpreting_prediction_markets%3A_a_stochastic_approach.html">161 nips-2012-Interpreting prediction markets: a stochastic approach</a></p>
<p>Author: Nicolas D. Penna, Mark D. Reid, Rafael M. Frongillo</p><p>Abstract: We strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a ﬁxed distribution. This provides new insights into how market prices (and price paths) may be interpreted as a summary of the market’s belief distribution by relating them to the optimization problem being solved. In particular, we show that under certain conditions the stationary point of the stochastic process of prices generated by the market is equal to the market’s Walrasian equilibrium of classic market analysis. Together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour. 1 Introduction and literature review This paper is part of an ongoing line of research, spanning several authors, into formal connections between markets and machine learning. In [5] an equivalence is shown between the theoretically popular prediction market makers based on sequences of proper scoring rules and follow the regularised leader, a form of no-regret online learning. By modelling the traders that demand the assets the market maker is oﬀering we are able to extend the equivalence to stochastic mirror decent. The dynamics of wealth transfer is studied in [3], for a sequence of markets between agents that behave as Kelly bettors (i.e. have log utilities), and an equivalence to stochastic gradient decent is analysed. More broadly, [9, 2] have analysed how a wide range of machine learning models can be implemented in terms of market equilibria. The literature on the interpretation of prediction market prices [7, 11] has had the goal of relating the equilibrium prices to the distribution of the beliefs of traders. More recent work [8] has looked at a stochastic model, and studied the behavior of simple agents sequentially interacting with the market. We continue this latter path of research, motivated by the observation that the equilibrium price may be a poor predictor of the behavior in a volitile prediction market. As such, we seek a more detailed understanding of the market than the equilibrium point – we would like to know what the “stationary distribution” of the price is, as time goes to inﬁnity. 1 As is standard in the literature, we assume a ﬁxed (product) distribution over traders’ beliefs and wealth. Our model features an automated market maker, following the framework of [1] is becoming a standard framework in the ﬁeld. We obtain two results. First, we prove that under certain conditions the stationary point of our stochastic process deﬁned by the market maker and a belief distribution of traders converges to the Walrasian equilibrium of the market as the market liquidity increases. This result, stated in Theorem 1, is general in the sense that only technical convergence conditions are placed on the demand functions of the traders – as such, we believe it is a generalisation of the stochastic result of [8] to cases where agents are are not limited to linear demands, and leave this precise connection to future work. Second, we show in Corollary 1 that when traders are Kelly bettors, the resulting stochastic market process is equivalent to stochastic mirror descent; see e.g. [6]. This result adds to the growing literature which relates prediction markets, and automated market makers in general, to online learning; see e.g. [1], [5], [3] . This connection to mirror descent seems to suggest that the prices in a prediction market at any given time may be meaningless, as the ﬁnal point in stochastic mirror descent often has poor convergence guarantees. However, standard results suggest that a prudent way to form a “consensus estimate” from a prediction market is to average the prices. The average price, assuming our market model is reasonable, is provably close to the stationary price. In Section 5 we give a natural example that exhibits this behavior. Beyond this, however, Theorem 2 gives us insight into the relationship between the market liquidity and the convergence of prices; in particular it suggests that we should increase liquidity at a rate √ of t if we wish the price to settle down at the right rate. 2 Model Our market model will follow the automated market maker framework of [1]. We will equip our market maker with a strictly convex function C : Rn → R which is twice continuously diﬀerentiable. For brevity we will write ϕ := C. The outcome space is Ω, and the contracts are determined by a payoﬀ function φ : Ω → Rn such that Π := ϕ(Rn ) = ConvHull(φ(Ω)). That is, the derivative space Π of C (the “instantaneous prices”) must be the convex hull of the payoﬀs. A trader purchasing shares at the current prices π ∈ Rn pays C(ϕ−1 (π) + r) − C(ϕ−1 (π)) for the bundle of contracts r ∈ Rn . Note that our dependence solely on π limits our model slightly, since in general the share space (domain of C) may contain more information than the current prices (cf. [1]). The bundle r is determined by an agent’s demand function d(C, π) which speciﬁes the bundle to buy given the price π and the cost function C. Our market dynamics are the following. The market maker posts the current price πt , and at each time t = 1 . . . T , a trader is chosen with demand function d drawn i.i.d. from some demand distribution D. Intuitively, these demands are parameterized by latent variables such as the agent’s belief p ∈ ∆Ω and total wealth W . The price is then updated to πt+1 = ϕ(ϕ−1 (πt ) + d(C, πt )). (1) After update T , the outcome is revealed and payout φ(ω)i is given for each contract i ∈ {1, . . . , n}. 3 Stationarity and equilibrium We ﬁrst would like to relate our stochastic model (1) to the standard notion of market equilibrium from the Economics literature, which we call the Walrasian equilibrium to avoid confusion. Here prices are ﬁxed, and the equilibrium price is one that clears the market, meaning that the sum of the demands r is 0 ∈ Rn . In fact, we will show that the stationary point of our process approaches the Walrasian equilibrium point as the liquidity of the market approaches inﬁnity. 2 First, we must add a liquidity parameter to our market. Following the LMSR (the cost function C(s) = b ln i esi /b ), we deﬁne Cb (s) := b C(s/b). (2) This transformation of a convex function is called a perspective function and is known to preserve convexity [4]. Observe that ϕb (s) := Cb (s) = C(s/b) = ϕ(s/b), meaning that the price under Cb at s is the same as the price under C at s/b. As with the LMSR, we call b the liquidity parameter ; this terminology is justiﬁed by noting that one deﬁnition of liquidity, 1/λmax 2 Cb (s) = b/λmax 2 C(s/b) (cf. [1]). In the following, we will consider the limit as b → ∞. Second, in order to connect to the Walrasian equilibrium, we need a notion of a ﬁxed-price demand function: if a trader has demand d(C, ·) given C, what would the same trader’s demand be under a market where prices are ﬁxed and do not “change” during a trade? For the sake of generality, we restrict our allowable demand functions to the ones for which the limit d(F, π) := lim d(Cb , π) (3) b→∞ exists; this demand d(F, ·) will be the corresponding ﬁxed-price demand for d. We now deﬁne the Walrasion equilibrium point π ∗ , which is simply the price at which the market clears when traders have demands distributed by D. Formally, this is the following condition:1 d(F, π ∗ ) dD(d) = 0 (4) D Note that 0 ∈ Rn ; the demand for each contract should be balanced. s The stationary point of our stochastic process, on the other hand, is the price πb for which the expected price ﬂuctuation is 0. Formally, we have s s E [∆(πb , d(Cb , πb ))] = 0, (5) d∼D where ∆(π, d) := ϕ(ϕ−1 (π) + d) − π is the price ﬂuctuation. We now consider the limit of our stochastic process as the market liquidity approaches ∞. Theorem 1. Let C be a strictly convex and α-smooth2 cost function, and assume that ∂ ∂b d(Cb , π) = o(1/b) uniformly in π and all d ∈ D. If furthermore the limit (3) is uniform s in π and d, then limb→∞ πb = π ∗ . s Proof. Note that by the stationarity condition (5) we may deﬁne π ∗ and πb to be the roots of the following “excess demand” functions, respectively: Z(π) := s Zb (π) := b E [∆(π, d(Cb , π))], d(F, π) dD(d), d∼D D where we scale the latter by b so that −1 Let s = ϕ s Zb does not limit to the zero function. (π) be the current share vector. Then we have lim b∆(π, d(Cb , π)) = lim b ϕ ϕ−1 (π) + d(Cb , π)/b − π b→∞ b→∞ ϕ s + a d(C1/a , π) − π a ∂ = lim ϕ s + a d(C1/a , π) d(C1/a , π) + a ∂a d(C1/a , π) = lim a→0 a→0 = lim ϕ s+ = lim 2 b→∞ b→∞ 1 b d(Cb , π) C(s) d(Cb , π) = d(Cb , π) + 2 2 1 ∂ b ∂b d(Cb , π)(−b ) C(s) d(F, π), 1 Here and throughout we ignore technical issues of uniqueness. One may simply restrict to the class of demands for which uniqueness is satisﬁed. 2 C is α-smooth if λmax 2 C ≤ α 3 where we apply L’Hopital’s rule for the third equality. Crucially, the above limit is uniform with respect to both d ∈ D and π ∈ Π; uniformity in d is by assumption, and uniformity in π follows from α-smoothness of C, since C is dominated by a quadratic. Since the limit is uniform with respect to D, we now have s lim Zb (π) = lim b E [∆(π, d(Cb , π))] = E b→∞ b→∞ = 2 d∼D d∼D C(s) E [d(F, π)] = 2 lim b∆(π, d(Cb , π)) b→∞ C(s) Z(π). d∼D s As 2 C(s) is positive deﬁnite by assumption on C, we can conclude that limb→∞ Zb and Z share the same zeroes. Since Z has compact domain and is assumed continuous with a unique zero π ∗ , for each ∈ (0, max ) there must be some δ > 0 s.t. |Z(π)| > for all π s.t. π − π ∗ > δ (otherwise there would be a sequence of πn → π s.t. f (π ) = 0 but π = π ∗ ). s By uniform convergence there must be a B > 0 s.t. for all b > B we have Zb − Z ∞ < /2. s s In particular, for π s.t. π − π ∗ > δ, |Zb (π)| > /2. Thus, the corresponding zeros πb must ∗ s ∗ 3 be within δ of π . Hence limb→∞ πb = π . 3.1 Utility-based demands Maximum Expected Utility (MEU) demand functions are a particular kind of demand function derived by assuming a trader has some belief p ∈ ∆n over the outcomes in Ω, some wealth W ≥ 0, and a monotonically increasing utility function of money u : R → R. If such a trader buys a bundle r of contracts from a market maker with cost function C and price π, her wealth after ω occurs is Υω (C, W, π, r) := W +φ(ω)·r−[C(ϕ−1 (π)+r)−C(ϕ−1 (π))]. We ensure traders do not go into debt by requiring that traders only make demands such that this ﬁnal wealth is nonnegative: ∀ω Υω (C, π, r) ≥ 0. The set of debt-free bundles for wealth W and market C at price π is denoted S(C, W, π) := {r ∈ Rn : minω Υω (C, W, π, r) ≥ 0}. A continuous MEU demand function du (C, π) is then just the demand that maximizes a W,p trader’s expected utility subject to the debt-free constraint. That is, du (C, π) := argmax W,p E [u (Υω (C, W, π, r))] . (6) r∈S(C,W,π) ω∼p We also deﬁne a ﬁxed-price MEU demand function du (F, π) similarly, where W,p Υω (F, W, π, r) := W + φ(ω) · r − π · r and S(F, W, π) := {r ∈ Rn : minω Υω (F, W, π, r) ≥ 0} are the ﬁxed price analogues to the continuously priced versions above. Using the notation bS := {b r | r ∈ S}, the following relationships between the continuous and ﬁxed price versions of Υ, SW , and the expected utility are a consequence of the convexity of C. Their main purpose is to highlight the relationship between wealth and liquidity in MEU demands. In particular, they show that scaling up of liquidity is equivalent to a scaling down of wealth and that the continuously priced constraints and wealth functions monotonically approach the ﬁxed priced versions. Lemma 1. For any strictly convex cost function C, wealth W > 0, price π, demand r, and liquidity parameter b > 0 the following properties hold: 1. Υω (Cb , W, π, r) = b Υω (C, W/b, π, r/b); 2. S(Cb , W, π) = b S(C, W/b, π); 3. S(C, W, π) is convex for all C; 4. S(C, W, π) ⊆ S(Cb , W, π) ⊆ S(F, W, π) for all b ≥ 1. 5. For monotone utilities u, Eω∼p [u (Υω (F, W, π, r))] ≥ Eω∼p [u (Υω (C, W, π, r))]. Proof. Property (1) follows from a simple computation: Υω (Cb , W, π, r) = W + φ(ω) · r − b C(ϕ−1 (π) + r/b) + b C(ϕ−1 (π)) = b W/b + φ(ω) · (r/b) − C(ϕ−1 (π) + r/b) + C(ϕ−1 (π)) , which equals b Υω (C, W/b, π, r/b) by deﬁnition. We now can see property (2) as well: S(Cb , W, π) = {r : min b Υω (C, W/b, π, r/b) ≥ 0} = {b r : min Υω (C, W/b, π, r) ≥ 0}. ω 3 ω We thank Avraham Ruderman for a helpful discussion regarding this proof. 4 For (3), deﬁne fC,s,ω (r) = C(s + r) − C(s) − φ(ω) · r, which is the ex-post cost of purchasing bundle r. As C is convex, and fC,s,ω is a shifted and translated version of C plus a linear term, fC,s,ω is convex also. The constraint Υω (C, W, π, r) ≥ 0 then translates to fC,s,ω (r) ≤ W , and thus the set of r which satisfy the constraint is convex as a sublevel set of a convex function. Now S(C, W, π) is convex as an intersection of convex sets, proving (3). For (4) suppose r satisﬁes fC,s,ω (r) ≤ W . Note that fC,s,ω (0) = 0 always. Then by convexity we have for f := fC,s,ω we have f (r/b) = f 1 r + b−1 0 ≤ 1 f (r) + b−1 0 ≤ W/b, b b b b which implies S(C, W, π) ⊆ S(Cb , W, π) when considering (3). To complete (4) note that fC,s,ω dominates fF,s : r → (ϕ(s) − φ(ω)) · r by convexity of C: C(s + r) − C(s) ≥ C(s) · r. Finally, proof of (5) is obtained by noting that the convexity of C means that C(ϕ−1 (π) + r) − C(ϕ−1 (π)) ≥ C(ϕ−1 (π)) · r = π · r and exploting the monotonicty of u. Lemma 1 shows us that MEU demands have a lot of structure, and in particular, properties (4) and (5) suggest that they may satisfy the conditions of Theorem 1; we leave this as an open question for future work. Another interesting aspect of Lemma 1 is the relationship between markets with cost function Cb and wealths W and markets with cost function C and wealths W/b – indeed, properties (1) and (2) suggest that the liquidity limit should in some sense be equivalent to a wealth limit, in that increasing liquidity by a factor b should yield similar dynamics to decreasing the wealths by b. This would relate our model to that of [8], where the authors essentially show a wealth-limit version of Theorem 1 for a binary-outcome market where traders have linear utilities (a special case of (6)). We leave this precise connection for future work. 4 Market making as mirror descent We now explore the surprising relationship between our stochastic price update and standard stochastic optimization techniques. In particular, we will relate our model to a stochastic mirror descent of the form xt+1 = argmin{η x · F (xt ; ξ) + DR (x, xt )}, (7) x∈R where at each step ξ ∼ Ξ are i.i.d. and R is some strictly convex function. We will refer to an algorithm of the form (7) a stochastic mirror descent of f (x) := Eξ∼Ξ [F (x; ξ)]. Theorem 2. If for all d ∈ D we have some F (· ; d) : Rn → Rn such that d(R∗ , π) = − F (π; d), then the stochastic update of our model (1) is exactly a stochastic mirror descent of f (π) = Ed∼D [F (π; d)]. Proof. By standard arguments, the mirror descent update (7) can be rewritten as xt+1 = R∗ ( R(xt ) − F (xt ; ξ)), where R∗ is the conjugate dual of R. Take R = C ∗ , and let ξ = d ∼ D. By assumption, we have F (x; d) = −d(R∗ , x) = −d(C, x) for all d. As R∗ = C = ϕ, we have ϕ−1 = ( R∗ )−1 = R by duality, and thus our update becomes xt+1 = ϕ ϕ−1 (xt ) + d(C, xt ) , which exactly matches the stochastic update of our model (1). As an example, consider Kelly betters, which correspond to ﬁxed-price demands d(C, π) := dlog (F, π) with utility u(x) = log x as deﬁned in (3). A simple calculation shows that our W,p update becomes W p−π πt+1 = ϕ ϕ−1 (πt ) + , (8) π 1−π where W and p are drawn (independently) from P and W. Corollary 1. The stochastic update for ﬁxed-price Kelly betters (8) is exactly a stochastic mirror descent of f (π) = W · KL(p, π), where p and W are the means of P and W, respectively. 5 Proof. We take F (x; dlog ) = W · (KL(p, x) + H(p)). Then W,p F (x; dlog ) = W W,p −p p − 1 + x 1−x =− W p−x = −dlog (F, x). W,p x 1−x Hence, by Theorem 2 our update is a stochastic mirror descent of: f (x) := E[F (x; dlog )] = E[W p log x + W (1 − p) log(1 − x)] = W · (KL(p, x) + H(p)) , W,p which of course is equivalent to W · KL(p, x) as the entropy term does not depend on x. Note that while this last result is quite compelling, we have mixed ﬁxed-price demands with a continuous-price market model – see Section 3.1. One could interpret this combination as a model in which the market maker can only adjust the prices after a trade, according to a ﬁxed convex cost function C. This of course diﬀers from the standard model, which adjusts the price continuously during a trade. 4.1 Leveraging existing learning results Theorem 2 not only identiﬁes a fascinating connection between machine learning and our stochastic prediction market model, but it also allows us to use powerful existing techniques to make broad conclusions about the behavior of our model. Consider the following result: ≤ G2 for all p, π, and R is σ-strongly convex, then log 1 δ . Price Avg price Avg belief 0.70 In our context, Proposition 1 says that the average of the prices will be a very good estimate of the minimizer of f , which as suggested by happens to be the underlying mean belief p of the traders! Moreover, as the Kelly demands are linear in both p and W , it is easy to see from Theorem 1 that p is also the stationary point and the Walrasian equilibrium point (the latter was also shown by [11]). On the other hand, as we demonstrate next, it is not hard to come up with an example where the instantaneous price πt is quite far from the equilibrium at any given time period. 1+4 0.65 π D2 G2 η + ηT 2σ 0.60 f (π T ) ≤ min f (π) + Price of contract 1 2 0.55 F (π; p) 0.50 Proposition 1 ([6]). If with probability 1 − δ, 0 500 1000 1500 2000 Trade number Before moving to our empirical work, we make one Figure 1: Price movement for Kelly ﬁnal point. The above relationship between our betters with binomial(q = 0.6, n = 6, stochastic market model and mirror descent sheds α = 0.5) beliefs in the LMSR market light on an important question: how might an auto- with liquidity b = 10. mated market maker adjust the liquidity so that the market actually converges to the mean of the traders’ beliefs? The learning parameter η can be thought of as the inverse of the liquidity, and as such, Proposition 1 suggests that √ increasing the liquidity as t may cause the mean price to converge to the mean belief (assuming a ﬁxed underlying belief distribution). 5 Empirical work Example: biased coin Consider a classic Bayesian setting where a coin has unknown bias Pr[heads] = q, and traders have a prior β(α, α) over q (i.e., traders are α-conﬁdent that the coin is fair). Now suppose each trader independently observes n ﬂips from the coin, and updates her belief; upon seeing k heads, a trader would have posterior β(α + k, α + n − k). When presented with a prediction market with contracts for a single toss of the coin, where and contract 0 pays $1 for tails and contract 1 pays $1 for heads, a trader would purchase 6 0 20 40 60 Trades 80 100 b = 10 Instant Averaged 0.06 Loss 0.04 0.02 0.00 0.02 0.04 Loss 0.06 0.08 Instant Averaged 0.00 0.00 0.02 0.04 Loss 0.06 0.08 Instant Averaged Square loss of price to mean belief for State 9 b=3 0.08 b=1 0.10 Square loss of price to mean belief for State 9 0.10 0.10 Square loss of price to mean belief for State 9 0 20 40 60 Trades 80 100 0 20 40 60 80 100 Trades Figure 2: Mean square loss of average and instantaneous prices relative to the mean belief of 0.26 over 20 simulations for State 9 for b = 1 (left), b = 3 (middle), and b = 10 (right). Bars show standard deviation. contracts as if according to the mean of their posterior. Hence, the belief distribution P of the market assigns weight P(p) = n q k (1 − q)n−k to belief p = (α + k)/(2α + n), yielding k a biased mean belief of (α + nq)/(2α + n). We show a typical simulation of this market in Figure 1, where traders behave as Kelly betters in the ﬁxed-price LMSR. Clearly, after almost every trade, the market price is quite far from the equilibrium/stationary point, and hence the classical supply and demand analysis of this market yields a poor description of the actual behavior, and in particular, of the predictive quality of the price at any given time. However, the mean price is consistently close to the mean belief of the traders, which in turn is quite close to the true parameter q. Election Survey Data We now compare the quality of the running average price versus the instantaneous price as a predictor of the mean belief of a market. We do so by simulating a market maker interacting with traders with unit wealth, log utility, and beliefs drawn from a ﬁxed distribution. The belief distributions are derived from the Princeton election survey data[10]. For each of the 50 US states, participants in the survey were asked to estimate the probability that one of two possible candidates were going to win that state.4 We use these 50 sets of estimates as 50 diﬀerent empirical distributions from which to draw trader beliefs. A simulation is conﬁgured by choosing one of the 50 empirical belief distributions S, a market liquidity parameter b to deﬁne the LMSR cost function C(s) = b ln i esi /b , and an initial market position vector of (0, 0) – that is, no contracts for either outcome. A conﬁgured simulation is run for T trades. At each trade, a belief p is drawn from S uniformly and with replacement. This belief is used to determine the demand of the trader relative to the current market pricing. The trader purchase a bundle of contracts according to its demand and the market moves its position and price accordingly. The complete price path πt for t t = 1, . . . , T of the market is recorded as well as a running average price πt := 1 i=1 πt for ¯ t t = 1 . . . , T . For each of the 50 empirical belief distributions we conﬁgured 9 markets with b ∈ {1, 2, 3, 5, 10, 15, 20, 30, 50} and ran 20 independent simulations of T = 100 trades. We present a portion of the results for the empirical distributions for states 9 and 11. States 9 and 11 have, respectively, sample sizes of 2,717 and 2,709; means 0.26 and 0.9; and variances 0.04 and 0.02. These are chosen as being representative of the rest of the simulation results: State 9 with mean oﬀ-center and a spread of beliefs (high uncertainty) and State 11 with highly concentrated beliefs around a single outcome (low uncertainty). The results are summarised in Figures 2, 3, and 4. The ﬁrst show the square loss of the average and instaneous prices relative to the mean belief for high uncertainty State 9 for b = 1, 3, 10. Clearly, the average price is a much more reliable estimator of the mean belief for low liquidity (b = 1) and is only outperformed by the instaneous price for higher liquidity (b = 10), but then only early in trading. Similar plots for State 11 are shown in Figure 3 where the advantage of using the average price is signiﬁcantly diminished. 4 The original dataset contains conjunctions of wins as well as conditional statements but we only use the single variable results of the survey. 7 0 20 40 60 80 100 b = 10 Instant Averaged 0.06 Loss 0.04 0.02 0.00 0.02 0.04 Loss 0.06 0.08 Instant Averaged 0.00 0.00 0.02 0.04 Loss 0.06 0.08 Instant Averaged Square loss of price to mean belief for State 11 b=3 0.08 b=1 0.10 Square loss of price to mean belief for State 11 0.10 0.10 Square loss of price to mean belief for State 11 0 20 40 Trades 60 80 100 0 20 40 Trades 60 80 100 Trades Figure 3: Mean square loss of average and instantaneous prices relative to the mean belief of 0.9 over 20 simulations for State 11 for b = 1 (left), b = 3 (middle), and b = 10 (right). Bars show standard deviation. Figure 4 shows the improvement the average price has over the instantaneous price in square loss relative to the mean belief for all liquidity settings and highlights that average prices work better in low liquidity settings, consistent with the theory. Similar trends were observed for all the other States, depending on whether they had high uncertainty – in which case average price was a much better estimator – or low uncertainty – in which case instanteous price was better. Improvement of Average over Instant Prices for State 9 Improvement of Average over Instant Prices for State 11 0.06 0.02 0.00 0.04 Loss Di fference fference -0.04 0.00 -0.06 -0.02 -0.08 10 20 100 80 80 30 40 40 30 60 Tra des b 60 Tra des 10 20 100 20 b Loss Di -0.02 0.02 40 40 20 50 50 Figure 4: An overview of the results for States 9 (left) and 11 (right). For each trade and choice of b, the vertical value shows the improvement of the average price over the instantaneous price as measure by square loss relative to the mean. 6 Conclusion and future work As noted in Section 3.1, there are several open questions with regard to maximum expected utility demands and Theorem 1, as well as the relationship between trader wealth and market liquidity. It would also be interesting to have a application of Theorem 2 to a continuousprice model, which yields a natural minimization as in Corollary 1. The equivalence to mirror decent stablished in Theorem 2 may also lead to a better understanding of the optimal manner in which a automated prediction market ought to increase liquidity so as to maximise eﬃciency. Acknowledgments This work was supported by the Australian Research Council (ARC). NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the ARC through the ICT Centre of Excellence program. The ﬁrst author was partially supported by NSF grant CC-0964033 and by a Google University Research Award. 8 References [1] J. Abernethy, Y. Chen, and J.W. Vaughan. An optimization-based framework for automated market-making. In Proceedings of the 11th ACM conference on Electronic Commerce (EC’11), 2011. [2] A. Barbu and N. Lay. An introduction to artiﬁcial prediction markets for classiﬁcation. Arxiv preprint arXiv:1102.1465, 2011. [3] A. Beygelzimer, J. Langford, and D. Pennock. Learning Performance of Prediction Markets with Kelly Bettors. 2012. [4] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, 2004. [5] Y. Chen and J.W. Vaughan. A new understanding of prediction markets via no-regret learning, pages 189–198. 2010. [6] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. COLT, 2010. [7] C.F. Manski. Interpreting the predictions of prediction markets. Technical report, National Bureau of Economic Research, 2004. [8] A. Othman and T. Sandholm. When do markets with simple agents fail? In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1, pages 865–872. International Foundation for Autonomous Agents and Multiagent Systems, 2010. [9] A. Storkey. Machine learning markets. AISTATS, 2012. [10] G. Wang, S.R. Kulkarni, H.V. Poor, and D.N. Osherson. Aggregating large sets of probabilistic forecasts by weighted coherent adjustment. Decision Analysis, 8(2):128, 2011. [11] J. Wolfers and E. Zitzewitz. Interpreting prediction market prices as probabilities. Technical report, National Bureau of Economic Research, 2006. 9</p><p>6 0.68194854 <a title="325-lsi-6" href="./nips-2012-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">76 nips-2012-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>7 0.67170668 <a title="325-lsi-7" href="./nips-2012-Query_Complexity_of_Derivative-Free_Optimization.html">285 nips-2012-Query Complexity of Derivative-Free Optimization</a></p>
<p>8 0.6227082 <a title="325-lsi-8" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>9 0.60365129 <a title="325-lsi-9" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>10 0.5819602 <a title="325-lsi-10" href="./nips-2012-Coding_efficiency_and_detectability_of_rate_fluctuations_with_non-Poisson_neuronal_firing.html">73 nips-2012-Coding efficiency and detectability of rate fluctuations with non-Poisson neuronal firing</a></p>
<p>11 0.57536733 <a title="325-lsi-11" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>12 0.57357043 <a title="325-lsi-12" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>13 0.57320231 <a title="325-lsi-13" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>14 0.55987686 <a title="325-lsi-14" href="./nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">206 nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>15 0.55949515 <a title="325-lsi-15" href="./nips-2012-Mixability_in_Statistical_Learning.html">217 nips-2012-Mixability in Statistical Learning</a></p>
<p>16 0.55075175 <a title="325-lsi-16" href="./nips-2012-Convergence_Rate_Analysis_of_MAP_Coordinate_Minimization_Algorithms.html">84 nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</a></p>
<p>17 0.53354424 <a title="325-lsi-17" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>18 0.52626145 <a title="325-lsi-18" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>19 0.52592409 <a title="325-lsi-19" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>20 0.51704746 <a title="325-lsi-20" href="./nips-2012-Convex_Multi-view_Subspace_Learning.html">86 nips-2012-Convex Multi-view Subspace Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.092), (47, 0.128), (64, 0.028), (67, 0.05), (70, 0.148), (85, 0.09), (87, 0.151), (94, 0.148), (99, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88752091 <a title="325-lda-1" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>Author: Arnak Dalalyan, Yin Chen</p><p>Abstract: In this paper, we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes the aforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish ﬁnite sample risk bounds and carry out an experimental evaluation on both synthetic and real data. 1</p><p>same-paper 2 0.88565987 <a title="325-lda-2" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>Author: Alekh Agarwal, Sahand Negahban, Martin J. Wainwright</p><p>Abstract: We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding a O(d/T ) convergence rate for strongly convex objectives in d dimensions and O( s(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm is based on successively solving a series of ℓ1 -regularized optimization problems using Nesterov’s dual averaging algorithm. We establish that the error of our solution after T iterations is at most O(s(log d)/T ), with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also conﬁrmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</p><p>3 0.87918639 <a title="325-lda-3" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Anima Anandkumar, Yi-kai Liu, Daniel J. Hsu, Dean P. Foster, Sham M. Kakade</p><p>Abstract: Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efﬁcient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k × k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space. 1</p><p>4 0.87383002 <a title="325-lda-4" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>5 0.87361956 <a title="325-lda-5" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>Author: Aaron Wilson, Alan Fern, Prasad Tadepalli</p><p>Abstract: We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent’s goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efﬁcient than random selection. 1</p><p>6 0.87321311 <a title="325-lda-6" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>7 0.87289637 <a title="325-lda-7" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>8 0.87035519 <a title="325-lda-8" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>9 0.86982274 <a title="325-lda-9" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>10 0.86982191 <a title="325-lda-10" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>11 0.86881006 <a title="325-lda-11" href="./nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>12 0.86872113 <a title="325-lda-12" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>13 0.86810225 <a title="325-lda-13" href="./nips-2012-The_Perturbed_Variation.html">338 nips-2012-The Perturbed Variation</a></p>
<p>14 0.86590934 <a title="325-lda-14" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>15 0.86570394 <a title="325-lda-15" href="./nips-2012-Putting_Bayes_to_sleep.html">283 nips-2012-Putting Bayes to sleep</a></p>
<p>16 0.86546254 <a title="325-lda-16" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>17 0.86283404 <a title="325-lda-17" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>18 0.86252934 <a title="325-lda-18" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>19 0.8618601 <a title="325-lda-19" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>20 0.86172169 <a title="325-lda-20" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
