<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-250" href="#">nips2012-250</a> knowledge-graph by maker-knowledge-mining</p><h1>250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</h1>
<br/><p>Source: <a title="nips-2012-250-pdf" href="http://papers.nips.cc/paper/4599-on-line-reinforcement-learning-using-incremental-kernel-based-stochastic-factorization.pdf">pdf</a></p><p>Author: Doina Precup, Joelle Pineau, Andre S. Barreto</p><p>Abstract: Kernel-based stochastic factorization (KBSF) is an algorithm for solving reinforcement learning tasks with continuous state spaces which builds a Markov decision process (MDP) based on a set of sample transitions. What sets KBSF apart from other kernel-based approaches is the fact that the size of its MDP is independent of the number of transitions, which makes it possible to control the trade-off between the quality of the resulting approximation and the associated computational cost. However, KBSF’s memory usage grows linearly with the number of transitions, precluding its application in scenarios where a large amount of data must be processed. In this paper we show that it is possible to construct KBSF’s MDP in a fully incremental way, thus freeing the space complexity of this algorithm from its dependence on the number of sample transitions. The incremental version of KBSF is able to process an arbitrary amount of data, which results in a model-based reinforcement learning algorithm that can be used to solve continuous MDPs in both off-line and on-line regimes. We present theoretical results showing that KBSF can approximate the value function that would be computed by conventional kernel-based learning with arbitrary precision. We empirically demonstrate the effectiveness of the proposed algorithm in the challenging threepole balancing task, in which the ability to process a large number of transitions is crucial for success. 1</p><p>Reference: <a title="nips-2012-250-reference" href="../nips2012_reference/nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sa', 0.568), ('kbsf', 0.534), ('ikbsf', 0.335), ('kbrl', 0.248), ('transit', 0.148), ('ra', 0.138), ('sta', 0.131), ('mdp', 0.124), ('dka', 0.112), ('tm', 0.09), ('pa', 0.09), ('na', 0.087), ('reinforc', 0.086), ('ya', 0.081), ('wi', 0.079), ('tv', 0.07), ('wa', 0.067), ('si', 0.064), ('cti', 0.062), ('hti', 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="250-tfidf-1" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>2 0.11610065 <a title="250-tfidf-2" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>3 0.095074937 <a title="250-tfidf-3" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>4 0.092689194 <a title="250-tfidf-4" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>5 0.086590193 <a title="250-tfidf-5" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>6 0.084739223 <a title="250-tfidf-6" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>7 0.075205475 <a title="250-tfidf-7" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>8 0.070378803 <a title="250-tfidf-8" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>9 0.06623362 <a title="250-tfidf-9" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>10 0.064870022 <a title="250-tfidf-10" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>11 0.055496857 <a title="250-tfidf-11" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>12 0.053609397 <a title="250-tfidf-12" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>13 0.051940892 <a title="250-tfidf-13" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>14 0.050217722 <a title="250-tfidf-14" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>15 0.050127715 <a title="250-tfidf-15" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>16 0.048102632 <a title="250-tfidf-16" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>17 0.047937617 <a title="250-tfidf-17" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>18 0.045071226 <a title="250-tfidf-18" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>19 0.04253922 <a title="250-tfidf-19" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>20 0.040218715 <a title="250-tfidf-20" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (1, 0.117), (2, -0.027), (3, 0.014), (4, 0.025), (5, -0.009), (6, -0.008), (7, -0.009), (8, 0.005), (9, 0.001), (10, -0.003), (11, -0.017), (12, 0.012), (13, 0.001), (14, -0.012), (15, -0.008), (16, -0.01), (17, -0.004), (18, -0.012), (19, -0.009), (20, 0.025), (21, -0.017), (22, -0.006), (23, -0.006), (24, -0.011), (25, -0.002), (26, -0.012), (27, -0.037), (28, 0.011), (29, 0.049), (30, -0.012), (31, 0.044), (32, 0.028), (33, 0.033), (34, -0.013), (35, -0.057), (36, 0.007), (37, 0.04), (38, -0.051), (39, -0.026), (40, -0.089), (41, 0.032), (42, -0.06), (43, -0.043), (44, 0.075), (45, 0.03), (46, 0.076), (47, 0.02), (48, 0.041), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85846549 <a title="250-lsi-1" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>2 0.73178971 <a title="250-lsi-2" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>3 0.7112872 <a title="250-lsi-3" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>4 0.69220793 <a title="250-lsi-4" href="./nips-2012-Approximating_Equilibria_in_Sequential_Auctions_with_Incomplete_Information_and_Multi-Unit_Demand.html">45 nips-2012-Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand</a></p>
<p>5 0.66228318 <a title="250-lsi-5" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>6 0.63241619 <a title="250-lsi-6" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>7 0.62887639 <a title="250-lsi-7" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>8 0.6191709 <a title="250-lsi-8" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>9 0.59599096 <a title="250-lsi-9" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>10 0.5742287 <a title="250-lsi-10" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>11 0.57399666 <a title="250-lsi-11" href="./nips-2012-Rational_inference_of_relative_preferences.html">288 nips-2012-Rational inference of relative preferences</a></p>
<p>12 0.55146217 <a title="250-lsi-12" href="./nips-2012-Bayesian_Hierarchical_Reinforcement_Learning.html">51 nips-2012-Bayesian Hierarchical Reinforcement Learning</a></p>
<p>13 0.54192895 <a title="250-lsi-13" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>14 0.53737181 <a title="250-lsi-14" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>15 0.51104385 <a title="250-lsi-15" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>16 0.48483267 <a title="250-lsi-16" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>17 0.48451877 <a title="250-lsi-17" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>18 0.47453293 <a title="250-lsi-18" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>19 0.46814904 <a title="250-lsi-19" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>20 0.45487976 <a title="250-lsi-20" href="./nips-2012-Value_Pursuit_Iteration.html">358 nips-2012-Value Pursuit Iteration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.052), (47, 0.098), (49, 0.018), (64, 0.024), (67, 0.024), (70, 0.12), (85, 0.036), (91, 0.358), (94, 0.079), (99, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.64702499 <a title="250-lda-1" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>2 0.53443879 <a title="250-lda-2" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>3 0.46687061 <a title="250-lda-3" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>4 0.46683741 <a title="250-lda-4" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>5 0.46322131 <a title="250-lda-5" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>6 0.46020016 <a title="250-lda-6" href="./nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">132 nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<p>7 0.45976561 <a title="250-lda-7" href="./nips-2012-Putting_Bayes_to_sleep.html">283 nips-2012-Putting Bayes to sleep</a></p>
<p>8 0.45937708 <a title="250-lda-8" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>9 0.45810503 <a title="250-lda-9" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>10 0.45748916 <a title="250-lda-10" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>11 0.45586812 <a title="250-lda-11" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>12 0.45579895 <a title="250-lda-12" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>13 0.45513114 <a title="250-lda-13" href="./nips-2012-Matrix_reconstruction_with_the_local_max_norm.html">208 nips-2012-Matrix reconstruction with the local max norm</a></p>
<p>14 0.45470804 <a title="250-lda-14" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>15 0.45271823 <a title="250-lda-15" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>16 0.45213199 <a title="250-lda-16" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>17 0.45181459 <a title="250-lda-17" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>18 0.45168847 <a title="250-lda-18" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>19 0.45102599 <a title="250-lda-19" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>20 0.45077637 <a title="250-lda-20" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
