<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-52" href="#">nips2012-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</h1>
<br/><p>Source: <a title="nips-2012-52-pdf" href="http://papers.nips.cc/paper/4826-bayesian-nonparametric-modeling-of-suicide-attempts.pdf">pdf</a></p><p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>Reference: <a title="nips-2012-52-reference" href="../nips2012_reference/nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc. [sent-11, score-0.083]
</p><p>2 In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). [sent-15, score-0.786]
</p><p>3 Due to the nature of the data, we need to adapt the observation model for discrete random variables. [sent-16, score-0.075]
</p><p>4 The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. [sent-18, score-0.074]
</p><p>5 Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. [sent-19, score-0.711]
</p><p>6 , where suicide prevention is one of the top public health priorities [1]. [sent-22, score-0.755]
</p><p>7 The current strategies for suicide prevention have focused mainly on both the detection and treatment of mental disorders [13], and on the treatment of the suicidal behaviors themselves [4]. [sent-23, score-0.888]
</p><p>8 However, despite prevention efforts including improvements in the treatment of depression, the lifetime prevalence of suicide attempts in the U. [sent-24, score-0.855]
</p><p>9 This suggests that there is a need to improve understanding of the risk factors for suicide attempts beyond psychiatric disorders, particularly in non-clinical populations. [sent-27, score-0.715]
</p><p>10 According to the National Strategy for Suicide Prevention, an important ﬁrst step in a public health approach to suicide prevention is to identify those at increased risk for suicide attempts [1]. [sent-28, score-1.47]
</p><p>11 Suicide attempts are, by far, the best predictor of completed suicide [12] and are also associated with major morbidity themselves [11]. [sent-29, score-0.656]
</p><p>12 The estimation of suicide attempt risk is a challenging and complex task, with multiple risk factors linked to increased risk. [sent-30, score-0.78]
</p><p>13 In the absence of reliable tools for identifying those at risk for suicide attempts, be they clinical or laboratory tests, risk detection still relays mainly on clinical variables. [sent-31, score-0.717]
</p><p>14 The adequacy of the current predictive models and screening methods has been 1  questioned [12], and it has been suggested that the methods currently used for research on suicide risk factors and prediction models need revamping [9]. [sent-32, score-0.658]
</p><p>15 Databases that model the behavior of human populations present typically many related questions and analyzing each one of them individually, or a small group of them, do not lead to conclusive results. [sent-33, score-0.14]
</p><p>16 population with nearly 3,000 questions regarding, among others, their way of life, their medical conditions, depression and other mental disorders. [sent-36, score-0.288]
</p><p>17 It contains yes-or-no questions, and some multiple-choice and questions with ordinal answers. [sent-37, score-0.14]
</p><p>18 In this paper, we propose to model the subjects in this database using a nonparametric latent model that allows us to seek hidden causes and compact in a few features the immense redundant information. [sent-38, score-0.283]
</p><p>19 Our starting point is the Indian Buffet Process (IBP) [5], because it allows us to infer which latent features inﬂuence the observations and how many features there are. [sent-39, score-0.229]
</p><p>20 We need to adapt the observation model for discrete random variables, as the discrete nature of the database does not allow us to use the standard Gaussian observation model. [sent-40, score-0.193]
</p><p>21 Furthermore, the multinomial-logit model, besides its versatility, allows the implementation of an efﬁcient Gibbs sampler where the Laplace approximation [10] is used to integrate out the weighting factors, which can be efﬁciently computed using the Matrix Inversion Lemma. [sent-42, score-0.101]
</p><p>22 The IBP model combined with discrete observations has already been tackled in several related works. [sent-43, score-0.074]
</p><p>23 They apply the ICD to focused topic modeling, where the instances are documents and the observations are words from a ﬁnite vocabulary, and focus on decoupling the prevalence of a topic in a document and its prevalence in all documents. [sent-45, score-0.176]
</p><p>24 Despite the discrete nature of the observations under this model, these assumptions are not appropriate for categorical observations such as the set of possible responses to the questions in the NESARC database. [sent-46, score-0.254]
</p><p>25 In this model, each (discrete) component in the observation vector of an instance depends only on one of the active latent features of that object, randomly drawn from a multinomial distribution. [sent-48, score-0.208]
</p><p>26 Our model is more ﬂexible in the sense that it allows different probability distributions for every component in the observation vector, which is accomplished by weighting differently the latent variables. [sent-50, score-0.149]
</p><p>27 2  The Indian Buffet Process  In latent feature modeling, each object can be represented by a vector of latent features, and the observations are generated from a distribution determined by those latent feature values. [sent-51, score-0.379]
</p><p>28 Typically, we have access to the set of observations and the main goal of these models is to ﬁnd out the latent variables that represent the data. [sent-52, score-0.123]
</p><p>29 The most common nonparametric tool for latent feature modeling is the Indian Buffet Process (IBP). [sent-53, score-0.128]
</p><p>30 The nth row of Z, denoted by zn· , represents the vector of latent features of the nth data point, and every entry nk is denoted by znk . [sent-61, score-0.449]
</p><p>31 Note that each element znk ∈ {0, 1} indicates whether the k th feature contributes to the nth data point. [sent-62, score-0.275]
</p><p>32 Given a binary latent feature matrix Z, we assume that the N × D observation matrix X, where the nth row contains a D-dimensional observation vector xn· , is distributed according to a probability distribution p(X|Z). [sent-63, score-0.312]
</p><p>33 Additionally, x·d stands for the dth column of X, and each element of the 2  matrix is denoted by xnd . [sent-64, score-0.206]
</p><p>34 MCMC (Markov Chain Monte Carlo) methods have been broadly applied to infer the latent structure Z from a given observation matrix X (see, e. [sent-66, score-0.155]
</p><p>35 In particular, we focus on the use of Gibbs sampling for posterior inference over the latent variables. [sent-69, score-0.116]
</p><p>36 The algorithm iteratively samples the value of each element znk given the remaining variables, i. [sent-70, score-0.19]
</p><p>37 , it samples from p(znk = 1|X, Z¬nk ) ∝ p(X|Z)p(znk = 1|Z¬nk ), (1) where Z¬nk denotes all the entries of Z other than znk . [sent-72, score-0.19]
</p><p>38 The distribution p(znk = 1|Z¬nk ) can be readily derived from the exchangeable IBP and can be written as p(znk = 1|Z¬nk ) = m−n,k /N, where m−n,k is the number of data points with feature k, not including n, i. [sent-73, score-0.08]
</p><p>39 , Rd }, where this ﬁnite set contains the indexes to all the possible values of xnd . [sent-81, score-0.146]
</p><p>40 We introduce matrices Bd of size K × R to model the probability distribution over X, such that Bd links the hidden latent variables with the dth column of the observation matrix X. [sent-83, score-0.226]
</p><p>41 We assume r that the probability of xnd taking value r (r = 1, . [sent-84, score-0.146]
</p><p>42 , r πnd = p(xnd = r|zn· , Bd ) =  exp (zn· bd ) ·r  ,  R  exp (zn· bd ·r  (2)  )  r =1  where bd denotes the rth column of Bd . [sent-89, score-1.497]
</p><p>43 Note that the matrices Bd are used to weight differently ·r the contribution of every latent feature for every component d, similarly as in the standard Gaussian observation model in [5]. [sent-90, score-0.169]
</p><p>44 We assume that the mixing vectors bd are Gaussian distributed with zero ·r 2 mean and covariance matrix Σb = σB I. [sent-91, score-0.53]
</p><p>45 We consider that elements xnd are independent given the latent feature matrix Z and the D matrices Bd . [sent-97, score-0.305]
</p><p>46 p(xnd |zn· , Bd ) =  (3)  n=1 d=1  Laplace approximation for inference  In Section 2, the (heuristic) Gibbs sampling algorithm for the posterior inference over the latent variables of the IBP has been reviewed and it is detailed in [5]. [sent-103, score-0.116]
</p><p>47 Recall that our model assumes independence among the observations given the hidden latent variables. [sent-108, score-0.165]
</p><p>48 By deﬁning (ρd )kr =  N r n=1 znk πnd ,  the gradient of ψ(Bd ) can be derived as ψ = Md − ρd −  1 d 2 B . [sent-124, score-0.19]
</p><p>49 The Hessian matrix can now be readily computed taking the derivatives of the gradient, yielding ψ=−  1 2 IRK + σB  log p(x·d |β d , Z) N  =−  1 diag(π nd ) − (π nd ) π nd ⊗ (zn· zn· ), 2 IRK − σB n=1  (7)  R 2 1 where π nd = πnd , πnd , . [sent-128, score-0.166]
</p><p>50 , πnd , and diag(π nd ) is a diagonal matrix with the values of the vector π nd as its diagonal elements. [sent-131, score-0.081]
</p><p>51 9 by replacing K by K+ , Z by the submatrix containing only the non-zero columns of Z, and Bd MAP by the submatrix containing the K+ corresponding rows. [sent-141, score-0.095]
</p><p>52 Since vn vn is a rank-one matrix, we can apply the Woodbury ·d identity [18] N times to invert the matrix − ψ, similar to the RLS (Recursive Least Squares) updates [7]. [sent-148, score-0.225]
</p><p>53 , N , we compute  (D(n) )−1 = D(n−1) − vn vn  −1  = (D(n−1) )−1 +  (D(n−1) )−1 vn vn (D(n−1) )−1 . [sent-152, score-0.388]
</p><p>54 1 − vn (D(n−1) )−1 vn  (12)  For the ﬁrst iteration, we deﬁne D(0) as the block-diagonal matrix D, whose inverse matrix involves computing the R matrix inversions of size K+ × K+ of the matrices in (11), which can be efﬁciently solved applying the Matrix Inversion Lemma. [sent-153, score-0.287]
</p><p>55 We also multiply each pixel independently with equiprobable binary noise, hence each white pixel in the composite image can be turned black 50% of the times, while black pixels always remain black. [sent-161, score-0.162]
</p><p>56 The Gibbs sampler has been initialized with K+ = 2, setting each znk = 1 2 with probability 1/2, and the hyperparameters have been set to α = 0. [sent-164, score-0.239]
</p><p>57 After 200 iterations, the Gibbs sampler returns four latent features. [sent-166, score-0.132]
</p><p>58 As expected, the black pixels are known to be black (almost zero probability of being white) and the white pixels have about a 50/50 chance of being black or white, due to the multiplicative noise. [sent-169, score-0.128]
</p><p>59 The Gibbs sampler has used as many as nine hidden features, but after iteration 60, the ﬁrst four features represent the base images and the others just lock on a noise pattern, which eventually fades away. [sent-170, score-0.144]
</p><p>60 2  National Epidemiologic Survey on Alcohol and Related Conditions (NESARC)  The NESARC was designed to determine the magnitude of alcohol use disorders and their associated disabilities. [sent-172, score-0.154]
</p><p>61 Two waves of interviews have been ﬁelded for this survey (ﬁrst wave in 2001-2002 and second wave in 2004-2005). [sent-173, score-0.151]
</p><p>62 (b) Probability of each pixel being white, when a single feature is active (ordered to match the images on the left), computed using Bd . [sent-180, score-0.106]
</p><p>63 (d) Probabilities of each pixel being white after 200 iterations of the Gibbs sampler inferred for the four data points on (c). [sent-183, score-0.129]
</p><p>64 (e) The number of latent features K+ and (f) the approximate log of p(X|Z) over the 200 iterations of the Gibbs sampler. [sent-185, score-0.136]
</p><p>65 The survey includes a question about having attempted suicide as well as other related questions such as ‘felt like wanted to die’ and ‘thought a lot about own death’. [sent-187, score-0.827]
</p><p>66 In the present paper, we use the IBP with discrete observations for a preliminary study in seeking the latent causes which lead to committing suicide. [sent-188, score-0.211]
</p><p>67 Most of the questions in the survey (over 2,500) are yes-or-no questions, which have four possible outcomes: ‘blank’ (B), ‘unknown’ (U), ‘yes’ (Y) and ‘no’ (N). [sent-189, score-0.175]
</p><p>68 If a question is left blank the question was not asked1 . [sent-190, score-0.13]
</p><p>69 If a question is said to be unknown either it was not answered or was unknown to the respondent. [sent-191, score-0.087]
</p><p>70 In our ongoing study, we want to ﬁnd a latent model that describes this database and can be used to infer patterns of behavior and, speciﬁcally, be able to predict suicide. [sent-192, score-0.126]
</p><p>71 In this paper, we build an unsupervised model with the 20 variables that present the highest mutual information with the suicide attempt question, which are shown in Table 1 together with their code in the questionnaire. [sent-193, score-0.662]
</p><p>72 We run the Gibbs sampler over 500 randomly chosen subjects out of the 13,670 that have answered afﬁrmatively to having had a period of low mood. [sent-194, score-0.142]
</p><p>73 We have initialized the sampler with an active feature, i. [sent-196, score-0.08]
</p><p>74 , K+ = 1, and have set znk = 1 randomly with probability 0. [sent-198, score-0.19]
</p><p>75 In Figure 2, we have plotted the posterior probability for each question when a single feature is active. [sent-201, score-0.107]
</p><p>76 In these plots, white means 0 and black 1, and each row sums up to one. [sent-202, score-0.076]
</p><p>77 Feature 1 is active for modeling the ‘blank’ and ‘no’ answers and, fundamentally, those who were not asked Questions 8 and 10. [sent-203, score-0.08]
</p><p>78 Feature 3 models blank answers for most of the questions and negative responses to 1, 2, 5, 8 and 10, which are questions related to suicide. [sent-205, score-0.401]
</p><p>79 Feature 5 models the ‘yes’ answer to Questions 3, 4, 6, 7, 8, 1 In a questionnaire of this size some questions are not asked when a previous question was answered in a predetermined way to reduce the burden of taking the survey. [sent-207, score-0.227]
</p><p>80 For example, if a person has never had a period of low mood, the attempt suicide question is not asked. [sent-208, score-0.691]
</p><p>81 Feature 7 models answering afﬁrmatively to Questions 15, 16, 19 and 20, which are related to alcohol abuse. [sent-211, score-0.133]
</p><p>82 We show the percentage of respondents that answered positively to the suicide attempt questions in Table 2, independently for the 500 samples that were used to learn the IBP and the 9,500 hold-out samples, together with the total number of respondents. [sent-212, score-0.86]
</p><p>83 A dash indicates that the feature can be active or inactive. [sent-213, score-0.076]
</p><p>84 Throughout the database, the prevalence of suicide attempt is 7. [sent-216, score-0.706]
</p><p>85 As expected, Features 2, 4, 5 and 7 favor suicide attempt risk, although Feature 5 only mildly, and Features 1, 3 and 6 decrease the probability of attempting suicide. [sent-218, score-0.7]
</p><p>86 From the above description of each feature, it is clear that having Features 4 or 7 active should increase the risk of attempting suicide, while having Features 3 and 1 active should cause the opposite effect. [sent-219, score-0.159]
</p><p>87 The other combinations favor an increased rate of suicide attempts that goes from doubling (‘11’) to quadrupling (‘00’), to a ten-fold increase (‘01’), and the percentages of population with these features are, respectively, 21%, 6% and 3%. [sent-221, score-0.742]
</p><p>88 In the ﬁnal part of Table 2, we show combinations of features that signiﬁcantly increase the suicide attempt rate for a reduced percentage of the population, as well as combinations of features that signiﬁcantly decrease the suicide attempt rate for a large chunk of the population. [sent-222, score-1.43]
</p><p>89 These results are interesting as they can be used to discard signiﬁcant portions of the population in suicide attempt studies and focus on the groups that present much higher risk. [sent-223, score-0.695]
</p><p>90 Hence, our IBP with discrete observations is being able to obtain features that describe the hidden structure of the NESARC database and makes it possible to pin-point the people that have a higher risk of attempting suicide. [sent-224, score-0.335]
</p><p>91 We have applied our model to the NESARC database to ﬁnd out the hidden features that characterize the suicide attempt risk. [sent-229, score-0.8]
</p><p>92 We 7  Hidden features 1 0 1 1  1 -  1 0 0 1 1 0 0 1 1 1  1 0 1 0 1 1 1 0 0 0  1 -  1 1 -  1 1 0 0 -  Suicide attempt probability Train Hold-out 6. [sent-230, score-0.116]
</p><p>93 The ‘train ensemble’ columns contain the results for the 500 data points used to obtain the model, whereas the ‘hold-out ensemble’ columns contain the results for the remaining subjects. [sent-264, score-0.078]
</p><p>94 These probabilities have been obtained with the posterior mean weights Bd MAP , when only one of the seven latent features (sorted from left to right to match the order in Table 2) is active. [sent-266, score-0.195]
</p><p>95 have analyzed how each of the seven inferred features contributes to the suicide attempt probability. [sent-267, score-0.741]
</p><p>96 8  References [1] Summary of national strategy for suicide prevention: Goals and objectives for action, 2007. [sent-273, score-0.599]
</p><p>97 Cognitive therapy for the prevention of suicide attempts: a randomized controlled trial. [sent-302, score-0.726]
</p><p>98 Trends in suicide ideation, plans, gestures, and attempts in the united states, 1990-1992 to 2001-2003. [sent-327, score-0.656]
</p><p>99 The struggle to prevent and evaluate: application of population attributable risk and preventive fraction to suicide prevention research. [sent-332, score-0.818]
</p><p>100 A suicide prevention program in a region with a very high suicide rate. [sent-384, score-1.325]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('suicide', 0.599), ('bd', 0.499), ('ibp', 0.222), ('znk', 0.19), ('nesarc', 0.166), ('xnd', 0.146), ('questions', 0.14), ('prevention', 0.127), ('alcohol', 0.108), ('vn', 0.097), ('zn', 0.089), ('latent', 0.083), ('blank', 0.072), ('mood', 0.066), ('attempt', 0.063), ('gibbs', 0.061), ('risk', 0.059), ('answered', 0.058), ('wave', 0.058), ('attempts', 0.057), ('laplace', 0.053), ('features', 0.053), ('buffet', 0.05), ('white', 0.05), ('bmap', 0.05), ('epidemiologic', 0.05), ('irk', 0.05), ('answers', 0.049), ('sampler', 0.049), ('depression', 0.048), ('disorders', 0.046), ('madrid', 0.046), ('indian', 0.046), ('feature', 0.045), ('prevalence', 0.044), ('emergency', 0.044), ('felt', 0.044), ('database', 0.043), ('yes', 0.043), ('nk', 0.043), ('hidden', 0.042), ('observation', 0.041), ('nth', 0.04), ('observations', 0.04), ('medical', 0.04), ('columns', 0.039), ('attempting', 0.038), ('carlos', 0.037), ('feel', 0.036), ('months', 0.036), ('map', 0.035), ('survey', 0.035), ('subjects', 0.035), ('readily', 0.035), ('went', 0.035), ('fernando', 0.035), ('md', 0.034), ('discrete', 0.034), ('drank', 0.033), ('dysthymia', 0.033), ('icd', 0.033), ('overnight', 0.033), ('rihmer', 0.033), ('rmatively', 0.033), ('ruiz', 0.033), ('suicidal', 0.033), ('valera', 0.033), ('population', 0.033), ('posterior', 0.033), ('matrix', 0.031), ('determinant', 0.031), ('active', 0.031), ('pixel', 0.03), ('dth', 0.029), ('isabel', 0.029), ('stayed', 0.029), ('question', 0.029), ('public', 0.029), ('kr', 0.029), ('treatment', 0.028), ('submatrix', 0.028), ('integrate', 0.027), ('psychiatry', 0.027), ('committing', 0.027), ('die', 0.027), ('mental', 0.027), ('causes', 0.027), ('dirichlet', 0.027), ('inversion', 0.026), ('black', 0.026), ('seven', 0.026), ('people', 0.026), ('weighting', 0.025), ('answering', 0.025), ('nd', 0.025), ('began', 0.024), ('mann', 0.024), ('wanted', 0.024), ('topic', 0.024), ('hessian', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="52-tfidf-1" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>2 0.17799577 <a title="52-tfidf-2" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>Author: Kei Wakabayashi, Takao Miura</p><p>Abstract: Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data. However, existing HHMM parameter estimation methods require large computations of time complexity O(T N 2D ) at least for model inference, where D is the depth of the hierarchy, N is the number of states in each level, and T is the sequence length. In this paper, we propose a new inference method of HHMMs for which the time complexity is O(T N D+1 ). A key idea of our algorithm is application of the forward-backward algorithm to state activation probabilities. The notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of HHMMs, enables us to conduct model inference efﬁciently. We present some experiments to demonstrate that our proposed method works more efﬁciently to estimate HHMM parameters than do some existing methods such as the ﬂattening method and Gibbs sampling method. 1</p><p>3 0.10091404 <a title="52-tfidf-3" href="./nips-2012-A_Divide-and-Conquer_Method_for_Sparse_Inverse_Covariance_Estimation.html">7 nips-2012-A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation</a></p>
<p>Author: Cho-jui Hsieh, Arindam Banerjee, Inderjit S. Dhillon, Pradeep K. Ravikumar</p><p>Abstract: We consider the composite log-determinant optimization problem, arising from the 1 regularized Gaussian maximum likelihood estimator of a sparse inverse covariance matrix, in a high-dimensional setting with a very large number of variables. Recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding Gaussian Markov Random Field, even in very high-dimensional regimes with a limited number of samples. In this paper, we are concerned with the computational cost in solving the above optimization problem. Our proposed algorithm partitions the problem into smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem. Our key idea for the divide step to obtain a sub-problem partition is as follows: we ﬁrst derive a tractable bound on the quality of the approximate solution obtained from solving the corresponding sub-divided problems. Based on this bound, we propose a clustering algorithm that attempts to minimize this bound, in order to ﬁnd effective partitions of the variables. For the conquer step, we use the approximate solution, i.e., solution resulting from solving the sub-problems, as an initial point to solve the original problem, and thereby achieve a much faster computational procedure. 1</p><p>4 0.075874202 <a title="52-tfidf-4" href="./nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">59 nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<p>Author: Francois Caron</p><p>Abstract: We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially inﬁnite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, a generative process for network growth, and a simple Gibbs sampler for posterior simulation. Our model is shown to be well ﬁtted to several real-world social networks. 1</p><p>5 0.069541343 <a title="52-tfidf-5" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>6 0.069174871 <a title="52-tfidf-6" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>7 0.060485817 <a title="52-tfidf-7" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>8 0.056445476 <a title="52-tfidf-8" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>9 0.053266805 <a title="52-tfidf-9" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>10 0.050164003 <a title="52-tfidf-10" href="./nips-2012-Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression.html">127 nips-2012-Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression</a></p>
<p>11 0.048933804 <a title="52-tfidf-11" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>12 0.046705708 <a title="52-tfidf-12" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>13 0.046271 <a title="52-tfidf-13" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>14 0.046069339 <a title="52-tfidf-14" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>15 0.045115188 <a title="52-tfidf-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.044137534 <a title="52-tfidf-16" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>17 0.043613367 <a title="52-tfidf-17" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>18 0.042828582 <a title="52-tfidf-18" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>19 0.042771667 <a title="52-tfidf-19" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>20 0.04270206 <a title="52-tfidf-20" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.03), (2, -0.021), (3, -0.002), (4, -0.082), (5, -0.024), (6, 0.006), (7, 0.019), (8, 0.046), (9, -0.026), (10, 0.02), (11, 0.037), (12, -0.028), (13, -0.002), (14, 0.016), (15, -0.021), (16, 0.001), (17, -0.02), (18, 0.009), (19, -0.025), (20, 0.023), (21, 0.025), (22, -0.051), (23, -0.007), (24, 0.025), (25, -0.016), (26, 0.047), (27, -0.027), (28, -0.026), (29, -0.024), (30, -0.035), (31, 0.089), (32, -0.052), (33, 0.004), (34, 0.008), (35, -0.014), (36, -0.128), (37, -0.018), (38, -0.016), (39, -0.034), (40, 0.014), (41, -0.002), (42, -0.007), (43, -0.098), (44, 0.057), (45, 0.087), (46, 0.051), (47, -0.002), (48, 0.055), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88407099 <a title="52-lsi-1" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>2 0.66975796 <a title="52-lsi-2" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>3 0.63222837 <a title="52-lsi-3" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>Author: Kei Wakabayashi, Takao Miura</p><p>Abstract: Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data. However, existing HHMM parameter estimation methods require large computations of time complexity O(T N 2D ) at least for model inference, where D is the depth of the hierarchy, N is the number of states in each level, and T is the sequence length. In this paper, we propose a new inference method of HHMMs for which the time complexity is O(T N D+1 ). A key idea of our algorithm is application of the forward-backward algorithm to state activation probabilities. The notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of HHMMs, enables us to conduct model inference efﬁciently. We present some experiments to demonstrate that our proposed method works more efﬁciently to estimate HHMM parameters than do some existing methods such as the ﬂattening method and Gibbs sampling method. 1</p><p>4 0.62954319 <a title="52-lsi-4" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>5 0.59852004 <a title="52-lsi-5" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>Author: Lucas Theis, Jascha Sohl-dickstein, Matthias Bethge</p><p>Abstract: We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. 1</p><p>6 0.56736261 <a title="52-lsi-6" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>7 0.52461421 <a title="52-lsi-7" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>8 0.52179521 <a title="52-lsi-8" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>9 0.50274897 <a title="52-lsi-9" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>10 0.49642161 <a title="52-lsi-10" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>11 0.49406227 <a title="52-lsi-11" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>12 0.493141 <a title="52-lsi-12" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>13 0.49083757 <a title="52-lsi-13" href="./nips-2012-A_nonparametric_variable_clustering_model.html">26 nips-2012-A nonparametric variable clustering model</a></p>
<p>14 0.49012539 <a title="52-lsi-14" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>15 0.48514569 <a title="52-lsi-15" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>16 0.47848818 <a title="52-lsi-16" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>17 0.46420029 <a title="52-lsi-17" href="./nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">365 nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<p>18 0.46140563 <a title="52-lsi-18" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>19 0.45690721 <a title="52-lsi-19" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>20 0.4513917 <a title="52-lsi-20" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.044), (21, 0.026), (38, 0.073), (39, 0.022), (42, 0.016), (54, 0.02), (55, 0.45), (74, 0.039), (76, 0.098), (80, 0.07), (92, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79339141 <a title="52-lda-1" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>2 0.79324579 <a title="52-lda-2" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>3 0.78862435 <a title="52-lda-3" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>4 0.77508295 <a title="52-lda-4" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>Author: Joseph L. Austerweil, Joshua T. Abbott, Thomas L. Griffiths</p><p>Abstract: The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. 1</p><p>5 0.70787293 <a title="52-lda-5" href="./nips-2012-Meta-Gaussian_Information_Bottleneck.html">211 nips-2012-Meta-Gaussian Information Bottleneck</a></p>
<p>Author: Melanie Rey, Volker Roth</p><p>Abstract: We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers. 1</p><p>6 0.62588233 <a title="52-lda-6" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>7 0.60909492 <a title="52-lda-7" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>8 0.4994165 <a title="52-lda-8" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>9 0.48662382 <a title="52-lda-9" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>10 0.47557655 <a title="52-lda-10" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>11 0.47048619 <a title="52-lda-11" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>12 0.47017914 <a title="52-lda-12" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>13 0.4684459 <a title="52-lda-13" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>14 0.45335791 <a title="52-lda-14" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>15 0.44798055 <a title="52-lda-15" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>16 0.44547945 <a title="52-lda-16" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>17 0.4393653 <a title="52-lda-17" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>18 0.43662348 <a title="52-lda-18" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>19 0.43569523 <a title="52-lda-19" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>20 0.42781508 <a title="52-lda-20" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
