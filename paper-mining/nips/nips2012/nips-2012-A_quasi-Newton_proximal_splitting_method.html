<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 nips-2012-A quasi-Newton proximal splitting method</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-27" href="#">nips2012-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 nips-2012-A quasi-Newton proximal splitting method</h1>
<br/><p>Source: <a title="nips-2012-27-pdf" href="http://papers.nips.cc/paper/4523-a-quasi-newton-proximal-splitting-method.pdf">pdf</a></p><p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>Reference: <a title="nips-2012-27-reference" href="../nips2012_reference/nips-2012-A_quasi-Newton_proximal_splitting_method_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Fadili†  Abstract A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. [sent-4, score-0.457]
</p><p>2 We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. [sent-5, score-0.285]
</p><p>3 The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. [sent-6, score-0.161]
</p><p>4 For large-scale unconstrained smooth convex problems, two classes of methods have seen the most success: limited memory quasi-Newton methods and non-linear conjugate gradient (CG) methods. [sent-11, score-0.253]
</p><p>5 Both of these methods generally outperform simpler methods, such as gradient descent. [sent-12, score-0.07]
</p><p>6 For problems with non-smooth terms and/or constraints, it is possible to generalize gradient descent with proximal gradient descent (which includes projected gradient descent as a sub-cases), which is just the application of the forward-backward algorithm [1]. [sent-13, score-0.548]
</p><p>7 Unlike gradient descent, it is not easy to adapt quasi-Newton and CG methods to problems involving constraints and non-smooth terms. [sent-14, score-0.07]
</p><p>8 In the limit, as the active-set is correctly identiﬁed, the methods behave similar to their unconstrained counterparts. [sent-16, score-0.071]
</p><p>9 These methods have seen success, but are not as efﬁcient or as elegant as the unconstrained versions. [sent-17, score-0.113]
</p><p>10 1  Problem statement N  Let H = (RN , ·, · ) equipped with the usual Euclidean scalar product x, y = i=1 xi yi and associated norm x = x, x . [sent-20, score-0.077]
</p><p>11 The domain of f is deﬁned by dom f = {x ∈ H : f (x) < +∞} and f is proper if dom f = ∅. [sent-25, score-0.124]
</p><p>12 1  class of all proper lsc convex functions from H to R ∪ {+∞} is denoted by Γ0 (H). [sent-35, score-0.116]
</p><p>13 The class we consider covers non-smooth convex optimization problems, including those with convex constraints. [sent-43, score-0.175]
</p><p>14 2  Contributions  This paper introduces a class of scaled norms for which we can compute a proximity operator; these results themselves are signiﬁcant, for previous results only cover diagonal scaling (the diagonal scaling result is trivial). [sent-52, score-0.569]
</p><p>15 Then, motivated by the discrepancy between constrained and unconstrained performance, we deﬁne a class of limited-memory quasi-Newton methods to solve (P) and that extends naturally and elegantly from the unconstrained to the constrained case. [sent-53, score-0.289]
</p><p>16 Most well-known quasi-Newton methods for constrained problems, such as L-BFGS-B [2], are only applicable to box constraints l ≤ x ≤ u. [sent-54, score-0.088]
</p><p>17 The power of our approach is that it applies to a wide-variety of useful non-smooth functionals (see §3. [sent-55, score-0.05]
</p><p>18 The approach uses the zero-memory SR1 algorithm, and we provide evidence that the non-diagonal term provides signiﬁcant improvements over diagonal Hessians. [sent-58, score-0.139]
</p><p>19 1  Quasi-Newton forward-backward splitting The algorithm  In the following, deﬁne the quadratic approximation QB (x) = f (xk ) + k  f (xk ), x − xk +  1 x − xk 2  2 B,  (4)  where B ∈ S++ (N ). [sent-60, score-0.568]
</p><p>20 k 2  (5)  Note that this specializes to the gradient descent when h = 0. [sent-62, score-0.117]
</p><p>21 Therefore, if f is a strictly convex quadratic function and one takes Bk = 2 f (xk ), then we obtain the Newton method. [sent-63, score-0.161]
</p><p>22 Since f is smooth and can be approximated by a quadratic, and inspired by quasi-Newton methods, this suggest picking Bk as an approximation of the Hessian. [sent-66, score-0.042]
</p><p>23 Our diagonal+rank 1 quasi-Newton forward-backward splitting algorithm is listed in Algorithm 1 (with details for the quasi-Newton update in Algorithm 2, see §4 for details). [sent-68, score-0.096]
</p><p>24 Algorithm 1: Zero-memory Symmetric Rank 1 (0SR1) algorithm to solve min f + h Require: x0 ∈ dom(f + h), Lipschitz constant estimate L of f , stopping criterion 1: for k = 1, 2, 3, . [sent-74, score-0.079]
</p><p>25 do 2: sk ← xk − xk−1 3: yk ← f (xk ) − f (xk−1 ) −1 4: Compute Hk via Algorithm 2, and deﬁne Bk = Hk . [sent-77, score-0.748]
</p><p>26 5: Compute the rank-1 proximity operator (see §3) xk+1 ← proxBk (xk − Hk f (xk )) ˆ h  (6)  6: pk ← xk+1 − xk and terminate if pk < ˆ 7: Line-search along the ray xk + tpk to determine xk+1 , or choose t = 1. [sent-78, score-0.776]
</p><p>27 2  Relation to prior work  First-order methods The algorithm in (5) is variously known as proximal descent or iterated shrinkage/thresholding algorithm (IST or ISTA). [sent-80, score-0.164]
</p><p>28 The spectral projected gradient (SPG) [4] method was designed as an extension of the BarzilaiBorwein spectral step-length method to constrained problems. [sent-82, score-0.203]
</p><p>29 In [5], it was extended to non-smooth problems by allowing general proximity operators; The Barzilai-Borwein method [6] uses a speciﬁc choice of step-length tk motivated by quasi-Newton methods. [sent-83, score-0.302]
</p><p>30 Nesterov acceleration can be viewed as an over-relaxed version of ISTA with a speciﬁc, non-constant over-relaxation parameter αk . [sent-87, score-0.049]
</p><p>31 The general diagonal case was considered in several papers in the 1980s as a simple quasi-Newton method, but never widely adapted. [sent-89, score-0.139]
</p><p>32 A convergence rate analysis of forward-backward splitting with static and variable Bk where one of the operators is maximal strongly monotone is given in [10]. [sent-91, score-0.158]
</p><p>33 Active set approaches Active set methods take a simple step, such as gradient projection, to identify active variables, and then uses a more advanced quadratic model to solve for the free variables. [sent-92, score-0.21]
</p><p>34 A recent bound-constrained solver is ASA [13] which uses a conjugate gradient (CG) solver on the free variables, and shows good results compared to L-BFGS-B, SPG, GENCAN and TRON. [sent-94, score-0.07]
</p><p>35 We also compare to several active set approaches specialized for 1 penalties: “Orthant-wise Learning” (OWL) [14], “Projected Scaled Sub-gradient + Active Set” (PSSas) [15], “Fixed-point continuation + Active Set” (FPC AS) [16], and “CG + IST” (CGIST) [17]. [sent-95, score-0.053]
</p><p>36 IPM requires solving a Newton-step equation, so ﬁrst-order like “Hessian-free” variants of IPM solve the Newton-step approximately, either by approximately solving the equation or by subsampling the Hessian. [sent-97, score-0.041]
</p><p>37 Yet another approach is to include the non-smooth h term in the quadratic approximation. [sent-99, score-0.046]
</p><p>38 The projected quasi-Newton (PQN) algorithm [19, 20] is perhaps the most elegant and logical extension of quasi-Newton methods, but it involves solving a sub-iteration. [sent-102, score-0.122]
</p><p>39 As discussed in [21], it is possible to generalize PQN to general non-smooth problems whenever the proximity operator is known (since, as mentioned above, it is possible to extend SPG to this case). [sent-105, score-0.35]
</p><p>40 3  Proximity operators and proximal calculus  For space limitation reasons, we only recall essential deﬁnitions. [sent-106, score-0.217]
</p><p>41 More notions, results from convex analysis as well as proofs can be found in the supplementary material. [sent-107, score-0.07]
</p><p>42 Then, for every x ∈ H, the function 2 1 z → 2 x − z + h(z) achieves its inﬁmum at a unique point denoted by proxh x. [sent-110, score-0.227]
</p><p>43 The uniquelyvalued operator proxh : H → H thus deﬁned is the proximity operator or proximal mapping of h. [sent-111, score-0.75]
</p><p>44 1  Proximal calculus in HV  Throughout, we denote proxV = (IHV + V −1 ∂h)−1 , where ∂h is the subdifferential of h, the h proximity operator of h w. [sent-113, score-0.388]
</p><p>45 Note that since V ∈ S++ (N ), the proximity operator proxV is well-deﬁned. [sent-117, score-0.35]
</p><p>46 1  (8)  Diagonal+rank-1: General case  Theorem 7 (Proximity operator in HV ). [sent-124, score-0.099]
</p><p>47 Let h ∈ Γ0 (H) and V = D + uuT , where D is diagonal with (strictly) positive diagonal elements di , and u ∈ RN . [sent-125, score-0.315]
</p><p>48 Then, proxV (x) = D−1/2 ◦ proxh◦D−1/2 (D1/2 x − v) , h where v = αD  −1/2  (9)  u and α is the unique root of  p(α) = u, x − D−1/2 ◦ proxh◦D−1/2 ◦D1/2 (x − αD−1 u) + α ,  (10)  which is a Lipschitz continuous and strictly increasing function on R with Lipschitz constant 1 + 2 i ui /di . [sent-126, score-0.17]
</p><p>49 • Computing proxV amounts to solving a scalar optimization problem that involves the comh putation of proxh◦D−1/2 . [sent-128, score-0.07]
</p><p>50 The latter can be much simpler to compute as D is diagonal (beyond the obvious separable case that we will consider shortly). [sent-129, score-0.296]
</p><p>51 2  Diagonal+rank-1: Separable case  The following corollary is key to our novel optimization algorithm. [sent-134, score-0.077]
</p><p>52 h(x) = i=1 hi (xi ), and V = D + uuT , where D is diagonal with (strictly) positive diagonal elements di , and u ∈ RN . [sent-138, score-0.315]
</p><p>53 Then, proxV (x) = proxhi /di (xi − vi /di ) h  ,  (11)  i  where v = αu and α is the unique root of p(α) = u, x − proxhi /di (xi − αui /di )  +α,  (12)  i  which is a Lipschitz continuous and strictly increasing function on R. [sent-139, score-0.41]
</p><p>54 is  Proof: diagonal,  As applying  h  is separable and Theorem 7 yields  ∈ desired  D the  S++ (N ) result. [sent-140, score-0.157]
</p><p>55 Assume that for 1 i N , proxhi is piecewise afﬁne on R with ki ≥ 1 segments, i. [sent-142, score-0.237]
</p><p>56 proxhi (xi ) = aj xi + bj , tj xi tj+1 , j ∈ {1, . [sent-144, score-0.201]
</p><p>57 Then proxV (x) can be obtained exactly by sorting at most the k real values h  (i,j)∈{1,. [sent-149, score-0.064]
</p><p>58 When proxhi is piecewise afﬁne with ki segments, it is easy to see that p(α) in (12) is also piecewise afﬁne with slopes and intercepts changing at the k transition points  di ui (xi  − tj )  (i,j)∈{1,. [sent-157, score-0.423]
</p><p>59 To get α , it is suf-  ﬁcient to isolate the unique segment that intersects the abscissa axis. [sent-164, score-0.043]
</p><p>60 This can be achieved by sorting the values of the transition points which can cost in average complexity O(k log k). [sent-165, score-0.064]
</p><p>61 • Corollary 9 can be extended to the “block” separable (i. [sent-173, score-0.157]
</p><p>62 separable in subsets of coordinates) when D is piecewise constant along the same block indices. [sent-175, score-0.204]
</p><p>63 If no closed-form is available, one can appeal to some efﬁcient iterative method to solve (10) (or (12)). [sent-179, score-0.041]
</p><p>64 The semi-smooth Newton method for the solution of (10) can be stated as the iteration αt+1 = αt − g(αt )−1 p(αt ) , (13) where g is a generalized derivative of p. [sent-181, score-0.074]
</p><p>65 If proxh◦D−1/2 is Newton differentiable with generalized derivative G, then so is the mapping p with a generalized derivative g(α) = 1 + u, D−1/2 ◦ G(D1/2 x − αD−1/2 u) ◦ D−1/2 u Furthermore, g is nonsingular with a uniformly bounded inverse on R. [sent-183, score-0.243]
</p><p>66 Thus, as p is Newton differentiable with nonsingular generalized derivative whose inverse is also bounded, the general semi-smooth Newton convergence theorem implies that (13) converges superlinearly to the unique root of (10). [sent-188, score-0.252]
</p><p>67 For instance, Table 1 summarizes a few of them where we can obtain either an exact answer by sorting when possible, or else by minimizing w. [sent-192, score-0.1]
</p><p>68 4  A primal rank 1 SR1 algorithm  Following the conventional quasi-Newton notation, we let B denote an approximation to the Hessian of f and H denote an approximation to the inverse Hessian. [sent-198, score-0.092]
</p><p>69 All quasi-Newton methods update an approximation to the (inverse) Hessian that satisﬁes the secant condition: Hk yk = sk ,  yk =  f (xk ) −  f (xk−1 ),  sk = xk − xk−1  (14)  Algorithm 1 follows the SR1 method [24], which uses a rank-1 update to the inverse Hessian approximation at every step. [sent-199, score-1.391]
</p><p>70 In particular, we use zero-memory, which means that at every iteration, a new diagonal plus rank-one matrix is formed. [sent-204, score-0.139]
</p><p>71 The other modiﬁcation is to extend the SR1 method to the general setting of minimizing f + h where f is smooth but h need not be smooth; this further generalizes the case when h is an indicator function of a convex set. [sent-205, score-0.112]
</p><p>72 Every step of the algorithm replaces f with a quadratic approximation, and keeps h unchanged. [sent-206, score-0.046]
</p><p>73 Choosing H0 step length  In our experience, the choice of H0 is best if scaled with a Barzilai-Borwein spectral  (we call it τBB2 sk , sk / sk , yk  τBB2 = sk , yk / yk , yk (15) to distinguish it from the other Barzilai-Borwein step size τBB1 = τBB2 ). [sent-208, score-2.18]
</p><p>74 In SR1 methods, the quantity sk − H0 yk , yk must be positive in order to have a well-deﬁned update for uk . [sent-209, score-0.93]
</p><p>75 The update is: Hk = H0 + uk uT , k  uk = (sk − H0 yk )/ 6  sk − H0 yk , yk . [sent-210, score-1.325]
</p><p>76 12: end if 13: end if −1 14: return Hk = H0 + uk uT {Bk = Hk can be computed via the Sherman-Morrison formula} k  For this reason, we choose H0 = γτBB2 IH with 0 < γ < 1, and thus 0 ≤ sk − H0 yk , yk = (1 − γ) sk , yk . [sent-212, score-1.465]
</p><p>77 If sk , yk = 0, then there is no symmetric rank-one update that satisﬁes the secant condition. [sent-213, score-0.591]
</p><p>78 The inequality sk , yk > 0 is the curvature condition, and it is guaranteed for all strictly convex objectives. [sent-214, score-0.65]
</p><p>79 Following the recommendation in [26], we skip updates whenever sk , yk cannot be guaranteed to be non-zero given standard ﬂoating-point precision. [sent-215, score-0.572]
</p><p>80 5  (b)  Figure 1: (a) is ﬁrst LASSO test, (b) is second LASSO test Consider the unconstrained LASSO problem (1). [sent-222, score-0.071]
</p><p>81 However, this constrained problem has twice the number of variables, and the Hessian of 7  AT A −AT A −AT A AT A n degenerate 0 eigenvalues and adversely affects solvers. [sent-226, score-0.053]
</p><p>82 ˜ the quadratic part changes from AT A to A =  which necessarily has (at least)  A similar situation occurs with the hinge-loss function. [sent-227, score-0.046]
</p><p>83 Our second example uses a square operator A with dimensions n = 133 = 2197 chosen as a 3D discrete differential operator. [sent-238, score-0.099]
</p><p>84 This example stems from a numerical analysis problem to solve a discretized PDE as suggested by [28]. [sent-239, score-0.041]
</p><p>85 6  Conclusions  In this paper, we proposed a novel variable metric (quasi-Newton) forward-backward splitting algorithm, designed to efﬁciently solve non-smooth convex problems structured as the sum of a smooth term and a non-smooth one. [sent-247, score-0.249]
</p><p>86 We introduced a class of weighted norms induced by a diagonal+rank 1 symmetric positive deﬁnite matrices, and proposed a whole framework to compute a proximity operator in the weighted norm. [sent-248, score-0.35]
</p><p>87 We also provided clear evidence that the non-diagonal term provides signiﬁcant acceleration over diagonal matrices. [sent-250, score-0.188]
</p><p>88 Another improvement would be to derive efﬁcient calculation for rank-2 proximity terms, thus allowing a 0-memory BFGS method. [sent-255, score-0.285]
</p><p>89 However, in general, one must solve an r-dimensional inner problem using the semismooth Newton method. [sent-257, score-0.154]
</p><p>90 A ﬁnal possible extension is to take Bk to be diagonal plus rank-1 on diagonal blocks, since if h is separable, this is still can be solved by our algorithm (see Remark 10). [sent-258, score-0.278]
</p><p>91 Nonmonotone spectral projected gradient methods on ı convex sets. [sent-303, score-0.22]
</p><p>92 A fast iterative shrinkage-thresholding algorithm for linear inverse problems. [sent-325, score-0.052]
</p><p>93 Diagonal preconditioning for ﬁrst order primal-dual algorithms in convex optimization. [sent-338, score-0.07]
</p><p>94 Remark on algorithm 778: L-BFGS-B: Fortran subroutines for e large-scale bound constrained optimization¨ ACM Trans. [sent-359, score-0.094]
</p><p>95 A new active set algorithm for box constrained optimization. [sent-368, score-0.141]
</p><p>96 A quasi-Newton approach to nonsmooth convex optimization problems in machine learning. [sent-405, score-0.105]
</p><p>97 Optimizing costly functions with simple constraints: A limited-memory projected quasi-Newton algorithm. [sent-413, score-0.08]
</p><p>98 Proximal Newton-type methods for minimizing convex objective functions in composite form. [sent-431, score-0.07]
</p><p>99 A semismooth Newton method for Tikhonov functionals with sparsity constraints. [sent-445, score-0.163]
</p><p>100 Tackling box-constrained optimization via a new projected quasi-Newton approach. [sent-473, score-0.115]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('proxv', 0.338), ('yk', 0.328), ('proximity', 0.251), ('xk', 0.213), ('sk', 0.207), ('proxh', 0.184), ('newton', 0.179), ('separable', 0.157), ('hv', 0.151), ('proxhi', 0.141), ('diagonal', 0.139), ('bk', 0.135), ('proximal', 0.117), ('ih', 0.115), ('nonseparable', 0.113), ('semismooth', 0.113), ('spg', 0.113), ('hk', 0.104), ('bfgs', 0.103), ('operator', 0.099), ('splitting', 0.096), ('asa', 0.092), ('ista', 0.086), ('cgist', 0.085), ('fpc', 0.085), ('pssas', 0.085), ('hessian', 0.081), ('projected', 0.08), ('fortran', 0.075), ('owl', 0.075), ('pqn', 0.075), ('unconstrained', 0.071), ('convex', 0.07), ('gradient', 0.07), ('ipm', 0.069), ('cg', 0.069), ('uk', 0.067), ('sorting', 0.064), ('operators', 0.062), ('dom', 0.062), ('tj', 0.06), ('projector', 0.059), ('lasso', 0.059), ('bauschke', 0.056), ('caen', 0.056), ('mem', 0.056), ('secant', 0.056), ('ax', 0.056), ('fista', 0.055), ('active', 0.053), ('constrained', 0.053), ('inverse', 0.052), ('siam', 0.052), ('tk', 0.051), ('functionals', 0.05), ('ki', 0.049), ('acceleration', 0.049), ('descent', 0.047), ('schmidt', 0.047), ('piecewise', 0.047), ('fadili', 0.046), ('lsc', 0.046), ('uut', 0.046), ('quadratic', 0.046), ('lipschitz', 0.045), ('strictly', 0.045), ('rn', 0.044), ('nonsingular', 0.043), ('byrd', 0.043), ('combettes', 0.043), ('unique', 0.043), ('corollary', 0.042), ('smooth', 0.042), ('yi', 0.042), ('ui', 0.042), ('elegant', 0.042), ('restart', 0.041), ('subroutines', 0.041), ('solve', 0.041), ('derivative', 0.041), ('root', 0.04), ('rank', 0.04), ('solvers', 0.04), ('scaled', 0.04), ('ist', 0.039), ('nocedal', 0.039), ('stopping', 0.038), ('calculus', 0.038), ('remark', 0.037), ('di', 0.037), ('skip', 0.037), ('else', 0.036), ('hinge', 0.036), ('bb', 0.035), ('box', 0.035), ('optimization', 0.035), ('scalar', 0.035), ('preprint', 0.034), ('calculation', 0.034), ('generalized', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="27-tfidf-1" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>2 0.33376262 <a title="27-tfidf-2" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><p>3 0.26524007 <a title="27-tfidf-3" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>4 0.22844251 <a title="27-tfidf-4" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>5 0.1740265 <a title="27-tfidf-5" href="./nips-2012-Newton-Like_Methods_for_Sparse_Inverse_Covariance_Estimation.html">240 nips-2012-Newton-Like Methods for Sparse Inverse Covariance Estimation</a></p>
<p>Author: Figen Oztoprak, Jorge Nocedal, Steven Rennie, Peder A. Olsen</p><p>Abstract: We propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem. The ﬁrst approach, which we call the Newton-LASSO method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step. We employ the fast iterative shrinkage thresholding algorithm (FISTA) to solve this subproblem. The second approach, which we call the Orthant-Based Newton method, is a two-phase algorithm that ﬁrst identiﬁes an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method. These methods exploit the structure of the Hessian to efﬁciently compute the search direction and to avoid explicitly storing the Hessian. We also propose a limited memory BFGS variant of the orthant-based Newton method. Numerical results, including comparisons with the method implemented in the QUIC software [1], suggest that all the techniques described in this paper constitute useful tools for the solution of the sparse inverse covariance estimation problem. 1</p><p>6 0.15282668 <a title="27-tfidf-6" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>7 0.1475049 <a title="27-tfidf-7" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>8 0.13358642 <a title="27-tfidf-8" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>9 0.11875609 <a title="27-tfidf-9" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>10 0.10902901 <a title="27-tfidf-10" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>11 0.10709167 <a title="27-tfidf-11" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>12 0.10541451 <a title="27-tfidf-12" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>13 0.10459798 <a title="27-tfidf-13" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>14 0.10366789 <a title="27-tfidf-14" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>15 0.09530963 <a title="27-tfidf-15" href="./nips-2012-Learning_Probability_Measures_with_respect_to_Optimal_Transport_Metrics.html">184 nips-2012-Learning Probability Measures with respect to Optimal Transport Metrics</a></p>
<p>16 0.091987975 <a title="27-tfidf-16" href="./nips-2012-Convergence_and_Energy_Landscape_for_Cheeger_Cut_Clustering.html">85 nips-2012-Convergence and Energy Landscape for Cheeger Cut Clustering</a></p>
<p>17 0.091881007 <a title="27-tfidf-17" href="./nips-2012-Learning_Manifolds_with_K-Means_and_K-Flats.html">179 nips-2012-Learning Manifolds with K-Means and K-Flats</a></p>
<p>18 0.089856915 <a title="27-tfidf-18" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>19 0.088638797 <a title="27-tfidf-19" href="./nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">206 nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>20 0.088069759 <a title="27-tfidf-20" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.222), (1, 0.045), (2, 0.189), (3, -0.119), (4, 0.129), (5, 0.157), (6, 0.013), (7, -0.158), (8, 0.28), (9, 0.05), (10, -0.124), (11, 0.133), (12, 0.09), (13, -0.049), (14, -0.088), (15, -0.019), (16, -0.064), (17, 0.064), (18, 0.019), (19, -0.006), (20, -0.085), (21, 0.057), (22, 0.02), (23, 0.029), (24, 0.035), (25, -0.026), (26, 0.006), (27, -0.049), (28, 0.003), (29, -0.032), (30, 0.013), (31, -0.001), (32, -0.041), (33, 0.023), (34, 0.027), (35, -0.082), (36, -0.009), (37, -0.035), (38, 0.037), (39, 0.05), (40, 0.019), (41, -0.008), (42, 0.014), (43, 0.045), (44, 0.037), (45, -0.017), (46, -0.013), (47, 0.004), (48, -0.028), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94859827 <a title="27-lsi-1" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>2 0.94076985 <a title="27-lsi-2" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><p>3 0.90211374 <a title="27-lsi-3" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>4 0.83430362 <a title="27-lsi-4" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>5 0.66441035 <a title="27-lsi-5" href="./nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">20 nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<p>Author: Nicolas L. Roux, Mark Schmidt, Francis R. Bach</p><p>Abstract: We propose a new stochastic gradient method for optimizing the sum of a ﬁnite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly. 1</p><p>6 0.65162247 <a title="27-lsi-6" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>7 0.57439494 <a title="27-lsi-7" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>8 0.56157464 <a title="27-lsi-8" href="./nips-2012-Convergence_and_Energy_Landscape_for_Cheeger_Cut_Clustering.html">85 nips-2012-Convergence and Energy Landscape for Cheeger Cut Clustering</a></p>
<p>9 0.56095445 <a title="27-lsi-9" href="./nips-2012-Newton-Like_Methods_for_Sparse_Inverse_Covariance_Estimation.html">240 nips-2012-Newton-Like Methods for Sparse Inverse Covariance Estimation</a></p>
<p>10 0.53217298 <a title="27-lsi-10" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>11 0.52180928 <a title="27-lsi-11" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>12 0.50131994 <a title="27-lsi-12" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>13 0.4677707 <a title="27-lsi-13" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>14 0.46480137 <a title="27-lsi-14" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>15 0.46145123 <a title="27-lsi-15" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>16 0.4509438 <a title="27-lsi-16" href="./nips-2012-Approximating_Concavely_Parameterized_Optimization_Problems.html">44 nips-2012-Approximating Concavely Parameterized Optimization Problems</a></p>
<p>17 0.44925579 <a title="27-lsi-17" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>18 0.43683553 <a title="27-lsi-18" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>19 0.43020228 <a title="27-lsi-19" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>20 0.42957401 <a title="27-lsi-20" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.098), (17, 0.011), (21, 0.028), (38, 0.203), (39, 0.012), (42, 0.037), (54, 0.027), (55, 0.022), (64, 0.187), (74, 0.027), (76, 0.153), (80, 0.09), (92, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89555264 <a title="27-lda-1" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>same-paper 2 0.88436973 <a title="27-lda-2" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>3 0.87041795 <a title="27-lda-3" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>Author: Arthur Guez, David Silver, Peter Dayan</p><p>Abstract: Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, ﬁnding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayesoptimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a signiﬁcant margin on several well-known benchmark problems – because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an inﬁnite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration. 1</p><p>4 0.86632687 <a title="27-lda-4" href="./nips-2012-Graphical_Models_via_Generalized_Linear_Models.html">147 nips-2012-Graphical Models via Generalized Linear Models</a></p>
<p>Author: Eunho Yang, Genevera Allen, Zhandong Liu, Pradeep K. Ravikumar</p><p>Abstract: Undirected graphical models, also known as Markov networks, enjoy popularity in a variety of applications. The popular instances of these models such as Gaussian Markov Random Fields (GMRFs), Ising models, and multinomial discrete models, however do not capture the characteristics of data in many settings. We introduce a new class of graphical models based on generalized linear models (GLMs) by assuming that node-wise conditional distributions arise from exponential families. Our models allow one to estimate multivariate Markov networks given any univariate exponential distribution, such as Poisson, negative binomial, and exponential, by ﬁtting penalized GLMs to select the neighborhood for each node. A major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly. We also provide examples of non-Gaussian high-throughput genomic networks learned via our GLM graphical models. 1</p><p>5 0.84460902 <a title="27-lda-5" href="./nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">221 nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>Author: Pinghua Gong, Jieping Ye, Chang-shui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer. To solve the non-convex optimization problem, we propose a MultiStage Multi-Task Feature Learning (MSMTFL) algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. 1</p><p>6 0.81650513 <a title="27-lda-6" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>7 0.81584561 <a title="27-lda-7" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>8 0.81483489 <a title="27-lda-8" href="./nips-2012-Convex_Multi-view_Subspace_Learning.html">86 nips-2012-Convex Multi-view Subspace Learning</a></p>
<p>9 0.81415802 <a title="27-lda-9" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>10 0.81394863 <a title="27-lda-10" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>11 0.81366855 <a title="27-lda-11" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>12 0.81333256 <a title="27-lda-12" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>13 0.81330162 <a title="27-lda-13" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>14 0.81309313 <a title="27-lda-14" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>15 0.81215996 <a title="27-lda-15" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>16 0.81171054 <a title="27-lda-16" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>17 0.81162107 <a title="27-lda-17" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>18 0.81016415 <a title="27-lda-18" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>19 0.80925393 <a title="27-lda-19" href="./nips-2012-Factoring_nonnegative_matrices_with_linear_programs.html">125 nips-2012-Factoring nonnegative matrices with linear programs</a></p>
<p>20 0.80895704 <a title="27-lda-20" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
