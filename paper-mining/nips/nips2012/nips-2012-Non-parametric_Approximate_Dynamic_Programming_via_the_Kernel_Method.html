<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-243" href="#">nips2012-243</a> knowledge-graph by maker-knowledge-mining</p><h1>243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</h1>
<br/><p>Source: <a title="nips-2012-243-pdf" href="http://papers.nips.cc/paper/4547-non-parametric-approximate-dynamic-programming-via-the-kernel-method.pdf">pdf</a></p><p>Author: Nikhil Bhat, Vivek Farias, Ciamac C. Moallemi</p><p>Abstract: This paper presents a novel non-parametric approximate dynamic programming (ADP) algorithm that enjoys graceful approximation and sample complexity guarantees. In particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric ADP algorithms, freeing the designer from carefully specifying an approximation architecture. We accomplish this by developing a kernel-based mathematical program for ADP. Via a computational study on a controlled queueing network, we show that our procedure is competitive with parametric ADP approaches. 1</p><p>Reference: <a title="nips-2012-243-reference" href="../nips2012_reference/nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rsalp', 0.598), ('adp', 0.431), ('queu', 0.385), ('policy', 0.246), ('salp', 0.215), ('sx', 0.128), ('alp', 0.12), ('rs', 0.114), ('faria', 0.096), ('bellm', 0.087), ('program', 0.086), ('job', 0.077), ('serv', 0.077), ('approxim', 0.071), ('scheduling', 0.07), ('wx', 0.064), ('architect', 0.057), ('lp', 0.054), ('offset', 0.051), ('hilbert', 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="243-tfidf-1" href="./nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<p>2 0.21202166 <a title="243-tfidf-2" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>3 0.18963212 <a title="243-tfidf-3" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>4 0.15058692 <a title="243-tfidf-4" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>5 0.13959657 <a title="243-tfidf-5" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>6 0.13729537 <a title="243-tfidf-6" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>7 0.12785921 <a title="243-tfidf-7" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>8 0.11993143 <a title="243-tfidf-8" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>9 0.11582211 <a title="243-tfidf-9" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>10 0.10690746 <a title="243-tfidf-10" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>11 0.098624021 <a title="243-tfidf-11" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>12 0.094104216 <a title="243-tfidf-12" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>13 0.086957484 <a title="243-tfidf-13" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>14 0.081858315 <a title="243-tfidf-14" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>15 0.08069361 <a title="243-tfidf-15" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>16 0.077910118 <a title="243-tfidf-16" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>17 0.075142086 <a title="243-tfidf-17" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>18 0.064221658 <a title="243-tfidf-18" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>19 0.061263379 <a title="243-tfidf-19" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>20 0.06017828 <a title="243-tfidf-20" href="./nips-2012-Convergence_Rate_Analysis_of_MAP_Coordinate_Minimization_Algorithms.html">84 nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.231), (2, -0.056), (3, 0.033), (4, 0.023), (5, 0.036), (6, -0.031), (7, 0.008), (8, 0.008), (9, -0.022), (10, 0.029), (11, -0.013), (12, -0.037), (13, -0.035), (14, 0.061), (15, -0.001), (16, -0.05), (17, -0.042), (18, 0.001), (19, -0.021), (20, 0.019), (21, -0.016), (22, 0.017), (23, 0.046), (24, -0.023), (25, -0.018), (26, -0.004), (27, -0.032), (28, -0.025), (29, -0.018), (30, 0.073), (31, -0.051), (32, 0.037), (33, 0.051), (34, 0.06), (35, -0.047), (36, -0.025), (37, -0.001), (38, 0.068), (39, -0.011), (40, 0.011), (41, -0.03), (42, -0.087), (43, 0.025), (44, -0.017), (45, 0.052), (46, -0.034), (47, -0.063), (48, 0.011), (49, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89598227 <a title="243-lsi-1" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>same-paper 2 0.83866733 <a title="243-lsi-2" href="./nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<p>3 0.82485741 <a title="243-lsi-3" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>4 0.81314385 <a title="243-lsi-4" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>5 0.80797672 <a title="243-lsi-5" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>6 0.78453565 <a title="243-lsi-6" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>7 0.7480703 <a title="243-lsi-7" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>8 0.73933285 <a title="243-lsi-8" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>9 0.73092139 <a title="243-lsi-9" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>10 0.65555727 <a title="243-lsi-10" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>11 0.63982981 <a title="243-lsi-11" href="./nips-2012-Value_Pursuit_Iteration.html">358 nips-2012-Value Pursuit Iteration</a></p>
<p>12 0.63497925 <a title="243-lsi-12" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>13 0.6038515 <a title="243-lsi-13" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>14 0.57350874 <a title="243-lsi-14" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>15 0.56838959 <a title="243-lsi-15" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>16 0.51008767 <a title="243-lsi-16" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>17 0.49817845 <a title="243-lsi-17" href="./nips-2012-Approximating_Equilibria_in_Sequential_Auctions_with_Incomplete_Information_and_Multi-Unit_Demand.html">45 nips-2012-Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand</a></p>
<p>18 0.49384338 <a title="243-lsi-18" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>19 0.48284513 <a title="243-lsi-19" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>20 0.47148582 <a title="243-lsi-20" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.089), (47, 0.119), (67, 0.031), (70, 0.125), (85, 0.046), (94, 0.111), (98, 0.318), (99, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.73774433 <a title="243-lda-1" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>2 0.72536361 <a title="243-lda-2" href="./nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">214 nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>3 0.72333813 <a title="243-lda-3" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>4 0.71740818 <a title="243-lda-4" href="./nips-2012-Learning_to_Discover_Social_Circles_in_Ego_Networks.html">194 nips-2012-Learning to Discover Social Circles in Ego Networks</a></p>
<p>same-paper 5 0.68596637 <a title="243-lda-5" href="./nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<p>6 0.58940077 <a title="243-lda-6" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>7 0.58878827 <a title="243-lda-7" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>8 0.58537263 <a title="243-lda-8" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>9 0.58497858 <a title="243-lda-9" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>10 0.58452886 <a title="243-lda-10" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>11 0.58416301 <a title="243-lda-11" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>12 0.58293408 <a title="243-lda-12" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>13 0.58292657 <a title="243-lda-13" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>14 0.58252883 <a title="243-lda-14" href="./nips-2012-Putting_Bayes_to_sleep.html">283 nips-2012-Putting Bayes to sleep</a></p>
<p>15 0.58229423 <a title="243-lda-15" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>16 0.58206487 <a title="243-lda-16" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>17 0.58187085 <a title="243-lda-17" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>18 0.58177131 <a title="243-lda-18" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>19 0.58140701 <a title="243-lda-19" href="./nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</a></p>
<p>20 0.58120477 <a title="243-lda-20" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
