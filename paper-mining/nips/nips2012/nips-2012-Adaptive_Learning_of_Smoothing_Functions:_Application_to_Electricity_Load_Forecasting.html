<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-35" href="#">nips2012-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</h1>
<br/><p>Source: <a title="nips-2012-35-pdf" href="http://papers.nips.cc/paper/4724-adaptive-learning-of-smoothing-functions-application-to-electricity-load-forecasting.pdf">pdf</a></p><p>Author: Amadou Ba, Mathieu Sinn, Yannig Goude, Pascal Pompey</p><p>Abstract: This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) ﬁlter. In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). e Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy. 1</p><p>Reference: <a title="nips-2012-35-reference" href="../nips2012_reference/nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. [sent-9, score-0.103]
</p><p>2 In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. [sent-11, score-0.415]
</p><p>3 The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. [sent-12, score-0.63]
</p><p>4 The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). [sent-14, score-0.741]
</p><p>5 This considerable attention comes from the ability of Additive Models to represent non-linear associations between covariates and response variables in an intuitive way, and the availability of efﬁcient training methods. [sent-17, score-0.086]
</p><p>6 The fundamental assumption of Additive Models is that the effect of covariates on the dependent variable follows an additive form. [sent-18, score-0.207]
</p><p>7 The separate effects are modeled by smoothing splines, which can be learned using penalized least squares. [sent-19, score-0.103]
</p><p>8 A particularly fruitful ﬁeld for the application of Additive Models is the modeling and forecasting of short term electricity load. [sent-20, score-0.557]
</p><p>9 Additive Models were applied, with good results, to the nation-wide load in France [11] and to regional loads in Australia [12]. [sent-22, score-0.41]
</p><p>10 Besides electricity load, Additive Models have also been applied to natural gas demand [13]. [sent-23, score-0.453]
</p><p>11 Several methods have been proposed to track time-varying behaviour of the smoothing splines in Additive Models. [sent-24, score-0.242]
</p><p>12 A componentwise smoothing spline is suggested by Chiang et al. [sent-29, score-0.236]
</p><p>13 Fan and Zhang [17] propose a two-stage algorithm which ﬁrst computes raw  estimates of the smoothing functions at different time points and then smoothes the estimates. [sent-31, score-0.103]
</p><p>14 In [19], an algorithm based on iterative QR decompositions is proposed, which yields promising results for the French electricity load but also highlights the need for a forgetting factor to be more reactive, e. [sent-35, score-1.156]
</p><p>15 Harvey and Koopman [20] propose an adaptive learning method which is restricted to changing periodic patterns. [sent-38, score-0.083]
</p><p>16 The contributions of our paper are threefold: First, we introduce a new algorithm which combines Additive Models with a Recursive Least Squares (RLS) ﬁlter to track time-varying behaviour of the smoothing splines. [sent-40, score-0.146]
</p><p>17 Second, in order to enhance the tracking ability, we consider ﬁlters that include a forgetting factor which can be either ﬁxed, or updapted using a gradient descent approach [23]. [sent-41, score-0.469]
</p><p>18 The basic idea is to decrease the forgetting factor (and hence increase the reactivity) in transient phases, and increasing the forgetting factor (thus decreasing the variability) during stationary regimes. [sent-42, score-0.83]
</p><p>19 Third, we evaluate the proposed methodology on 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). [sent-44, score-0.741]
</p><p>20 The results show that e the adaptive learning algorithm outperforms state-of-the-art methods in terms of model tracking and prediction accuracy. [sent-45, score-0.137]
</p><p>21 Moreover, the experiments demonstrate that using an adaptive forgetting factor stabilizes the algorithm and yields results comparable to those obtained by using the (a priori unknown) optimal value for a ﬁxed forgetting factor. [sent-46, score-0.901]
</p><p>22 The reason is that we are speciﬁcally interested in adaptive versions of Additive Models, which have been shown to be particularly well-suited for modeling and forecasting electricity load. [sent-48, score-0.64]
</p><p>23 Section 2 reviews the deﬁnition of Additive Models and provides some background on the spline representation of smoothing functions. [sent-50, score-0.236]
</p><p>24 In Section 3 we present our adaptive learning algorithms which combine Additive Models with a Recursive Least Squares (RLS) ﬁlter. [sent-51, score-0.083]
</p><p>25 We discuss different approaches for including forgetting factors and analyze the learning rate for the gradient descent method in the adaptive forgetting factor approach. [sent-52, score-0.866]
</p><p>26 A case study with real electricity load data from EDF is presented in Section 4. [sent-53, score-0.741]
</p><p>27 2  Additive Models  In this section we review the Additive Models and provide background information on the spline representation of smoothing functions. [sent-55, score-0.236]
</p><p>28 Additive Models have the following form: I  yk  =  fi (xk ) +  k. [sent-56, score-0.073]
</p><p>29 i=1  In this formulation, xk is a vector of covariates which can be either categorical or continuous, and yk is the dependent variable, which is assumed to be continuous. [sent-57, score-0.302]
</p><p>30 The functions fi are the transfer functions of the model, which can be of the following types: constant (exactly one transfer function, representing the intercept of the model), categorical (evaluating to 0 or 1 depending on whether the covariates satisfy certain conditions), or continuous. [sent-59, score-0.159]
</p><p>31 The continuous transfer functions can be either linear functions of covariates (representing simple linear trends), or smoothing splines. [sent-60, score-0.189]
</p><p>32 Typically, smoothing splines depend on only 1-2 of the continuous covariates. [sent-61, score-0.199]
</p><p>33 An interesting possibility is to combine smoothing splines with categorical conditions; in the context of electricity load modeling this allows, e. [sent-62, score-0.971]
</p><p>34 , for having different effects of the time of the day depending on the day of the week. [sent-64, score-0.116]
</p><p>35 Note that the basis functions are deﬁned by a (ﬁxed) sequence of knot points, while the coefﬁcients are used to ﬁt the spline to the data (see [1] for details). [sent-66, score-0.133]
</p><p>36 The quantity Ji in equation (1) is the number of spline coefﬁcients associated with the transfer function fi . [sent-67, score-0.133]
</p><p>37 Now, let β denote the stacked vector containing the spline coefﬁcients, and b(xk ) the stacked vector containing the spline basis functions of all the transfer functions. [sent-68, score-0.266]
</p><p>38 , b(xK )T containing the evaluated spline basis functions. [sent-83, score-0.133]
</p><p>39 In this paper, we consider two scenarios: ΩK is the identity matrix (putting equal weight on the K regressors), or a diagonal matrix which puts exponentially decreasing weights on the samples, according to the order of their arrival (thus giving rise to the notion of forgetting factors). [sent-85, score-0.368]
</p><p>40 The matrix S K in (3) introduces a penalizing term in order to avoid overﬁtting of the smoothing splines. [sent-87, score-0.103]
</p><p>41 Note that this penalizer shrinks the smoothing splines towards zero functions, and the strength of this effect is tuned by γ. [sent-92, score-0.34]
</p><p>42 K K  (5)  Adaptive learning of smoothing functions  Equation (5) gives rise to an efﬁcient batch learning algorithm for Additive Models. [sent-94, score-0.103]
</p><p>43 Next, we propose an adaptive method which allows us to track changes in the smoothing functions in an online fashion. [sent-95, score-0.186]
</p><p>44 To improve the tracking behaviour, we introduce a forgetting factor which puts more weight on recent samples. [sent-97, score-0.469]
</p><p>45 The initial precision matrix P 0 is set equal to the inverse of the penalizer S in (4). [sent-102, score-0.109]
</p><p>46 Let us discuss the role of the forgetting factor ω in the adaptive learning algorithm. [sent-104, score-0.498]
</p><p>47 , ω 2 , ω, 1) and the penalizer S as deﬁned in (4). [sent-108, score-0.109]
</p><p>48 In general, a smaller forgetting factor improves the tracking of temporal changes in the model coefﬁcients β. [sent-111, score-0.469]
</p><p>49 Therefore, ﬁnding the right balance between the forgetting factor ω and the strength γ of the penalizer in (4) is crucial for a good performance of the forecasting algorithm. [sent-113, score-0.727]
</p><p>50 Algorithm 1 Adaptive learning (ﬁxed forgetting factor) 1: Input: Initial estimate β 0 , forgetting factor ω ∈ (0, 1], penalizer strength γ > 0. [sent-114, score-0.924]
</p><p>51 do 4: Obtain new covariates xk and dependent variable yk . [sent-122, score-0.271]
</p><p>52 5: Compute the spline basis functions bk = b(xk ). [sent-123, score-0.163]
</p><p>53 6: Compute the a priori error and the Kalman gain: k  gk 7:  = yk − bT β k−1 , k P k−1 bk = . [sent-124, score-0.138]
</p><p>54 k  8: end for  Algorithm 2 Adaptive learning (adaptive forgetting factor) 1: Input: Initial estimate β 0 , initial forgetting factor ω0 ∈ (0, 1], lower bound for the forgetting  factor ωmin ∈ (0, 1], learning rate η > 0, penalizer strength γ > 0. [sent-126, score-1.339]
</p><p>55 6: Update the forgetting factor:  ωk 7: 8:  = ωk−1 + η bT ψ k−1 k . [sent-133, score-0.368]
</p><p>56 1  Adaptive forgetting factors  In this section we present a modiﬁcation of Algorithm 1 which uses adaptive forgetting factors in order to improve the stability and the tracking behaviour. [sent-139, score-0.927]
</p><p>57 The basic idea is to choose a large forgetting factor during stationary regimes (when the a priori errors are small), and small forgetting factors during transient phases (when the a priori error is large). [sent-140, score-0.853]
</p><p>58 In this paper we adopt the gradient descent approach in [23] and update the forgetting factor according to the following formula: ωk  = ωk−1 − η  2 ∂ E[ k ] . [sent-141, score-0.415]
</p><p>59 The learning rate η > 0 determines the reactivity of the algorithm: if it is high, then the errors lead to large decreases of the forgetting factor, and vice versa. [sent-143, score-0.395]
</p><p>60 The details of the adaptive forgetting factor approach are given in Algorithm 2. [sent-144, score-0.498]
</p><p>61 Recall the deﬁnition of the a priori error, k = yk − bT β k−1 . [sent-149, score-0.108]
</p><p>62 bT ψ k−1 k  Case study: Forecasting of electricity load  In this section, we apply our adaptive learning algorithms to real electricity load data provided by the French utility company Electricit´ de France (EDF). [sent-162, score-1.565]
</p><p>63 Modeling and forecasting electricity load e is a challenging task due to the non-linear effects, e. [sent-163, score-0.912]
</p><p>64 Moreover, the electricity load exhibits many non-stationary patterns, e. [sent-166, score-0.741]
</p><p>65 , due to changing macroeconomic conditions (leading to an increase/decrease in electricity demand), or varying customer portfolios resulting from the liberalization of European electricity markets. [sent-168, score-0.799]
</p><p>66 The performance on these highly complex, non-linear and non-stationary learning tasks is a challenging benchmark for our adaptive algorithms. [sent-169, score-0.083]
</p><p>67 1  Experimental data  The dependent variables yk in the data provided by EDF represent half-hourly electricity load measurements between February 2, 2006 and April 6, 2011. [sent-171, score-0.814]
</p><p>68 The covariates xk include the following information: xk  =  xDayType , xTimeOfDay , xTimeOfYear , xTemperature , xCloudCover , xLoadDecrease . [sent-172, score-0.31]
</p><p>69 k k k k k k  Let us explain these components in more detail: • xDayType is a categorical variable representing the day type: 1 for Sunday, 2 for Monday, 3 k for Tuesday-Wednesday-Thursday, 4 for Friday, 5 for Saturday, and 6 for bank holidays. [sent-173, score-0.089]
</p><p>70 • xTemperature and xCloudCover represent the temperature and the cloud cover (ranging from 0 for a k k blue sky to 8 for overcast). [sent-179, score-0.088]
</p><p>71 These meteorological covariates have been provided by M´ t´ o ee France; the raw data include temperature and cloud cover data recorded every 3 hours from 26 weather stations all over France. [sent-180, score-0.201]
</p><p>72 A weighted average – the weights reﬂecting the importance of a region in terms of the national electricity load – is computed to obtain the national temperature and cloud cover covariates. [sent-182, score-0.829]
</p><p>73 • xLoadDecrease contains information about the activation of contracts between EDF and some k big customers to reduce the electricity load during peak days. [sent-183, score-0.741]
</p><p>74 • f LagLoad (yk−48 ) takes into account the electricity load of the previous day. [sent-190, score-0.741]
</p><p>75 • f CloudCover (xk ) and f Temperature/TimeOfDay (xk ) represent respectively the effect of the cloud cover and the bivariate effect of the temperature and the time of the day. [sent-192, score-0.088]
</p><p>76 • f TimeOfYear (xk ) represents yearly cycles, and xLoadDecrease f LoadDecrease (xk ) models the effect of k contracts to reduce peak loads depending on the time of the day. [sent-194, score-0.096]
</p><p>77 For more information about the design of models for electricity data we refer to [19, 11]. [sent-196, score-0.386]
</p><p>78 Figure 1 shows the estimated joint effect of the temperature and the time of the day, and the estimated yearly cycle. [sent-197, score-0.092]
</p><p>79 high) temperatures lead to an increase of the electricity load due to electrical heating (resp. [sent-199, score-0.809]
</p><p>80 cooling), whereas temperatures between 10◦ and 20◦ Celsius have almost no effect on the electricity load. [sent-200, score-0.413]
</p><p>81 Due to the widespread usage of electrical heating and relatively low usage of air conditioning in France, the effect of heating is approximately four times higher than the effect of cooling. [sent-201, score-0.082]
</p><p>82 The yearly cycle reveals a strong decrease of the electricity load during the summer and Christmas holidays (around 0. [sent-202, score-0.782]
</p><p>83 The ﬁtted model consists of 268 spline basis coefﬁcients, which indicates the complexity of modeling electricity load data. [sent-209, score-0.874]
</p><p>84 q q q qq q q q q q qq q q qq qq q qq q q q q q q q q q q q qq q q q qq q qq q q q q q q q q q q q q q q q qq qq q q q q  1. [sent-210, score-4.07]
</p><p>85 5  q  q q q qq q q q q q q q q qq q q q  q  q q q q q q  q q q qq q q q q q q q q q q q q q qq q qq q qq q q q q q q q q q q q q q q q q q  q  E  q  Y  C  0. [sent-212, score-2.442]
</p><p>86 Values of the forgetting factor close to 1 result in reduced tracking behaviour and less improvement over the ofﬂine approach. [sent-214, score-0.512]
</p><p>87 Choosing too small values for the forgetting factor can lead to loss of information and instabilities of the algorithm. [sent-215, score-0.415]
</p><p>88 Increasing the penalizer reduces the variability of the smoothing splines, however, it also introduces a bias as the splines are shrinked towards zero. [sent-216, score-0.308]
</p><p>89 0  Table 1: Performance of the ﬁve different forecasting methods  −1. [sent-228, score-0.171]
</p><p>90 We have introduced methods to improve the tracking behaviour based on forgetting factors and analyzed theoretical properties using results from Lyapunov stability theory. [sent-230, score-0.519]
</p><p>91 The signiﬁcance of the proposed algorithms was demonstrated in the context of forecasting electricity load data. [sent-231, score-0.912]
</p><p>92 Experiments on 5 years of data from Electricit´ de France e have shown the superior performance of algorithms using an adaptive forgetting factor. [sent-233, score-0.451]
</p><p>93 As it turned out, a crucial point is to ﬁnd the right combination of forgetting factors and the strength of the penalizer. [sent-234, score-0.4]
</p><p>94 While forgetting factors tend to reduce the bias of models evolving over time, they typically increase the variance, an effect which can be compensated by choosing stronger penalizer. [sent-235, score-0.368]
</p><p>95 , by integrating beliefs for the initial values of the adaptive algorithms. [sent-239, score-0.083]
</p><p>96 Modeling electricity loads in California: ARMA models with hyperbolic noise. [sent-244, score-0.441]
</p><p>97 Short-term load forecasting based on an adaptive hybrid method . [sent-273, score-0.609]
</p><p>98 A statistical model for natural gas standardized load proﬁles. [sent-286, score-0.382]
</p><p>99 Nonparametric smoothing estimates of time-varying coefﬁcient models with longitudinal data. [sent-292, score-0.103]
</p><p>100 Smoothing spline estimation for varying coefﬁcient models with repeatedly measured dependent variables. [sent-306, score-0.133]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qq', 0.407), ('electricity', 0.386), ('forgetting', 0.368), ('load', 0.355), ('forecasting', 0.171), ('forge', 0.157), ('fac', 0.145), ('spline', 0.133), ('additive', 0.121), ('xk', 0.112), ('penalizer', 0.109), ('smoothing', 0.103), ('splines', 0.096), ('edf', 0.096), ('hod', 0.096), ('ng', 0.094), ('bt', 0.092), ('covariates', 0.086), ('lyapunov', 0.086), ('adaptive', 0.083), ('adap', 0.082), ('mape', 0.082), ('yk', 0.073), ('gor', 0.072), ('deno', 0.068), ('erms', 0.068), ('fff', 0.068), ('reng', 0.068), ('zer', 0.068), ('qqq', 0.063), ('rls', 0.063), ('transaction', 0.063), ('pena', 0.06), ('day', 0.058), ('aff', 0.055), ('bes', 0.055), ('electricit', 0.055), ('hms', 0.055), ('loads', 0.055), ('xloaddecrease', 0.055), ('tracking', 0.054), ('stability', 0.054), ('temperature', 0.051), ('france', 0.049), ('se', 0.049), ('va', 0.049), ('ve', 0.048), ('wh', 0.048), ('factor', 0.047), ('ch', 0.043), ('behaviour', 0.043), ('intercept', 0.042), ('comb', 0.042), ('fan', 0.042), ('affg', 0.041), ('heating', 0.041), ('parame', 0.041), ('resu', 0.041), ('upda', 0.041), ('xdaytype', 0.041), ('yearly', 0.041), ('demand', 0.04), ('rmse', 0.037), ('cloud', 0.037), ('eva', 0.036), ('mulhuddart', 0.036), ('squares', 0.035), ('priori', 0.035), ('na', 0.034), ('da', 0.034), ('dublin', 0.033), ('strength', 0.032), ('ireland', 0.031), ('categorical', 0.031), ('french', 0.031), ('pos', 0.03), ('bk', 0.03), ('coef', 0.029), ('bra', 0.027), ('cloudcover', 0.027), ('colin', 0.027), ('daytype', 0.027), ('eubank', 0.027), ('fltimeofday', 0.027), ('forecas', 0.027), ('gas', 0.027), ('goude', 0.027), ('jianqing', 0.027), ('lagload', 0.027), ('lagtemperature', 0.027), ('loaddecrease', 0.027), ('meteorological', 0.027), ('mgcv', 0.027), ('mprovemen', 0.027), ('penalizers', 0.027), ('portfolios', 0.027), ('reactivity', 0.027), ('temperatures', 0.027), ('timeofyear', 0.027), ('xcloudcover', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="35-tfidf-1" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>Author: Amadou Ba, Mathieu Sinn, Yannig Goude, Pascal Pompey</p><p>Abstract: This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) ﬁlter. In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). e Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy. 1</p><p>2 0.39949682 <a title="35-tfidf-2" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>3 0.39111444 <a title="35-tfidf-3" href="./nips-2012-Semi-Supervised_Domain_Adaptation_with_Non-Parametric_Copulas.html">308 nips-2012-Semi-Supervised Domain Adaptation with Non-Parametric Copulas</a></p>
<p>Author: David Lopez-paz, Jose M. Hernández-lobato, Bernhard Schölkopf</p><p>Abstract: A new framework based on the theory of copulas is proposed to address semisupervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efﬁcacy of the proposed approach when compared to state-of-the-art techniques. 1</p><p>4 0.25105974 <a title="35-tfidf-4" href="./nips-2012-A_Conditional_Multinomial_Mixture_Model_for_Superset_Label_Learning.html">5 nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</a></p>
<p>Author: Liping Liu, Thomas G. Dietterich</p><p>Abstract: In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic StickBreaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSBCMM is derived from the logistic stick-breaking process. It ﬁrst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speciﬁc multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classiﬁcation predictions. 1</p><p>5 0.097904302 <a title="35-tfidf-5" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>6 0.071590587 <a title="35-tfidf-6" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>7 0.064862065 <a title="35-tfidf-7" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>8 0.062396668 <a title="35-tfidf-8" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>9 0.056027994 <a title="35-tfidf-9" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>10 0.050104514 <a title="35-tfidf-10" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>11 0.046872359 <a title="35-tfidf-11" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>12 0.046638012 <a title="35-tfidf-12" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>13 0.045444719 <a title="35-tfidf-13" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>14 0.044176247 <a title="35-tfidf-14" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>15 0.041650459 <a title="35-tfidf-15" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>16 0.040985107 <a title="35-tfidf-16" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>17 0.036590133 <a title="35-tfidf-17" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>18 0.035340153 <a title="35-tfidf-18" href="./nips-2012-Learning_curves_for_multi-task_Gaussian_process_regression.html">187 nips-2012-Learning curves for multi-task Gaussian process regression</a></p>
<p>19 0.035317138 <a title="35-tfidf-19" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>20 0.035176646 <a title="35-tfidf-20" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.099), (1, 0.022), (2, 0.044), (3, -0.012), (4, 0.011), (5, 0.027), (6, 0.557), (7, -0.077), (8, 0.047), (9, 0.04), (10, -0.001), (11, 0.001), (12, 0.002), (13, -0.008), (14, -0.02), (15, -0.008), (16, -0.045), (17, 0.016), (18, -0.001), (19, -0.034), (20, 0.003), (21, 0.019), (22, -0.037), (23, -0.056), (24, -0.0), (25, 0.027), (26, 0.008), (27, -0.009), (28, 0.045), (29, -0.028), (30, 0.003), (31, -0.001), (32, -0.001), (33, 0.046), (34, 0.046), (35, 0.042), (36, -0.018), (37, 0.002), (38, 0.005), (39, -0.001), (40, -0.02), (41, -0.03), (42, 0.001), (43, -0.036), (44, 0.034), (45, 0.001), (46, 0.013), (47, -0.011), (48, -0.051), (49, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93676978 <a title="35-lsi-1" href="./nips-2012-Semi-Supervised_Domain_Adaptation_with_Non-Parametric_Copulas.html">308 nips-2012-Semi-Supervised Domain Adaptation with Non-Parametric Copulas</a></p>
<p>Author: David Lopez-paz, Jose M. Hernández-lobato, Bernhard Schölkopf</p><p>Abstract: A new framework based on the theory of copulas is proposed to address semisupervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efﬁcacy of the proposed approach when compared to state-of-the-art techniques. 1</p><p>same-paper 2 0.92582542 <a title="35-lsi-2" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>Author: Amadou Ba, Mathieu Sinn, Yannig Goude, Pascal Pompey</p><p>Abstract: This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) ﬁlter. In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). e Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy. 1</p><p>3 0.92422903 <a title="35-lsi-3" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>4 0.75277853 <a title="35-lsi-4" href="./nips-2012-A_Conditional_Multinomial_Mixture_Model_for_Superset_Label_Learning.html">5 nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</a></p>
<p>Author: Liping Liu, Thomas G. Dietterich</p><p>Abstract: In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic StickBreaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSBCMM is derived from the logistic stick-breaking process. It ﬁrst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speciﬁc multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classiﬁcation predictions. 1</p><p>5 0.25520298 <a title="35-lsi-5" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>6 0.23634806 <a title="35-lsi-6" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>7 0.22652346 <a title="35-lsi-7" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>8 0.21601129 <a title="35-lsi-8" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>9 0.20006014 <a title="35-lsi-9" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>10 0.19539116 <a title="35-lsi-10" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>11 0.19121838 <a title="35-lsi-11" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>12 0.18108621 <a title="35-lsi-12" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>13 0.17232262 <a title="35-lsi-13" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>14 0.16990305 <a title="35-lsi-14" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>15 0.16635789 <a title="35-lsi-15" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>16 0.16310537 <a title="35-lsi-16" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>17 0.16218388 <a title="35-lsi-17" href="./nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">299 nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>18 0.15396158 <a title="35-lsi-18" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>19 0.15221372 <a title="35-lsi-19" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>20 0.15209258 <a title="35-lsi-20" href="./nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">20 nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (21, 0.033), (36, 0.485), (38, 0.06), (42, 0.011), (54, 0.032), (55, 0.018), (74, 0.018), (76, 0.11), (80, 0.054), (92, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74785626 <a title="35-lda-1" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>Author: Amadou Ba, Mathieu Sinn, Yannig Goude, Pascal Pompey</p><p>Abstract: This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) ﬁlter. In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). e Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy. 1</p><p>2 0.61034107 <a title="35-lda-2" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>Author: Amir Sani, Alessandro Lazaric, Rémi Munos</p><p>Abstract: Stochastic multi–armed bandits solve the Exploration–Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk–aversion where the objective is to compete against the arm with the best risk–return trade–off. This setting proves to be more difﬁcult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we deﬁne two algorithms, investigate their theoretical guarantees, and report preliminary empirical results. 1</p><p>3 0.58826351 <a title="35-lda-3" href="./nips-2012-Rational_inference_of_relative_preferences.html">288 nips-2012-Rational inference of relative preferences</a></p>
<p>Author: Nisheeth Srivastava, Paul R. Schrater</p><p>Abstract: Statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them optionspeciﬁc scalar utility functions. However, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented. In this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects. Complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function. 1</p><p>4 0.43696812 <a title="35-lda-4" href="./nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric</p><p>Abstract: We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: ﬁxed budget and ﬁxed conﬁdence. We propose a unifying approach that leads to a meta-algorithm called uniﬁed gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing ﬁxed budget and ﬁxed conﬁdence algorithms. 1</p><p>5 0.41605806 <a title="35-lda-5" href="./nips-2012-Structure_estimation_for_discrete_graphical_models%3A_Generalized_covariance_matrices_and_their_inverses.html">326 nips-2012-Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses</a></p>
<p>Author: Po-ling Loh, Martin J. Wainwright</p><p>Abstract: We investigate a curious relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reﬂects the conditional independence structure of the graph. Our work extends results that have previously been established only in the context of multivariate Gaussian graphical models, thereby addressing an open question about the signiﬁcance of the inverse covariance matrix of a non-Gaussian distribution. Based on our population-level results, we show how the graphical Lasso may be used to recover the edge structure of certain classes of discrete graphical models, and present simulations to verify our theoretical results. 1</p><p>6 0.41411358 <a title="35-lda-6" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>7 0.34487239 <a title="35-lda-7" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>8 0.31536043 <a title="35-lda-8" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>9 0.31456625 <a title="35-lda-9" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>10 0.30834466 <a title="35-lda-10" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>11 0.30566037 <a title="35-lda-11" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>12 0.30432209 <a title="35-lda-12" href="./nips-2012-Online_allocation_and_homogeneous_partitioning_for_piecewise_constant_mean-approximation.html">261 nips-2012-Online allocation and homogeneous partitioning for piecewise constant mean-approximation</a></p>
<p>13 0.30348781 <a title="35-lda-13" href="./nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">132 nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<p>14 0.30152097 <a title="35-lda-14" href="./nips-2012-A_Marginalized_Particle_Gaussian_Process_Regression.html">11 nips-2012-A Marginalized Particle Gaussian Process Regression</a></p>
<p>15 0.299503 <a title="35-lda-15" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>16 0.29331762 <a title="35-lda-16" href="./nips-2012-Value_Pursuit_Iteration.html">358 nips-2012-Value Pursuit Iteration</a></p>
<p>17 0.29305348 <a title="35-lda-17" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>18 0.29140607 <a title="35-lda-18" href="./nips-2012-The_Coloured_Noise_Expansion_and_Parameter_Estimation_of_Diffusion_Processes.html">336 nips-2012-The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes</a></p>
<p>19 0.2912091 <a title="35-lda-19" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>20 0.29072803 <a title="35-lda-20" href="./nips-2012-Spectral_learning_of_linear_dynamics_from_generalised-linear_observations_with_application_to_neural_population_data.html">321 nips-2012-Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
