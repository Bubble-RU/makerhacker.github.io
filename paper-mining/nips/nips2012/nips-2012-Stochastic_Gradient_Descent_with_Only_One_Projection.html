<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>324 nips-2012-Stochastic Gradient Descent with Only One Projection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-324" href="#">nips2012-324</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>324 nips-2012-Stochastic Gradient Descent with Only One Projection</h1>
<br/><p>Source: <a title="nips-2012-324-pdf" href="http://papers.nips.cc/paper/4797-stochastic-gradient-descent-with-only-one-projection.pdf">pdf</a></p><p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, Jinfeng Yi</p><p>Abstract: Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. 1</p><p>Reference: <a title="nips-2012-324-reference" href="../nips2012_reference/nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. [sent-5, score-0.658]
</p><p>2 , positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. [sent-8, score-0.54]
</p><p>3 We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. [sent-9, score-0.232]
</p><p>4 Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. [sent-10, score-0.259]
</p><p>5 Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. [sent-11, score-0.83]
</p><p>6 To ﬁnd a solution within the domain K that optimizes the given objective function f (x), SGD computes an unbiased estimate of the gradient of f (x), and updates the solution by moving it in the opposite direction of the estimated gradient. [sent-15, score-0.313]
</p><p>7 To ensure that the solution stays within the domain K, SGD has to project the updated solution back into the K at every iteration. [sent-16, score-0.172]
</p><p>8 Although efﬁcient algorithms have been developed for projecting solutions into special domains (e. [sent-17, score-0.137]
</p><p>9 The central theme of this paper is to develop a SGD based method that does not require projection at each iteration. [sent-21, score-0.191]
</p><p>10 But, one main shortcoming of the algo1  rithm proposed in [10] is that it has a slower convergence rate (i. [sent-23, score-0.163]
</p><p>11 In this work, we demonstrate that a properly modiﬁed SGD algorithm can achieve the optimal convergence rate of O(T −1/2 ) using only ONE projection for general stochastic convex optimization problem. [sent-28, score-0.697]
</p><p>12 We further develop an SGD based algorithm for strongly convex optimization that achieves a convergence rate of O(ln T /T ), which is only a logarithmic factor worse than the optimal rate [9]. [sent-29, score-0.615]
</p><p>13 The key idea of both algorithms is to appropriately penalize the intermediate solutions when they are outside the domain. [sent-30, score-0.14]
</p><p>14 With an appropriate design of penalization mechanism, the average solution xT obtained by the SGD after T iterations will be very close to the domain K, even without intermediate projections. [sent-31, score-0.191]
</p><p>15 As a result, the ﬁnal feasible solution xT can be obtained by projecting xT into the domain K, the only projection that is needed for the entire algorithm. [sent-32, score-0.352]
</p><p>16 We note that our approach is very different from the previous efforts in developing projection free convex optimization algorithms (see [8, 12, 11] and references therein), where the key idea is to develop appropriate updating procedures to restore the feasibility of solutions at every iteration. [sent-33, score-0.578]
</p><p>17 The proposed algorithm achieves the optimal convergence rate of √ O(1/ T ) with only one projection; • We propose a stochastic gradient descent algorithm for strongly convex optimization that constructs the penalty function using a smoothing technique. [sent-35, score-0.868]
</p><p>18 This algorithm attains an O(ln T /T ) convergence rate with only one projection. [sent-36, score-0.162]
</p><p>19 2  Related Works  Generally, the computational complexity of the projection step in SGD has seldom been taken into account in the literature. [sent-37, score-0.156]
</p><p>20 Here, we brieﬂy review the previous works on projection free convex optimization, which is closely related to the theme of this study. [sent-38, score-0.375]
</p><p>21 For some speciﬁc domains, efﬁcient algorithms have been developed to circumvent the high computational cost caused by projection step at each iteration of gradient descent methods. [sent-39, score-0.355]
</p><p>22 Clarkson [5] proposed a sparse greedy approximation algorithm for convex optimization over a simplex domain, which is a generalization of an old algorithm by Frank and Wolfe [7] (a. [sent-41, score-0.35]
</p><p>23 Zhang [21] introduced a similar sequential greedy approximation algorithm for certain convex optimization problems over a domain given by a convex hull. [sent-44, score-0.536]
</p><p>24 Hazan [8] devised an algorithm for approximately maximizing a concave function over a trace norm bounded PSD cone, which only needs to compute the maximum eigenvalue and the corresponding eigenvector of a symmetric matrix. [sent-45, score-0.168]
</p><p>25 Recently, Jaggi [11] put these ideas into a general framework for convex optimization with a general convex domain. [sent-48, score-0.415]
</p><p>26 Instead of projecting the intermediate solution into a complex convex domain, Jaggi’s algorithm solves a linearized problem over the same domain. [sent-49, score-0.344]
</p><p>27 It is a projection free online learning algorithm, built on the the assumption that it is possible to efﬁciently minimize a linear function over the complex domain. [sent-53, score-0.263]
</p><p>28 One main shortcoming of the OFW algorithm is that its convergence rate for general stochastic optimization is O(T −1/3 ), signiﬁcantly slower than that of a standard stochastic gradient descent algorithm (i. [sent-54, score-0.681]
</p><p>29 It achieves a convergence rate of O(T −1/2 ) only when the objective function is smooth, which unfortunately does not hold for many machine learning problems where either a non-smooth regularizer or a non-smooth loss function is used. [sent-57, score-0.178]
</p><p>30 Another limitation of OFW is that it assumes a linear optimization problem over the domain K can be solved efﬁciently. [sent-58, score-0.155]
</p><p>31 2  3  Preliminaries  Throughout this paper, we consider the following convex optimization problem: min f (x), x∈K  (1)  where K is a bounded convex domain. [sent-62, score-0.486]
</p><p>32 We assume that K can be characterized by an inequality constraint and without loss of generality is bounded by the unit ball, i. [sent-63, score-0.135]
</p><p>33 , K = {x ∈ Rd : g(x) ≤ 0} ⊆ B = {x ∈ Rd : x 2 ≤ 1}, (2) where g(x) is a convex constraint function. [sent-65, score-0.203]
</p><p>34 Note that when a domain is characterized by multiple convex constraint functions, say gi (x) ≤ 0, i = 1, . [sent-71, score-0.294]
</p><p>35 To solve the optimization problem in (1), we assume that the only information available to the algorithm is through a stochastic oracle that provides unbiased estimates of the gradient of f (x). [sent-75, score-0.381]
</p><p>36 At each iteration t, given solution xt , the oracle returns f (xt ; ξt ), an unbiased estimate of the true gradient f (xt ), i. [sent-82, score-0.901]
</p><p>37 Before proceeding, we recall a few deﬁnitions from convex analysis [17]. [sent-86, score-0.164]
</p><p>38 (3)  In particular, a convex function f (x) with a bounded (sub)gradient ∂f (x) ∗ ≤ G is G-Lipschitz continuous, where · ∗ is the dual norm to · . [sent-91, score-0.277]
</p><p>39 In the sequel, we use the standard Euclidean norm to deﬁne 2 Lipschitz and strongly convex functions. [sent-97, score-0.282]
</p><p>40 Stochastic gradient descent method is an iterative algorithm and produces a sequence of solutions xt , t = 1, . [sent-98, score-0.921]
</p><p>41 (5) 2 √ For general convex optimization, stochastic gradient descent methods can obtain an O(1/ T ) convergence rate in expectation or in a high probability provided (5) [16]. [sent-102, score-0.578]
</p><p>42 As we mentioned in the Introduction section, SGD methods are computationally efﬁcient only when the projection ΠK (x) can be carried out efﬁciently. [sent-103, score-0.156]
</p><p>43 The objective of this work is to develop computationally efﬁcient stochastic optimization algorithms that are able to yield the same performance guarantee as the standard SGD algorithm but with only ONE projection when applied to the problem in (1). [sent-104, score-0.394]
</p><p>44 4  Algorithms and Main Results  We now turn to extending the SGD method to the setting where only one projection is allowed to perform for the entire sequence of updating. [sent-105, score-0.177]
</p><p>45 The main idea is to incorporate the constraint function g(x) into the objective function to penalize the intermediate solutions that are outside the domain. [sent-106, score-0.201]
</p><p>46 A projection is performed at the end of the iterations to restore the feasibility of the average solution. [sent-108, score-0.247]
</p><p>47 , T do 4: Compute xt+1 = xt − ηt ( f (xt , ξt ) + λt g(xt )) 5: Update xt+1 = xt+1 / max ( xt+1 2 , 1), 6: Update λt+1 = [(1 − γηt )λt + ηt g(xt )]+ 7: end for T 8: Output: xT = ΠK (xT ), where xT = t=1 xt /T . [sent-112, score-1.36]
</p><p>48 The key ingredient of proposed algorithms is to replace the projection step with the gradient computation of the constraint function deﬁning the domain K, which is signiﬁcantly cheaper than projection step. [sent-113, score-0.551]
</p><p>49 , X 0 where X is a symmetric matrix, the corresponding inequality constraint is g(X) = λmax (−X) ≤ 0, where λmax (X) computes the largest eigenvalue of X and is a convex function. [sent-116, score-0.291]
</p><p>50 In this case, g(X) only requires computing the minimum eigenvector of a matrix, which is cheaper than a full eigenspectrum computation required at each iteration of the standard SGD algorithm to restore feasibility. [sent-117, score-0.164]
</p><p>51 Below, we state a few assumptions about f (x) and g(x) often made in stochastic optimization as: A1  f (x)  2  ≤ G1 ,  g(x)  2  ≤ G2 ,  |g(x)| ≤ C2 ,  ∀x ∈ B,  2 2 2 /σ )]  A2 Eξt [exp( f (x, ξt ) − f (x) ≤ exp(1), ∀x ∈ B. [sent-118, score-0.221]
</p><p>52 We also make the following mild assumption about the boundary of the convex domain K as: A3  there exists a constant ρ > 0 such that min  g(x)=0  g(x)  2  ≥ ρ. [sent-119, score-0.333]
</p><p>53 The purpose of introducing assumption A3 is to ensure that the optimal dual variable for the constrained optimization problem in (1) is well bounded from the above, a key factor for our analysis. [sent-121, score-0.214]
</p><p>54 To see this, we write the problem in (1) into a convex-concave optimization problem: min max f (x) + λg(x). [sent-122, score-0.135]
</p><p>55 x∈B λ≥0  Let (x∗ , λ∗ ) be the optimal solution to the above convex-concave optimization problem. [sent-123, score-0.152]
</p><p>56 We propose two different ways of incorporating the constraint function into the objective function, which result in two algorithms, one for general convex and the other for strongly convex functions. [sent-130, score-0.477]
</p><p>57 Instead of solving the constrained optimization problem in (1), we try to solve the following convex-concave optimization problem min max L(x, λ). [sent-135, score-0.222]
</p><p>58 It differs from the existing stochastic gradient descent methods in that it updates both the primal variable x (steps 4 and 5) and the dual variable λ (step 6), which shares the same step sizes. [sent-137, score-0.372]
</p><p>59 It is noticeable that a similar primal-dual updating is explored in [15] to avoid projection in online learning. [sent-139, score-0.228]
</p><p>60 Our work differs from [15] in that their algorithm and analysis only lead to a bound for the regret and the violation of the constraints in a long run, which does not necessarily guarantee the feasibility of ﬁnal solution. [sent-140, score-0.16]
</p><p>61 Also our proof techniques differ from [16], where the convergence rate is obtained for the saddle point; however our goal is to attain bound on the convergence of the primal feasible solution. [sent-141, score-0.345]
</p><p>62 This is because, in order to obtain a regret of O( T ), we need a to set γ = Ω( T ), which unfortunately will lead to a blowup of the gradients and consequently a poor regret bound. [sent-145, score-0.194]
</p><p>63 Using a primal-dual updating schema allows us to adjust the penalization term √ more carefully to obtain an O(1/ T ) convergence rate. [sent-146, score-0.154]
</p><p>64 2  SGD with One Projection for Strongly Convex Optimization  We ﬁrst emphasize that it is difﬁcult to extend Algorithm 1 to achieve an O(ln T /T ) convergence rate for strongly convex optimization. [sent-150, score-0.389]
</p><p>65 This is because although −L(x, λ) is strongly convex in λ, its modulus for strong convexity is γ, which is too small to obtain an O(ln T ) regret bound. [sent-151, score-0.404]
</p><p>66 To achieve a faster convergence rate for strongly convex optimization, we change assumptions A1 and A2 to A4 f (x, ξt ) 2 ≤ G1 , g(x) 2 ≤ G2 , ∀x ∈ B, where we slightly abuse the same notation G1 . [sent-152, score-0.419]
</p><p>67 Note that A1 only requires that f (x) 2 is bounded and A2 assumes a mild condition on the stochastic gradient. [sent-153, score-0.178]
</p><p>68 In contrast, for strongly convex optimization we need to assume a bound on the stochastic gradient f (x, ξt ) 2 . [sent-154, score-0.567]
</p><p>69 According to the discussion in the last subsection, we know that the optimal dual variable λ∗ is upper bounded by G1 /ρ, and consequently is upper bounded by λ0 . [sent-157, score-0.152]
</p><p>70 Similar to the last approach, we write the optimization problem (1) into an equivalent convexconcave optimization problem: min f (x) = min max f (x) + λg(x) = min f (x) + λ0 [g(x)]+ . [sent-158, score-0.274]
</p><p>71 , T do exp (λ0 g(xt )/γ) 4: Compute xt+1 = xt − ηt f (xt , ξt ) + λ0 g(xt ) 1 + exp(λ0 g(xt )/γ) 5: Update xt+1 = xt+1 / max( xt+1 2 , 1) 6: end for T 7: Output: xT = ΠK (xT ), where xT = t=1 xt /T . [sent-164, score-1.367]
</p><p>72 Unlike Algorithm 1, only the primal variable x is updated in each iteration using the stochastic gradient computed in (11). [sent-167, score-0.289]
</p><p>73 F (x) =  f (x) +  The following theorem shows that Algorithm 2 achieves an O(ln T /T ) convergence rate if the cost functions are strongly convex. [sent-168, score-0.275]
</p><p>74 For any β-strongly convex function f (x), if we set ηt = 1/(2βt), t = 1, . [sent-170, score-0.164]
</p><p>75 , T , γ = ln T /T , and λ0 > G1 /ρ in Algorithm 2, under assumptions A3 and A4, we have with a probability at least 1 − δ, ln T f (xT ) ≤ min f (x) + O , x∈K T where O(·) suppresses polynomial factors that depend on ln(1/δ), 1/β, G1 , G2 , ρ, and λ0 . [sent-173, score-0.826]
</p><p>76 It is well known that the optimal convergence rate of SGD for strongly convex optimization is O(1/T ) [9] which has been proven to be tight in stochastic optimization setting [1]. [sent-174, score-0.691]
</p><p>77 According to Theorem 2, Algorithm 2 achieves an almost optimal convergence rate except for the factor of ln T . [sent-175, score-0.531]
</p><p>78 It is worth mentioning that although it is not explicitly given in Theorem 2, the detailed expression for the convergence rate of Algorithm 2 exhibits a tradeoff in setting λ0 (more can be found in the √ proof of Theorem 2). [sent-176, score-0.164]
</p><p>79 Finally, under assumptions A1-A3, Algorithm 2 can achieve an O(1/ T ) convergence rate for general convex functions, similar to Algorithm 1. [sent-177, score-0.331]
</p><p>80 The lemma below states two key inequalities, which follows the standard analysis of gradient descent. [sent-183, score-0.156]
</p><p>81 For any general convex function f (x), if we set ηt = γ/(2G2 ), t = 1, · · · , T , we have 2 T  T  (f (xt ) − f (x∗ )) + t=1  2 [ t=1 g(xt )]2 G2 (G2 + C2 ) γ + 1 ≤ 2+ γT + 2 2 /γ) 2 2(γT + 2G2 γ G2 G2  where x∗ = arg minx∈K f (x). [sent-188, score-0.164]
</p><p>82 Recalling the deﬁnition of xT =  T t=1  xt /T and using the convexity of f (x) and g(x), we have √ T 1 ∗ f (xT ) − f (x ) + [g(xT )]2 ≤ O √ . [sent-195, score-0.71]
</p><p>83 If we only update the primal variable using the penalized objective in (10), whose gradient depends on 1/γ, it will cause a blowup in the regret bound with (1/γ + γT + T /γ), which leads to a non-convergent bound. [sent-208, score-0.323]
</p><p>84 2  Proof of Theorem 2  Our proof of Theorem 2 for the convergence rate of Algorithm 2 when applied to strongly convex functions starts with the following lemma by analogy of Lemma 2. [sent-210, score-0.47]
</p><p>85 For any β-strongly convex function f (x), if we set ηt = 1/(2βt), we have T  T  (F (x) − F (x∗ )) ≤ t=1  (G2 + λ2 G2 )(1 + ln T ) β 1 0 2 + ζt (x∗ ) − 2β 4 t=1  T  x∗ − xt  2 2  t=1  where x∗ = arg minx∈K f (x). [sent-212, score-1.184]
</p><p>86 We have 4 + Pr ΛT ≤ 4G1 Pr DT ≤ T  xt − x 2 , ΛT = 2  DT ln  7  m m + 4G1 ln δ δ  T t=1 ζt (x),  ≥ 1 − δ. [sent-216, score-1.371]
</p><p>87 As a result, we have T  T  ζt (x∗ ) = t=1  ( f (xt ) −  f (xt , ξt )) (x∗ − xt ) ≤ 2G1  T DT ≤ 4G1 ,  t=1  which together with the inequality in Lemma 3 leads to the bound T  (F (xt ) − F (x∗ )) ≤ 4G1 + t=1  (G2 + λ2 G2 )(1 + ln T ) 1 0 2 . [sent-221, score-1.093]
</p><p>88 2β  In the second case, we assume T  ζt (x∗ ) ≤ 4G1  DT ln  m m β + 4G1 ln ≤ DT + δ δ 4  t=1 √ where the last step uses the fact 2 ab ≤ a2 + b2 . [sent-222, score-0.702]
</p><p>89 We thus have T  (F (xt ) − F (x∗ )) ≤ t=1  16G2 m 1 + 4G1 ln , β δ  m (G2 + λ2 G2 )(1 + ln T ) 16G2 1 0 2 1 + 4G1 ln + . [sent-223, score-1.053]
</p><p>90 β δ 2β  Combing the results of the two cases, we have, with a probability 1 − δ, T  (F (xt ) − F (x∗ )) ≤ t=1  16G2 (G2 + λ2 G2 )(1 + ln T ) m 1 1 0 2 + 4G1 ln + 4G1 + . [sent-224, score-0.702]
</p><p>91 Noting that x∗ ∈ K, g(x∗ ) ≤ 0, we have F (x∗ ) ≤ f (x∗ ) + γ ln 2. [sent-226, score-0.351]
</p><p>92 On the other hand, λ0 g(xT ) F (xT ) = f (xT ) + γ ln 1 + exp ≥ f (xT ) + max (0, λ0 g(xT )) . [sent-227, score-0.402]
</p><p>93 γ Therefore, with the value of γ = ln T /T , we have ln T f (xT ) ≤ f (x∗ ) + O , (15) T ln T f (xT ) + λ0 g(xT ) ≤ f (x∗ ) + O . [sent-228, score-1.053]
</p><p>94 (16) T Applying the inequalities (13) and (14) to (16), and noting that γ = ln T /T , we have ln T λ0 ρ xT − xT 2 ≤ G1 xT − xT 2 + O . [sent-229, score-0.751]
</p><p>95 Therefore ln T ln T f (xT ) ≤ f (xT ) − f (xT ) + f (xT ) ≤ G1 xT − xT 2 + f (x∗ ) + O ≤ f (x∗ ) + O , T T where in the second inequality we use inequality (15). [sent-231, score-0.804]
</p><p>96 6  Conclusions  In the present paper, we made a progress towards making the SGD method efﬁcient by proposing a framework in which it is possible to exclude the projection steps from the SGD algorithm. [sent-232, score-0.156]
</p><p>97 We have proposed two novel algorithms to overcome the computational bottleneck of the projection step in applying SGD to optimization problems with complex domains. [sent-233, score-0.265]
</p><p>98 Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. [sent-245, score-0.292]
</p><p>99 Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. [sent-286, score-0.227]
</p><p>100 Trading regret for efﬁciency: online convex optimization with long term constraints. [sent-318, score-0.37]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xt', 0.669), ('sgd', 0.4), ('ln', 0.351), ('convex', 0.164), ('projection', 0.156), ('stochastic', 0.104), ('gradient', 0.102), ('strongly', 0.088), ('optimization', 0.087), ('regret', 0.074), ('descent', 0.071), ('rate', 0.071), ('psd', 0.069), ('domain', 0.068), ('ofw', 0.068), ('convergence', 0.066), ('cone', 0.063), ('dt', 0.058), ('primal', 0.057), ('jaggi', 0.056), ('lemma', 0.054), ('domains', 0.053), ('restore', 0.052), ('projecting', 0.051), ('inequality', 0.051), ('lagrangian', 0.05), ('inequalities', 0.049), ('suppresses', 0.048), ('ming', 0.046), ('blowup', 0.046), ('online', 0.045), ('hazan', 0.045), ('bounded', 0.045), ('solution', 0.041), ('intermediate', 0.041), ('convexity', 0.041), ('minx', 0.041), ('penalization', 0.041), ('penalize', 0.041), ('mahdavi', 0.04), ('unbiased', 0.039), ('constraint', 0.039), ('feasibility', 0.039), ('dual', 0.038), ('modulus', 0.037), ('eigenvalue', 0.037), ('feasible', 0.036), ('absorb', 0.035), ('theme', 0.035), ('solutions', 0.033), ('ying', 0.032), ('eigenvector', 0.031), ('theorem', 0.031), ('cheaper', 0.03), ('norm', 0.03), ('assumptions', 0.03), ('mild', 0.029), ('exp', 0.029), ('greedy', 0.028), ('martingale', 0.028), ('proof', 0.027), ('updating', 0.027), ('semide', 0.026), ('jin', 0.026), ('min', 0.026), ('ball', 0.026), ('shortcoming', 0.026), ('iteration', 0.026), ('boundary', 0.026), ('algorithm', 0.025), ('outside', 0.025), ('optimal', 0.024), ('oracle', 0.024), ('gi', 0.023), ('projections', 0.023), ('smoothing', 0.022), ('remark', 0.022), ('bound', 0.022), ('max', 0.022), ('stays', 0.022), ('complex', 0.022), ('objective', 0.022), ('simplex', 0.021), ('sequence', 0.021), ('icml', 0.02), ('frank', 0.02), ('unattractive', 0.02), ('coresets', 0.02), ('latin', 0.02), ('sulovsk', 0.02), ('zsh', 0.02), ('urgent', 0.02), ('schema', 0.02), ('combing', 0.02), ('jinfeng', 0.02), ('assumption', 0.02), ('summation', 0.02), ('free', 0.02), ('polynomial', 0.02), ('achieves', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="324-tfidf-1" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, Jinfeng Yi</p><p>Abstract: Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. 1</p><p>2 0.41432828 <a title="324-tfidf-2" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>Author: Marc Deisenroth, Shakir Mohamed</p><p>Abstract: Rich and complex time-series data, such as those generated from engineering systems, ﬁnancial markets, videos, or neural recordings are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires ﬂexible and accurate models. In this paper, we promote Gaussian process dynamical systems as a rich model class that is appropriate for such an analysis. We present a new approximate message-passing algorithm for Bayesian state estimation and inference in Gaussian process dynamical systems, a nonparametric probabilistic generalization of commonly used state-space models. We derive our message-passing algorithm using Expectation Propagation and provide a unifying perspective on message passing in general state-space models. We show that existing Gaussian ﬁlters and smoothers appear as special cases within our inference framework, and that these existing approaches can be improved upon using iterated message passing. Using both synthetic and real-world data, we demonstrate that iterated message passing can improve inference in a wide range of tasks in Bayesian state estimation, thus leading to improved predictions and more effective decision making. 1</p><p>3 0.31300741 <a title="324-tfidf-3" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>Author: Mathieu Sinn, Bei Chen</p><p>Abstract: Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been ﬁrst studied in [1]. The paper extends this work in two directions: ﬁrst, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identiﬁability and the uniqueness of maximum likelihood estimates are being given. 1</p><p>4 0.25663713 <a title="324-tfidf-4" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>Author: Pedro Ortega, Jordi Grau-moya, Tim Genewein, David Balduzzi, Daniel Braun</p><p>Abstract: We propose a novel Bayesian approach to solve stochastic optimization problems that involve ﬁnding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ﬁrst, doing inference over the function space and second, ﬁnding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. Given t observations of the function, the posterior can be evaluated efﬁciently in time O(t2 ) up to a multiplicative constant. Finally, we show how to apply our model to optimize a noisy, non-convex, high-dimensional objective function.</p><p>5 0.25134164 <a title="324-tfidf-5" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>Author: Sasha Rakhlin, Ohad Shamir, Karthik Sridharan</p><p>Abstract: We show a principled way of deriving online learning algorithms from a minimax analysis. Various upper bounds on the minimax value, previously thought to be non-constructive, are shown to yield algorithms. This allows us to seamlessly recover known methods and to derive new ones, also capturing such “unorthodox” methods as Follow the Perturbed Leader and the R2 forecaster. Understanding the inherent complexity of the learning problem thus leads to the development of algorithms. To illustrate our approach, we present several new algorithms, including a family of randomized methods that use the idea of a “random playout”. New versions of the Follow-the-Perturbed-Leader algorithms are presented, as well as methods based on the Littlestone’s dimension, efﬁcient methods for matrix completion with trace norm, and algorithms for the problems of transductive learning and prediction with static experts. 1</p><p>6 0.1897943 <a title="324-tfidf-6" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>7 0.18946961 <a title="324-tfidf-7" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>8 0.18019599 <a title="324-tfidf-8" href="./nips-2012-Large_Scale_Distributed_Deep_Networks.html">170 nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>9 0.17642699 <a title="324-tfidf-9" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>10 0.17195731 <a title="324-tfidf-10" href="./nips-2012-Selective_Labeling_via_Error_Bound_Minimization.html">305 nips-2012-Selective Labeling via Error Bound Minimization</a></p>
<p>11 0.16828088 <a title="324-tfidf-11" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>12 0.16807932 <a title="324-tfidf-12" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>13 0.13882756 <a title="324-tfidf-13" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>14 0.13585158 <a title="324-tfidf-14" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>15 0.13255389 <a title="324-tfidf-15" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>16 0.13144076 <a title="324-tfidf-16" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>17 0.13136895 <a title="324-tfidf-17" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>18 0.13046628 <a title="324-tfidf-18" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>19 0.11776807 <a title="324-tfidf-19" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>20 0.11225751 <a title="324-tfidf-20" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.011), (2, 0.289), (3, 0.283), (4, 0.192), (5, -0.053), (6, -0.031), (7, -0.096), (8, 0.003), (9, -0.077), (10, 0.102), (11, -0.004), (12, -0.092), (13, 0.117), (14, 0.011), (15, 0.092), (16, -0.085), (17, 0.02), (18, 0.029), (19, 0.092), (20, 0.014), (21, -0.126), (22, 0.063), (23, -0.201), (24, -0.096), (25, 0.005), (26, -0.014), (27, 0.199), (28, 0.016), (29, -0.047), (30, -0.074), (31, -0.126), (32, 0.1), (33, -0.187), (34, -0.116), (35, 0.014), (36, -0.041), (37, -0.05), (38, 0.073), (39, 0.023), (40, -0.129), (41, -0.05), (42, -0.041), (43, -0.085), (44, -0.03), (45, -0.057), (46, -0.091), (47, 0.023), (48, 0.025), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98951542 <a title="324-lsi-1" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, Jinfeng Yi</p><p>Abstract: Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. 1</p><p>2 0.84915501 <a title="324-lsi-2" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>Author: Marc Deisenroth, Shakir Mohamed</p><p>Abstract: Rich and complex time-series data, such as those generated from engineering systems, ﬁnancial markets, videos, or neural recordings are now a common feature of modern data analysis. Explaining the phenomena underlying these diverse data sets requires ﬂexible and accurate models. In this paper, we promote Gaussian process dynamical systems as a rich model class that is appropriate for such an analysis. We present a new approximate message-passing algorithm for Bayesian state estimation and inference in Gaussian process dynamical systems, a nonparametric probabilistic generalization of commonly used state-space models. We derive our message-passing algorithm using Expectation Propagation and provide a unifying perspective on message passing in general state-space models. We show that existing Gaussian ﬁlters and smoothers appear as special cases within our inference framework, and that these existing approaches can be improved upon using iterated message passing. Using both synthetic and real-world data, we demonstrate that iterated message passing can improve inference in a wide range of tasks in Bayesian state estimation, thus leading to improved predictions and more effective decision making. 1</p><p>3 0.82290435 <a title="324-lsi-3" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>Author: Sasha Rakhlin, Ohad Shamir, Karthik Sridharan</p><p>Abstract: We show a principled way of deriving online learning algorithms from a minimax analysis. Various upper bounds on the minimax value, previously thought to be non-constructive, are shown to yield algorithms. This allows us to seamlessly recover known methods and to derive new ones, also capturing such “unorthodox” methods as Follow the Perturbed Leader and the R2 forecaster. Understanding the inherent complexity of the learning problem thus leads to the development of algorithms. To illustrate our approach, we present several new algorithms, including a family of randomized methods that use the idea of a “random playout”. New versions of the Follow-the-Perturbed-Leader algorithms are presented, as well as methods based on the Littlestone’s dimension, efﬁcient methods for matrix completion with trace norm, and algorithms for the problems of transductive learning and prediction with static experts. 1</p><p>4 0.72916514 <a title="324-lsi-4" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>Author: Bo Liu, Sridhar Mahadevan, Ji Liu</p><p>Abstract: We present a novel l1 regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying ROTD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables ﬁrst-order solvers and feature selection using online convex regularization. A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm. 1</p><p>5 0.7268694 <a title="324-lsi-5" href="./nips-2012-Selective_Labeling_via_Error_Bound_Minimization.html">305 nips-2012-Selective Labeling via Error Bound Minimization</a></p>
<p>Author: Quanquan Gu, Tong Zhang, Jiawei Han, Chris H. Ding</p><p>Abstract: In many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming. This motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized. We propose a selective labeling method by analyzing the out-of-sample error of Laplacian regularized Least Squares (LapRLS). In particular, we derive a deterministic out-of-sample error bound for LapRLS trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound. Since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent. Experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.</p><p>6 0.67655009 <a title="324-lsi-6" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>7 0.65018046 <a title="324-lsi-7" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>8 0.58806753 <a title="324-lsi-8" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>9 0.5812946 <a title="324-lsi-9" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>10 0.53742844 <a title="324-lsi-10" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>11 0.53465396 <a title="324-lsi-11" href="./nips-2012-Query_Complexity_of_Derivative-Free_Optimization.html">285 nips-2012-Query Complexity of Derivative-Free Optimization</a></p>
<p>12 0.5175485 <a title="324-lsi-12" href="./nips-2012-Semi-supervised_Eigenvectors_for_Locally-biased_Learning.html">309 nips-2012-Semi-supervised Eigenvectors for Locally-biased Learning</a></p>
<p>13 0.51672137 <a title="324-lsi-13" href="./nips-2012-Confusion-Based_Online_Learning_and_a_Passive-Aggressive_Scheme.html">80 nips-2012-Confusion-Based Online Learning and a Passive-Aggressive Scheme</a></p>
<p>14 0.51367301 <a title="324-lsi-14" href="./nips-2012-A_Marginalized_Particle_Gaussian_Process_Regression.html">11 nips-2012-A Marginalized Particle Gaussian Process Regression</a></p>
<p>15 0.50900209 <a title="324-lsi-15" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>16 0.49402121 <a title="324-lsi-16" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>17 0.48171306 <a title="324-lsi-17" href="./nips-2012-Causal_discovery_with_scale-mixture_model_for_spatiotemporal_variance_dependencies.html">66 nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</a></p>
<p>18 0.45595545 <a title="324-lsi-18" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>19 0.44843012 <a title="324-lsi-19" href="./nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">23 nips-2012-A lattice filter model of the visual pathway</a></p>
<p>20 0.43616262 <a title="324-lsi-20" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (21, 0.021), (23, 0.128), (38, 0.216), (42, 0.056), (53, 0.01), (54, 0.023), (68, 0.023), (74, 0.027), (76, 0.148), (80, 0.165), (92, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95686376 <a title="324-lda-1" href="./nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">214 nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>Author: Andrew Delong, Olga Veksler, Anton Osokin, Yuri Boykov</p><p>Abstract: Inference in high-order graphical models has become important in recent years. Several approaches are based, for example, on generalized message-passing, or on transformation to a pairwise model with extra ‘auxiliary’ variables. We focus on a special case where a much more efﬁcient transformation is possible. Instead of adding variables, we transform the original problem into a comparatively small instance of submodular vertex-cover. These vertex-cover instances can then be attacked by existing algorithms (e.g. belief propagation, QPBO), where they often run 4–15 times faster and ﬁnd better solutions than when applied to the original problem. We evaluate our approach on synthetic data, then we show applications within a fast hierarchical clustering and model-ﬁtting framework. 1</p><p>2 0.93933845 <a title="324-lda-2" href="./nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">132 nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<p>Author: Antonino Freno, Mikaela Keller, Marc Tommasi</p><p>Abstract: Statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions. Moreover, the vast majority of currently available models are explicitly designed for capturing some speciﬁc graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori. The key contribution of this paper is twofold. First, we introduce the Fiedler delta statistic, based on the Laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties. Second, we use the deﬁned statistic to develop the Fiedler random ﬁeld model, which allows for efﬁcient estimation of edge distributions over large-scale random networks. After analyzing the dependence structure involved in Fiedler random ﬁelds, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.</p><p>same-paper 3 0.92954814 <a title="324-lda-3" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, Jinfeng Yi</p><p>Abstract: Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. 1</p><p>4 0.91734457 <a title="324-lda-4" href="./nips-2012-Provable_ICA_with_Unknown_Gaussian_Noise%2C_with_Implications_for_Gaussian_Mixtures_and_Autoencoders.html">281 nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</a></p>
<p>Author: Sanjeev Arora, Rong Ge, Ankur Moitra, Sushant Sachdeva</p><p>Abstract: We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form y = Ax + η where A is an unknown n × n matrix and x is a random variable whose components are independent and have a fourth moment strictly less than that of a standard Gaussian random variable and η is an n-dimensional Gaussian random variable with unknown covariance Σ: We give an algorithm that provable recovers A and Σ up to an additive and whose running time and sample complexity are polynomial in n and 1/ . To accomplish this, we introduce a novel “quasi-whitening” step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for ﬁnding all local optima of a function (given an oracle for approximately ﬁnding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we ﬁnd the columns of A one by one via local search. 1</p><p>5 0.91473746 <a title="324-lda-5" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>Author: Nicolò Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, Gilles Stoltz</p><p>Abstract: Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel uniﬁed analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be reﬁned in several ways, including improvements for small losses and adaptive tuning of parameters. 1</p><p>6 0.91271996 <a title="324-lda-6" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>7 0.91258824 <a title="324-lda-7" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>8 0.90963984 <a title="324-lda-8" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>9 0.9071945 <a title="324-lda-9" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>10 0.90638846 <a title="324-lda-10" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>11 0.90454578 <a title="324-lda-11" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>12 0.9038409 <a title="324-lda-12" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>13 0.90350521 <a title="324-lda-13" href="./nips-2012-Bayesian_active_learning_with_localized_priors_for_fast_receptive_field_characterization.html">56 nips-2012-Bayesian active learning with localized priors for fast receptive field characterization</a></p>
<p>14 0.90323156 <a title="324-lda-14" href="./nips-2012-Value_Pursuit_Iteration.html">358 nips-2012-Value Pursuit Iteration</a></p>
<p>15 0.90276146 <a title="324-lda-15" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>16 0.90233803 <a title="324-lda-16" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>17 0.90028334 <a title="324-lda-17" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>18 0.90016431 <a title="324-lda-18" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>19 0.89960086 <a title="324-lda-19" href="./nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">230 nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<p>20 0.89930987 <a title="324-lda-20" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
