<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-241" href="#">nips2012-241</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</h1>
<br/><p>Source: <a title="nips-2012-241-pdf" href="http://papers.nips.cc/paper/4709-no-regret-algorithms-for-unconstrained-online-convex-optimization.pdf">pdf</a></p><p>Author: Brendan Mcmahan, Matthew Streeter</p><p>Abstract: Some of the most compelling applications of online convex optimization, including online prediction and classiﬁcation, are unconstrained: the natural feasible set is Rn . Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point ˚ are known in advance. We present algox rithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of ˚. In particular, regret with respect to ˚ = 0 is constant. x x We then prove lower bounds showing that our guarantees are near-optimal in this setting. 1</p><p>Reference: <a title="nips-2012-241-reference" href="../nips2012_reference/nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Some of the most compelling applications of online convex optimization, including online prediction and classiﬁcation, are unconstrained: the natural feasible set is Rn . [sent-6, score-0.342]
</p><p>2 Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point ˚ are known in advance. [sent-7, score-0.561]
</p><p>3 We present algox rithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of ˚. [sent-8, score-0.441]
</p><p>4 In particular, regret with respect to ˚ = 0 is constant. [sent-9, score-0.359]
</p><p>5 1  Introduction  Over the past several years, online convex optimization has emerged as a fundamental tool for solving problems in machine learning (see, e. [sent-11, score-0.195]
</p><p>6 The reduction from general online convex optimization to online linear optimization means that simple and efﬁcient (in memory and time) algorithms can be used to tackle large-scale machine learning problems. [sent-14, score-0.334]
</p><p>7 The key theoretical techniques behind essentially all the algorithms in this ﬁeld are the use of a ﬁxed or increasing strongly convex regularizer (for gradient descent algorithms, this is equivalent to a ﬁxed or decreasing learning rate sequence). [sent-15, score-0.273]
</p><p>8 √ This approach produces regret bounds of the form O R T log((1 + R)T ) , where R = ˚ 2 is the x L2 norm of an arbitrary comparator. [sent-19, score-0.425]
</p><p>9 A consequence of this is that we can x guarantee at most constant regret with respect to the origin, ˚ = 0. [sent-21, score-0.424]
</p><p>10 This technique can be applied to x any online convex optimization problem where a ﬁxed feasible set is not an essential component of the problem. [sent-22, score-0.239]
</p><p>11 For appropriately chosen σ and , this becomes a problem of online convex optimization against functions ft (x) = (σ(at ·x), yt ). [sent-25, score-0.248]
</p><p>12 1  likely than large ones, but this is rarely best encoded as a feasible set F, which says: “all xt ∈ F are equally likely, and all other xt are ruled out. [sent-28, score-0.3]
</p><p>13 While 2 algorithms of this form have proved very effective at solving these problems, theoretical guarantees usually require ﬁxing a feasible set of radius R, or at least an intelligent guess of the norm of an optimal comparator ˚. [sent-30, score-0.34]
</p><p>14 , [3]), there are n experts, and on each round t the player selects an expert (say i), and obtains reward gt,i from a bounded interval (say [−1, 1]). [sent-33, score-0.458]
</p><p>15 Typically, one uses an algorithm that proposes a probability distribution pt on experts, so the expected reward is pt · gt . [sent-34, score-0.662]
</p><p>16 Our algorithms apply to an unconstrained version of this problem: there are still n experts with payouts in [−1, 1], but rather than selecting an individual expert, the player can place a “bet” of xt,i on each expert i, and then receives reward i xt,i gt,i = xt · gt . [sent-35, score-0.93]
</p><p>17 The bets are unconstrained (betting a negative value corresponds to betting against the expert). [sent-36, score-0.173]
</p><p>18 In this setting, a natural goal is the following: place bets so as to achieve as much reward as possible, subject to the constraint that total losses are bounded by a constant (which can be set equal to some starting budget which is to be invested). [sent-37, score-0.348]
</p><p>19 Our algorithms can satisfy constraints of this form because regret with respect to ˚ = 0 x (which equals total loss) is bounded by a constant. [sent-38, score-0.359]
</p><p>20 It is useful to contrast our results in this setting to previous applications of online convex optimization to portfolio management, for example [6] and [2]. [sent-39, score-0.233]
</p><p>21 Therefore, we consider online linear optimization where the goal is to maximize cumulative reward given adversarially selected linear reward functions ft (x) = gt · x. [sent-49, score-1.077]
</p><p>22 T , the algorithm selects a point xt ∈ Rn , receives reward ft (xt ) = gt · xt , and observes gt . [sent-53, score-1.303]
</p><p>23 For simplicity, we assume gt,i ∈ [−1, 1], that is, gt ∞ ≤ 1. [sent-54, score-0.349]
</p><p>24 If the real problem is against convex loss functions t (x), they can be converted to our framework by taking gt = − t (xt ) (see pseudo-code for R EWARD -D OUBLING), using the standard reduction from online convex optimization to online linear optimization [13]. [sent-55, score-0.755]
</p><p>25 We study the reward of our algorithms, and their regret against a ﬁxed comparator ˚: x T  Reward ≡  T  gt · xt  and  Regret(˚) ≡ g1:T · ˚ − x x  t=1  gt · xt . [sent-57, score-1.773]
</p><p>26 t=1  Comparison of Regret Bounds The primary contribution of this paper is to establish matching upper and lower bounds for unconstrained online convex optimization problems, using algorithms that require no prior information about the comparator point √. [sent-58, score-0.577]
</p><p>27 To obtain x x x x √ this guarantee, we show that it is sufﬁcient (and necessary) that reward is Ω(exp(|g1:T |/ T )) (see Theorem 1). [sent-60, score-0.258]
</p><p>28 x Table 1 compares the bounds for R EWARD -D OUBLING (this paper) to those of two previous algorithms: online gradient descent [13] and projected exponentiated gradient descent [8, 12]. [sent-62, score-0.58]
</p><p>29 For each Our bounds are not directly comparable to the bounds cited above: a O(log(T )) regret bound on log√ wealth implies wealth at least O OPT/T , whereas we guarantee wealth like O OPT’ − T . [sent-63, score-0.746]
</p><p>30 2  Assuming gt  2  ≤ 1:  Gradient Descent, η =  ˚= 0 x √ R T  R √ T  R EWARD -D OUBLING Assuming gt  ∞  ˚ 2≤R x √ R T  √ R T log  n(1+R)T  ˚ x  √ 2  Arbitrary ˚ x ˚ 2T x T log  n(1+ ˚ x  2 )T  ≤ 1:  Exponentiated G. [sent-65, score-0.794]
</p><p>31 R EWARD -D OUBLING  ˚= 0 x √ R T log n  ˚ 1≤R x √ R T log n √ R T log n(1+R)T  Arbitrary ˚ x ˚ 1T x √ x ˚ 1 T log n(1+ ˚ x  √  1)  T  Table 1: Worst-case regret bounds for various algorithms (up to constant factors). [sent-67, score-0.641]
</p><p>32 algorithm, we consider a ﬁxed choice of parameter settings and then look at how regret changes as we vary the comparator point ˚. [sent-71, score-0.561]
</p><p>33 x Gradient descent is minimax-optimal [1] when the comparator point is contained in a hypershere whose radius is known in advance ( ˚ 2 ≤ R) and gradients are sparse ( gt 2 ≤ 1, top table). [sent-72, score-0.764]
</p><p>34 x Exponentiated gradient descent excels when gradients are dense ( gt ∞ ≤ 1, bottom table) but the comparator point is sparse ( ˚ 1 ≤ R for R known in advance). [sent-73, score-0.789]
</p><p>35 When ˚ = 0, R EWARD -D OUBLING offers constant regret x √ x compared to Ω( T ) for the other algorithms. [sent-76, score-0.383]
</p><p>36 When ˚ can be arbitrary, only R EWARD -D OUBLING offers sub-linear regret (and in fact its regret bound is optimal, as shown in Theorem 8). [sent-77, score-0.756]
</p><p>37 Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9]. [sent-80, score-0.191]
</p><p>38 In Follow-The-Regularized-Leader terms, the exponentiated gradient descent algorithm with unnor1 malized weights of Kivinen and Warmuth [8] plays xt+1 = arg minx∈Rn g1:t · x + η (x log x − x), + which has closed-form solution xt+1 = exp(−ηg1:t ). [sent-82, score-0.339]
</p><p>39 Like our algorithm, this algorithm moves away from the origin exponentially fast, but unlike our algorithm it can incur arbitrarily large regret with respect to ˚ = 0. [sent-83, score-0.442]
</p><p>40 Hazan and Kale [5] give regret bounds in terms of the variance of the gt . [sent-85, score-0.774]
</p><p>41 Letting G = |g1:t | and √ T 2 the H = t=1 gt , they prove regret bounds of √ form O( V ) where V = H − G2 /T . [sent-86, score-0.774]
</p><p>42 However, they consider the case of a known feasible set, and their algorithm (gradient descent with a constant learning rate) cannot obtain bounds of the form we prove. [sent-88, score-0.243]
</p><p>43 2  Reward and Regret  In this section we present a general result that converts lower bounds on reward into upper bounds on regret, for one-dimensional online linear optimization. [sent-89, score-0.522]
</p><p>44 In the unconstrained setting, this result will be sufﬁcient to provide guarantees for general n-dimensional online convex optimization. [sent-90, score-0.293]
</p><p>45 Consider an algorithm for one-dimensional online linear optimization that, when run on a sequence of gradients g1 , g2 , . [sent-92, score-0.272]
</p><p>46 , gT , with gt ∈ [−1, 1] for all t, guarantees Reward ≥ κ exp (γ|g1:T |) − , (1) where γ, κ > 0 and ≥ 0 are constants. [sent-95, score-0.41]
</p><p>47 Then, against any comparator ˚ ∈ [−R, R], we have x R R log −1 + , (2) γ κγ letting 0 log 0 = 0 when R = 0. [sent-96, score-0.333]
</p><p>48 Further, any algorithm with the regret guarantee of Eq. [sent-97, score-0.421]
</p><p>49 The duality between reward and regret can also be seen as a consequence of the fact that exp(x) and y log y − y are convex conjugates. [sent-101, score-0.721]
</p><p>50 This bound holds for all R, and so for some small R the log term becomes negative; however, for real algorithms the term will ensure the regret bound remains positive. [sent-103, score-0.483]
</p><p>51 3  Gradient Descent with Increasing Learning Rates  In this section we show that allowing the learning rate of gradient descent to sometimes increase leads to novel theoretical guarantees. [sent-105, score-0.216]
</p><p>52 To build intuition, consider online linear optimization in one dimension, with gradients g1 , g2 , . [sent-106, score-0.212]
</p><p>53 In this setting, the reward of unconstrained gradient descent has a simple closed form: Lemma 2. [sent-110, score-0.515]
</p><p>54 Consider unconstrained gradient descent in one dimension, with learning rate η. [sent-111, score-0.288]
</p><p>55 On T 2 round t, this algorithm plays the point xt = ηg1:t−1 . [sent-112, score-0.281]
</p><p>56 Letting G = |g1:t | and H = t=1 gt , the cumulative reward of the algorithm is exactly η Reward = G2 − H . [sent-113, score-0.648]
</p><p>57 Perhaps surprisingly, this result implies that the reward is totally independent of the order of the linear functions selected by the adversary. [sent-115, score-0.275]
</p><p>58 Examining the expression in Lemma 2, we see that the optimal choice of learning rate η depends fundamentally on two quantities: the absolute value of the sum of gradients (G), and the sum of the squared gradients (H). [sent-116, score-0.197]
</p><p>59 One of the motivations for this work is the observation that the state-of-the-art online gradient descent algorithms adjust their learning rates based only on the observed value of H (or its upper bound T ); for example [4, 10]. [sent-119, score-0.373]
</p><p>60 We would like to increase reward by also accounting for G. [sent-120, score-0.278]
</p><p>61 We suppose for the moment that an upper bound H on H = t=1 gt is known in advance. [sent-125, score-0.409]
</p><p>62 In the ﬁrst epoch, we run gradient descent with a small initial learning rate η = η1 . [sent-126, score-0.196]
</p><p>63 ¯ Whenever the total reward accumulated in the current epoch reaches η H, we double η and start a new epoch (returning to the origin and forgetting all previous gradients except the most recent one). [sent-127, score-0.514]
</p><p>64 , gT , all in [−1, 1], where H = ¯ H, R EWARD -D OUBLING -1D obtains reward satisfying T  xt gt ≥  Reward = √ for a = log(2)/ 3. [sent-132, score-0.755]
</p><p>65 t=1  1 ¯ |g1:T | η1 H exp a √ ¯ 4 H  4  ¯ − η1 H,  T 2 t=1 gt  ≤  (3)  Algorithm 1 R EWARD -D OUBLING -1D Parameters: initial learning rate η1 , upper T 2 ¯ bound H ≥ t=1 gt . [sent-133, score-0.815]
</p><p>66 , k let ti denote the round on which Qi is initialized to 0, with t1 ≡ 1, and deﬁne tk+1 ≡ T . [sent-165, score-0.183]
</p><p>67 By construction, Qi is the total reward of a gradient descent algorithm that is active on rounds ti through ti+1 inclusive, and that uses learning rate ηi (note that on round ti , this algorithm gets 0 reward and we initialize Qi to 0 on that round). [sent-166, score-1.057]
</p><p>68 At the end of round ti+1 − 1, we must have had Qi < ηi H (otherwise ¯ 3H epoch i + 1 would have begun earlier). [sent-171, score-0.163]
</p><p>69 We can now apply Theorem 1 to the reward (given by Eq. [sent-176, score-0.258]
</p><p>70 When the feasible set is also ﬁxed in x √ advance, online gradient descent with a ﬁxed learning obtains a regret bound of O(R T ). [sent-179, score-0.736]
</p><p>71 By choosing η1 = T , we guarantee constant regret against the origin, ˚ = 0 (equivalently, constant total loss). [sent-181, score-0.448]
</p><p>72 Further, for any feasible set of radius R, we still have x 5  √ worst-case regret of at most O(R T log((1 + R)T )), which is only modestly worse than that of gradient descent with the optimal R known in advance. [sent-182, score-0.597]
</p><p>73 ¯ The need for an upper bound H can be removed using a standard guess-and-doubling approach, at the cost of a constant factor increase in regret (see appendix for proof). [sent-183, score-0.488]
</p><p>74 On ¯ each era i, the algorithm runs R EWARD -D OUBLING -1D with an upper bound of Hi = 2i−1 , and i ¯ initial learning rate η1 = 2−2i . [sent-186, score-0.142]
</p><p>75 An era ends when Hi is no longer an upper bound on the sum of √ 2 squared gradients seen during that era. [sent-187, score-0.163]
</p><p>76 Letting c = √2−1 , this algorithm has regret at most √ Regret ≤ cR H + 1 log 3. [sent-188, score-0.428]
</p><p>77 Extension to n dimensions  To extend our results to general online convex optimization, it is sufﬁcient to run a separate copy of R EWARD -D OUBLING -1D-G UESS for each coordinate, as is done in R EWARD -D OUBLING (Algorithm 2). [sent-190, score-0.187]
</p><p>78 The key to the analysis of this algorithm is that overall regret is simply the sum of regret on n one-dimensional subproblems which can be analyzed independently. [sent-191, score-0.739]
</p><p>79 , fT from Rn to R, R EWARD -D OUBLING with i = n has regret bounded by n  i=1  ≤ +c ˚ x for c =  √ √ 2 , 2−1  where Hi =  n  |˚i | Hi + 1 log x  Regret(˚) ≤ + c x  √ 2  T 2 t=1 gt,i  n  H + n log T t=1  and H =  |˚i |(2Hi + 2)5/2 − 1 x  ˚ 2 (2H + 2)5/2 − 1 x 2  gt 2 . [sent-196, score-0.804]
</p><p>80 For any coordinate i, deﬁne x T  T  ˚i gt,i − x  Regreti = t=1  Observe that  n  T  T  ˚ · gt − x  Regreti = i=1  xt,i gt,i . [sent-199, score-0.38]
</p><p>81 x t=1  Furthermore, Regreti is simply the regret of R EWARD -D OUBLING -1D-G UESS on the gradient sequence g1,i , g2,i , . [sent-201, score-0.475]
</p><p>82 76η, (7) η for all T and R, which is better (by constant factors) than Theorem 4 when gt ∈ {−1, 1} (which implies T = H). [sent-214, score-0.39]
</p><p>83 Because reward changes by gt xt on round t, it sufﬁces to guarantee that for any g ∈ [−1, 1], N (g1:t , t) + gxt+1 ≥ N (g1:t + g, t + 1)  (8)  where xt+1 is the point the algorithm plays on round t + 1, and we assume N (0, 1) = 0. [sent-217, score-1.03]
</p><p>84 ∂g g g √ This suggests that if we want to maintain reward at least N (g1:t , t) = 1 (exp(|g1:t |/ t) − 1) , we t xt+1 =  √ should set xt+1 ≈ sign(g1:t )t−3/2 exp |g1:t | . [sent-219, score-0.284]
</p><p>85 Fix a sequence of reward functions ft (x) = gt x with gt ∈ [−1, 1], and let Gt = |g1:t |. [sent-222, score-1.048]
</p><p>86 We consider S MOOTH -R EWARD -D OUBLING, which plays 0 on round 1 and whenever Gt = 0; otherwise, it plays xt+1 = η sign(g1:t )B(Gt , t + 5) (9) with η > 0 a learning-rate parameter and  B(G, t) =  1 t3/2  exp  G √ t  . [sent-223, score-0.189]
</p><p>87 (10)  Then, at the end of each round t, this algorithm has Reward(t) ≥ η  1 exp t+5  √  Gt t+5  − 1. [sent-224, score-0.148]
</p><p>88 Consider the problem of unconstrained online linear optimization in one dimension, and an online algorithm that guarantees origin-regret at most . [sent-235, score-0.397]
</p><p>89 Then, for any ﬁxed comparator ˚, x and any integer T0 , there exists a gradient sequence {gt } ∈ [−1, 1]T of length T ≥ T0 for which the algorithm’s regret satisﬁes Regret(˚) ≥ 0. [sent-236, score-0.677]
</p><p>90 Let Q be the algorithm’s reward x when each gt is drawn independently uniformly from {−1, 1}. [sent-240, score-0.607]
</p><p>91 On this sequence, regret is at least a with √ G˚ − Q ≥ R kT − R T = Ω(R kT ). [sent-248, score-0.359]
</p><p>92 Consider the problem of unconstrained online linear optimization in Rn , and consider an online algorithm that guarantees origin-regret at most . [sent-250, score-0.397]
</p><p>93 For any radius R, and any T0 , there exists a gradient sequence gradient sequence {gt } ∈ ([−1, 1]n )T of length T ≥ T0 , and a comparator ˚ with ˚ 1 = R, for which the algorithm’s regret satisﬁes x x √ n |˚i | T x |˚i | T log x Regret(˚) ≥ 0. [sent-251, score-0.87]
</p><p>94 For each coordinate i, Theorem 7 implies that there exists a T ≥ T0 and a sequence of gradients gt,i such that √ T T |˚i | T x ˚i gt,i − x xt,i gt,i ≥ 0. [sent-254, score-0.16]
</p><p>95 ) Summing this inequality across all n coordinates then gives the regret bound stated in the theorem. [sent-257, score-0.413]
</p><p>96 The following theorem presents a stronger negative result for Follow-the-Regularized-Leader algorithms with a ﬁxed regularizer: for any such algorithm that guarantees origin-regret at most T after T rounds, worst-case regret with respect to any point outside [− T , T ] grows linearly with T . [sent-258, score-0.457]
</p><p>97 Consider a Follow-The-Regularized-Leader algorithm that sets xt = arg min (g1:t−1 x + ψT (x)) x  where ψT is a convex, non-negative function with ψT (0) = 0. [sent-260, score-0.149]
</p><p>98 Then, for any ˚ with |˚| > T , there exists a x x x sequence of T gradients such that the algorithm’s regret with respect to ˚ is at least T −1 (|˚| − T ). [sent-262, score-0.471]
</p><p>99 It should be possible to apply our techniques to problems that do have constrained feasible sets; for example, it is natural to consider the unconstrained experts problem on the positive orthant. [sent-265, score-0.177]
</p><p>100 Optimal strategies and minimax lower bounds for online convex games. [sent-270, score-0.232]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('eward', 0.459), ('oubling', 0.459), ('regret', 0.359), ('gt', 0.349), ('reward', 0.258), ('comparator', 0.202), ('xt', 0.128), ('online', 0.11), ('round', 0.101), ('unconstrained', 0.092), ('descent', 0.088), ('qi', 0.087), ('ti', 0.082), ('gradient', 0.077), ('exponentiated', 0.074), ('gti', 0.073), ('regreti', 0.073), ('uess', 0.073), ('gradients', 0.073), ('bounds', 0.066), ('epoch', 0.062), ('convex', 0.056), ('mooth', 0.055), ('wealth', 0.053), ('ft', 0.053), ('hi', 0.052), ('satyen', 0.049), ('bets', 0.049), ('mcmahan', 0.049), ('log', 0.048), ('rn', 0.045), ('hazan', 0.045), ('pr', 0.044), ('feasible', 0.044), ('elad', 0.044), ('theorem', 0.042), ('expert', 0.042), ('experts', 0.041), ('guarantee', 0.041), ('origin', 0.041), ('qk', 0.04), ('sequence', 0.039), ('portfolio', 0.038), ('rounds', 0.038), ('bound', 0.038), ('letting', 0.035), ('guarantees', 0.035), ('betting', 0.032), ('plays', 0.031), ('rate', 0.031), ('coordinate', 0.031), ('guess', 0.03), ('era', 0.03), ('optimization', 0.029), ('radius', 0.029), ('kt', 0.029), ('investment', 0.028), ('brendan', 0.027), ('colt', 0.026), ('exp', 0.026), ('momentum', 0.026), ('management', 0.025), ('kivinen', 0.025), ('appendix', 0.025), ('kale', 0.024), ('constant', 0.024), ('advance', 0.023), ('play', 0.023), ('prediction', 0.022), ('upper', 0.022), ('gs', 0.022), ('algorithm', 0.021), ('regularizer', 0.021), ('copy', 0.021), ('obtains', 0.02), ('fundamentally', 0.02), ('player', 0.02), ('lemma', 0.02), ('cumulative', 0.02), ('increase', 0.02), ('rates', 0.02), ('proof', 0.02), ('fix', 0.019), ('doesn', 0.018), ('receive', 0.018), ('adjust', 0.018), ('double', 0.018), ('implies', 0.017), ('selects', 0.017), ('losses', 0.017), ('matthew', 0.017), ('pt', 0.017), ('sign', 0.016), ('inequality', 0.016), ('matt', 0.016), ('gxt', 0.016), ('invested', 0.016), ('streeter', 0.016), ('loss', 0.016), ('offer', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="241-tfidf-1" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>Author: Brendan Mcmahan, Matthew Streeter</p><p>Abstract: Some of the most compelling applications of online convex optimization, including online prediction and classiﬁcation, are unconstrained: the natural feasible set is Rn . Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point ˚ are known in advance. We present algox rithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of ˚. In particular, regret with respect to ˚ = 0 is constant. x x We then prove lower bounds showing that our guarantees are near-optimal in this setting. 1</p><p>2 0.20439611 <a title="241-tfidf-2" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>Author: Amir Sani, Alessandro Lazaric, Rémi Munos</p><p>Abstract: Stochastic multi–armed bandits solve the Exploration–Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk–aversion where the objective is to compete against the arm with the best risk–return trade–off. This setting proves to be more difﬁcult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we deﬁne two algorithms, investigate their theoretical guarantees, and report preliminary empirical results. 1</p><p>3 0.16828088 <a title="241-tfidf-3" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>Author: Mehrdad Mahdavi, Tianbao Yang, Rong Jin, Shenghuo Zhu, Jinfeng Yi</p><p>Abstract: Although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at each iteration to ensure that the obtained solution stays within the feasible domain. For complex domains (e.g., positive semideﬁnite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems. We address this limitation by developing novel stochastic optimization algorithms that do not need intermediate projections. Instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain. Our theoretical analysis shows that with a high probability, √ the proposed algorithms achieve an O(1/ T ) convergence rate for general convex optimization, and an O(ln T /T ) rate for strongly convex optimization under mild conditions about the domain and the objective function. 1</p><p>4 0.16198003 <a title="241-tfidf-4" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>Author: Ronald Ortner, Daniil Ryabko</p><p>Abstract: We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper conﬁdence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisﬁes the Poisson equation, the only assumptions made are H¨ lder continuity of rewards and transition o probabilities. 1</p><p>5 0.15704088 <a title="241-tfidf-5" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>Author: Jaedeug Choi, Kee-eung Kim</p><p>Abstract: We present a nonparametric Bayesian approach to inverse reinforcement learning (IRL) for multiple reward functions. Most previous IRL algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to guarantee in practice. Our approach is based on integrating the Dirichlet process mixture model into Bayesian IRL. We provide an efﬁcient Metropolis-Hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms previous ones via experiments on a number of problem domains. 1</p><p>6 0.14720088 <a title="241-tfidf-6" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>7 0.13742816 <a title="241-tfidf-7" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>8 0.13292243 <a title="241-tfidf-8" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>9 0.13177027 <a title="241-tfidf-9" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>10 0.12252782 <a title="241-tfidf-10" href="./nips-2012-Distributed_Non-Stochastic_Experts.html">102 nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>11 0.11753377 <a title="241-tfidf-11" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>12 0.11601961 <a title="241-tfidf-12" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>13 0.11571461 <a title="241-tfidf-13" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>14 0.10545138 <a title="241-tfidf-14" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>15 0.10293599 <a title="241-tfidf-15" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>16 0.10227117 <a title="241-tfidf-16" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>17 0.10180202 <a title="241-tfidf-17" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>18 0.092907302 <a title="241-tfidf-18" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>19 0.08722914 <a title="241-tfidf-19" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>20 0.083149388 <a title="241-tfidf-20" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.174), (1, -0.168), (2, 0.139), (3, 0.108), (4, 0.07), (5, 0.018), (6, -0.019), (7, -0.01), (8, -0.007), (9, 0.029), (10, 0.118), (11, 0.103), (12, -0.146), (13, -0.029), (14, -0.096), (15, 0.103), (16, -0.104), (17, 0.017), (18, -0.057), (19, -0.074), (20, 0.074), (21, -0.138), (22, 0.012), (23, -0.031), (24, -0.003), (25, 0.003), (26, -0.034), (27, -0.063), (28, 0.003), (29, -0.012), (30, -0.073), (31, -0.105), (32, 0.13), (33, -0.053), (34, -0.038), (35, -0.002), (36, 0.12), (37, -0.014), (38, 0.013), (39, -0.089), (40, -0.005), (41, -0.036), (42, 0.03), (43, -0.04), (44, -0.085), (45, 0.125), (46, -0.08), (47, -0.16), (48, 0.011), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93640006 <a title="241-lsi-1" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>Author: Brendan Mcmahan, Matthew Streeter</p><p>Abstract: Some of the most compelling applications of online convex optimization, including online prediction and classiﬁcation, are unconstrained: the natural feasible set is Rn . Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point ˚ are known in advance. We present algox rithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of ˚. In particular, regret with respect to ˚ = 0 is constant. x x We then prove lower bounds showing that our guarantees are near-optimal in this setting. 1</p><p>2 0.70678109 <a title="241-lsi-2" href="./nips-2012-Distributed_Non-Stochastic_Experts.html">102 nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>Author: Varun Kanade, Zhenming Liu, Bozidar Radunovic</p><p>Abstract: We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator. At each time-step t, one of the k site nodes has to pick an expert from the set {1, . . . , n}, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon T , while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the nondistributed setting to obtain the optimal O( log(n)T ) regret bound at the cost of T communication. (ii) No communication: Each site runs an independent copy – the regret is O( log(n)kT ) and the communication is 0. This paper shows the √ difﬁculty of simultaneously achieving regret asymptotically better than kT and communication better than T . We give a novel algorithm that for an oblivious √ adversary achieves a non-trivial trade-off: regret O( k 5(1+ )/6 T ) and communication O(T /k ), for any value of ∈ (0, 1/5). We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efﬁcient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off. 1</p><p>3 0.61263782 <a title="241-lsi-3" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>Author: Amir Sani, Alessandro Lazaric, Rémi Munos</p><p>Abstract: Stochastic multi–armed bandits solve the Exploration–Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk–aversion where the objective is to compete against the arm with the best risk–return trade–off. This setting proves to be more difﬁcult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we deﬁne two algorithms, investigate their theoretical guarantees, and report preliminary empirical results. 1</p><p>4 0.59576321 <a title="241-lsi-4" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>Author: Ronald Ortner, Daniil Ryabko</p><p>Abstract: We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper conﬁdence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisﬁes the Poisson equation, the only assumptions made are H¨ lder continuity of rewards and transition o probabilities. 1</p><p>5 0.55457044 <a title="241-lsi-5" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>Author: Morteza Ibrahimi, Adel Javanmard, Benjamin V. Roy</p><p>Abstract: We study the problem of adaptive control of a high dimensional linear quadratic (LQ) system. Previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes. More recently, for the average √ cost LQ problem, a regret bound of O( T ) was shown, apart form logarithmic factors. However, this bound scales exponentially with p, the dimension of the state space. In this work we consider the case where the matrices describing the dynamic of the LQ system are sparse and their dimensions are large. We present √ an adaptive control scheme that achieves a regret bound of O(p T ), apart from logarithmic factors. In particular, our algorithm has an average cost of (1 + ) times the optimum cost after T = polylog(p)O(1/ 2 ). This is in comparison to previous work on the dense dynamics where the algorithm requires time that scales exponentially with dimension in order to achieve regret of times the optimal cost. We believe that our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks. 1</p><p>6 0.53699851 <a title="241-lsi-6" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>7 0.5289762 <a title="241-lsi-7" href="./nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>8 0.52626169 <a title="241-lsi-8" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>9 0.52587736 <a title="241-lsi-9" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>10 0.50472653 <a title="241-lsi-10" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>11 0.50243878 <a title="241-lsi-11" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>12 0.46504048 <a title="241-lsi-12" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>13 0.45075819 <a title="241-lsi-13" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>14 0.45056674 <a title="241-lsi-14" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>15 0.44116759 <a title="241-lsi-15" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>16 0.38704073 <a title="241-lsi-16" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>17 0.37860239 <a title="241-lsi-17" href="./nips-2012-Interpreting_prediction_markets%3A_a_stochastic_approach.html">161 nips-2012-Interpreting prediction markets: a stochastic approach</a></p>
<p>18 0.37618798 <a title="241-lsi-18" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>19 0.36939716 <a title="241-lsi-19" href="./nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<p>20 0.36764827 <a title="241-lsi-20" href="./nips-2012-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">76 nips-2012-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (17, 0.013), (21, 0.05), (38, 0.165), (42, 0.06), (54, 0.043), (55, 0.011), (57, 0.199), (68, 0.022), (74, 0.038), (76, 0.09), (80, 0.107), (92, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81638664 <a title="241-lda-1" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>Author: Brendan Mcmahan, Matthew Streeter</p><p>Abstract: Some of the most compelling applications of online convex optimization, including online prediction and classiﬁcation, are unconstrained: the natural feasible set is Rn . Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point ˚ are known in advance. We present algox rithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of ˚. In particular, regret with respect to ˚ = 0 is constant. x x We then prove lower bounds showing that our guarantees are near-optimal in this setting. 1</p><p>2 0.78950053 <a title="241-lda-2" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>Author: Bruno Scherrer, Boris Lesner</p><p>Abstract: We consider inﬁnite-horizon stationary γ-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error at each iteration, it is well-known that one 2γ can compute stationary policies that are (1−γ)2 -optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for com2γ puting non-stationary policies that can be up to 1−γ -optimal, which constitutes a signiﬁcant improvement in the usual situation when γ is close to 1. Surprisingly, this shows that the problem of “computing near-optimal non-stationary policies” is much simpler than that of “computing near-optimal stationary policies”. 1</p><p>3 0.74306732 <a title="241-lda-3" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>Author: Nicolò Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, Gilles Stoltz</p><p>Abstract: Mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension. This is done using either a carefully designed projection or by a weight sharing technique. Via a novel uniﬁed analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets. Our analysis also captures and extends the generalized weight sharing technique of Bousquet and Warmuth, and can be reﬁned in several ways, including improvements for small losses and adaptive tuning of parameters. 1</p><p>4 0.73902655 <a title="241-lda-4" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>Author: Jake Bouvrie, Jean-jeacques Slotine</p><p>Abstract: To learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization. Here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system. The functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise. Noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time. The resulting qualitative behavior matches experimental data from visual cortex. 1</p><p>5 0.73655891 <a title="241-lda-5" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>Author: Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-jeacques Slotine</p><p>Abstract: In this paper we discuss a novel framework for multiclass learning, deﬁned by a suitable coding/decoding strategy, namely the simplex coding, that allows us to generalize to multiple classes a relaxation approach commonly used in binary classiﬁcation. In this framework, we develop a relaxation error analysis that avoids constraints on the considered hypotheses class. Moreover, using this setting we derive the ﬁrst provably consistent regularized method with training/tuning complexity that is independent to the number of classes. We introduce tools from convex analysis that can be used beyond the scope of this paper. 1</p><p>6 0.73623323 <a title="241-lda-6" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>7 0.73323882 <a title="241-lda-7" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>8 0.73249441 <a title="241-lda-8" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>9 0.73130172 <a title="241-lda-9" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>10 0.73118222 <a title="241-lda-10" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>11 0.73076862 <a title="241-lda-11" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>12 0.72789598 <a title="241-lda-12" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>13 0.727135 <a title="241-lda-13" href="./nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">230 nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<p>14 0.72606546 <a title="241-lda-14" href="./nips-2012-Value_Pursuit_Iteration.html">358 nips-2012-Value Pursuit Iteration</a></p>
<p>15 0.72569072 <a title="241-lda-15" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>16 0.72523731 <a title="241-lda-16" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>17 0.72476488 <a title="241-lda-17" href="./nips-2012-Provable_ICA_with_Unknown_Gaussian_Noise%2C_with_Implications_for_Gaussian_Mixtures_and_Autoencoders.html">281 nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</a></p>
<p>18 0.72374809 <a title="241-lda-18" href="./nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">114 nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>19 0.72262818 <a title="241-lda-19" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>20 0.72259736 <a title="241-lda-20" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
