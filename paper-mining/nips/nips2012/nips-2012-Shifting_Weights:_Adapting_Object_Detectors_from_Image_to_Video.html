<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-311" href="#">nips2012-311</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</h1>
<br/><p>Source: <a title="nips-2012-311-pdf" href="http://papers.nips.cc/paper/4691-shifting-weights-adapting-object-detectors-from-image-to-video.pdf">pdf</a></p><p>Author: Kevin Tang, Vignesh Ramanathan, Li Fei-fei, Daphne Koller</p><p>Abstract: Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest ﬁrst. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features speciﬁc to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video [2] datasets that illustrate the beneﬁt of our approach to adapt object detectors to video. 1</p><p>Reference: <a title="nips-2012-311-reference" href="../nips2012_reference/nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('video', 0.577), ('detect', 0.282), ('target', 0.265), ('initialbl', 0.209), ('sandwich', 0.204), ('box', 0.192), ('domain', 0.186), ('trecvid', 0.151), ('track', 0.135), ('videoposbl', 0.133), ('scor', 0.128), ('discov', 0.124), ('unlabel', 0.123), ('sourc', 0.121), ('gop', 0.116), ('wtarget', 0.114), ('fram', 0.103), ('tir', 0.102), ('adapt', 0.098), ('sew', 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="311-tfidf-1" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>2 0.40526044 <a title="311-tfidf-2" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>3 0.3246505 <a title="311-tfidf-3" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>4 0.24070638 <a title="311-tfidf-4" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>5 0.18249029 <a title="311-tfidf-5" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>6 0.14103296 <a title="311-tfidf-6" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>7 0.13075884 <a title="311-tfidf-7" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>8 0.12064106 <a title="311-tfidf-8" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>9 0.11487082 <a title="311-tfidf-9" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>10 0.11045162 <a title="311-tfidf-10" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>11 0.10985956 <a title="311-tfidf-11" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>12 0.10935664 <a title="311-tfidf-12" href="./nips-2012-Volume_Regularization_for_Binary_Classification.html">361 nips-2012-Volume Regularization for Binary Classification</a></p>
<p>13 0.10574723 <a title="311-tfidf-13" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>14 0.10505795 <a title="311-tfidf-14" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>15 0.1047608 <a title="311-tfidf-15" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>16 0.099778771 <a title="311-tfidf-16" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>17 0.099402763 <a title="311-tfidf-17" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>18 0.099402763 <a title="311-tfidf-18" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>19 0.095677614 <a title="311-tfidf-19" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>20 0.091133036 <a title="311-tfidf-20" href="./nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">140 nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.212), (1, -0.018), (2, -0.055), (3, 0.194), (4, -0.221), (5, 0.101), (6, 0.036), (7, 0.024), (8, 0.051), (9, 0.044), (10, 0.031), (11, 0.032), (12, -0.104), (13, 0.042), (14, 0.019), (15, 0.046), (16, 0.222), (17, 0.125), (18, 0.011), (19, -0.006), (20, -0.031), (21, 0.058), (22, -0.013), (23, 0.009), (24, 0.005), (25, 0.007), (26, 0.124), (27, 0.012), (28, -0.041), (29, -0.28), (30, 0.089), (31, 0.12), (32, 0.076), (33, -0.008), (34, -0.095), (35, 0.122), (36, 0.078), (37, -0.146), (38, -0.016), (39, -0.052), (40, -0.027), (41, 0.073), (42, 0.022), (43, 0.093), (44, 0.069), (45, 0.058), (46, -0.014), (47, -0.08), (48, 0.227), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.964643 <a title="311-lsi-1" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>2 0.89339024 <a title="311-lsi-2" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>3 0.74466509 <a title="311-lsi-3" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>4 0.70471185 <a title="311-lsi-4" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>5 0.65341347 <a title="311-lsi-5" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>6 0.62558472 <a title="311-lsi-6" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>7 0.59823394 <a title="311-lsi-7" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>8 0.57817101 <a title="311-lsi-8" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>9 0.56014562 <a title="311-lsi-9" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>10 0.54395586 <a title="311-lsi-10" href="./nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">2 nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<p>11 0.53000391 <a title="311-lsi-11" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>12 0.50111872 <a title="311-lsi-12" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>13 0.49758598 <a title="311-lsi-13" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>14 0.46927771 <a title="311-lsi-14" href="./nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">140 nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<p>15 0.46282455 <a title="311-lsi-15" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>16 0.45980826 <a title="311-lsi-16" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>17 0.45743769 <a title="311-lsi-17" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>18 0.44620469 <a title="311-lsi-18" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>19 0.43641716 <a title="311-lsi-19" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>20 0.42486656 <a title="311-lsi-20" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.122), (47, 0.094), (52, 0.222), (60, 0.014), (67, 0.128), (70, 0.048), (85, 0.054), (94, 0.096), (99, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80084455 <a title="311-lda-1" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>2 0.78920621 <a title="311-lda-2" href="./nips-2012-Approximating_Equilibria_in_Sequential_Auctions_with_Incomplete_Information_and_Multi-Unit_Demand.html">45 nips-2012-Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand</a></p>
<p>3 0.73113209 <a title="311-lda-3" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>4 0.72445893 <a title="311-lda-4" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>5 0.72272718 <a title="311-lda-5" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>6 0.72033346 <a title="311-lda-6" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>7 0.71908551 <a title="311-lda-7" href="./nips-2012-Diffusion_Decision_Making_for_Adaptive_k-Nearest_Neighbor_Classification.html">97 nips-2012-Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification</a></p>
<p>8 0.71495718 <a title="311-lda-8" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>9 0.71432436 <a title="311-lda-9" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>10 0.71362829 <a title="311-lda-10" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>11 0.71331191 <a title="311-lda-11" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>12 0.70919645 <a title="311-lda-12" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>13 0.70596868 <a title="311-lda-13" href="./nips-2012-Learning_Multiple_Tasks_using_Shared_Hypotheses.html">181 nips-2012-Learning Multiple Tasks using Shared Hypotheses</a></p>
<p>14 0.70558536 <a title="311-lda-14" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>15 0.70525789 <a title="311-lda-15" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>16 0.70470321 <a title="311-lda-16" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>17 0.70435858 <a title="311-lda-17" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>18 0.70433307 <a title="311-lda-18" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>19 0.70364833 <a title="311-lda-19" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>20 0.70252955 <a title="311-lda-20" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
