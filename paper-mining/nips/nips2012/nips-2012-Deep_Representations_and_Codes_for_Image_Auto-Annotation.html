<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-92" href="#">nips2012-92</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</h1>
<br/><p>Source: <a title="nips-2012-92-pdf" href="http://papers.nips.cc/paper/4704-deep-representations-and-codes-for-image-auto-annotation.pdf">pdf</a></p><p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>Reference: <a title="nips-2012-92-reference" href="../nips2012_reference/nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. [sent-3, score-0.533]
</p><p>2 Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. [sent-4, score-0.366]
</p><p>3 In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. [sent-5, score-0.48]
</p><p>4 ), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. [sent-8, score-0.571]
</p><p>5 Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. [sent-9, score-0.296]
</p><p>6 1  Introduction  The development of successful methods for training deep architectures have inﬂuenced the development of representation learning algorithms either on top of SIFT descriptors [1, 2] or raw pixel input [3, 4, 5] for feature extraction of full-sized images. [sent-11, score-0.327]
</p><p>7 Furthermore, self-taught learning [6] can be employed, taking advantage of feature learning from image databases independent of the target dataset. [sent-13, score-0.304]
</p><p>8 Image auto-annotation is a multi-label classiﬁcation task of assigning a set of relevant, descriptive tags to an image where tags often come from a vocabulary of hundreds to thousands of words. [sent-14, score-0.705]
</p><p>9 Tags may describe objects, colors, scenes, local regions of the image (e. [sent-17, score-0.238]
</p><p>10 Consequently, many of the most successful annotation algorithms in the literature [7, 8, 9, 10, 11] have opted to focus on tag assignment and often ﬁx a large number of hand-crafted features for input to their algorithms. [sent-22, score-0.405]
</p><p>11 Our main contribution in this paper is to remove the need to compute over a dozen hand-crafted features for annotating images and consequently remove the need for feature selection. [sent-26, score-0.402]
</p><p>12 We introduce a deep learning algorithm for learning hierarchical representations of full-sized color images from the pixel level, which may be seen as a generalization of the approach by Coates et al. [sent-27, score-0.453]
</p><p>13 For annotation, we use the TagProp discriminitve metric learning algorithm [9] which has enjoyed state-of-the-art performance on popular annotation benchmarks. [sent-31, score-0.278]
</p><p>14 This gives the advantage of focusing new research on improving tag assignment algorithms without the need of deciding which features are best suited for the task. [sent-34, score-0.163]
</p><p>15 Figure 1: Sample annotation results on IAPRTC-12 (top) and ESP-Game (bottom) using TagProp when each image is represented by a 256-bit code. [sent-35, score-0.445]
</p><p>16 The ﬁrst column of tags is the gold standard and the second column are the predicted tags. [sent-36, score-0.232]
</p><p>17 Predicted tags that are italic are those that are also gold standard. [sent-37, score-0.232]
</p><p>18 [10] who construct visual synsets of images and Weston et al. [sent-40, score-0.168]
</p><p>19 Our second contribution proposes the use of representing an image with a 256-bit code for annotation. [sent-42, score-0.203]
</p><p>20 [14] performed an extensive analysis of small codes for image retrieval showing that even on databases with millions of images, linear search with Hamming distance can be performed efﬁciently. [sent-44, score-0.549]
</p><p>21 We utilize an autoencoder with a single hidden layer on top of our learned hierarchical representations to construct codes. [sent-45, score-0.287]
</p><p>22 In exchange, 256-bit codes are efﬁcient to store and can be compared quickly with bitwise operations. [sent-47, score-0.166]
</p><p>23 To our knowledge, our approach is the ﬁrst to learn binary codes from full-sized color images without the use of handcrafted features. [sent-48, score-0.401]
</p><p>24 2  Hierarchical representation learning  In this section we describe our approach for learning a deep feature representation from the pixellevel of a color image. [sent-51, score-0.261]
</p><p>25 Our approach involves aspects of typical pipelines: pre-processing and whitening, dictionary learning, convolutional extraction and pooling. [sent-52, score-0.22]
</p><p>26 We deﬁne a module as a pass through each of the above operations. [sent-53, score-0.46]
</p><p>27 Extract randomly selected patches from each image and apply pre-processing. [sent-57, score-0.203]
</p><p>28 Convolve the dictionary with larger tiles extracted across the image with a pre-deﬁned stride length. [sent-61, score-0.447]
</p><p>29 Pool over the reassembled features with a 2 layer pyramid. [sent-64, score-0.163]
</p><p>30 Given a receptive ﬁeld of size r ×c, we ﬁrst extract np patches across all images of size r ×c×3, followed by ﬂatting each patch into a column vector. [sent-77, score-0.401]
</p><p>31 Next we follow [13] by performing ZCA whitening, which results in patches having zero mean, np np 1 (i) = 0, and identity covariance, np i=1 x(i) (x(i) )T = I. [sent-87, score-0.369]
</p><p>32 K-SVD constructs a ˆ dictionary D ∈ Rn×k and a sparse representation S ∈ Rk×np by solving the following optimization problem: ˆ S − DS  minimize ˆ D,S  2 F  subject to ||ˆ(i) ||0 ≤ q ∀i s  (1)  where k is the desired number of bases. [sent-98, score-0.156]
</p><p>33 When D is ﬁxed, the ˆ problem of obtaining S can be decomposed into np subproblems of the form s(i) − Dˆ(i) 2 subject s (i) to ||ˆ ||0 ≤ q ∀i which can be solved approximately using batch orthogonal matching pursuit [15]. [sent-100, score-0.212]
</p><p>34 3  Convolutional feature extraction  Given an image I (i) , we ﬁrst partition the image into a set of tiles T (i) of size nt × nt with a pre(i) deﬁned stride length s between each tile. [sent-109, score-0.752]
</p><p>35 Each patch in tile Tt is processed in the same way as before dictionary construction (mean centering, contrast normalization, whitening) for which the (i) mean and whitening matrices M and W are used. [sent-110, score-0.347]
</p><p>36 Let Ttj denote the t-th tile and j-th channel with (l)  respect to image I (i) and let Dj ∈ Rr×c denote the l-th basis for channel j of D. [sent-111, score-0.312]
</p><p>37 The encoding (i)  ftl for tile t and basis l is given by: 3 (i)  (i)  (l)  Ttj ∗ Dj  ftl = max tanh  ,0  (3)  j=1  where * denotes convolution and max and tanh operations are applied componentwise. [sent-112, score-0.281]
</p><p>38 Let ft denote the concatenated encodings over bases, which have a resulting dimension of (nt − r + 1) × (nt − c + 1) × k. [sent-120, score-0.156]
</p><p>39 Figure 2: Left: D is convolved with each tile (large green square) with receptive ﬁeld (small blue square) over a given stride. [sent-127, score-0.193]
</p><p>40 4  Pooling  The ﬁnal step of our pipeline is to perform spatial pooling over the re-assembled regions of the (i) encodings ft . [sent-131, score-0.31]
</p><p>41 Consider the l-th cross section corresponding to the l-th dictionary element, l ∈ {1, . [sent-132, score-0.202]
</p><p>42 We may then pool over each of the spatial regions of this cross section by summing over the activations of the corresponding spatial regions. [sent-136, score-0.228]
</p><p>43 This is done in the form of a 2-layer spatial pyramid, where the base of the pyramid consists of 4 blocks of 2×2 tiling and the top of the pyramid consisting of a single block across the whole cross section. [sent-137, score-0.317]
</p><p>44 Once pooling is performed, the re-assembled encodings result in a shape of size 1 × 1 × k and 2 × 2 × k from each layer of the pyramid. [sent-139, score-0.238]
</p><p>45 To obtain the ﬁnal feature vector, each layer is ﬂattened into a vector and the resulting vectors are concatinated into a single long feature vector of dimension 5k for each image I (i) . [sent-140, score-0.391]
</p><p>46 5  Training multiple modules  What we have described up until now is how to extract features using a single module corresponding to dictionary learning, extraction and pooling. [sent-143, score-0.772]
</p><p>47 Once the ﬁrst module has been trained, we can take the pooled features to be input to a second module. [sent-145, score-0.557]
</p><p>48 Freezing the learned dictionary from the ﬁrst module, we can then apply all the same steps a second time to the pooled representations. [sent-146, score-0.228]
</p><p>49 To be more speciﬁc on the input to the second module, we use an additional spatial pooling operation on the re-assembled encodings of the ﬁrst module, where we extract 256 blocks of 16 × 16 tiling, resulting in a representation of size 16×16×k. [sent-148, score-0.266]
</p><p>50 As an illustration, the same operations for the second module are used as in ﬁgure 2 except the image is replaced with the 16 × 16 × k pooled features. [sent-151, score-0.695]
</p><p>51 3  Code construction and discriminitive metric learning  In this section we ﬁrst show to learn binary codes from our learned features, followed by a review of the TagProp algorithm [9] used for annotation. [sent-153, score-0.239]
</p><p>52 1  Learning binary codes for annotation  Our codes are learned by adding an autoencoder with a single hidden layer on top of the learned output representations. [sent-155, score-0.802]
</p><p>53 Let f (i) ∈ Rdm denote the learned representation for image I (i) of dimension dm using either a one or two module architecture. [sent-156, score-0.697]
</p><p>54 Figure 3: Coding layer activaAs is, the optimization does not take into consideration the round- tion values after training the auing used in the coding layer and consequently the output is not toencoder. [sent-160, score-0.313]
</p><p>55 [17] and use additive ‘deterministic’ Gaussian noise with zero mean in the coding layer that is ﬁxed in advance for each datapoint when performing a bottom-up pass through the network. [sent-163, score-0.201]
</p><p>56 Figure 3 shows the coding layer activation values after backpropagation when noise has been added. [sent-166, score-0.165]
</p><p>57 2  The tag propagation (TagProp) algorithm  Let V denote a ﬁxed vocabulary of tags and I denote a list of input images. [sent-168, score-0.368]
</p><p>58 Our goal at test time, given a new input image i , is to assign a set of tags v ∈ V that are most relevant to the content of i . [sent-169, score-0.435]
</p><p>59 More speciﬁcally, let yiw ∈ {1, −1}, i ∈ I, w ∈ V be an indicator for whether tag w is present in image i. [sent-171, score-0.408]
</p><p>60 In TagProp, the probability that yiw = 1 is given by σ(αw xiw + βw ), xiw = −1 is the logistic function, (αw , βw ) are word-speciﬁc j πij yjw where σ(z) = (1 + exp(−z)) model parameters to be estimated and πij are distance-based weights also to be estimated. [sent-172, score-0.213]
</p><p>61 More speciﬁcally, πij is expressed as  πij =  exp(−dh (i, j)) , j exp(−dh (i, j ))  dh (i, j) = hdij ,  h≥0  (4)  where we shall call dij the base distance between images i and j. [sent-173, score-0.274]
</p><p>62 The model is trained to maximize the following quasilikelihood of the data given by L = i,w ciw log p(yiw ), ciw = n1 if yiw = 1 and n1 otherwise, + − where n+ is the total number of positive labels of w and likewise for n− and missing labels. [sent-175, score-0.213]
</p><p>63 Combined with the logistic word models, it accounts for much higher recall in rare tags which would normally be less likely to be recalled in a basic k-NN setup. [sent-177, score-0.293]
</p><p>64 The choice of base distance used depends on the image representation. [sent-179, score-0.279]
</p><p>65 For all our experiments, we use k1 = 512 ﬁrst module bases, k2 = 1024 second module bases, receptive ﬁeld sizes of 6 × 6 and 2 × 2 and tile sizes (nt ) of 16 × 16 and 6 × 6. [sent-187, score-1.041]
</p><p>66 The total number of features for the combined ﬁrst and second module representation is thus 5(k1 + k2 ) = 7680. [sent-188, score-0.522]
</p><p>67 The ﬁrst module stride length is chosen based on the length of the longest side of the image: 4 if the side is less than 128 pixels, 6 if less than 214 pixels and 8 otherwise. [sent-190, score-0.532]
</p><p>68 3 We also incorporate the use of self-taught learning [6] in our annotation experiments by utilizing the Mirﬂickr dataset for dictionary learning. [sent-194, score-0.408]
</p><p>69 We randomly sampled 10000 images from this dataset for training K-SVD on both modules. [sent-196, score-0.217]
</p><p>70 1  STL-10  The STL-10 dataset is a collection of 96 × 96 images of 10 classes with images partitioned into 10 folds of 1000 Table 1: A selection of the best results obtained on the STLimages each and a test set of size 10 dataset. [sent-200, score-0.291]
</p><p>71 1 % randomly chose 10000 images from the unlabeled set for training and use a linear L2-SVM for classiﬁcation with 5-fold cross validation for model selection. [sent-212, score-0.253]
</p><p>72 Our 2 module architecture outperforms all existing approaches except for the recently proposed hierarchical matching pursuit (HMP). [sent-214, score-0.569]
</p><p>73 2  Natural scenes  The Natural Scenes dataset is a multi-label collection of 2000 images from 5 classes: desert, forest, mountain, ocean and sunset. [sent-220, score-0.269]
</p><p>74 3  IAPRTC-12 and ESP-Game  IAPRTC-12 is a collection of 20000 images with a vocabulary size of |V | = 291 and an average of 5. [sent-318, score-0.162]
</p><p>75 Using standard protocol performance is evaluated using 3 measures: precision (P), recall (R) and the number of recalled tags (N+). [sent-324, score-0.293]
</p><p>76 N + indicates the number of tags that were recalled at least once for annotation on the test set. [sent-325, score-0.535]
</p><p>77 Annotations are made by choosing the 5 most probable tags for each image as is done with previous evaluations. [sent-326, score-0.435]
</p><p>78 As with the natural scenes dataset, we perform 5-fold cross validation to determine K for training TagProp. [sent-327, score-0.231]
</p><p>79 Our 256-bit codes suffer a loss of performance on IAPRTC-12 but give near equivalent results on ESP-Game. [sent-332, score-0.166]
</p><p>80 Figure 4 shows sample unsupervised retrieval results using the learned 256-bit codes on IAPRTC-12 and ESP-Game while ﬁgure 5 illustrates sample annotation performance when training on one dataset and annotating the other. [sent-335, score-0.724]
</p><p>81 These results show that our codes are able to capture high-level semantic concepts that perform well for retrieval and transfer learning across datasets. [sent-336, score-0.205]
</p><p>82 We note however, that when annotating ESP-game when training was done on IAPRTC-12 led to more false human annotations (such as the bottom-right 7  Figure 4: Sample 256-bit unsupervised retrieval results on ESP-Game (top) and IAPRTC-12 (bottom). [sent-337, score-0.236]
</p><p>83 A query image from the test set is used to retrieve the four nearest neighbors from the training set. [sent-338, score-0.288]
</p><p>84 Figure 5: Sample 256-bit annotation results when training on one dataset and annotating the other. [sent-339, score-0.442]
</p><p>85 5  Conclusion  In this paper we introduced a hierarchical model for learning feature representations of standard sized color images for the task of image annotation. [sent-344, score-0.559]
</p><p>86 Our results compare favorably to existing approaches that use over a dozen handcrafted image descriptors. [sent-345, score-0.329]
</p><p>87 Our primary goal for future work is to test the effectiveness of this approach on web-scale annotation systems with millions of images. [sent-346, score-0.284]
</p><p>88 The success of self-taught learning in this setting means only one dictionary per module ever needs to be learned. [sent-347, score-0.547]
</p><p>89 It is our hope that the successful use of binary codes for annotation will allow further research to bridge the gap between the annotation algorithms used on small scale problems to those required for web scale tasks. [sent-349, score-0.65]
</p><p>90 We also intend to evaluate the effectiveness of semantic hashing on large databases when much smaller codes are used. [sent-350, score-0.222]
</p><p>91 Linear spatial pyramid matching using sparse coding for image classiﬁcation. [sent-357, score-0.425]
</p><p>92 Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. [sent-373, score-0.2]
</p><p>93 Learning image representations from the pixel level via hierarchical sparse coding. [sent-379, score-0.338]
</p><p>94 Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation. [sent-405, score-0.274]
</p><p>95 Large scale image annotation: learning to rank with joint wordimage embeddings. [sent-422, score-0.203]
</p><p>96 Discovering binary codes for documents by learning deep generative models. [sent-462, score-0.27]
</p><p>97 Beyond spatial pyramids: Receptive ﬁeld learning for pooled image features. [sent-496, score-0.328]
</p><p>98 Multi-label learning by image-to-class distance for scene classiﬁcation and image annotation. [sent-510, score-0.246]
</p><p>99 Multiple Bernoulli relevance models for image and video annotation. [sent-522, score-0.203]
</p><p>100 Using very deep autoencoders for content-based image retrieval. [sent-529, score-0.341]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('module', 0.424), ('tagprop', 0.374), ('annotation', 0.242), ('tags', 0.232), ('image', 0.203), ('codes', 0.166), ('images', 0.124), ('dictionary', 0.123), ('np', 0.123), ('tile', 0.109), ('annotating', 0.107), ('yiw', 0.107), ('deep', 0.104), ('scenes', 0.102), ('layer', 0.098), ('tag', 0.098), ('ickr', 0.094), ('receptive', 0.084), ('whitening', 0.081), ('rdm', 0.08), ('cross', 0.079), ('encodings', 0.078), ('ft', 0.078), ('coates', 0.076), ('dh', 0.074), ('stride', 0.074), ('guillaumin', 0.071), ('modules', 0.068), ('pooled', 0.068), ('coding', 0.067), ('handcrafted', 0.065), ('mir', 0.065), ('features', 0.065), ('hamming', 0.064), ('bases', 0.064), ('nt', 0.062), ('pooling', 0.062), ('recalled', 0.061), ('dozen', 0.061), ('rubinstein', 0.058), ('spatial', 0.057), ('hierarchical', 0.056), ('extraction', 0.056), ('databases', 0.056), ('autoencoder', 0.056), ('bo', 0.055), ('cvpr', 0.054), ('ccd', 0.053), ('centering', 0.053), ('ciw', 0.053), ('ftl', 0.053), ('rdb', 0.053), ('ttj', 0.053), ('xiw', 0.053), ('pyramid', 0.052), ('training', 0.05), ('rgb', 0.049), ('alberta', 0.049), ('hmp', 0.047), ('ages', 0.047), ('tiles', 0.047), ('db', 0.046), ('color', 0.046), ('matching', 0.046), ('sized', 0.045), ('feature', 0.045), ('et', 0.044), ('tsai', 0.044), ('krizhevsky', 0.044), ('omp', 0.044), ('tiling', 0.044), ('distance', 0.043), ('dataset', 0.043), ('pursuit', 0.043), ('millions', 0.042), ('convolutional', 0.041), ('representations', 0.04), ('unsupervised', 0.04), ('edmonton', 0.039), ('pixel', 0.039), ('retrieval', 0.039), ('vocabulary', 0.038), ('exchange', 0.037), ('eld', 0.037), ('learned', 0.037), ('extract', 0.036), ('metric', 0.036), ('pass', 0.036), ('oe', 0.036), ('huang', 0.036), ('distances', 0.035), ('nearest', 0.035), ('regions', 0.035), ('autoencoders', 0.034), ('longest', 0.034), ('patch', 0.034), ('representation', 0.033), ('gure', 0.033), ('base', 0.033), ('tanh', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="92-tfidf-1" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>2 0.18366411 <a title="92-tfidf-2" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>3 0.16545559 <a title="92-tfidf-3" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>4 0.15072137 <a title="92-tfidf-4" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>Author: Mohammad Norouzi, David M. Blei, Ruslan Salakhutdinov</p><p>Abstract: Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efﬁcient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a ﬂexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST, with promising classiﬁcation results using no more than kNN on the binary codes. 1</p><p>5 0.14833777 <a title="92-tfidf-5" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>6 0.13972878 <a title="92-tfidf-6" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>7 0.12107044 <a title="92-tfidf-7" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>8 0.1186135 <a title="92-tfidf-8" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>9 0.11521566 <a title="92-tfidf-9" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>10 0.11310365 <a title="92-tfidf-10" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>11 0.11310365 <a title="92-tfidf-11" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>12 0.11028014 <a title="92-tfidf-12" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>13 0.10404886 <a title="92-tfidf-13" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>14 0.10226095 <a title="92-tfidf-14" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>15 0.095000423 <a title="92-tfidf-15" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>16 0.093641721 <a title="92-tfidf-16" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>17 0.092596784 <a title="92-tfidf-17" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>18 0.091784045 <a title="92-tfidf-18" href="./nips-2012-Discriminative_Learning_of_Sum-Product_Networks.html">100 nips-2012-Discriminative Learning of Sum-Product Networks</a></p>
<p>19 0.091220044 <a title="92-tfidf-19" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>20 0.086237065 <a title="92-tfidf-20" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, 0.07), (2, -0.249), (3, -0.016), (4, 0.139), (5, -0.055), (6, -0.017), (7, -0.029), (8, 0.05), (9, -0.014), (10, 0.053), (11, 0.023), (12, -0.037), (13, 0.122), (14, -0.14), (15, -0.01), (16, 0.041), (17, -0.012), (18, 0.022), (19, -0.107), (20, 0.017), (21, -0.046), (22, -0.003), (23, 0.012), (24, 0.013), (25, 0.017), (26, -0.039), (27, 0.028), (28, 0.011), (29, 0.064), (30, -0.035), (31, 0.031), (32, 0.059), (33, -0.122), (34, 0.095), (35, 0.1), (36, 0.014), (37, 0.076), (38, 0.078), (39, -0.061), (40, 0.014), (41, -0.018), (42, -0.046), (43, 0.087), (44, 0.009), (45, -0.051), (46, -0.06), (47, -0.044), (48, -0.007), (49, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95412755 <a title="92-lsi-1" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>2 0.80664337 <a title="92-lsi-2" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>Author: Aditya Khosla, Jianxiong Xiao, Antonio Torralba, Aude Oliva</p><p>Abstract: While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. 1</p><p>3 0.80512011 <a title="92-lsi-3" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>Author: Richard Socher, Brody Huval, Bharath Bath, Christopher D. Manning, Andrew Y. Ng</p><p>Abstract: Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, ﬁxed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efﬁcient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. 1</p><p>4 0.79987705 <a title="92-lsi-4" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>5 0.75884449 <a title="92-lsi-5" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>6 0.74811929 <a title="92-lsi-6" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>7 0.74374282 <a title="92-lsi-7" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>8 0.73449558 <a title="92-lsi-8" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>9 0.73368394 <a title="92-lsi-9" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>10 0.72857445 <a title="92-lsi-10" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>11 0.66742086 <a title="92-lsi-11" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>12 0.64437509 <a title="92-lsi-12" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>13 0.62990379 <a title="92-lsi-13" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>14 0.59264988 <a title="92-lsi-14" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>15 0.57404506 <a title="92-lsi-15" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>16 0.57295948 <a title="92-lsi-16" href="./nips-2012-Large_Scale_Distributed_Deep_Networks.html">170 nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>17 0.56410414 <a title="92-lsi-17" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>18 0.55910176 <a title="92-lsi-18" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>19 0.52090931 <a title="92-lsi-19" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>20 0.51388347 <a title="92-lsi-20" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.031), (21, 0.026), (38, 0.1), (42, 0.026), (44, 0.012), (47, 0.233), (53, 0.012), (54, 0.018), (55, 0.05), (74, 0.101), (76, 0.126), (80, 0.095), (92, 0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79467648 <a title="92-lda-1" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>2 0.68134886 <a title="92-lda-2" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>3 0.68059379 <a title="92-lda-3" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>Author: Gary Huang, Marwan Mattar, Honglak Lee, Erik G. Learned-miller</p><p>Abstract: Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face veriﬁcation. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Speciﬁcally, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the speciﬁc data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned ﬁlters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face veriﬁcation compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method. 1</p><p>4 0.68036723 <a title="92-lda-4" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>5 0.67719764 <a title="92-lda-5" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>6 0.67558175 <a title="92-lda-6" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>7 0.67475593 <a title="92-lda-7" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>8 0.67414278 <a title="92-lda-8" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>9 0.67062086 <a title="92-lda-9" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>10 0.66916108 <a title="92-lda-10" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>11 0.66912556 <a title="92-lda-11" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>12 0.66830999 <a title="92-lda-12" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>13 0.66727912 <a title="92-lda-13" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>14 0.66617173 <a title="92-lda-14" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>15 0.66273779 <a title="92-lda-15" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>16 0.66093767 <a title="92-lda-16" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>17 0.66061521 <a title="92-lda-17" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>18 0.66022849 <a title="92-lda-18" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>19 0.65816212 <a title="92-lda-19" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>20 0.6578663 <a title="92-lda-20" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
