<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-106" href="#">nips2012-106</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</h1>
<br/><p>Source: <a title="nips-2012-106-pdf" href="http://papers.nips.cc/paper/4770-dynamical-and-or-graph-learning-for-object-shape-modeling-and-detection.pdf">pdf</a></p><p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>Reference: <a title="nips-2012-106-reference" href="../nips2012_reference/nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. [sent-7, score-0.266]
</p><p>2 We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. [sent-8, score-0.161]
</p><p>3 (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. [sent-13, score-0.65]
</p><p>4 1  Introduction  Part-based and hierarchical representations have been widely studied in computer vision, and lead to some elegant frameworks for complex object detection and recognition. [sent-18, score-0.366]
</p><p>5 structural switch) in hierarchy, which is the key to handle the large intra-class variance in object detection. [sent-21, score-0.277]
</p><p>6 And-Or graph models are recently explored in [26, 27] to hierarchically model object categories via “and-nodes” and “or-nodes” that represent, respectively, compositions of parts and structural variation of parts. [sent-23, score-0.436]
</p><p>7 The leaf-nodes in the bottom layer represent a batch of local classiﬁers of contour fragments. [sent-29, score-0.261]
</p><p>8 1  the problem that the true contours of objects are often connected to background clutters due to unreliable edge extraction. [sent-36, score-0.399]
</p><p>9 Each or-node is used to select one contour from the candidates detected via the associated leaf-nodes in the bottom layer. [sent-39, score-0.406]
</p><p>10 The contours selected via the or-nodes are further veriﬁed as a whole, in order to make the detection robust against the background clutters. [sent-44, score-0.466]
</p><p>11 Concretely, our model allows nearby contours to interact with each other. [sent-46, score-0.274]
</p><p>12 , the layout of or-nodes and the activation of leaf-nodes) are implicitly inferred with the latent variables. [sent-52, score-0.161]
</p><p>13 2  Related Work  Remarkable progress has been made in shape-based object detection [6, 10, 9, 11, 19]. [sent-53, score-0.366]
</p><p>14 By employing some shape descriptors and matching schemes, many works represent and recognize object shapes as a loose collection of local contours. [sent-54, score-0.331]
</p><p>15 [6] used a codebook of PAS (pairwise adjacent segments) to localize object of interest; Maji et al. [sent-56, score-0.174]
</p><p>16 Recently, the tree structure latent models [25, 5] have provided signiﬁcant improvements on object detection. [sent-58, score-0.223]
</p><p>17 using conﬁgurable graph structures with And, Or nodes, has been applied in object and scene parsing [26, 18, 24] and action classiﬁcation [20]. [sent-67, score-0.346]
</p><p>18 3(a) illustrates, the square on the top is the root-node representing the complete object instances. [sent-70, score-0.174]
</p><p>19 The dashed circles derived from the root are z or-nodes arranged in a layout of b1 × b2 blocks, representing the object parts. [sent-71, score-0.246]
</p><p>20 , collaborative edges) are deﬁned between the leaf-nodes that are associated with different or-nodes, in order to encode the compatibility of object parts. [sent-85, score-0.25]
</p><p>21 Suppose a contour fragment c on the edge map X is captured by the block located at pi = (px , py ), as the input of classiﬁer. [sent-91, score-0.55]
</p><p>22 The response of classiﬁer Lj at location pi of the edge map X is deﬁned as: l RLj (X, pi ) = max ωj · ϕl (pi , c),  (1)  c∈X  l where ωj is a parameter vector, which is set to zero if the corresponding leaf-node Lj is nonexistent. [sent-94, score-0.479]
</p><p>23 l Then we can detect the contour from edge map X via the classiﬁer, cj = argmaxc∈X ωj · ϕl (pi , c). [sent-95, score-0.381]
</p><p>24 , z is proposed to specify a proper contour from a set of candidates detected via its children leaf-nodes. [sent-99, score-0.406]
</p><p>25 For each or-node Ui , we deﬁne the deformation feature as ϕs (p0 , pi ) = (dx, dy, dx2 , dy 2 ), where (dx, dy) is the displacement of the or-node position pi to the expected position p0 determined by the root-node. [sent-102, score-0.475]
</p><p>26 Then the cost of locating Ui at pi is: s Costi (p0 , pi ) = −ωi · ϕs (p0 , pi ),  (2)  s ωi  s  where is a 4-dimensional parameter vector corresponding to ϕ (p0 , pi ). [sent-103, score-0.704]
</p><p>27 For each leaf-node Lj associated with Ui , we introduce an indicator variable vj ∈ {0, 1} representing whether it is activated or not. [sent-105, score-0.186]
</p><p>28 Thus, the response of the or-node Ui is deﬁned as, ∑ RUi (X, p0 , pi , vi ) = RLj (X, pi ) · vj + Costi (p0 , pi ). [sent-110, score-0.728]
</p><p>29 (3) j∈ch(i)  Collaborative Edge: For any pair of leaf-nodes (Lj , Lj ′ ) respectively associated with two different or-nodes, we deﬁne the collaborative edge between them according to their contextual cooccurrence. [sent-111, score-0.155]
</p><p>30 That is, how likely it is that the object contains contours detected via the two leaf-nodes. [sent-112, score-0.549]
</p><p>31 Root-node: The root-node represents a global classiﬁer to verify the ensemble of contour fragments C r = {c1 , . [sent-121, score-0.392]
</p><p>32 For better understanding, we refer H = (P, V ) as the latent variables during inference, where P implies the deformation of parts represented by the or-nodes and V implies the discrete distribution of leaf-nodes (i. [sent-130, score-0.149]
</p><p>33 4  Inference  The inference task is to localize the optimal contour fragments within the detection window, which is slidden at all scales and positions of the edge map X. [sent-156, score-0.663]
</p><p>34 Assuming the root-node is located at p0 , the object shape is localized by maximizing RG (X, H) deﬁned in (6): S(p0 , X) = max RG (X, H). [sent-157, score-0.297]
</p><p>35 the local classiﬁers) are utilized to detect contour fragments within the edge map X. [sent-160, score-0.471]
</p><p>36 Assume that leaf-node Lj , j ∈ ch(i) associated with Ui is activated, vj = 1, and the optimal contour fragment cj is localized by maximizing the response in Eq. [sent-161, score-0.477]
</p><p>37 Then we generate i,j a set of candidates for each or-node, {cj , p∗ }, each of which is one detected contour fragments via i,j the leaf-nodes. [sent-163, score-0.537]
</p><p>38 These sets of candidates will be passed to the top-down step where the leaf-node activation vi for Ui can be further validated. [sent-164, score-0.143]
</p><p>39 We calculate the response for the bottom-up step, as, z ∑ Rbot (V ) = (11) RUi (X, p0 , p∗ , vi ), i i=1  where V = {vi } denotes a hypothesis of leaf-node activation for all or-nodes. [sent-165, score-0.147]
</p><p>40 In practice, we can further prune the candidate contours by setting a threshold on Rbot (V ). [sent-166, score-0.274]
</p><p>41 Thus, given the V = {vi }, we can select an ensemble of contours C r = {c1 , . [sent-167, score-0.274]
</p><p>42 , cz }, each of which is detected by an activated leaf-node, Lj , vj = 1. [sent-170, score-0.337]
</p><p>43 Top-down veriﬁcation: Given the ensemble of contours C r , we then apply the global classiﬁer at the root-node to verify C r by Eq. [sent-171, score-0.274]
</p><p>44 By incorporating the bottom-up and top-down steps, we obtain the response of And-Or graph model by Eq. [sent-174, score-0.163]
</p><p>45 The ﬁnal detection is acquired by selecting the maximum score in Eq. [sent-176, score-0.192]
</p><p>46 This algorithm iterates to determine the And-Or graph structure in a dynamical manner: given the inferred latent variables H = (P, V ) in each step, the leaf-nodes can be automatically created or removed to generate a new structural conﬁguration. [sent-179, score-0.415]
</p><p>47 To be speciﬁc, a new leaf-node is encouraged to be created as the local detector for contours that cannot be handled by the current model(Fig. [sent-180, score-0.359]
</p><p>48 (13)  The optimization of this function can be solved by using structural SVM with latent variables, ∑ 1 min ∥ω∥2 + D [max(ω · ϕ(Xk , y, H) + L(yk , y, H)) − max(ω · ϕ(Xk , yk , H))], ω 2 y,H H N  (14)  k=1  where D is a penalty parameter(set as 0. [sent-192, score-0.4]
</p><p>49 We deﬁne that L(yk , y, H) = 0 if yk = y, “1” if yk ̸= y in our method. [sent-194, score-0.496]
</p><p>50 (14) into a convex and concave form as, ∑ ∑ 1 min[ ∥ω∥2 + D max(ω · ϕ(Xk , y, H) + L(yk , y, H))] − [D max(ω · ϕ(Xk , yk , H))] ω y,H H 2 N  N  k=1  k=1  = min[f (ω) − g(ω)],  (15) (16)  ω  where f (ω) represents the ﬁrst two terms, and g(ω) represents the last term in (15). [sent-203, score-0.248]
</p><p>51 The original CCCP includes two iterative steps: (I) ﬁxing the model parameters, estimate the latent variables H ∗ for each positive samples; (II) compute the model parameters by the traditional structural SVM method. [sent-204, score-0.152]
</p><p>52 (I) For optimization, we ﬁrst ﬁnd a hyperplane qt to upper bound the concave part −g(ω) in Eq. [sent-210, score-0.187]
</p><p>53 We construct qt by ∗ calculating the optimal latent variables Hk = argmaxH (ωt ·ϕ(Xk , yk , H)). [sent-213, score-0.417]
</p><p>54 Since ϕ(Xk , yk , H) = 0 when yk = −1, we only take the positive training samples into account during computation. [sent-214, score-0.496]
</p><p>55 Then ∑N ∗ the hyperplane is constructed as qt = −D k=1 ϕ(Xk , yk , Hk ). [sent-215, score-0.435]
</p><p>56 Accordingly, the hyperplane qt would change with ϕ(X, y, H ∗ ), and would lead to non-convergence of learning. [sent-219, score-0.187]
</p><p>57 Given ϕ(Xk , yk , Hk ) of all positive samples, we apply PCA on them, K ∑ ∗ ϕ(Xk , yk , Hk ) ≈ u + βk,i ei , (18) i=1  where K is the number of the eigenvectors, ei the eigenvector with its parameter βk,i . [sent-225, score-0.578]
</p><p>58 We set K a ∑K ∗ large number so that ||ϕ(Xk , yk , Hk ) − (u + i=1 βk,i ei )||2 < σ, ∀k. [sent-226, score-0.289]
</p><p>59 For the jth bin of the feature 5  ( 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  (a)  (b)  (c)  Figure 2: A toy example for structural clustering. [sent-227, score-0.206]
</p><p>60 (a) shows the feature vectors ϕ of the samples associated with Ui , and the intensity of the feature bin indicates the feature value. [sent-232, score-0.271]
</p><p>61 The red and green bounding boxes on the vectors indicate the non-principal features representing the detected contour fragments via two different leaf-nodes. [sent-233, score-0.527]
</p><p>62 For each or-node Ui , a set of detected contour fragments, {c1 , c2 , . [sent-242, score-0.362]
</p><p>63 The feature vectors for these contours that are generated by the leaf-nodes, {ϕl (p1 , c1 ), . [sent-246, score-0.375]
</p><p>64 More speciﬁcally, once we select the jth bin for the l all feature vectors ϕ , it can be either principal or not in different vectors ϕ. [sent-253, score-0.171]
</p><p>65 We thus refactor the feature vectors of these contours as {ϕ′ (p1 , c1 ), . [sent-255, score-0.375]
</p><p>66 To trigger the structural reconﬁguration, for each ornode Ui , we perform the clustering for detected contour fragments represented by the newly formed feature vectors. [sent-260, score-0.709]
</p><p>67 We ﬁrst group the contours detected by the same leaf-node into the same cluster as a temporary partition. [sent-261, score-0.375]
</p><p>68 And the close contours are grouped into the same cluster. [sent-263, score-0.315]
</p><p>69 represent the similar contour with the same bins in the complete feature vector ϕ. [sent-266, score-0.328]
</p><p>70 Please recall that the vector of one contour is part of ϕ. [sent-267, score-0.261]
</p><p>71 Their parameters can be learned based on the feature vectors of contours within the clusters. [sent-274, score-0.375]
</p><p>72 • One leaf-node is removed when the feature bins related to it are zero, which implies the contours detected by the leaf-node are grouped to another cluster. [sent-275, score-0.528]
</p><p>73 After the structural reconﬁguration, we denote ∗ ∗ all the feature vectors ϕ(Xk , yk , Hk ) are adjusted to ϕd (Xk , yk , Hk ). [sent-279, score-0.7]
</p><p>74 Then the new hyperplane is ∑N d ∗ generated as qt = −D k=1 ϕd (Xk , yk , Hk ). [sent-280, score-0.435]
</p><p>75 ∗ (III) Given the newly generated model structures represented by the feature vectors ϕd (Xk , yk , Hk ), d we can learn the model parameters by solving ωt+1 = argminω [f (ω) + ω · qt ]. [sent-281, score-0.469]
</p><p>76 By substituting d −g(ω) with the upper bound hyperplane qt , the optimization task in Eq. [sent-282, score-0.187]
</p><p>77 (15) can be rewritten as,  ∑ 1 ∗ min ∥ω∥2 + D [max(ω · ϕ(Xk , y, H) + L(yk , y, H)) − ω · ϕd (Xk , yk , Hk )]. [sent-283, score-0.248]
</p><p>78 , U8 ; a practical detection with the activated leafnodes are highlighted by red. [sent-296, score-0.285]
</p><p>79 ω∗ = D  ∑  ∗ αk,y,H ∆ϕ(Xk , y, H),  (20)  k,y,H ∗ where ∆ϕ(Xk , y, H) = ϕd (Xk , yk , Hk ) − ϕ(Xk , y, H). [sent-298, score-0.248]
</p><p>80 For each training sample (whose contours have been extracted), we partition it into a regular layout of several blocks, each of which corresponds to one or-node. [sent-305, score-0.346]
</p><p>81 The contours fallen into the block are treated as the input for learning. [sent-306, score-0.32]
</p><p>82 Once there are more than two contours in one block, we select the one with largest length. [sent-307, score-0.274]
</p><p>83 Then the leaf-nodes are generated by clustering the selected contours without any constraints, and we can thus obtain the initial feature vector ϕd for each sample. [sent-308, score-0.341]
</p><p>84 6  Experiments  We evaluate our method for object shape detection, using three benchmark datasets: the UIUCPeople [17], the ETHZ-Shape [7] and the INRIA-Horse [7]. [sent-309, score-0.297]
</p><p>85 For positive samples, we extract their clutter-free object contours; for negative samples, we compute their edge maps by using the Pb edge detector [12] with an edge link method. [sent-316, score-0.445]
</p><p>86 During detection, the edge maps of test images are extracted as for negative training samples, within which the object is searched at 6 different scales, 2 per octave. [sent-318, score-0.307]
</p><p>87 For each contour as the input to the leaf-node, we sample 20 points and compute the Shape Context descriptor for each point; the descriptor is quantized with 6 polar angles and 2 radial bins. [sent-319, score-0.329]
</p><p>88 We adopt the testing criterion deﬁned in the PASCAL VOC challenge: a detection is counted as correct if the intersection over union with the groundtruth is at least 50%. [sent-320, score-0.192]
</p><p>89 To evaluate the beneﬁt from the collaborative edges, we degenerate our model to the And-Or Tree (AOT) by removing the collaborative edges. [sent-326, score-0.152]
</p><p>90 3(c) illustrates, the average precisions (AP) of detection by applying AOG and AOT are 56. [sent-328, score-0.192]
</p><p>91 709  (b)  Table 1: (a) Comparisons of detection accuracies on the UIUC-People dataset. [sent-383, score-0.192]
</p><p>92 metric mentioned in [18], to calculate the detection accuracy, we only consider the detection with the highest score on an image for all the methods. [sent-385, score-0.384]
</p><p>93 (b),(c) and (d) shows a few object shape detections by applying our method on the three datasets, and the false positives are annotated by blue frames. [sent-402, score-0.297]
</p><p>94 It is shown that our system substantially outperforms the recent methods: the AOG and AOT models achieve detection rates of 89. [sent-409, score-0.192]
</p><p>95 We test our method with more object categories on the ETHZ-Shape dataset: Applelogos, Bottles, Giraffes, Mugs and Swans. [sent-418, score-0.174]
</p><p>96 7  Conclusion  This paper proposes a discriminative contour-based object model with the And-Or graph representation. [sent-425, score-0.347]
</p><p>97 Our method achieves the state-of-art of object shape detection on challenging datasets. [sent-427, score-0.489]
</p><p>98 Schiele, Pictorial structures revisited: People detection and articulated pose estimation, In CVPR, 2009. [sent-435, score-0.192]
</p><p>99 Schiele, Discriminative structure learning of hierarchical representations for object detection, In CVPR, 2009. [sent-493, score-0.174]
</p><p>100 Joachims, Learning structural svms with latent variables, In ICML, 2009. [sent-527, score-0.152]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('recon', 0.3), ('contours', 0.274), ('contour', 0.261), ('yk', 0.248), ('cccp', 0.206), ('detection', 0.192), ('pi', 0.176), ('object', 0.174), ('xk', 0.167), ('aog', 0.161), ('aot', 0.161), ('lj', 0.159), ('hk', 0.138), ('fragments', 0.131), ('shape', 0.123), ('qt', 0.12), ('graph', 0.115), ('ui', 0.111), ('structural', 0.103), ('detected', 0.101), ('guration', 0.097), ('vz', 0.094), ('vj', 0.093), ('activated', 0.093), ('edge', 0.079), ('collaborative', 0.076), ('latecki', 0.075), ('layout', 0.072), ('fppi', 0.069), ('iksvm', 0.069), ('neigh', 0.069), ('feature', 0.067), ('malik', 0.067), ('hyperplane', 0.067), ('maji', 0.062), ('rg', 0.062), ('cvpr', 0.06), ('vi', 0.059), ('discriminative', 0.058), ('ch', 0.058), ('parsing', 0.057), ('deformation', 0.056), ('zhu', 0.054), ('images', 0.054), ('pz', 0.053), ('rui', 0.053), ('dynamical', 0.052), ('created', 0.051), ('yuille', 0.05), ('cz', 0.05), ('veri', 0.05), ('switch', 0.05), ('latent', 0.049), ('vn', 0.049), ('response', 0.048), ('srinivasan', 0.046), ('applelogos', 0.046), ('clutters', 0.046), ('costi', 0.046), ('dcccp', 0.046), ('fallen', 0.046), ('felz', 0.046), ('giraffes', 0.046), ('mugs', 0.046), ('ornode', 0.046), ('rbot', 0.046), ('rlj', 0.046), ('uiucpeople', 0.046), ('removed', 0.045), ('ferrari', 0.045), ('tpami', 0.045), ('guided', 0.045), ('ap', 0.045), ('parts', 0.044), ('candidates', 0.044), ('ei', 0.041), ('grouped', 0.041), ('cj', 0.041), ('guangzhou', 0.041), ('andriluka', 0.041), ('bottles', 0.041), ('mumford', 0.041), ('schnitzspan', 0.041), ('tran', 0.041), ('activation', 0.04), ('eccv', 0.039), ('dynamically', 0.037), ('particle', 0.037), ('er', 0.036), ('bin', 0.036), ('schiele', 0.035), ('bourdev', 0.035), ('edges', 0.035), ('voting', 0.035), ('china', 0.035), ('vectors', 0.034), ('detector', 0.034), ('recognize', 0.034), ('descriptor', 0.034), ('fragment', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="106-tfidf-1" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>2 0.20202087 <a title="106-tfidf-2" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>3 0.17466502 <a title="106-tfidf-3" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>4 0.16262116 <a title="106-tfidf-4" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>5 0.15282668 <a title="106-tfidf-5" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>6 0.14533134 <a title="106-tfidf-6" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>7 0.14384542 <a title="106-tfidf-7" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>8 0.13256101 <a title="106-tfidf-8" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>9 0.13252604 <a title="106-tfidf-9" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>10 0.13158651 <a title="106-tfidf-10" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>11 0.12494597 <a title="106-tfidf-11" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>12 0.11267956 <a title="106-tfidf-12" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>13 0.10900647 <a title="106-tfidf-13" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>14 0.10629623 <a title="106-tfidf-14" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>15 0.10493867 <a title="106-tfidf-15" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>16 0.10493435 <a title="106-tfidf-16" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>17 0.10138432 <a title="106-tfidf-17" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>18 0.089822128 <a title="106-tfidf-18" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>19 0.088017076 <a title="106-tfidf-19" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>20 0.083249398 <a title="106-tfidf-20" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.226), (1, 0.048), (2, -0.122), (3, -0.078), (4, 0.13), (5, -0.096), (6, 0.018), (7, -0.154), (8, 0.096), (9, 0.072), (10, -0.125), (11, 0.1), (12, 0.166), (13, -0.153), (14, 0.001), (15, 0.091), (16, -0.0), (17, -0.014), (18, -0.088), (19, -0.009), (20, -0.084), (21, -0.044), (22, 0.016), (23, -0.036), (24, -0.051), (25, 0.026), (26, -0.02), (27, -0.048), (28, 0.021), (29, -0.063), (30, -0.003), (31, 0.018), (32, -0.033), (33, 0.085), (34, 0.032), (35, -0.074), (36, -0.012), (37, 0.042), (38, -0.014), (39, -0.045), (40, 0.063), (41, -0.015), (42, -0.018), (43, 0.008), (44, 0.003), (45, -0.006), (46, -0.106), (47, -0.05), (48, -0.078), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9585585 <a title="106-lsi-1" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>2 0.74843019 <a title="106-lsi-2" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>3 0.72603059 <a title="106-lsi-3" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>4 0.68804157 <a title="106-lsi-4" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>5 0.65166718 <a title="106-lsi-5" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>6 0.64983392 <a title="106-lsi-6" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>7 0.62156922 <a title="106-lsi-7" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>8 0.6143682 <a title="106-lsi-8" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>9 0.59859586 <a title="106-lsi-9" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>10 0.59438998 <a title="106-lsi-10" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>11 0.54265821 <a title="106-lsi-11" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>12 0.53713715 <a title="106-lsi-12" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>13 0.51643682 <a title="106-lsi-13" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>14 0.51194948 <a title="106-lsi-14" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>15 0.49244818 <a title="106-lsi-15" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>16 0.49087921 <a title="106-lsi-16" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>17 0.48543665 <a title="106-lsi-17" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>18 0.4746587 <a title="106-lsi-18" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>19 0.46235791 <a title="106-lsi-19" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>20 0.45292589 <a title="106-lsi-20" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.054), (21, 0.038), (38, 0.067), (39, 0.341), (42, 0.023), (54, 0.027), (55, 0.028), (74, 0.13), (76, 0.125), (80, 0.051), (92, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82009411 <a title="106-lda-1" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>2 0.76878071 <a title="106-lda-2" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family, even if the distributions are very heavy-tailed, have inﬁnite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness. Both theories and experiments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and robustness at almost no cost. 1</p><p>3 0.751113 <a title="106-lda-3" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>Author: Gal Elidan, Cobi Cario</p><p>Abstract: The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models. In this work we present Nonparanormal BP for performing efﬁcient inference on distributions parameterized by a Gaussian copulas network and any univariate marginals. For tree structured networks, our approach is guaranteed to be exact for this powerful class of non-Gaussian models. Importantly, the method is as efﬁcient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used. 1</p><p>4 0.72735333 <a title="106-lda-4" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>5 0.69530964 <a title="106-lda-5" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><p>6 0.6719889 <a title="106-lda-6" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>7 0.64131641 <a title="106-lda-7" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>8 0.54485869 <a title="106-lda-8" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>9 0.53707594 <a title="106-lda-9" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>10 0.53528661 <a title="106-lda-10" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>11 0.5345996 <a title="106-lda-11" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>12 0.53403455 <a title="106-lda-12" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>13 0.5330689 <a title="106-lda-13" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>14 0.52680266 <a title="106-lda-14" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>15 0.52661711 <a title="106-lda-15" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>16 0.52031344 <a title="106-lda-16" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>17 0.5156672 <a title="106-lda-17" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>18 0.51566058 <a title="106-lda-18" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>19 0.51475334 <a title="106-lda-19" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>20 0.51428038 <a title="106-lda-20" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
