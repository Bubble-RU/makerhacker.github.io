<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-106" href="#">nips2012-106</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</h1>
<br/><p>Source: <a title="nips-2012-106-pdf" href="http://papers.nips.cc/paper/4770-dynamical-and-or-graph-learning-for-object-shape-modeling-and-detection.pdf">pdf</a></p><p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>Reference: <a title="nips-2012-106-reference" href="../nips2012_reference/nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('conto', 0.457), ('recon', 0.312), ('yk', 0.254), ('detect', 0.249), ('cccp', 0.215), ('pi', 0.183), ('xk', 0.174), ('aog', 0.168), ('aot', 0.168), ('lj', 0.166), ('frag', 0.145), ('hk', 0.144), ('shap', 0.13), ('qt', 0.125), ('ui', 0.116), ('structural', 0.107), ('edg', 0.098), ('vz', 0.098), ('vj', 0.097), ('lateck', 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="106-tfidf-1" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>2 0.25410911 <a title="106-tfidf-2" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>3 0.19208579 <a title="106-tfidf-3" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>4 0.17229907 <a title="106-tfidf-4" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>5 0.16284001 <a title="106-tfidf-5" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>6 0.15402223 <a title="106-tfidf-6" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>7 0.15163833 <a title="106-tfidf-7" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>8 0.14750156 <a title="106-tfidf-8" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>9 0.14649628 <a title="106-tfidf-9" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>10 0.13809414 <a title="106-tfidf-10" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>11 0.12064106 <a title="106-tfidf-11" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>12 0.11762806 <a title="106-tfidf-12" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>13 0.11695992 <a title="106-tfidf-13" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>14 0.10072547 <a title="106-tfidf-14" href="./nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">140 nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<p>15 0.099078476 <a title="106-tfidf-15" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>16 0.096578099 <a title="106-tfidf-16" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>17 0.093808226 <a title="106-tfidf-17" href="./nips-2012-Topology_Constraints_in_Graphical_Models.html">346 nips-2012-Topology Constraints in Graphical Models</a></p>
<p>18 0.093280479 <a title="106-tfidf-18" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>19 0.090857178 <a title="106-tfidf-19" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>20 0.088694371 <a title="106-tfidf-20" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.225), (1, -0.04), (2, 0.031), (3, 0.12), (4, -0.08), (5, 0.1), (6, -0.025), (7, 0.055), (8, 0.032), (9, 0.184), (10, 0.063), (11, -0.108), (12, -0.161), (13, 0.067), (14, -0.091), (15, 0.025), (16, 0.188), (17, 0.047), (18, -0.083), (19, 0.065), (20, -0.034), (21, 0.022), (22, 0.069), (23, 0.014), (24, 0.006), (25, 0.047), (26, -0.041), (27, 0.047), (28, -0.019), (29, -0.097), (30, 0.061), (31, 0.01), (32, -0.044), (33, 0.044), (34, -0.012), (35, -0.012), (36, -0.108), (37, 0.09), (38, -0.018), (39, -0.028), (40, 0.044), (41, -0.011), (42, 0.087), (43, -0.042), (44, 0.037), (45, 0.029), (46, 0.034), (47, -0.083), (48, -0.043), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91231036 <a title="106-lsi-1" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>2 0.66639447 <a title="106-lsi-2" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>3 0.6494866 <a title="106-lsi-3" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>4 0.6091361 <a title="106-lsi-4" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>Author: Shulin Yang, Liefeng Bo, Jue Wang, Linda G. Shapiro</p><p>Abstract: Fine-grained recognition refers to a subordinate level of recognition, such as recognizing different species of animals and plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape and structure shared cross different categories, and the differences are in the details of object parts. We suggest that the key to identifying the ﬁne-grained differences lies in ﬁnding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the cooccurrence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classiﬁcation. Learning of the template model is efﬁcient, and the recognition results we achieve signiﬁcantly outperform the stateof-the-art algorithms. 1</p><p>5 0.60587406 <a title="106-lsi-5" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>6 0.60274577 <a title="106-lsi-6" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>7 0.58916813 <a title="106-lsi-7" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>8 0.58048946 <a title="106-lsi-8" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>9 0.54369622 <a title="106-lsi-9" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>10 0.52492064 <a title="106-lsi-10" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>11 0.52091008 <a title="106-lsi-11" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>12 0.51453251 <a title="106-lsi-12" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>13 0.51299632 <a title="106-lsi-13" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>14 0.49142557 <a title="106-lsi-14" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>15 0.48791212 <a title="106-lsi-15" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>16 0.48224387 <a title="106-lsi-16" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>17 0.47188962 <a title="106-lsi-17" href="./nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">140 nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<p>18 0.4620024 <a title="106-lsi-18" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>19 0.45543563 <a title="106-lsi-19" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>20 0.45432869 <a title="106-lsi-20" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.074), (13, 0.312), (47, 0.113), (64, 0.014), (67, 0.098), (70, 0.048), (85, 0.069), (94, 0.111), (99, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.77929646 <a title="106-lda-1" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>Author: Yuxuan Wang, Deliang Wang</p><p>Abstract: While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random ﬁelds (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classiﬁcation allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.</p><p>same-paper 2 0.75397116 <a title="106-lda-2" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>3 0.71031588 <a title="106-lda-3" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>Author: Joonseok Lee, Mingxuan Sun, Guy Lebanon, Seung-jean Kim</p><p>Abstract: Recent approaches to collaborative ﬁltering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings. In this paper we observe that different models have relative advantages in different regions of the input space. This motivates our approach of using stagewise linear combinations of collaborative ﬁltering algorithms, with non-constant combination coefﬁcients based on kernel smoothing. The resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative ﬁltering algorithms. 1</p><p>4 0.69672537 <a title="106-lda-4" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>Author: Chi Jin, Liwei Wang</p><p>Abstract: Margin is one of the most important concepts in machine learning. Previous margin bounds, both for SVM and for boosting, are dimensionality independent. A major advantage of this dimensionality independency is that it can explain the excellent performance of SVM whose feature spaces are often of high or inﬁnite dimension. In this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds. We prove a dimensionality dependent PAC-Bayes margin bound. The bound is monotone increasing with respect to the dimension when keeping all other factors ﬁxed. We show that our bound is strictly sharper than a previously well-known PAC-Bayes margin bound if the feature space is of ﬁnite dimension; and the two bounds tend to be equivalent as the dimension goes to inﬁnity. In addition, we show that the VC bound for linear classiﬁers can be recovered from our bound under mild conditions. We conduct extensive experiments on benchmark datasets and ﬁnd that the new bound is useful for model selection and is usually signiﬁcantly sharper than the dimensionality independent PAC-Bayes margin bound as well as the VC bound for linear classiﬁers.</p><p>5 0.6190449 <a title="106-lda-5" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>Author: Xaq Pitkow</p><p>Abstract: This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a highdimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain. 1</p><p>6 0.61111051 <a title="106-lda-6" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>7 0.5960198 <a title="106-lda-7" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>8 0.59552419 <a title="106-lda-8" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>9 0.58766836 <a title="106-lda-9" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>10 0.58083797 <a title="106-lda-10" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>11 0.5805853 <a title="106-lda-11" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>12 0.57753229 <a title="106-lda-12" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>13 0.57581842 <a title="106-lda-13" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>14 0.57566655 <a title="106-lda-14" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>15 0.57544392 <a title="106-lda-15" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>16 0.5751248 <a title="106-lda-16" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>17 0.57507104 <a title="106-lda-17" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>18 0.57507098 <a title="106-lda-18" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>19 0.57464433 <a title="106-lda-19" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>20 0.57424867 <a title="106-lda-20" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
