<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-119" href="#">nips2012-119</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</h1>
<br/><p>Source: <a title="nips-2012-119-pdf" href="http://papers.nips.cc/paper/4667-entropy-estimations-using-correlated-symmetric-stable-random-projections.pdf">pdf</a></p><p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><p>Reference: <a title="nips-2012-119-reference" href="../nips2012_reference/nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e. [sent-4, score-0.783]
</p><p>2 For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. [sent-7, score-0.512]
</p><p>3 However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. [sent-8, score-0.254]
</p><p>4 In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. [sent-9, score-0.538]
</p><p>5 In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. [sent-10, score-0.78]
</p><p>6 Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. [sent-11, score-1.365]
</p><p>7 Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage. [sent-12, score-0.294]
</p><p>8 1 Introduction Computing the Shannon entropy in massive data have important applications in neural computation [17], graph estimation [5], query logs analysis in Web search [14], network anomaly detection [21], etc. [sent-13, score-0.641]
</p><p>9 In modern applications, as massive datasets are often generated in a streaming fashion, entropy estimation in data streams has become a challenging and interesting problem. [sent-17, score-0.661]
</p><p>10 Mining data streams at petabyte scale has become an important research area [1], as network data can easily reach that scale [20]. [sent-21, score-0.279]
</p><p>11 In the standard turnstile model [15], a data stream is a vector At of length D, where D = 264 or even D = 2128 is possible in network applications, e. [sent-22, score-0.295]
</p><p>12 At time t, there is an input stream at = (it , It ), it ∈ [1, D] which updates At by a linear rule: At [it ] = At−1 [it ] + It . [sent-25, score-0.151]
</p><p>13 For network trafﬁc, normally At [i] ≥ 0, which is called the strict turnstile model and sufﬁces for describing certain natural phenomena. [sent-27, score-0.144]
</p><p>14 Also, many applications (such as anomaly detections of network trafﬁc) require computing the summary statistics in real-time. [sent-32, score-0.234]
</p><p>15 An effective and reliable measurement of network trafﬁc in real-time is crucial for anomaly detection and network diagnosis; and one such measurement metric is the Shannon entropy [4, 8, 19, 2, 9, 21]. [sent-36, score-0.713]
</p><p>16 The exact entropy measurement in real-time on high-speed links is however computationally prohibitive. [sent-37, score-0.332]
</p><p>17 The Distributed Denial of Service (DDoS) attack is a representative example of network anomalies. [sent-38, score-0.187]
</p><p>18 A DDoS attack attempts to make computers unavailable to intended users, either by forcing users to reset the computers or by exhausting the resources of service-hosting sites. [sent-39, score-0.165]
</p><p>19 A DDoS attack normally changes the statistical distribution of network trafﬁc, which could be reliably captured by the abnormal variations in the measurements of Shannon entropy [4]. [sent-42, score-0.481]
</p><p>20 source IP address: entropy value 10 9 8 7 6 5 4 3 2 1 0 200  400 600 800 1000 packet counts (thousands)  1200  Figure 1: This plot is reproduced from a DARPA conference [4]. [sent-44, score-0.334]
</p><p>21 Apparently, the entropy measurements do not have to be “perfect” for detecting attacks. [sent-47, score-0.316]
</p><p>22 3 Symmetric Stable Random Projections and Entropy Estimation Using Moments It turns out that, for 0 < α ≤ 2, one can use stable random projections to compute F(α) efﬁciently because the Turnstile model (1) is a linear model and the random projection operation is also linear (i. [sent-52, score-0.179]
</p><p>23 Conceptually, we multiply the data stream vector At ∈ RD by a random matrix R ∈ RD×k , resulting in a vector X = At × R ∈ Rk with entries D  xj = [At × R]j =  rij At [i], j = 1, 2, . [sent-55, score-0.336]
</p><p>24 In data stream computations, the matrix R is not materialized. [sent-60, score-0.151]
</p><p>25 When a stream element at = (it , It ) arrives, one updates the entries of X: xj ← xj + It rit j , j = 1, 2, . [sent-63, score-0.318]
</p><p>26 (3)  By property of stable distributions, the samples xj , j = 1 to k, are also i. [sent-67, score-0.219]
</p><p>27 stable D  xj =  D  rij At [i] ∼ S  |At [i]|α  α, F(α) =  i=1  (4)  i=1  Therefore, the task boils down to estimating the scale parameter from k i. [sent-70, score-0.279]
</p><p>28 Because the Shannon entropy is essentially the derivative of the frequency moment at α = 1, the popular approach is to approximate the Shannon entropy by the Tsallis entropy [18]: Tα =  1 α−1  1−  F(α) α F(1)  . [sent-74, score-1.037]
</p><p>29 (5)  which approaches the Shannon entropy H as α → 1. [sent-75, score-0.294]
</p><p>30 The estimated moments are then plugged in (5) to estimate the Shannon entropy H. [sent-81, score-0.364]
</p><p>31 Ideally, if V ar  ˆ F(α) ˆ Fα  = O ∆2 , the  (1)  (1)  ˆ variance of the estimated Tsallis entropy Tα =  It turns out that ﬁnding an estimator with V ar  1 α−1 ˆ F(α) ˆ Fα  1−  ˆ F(α) ˆ Fα  will be essentially independent of ∆. [sent-98, score-0.784]
</p><p>32 It is known  (1)  that around α = 1, the geometric mean estimator [10] is nearly statistically optimal. [sent-100, score-0.342]
</p><p>33 Interestingly, our analysis and simulation show that using the geometric mean estimator, we can essentially only achieve V ar  ˆ F(α) ˆ Fα  = O (∆), which, albeit a large improvement, is not small sufﬁcient to cancel  (1)  1 ∆2  the O term. [sent-101, score-0.271]
</p><p>34 Therefore, our second key component is a new estimator of Tα using a moment estimator which does not have (or barely has) ﬁnite variance. [sent-102, score-0.488]
</p><p>35 Even though such an estimator is not good for estimating the single moment compared to the geometric mean, due to the high correlation, the ratio  ˆ F(α) ˆα F(1)  is still very well-behaved and its variance is essentially O ∆2 , as shown in our  theoretical analysis and experiments. [sent-103, score-0.55]
</p><p>36 5 Compressed Counting (CC) for Nonnegative Data Streams The recent work [13] on Compressed Counting (CC) [11] provides an ideal solution to the problem of entropy estimation in nonnegative data streams. [sent-105, score-0.362]
</p><p>37 This observation lead to the conjecture that estimating F(α) should be also easy if α ≈ 1, which consequently lead to the development of Compressed Counting which used maximally-skewed stable random projections instead of symmetric stable projections. [sent-110, score-0.406]
</p><p>38 The most recent work of CC [13] provided a new moment estimator to achieve the variance ∝ O ∆2 . [sent-111, score-0.341]
</p><p>39 Unfortunately, for general data streams where entries can be negative, we have to resort to symmetric stable random projections. [sent-112, score-0.441]
</p><p>40 Fundamentally, the reason that skewed projections work well on nonnegative data streams is because the data themselves are skewed. [sent-113, score-0.307]
</p><p>41 [21] used the difference between data streams from a slightly different motivation. [sent-118, score-0.239]
</p><p>42 The goal of [21] is to measure the entropies of all OD pairs (origin-destination) in a network, because entropy measurements are crucial for detecting anomaly events such as DDoS attacks and network failures. [sent-119, score-0.675]
</p><p>43 They argued that the change of entropy of the trafﬁc distribution may be invisible (i. [sent-120, score-0.327]
</p><p>44 Instead, they proposed to measure the entropy from a number of locations across the network, i. [sent-123, score-0.294]
</p><p>45 , 3  by examining the entropy of every OD ﬂow in the network. [sent-125, score-0.294]
</p><p>46 In a similar argument, a DDoS attack may be invisible in terms of the trafﬁc volume change, if the attack is launched outside the network. [sent-126, score-0.279]
</p><p>47 While [21] successfully demonstrated that measuring the Shannon entropy of OD ﬂows is effective for detecting anomaly events, at that time they did not have the tools for efﬁciently estimating the entropy. [sent-127, score-0.502]
</p><p>48 Using symmetric stable random projections and independent samples, they needed a large 1 number of samples (e. [sent-128, score-0.269]
</p><p>49 For anomaly detection, reducing the sample size (k) is crucial because k determines the storage and estimation speed; and it is often required to detect the events at real time. [sent-131, score-0.231]
</p><p>50 2 Our Proposed Algorithm Recall that a data stream is a long vector At [i], i = 1 to D. [sent-133, score-0.151]
</p><p>51 Conceptually, we generate a random matrix R ∈ RD×k whose entries are sampled from a stable distribution and multiply it with At : X = At × R. [sent-135, score-0.198]
</p><p>52 The matrix multiplication is linear and can be conducted incrementally as the new stream elements arrive. [sent-136, score-0.187]
</p><p>53 R is not materialized; its entries are re-generated on demand using pseudorandom numbers, as the standard practice in data stream computations [7]. [sent-137, score-0.261]
</p><p>54 Our method does not require At [i] ≥ 0 and hence it can handle the difference between two streams (e. [sent-138, score-0.239]
</p><p>55 1 The Symmetric Stable Law Our work utilizes the symmetric stable distribution. [sent-142, score-0.187]
</p><p>56 random numbers: wij ∼ exp(1),  uij ∼ unif orm(−π/2, π/2),  i = 1, 2, . [sent-150, score-0.172]
</p><p>57 , k,  (9)  As new stream elements arrive, we incrementally maintain two sets of samples, i. [sent-156, score-0.187]
</p><p>58 , for i = 1 to k, D  xj =  D  At [i]g(wij , uij , 1),  yj =  i=1  At [i]g(wij , uij , α)  (10)  i=1  Note that xj and yj are highly correlated because they are generated using the same random numbers (with different α). [sent-158, score-0.564]
</p><p>59 Our recommended estimator of the Tsallis entropy Tα is  √ 1  π ˆα,0. [sent-160, score-0.511]
</p><p>60 When ∆ is sufﬁciently small, the estimated Tsallis entropy will be sufﬁciently close to the Shannon entropy. [sent-163, score-0.32]
</p><p>61 While it is intuitively clear that it is beneﬁcial to make xj and yj ˆ highly correlated for the sake of reducing the variance, it might not be as intuitive why Tα,0. [sent-165, score-0.225]
</p><p>62 We will explain why the obvious geometric mean estimator [10] is not sufﬁcient for entropy estimation. [sent-167, score-0.636]
</p><p>63 4  3  The Geometric Mean Estimator  For estimating F(α) , the geometric mean estimator [10] is close to be statistically optimal (efﬁciency ≈ 80%) at α ≈ 1. [sent-168, score-0.382]
</p><p>64 Thus, it was our ﬁrst attempt to test the following estimator of the Tsallis entropy: ˆ Tα,gm =  1 α−1  1−  ˆ F(α),gm ˆ Fα  ˆ , where F(α),gm =  (1),gm  k α/k j=1 |yj | , Gk (α, α/k)  ˆ F(1),gm =  k 1/k j=1 |xj | , Gk (1, 1/k)  where G() is deﬁned in (8). [sent-169, score-0.194]
</p><p>65 1  (12)  Theoretical Analysis  ˆ The theoretical analysis of T(α),gm , however, turns out to be difﬁcult, as it requires computing   sα/k sα/k D At [i]g(wij , uij , α) yj i=1 , E =E s = 1, 2, (13) D xj i=1 At [i]g(wij , uij , 1) where g() is deﬁned in (7). [sent-172, score-0.307]
</p><p>66 6822γ 2 ∆2 + O γ∆4 + O γ 2 ∆3  (14)  Note that we need to keep higher order terms in order to prove Lemma 2, to show the properties of the geometric mean estimator, when D = 1 (i. [sent-179, score-0.148]
</p><p>67 In this case, the geometric mean estimator Tα,gm is 1 asymptotically unbiased with variance essentially free of ∆ , which is very encouraging. [sent-186, score-0.468]
</p><p>68 2 and the theoretical analysis of a more general estimator 1 ˆ in Sec. [sent-194, score-0.194]
</p><p>69 Independent samples)  ˆ We present some experimental results for evaluating Tα,gm , to demonstrate that (i) using correlation does substantially reduce variance and hence reduces the required sample size, and (ii) the variance 1 ˆ (or MSE, the mean square error) of Tα,gm is roughly O ∆ . [sent-198, score-0.18]
</p><p>70 , the prior work [21]) and the geometric mean estimator. [sent-213, score-0.148]
</p><p>71 The middle panels contain the results using correlated sampling (i. [sent-214, score-0.17]
</p><p>72 The right panels multiply the results of the middle panels by ∆ to illustrate that the variance of the geometric mean 1 ˆ estimator for entropy Tα,gm is essentially O ∆ . [sent-217, score-0.991]
</p><p>73 We conducted symmetric random projections using both independent sampling (left panels, as in [21]) and correlated sampling (middle panels, as our proposal). [sent-226, score-0.186]
</p><p>74 The Tsallis entropy (of the difference vector) is estimated using the geometric mean estimator (12) with three sample sizes k = 10, 100, and 1000. [sent-227, score-0.686]
</p><p>75 The normalized mean square errors ˆ (MSE: E|Tα,gm − H|2 /H 2 ) verify that correlated sampling reduces the errors substantially. [sent-228, score-0.283]
</p><p>76 4  The General Estimator  Since the geometric mean estimator could not satisfactorily solve the entropy estimation problem, we resort to estimators which behave dramatically different from the geometric mean. [sent-229, score-0.808]
</p><p>77 Thus, it is clear that, as long as 0 < λ < 1, F(α),γ is a consistent estimator of ˆ ˆ F(α) and E F(α),γ is ﬁnite. [sent-238, score-0.194]
</p><p>78 5: ˆ E F(α),γ = F(α) + O  1 k  ,  ˆ V ar F(α),γ =  6  2 F(α) α2 G(α, 2γ) − G2 (α, γ)  k γ2  G2 (α, γ)  +O  1 k2  The variance is unbounded if γ = 0. [sent-240, score-0.147]
</p><p>79 In fact, when γ → 0, F(α),γ ˆ ˆ converges to the geometric mean estimator F(α),gm . [sent-243, score-0.342]
</p><p>80 1  Theoretical Analysis  Based on Lemma 3 and Lemma 4 (which is a fairly technical proof), we know that the variance of the 2γ−1 ˆ general estimator is essentially V ar Tα,γ = O ∆ k , for ﬁxed γ ∈ (0, 1/2). [sent-246, score-0.392]
</p><p>81 In other words, when γ is close to 0, the variance of the entropy estimator is essentially on the order of O (1/(k∆)), and while γ is close to 1/2, the variance is essentially O(1/k) as desired. [sent-247, score-0.74]
</p><p>82 2  Experimental Results  ˆ Figure 3 presents some empirical results, for testing the general estimator Tα,γ (17), using more word vector pairs (including the same 2 pairs in Figure 2). [sent-261, score-0.222]
</p><p>83 If the goal is to estimate the Shannon entropy within a few √ percentages of the the true value, then k = 100 ∼ 1000 should be sufﬁcient, because M SE/H < 0. [sent-268, score-0.294]
</p><p>84 5 Conclusion Entropy estimation is an important task in machine learning, data mining, network measurement, anomaly detection, neural computations, etc. [sent-270, score-0.239]
</p><p>85 In modern applications, the data are often generated in a streaming fashion and many operations on the streams can only be conducted in one-pass of the data. [sent-271, score-0.326]
</p><p>86 It has been a challenging problem to estimate the Shannon entropy of data streams. [sent-272, score-0.294]
</p><p>87 The prior work [21] achieved some success in entropy estimation using symmetric stable random projections. [sent-273, score-0.51]
</p><p>88 In our approach, we approximate the Shannon entropy using two high correlated estimates of the frequent comments. [sent-277, score-0.366]
</p><p>89 The positive correlation can substantially reduce the variance of the Shannon entropy estimate. [sent-278, score-0.369]
</p><p>90 However, ﬁnding the appropriate estimator of the frequency moment is another challenging task. [sent-279, score-0.298]
</p><p>91 We successfully ﬁnd such an estimator and show that its variance (of the Shannon entropy estimate) is very small. [sent-280, score-0.563]
</p><p>92 7 −4 10 −5 −4 −3 −2 −1 0 10 10 10 10 10 10 ∆=α−1  Figure 3: The ﬁrst two rows are the normalized MSEs for same two vectors used in Figure 2, for ˆ estimating Shannon entropy using the general estimator Tα,γ with γ = 0. [sent-352, score-0.709]
</p><p>93 , the prior work [21]) and the geometric mean estimator. [sent-360, score-0.148]
</p><p>94 The second column of panels are the results of using correlated samples and the geometric mean estimator. [sent-361, score-0.347]
</p><p>95 The right three columns of panels are for the ˆ proposed general estimator Tα,γ with γ = 0. [sent-362, score-0.292]
</p><p>96 Stable distributions, pseudorandom generators, embeddings, and data stream computation. [sent-396, score-0.2]
</p><p>97 Data streaming algorithms for estimating entropy of network trafﬁc. [sent-402, score-0.486]
</p><p>98 A uniﬁed near-optimal estimator for dimension reduction in lα (0 < α ≤ 2) using stable random projections. [sent-412, score-0.32]
</p><p>99 A new algorithm for compressed counting with applications in shannon entropy estimation in dynamic data. [sent-415, score-0.684]
</p><p>100 A data streaming algorithm for estimating entropies of od ﬂows. [sent-443, score-0.292]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mse', 0.521), ('entropy', 0.294), ('shannon', 0.266), ('gm', 0.226), ('streams', 0.215), ('estimator', 0.194), ('normalized', 0.181), ('ddos', 0.16), ('stream', 0.151), ('anomaly', 0.146), ('traf', 0.134), ('stable', 0.126), ('attack', 0.123), ('od', 0.12), ('geometric', 0.118), ('tsallis', 0.106), ('panels', 0.098), ('yj', 0.089), ('mses', 0.088), ('streaming', 0.088), ('ping', 0.081), ('turnstile', 0.08), ('uij', 0.077), ('love', 0.077), ('variance', 0.075), ('correlated', 0.072), ('moment', 0.072), ('ar', 0.072), ('cc', 0.065), ('food', 0.065), ('network', 0.064), ('xj', 0.064), ('symmetric', 0.061), ('news', 0.058), ('projections', 0.053), ('compressed', 0.053), ('wij', 0.051), ('essentially', 0.051), ('washington', 0.05), ('united', 0.05), ('pseudorandom', 0.049), ('attacks', 0.049), ('rij', 0.049), ('entropies', 0.044), ('unif', 0.044), ('moments', 0.044), ('mining', 0.043), ('counting', 0.042), ('kg', 0.042), ('review', 0.041), ('detection', 0.04), ('anukool', 0.04), ('imc', 0.04), ('lall', 0.04), ('mitsunori', 0.04), ('ogihara', 0.04), ('packet', 0.04), ('estimating', 0.04), ('nonnegative', 0.039), ('entries', 0.039), ('measurement', 0.038), ('recommend', 0.037), ('incrementally', 0.036), ('sigcomm', 0.035), ('ashwin', 0.035), ('massive', 0.035), ('multiply', 0.033), ('lemma', 0.033), ('orm', 0.033), ('invisible', 0.033), ('logs', 0.033), ('numbers', 0.032), ('frequency', 0.032), ('sin', 0.031), ('generators', 0.031), ('mean', 0.03), ('samples', 0.029), ('crucial', 0.029), ('estimation', 0.029), ('barely', 0.028), ('word', 0.028), ('conceptually', 0.027), ('ows', 0.027), ('events', 0.027), ('estimated', 0.026), ('carefully', 0.026), ('estimators', 0.025), ('jun', 0.024), ('detections', 0.024), ('ip', 0.024), ('difference', 0.024), ('fashion', 0.023), ('positively', 0.023), ('gk', 0.023), ('recommended', 0.023), ('soda', 0.023), ('store', 0.023), ('detecting', 0.022), ('computations', 0.022), ('computers', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="119-tfidf-1" href="./nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</a></p>
<p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><p>2 0.39465889 <a title="119-tfidf-2" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, for large feature dimension d, kernel plug-in estimators suﬀer from the curse of dimensionality: the MSE rate of convergence is glacially slow - of order O(T −γ/d ), where T is the number of samples, and γ > 0 is a rate parameter. In this paper, it is shown that for suﬃciently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order O(T −1 ). Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed oﬄine. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suﬀer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates. 1</p><p>3 0.14283173 <a title="119-tfidf-3" href="./nips-2012-Bayesian_estimation_of_discrete_entropy_with_mixtures_of_stick-breaking_priors.html">57 nips-2012-Bayesian estimation of discrete entropy with mixtures of stick-breaking priors</a></p>
<p>Author: Evan Archer, Il M. Park, Jonathan W. Pillow</p><p>Abstract: We consider the problem of estimating Shannon’s entropy H in the under-sampled regime, where the number of possible symbols may be unknown or countably inﬁnite. Dirichlet and Pitman-Yor processes provide tractable prior distributions over the space of countably inﬁnite discrete distributions, and have found major applications in Bayesian non-parametric statistics and machine learning. Here we show that they provide natural priors for Bayesian entropy estimation, due to the analytic tractability of the moments of the induced posterior distribution over entropy H. We derive formulas for the posterior mean and variance of H given data. However, we show that a ﬁxed Dirichlet or Pitman-Yor process prior implies a narrow prior on H, meaning the prior strongly determines the estimate in the under-sampled regime. We therefore deﬁne a family of continuous mixing measures such that the resulting mixture of Dirichlet or Pitman-Yor processes produces an approximately ﬂat prior over H. We explore the theoretical properties of the resulting estimators and show that they perform well on data sampled from both exponential and power-law tailed distributions. 1</p><p>4 0.10745091 <a title="119-tfidf-4" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph. 1</p><p>5 0.094401203 <a title="119-tfidf-5" href="./nips-2012-A_new_metric_on_the_manifold_of_kernel_matrices_with_application_to_matrix_geometric_means.html">25 nips-2012-A new metric on the manifold of kernel matrices with application to matrix geometric means</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: Symmetric positive deﬁnite (spd) matrices pervade numerous scientiﬁc disciplines, including machine learning and optimization. We consider the key task of measuring distances between two spd matrices; a task that is often nontrivial whenever the distance function must respect the non-Euclidean geometry of spd matrices. Typical non-Euclidean distance measures such as the Riemannian metric δR (X, Y ) = log(Y −1/2 XY −1/2 ) F , are computationally demanding and also complicated to use. To allay some of these difﬁculties, we introduce a new metric on spd matrices, which not only respects non-Euclidean geometry but also offers faster computation than δR while being less complicated to use. We support our claims theoretically by listing a set of theorems that relate our metric to δR (X, Y ), and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances. 1</p><p>6 0.086720601 <a title="119-tfidf-6" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>7 0.076851398 <a title="119-tfidf-7" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>8 0.073572166 <a title="119-tfidf-8" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>9 0.070882186 <a title="119-tfidf-9" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>10 0.06913162 <a title="119-tfidf-10" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>11 0.066299893 <a title="119-tfidf-11" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>12 0.061212953 <a title="119-tfidf-12" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>13 0.057690896 <a title="119-tfidf-13" href="./nips-2012-Multi-Task_Averaging.html">222 nips-2012-Multi-Task Averaging</a></p>
<p>14 0.057408687 <a title="119-tfidf-14" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>15 0.057099003 <a title="119-tfidf-15" href="./nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</a></p>
<p>16 0.056583811 <a title="119-tfidf-16" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>17 0.055889454 <a title="119-tfidf-17" href="./nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<p>18 0.053782094 <a title="119-tfidf-18" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>19 0.052966289 <a title="119-tfidf-19" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>20 0.051971391 <a title="119-tfidf-20" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.144), (1, 0.035), (2, 0.02), (3, -0.042), (4, -0.014), (5, 0.038), (6, 0.0), (7, 0.092), (8, -0.029), (9, -0.077), (10, -0.022), (11, -0.041), (12, 0.046), (13, -0.122), (14, -0.034), (15, -0.099), (16, 0.219), (17, -0.049), (18, -0.05), (19, -0.071), (20, 0.059), (21, 0.03), (22, 0.231), (23, 0.014), (24, -0.029), (25, -0.023), (26, 0.195), (27, 0.044), (28, 0.191), (29, -0.036), (30, 0.128), (31, -0.053), (32, 0.155), (33, 0.097), (34, -0.053), (35, -0.099), (36, 0.188), (37, 0.154), (38, -0.103), (39, -0.046), (40, -0.015), (41, -0.099), (42, -0.029), (43, -0.071), (44, 0.09), (45, -0.018), (46, -0.092), (47, -0.042), (48, 0.017), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97922504 <a title="119-lsi-1" href="./nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</a></p>
<p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><p>2 0.86878908 <a title="119-lsi-2" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, for large feature dimension d, kernel plug-in estimators suﬀer from the curse of dimensionality: the MSE rate of convergence is glacially slow - of order O(T −γ/d ), where T is the number of samples, and γ > 0 is a rate parameter. In this paper, it is shown that for suﬃciently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order O(T −1 ). Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed oﬄine. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suﬀer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates. 1</p><p>3 0.71604306 <a title="119-lsi-3" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph. 1</p><p>4 0.69237 <a title="119-lsi-4" href="./nips-2012-Bayesian_estimation_of_discrete_entropy_with_mixtures_of_stick-breaking_priors.html">57 nips-2012-Bayesian estimation of discrete entropy with mixtures of stick-breaking priors</a></p>
<p>Author: Evan Archer, Il M. Park, Jonathan W. Pillow</p><p>Abstract: We consider the problem of estimating Shannon’s entropy H in the under-sampled regime, where the number of possible symbols may be unknown or countably inﬁnite. Dirichlet and Pitman-Yor processes provide tractable prior distributions over the space of countably inﬁnite discrete distributions, and have found major applications in Bayesian non-parametric statistics and machine learning. Here we show that they provide natural priors for Bayesian entropy estimation, due to the analytic tractability of the moments of the induced posterior distribution over entropy H. We derive formulas for the posterior mean and variance of H given data. However, we show that a ﬁxed Dirichlet or Pitman-Yor process prior implies a narrow prior on H, meaning the prior strongly determines the estimate in the under-sampled regime. We therefore deﬁne a family of continuous mixing measures such that the resulting mixture of Dirichlet or Pitman-Yor processes produces an approximately ﬂat prior over H. We explore the theoretical properties of the resulting estimators and show that they perform well on data sampled from both exponential and power-law tailed distributions. 1</p><p>5 0.60012174 <a title="119-lsi-5" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>Author: Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus D. Plessis, Song Liu, Ichiro Takeuchi</p><p>Abstract: We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of ﬁrst estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the ﬁrst step is performed without regard to the second step and thus a small estimation error incurred in the ﬁrst stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric ﬁnite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be utilized in L2 -distance approximation. Finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.</p><p>6 0.52745646 <a title="119-lsi-6" href="./nips-2012-Multi-Task_Averaging.html">222 nips-2012-Multi-Task Averaging</a></p>
<p>7 0.51372987 <a title="119-lsi-7" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>8 0.49842897 <a title="119-lsi-8" href="./nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<p>9 0.49440229 <a title="119-lsi-9" href="./nips-2012-Learning_High-Density_Regions_for_a_Generalized_Kolmogorov-Smirnov_Test_in_High-Dimensional_Data.html">175 nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</a></p>
<p>10 0.39797065 <a title="119-lsi-10" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>11 0.38687086 <a title="119-lsi-11" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>12 0.37255964 <a title="119-lsi-12" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>13 0.34132335 <a title="119-lsi-13" href="./nips-2012-Learning_from_the_Wisdom_of_Crowds_by_Minimax_Entropy.html">189 nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</a></p>
<p>14 0.33508646 <a title="119-lsi-14" href="./nips-2012-Perfect_Dimensionality_Recovery_by_Variational_Bayesian_PCA.html">268 nips-2012-Perfect Dimensionality Recovery by Variational Bayesian PCA</a></p>
<p>15 0.32012385 <a title="119-lsi-15" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>16 0.31300187 <a title="119-lsi-16" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>17 0.30617827 <a title="119-lsi-17" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>18 0.29865322 <a title="119-lsi-18" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>19 0.2924611 <a title="119-lsi-19" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>20 0.28511426 <a title="119-lsi-20" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.056), (21, 0.043), (31, 0.31), (38, 0.105), (39, 0.017), (42, 0.024), (54, 0.033), (55, 0.033), (74, 0.052), (76, 0.126), (80, 0.067), (92, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7324394 <a title="119-lda-1" href="./nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</a></p>
<p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><p>2 0.71046185 <a title="119-lda-2" href="./nips-2012-Homeostatic_plasticity_in_Bayesian_spiking_networks_as_Expectation_Maximization_with_posterior_constraints.html">152 nips-2012-Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</a></p>
<p>Author: Stefan Habenschuss, Johannes Bill, Bernhard Nessler</p><p>Abstract: Recent spiking network models of Bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules. Here we show in a rigorous mathematical treatment how homeostatic processes, which have previously received little attention in this context, can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models. In particular, we show that homeostatic plasticity can be understood as the enforcement of a ’balancing’ posterior constraint during probabilistic inference and learning with Expectation Maximization. We link homeostatic dynamics to the theory of variational inference, and show that nontrivial terms, which typically appear during probabilistic inference in a large class of models, drop out. We demonstrate the feasibility of our approach in a spiking WinnerTake-All architecture of Bayesian inference and learning. Finally, we sketch how the mathematical framework can be extended to richer recurrent network architectures. Altogether, our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits, and points to an essential role of homeostasis during inference and learning in spiking networks. 1</p><p>3 0.64996248 <a title="119-lda-3" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>Author: Ronald Ortner, Daniil Ryabko</p><p>Abstract: We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper conﬁdence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisﬁes the Poisson equation, the only assumptions made are H¨ lder continuity of rewards and transition o probabilities. 1</p><p>4 0.62577713 <a title="119-lda-4" href="./nips-2012-Near-Optimal_MAP_Inference_for_Determinantal_Point_Processes.html">236 nips-2012-Near-Optimal MAP Inference for Determinantal Point Processes</a></p>
<p>Author: Jennifer Gillenwater, Alex Kulesza, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) have recently been proposed as computationally efﬁcient probabilistic models of diverse sets for a variety of applications, including document summarization, image search, and pose estimation. Many DPP inference operations, including normalization and sampling, are tractable; however, ﬁnding the most likely conﬁguration (MAP), which is often required in practice for decoding, is NP-hard, so we must resort to approximate inference. This optimization problem, which also arises in experimental design and sensor placement, involves ﬁnding the largest principal minor of a positive semideﬁnite matrix. Because the objective is log-submodular, greedy algorithms have been used in the past with some empirical success; however, these methods only give approximation guarantees in the special case of monotone objectives, which correspond to a restricted class of DPPs. In this paper we propose a new algorithm for approximating the MAP problem based on continuous techniques for submodular function maximization. Our method involves a novel continuous relaxation of the log-probability function, which, in contrast to the multilinear extension used for general submodular functions, can be evaluated and differentiated exactly and efﬁciently. We obtain a practical algorithm with a 1/4-approximation guarantee for a more general class of non-monotone DPPs; our algorithm also extends to MAP inference under complex polytope constraints, making it possible to combine DPPs with Markov random ﬁelds, weighted matchings, and other models. We demonstrate that our approach outperforms standard and recent methods on both synthetic and real-world data. 1</p><p>5 0.56302238 <a title="119-lda-5" href="./nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>Author: Tingni Sun, Cun-hui Zhang</p><p>Abstract: This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a uniﬁed analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. 1</p><p>6 0.53848565 <a title="119-lda-6" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>7 0.53805476 <a title="119-lda-7" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>8 0.53673398 <a title="119-lda-8" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>9 0.53439635 <a title="119-lda-9" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>10 0.53336716 <a title="119-lda-10" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>11 0.53333002 <a title="119-lda-11" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>12 0.53215253 <a title="119-lda-12" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>13 0.53181922 <a title="119-lda-13" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>14 0.53128582 <a title="119-lda-14" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>15 0.53075516 <a title="119-lda-15" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>16 0.53071129 <a title="119-lda-16" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>17 0.53061819 <a title="119-lda-17" href="./nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">232 nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>18 0.53061581 <a title="119-lda-18" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>19 0.53051984 <a title="119-lda-19" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>20 0.53040707 <a title="119-lda-20" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
