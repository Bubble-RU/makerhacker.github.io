<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>198 nips-2012-Learning with Target Prior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-198" href="#">nips2012-198</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>198 nips-2012-Learning with Target Prior</h1>
<br/><p>Source: <a title="nips-2012-198-pdf" href="http://papers.nips.cc/paper/4849-learning-with-target-prior.pdf">pdf</a></p><p>Author: Zuoguan Wang, Siwei Lyu, Gerwin Schalk, Qiang Ji</p><p>Abstract: In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables y can be modeled with a prior model p(y) and the relations between data and target variables are estimated with p(y) and a set of uncorresponded data X in training. We term this method as learning with target priors (LTP). Speciﬁcally, LTP learning seeks parameter θ that maximizes the log likelihood of fθ (X) on a uncorresponded training set with regards to p(y). Compared to the conventional (semi)supervised learning approach, LTP can make efﬁcient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efﬁciently implemented and deployed in tasks where running efﬁciency is critical. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video. 1</p><p>Reference: <a title="nips-2012-198-reference" href="../nips2012_reference/nips-2012-Learning_with_Target_Prior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. [sent-12, score-0.761]
</p><p>2 In this work, we describe a new learning scheme for parametric learning, in which the target variables y can be modeled with a prior model p(y) and the relations between data and target variables are estimated with p(y) and a set of uncorresponded data X in training. [sent-13, score-0.784]
</p><p>3 We term this method as learning with target priors (LTP). [sent-14, score-0.227]
</p><p>4 Compared to the conventional (semi)supervised learning approach, LTP can make efﬁcient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. [sent-16, score-0.375]
</p><p>5 Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efﬁciently implemented and deployed in tasks where running efﬁciency is critical. [sent-17, score-0.227]
</p><p>6 We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video. [sent-18, score-0.449]
</p><p>7 1  Introduction  One of the central problems in machine learning is prediction/inference, where given an input datum X, we would like to predict or infer the value of a target variable of interest, y, assuming X and y have some intrinsic relationship. [sent-19, score-0.2]
</p><p>8 The prediction/inference task in many practical applications involves high dimensional and structured data and target variables. [sent-20, score-0.2]
</p><p>9 In the Bayesian approach, our knowledge about input and target variables, as well as their relationships, are all represented as probability distributions. [sent-22, score-0.2]
</p><p>10 The posterior distribution can be explicitly constructed from the target prior, p(y), which encodes our knowledge on the internal structure of the target y, and the likelihood, p(X|y), which summarizes the process of generating X from y, as p(y|X) ∝ p(X|y)p(y). [sent-24, score-0.4]
</p><p>11 Or it can be directly handled as in the conditional random ﬁelds [9] without referring to the target prior or the likelihood. [sent-25, score-0.279]
</p><p>12 The advantage of the Bayesian approach is that it incorporates prior knowledge about data and target variables into the prediction/inference task in a principled manner. [sent-26, score-0.315]
</p><p>13 In this work, we describe a new approach to learning a parametric regressor fθ (X), which we term as learning with target prior (LTP). [sent-36, score-0.447]
</p><p>14 In many practical applications, the target variables y follow the some regular spatial and temporal patterns that can be described probabilistically, and the observed target variables are samples of such distributions. [sent-37, score-0.534]
</p><p>15 Such regular patterns can beneﬁt the task of decoding the ﬁnger movements from ECoG signals in a brain computer interface (BCI) system, Fig. [sent-39, score-0.39]
</p><p>16 In LTP learning, we incorporate such spatial and temporal regular patterns of the target variables into the learning framework. [sent-44, score-0.272]
</p><p>17 Speciﬁcally, we learn a probability distribution p(y) that captures the spatial and temporal regularities of the target variable y, then we estimate the function parameters θ, by maximizing the log-likelihood of the output y = fθ (X) with respect to the the prior distribution. [sent-45, score-0.315]
</p><p>18 LTP learning can be applied to both unsupervised learning, in which no corresponded input and output are available, and semi-supervised learning in which part of corresponding outputs are available. [sent-46, score-0.217]
</p><p>19 We demonstrate the effectiveness of LTP learning in two problems: BCI decoding and pose estimation. [sent-47, score-0.282]
</p><p>20 In Sections 4 and 5, details on deployment and experimental evaluation of this general framework in two applications, namely BCI decoding and pose estimation from video, are described. [sent-50, score-0.282]
</p><p>21 The prior knowledge about the target variables in classiﬁcation problems is exploited in recent works as learning with uncertain labels, in which the distribution over the target class labels for each data example is used in place of corresponding pairs of data/target variables [10]. [sent-53, score-0.551]
</p><p>22 There are several works directly embed domain constraints about the target variables in learning. [sent-56, score-0.236]
</p><p>23 For instance, constraint driven learning (CODL) [3] enforces task speciﬁc constraints on the target labels by appending a penalty term in the objective function. [sent-57, score-0.2]
</p><p>24 Posterior regularization [5] directly imposes regularization on the posterior of the latent target variables, of which CODL can be seen as a special case with MAP approximation. [sent-58, score-0.228]
</p><p>25 However, all these approaches have only been applied to problems with discrete outputs (classiﬁcation or labeling) and may be difﬁcult to extend to incorporate complex dependencies in high-dimensional continuous target variables. [sent-60, score-0.225]
</p><p>26 Dependencies in the target variables can be directly modeled in conditional random ﬁelds (CRF) [9], as a probabilistic graphical model between the output components. [sent-62, score-0.236]
</p><p>27 Some of the recent supervised parametric learning methods can take advantage of some structure constraints over the target variables. [sent-64, score-0.333]
</p><p>28 These methods can be viewed as special cases of LTP learning, where general probabilistic models for target variables can be incorporated. [sent-68, score-0.236]
</p><p>29 3  General Framework  In this section, we describe the general framework of learning with target priors. [sent-69, score-0.2]
</p><p>30 Speciﬁcally, our task is to learn the parameter θ in a parametric family of functions of X, fθ (x), to best predict the corresponding target variable y. [sent-70, score-0.294]
</p><p>31 Both the data and target variable can be of high dimensions. [sent-71, score-0.2]
</p><p>32 Knowledge about target variable is provided through a target prior model in the form of a parametric probability distribution, pη (y), with model parameter η. [sent-72, score-0.573]
</p><p>33 In the following, we apply the LTP learning to unsupervised learning in which no corresponded input and output are available, as well as semi-supervised learning in which part of corresponding outputs are available. [sent-75, score-0.217]
</p><p>34 For the unsupervised learning, assume we are given a set of outputs y ∈ RY×m , as well as a set of uncorresponded inputs X ∈ RX ×n , where Y and X are the dimensionality, and m and n are the temporal length for y and X respectively. [sent-76, score-0.26]
</p><p>35 This is applicable to the case of BCI where it is easier to gather inputs X or structured targets y than it is to gather corresponded inputs and targets (X, y). [sent-77, score-0.291]
</p><p>36 In many real BCI applications the input brain signals X are collected only under thoughts without actual body movement y. [sent-78, score-0.257]
</p><p>37 The body movements could be easily collected when the brain signals are not being recorded. [sent-79, score-0.24]
</p><p>38 In both the ﬁnger movement decoding and pose estimation, y and X could be extracted from different subjects. [sent-81, score-0.353]
</p><p>39 A prior model pη (y) is learned from {yi }m , where yi ∈ RY×1 i=1 and η is parameter of the prior model. [sent-82, score-0.246]
</p><p>40 2, the parameter θ is chosen in the way that the outputs not only minimize the loss function on training data, but also make the predicted targets on the unlabeled data comply with the target prior. [sent-88, score-0.309]
</p><p>41 Next, we adapt unsupervised/semi-supervised learning with LTP to the prediction/inference in two applications, namely, decoding ECoG signal to predict ﬁnger movement in BCI and estimation of body poses from videos, where the-state-of-the-art performances are achieved. [sent-89, score-0.353]
</p><p>42 Many recent studies in neurobiology have suggested that electrocorticographic (ECoG) signals 3  recorded near the brain surface show strong correlations with limb motions [2, 8]. [sent-93, score-0.218]
</p><p>43 ECoG signal decoding is the critical step in ECoG based BCI systems, the goal of which is to obtain a functional mapping between the ECoG signals and the kinematic variables (e. [sent-94, score-0.346]
</p><p>44 The ECoG decoding problem has been widely solved with supervised parametric learning [26, 8, 25], where corresponded ECoG signals and target kinematic variables are collected from one subject and used to train a parametric regressor. [sent-97, score-1.009]
</p><p>45 However, the decoder learned from data collected from one subject in a controlled experiment usually has trouble to generalize for the same subject over time and in an open environment (temporal generalization) [18], or to decode signals from other subjects (cross-subject generalization) [24]. [sent-98, score-0.635]
</p><p>46 To generalize better across subjects, a collaborative paradigm was proposed to integrating information from multiple subjects [24]. [sent-105, score-0.225]
</p><p>47 In [17] it is investigated that certain spectral features of ECoG signals can be used across subjects to classify movements. [sent-106, score-0.306]
</p><p>48 At the same time, in BCI it is typically much easier to gather samples of uncorresponded target variables, i. [sent-108, score-0.378]
</p><p>49 e, traces of ﬁnger movements recorded by digital gloves, than it is to gather corresponding pairs of training samples. [sent-109, score-0.203]
</p><p>50 Thus in this work, we propose to improve the temporal and cross-subject generalization of BCI decoders with the learning with target priors framework. [sent-110, score-0.291]
</p><p>51 In the ﬁrst step, we obtain a parametric target prior model using uncorresponded samples of the target data, in this case, the traces of ﬁnger positions. [sent-111, score-0.752]
</p><p>52 Let us ﬁrst deﬁne notations that are to be used subsequently: we use a linear decoding function, as: fθ (x) = XT θ, to predict the traces of ﬁnger movements y as target variable. [sent-113, score-0.474]
</p><p>53 Linear decoding function are widely used in BCI decoding [1] for its simplicity and run-time efﬁciency in constructing hardware based BCI system. [sent-117, score-0.354]
</p><p>54 1  Target Prior Model  We use the Gaussian-Bernoulli restricted Boltzmann machine (GB-RBM) [14]: pη (y) = 1 −Eη (y,h) , where Z is the normalizing constant, and h ∈ {0, 1}H are binary hidden varihe Z ables, as the parametric target prior model. [sent-119, score-0.373]
</p><p>55 The target variable y is normalized to have zero mean and unit standard variance. [sent-123, score-0.2]
</p><p>56 2  Learning Regressor Parameter θ  With training data and the GB-RBM as the target prior model, we optimize the objective function of LTP in Eq. [sent-127, score-0.311]
</p><p>57 3  Experimental Settings  The ECoG data and target ﬁnger movement variables are collected from a clinical setting based on ﬁve subjects (A-E) who underwent brain surgeries [8]. [sent-137, score-0.559]
</p><p>58 For each channel, features are extracted based on signal power of three bands (1-60Hz, 60-100Hz, 100-200Hz) [2], which results in 144 or 204 features for subjects with 48 or 64 channels, respectively. [sent-144, score-0.276]
</p><p>59 4  Learning Target Prior Model and Decoding Function  The training data for the prior model pη (y) are either from other subjects or from the same subject but were collected at a different time and do not have correspondence with the training input data. [sent-146, score-0.452]
</p><p>60 5  Generalization Across Subjects  We learn the decoding function for new subjects by deploying the unsupervised LTP learning in Section 3. [sent-165, score-0.436]
</p><p>61 Even though it is difﬁcult to get the corresponded samples from new subjects, we always have the input ECoG signals, whose features will be used as the input of the unsupervised LTP learning. [sent-166, score-0.256]
</p><p>62 We compare the unsupervised LTP learning with linear regression [2] in two ways: 1) the linear regression (intra subject) in which the corresponded data and target variables are available. [sent-167, score-0.514]
</p><p>63 The accuracy of linear regression is calculated based on ﬁve fold cross-validation, that is, 4/5 trials (25 trials) are used for training and 1/5 trials (5 trials) are used for testing. [sent-168, score-0.208]
</p><p>64 2) the linear regression (inter 5  Table 1: Results on thumb of subjects based on 2 fold cross validation (correlation coefﬁcient). [sent-169, score-0.284]
</p><p>65 The results for inter subjects are calculated based on 5 fold cross-validation (each time one subject is used for training and the model is tested on other four subjects). [sent-181, score-0.429]
</p><p>66 Linear regression is trained on pairs of features and targets while LTP only uses the targets to train the prior model. [sent-182, score-0.313]
</p><p>67 For the linear regression trained and tested on different subjects, the channels across subjects are aligned by the 3-d position of the sensors. [sent-183, score-0.288]
</p><p>68 Note that the performances of the unsupervised LTP learning is on par with those of the linear regression (intra) on subject A, B, C and D, which suggests that the decoder learned by unsupervised LTP learning can generalize across subjects. [sent-185, score-0.469]
</p><p>69 On the other hand, not surprisingly, the performances of linear regression (inter subjects) suggest that it cannot be extended across subjects, which is due to brain difference for different subjects as stated above. [sent-187, score-0.293]
</p><p>70 The generalization ability gained by unsupervised LTP learning is mainly because it directly learns decoding functions on the new subject without using brain signal from existing subjects, which are believed to change dramatically among subjects. [sent-188, score-0.461]
</p><p>71 One thing we noticed is that the unsupervised LTP learning does not work well on subject E, which is because the thumb movement speed of subject E is much slower than subject A, on which the prior model is trained. [sent-189, score-0.608]
</p><p>72 This suggests that the quality of the target prior model is critical for the performance. [sent-190, score-0.279]
</p><p>73 5 300 0  50  100  150  200  250 300  (C)  Figure 3: (A) Comparison among three models across subjects; (B) Sample results for subject A; (C) Sample results for subject B. [sent-199, score-0.248]
</p><p>74 6  Online Learning for Decoding Functions  In the next set of experiment, we use the learning with target priors framework for learning decoding functions that generalize over time. [sent-201, score-0.433]
</p><p>75 The new samples come sequentially and thus we want the decoding function to be online updated. [sent-206, score-0.203]
</p><p>76 Then the decoding function i=1 with parameter θ is used to decode the ﬁrst batch {Xj }Y . [sent-210, score-0.253]
</p><p>77 After the batch {Xj }Y is decoded, j=1 i=1 {Xj }Y , not including the predicted target variables, is included as part of the unlabeled training j=1 data to update the parameter θ by the semi-supervised learning in section 3. [sent-211, score-0.276]
</p><p>78 Generally, we are trying to maximally use the ”seen” data to get the decoding function prepared for the ”unseen” coming samples. [sent-214, score-0.209]
</p><p>79 The model is tested on the thumb of ﬁve subjects based on 2 fold cross validation, that is, we treat the ﬁrst 15 trials as the paired data/target variables and then online test the remaining trials. [sent-216, score-0.394]
</p><p>80 This means that by regularizing 6  the new features with the target prior, the semi-supervised learning in Section 3 successfully obtains information from the new features and adapts the decoders well for new coming samples. [sent-219, score-0.336]
</p><p>81 5  Pose Estimation from Videos  In this section, we apply learning with target priors to the problem of the pose estimation problem, the goal of which is to extract 3D human pose from images or video sequences. [sent-220, score-0.488]
</p><p>82 We will show that the algorithms learned by LTP are more generalizable both across subjects and over time on the same subject respectively. [sent-222, score-0.374]
</p><p>83 sGPLVM models a shared latent space by pose and image features through GPLVM, while FOLS-GPLVM models a shared latent space and a private latent space for each part. [sent-238, score-0.227]
</p><p>84 The training of both sGPLVM and FOLS-GPLVM require corresponded images and poses (X, y) while LTP does not require this. [sent-242, score-0.201]
</p><p>85 For the unsupervised LTP learning, the target prior model is trained on the subspace of the joint angles {yi }n on sequence 1 and tested on the features of all 6 sequences. [sent-243, score-0.572]
</p><p>86 Ridge regression, sGPLVM and FOLS-GPLVM are trained on the ﬁrst sequence with paired samples {Xi , yi }n and tested on all the 6 sequences. [sent-246, score-0.206]
</p><p>87 We can see that when testing on the sequence from the same subject (sequence 2), unsupervised LTP learning is not the best. [sent-249, score-0.235]
</p><p>88 In contrast, when testing on the sequences from subjects B and C, unsupervised LTP learning achieves the best results, which is slightly better than sGPLVM. [sent-250, score-0.286]
</p><p>89 Considering that only linear dimension reduction and linear function are assumed for unsupervised LTP learning and paired samples are not required, unsupervised LTP learning is even more competitive. [sent-251, score-0.248]
</p><p>90 Thus the experiments demonstrate that the algorithm learned by unsupervised 7  Table 2: Train prior model on the ﬁrst sequence and test on all with mean absolute joint angle error. [sent-253, score-0.293]
</p><p>91 The reason that ridge regression, sGPLVM and FOLS-GPLVM do not generalize well is that the relations between poses and images are solely learned from corresponded poses and images, and these relations may have difﬁculty to hold for the new subjects due to may factors (i. [sent-292, score-0.559]
</p><p>92 LTP avoids this problem by learning the relations using the generalizable prior distribution over the targets and the images from the new subjects. [sent-294, score-0.214]
</p><p>93 In this experiment, for each subject we treat the ﬁrst sequence as the paired samples {Xi , yi }m and estimate the 3-D pose of the second sequence i=1 {Xi }n . [sent-296, score-0.408]
</p><p>94 The prior model is trained on the joint angles of the ﬁrst sequence {yi }m . [sent-297, score-0.221]
</p><p>95 6  Conclusion and Discussion  In this work, we describe a new learning scheme for parametric learning, known as learning with target priors, that uses a prior model over the target variables and a set of uncorresponded data in training. [sent-300, score-0.719]
</p><p>96 Compared to the conventional (semi)supervised learning approach, LTP can make efﬁcient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. [sent-301, score-0.375]
</p><p>97 Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efﬁciently implemented and deployed in tasks where running efﬁciency is critical, such as on-line BCI signal decoding. [sent-302, score-0.257]
</p><p>98 We demonstrate the effectiveness of the proposed approach in terms of generalization on parametric regression tasks for BCI signal decoding and pose estimation from video. [sent-303, score-0.449]
</p><p>99 First, in the current work we only use a simple target prior model in the form of GB-RBM. [sent-305, score-0.279]
</p><p>100 Anatomically constrained decoding of ﬁnger ﬂexion from electrocorticographic signals. [sent-436, score-0.237]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ltp', 0.705), ('ecog', 0.207), ('target', 0.2), ('bci', 0.194), ('nger', 0.192), ('decoding', 0.177), ('subjects', 0.17), ('sgplvm', 0.141), ('subject', 0.111), ('uncorresponded', 0.11), ('pose', 0.105), ('corresponded', 0.103), ('parametric', 0.094), ('unsupervised', 0.089), ('prior', 0.079), ('regressor', 0.074), ('schalk', 0.072), ('signals', 0.072), ('movement', 0.071), ('exion', 0.064), ('albany', 0.063), ('zuoguan', 0.063), ('electrocorticographic', 0.06), ('inter', 0.057), ('angles', 0.056), ('movements', 0.054), ('ridge', 0.054), ('brain', 0.054), ('yi', 0.052), ('targets', 0.052), ('trials', 0.049), ('gerwin', 0.047), ('intra', 0.047), ('gs', 0.047), ('decoder', 0.046), ('batch', 0.044), ('paired', 0.044), ('poses', 0.043), ('traces', 0.043), ('regression', 0.043), ('xt', 0.042), ('gather', 0.042), ('silhouette', 0.041), ('supervised', 0.039), ('features', 0.038), ('temporal', 0.036), ('ngers', 0.036), ('thumb', 0.036), ('learned', 0.036), ('variables', 0.036), ('sequence', 0.035), ('fold', 0.035), ('semi', 0.034), ('interface', 0.033), ('miller', 0.033), ('ry', 0.033), ('training', 0.032), ('body', 0.032), ('coming', 0.032), ('recorded', 0.032), ('decode', 0.032), ('codl', 0.031), ('ecse', 0.031), ('generalizable', 0.031), ('imcrbm', 0.031), ('kinematic', 0.031), ('siwei', 0.031), ('signal', 0.03), ('qiang', 0.03), ('generalize', 0.029), ('relations', 0.029), ('conventional', 0.028), ('collected', 0.028), ('video', 0.028), ('angle', 0.028), ('ji', 0.028), ('latent', 0.028), ('rensselaer', 0.028), ('rajesh', 0.028), ('decoders', 0.028), ('finger', 0.028), ('gideon', 0.028), ('ojemann', 0.028), ('sequences', 0.027), ('priors', 0.027), ('across', 0.026), ('samples', 0.026), ('joint', 0.026), ('polytechnic', 0.026), ('troy', 0.026), ('outputs', 0.025), ('ny', 0.025), ('trained', 0.025), ('tested', 0.024), ('xi', 0.024), ('shenoy', 0.024), ('train', 0.024), ('images', 0.023), ('rao', 0.023), ('deployed', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="198-tfidf-1" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>Author: Zuoguan Wang, Siwei Lyu, Gerwin Schalk, Qiang Ji</p><p>Abstract: In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables y can be modeled with a prior model p(y) and the relations between data and target variables are estimated with p(y) and a set of uncorresponded data X in training. We term this method as learning with target priors (LTP). Speciﬁcally, LTP learning seeks parameter θ that maximizes the log likelihood of fθ (X) on a uncorresponded training set with regards to p(y). Compared to the conventional (semi)supervised learning approach, LTP can make efﬁcient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efﬁciently implemented and deployed in tasks where running efﬁciency is critical. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video. 1</p><p>2 0.15705034 <a title="198-tfidf-2" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>Author: Hachem Kadri, Alain Rakotomamonjy, Philippe Preux, Francis R. Bach</p><p>Abstract: Positive deﬁnite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a ﬁnite linear combination of inﬁnite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an r -norm constraint on the combination coefﬁcients (r ≥ 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of ﬁnger movement prediction in brain-computer interfaces. 1</p><p>3 0.14295116 <a title="198-tfidf-3" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>Author: Joan Fruitet, Alexandra Carpentier, Maureen Clerc, Rémi Munos</p><p>Abstract: Brain-computer interfaces (BCI) allow users to “communicate” with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand, to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue. This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm, UCB-classif , based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefﬁcient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efﬁcient use of the BCI training session. Comparing the proposed method to the standard practice in task selection, for a ﬁxed time budget, UCB-classif leads to an improved classiﬁcation rate, and for a ﬁxed classiﬁcation rate, to a reduction of the time spent in training by 50%. 1</p><p>4 0.09506876 <a title="198-tfidf-4" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>Author: Kevin Tang, Vignesh Ramanathan, Li Fei-fei, Daphne Koller</p><p>Abstract: Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest ﬁrst. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features speciﬁc to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video [2] datasets that illustrate the beneﬁt of our approach to adapt object detectors to video. 1</p><p>5 0.093815267 <a title="198-tfidf-5" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>Author: Pieter-jan Kindermans, Hannes Verschore, David Verstraeten, Benjamin Schrauwen</p><p>Abstract: The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session. 1</p><p>6 0.082167886 <a title="198-tfidf-6" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>7 0.060817469 <a title="198-tfidf-7" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>8 0.059317276 <a title="198-tfidf-8" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>9 0.058316216 <a title="198-tfidf-9" href="./nips-2012-Pointwise_Tracking_the_Optimal_Regression_Function.html">271 nips-2012-Pointwise Tracking the Optimal Regression Function</a></p>
<p>10 0.054520011 <a title="198-tfidf-10" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>11 0.053436477 <a title="198-tfidf-11" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>12 0.048491046 <a title="198-tfidf-12" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>13 0.046505071 <a title="198-tfidf-13" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>14 0.046482991 <a title="198-tfidf-14" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>15 0.04416975 <a title="198-tfidf-15" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>16 0.043001916 <a title="198-tfidf-16" href="./nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">291 nips-2012-Reducing statistical time-series problems to binary classification</a></p>
<p>17 0.042770423 <a title="198-tfidf-17" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>18 0.042681877 <a title="198-tfidf-18" href="./nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">114 nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>19 0.042556707 <a title="198-tfidf-19" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>20 0.041892942 <a title="198-tfidf-20" href="./nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">230 nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.019), (2, -0.05), (3, 0.033), (4, 0.037), (5, 0.005), (6, 0.007), (7, 0.041), (8, -0.029), (9, -0.045), (10, -0.026), (11, 0.019), (12, 0.059), (13, 0.008), (14, 0.044), (15, -0.041), (16, -0.023), (17, 0.041), (18, -0.02), (19, -0.017), (20, 0.032), (21, 0.0), (22, -0.072), (23, -0.035), (24, 0.042), (25, -0.058), (26, 0.041), (27, 0.037), (28, 0.051), (29, 0.034), (30, -0.0), (31, 0.069), (32, 0.048), (33, 0.002), (34, -0.086), (35, -0.055), (36, -0.018), (37, -0.12), (38, -0.033), (39, 0.047), (40, -0.016), (41, -0.145), (42, 0.094), (43, 0.029), (44, -0.195), (45, 0.023), (46, -0.025), (47, 0.041), (48, 0.02), (49, -0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88261849 <a title="198-lsi-1" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>Author: Zuoguan Wang, Siwei Lyu, Gerwin Schalk, Qiang Ji</p><p>Abstract: In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables y can be modeled with a prior model p(y) and the relations between data and target variables are estimated with p(y) and a set of uncorresponded data X in training. We term this method as learning with target priors (LTP). Speciﬁcally, LTP learning seeks parameter θ that maximizes the log likelihood of fθ (X) on a uncorresponded training set with regards to p(y). Compared to the conventional (semi)supervised learning approach, LTP can make efﬁcient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efﬁciently implemented and deployed in tasks where running efﬁciency is critical. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video. 1</p><p>2 0.75281221 <a title="198-lsi-2" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>Author: Joan Fruitet, Alexandra Carpentier, Maureen Clerc, Rémi Munos</p><p>Abstract: Brain-computer interfaces (BCI) allow users to “communicate” with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand, to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue. This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm, UCB-classif , based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefﬁcient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efﬁcient use of the BCI training session. Comparing the proposed method to the standard practice in task selection, for a ﬁxed time budget, UCB-classif leads to an improved classiﬁcation rate, and for a ﬁxed classiﬁcation rate, to a reduction of the time spent in training by 50%. 1</p><p>3 0.735331 <a title="198-lsi-3" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>Author: Pieter-jan Kindermans, Hannes Verschore, David Verstraeten, Benjamin Schrauwen</p><p>Abstract: The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session. 1</p><p>4 0.70401001 <a title="198-lsi-4" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>Author: Uri Maoz, Shengxuan Ye, Ian Ross, Adam Mamelak, Christof Koch</p><p>Abstract: The ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientiﬁc study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action as well as for brain-machine interfaces. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a “matching-pennies” game against an opponent. In each trial, subjects were given a 5 s countdown, after which they had to raise their left or right hand immediately as the “go” signal appeared on a computer screen. They won a ﬁxed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The question we here studied was the extent to which neural precursors of the subjects’ decisions can be detected in intracranial local ﬁeld potentials (LFP) prior to the onset of the action. We found that combined low-frequency (0.1–5 Hz) LFP signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5 s before the go signal with 68±3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 electrodes simultaneously, and tested it on retrospective data from 7 patients. On average, we could predict the correct hand choice in 83% of the trials, which rose to 92% if we let the system drop 3/10 of the trials on which it was less conﬁdent. Our system demonstrates— for the ﬁrst time—the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings, well before the action occurs. 1 1</p><p>5 0.58121967 <a title="198-lsi-5" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>Author: Weixin Li, Nuno Vasconcelos</p><p>Abstract: In this work, we consider the problem of modeling the dynamic structure of human activities in the attributes space. A video sequence is Ä?Ĺš rst represented in a semantic feature space, where each feature encodes the probability of occurrence of an activity attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both the binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining binary observation variables with a hidden Gauss-Markov state process. In this way, it integrates the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by a popular LDS learning method from dynamic textures, is proposed. A similarity measure between BDSs, which generalizes the BinetCauchy kernel for LDS, is then introduced and used to design activity classiÄ?Ĺš ers. The proposed method is shown to outperform similar classiÄ?Ĺš ers derived from the kernel dynamic system (KDS) and state-of-the-art approaches for dynamics-based or attribute-based action recognition. 1</p><p>6 0.54095984 <a title="198-lsi-6" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>7 0.53207797 <a title="198-lsi-7" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>8 0.5163362 <a title="198-lsi-8" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>9 0.50445271 <a title="198-lsi-9" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>10 0.50148082 <a title="198-lsi-10" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>11 0.50070107 <a title="198-lsi-11" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>12 0.47451288 <a title="198-lsi-12" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>13 0.46013111 <a title="198-lsi-13" href="./nips-2012-Feature-aware_Label_Space_Dimension_Reduction_for_Multi-label_Classification.html">130 nips-2012-Feature-aware Label Space Dimension Reduction for Multi-label Classification</a></p>
<p>14 0.45389986 <a title="198-lsi-14" href="./nips-2012-Perceptron_Learning_of_SAT.html">267 nips-2012-Perceptron Learning of SAT</a></p>
<p>15 0.4525153 <a title="198-lsi-15" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>16 0.44785067 <a title="198-lsi-16" href="./nips-2012-Large_Scale_Distributed_Deep_Networks.html">170 nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>17 0.44450408 <a title="198-lsi-17" href="./nips-2012-Assessing_Blinding_in_Clinical_Trials.html">46 nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>18 0.42351741 <a title="198-lsi-18" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>19 0.40373462 <a title="198-lsi-19" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>20 0.40305996 <a title="198-lsi-20" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (17, 0.024), (21, 0.022), (38, 0.107), (39, 0.011), (42, 0.035), (54, 0.041), (55, 0.019), (71, 0.249), (74, 0.052), (76, 0.153), (77, 0.012), (80, 0.1), (92, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82266635 <a title="198-lda-1" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>Author: Takayuki Osogami</p><p>Abstract: We uncover relations between robust MDPs and risk-sensitive MDPs. The objective of a robust MDP is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties. The objective of a risk-sensitive MDP is to minimize a risk measure of the cumulative cost when the parameters are known. We show that a risk-sensitive MDP of minimizing the expected exponential utility is equivalent to a robust MDP of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the Kullback-Leibler divergence. We also show that a risk-sensitive MDP of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust MDP of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function. 1</p><p>same-paper 2 0.78000897 <a title="198-lda-2" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>Author: Zuoguan Wang, Siwei Lyu, Gerwin Schalk, Qiang Ji</p><p>Abstract: In the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables. In this work, we describe a new learning scheme for parametric learning, in which the target variables y can be modeled with a prior model p(y) and the relations between data and target variables are estimated with p(y) and a set of uncorresponded data X in training. We term this method as learning with target priors (LTP). Speciﬁcally, LTP learning seeks parameter θ that maximizes the log likelihood of fθ (X) on a uncorresponded training set with regards to p(y). Compared to the conventional (semi)supervised learning approach, LTP can make efﬁcient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning. Compared to the Bayesian approach, the learned parametric regressor in LTP can be more efﬁciently implemented and deployed in tasks where running efﬁciency is critical. We demonstrate the effectiveness of the proposed approach on parametric regression tasks for BCI signal decoding and pose estimation from video. 1</p><p>3 0.75284255 <a title="198-lda-3" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>Author: Emanuele Coviello, Gert R. Lanckriet, Antoni B. Chan</p><p>Abstract: In this paper, we derive a novel algorithm to cluster hidden Markov models (HMMs) according to their probability distributions. We propose a variational hierarchical EM algorithm that i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a “cluster center”, i.e., a novel HMM that is representative for the group. We illustrate the beneﬁts of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging. 1</p><p>4 0.71648228 <a title="198-lda-4" href="./nips-2012-Learning_Manifolds_with_K-Means_and_K-Flats.html">179 nips-2012-Learning Manifolds with K-Means and K-Flats</a></p>
<p>Author: Guillermo Canas, Tomaso Poggio, Lorenzo Rosasco</p><p>Abstract: We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-ﬂats, and analyze their performance. We extend previous results for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-ﬂats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-ﬂats, both the results and the mathematical tools are new. 1</p><p>5 0.69035113 <a title="198-lda-5" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>Author: Katherine Chen, Michael Bowling</p><p>Abstract: Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance. One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations. In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty. Instead we focus on identifying optimization objectives for which solutions can be efﬁciently approximated. We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efﬁciently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP. 1</p><p>6 0.66856617 <a title="198-lda-6" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>7 0.66793728 <a title="198-lda-7" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>8 0.66694427 <a title="198-lda-8" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>9 0.66493517 <a title="198-lda-9" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>10 0.66478533 <a title="198-lda-10" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>11 0.6647352 <a title="198-lda-11" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>12 0.66448462 <a title="198-lda-12" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>13 0.66329008 <a title="198-lda-13" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>14 0.6615023 <a title="198-lda-14" href="./nips-2012-Spectral_learning_of_linear_dynamics_from_generalised-linear_observations_with_application_to_neural_population_data.html">321 nips-2012-Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</a></p>
<p>15 0.66145414 <a title="198-lda-15" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>16 0.66114545 <a title="198-lda-16" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>17 0.66094941 <a title="198-lda-17" href="./nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">232 nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>18 0.66079557 <a title="198-lda-18" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>19 0.66078484 <a title="198-lda-19" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>20 0.66057003 <a title="198-lda-20" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
