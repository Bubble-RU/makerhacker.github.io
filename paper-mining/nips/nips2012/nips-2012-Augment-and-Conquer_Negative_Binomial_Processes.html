<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 nips-2012-Augment-and-Conquer Negative Binomial Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-47" href="#">nips2012-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 nips-2012-Augment-and-Conquer Negative Binomial Processes</h1>
<br/><p>Source: <a title="nips-2012-47-pdf" href="http://papers.nips.cc/paper/4677-augment-and-conquer-negative-binomial-processes.pdf">pdf</a></p><p>Author: Mingyuan Zhou, Lawrence Carin</p><p>Abstract: By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efﬁcient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. 1</p><p>Reference: <a title="nips-2012-47-reference" href="../nips2012_reference/nips-2012-Augment-and-Conquer_Negative_Binomial_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. [sent-7, score-0.438]
</p><p>2 We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. [sent-9, score-0.2]
</p><p>3 A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. [sent-10, score-0.396]
</p><p>4 1  Introduction  There has been increasing interest in count modeling using the Poisson process, geometric process [1, 2, 3, 4] and recently the negative binomial (NB) process [5, 6]. [sent-11, score-0.467]
</p><p>5 Notably, it has been independently shown in [5] and [6] that the NB process, originally constructed for count analysis, can be naturally applied for mixture modeling of grouped data x1 , · · · , xJ , where each group xj = {xji }i=1,Nj . [sent-12, score-0.381]
</p><p>6 For a territory long occupied by the hierarchical Dirichlet process (HDP) [7] and related models, the inference of which may require substantial bookkeeping and suffer from slow convergence [7], the discovery of the NB process for mixture modeling can be signiﬁcant. [sent-13, score-0.356]
</p><p>7 As the seemingly distinct problems of count and mixture modeling are united under the NB process framework, new opportunities emerge for better data ﬁtting, more efﬁcient inference and more ﬂexible model constructions. [sent-14, score-0.396]
</p><p>8 We perform joint count and mixture modeling under the NB process framework, using completely random measures [1, 8, 9] that are simple to construct and amenable for posterior computation. [sent-18, score-0.326]
</p><p>9 1 Poisson process for count and mixture modeling Before introducing the NB process, we ﬁrst illustrate how the seemingly distinct problems of count and mixture modeling can be united under the Poisson process. [sent-23, score-0.612]
</p><p>10 Denote Ω as a measure space and for each Borel set A ⊂ Ω, denote Xj (A) as a count random variable describing the number of observations in xj that reside within A. [sent-24, score-0.249]
</p><p>11 Given grouped data x1 , · · · , xJ , for any measurable disjoint partition A1 , · · · , AQ of Ω, we aim to jointly model the count random variables {Xj (Aq )}. [sent-25, score-0.179]
</p><p>12 (2) Thus the Poisson process provides not only a way to generate independent counts from each Aq , but also a mechanism for mixture modeling, which allocates the observations into any measurable disjoint partition {Aq }1,Q of Ω, conditioning on Xj (Ω) and the normalized mean measure G. [sent-30, score-0.257]
</p><p>13 To complete the model, we may place a gamma process [9] prior on the shared measure as G ∼ GaP(c, G0 ), with concentration parameter c and base measure G0 , such that G(A) ∼ Gamma(G0 (A), 1/c) for each A ⊂ Ω, where G0 can be continuous, discrete or a combination of both. [sent-31, score-0.456]
</p><p>14 The normalized gamma representation of the DP is discussed in [10, 11, 9] and has been used to construct the group-level DPs for an HDP [12]. [sent-33, score-0.212]
</p><p>15 The Poisson process has an equal-dispersion assumption for count modeling. [sent-34, score-0.212]
</p><p>16 As shown in (2), the construction of Poisson processes with a shared gamma process mean measure implies the same mixture proportions across groups, which is essentially the same as the DP when used for mixture modeling when the total counts {Xj (Ω)}j are not treated as random variables. [sent-35, score-0.667]
</p><p>17 The NB distribution can be augmented into a gamma-Poisson construction as m ∼ Pois(λ), λ ∼ Gamma (r, p/(1 − p)), where the gamma distribution is parameterized by its shape r and scale p/(1 − p). [sent-42, score-0.236]
</p><p>18 The inference of the NB dispersion parameter r has long been a challenge [13, 18, 19]. [sent-45, score-0.145]
</p><p>19 In this paper, we ﬁrst place a gamma prior on it as r ∼ Gamma(r1 , 1/c1 ). [sent-46, score-0.212]
</p><p>20 Since l ∼ Pois(−r ln(1 − p)) by construction, we can use the gamma Poisson conjugacy to update r. [sent-49, score-0.237]
</p><p>21 2 (below), we can further infer an augmented latent count l for each l, and then use these latent counts to update r1 , assuming r1 ∼ Gamma(r2 , 1/c2 ). [sent-51, score-0.234]
</p><p>22 2, we can continue this process repeatedly, suggesting that we may build a NB process to model data that have subgroups within groups. [sent-54, score-0.18]
</p><p>23 The conditional posterior of the latent count l was ﬁrst derived by us but was not given an analytical form [20]. [sent-55, score-0.173]
</p><p>24 We denote l ∼ CRT(m, r) as a Chinese restaurant table (CRT) count random variable with such a PMF and as proved in the m supplementary material, we can sample it as l = n=1 bn , bn ∼ Bernoulli (r/(n − 1 + r)). [sent-57, score-0.147]
</p><p>25 3  Gamma-Negative Binomial Process  We explore sharing the NB dispersion across groups while the probability parameters are group dependent. [sent-84, score-0.174]
</p><p>26 We deﬁne a NB process X ∼ NBP(G, p) as X(A) ∼ NB(G(A), p) for each A ⊂ Ω and construct a gamma-NB process for joint count and mixture modeling as Xj ∼ NBP(G, pj ), G ∼ GaP(c, G0 ), which can be augmented as a gamma-gamma-Poisson process as Xj ∼ PP(Λj ), Λj ∼ GaP((1 − pj )/pj , G), G ∼ GaP(c, G0 ). [sent-85, score-1.166]
</p><p>27 (5) In the above PP(·) and GaP(·) represent the Poisson and gamma processes, respectively, as deﬁned in Section 1. [sent-86, score-0.212]
</p><p>28 2, the gamma-NB process can also be augmented as Lj Xj ∼ t=1 Log(pj ), Lj ∼ PP(−G ln(1 − pj )), G ∼ GaP(c, G0 ); (6) L=  j  Lj ∼  L t=1  Log(p ), L ∼ PP(−G0 ln(1 − p )), p =  − c−  j j  ln(1−pj ) ln(1−pj ) . [sent-89, score-0.432]
</p><p>29 Using the gamma Poisson conjugacy on (5), for each A ⊂ Ω, we have Λj (A)|G, Xj , pj ∼ Gamma (G(A) + Xj (A), pj ), thus the conditional posterior of Λj is Λj |G, Xj , pj ∼ GaP 1/pj , G + Xj . [sent-91, score-1.191]
</p><p>30 In k=1 K δωk , then L (ωk ) = CRT(L(ωk ), K ) ≥ 1 if either case, let γ0 ∼ Gamma(e0 , 1/f0 ), with the gamma Poisson conjugacy on (6) and (7), we have 1 γ0 |{L (Ω), p } ∼ Gamma e0 + L (Ω), f0 −ln(1−p ) ; (11) G|G0 , {Lj , pj } ∼ GaP c −  j  ln(1 − pj ), G0 +  j  Lj . [sent-96, score-0.873]
</p><p>31 Thus the normalized gamma-NB process leads to an HDP, yet we cannot return from the HDP to the gamma-NB process without modeling Xj (Ω) and Λj (Ω) as random variables. [sent-100, score-0.232]
</p><p>32 Practically, the gamma-NB process can exploit conjugacy to achieve analytical conditional posteriors for all latent parameters. [sent-102, score-0.187]
</p><p>33 In the HDP, pj is not explicitly modeled, and since its value becomes irrelevant when taking the normalized constructions in (14), it is usually treated as a nuisance parameter and perceived as pj = 0. [sent-110, score-0.663]
</p><p>34 2 Augment-and-conquer inference for joint count and mixture modeling For a ﬁnite continuous base measure, the gamma process G ∼ GaP(c, G0 ) can also be deﬁned with its L´ vy measure on a product space R+ × Ω, expressed as ν(drdω) = r−1 e−cr drG0 (dω) [9]. [sent-116, score-0.65]
</p><p>35 e Since the Poisson intensity ν + = ν(R+ ×Ω) = ∞ and rν(drdω) is ﬁnite, a draw from this R+ ×Ω ∞ process can be expressed as G = k=1 rk δωk , (rk , ωk ) ∼ π(drdω), π(drdω)ν + ≡ ν(drdω) [9]. [sent-117, score-0.277]
</p><p>36 K Here we consider a discrete base measure as G0 = k=1 γ0 δωk , ωk ∼ g0 (ωk ), then we have G = K K k=1 rk δωk , rk ∼ Gamma(γ0 /K, 1/c), ωk ∼ g0 (ωk ), which becomes a draw from the gamma process with a continuous base measure as K → ∞. [sent-118, score-0.836]
</p><p>37 Let xji ∼ F (ωzji ) be observation i in group j, Nj linked to a mixture component ωzji ∈ Ω through a distribution F . [sent-119, score-0.204]
</p><p>38 Using the equivalence between (1) and (2), we can equivalently express Nj and njk in the above model as Nj ∼ Pois (λj ) , [nj1 , · · · , njK ] ∼ K Mult (Nj ; λj1 /λj , · · · , λjK /λj ), where λj = k=1 λjk . [sent-121, score-0.257]
</p><p>39 Since the data {xji }i=1,Nj are fully exchangeable, rather than drawing [nj1 , · · · , njK ] once, we may equivalently draw the index zji ∼ Discrete (λj1 /λj , · · · , λjK /λj ) (17) 4  N  j for each xji and then let njk = i=1 δ(zji = k). [sent-122, score-0.535]
</p><p>40 This provides further insights on how the seemingly disjoint problems of count and mixture modeling are united under the NB process framework. [sent-123, score-0.377]
</p><p>41 Note that when K → ∞, we have (lk |−) = δ( j ljk > 0) = δ( j njk > 0). [sent-127, score-0.3]
</p><p>42 This also implies that by using the Dirichlet process as the foundation, traditional mixture modeling may discard useful count information from the beginning. [sent-129, score-0.326]
</p><p>43 4  The Negative Binomial Process Family and Related Algorithms  The gamma-NB process shares the NB dispersion across groups. [sent-130, score-0.215]
</p><p>44 Since the NB distribution has two adjustable parameters, we may explore alternative ideas, with the NB probability measure shared across groups as in [6], or with both the dispersion and probability measures shared as in [5]. [sent-131, score-0.242]
</p><p>45 It is natural to let the probability measure be drawn from a beta process [25, 26], which can be deﬁned by its L´ vy measure on a product space [0, 1] × Ω as ν(dpdω) = cp−1 (1 − p)c−1 dpB0 (dω). [sent-133, score-0.254]
</p><p>46 e A draw from the beta process B ∼ BP(c, B0 ) with concentration parameter c and base measure B0 ∞ can be expressed as B = k=1 pk δωk . [sent-134, score-0.398]
</p><p>47 A beta-NB process [5, 6] can be constructed by letting Xj ∼ ∞ NBP(rj , B), with a random draw expressed as Xj = k=1 njk δωk , njk ∼ NB(rj , pk ). [sent-135, score-0.782]
</p><p>48 Under this construction, the NB probability measure is shared and the NB dispersion parameters are group dependent. [sent-136, score-0.189]
</p><p>49 As in [5], we may also consider a marked-beta-NB1 process that both the NB probability and dispersion measures are shared, in which each point of the beta process is marked with an independent gamma random variable. [sent-137, score-0.597]
</p><p>50 Thus a draw from the marked-beta process becomes (R, B) = ∞ ∞ k=1 njk δωk , njk ∼ k=1 (rk , pk )δωk , and the NB process Xj ∼ NBP(R, B) becomes Xj = NB(rk , pk ). [sent-138, score-1.01]
</p><p>51 This construction can be linked to the model in [27] with appropriate normalization, with advantages that there is no need to ﬁx pj = 0. [sent-141, score-0.318]
</p><p>52 The zero inﬂated construction can also be linked to models for real valued data using the Indian buffet process (IBP) or beta-Bernoulli process spike-and-slab prior [28, 29, 30, 31]. [sent-143, score-0.2]
</p><p>53 1  Related Algorithms  To show how the NB processes can be diversely constructed and to make connections to previous parametric and nonparametric mixture models, we show in Table 1 a variety of NB processes, which differ on how the dispersion and probability measures are shared. [sent-145, score-0.293]
</p><p>54 5  Table 1: A variety of negative binomial processes are constructed with distinct sharing mechanisms, reﬂected with which parameters from rk , rj , pk , pj and πk (bjk ) are inferred (indicated by a check-mark ), and the implied VMR and ODL for counts {njk }j,k . [sent-147, score-1.064]
</p><p>55 They are applied for topic modeling of a document corpus, a typical example of mixture modeling of grouped data. [sent-148, score-0.361]
</p><p>56 Algorithms rk rj pk pj πk VMR ODL Related Algorithms −1 NB-LDA (1 − pj )−1 rj LDA [32], Dir-PFA [5] −1 NB-HDP 0. [sent-150, score-1.199]
</p><p>57 5 2 (rk ) bjk FTM [27], SγΓ-PFA [5] −1 Beta-NB (1 − pk )−1 rj BNBP [5], BNBP [6] −1 Gamma-NB (1 − pj )−1 rk CRF-HDP [7, 24] −1 Marked-Beta-NB (1 − pk )−1 rk BNBP [5] settings. [sent-152, score-1.155]
</p><p>58 We consider topic modeling of a document corpus, a typical example of mixture modeling of grouped data, where each a-bag-of-words document constitutes a group, each word is an exchangeable group member, and F (xji ; ωk ) is simply the probability of word xji in topic ωk . [sent-153, score-0.713]
</p><p>59 We consider six differently constructed NB processes in Table 1: (i) Related to latent Dirichlet allocation (LDA) [32] and Dirichlet Poisson factor analysis (Dir-PFA) [5], the NB-LDA is also a parametric topic model that requires tuning the number of topics. [sent-154, score-0.213]
</p><p>60 However, it uses a document dependent rj and pj to automatically learn the smoothing of the gamma distributed topic weights, and it lets rj ∼ Gamma(γ0 , 1/c), γ0 ∼ Gamma(e0 , 1/f0 ) to share statistical strength between documents, with closed-form Gibbs sampling inference. [sent-155, score-0.914]
</p><p>61 Thus even the most basic parametric LDA topic model can be improved under the NB count modeling framework. [sent-156, score-0.289]
</p><p>62 (ii) The NB-HDP model is related to the HDP [7], and since pj is an irrelevant parameter in the HDP due to normalization, we set it in the NB-HDP as 0. [sent-157, score-0.318]
</p><p>63 The NB-HDP model is comparable to the DILN-HDP [12] that constructs the group-level DPs with normalized gamma processes, whose scale parameters are also set as one. [sent-159, score-0.212]
</p><p>64 (iii) The NB-FTM model introduces an additional beta-Bernoulli process under the NB process framework to explicitly model zero counts. [sent-160, score-0.18]
</p><p>65 It is the same as the sparse-gamma-gamma-PFA (SγΓ-PFA) in [5] and is comparable to the focused topic model (FTM) [27], which is constructed from the IBP compound DP. [sent-161, score-0.191]
</p><p>66 The Zero-Inﬂated-NB process improves over them by allowing pj to be inferred, which generally yields better data ﬁtting. [sent-163, score-0.408]
</p><p>67 (iv) The Gamma-NB process explores the idea that the dispersion measure is shared across groups, and it improves over the NBHDP by allowing the learning of pj . [sent-164, score-0.597]
</p><p>68 It reduces to the HDP [7] by normalizing both the group-level and the shared gamma processes. [sent-165, score-0.245]
</p><p>69 (v) The Beta-NB process explores sharing the probability measure across groups, and it improves over the beta negative binomial process (BNBP) proposed in [6], allowing inference of rj . [sent-166, score-0.562]
</p><p>70 (vi) The Marked-Beta-NB process is comparable to the BNBP proposed in [5], with the distinction that it allows analytical update of rk . [sent-167, score-0.307]
</p><p>71 , λjk , rk and pk , are also of interest, then the NB process based joint count and mixture models would apparently be more appropriate than the HDP based mixture models. [sent-172, score-0.681]
</p><p>72 5  Example Results  Motivated by Table 1, we consider topic modeling using a variety of NB processes, which differ on which parameters are learned and consequently how the VMR and ODL of the latent counts {njk }j,k are modeled. [sent-173, score-0.234]
</p><p>73 For fair comparison, they are all implemented with block Gibbs sampling using a discrete base measure with K atoms, and for the ﬁrst ﬁfty iterations, the Gamma-NB process with rk ≡ 50/K and pj ≡ 0. [sent-175, score-0.685]
</p><p>74 For LDA, we set the topic proportion Dirichlet smoothing parameter as 50/K, following the topic model toolbox2 provided for [35]. [sent-181, score-0.23]
</p><p>75 be assigned to a topic k based on both F (xji ; ωk ) and the topic weights {λjk }k=1,K ; each topic is drawn from a Dirichlet base measure as ωk ∼ Dir(η, · · · , η) ∈ RV , where V is the number of unique terms in the vocabulary and η is a smoothing parameter. [sent-194, score-0.415]
</p><p>76 Let vji denote the location of word xji in the vocabulary, then we have (ωk |−) ∼ Dir η + j i δ(zji = k, vji = 1), · · · , η + j i δ(zji = k, vji = V ) . [sent-195, score-0.26]
</p><p>77 Note that the perplexity per test word is the fair metric to compare topic models. [sent-201, score-0.163]
</p><p>78 However, when the actual Poisson rates or distribution parameters for counts instead of the mixture proportions are of interest, it is obvious that a NB process based joint count and mixture model would be more appropriate than an HDP based mixture model. [sent-202, score-0.466]
</p><p>79 Figure 2 shows the learned model parameters by various algorithms under the NB process framework, revealing distinct sharing mechanisms and model properties. [sent-206, score-0.169]
</p><p>80 When (rj , pk ) is used to model the latent counts {njk }j,k , as in the Beta-NB process, the transition between active and non-active topics is very sharp that pk is either close to one or close to zero. [sent-208, score-0.416]
</p><p>81 When (rk , pj ) is used, as in the Gamma-NB process, the transition is much smoother that rk gradually decreases. [sent-210, score-0.505]
</p><p>82 Therefore, we can expect that (rk , pj ) would allow 2  http://psiexp. [sent-212, score-0.318]
</p><p>83 Note that the transition between active and non-active topics is very sharp when pk is used and much smoother when rk is used. [sent-225, score-0.378]
</p><p>84 more topics than (rj , pk ), as conﬁrmed in Figure 1 (a) that the Gamma-NB process learns 177 active topics, signiﬁcantly more than the 107 ones of the Beta-NB process. [sent-229, score-0.281]
</p><p>85 With these analysis, we can conclude that the mean and the amount of overdispersion (measure by the VMR or ODL) for the usage of topic k is positively correlated under (rj , pk ) and negatively correlated under (rk , pj ). [sent-230, score-0.695]
</p><p>86 When (rk , pk ) is used, as in the Marked-Beta-NB process, more diverse combinations of mean and overdispersion would be allowed as both rk and pk are now responsible for the mean E[ j njk ] = Jrk pk /(1−pk ). [sent-231, score-1.022]
</p><p>87 For example, there could be not only large mean and small overdispersion (large rk and small pk ), but also large mean and large overdispersion (small rk and large pk ). [sent-232, score-0.898]
</p><p>88 Thus (rk , pk ) may combine the advantages of using only rk or pk to model topic k, as conﬁrmed by the superior performance of the Marked-Beta-NB over the Beta-NB and Gamma-NB processes. [sent-233, score-0.618]
</p><p>89 When (rk , πk ) is used, as in the NB-FTM model, our results show that we usually have a small πk and a large rk , indicating topic k is sparsely used across the documents but once it is used, the amount of variation on usage is small. [sent-234, score-0.302]
</p><p>90 This modeling properties might be helpful when there are excessive number of zeros which might not be well modeled by the NB process alone. [sent-235, score-0.162]
</p><p>91 In our experiments, we ﬁnd the more direct approaches of using pk or pj generally yield better results, but this might not be the case when excessive number of zeros are better explained with the underlying beta-Bernoulli or IBP processes, e. [sent-236, score-0.496]
</p><p>92 However, from a count modeling viewpoint, this would make a restrictive assumption that each count vector {njk }k=1,K has the same VMR of 2, and the experimental results in Figure 1 conﬁrm the importance of learning pj together with rk . [sent-242, score-0.801]
</p><p>93 It is also interesting to examine (15), which can be viewed as the concentration parameter α in the HDP, allowing the adjustment of pj would allow a more ﬂexible model assumption on the amount of variations between the topic proportions, and thus potentially better data ﬁtting. [sent-243, score-0.433]
</p><p>94 6  Conclusions  We propose a variety of negative binomial (NB) processes to jointly model counts across groups, which can be naturally applied for mixture modeling of grouped data. [sent-244, score-0.359]
</p><p>95 The proposed NB processes are completely random measures that they assign independent random variables to disjoint Borel sets of the measure space, as opposed to the hierarchical Dirichlet process (HDP) whose measures on disjoint Borel sets are negatively correlated. [sent-245, score-0.254]
</p><p>96 We demonstrate that the gamma-NB process, which shares the NB dispersion measure across groups, can be normalized to produce the HDP and we show in detail its theoretical, structural and computational advantages over the HDP. [sent-247, score-0.156]
</p><p>97 We examine the distinct sharing mechanisms and model properties of various NB processes, with connections to existing algorithms, with experimental results on topic modeling showing the importance of modeling both the NB dispersion and probability parameters. [sent-248, score-0.423]
</p><p>98 The IBP compound Dirichlet process and its application to focused topic modeling. [sent-432, score-0.261]
</p><p>99 Dependent hierarchical beta process for image interpolation and denoising. [sent-458, score-0.19]
</p><p>100 On the integration of topic modeling and dictionary learning. [sent-465, score-0.167]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nb', 0.62), ('pj', 0.318), ('njk', 0.257), ('gamma', 0.212), ('hdp', 0.187), ('rk', 0.187), ('pk', 0.158), ('xji', 0.142), ('dispersion', 0.125), ('count', 0.122), ('topic', 0.115), ('rj', 0.109), ('poisson', 0.109), ('vmr', 0.107), ('zji', 0.107), ('overdispersion', 0.104), ('pois', 0.104), ('odl', 0.097), ('xj', 0.096), ('aq', 0.094), ('process', 0.09), ('binomial', 0.088), ('ln', 0.087), ('beta', 0.08), ('crt', 0.075), ('jk', 0.067), ('mixture', 0.062), ('dirichlet', 0.058), ('processes', 0.057), ('compound', 0.056), ('lj', 0.054), ('bnbp', 0.054), ('drd', 0.054), ('modeling', 0.052), ('document', 0.051), ('lda', 0.05), ('nbp', 0.047), ('counts', 0.046), ('ljk', 0.043), ('base', 0.039), ('bjk', 0.038), ('ftm', 0.038), ('gibbs', 0.036), ('topics', 0.033), ('shared', 0.033), ('crtp', 0.032), ('vji', 0.032), ('measure', 0.031), ('gap', 0.031), ('pp', 0.03), ('analytical', 0.03), ('index', 0.029), ('grouped', 0.029), ('sharing', 0.029), ('nonparametric', 0.029), ('ibp', 0.029), ('paisley', 0.029), ('dp', 0.028), ('zhou', 0.028), ('dir', 0.028), ('disjoint', 0.028), ('distinct', 0.027), ('corpus', 0.027), ('augmenting', 0.027), ('constructions', 0.027), ('pmf', 0.027), ('perplexity', 0.026), ('borel', 0.025), ('sr', 0.025), ('conjugacy', 0.025), ('nj', 0.025), ('restaurant', 0.025), ('dunson', 0.025), ('wj', 0.025), ('negative', 0.025), ('augmented', 0.024), ('dps', 0.023), ('seemingly', 0.023), ('mechanisms', 0.023), ('occupied', 0.022), ('vy', 0.022), ('word', 0.022), ('proportions', 0.022), ('posteriors', 0.021), ('augmentations', 0.021), ('cwj', 0.021), ('fjv', 0.021), ('overdispersed', 0.021), ('perplexities', 0.021), ('pgf', 0.021), ('latent', 0.021), ('biometrics', 0.021), ('sapiro', 0.021), ('discrete', 0.02), ('hierarchical', 0.02), ('constructed', 0.02), ('groups', 0.02), ('inference', 0.02), ('excessive', 0.02), ('buffet', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="47-tfidf-1" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>Author: Mingyuan Zhou, Lawrence Carin</p><p>Abstract: By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efﬁcient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. 1</p><p>2 0.25884008 <a title="47-tfidf-2" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>Author: James Scott, Jonathan W. Pillow</p><p>Abstract: Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses. The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability. Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latentvariable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals. This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efﬁcient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models. We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains. 1</p><p>3 0.12373737 <a title="47-tfidf-3" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>4 0.11541519 <a title="47-tfidf-4" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>5 0.11398416 <a title="47-tfidf-5" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><p>6 0.11113959 <a title="47-tfidf-6" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>7 0.11008543 <a title="47-tfidf-7" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>8 0.10010615 <a title="47-tfidf-8" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>9 0.098525852 <a title="47-tfidf-9" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>10 0.095984511 <a title="47-tfidf-10" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>11 0.089787215 <a title="47-tfidf-11" href="./nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">140 nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<p>12 0.086026601 <a title="47-tfidf-12" href="./nips-2012-Newton-Like_Methods_for_Sparse_Inverse_Covariance_Estimation.html">240 nips-2012-Newton-Like Methods for Sparse Inverse Covariance Estimation</a></p>
<p>13 0.08426322 <a title="47-tfidf-13" href="./nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">59 nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<p>14 0.083584219 <a title="47-tfidf-14" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>15 0.082168549 <a title="47-tfidf-15" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>16 0.070774212 <a title="47-tfidf-16" href="./nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>17 0.07016705 <a title="47-tfidf-17" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>18 0.068718016 <a title="47-tfidf-18" href="./nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">214 nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>19 0.067393981 <a title="47-tfidf-19" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>20 0.06628862 <a title="47-tfidf-20" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.05), (2, 0.001), (3, 0.054), (4, -0.189), (5, -0.027), (6, 0.002), (7, -0.01), (8, 0.121), (9, -0.004), (10, 0.121), (11, 0.113), (12, 0.033), (13, -0.076), (14, 0.055), (15, 0.012), (16, 0.06), (17, 0.027), (18, -0.009), (19, 0.02), (20, 0.053), (21, -0.019), (22, 0.027), (23, 0.026), (24, -0.0), (25, -0.025), (26, 0.045), (27, -0.06), (28, -0.086), (29, -0.022), (30, -0.07), (31, 0.055), (32, 0.084), (33, 0.01), (34, 0.099), (35, 0.012), (36, -0.032), (37, 0.079), (38, -0.039), (39, 0.131), (40, -0.22), (41, 0.064), (42, 0.027), (43, 0.062), (44, -0.051), (45, -0.062), (46, -0.035), (47, 0.026), (48, 0.028), (49, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96039093 <a title="47-lsi-1" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>Author: Mingyuan Zhou, Lawrence Carin</p><p>Abstract: By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efﬁcient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. 1</p><p>2 0.62949175 <a title="47-lsi-2" href="./nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">59 nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<p>Author: Francois Caron</p><p>Abstract: We develop a novel Bayesian nonparametric model for random bipartite graphs. The model is based on the theory of completely random measures and is able to handle a potentially inﬁnite number of nodes. We show that the model has appealing properties and in particular it may exhibit a power-law behavior. We derive a posterior characterization, a generative process for network growth, and a simple Gibbs sampler for posterior simulation. Our model is shown to be well ﬁtted to several real-world social networks. 1</p><p>3 0.61886352 <a title="47-lsi-3" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>Author: Dahua Lin, John W. Fisher</p><p>Abstract: Mixture distributions are often used to model complex data. In this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them. Speciﬁcally, we introduce a set of latent Dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent DPs. Each mixture model may acquire atoms from different latent DPs, while each atom may be shared by multiple mixtures. This multi-to-multi association distinguishes the proposed method from previous ones that require the model structure to be a tree or a chain, allowing more ﬂexible designs. We also derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling. 1</p><p>4 0.56443012 <a title="47-lsi-4" href="./nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>Author: Zhihua Zhang, Bojun Tu</p><p>Abstract: In this paper we study sparsity-inducing nonconvex penalty functions using L´ vy e processes. We deﬁne such a penalty as the Laplace exponent of a subordinator. Accordingly, we propose a novel approach for the construction of sparsityinducing nonconvex penalties. Particularly, we show that the nonconvex logarithmic (LOG) and exponential (EXP) penalty functions are the Laplace exponents of Gamma and compound Poisson subordinators, respectively. Additionally, we explore the concave conjugate of nonconvex penalties. We ﬁnd that the LOG and EXP penalties are the concave conjugates of negative Kullback-Leiber (KL) distance functions. Furthermore, the relationship between these two penalties is due to asymmetricity of the KL distance. 1</p><p>5 0.55046266 <a title="47-lsi-5" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>6 0.5360418 <a title="47-lsi-6" href="./nips-2012-Fusion_with_Diffusion_for_Robust_Visual_Tracking.html">140 nips-2012-Fusion with Diffusion for Robust Visual Tracking</a></p>
<p>7 0.5018388 <a title="47-lsi-7" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>8 0.48120686 <a title="47-lsi-8" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>9 0.47216028 <a title="47-lsi-9" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>10 0.46603075 <a title="47-lsi-10" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>11 0.45980036 <a title="47-lsi-11" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>12 0.44848704 <a title="47-lsi-12" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>13 0.44048804 <a title="47-lsi-13" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>14 0.43761355 <a title="47-lsi-14" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>15 0.4343088 <a title="47-lsi-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.43373629 <a title="47-lsi-16" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>17 0.43175411 <a title="47-lsi-17" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>18 0.43146387 <a title="47-lsi-18" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>19 0.43024009 <a title="47-lsi-19" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>20 0.42258954 <a title="47-lsi-20" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.089), (21, 0.014), (38, 0.1), (39, 0.062), (42, 0.016), (53, 0.017), (54, 0.019), (55, 0.021), (63, 0.031), (73, 0.232), (74, 0.033), (76, 0.13), (80, 0.101), (92, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80739421 <a title="47-lda-1" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>Author: Mingyuan Zhou, Lawrence Carin</p><p>Abstract: By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efﬁcient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. 1</p><p>2 0.73879105 <a title="47-lda-2" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>Author: Sejong Yoon, Vladimir Pavlovic</p><p>Abstract: Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. 1</p><p>3 0.69526798 <a title="47-lda-3" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>Author: Vasiliy Karasev, Alessandro Chiuso, Stefano Soatto</p><p>Abstract: We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of “visual search” of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a “passive” agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an “omnipotent” agent, capable of inﬁnite control authority, can achieve arbitrarily good performance (asymptotically). In between these limiting cases, the tradeoff can be characterized empirically. 1</p><p>4 0.67895985 <a title="47-lda-4" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>5 0.66129649 <a title="47-lda-5" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><p>6 0.65818036 <a title="47-lda-6" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>7 0.65811336 <a title="47-lda-7" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>8 0.65717441 <a title="47-lda-8" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>9 0.65550137 <a title="47-lda-9" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>10 0.654001 <a title="47-lda-10" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>11 0.65376103 <a title="47-lda-11" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>12 0.65310097 <a title="47-lda-12" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>13 0.65276122 <a title="47-lda-13" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>14 0.65123135 <a title="47-lda-14" href="./nips-2012-Multiresolution_Gaussian_Processes.html">233 nips-2012-Multiresolution Gaussian Processes</a></p>
<p>15 0.65052873 <a title="47-lda-15" href="./nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">270 nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>16 0.64945525 <a title="47-lda-16" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>17 0.64882076 <a title="47-lda-17" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>18 0.64613205 <a title="47-lda-18" href="./nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</a></p>
<p>19 0.64562726 <a title="47-lda-19" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>20 0.64463264 <a title="47-lda-20" href="./nips-2012-Bayesian_models_for_Large-scale_Hierarchical_Classification.html">58 nips-2012-Bayesian models for Large-scale Hierarchical Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
