<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2012-FastEx: Hash Clustering with Exponential Families</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-126" href="#">nips2012-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2012-FastEx: Hash Clustering with Exponential Families</h1>
<br/><p>Source: <a title="nips-2012-126-pdf" href="http://papers.nips.cc/paper/4493-fastex-hash-clustering-with-exponential-families.pdf">pdf</a></p><p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>Reference: <a title="nips-2012-126-reference" href="../nips2012_reference/nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper we present a sampler, capable of estimating mixtures of exponential families. [sent-9, score-0.157]
</p><p>2 At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. [sent-10, score-0.36]
</p><p>3 1  Introduction  Fast clustering algorithms are a staple of exploratory data analysis. [sent-11, score-0.126]
</p><p>4 in large scale document analysis, or to provide a modicum of adaptivity to content personalization for a large basis of users [2, 3]. [sent-18, score-0.157]
</p><p>5 The latter is often ignored but vital to the rationale for using more data — after all, for ﬁxed model complexity there are rapidly diminishing returns afforded by extra data once a given threshold is exceeded. [sent-26, score-0.143]
</p><p>6 by means of multicore sampling and we need to draw from a large number of clusters. [sent-33, score-0.218]
</p><p>7 When sampling from many clusters, the time to compute the object likelihood with respect to all clusters dominates the inference procedure. [sent-34, score-0.476]
</p><p>8 For instance, for 1000 clusters and documents of 1000 words a naive sampler needs to perform 106 ﬂoating point operations. [sent-35, score-0.461]
</p><p>9 Given 10M documents this amounts to approximately 3 hours for a single Gibbs sampling iteration, which is clearly infeasible: sampling requires hundreds ∗  This work was carried out while AA, SR, SMN and AJS were with Yahoo Research. [sent-37, score-0.295]
</p><p>10 To alleviate this issue we use binary hashing to compute a fast proposal distribution. [sent-40, score-0.293]
</p><p>11 2  Mixtures of Exponential Families  Our models are mixtures of exponential families due to their ﬂexilibility. [sent-41, score-0.27]
</p><p>12 For convenience we focus on mixtures of multinomials with correspondingly conjugate Dirichlet distributions. [sent-43, score-0.287]
</p><p>13 In exponential families distributions over random variables are given by p(x; θ) = exp ( φ(x), θ − g(θ)) . [sent-54, score-0.208]
</p><p>14 Empirical averages and probability estimates are directly connected via p(x; θ) = nx /m = eθx . [sent-67, score-0.148]
</p><p>15 Here nx denotes the number of times we observe x. [sent-68, score-0.148]
</p><p>16 (4) Here the conjugate prior itself is a member of the exponential family with sufﬁcient statistic φ(θ) = (θ, −g(θ)) and with natural parameters (m0 , m0 µ0 ). [sent-75, score-0.228]
</p><p>17 Finally, h(m0 , m0 µ0 ) is a log-partition function in the parameters of the conjugate prior. [sent-79, score-0.133]
</p><p>18 Normalization in (4) implies p(θ|X) ∝ p(X|θ)p(θ|m0 , m0 µ0 ) =⇒ p(θ|X) = p (θ|m0 + m, m0 µ0 + mµ[X]) (5) We simply add the virtual observations m0 µ0 described by the conjugate prior to the actual observations X and compute the maximum likelihood estimate with respect to the augmented dataset. [sent-81, score-0.182]
</p><p>19 This yields the smoothed estimates for event probabilities in x: nx + m0 [µ0 ]x nx + m0 [µ0 ]x p(x; θ) = and equivalently θx = log . [sent-83, score-0.296]
</p><p>20 For the sake of brevity and to ensure computational tractability (we need to limit the time it takes to sample from the cluster distribution for a given instance) we limit ourselves to a Dirichlet-Multinomial model with k components: • Draw discrete mixture p(y|θ) with y ∈ {1, . [sent-87, score-0.22]
</p><p>21 0 0 • For each component k draw exponential families distribution p(x|θy ) from conjugate with parameters (mcomponent , µcomponent ). [sent-91, score-0.412]
</p><p>22 0 0 • For each i ﬁrst draw component yi ∼ p(y|θ), then draw observation xi ∼ p(x|θyi ). [sent-92, score-0.285]
</p><p>23 Note that we have two exponential families components here — a smoothed multinomial to capture cluster membership, i. [sent-93, score-0.474]
</p><p>24 y ∼ p(y|θ) and one pertaining to the cluster distribution. [sent-95, score-0.179]
</p><p>25 For large numbers, however, Gibbs sampling of the collapsed likelihood is computationally more advantageous since it only requires updates of the sufﬁcient statistics of two clusters per sample, whereas EM necessitates an update of all clusters. [sent-98, score-0.618]
</p><p>26 For a given xi draw yi ∼ p(yi |X, Y −i ) ∝ p(yi |Y ) · p(xi |yi , X −i , Y −i ). [sent-100, score-0.214]
</p><p>27 We now show how this step can be accelerated signiﬁcantly using a good proposal distribution, parallel sampling, and a Taylor expansion for general exponential families. [sent-104, score-0.369]
</p><p>28 1  Acceleration Taylor Approximation for Collapsed Inference  Let us brieﬂy review the key equations involved in collapsed inference. [sent-106, score-0.162]
</p><p>29 We exploit the properties of the log-partition function h of the conjugate prior for an approximation: ∂ h(m0 , µ0 m0 ) = (m0 ,m0 µ0 )  E  [−g(θ), θ] =: (−γ ∗ , θ∗ )  θ∼p(θ|m0 ,m0 µ0 )  hence h(m0 + 1, m0 µ0 + φ(x)) ≈ h(m0 , m0 µ0 ) + θ∗ , φ(x) − γ ∗ . [sent-117, score-0.133]
</p><p>30 Applying the Taylor expansion in h to (7) yields an approximation of x|X as p(x|X, m0 , m0 µ0 ) ≈ exp ( φ(x), θ∗ − g(θ∗ ))  (11)  Here the normalization g(θ∗ ) is an immediate consequence of the fact that this is a member of the exponential family. [sent-120, score-0.174]
</p><p>31 Lemma 1 The expected parameter θ∗ = Eθ∼p(θ|X) [θ] induces at most O(m−1 ) sampler error. [sent-123, score-0.151]
</p><p>32 Hence, (11) explains why updates obtained in collapsed inference often resemble (or are identical to) a maximum-a-posteriori estimate obtained by conjugate priors, such as in Dirichlet-multinomial smoothing. [sent-127, score-0.337]
</p><p>33 2  Locality Sensitive Importance Sampling  ∗ The next step is to accelerate the inner product φ(x), θy in (11) since this expression is evaluated k times at each Gibbs sampler step. [sent-130, score-0.263]
</p><p>34 This provides a good approximation and therefore a proposal distribution that can be used in a Metropolis-Hastings scheme without an excessive rejection rate. [sent-133, score-0.236]
</p><p>35 To provide some motivation consider metric-based clustering algorithms such as k-means. [sent-134, score-0.126]
</p><p>36 They do not suffer greatly from dealing with large numbers of clusters — after all, we only need to ﬁnd the closest prototype. [sent-135, score-0.287]
</p><p>37 In a nutshell it involves transforming the set of cluster centers into a data structure that is only dependent on the inherent dimensionality of the data rather than the number of objects or the dimensionality of the actual data vector. [sent-137, score-0.179]
</p><p>38 The problem with sampling from the collapsed distribution is that for a proper sampler we need to consider all cluster probabilities including those related to clusters which are highly implausible and unlikely to be chosen for a given instance. [sent-138, score-0.838]
</p><p>39 Instead, we design a sampler which typically will only explore the clusters which are sufﬁciently close to the “best” matching cluster by means of a proposal distribution. [sent-141, score-0.773]
</p><p>40 Since exponential families rely on inner products to determine the log-likelihood of how well the data ﬁts, we can use hashing to accelerate the expensive part considerably, namely comparing data with clusters. [sent-144, score-0.418]
</p><p>41 More speciﬁcally, u, v = u · v · cos (u, v) allows us to store the signature of a vector in terms of its signs and its norm to estimate the inner product efﬁciently. [sent-145, score-0.198]
</p><p>42 l  Deﬁnition 3 We denote by hl (v) ∈ {0, 1} a binary hash of v and by z l (u, v) an estimate of the probability of matching signs, obtained as follows hl (v)  i  := sgn [ v, wi ] where wi ∼ Um ﬁxed and z l (u, v) :=  4  1 h(u) − h(v) l  1  . [sent-146, score-0.465]
</p><p>43 (13)  That is, z l (u, v) measures how many bits differ between the hash vectors h(u) and h(v) associated with u, v. [sent-147, score-0.464]
</p><p>44 In this case we may estimate the unnormalized log-likelihood of an instance being assigned to a cluster via sl (x, y) = θy  φ(x) cos πz l (φ(x), θy ) − g(θy ) − log ny  (14)  We omitted the normalization log n of the cluster probability since it is identical for all components. [sent-148, score-0.531]
</p><p>45 The binary representation is signiﬁcant since on modern CPUs computing the Hamming distance between h(u) and h(v) via z l (u, v) can be achieved in a fraction of a single clock cycle by means of a vectorized instruction set. [sent-150, score-0.104]
</p><p>46 This is supported by current generation ARM and Intel CPU cores and by AMD and Nvidia GPUs (for instance Intel’s SandyBridge series of processors can process up to 256 bits in one clock cycle per core) and easily accessible via compiler optimization. [sent-151, score-0.268]
</p><p>47 3  Error Guarantees  Note, though, that sl (x, y) is not accurate, since we only use an estimate of the inner product. [sent-153, score-0.134]
</p><p>48 Theorem 4 Given k ∈ N mixture components and let l the number of bits used for hashing. [sent-156, score-0.182]
</p><p>49 Then the unnormalized cluster log-likelihood is bounded with probability at least 1 − δ by sl (x, y) = θy ¯  φ(x) cos π max 0, z l (φ(x), θy ) −  − g(θy ) − log ny (15)  (log k/δ) /2l  P ROOF. [sent-157, score-0.312]
</p><p>50 For convenience denote by z ∞ (φ(x), θy ) the expected value of z l (φ(x), θy ) over all hash functions. [sent-160, score-0.36]
</p><p>51 Since we know that z (φ(x), θy ) ≥ 0 we can bound it for all k clusters with probability δ by taking the union bound over all events with δ/k probability. [sent-162, score-0.248]
</p><p>52 Remark 5 Using 128 hash bits and with a failure probability of at most 10−4 for k = 104 clusters the correction applied to z l (x, z) is less than 0. [sent-163, score-0.712]
</p><p>53 Secondly, we use hashing to generate a proposal distribution: once we select a particular cluster we verify the estimate using the true likelihood. [sent-166, score-0.472]
</p><p>54 4  Metropolis Hastings  An alternative to using the approximate upper bound directly, we employ it as a proposal distribution in a Metropolis Hastings (MH) framework. [sent-168, score-0.195]
</p><p>55 Denote by q the proposal distribution constructed from the bound on the log-likelihood after normalization. [sent-169, score-0.195]
</p><p>56 For a given xi we ﬁrst sample a new cluster new assignment yi ∼ q(. [sent-170, score-0.322]
</p><p>57 ) and then accept the proposal using (15) with probability r where l  ¯ q(y) ∝ es (x,y) and r =  new i new q(y old ) p(yi )p(xi |Xyi , m0 , µ0 ) new ) old )p(x |X i q(yi p(yi , m0 , µ0 ) i y old  (17)  i  Here p(xi |X, m0 , µ0 ) is the true collapsed conditional likelihood of (8). [sent-171, score-0.658]
</p><p>58 Note that for a standard collapsed Gibbs sampler, p(x|X, µ0 , m0 ) would be computed for all k candidate clusters, however, in our framework, we only need to compute it for 2 clusters: the proposed and old clusters: an O(k) time saving per sample, albeit with a nontrivial rejection probability. [sent-175, score-0.336]
</p><p>59 5  Example 3 For discrete distributions the conjugate is the Dirichlet distribution Dir(α1:d ) with components given by αj = m0 [µ0 ]j and the sum of the components is given by m0 , where j ∈ {1 · · · d}. [sent-176, score-0.133]
</p><p>60 1 We have the following predictive posterior p(xi |X, yi , µ0 , m0 ) =  3. [sent-180, score-0.101]
</p><p>61 D yi Γ nyi + αd d d=1 [xd + nd + αd ] d=1 [nyi + αd ] d  (18)  Updating the Sufﬁcient Statistics  We conclude our discussion of past proposals by discussing the updates involved in the sufﬁcient statistics. [sent-182, score-0.22]
</p><p>62 it is the sufﬁcient statistic obtained from X by considering only instances for which yi = y. [sent-188, score-0.101]
</p><p>63 This is then used to update the natural parameter θy and the hash representation h(θy ). [sent-190, score-0.384]
</p><p>64 For l bits a naive update would perform the dot-product between the mean natural parameters and each random vector, which scales as O(Dl), where D is the vocabulary size. [sent-196, score-0.251]
</p><p>65 However we can cache the l dot product values (as ﬂoating point numbers) for each cluster and update only those dot product values. [sent-197, score-0.24]
</p><p>66 Note that we never need to store the random vectors w since we can compute them on the ﬂy by means of hash functions rather than invoking a random number generator. [sent-199, score-0.361]
</p><p>67 We use murmurhash as a fast and high quality hash function. [sent-200, score-0.38]
</p><p>68 More speciﬁcally, we extracted the articles and category attributes from a dump of its database. [sent-203, score-0.113]
</p><p>69 We generated multiple datasets for our experiments by ﬁrst sampling a set of k categories and then by pooling all the articles from the chosen categories to form a document collection. [sent-204, score-0.382]
</p><p>70 This way the data was comparable and the apparent and desired diversity in terms of cluster sizes was matched. [sent-205, score-0.179]
</p><p>71 We extracted both 100 and 1000 categories, yielding the following datasets: W100 100 clusters 292k articles 2. [sent-206, score-0.361]
</p><p>72 5M unique words vocabulary W1000 1000 clusters 710k articles 5. [sent-207, score-0.41]
</p><p>73 6M unique words vocabulary We compare our fast sampler to a more conventional uncollapsed inference procedure. [sent-208, score-0.31]
</p><p>74 It uses an uncollapsed likelihood and alternates between sampling cluster assignments and drawing from the Dirichlet distribution of the posterior. [sent-210, score-0.394]
</p><p>75 (Right) The effect of the hash size on performance. [sent-216, score-0.323]
</p><p>76 Unless stated otherwise we use l = 32 bit to represent a document and cluster. [sent-219, score-0.207]
</p><p>77 This choice was made since it provides an efﬁcient trade-off between memory usage and cost to compute the hash signatures. [sent-220, score-0.323]
</p><p>78 2  Evaluation  For each clustering method, we report results in terms of two different measures: efﬁciency and clustering quality. [sent-222, score-0.252]
</p><p>79 Suppose we have two clusterings (partition of a document set into several subsets) C1 and C2 then: VI(C1 , C2 ) = H(C1 ) + H(C2 ) − 2I(C1 , C2 )  (19)  where H(. [sent-227, score-0.138]
</p><p>80 As evident from this Figure, our method both converges much faster than the baseline and achieves the same clustering quality. [sent-233, score-0.236]
</p><p>81 Figure 1 also displays the effect of the number of hash bits l on solution quality. [sent-234, score-0.464]
</p><p>82 As evident form the ﬁgure, increasing the number of bits caused our method to converge faster due to a tighter bound on the log-likelihood and thus a higher acceptance ratio. [sent-236, score-0.217]
</p><p>83 We also observe that beyond 64 to 128 bits we do not observe any noticeable improvement as predicted by our theoretical guarantees. [sent-237, score-0.141]
</p><p>84 As shown in this Table, thanks to the fast instruction set support for XOR and bitcount operations on modern processors, the time does not increase signiﬁcantly as we increase the number of clusters and the overall time increases modestly as the number of clusters increases. [sent-239, score-0.556]
</p><p>85 61  Table 1: Average time in microseconds spent per document for hash sampling in terms of computing the proposal distribution and total computation time. [sent-261, score-0.713]
</p><p>86 As can be seen, the total computation time for sampling 10x more clusters only increases slightly, mostly due to the increase in proposal time. [sent-262, score-0.541]
</p><p>87 37  Table 2: Clustering quality (VI) and absolute speedup achieved by hash sampling over the baseline (DP) clustering for different Wikipedia datasets. [sent-269, score-0.676]
</p><p>88 Due to a high quality proposals the time to draw from 1000 rather than 100 clusters increases slightly. [sent-271, score-0.429]
</p><p>89 5  Discussion and Future Work  We presented a new efﬁcient parallel algorithm to perform scalable clustering for exponential families. [sent-272, score-0.307]
</p><p>90 It is general and uses techniques from hashing and information retrieval to circumvent the problem of large numbers of clusters. [sent-273, score-0.174]
</p><p>91 Future work includes the application to a larger range of exponential family models and the extension of the fast retrieval scheme to hierarchical clustering. [sent-274, score-0.132]
</p><p>92 Parallelization So far we only described a single processor sampling procedure. [sent-275, score-0.141]
</p><p>93 To address the problem within single machines we use a multicore sampler to parallelize inference. [sent-277, score-0.246]
</p><p>94 This requires a small amount of approximation — rather than sampling p(yi |xi , X −i , Y −i ) in sequence we sample up to c latent variables yi in parallel in c processor cores. [sent-278, score-0.282]
</p><p>95 The latter approximation is negligible since c is tiny compared to the total number of documents we have. [sent-279, score-0.109]
</p><p>96 updater  writer  disk  sampler n A key advantage is that all samplers share the same sufﬁcient statistics regardless of the number of cores used. [sent-285, score-0.313]
</p><p>97 By delegating write permissions to a separate updater thread the code is considerably simpliﬁed. [sent-286, score-0.107]
</p><p>98 Sequential Estimation Our approach is compatible with sequential estimation methods and it is possible to use hash signatures for Sequential Monte Carlo estimation for clustering as in [21, 22]. [sent-290, score-0.449]
</p><p>99 Again, sampling is the dominant cost for inference and it can be accelerated by binary hashing. [sent-293, score-0.14]
</p><p>100 Correlation clustering — minimizing disagreements on arbitrary weighted graphs. [sent-318, score-0.126]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hash', 0.323), ('fastex', 0.306), ('clusters', 0.248), ('proposal', 0.195), ('cluster', 0.179), ('collapsed', 0.162), ('sampler', 0.151), ('nx', 0.148), ('bits', 0.141), ('conjugate', 0.133), ('clustering', 0.126), ('families', 0.113), ('articles', 0.113), ('bit', 0.11), ('yi', 0.101), ('hashing', 0.098), ('dirichlet', 0.098), ('sampling', 0.098), ('document', 0.097), ('exponential', 0.095), ('ahmed', 0.091), ('multinomial', 0.087), ('vi', 0.086), ('old', 0.084), ('sl', 0.081), ('gibbs', 0.073), ('baseline', 0.072), ('draw', 0.071), ('narayanamurthy', 0.068), ('uncollapsed', 0.068), ('updater', 0.068), ('ex', 0.067), ('nyi', 0.066), ('mountain', 0.064), ('wikipedia', 0.064), ('smola', 0.063), ('mixtures', 0.062), ('sgn', 0.062), ('documents', 0.062), ('update', 0.061), ('personalization', 0.06), ('instruction', 0.06), ('accelerate', 0.059), ('suf', 0.058), ('quality', 0.057), ('taylor', 0.055), ('disk', 0.055), ('eisenstein', 0.055), ('multinomials', 0.055), ('oating', 0.055), ('signs', 0.055), ('proposals', 0.053), ('inner', 0.053), ('google', 0.052), ('cos', 0.052), ('chernoff', 0.049), ('multicore', 0.049), ('vital', 0.049), ('likelihood', 0.049), ('vocabulary', 0.049), ('nontrivial', 0.049), ('eh', 0.047), ('hastings', 0.047), ('afforded', 0.047), ('latter', 0.047), ('scalable', 0.046), ('parallelize', 0.046), ('nishes', 0.046), ('vldb', 0.044), ('metropolis', 0.044), ('processors', 0.044), ('clock', 0.044), ('processor', 0.043), ('streaming', 0.043), ('inference', 0.042), ('xi', 0.042), ('rejection', 0.041), ('clusterings', 0.041), ('mixture', 0.041), ('ca', 0.04), ('hl', 0.04), ('gold', 0.04), ('ho', 0.04), ('parallel', 0.04), ('normalization', 0.04), ('dominates', 0.039), ('cores', 0.039), ('considerably', 0.039), ('numbers', 0.039), ('expansion', 0.039), ('store', 0.038), ('poisson', 0.038), ('acceptance', 0.038), ('evident', 0.038), ('amounts', 0.037), ('convenience', 0.037), ('web', 0.037), ('categories', 0.037), ('gaussians', 0.037), ('retrieval', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="126-tfidf-1" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>2 0.27225909 <a title="126-tfidf-2" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>3 0.20394634 <a title="126-tfidf-3" href="./nips-2012-Sketch-Based_Linear_Value_Function_Approximation.html">313 nips-2012-Sketch-Based Linear Value Function Approximation</a></p>
<p>Author: Marc Bellemare, Joel Veness, Michael Bowling</p><p>Abstract: Hashing is a common method to reduce large, potentially inﬁnite feature vectors to a ﬁxed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and ﬁfty-ﬁve Atari 2600 games to highlight the superior learning performance obtained when using tug-of-war hashing. 1</p><p>4 0.18809439 <a title="126-tfidf-4" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>Author: Mohammad Norouzi, David M. Blei, Ruslan Salakhutdinov</p><p>Abstract: Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efﬁcient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a ﬂexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST, with promising classiﬁcation results using no more than kNN on the binary codes. 1</p><p>5 0.18525936 <a title="126-tfidf-5" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>Author: Yudong Chen, Sujay Sanghavi, Huan Xu</p><p>Abstract: We develop a new algorithm to cluster sparse unweighted graphs – i.e. partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters. By sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph. Sparsity makes the problem noisier, and hence more difﬁcult to solve. Any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters. Our insight is that in the sparse case, these must be penalized differently. We analyze our algorithm’s performance on the natural, classical and widely studied “planted partition” model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods. This is seen empirically as well. 1</p><p>6 0.16055125 <a title="126-tfidf-6" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>7 0.15430732 <a title="126-tfidf-7" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>8 0.14766328 <a title="126-tfidf-8" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>9 0.13284796 <a title="126-tfidf-9" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>10 0.12886012 <a title="126-tfidf-10" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>11 0.11912447 <a title="126-tfidf-11" href="./nips-2012-Fast_Variational_Inference_in_the_Conjugate_Exponential_Family.html">129 nips-2012-Fast Variational Inference in the Conjugate Exponential Family</a></p>
<p>12 0.10773414 <a title="126-tfidf-12" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>13 0.10561857 <a title="126-tfidf-13" href="./nips-2012-A_nonparametric_variable_clustering_model.html">26 nips-2012-A nonparametric variable clustering model</a></p>
<p>14 0.10493563 <a title="126-tfidf-14" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>15 0.10404634 <a title="126-tfidf-15" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>16 0.10268874 <a title="126-tfidf-16" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>17 0.10265872 <a title="126-tfidf-17" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>18 0.10264323 <a title="126-tfidf-18" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>19 0.10241821 <a title="126-tfidf-19" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>20 0.10201128 <a title="126-tfidf-20" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.284), (1, 0.083), (2, -0.016), (3, -0.05), (4, -0.198), (5, -0.123), (6, -0.027), (7, 0.039), (8, 0.186), (9, 0.064), (10, 0.21), (11, -0.192), (12, -0.01), (13, 0.062), (14, -0.054), (15, -0.05), (16, -0.092), (17, -0.076), (18, -0.186), (19, -0.085), (20, -0.043), (21, 0.038), (22, -0.01), (23, 0.009), (24, -0.061), (25, -0.049), (26, -0.011), (27, -0.011), (28, -0.002), (29, -0.018), (30, 0.011), (31, -0.057), (32, -0.0), (33, 0.049), (34, -0.034), (35, 0.022), (36, -0.004), (37, 0.012), (38, -0.006), (39, -0.032), (40, 0.1), (41, -0.037), (42, -0.006), (43, 0.072), (44, -0.04), (45, -0.085), (46, 0.071), (47, 0.038), (48, 0.058), (49, -0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94139355 <a title="126-lsi-1" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>2 0.7746383 <a title="126-lsi-2" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>3 0.69345421 <a title="126-lsi-3" href="./nips-2012-A_nonparametric_variable_clustering_model.html">26 nips-2012-A nonparametric variable clustering model</a></p>
<p>Author: Konstantina Palla, Zoubin Ghahramani, David A. Knowles</p><p>Abstract: Factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret. This motivates attempting to ﬁnd a disjoint partition, i.e. a simple clustering, of observed variables into highly correlated subsets. We introduce a Bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date. Our Dirichlet process variable clustering (DPVC) model can discover blockdiagonal covariance structures in data. We evaluate our method on both synthetic and gene expression analysis problems. 1</p><p>4 0.68469512 <a title="126-lsi-4" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>Author: Argyris Kalogeratos, Aristidis Likas</p><p>Abstract: Learning the number of clusters is a key problem in data clustering. We present dip-means, a novel robust incremental method to learn the number of data clusters that can be used as a wrapper around any iterative clustering algorithm of k-means family. In contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution. The proposed algorithm considers each cluster member as an individual ‘viewer’ and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of distances between the viewer and the cluster members. Important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations. Experimental results on artiﬁcial and real datasets indicate the eﬀectiveness of our method and its superiority over analogous approaches.</p><p>5 0.65269679 <a title="126-lsi-5" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>Author: Deepak Venugopal, Vibhav Gogate</p><p>Abstract: First-order probabilistic models combine the power of ﬁrst-order logic, the de facto tool for handling relational structure, with probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the accuracy and scalability of existing graphical models’ inference algorithms by exploiting symmetry in the ﬁrst-order representation. In this paper, we consider blocked Gibbs sampling, an advanced MCMC scheme, and lift it to the ﬁrst-order level. We propose to achieve this by partitioning the ﬁrst-order atoms in the model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing the clusters and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy, scalability and convergence.</p><p>6 0.61908799 <a title="126-lsi-6" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>7 0.59355885 <a title="126-lsi-7" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>8 0.5910145 <a title="126-lsi-8" href="./nips-2012-Sketch-Based_Linear_Value_Function_Approximation.html">313 nips-2012-Sketch-Based Linear Value Function Approximation</a></p>
<p>9 0.58846182 <a title="126-lsi-9" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>10 0.58677155 <a title="126-lsi-10" href="./nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">299 nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>11 0.55532688 <a title="126-lsi-11" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>12 0.55485636 <a title="126-lsi-12" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>13 0.55408108 <a title="126-lsi-13" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>14 0.53964442 <a title="126-lsi-14" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>15 0.52774119 <a title="126-lsi-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.52722013 <a title="126-lsi-16" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>17 0.52274966 <a title="126-lsi-17" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>18 0.52001059 <a title="126-lsi-18" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>19 0.50395769 <a title="126-lsi-19" href="./nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</a></p>
<p>20 0.50006109 <a title="126-lsi-20" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.066), (17, 0.022), (21, 0.026), (38, 0.123), (39, 0.015), (42, 0.025), (54, 0.023), (55, 0.026), (68, 0.2), (74, 0.058), (76, 0.186), (80, 0.103), (92, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87268221 <a title="126-lda-1" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>2 0.83841848 <a title="126-lda-2" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>Author: Mert Pilanci, Laurent E. Ghaoui, Venkat Chandrasekaran</p><p>Abstract: We consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints. The classical 1 regularizer fails to promote sparsity on the probability simplex since 1 norm on the probability simplex is trivially constant. We propose a direct relaxation of the minimum cardinality problem and show that it can be efﬁciently solved using convex programming. As a ﬁrst application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efﬁciently. A sufﬁcient condition for exact recovery of the minimum cardinality solution is derived for arbitrary afﬁne constraints. We then develop a penalized version for the noisy setting which can be solved using second order cone programs. The proposed method outperforms known rescaling heuristics based on 1 norm. As a second application we consider convex clustering using a sparse Gaussian mixture and compare our results with the well known soft k-means algorithm. 1</p><p>3 0.79286134 <a title="126-lda-3" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><p>4 0.78706288 <a title="126-lda-4" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>5 0.78666586 <a title="126-lda-5" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>Author: Ke Jiang, Brian Kulis, Michael I. Jordan</p><p>Abstract: Sampling and variational inference techniques are two standard methods for inference in probabilistic models, but for many problems, neither approach scales effectively to large-scale data. An alternative is to relax the probabilistic model into a non-probabilistic formulation which has a scalable associated algorithm. This can often be fulﬁlled by performing small-variance asymptotics, i.e., letting the variance of particular distributions in the model go to zero. For instance, in the context of clustering, such an approach yields connections between the kmeans and EM algorithms. In this paper, we explore small-variance asymptotics for exponential family Dirichlet process (DP) and hierarchical Dirichlet process (HDP) mixture models. Utilizing connections between exponential family distributions and Bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the DP and HDP mixtures that features the scalability of existing hard clustering methods as well as the ﬂexibility of Bayesian nonparametric models. We focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis. 1</p><p>6 0.78648627 <a title="126-lda-6" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>7 0.78595519 <a title="126-lda-7" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>8 0.78528386 <a title="126-lda-8" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>9 0.78500217 <a title="126-lda-9" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>10 0.78458357 <a title="126-lda-10" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>11 0.78450394 <a title="126-lda-11" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>12 0.78404337 <a title="126-lda-12" href="./nips-2012-Spectral_learning_of_linear_dynamics_from_generalised-linear_observations_with_application_to_neural_population_data.html">321 nips-2012-Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</a></p>
<p>13 0.78358382 <a title="126-lda-13" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>14 0.78321517 <a title="126-lda-14" href="./nips-2012-Bayesian_models_for_Large-scale_Hierarchical_Classification.html">58 nips-2012-Bayesian models for Large-scale Hierarchical Classification</a></p>
<p>15 0.78308994 <a title="126-lda-15" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>16 0.78189176 <a title="126-lda-16" href="./nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">232 nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>17 0.78127325 <a title="126-lda-17" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>18 0.78126073 <a title="126-lda-18" href="./nips-2012-Practical_Bayesian_Optimization_of_Machine_Learning_Algorithms.html">272 nips-2012-Practical Bayesian Optimization of Machine Learning Algorithms</a></p>
<p>19 0.78064567 <a title="126-lda-19" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>20 0.78059518 <a title="126-lda-20" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
