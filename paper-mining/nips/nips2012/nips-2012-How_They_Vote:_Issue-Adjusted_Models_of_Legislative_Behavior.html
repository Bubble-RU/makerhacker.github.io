<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-154" href="#">nips2012-154</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</h1>
<br/><p>Source: <a title="nips-2012-154-pdf" href="http://papers.nips.cc/paper/4715-how-they-vote-issue-adjusted-models-of-legislative-behavior.pdf">pdf</a></p><p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>Reference: <a title="nips-2012-154-reference" href="../nips2012_reference/nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. [sent-7, score-0.642]
</p><p>2 1  Introduction  Legislative behavior centers around the votes made by lawmakers. [sent-11, score-0.333]
</p><p>3 Capturing regularity in these votes, and characterizing patterns of legislative behavior, is one of the main goals of quantitative political science. [sent-12, score-0.328]
</p><p>4 Voting behavior exhibits enough regularity that simple statistical models, particularly ideal point models, easily capture the broad political structure of legislative bodies. [sent-13, score-0.78]
</p><p>5 However, some lawmakers do not ﬁt neatly into the assumptions made by these models. [sent-14, score-0.464]
</p><p>6 In this paper, we develop a new model of legislative behavior that captures when and how lawmakers vote differently than expected. [sent-15, score-0.762]
</p><p>7 Ideal point models assume that lawmakers and bills are represented as points in a latent space. [sent-16, score-0.823]
</p><p>8 Given the data of how each lawmaker votes on each bill (known as a roll call), we can use ideal point models to infer the latent position of each lawmaker. [sent-18, score-1.281]
</p><p>9 politics, these inferred positions reveal the commonly-known political spectrum: right-wing lawmakers are at one extreme, and left-wing lawmakers are at the other. [sent-21, score-1.056]
</p><p>10 Figure 1 illustrates example inferences from an ideal point model. [sent-22, score-0.424]
</p><p>11 But there are some votes that ideal point models fail to capture. [sent-23, score-0.729]
</p><p>12 For example, Ronald Paul, Republican representative from Texas, and Dennis Kucinich, Democratic representative from Ohio, are poorly modeled by ideal points because they diverge from the left-right spectrum on issues like foreign policy. [sent-24, score-0.547]
</p><p>13 Because some lawmakers deviate from their party on certain issues, their positions on these issues are not captured by ideal point models. [sent-25, score-1.044]
</p><p>14 To this end, we develop the issue-adjusted ideal point model, a latent variable model of roll-call data that accounts for the contents of the bills that lawmakers are voting on. [sent-26, score-1.291]
</p><p>15 The votes on a bill depend on a lawmaker’s position, adjusted for the bill’s content. [sent-28, score-0.554]
</p><p>16 The text of the bill encodes the issues it discusses. [sent-29, score-0.365]
</p><p>17 1  −2  −1  0  1  2  3  Figure 1: Traditional ideal points separate Republicans (red) from Democrats (blue). [sent-31, score-0.409]
</p><p>18 exceptional voting patterns of individual legislators, and it provides a richer description of lawmakers’ voting behavior than the models traditionally used in political science. [sent-32, score-0.415]
</p><p>19 We show that our model gives a better ﬁt to legislative data and provides an interesting exploratory tool for analyzing legislative behavior. [sent-35, score-0.426]
</p><p>20 2  Exceptional issue voting  We ﬁrst review ideal point models of legislative roll call data and discuss their limitations. [sent-36, score-0.898]
</p><p>21 Applied to voting records, one-dimensional ideal point models place lawmakers on an interpretable political spectrum. [sent-40, score-1.129]
</p><p>22 One-dimensional ideal point models posit an ideal point xu ∈ R for each lawmaker u. [sent-42, score-1.151]
</p><p>23 Each bill d is characterized by its polarity ad and its popularity bd . [sent-43, score-0.531]
</p><p>24 1 The probability that lawmaker u votes “Yes” on bill d is given by the logistic regression p(vud = yes | xu , ad , bd ) = σ(xu ad + bd ),  (1)  exp(s) where σ(s) = 1+exp(s) is the logistic function. [sent-44, score-1.329]
</p><p>25 2 When the popularity of a bill bd is high, nearly everyone votes “Yes”; when the popularity is low, nearly everyone votes “No”. [sent-45, score-1.027]
</p><p>26 When the popularity is near zero, the probability that a lawmaker votes “Yes” depends on how her ideal point xu interacts with bill polarity ad . [sent-46, score-1.457]
</p><p>27 Given a matrix of votes, we can infer the ideal point of each lawmaker. [sent-48, score-0.424]
</p><p>28 The model has clearly separated lawmakers by their political party (colour) and provides an intuitive measure of their political leanings. [sent-52, score-0.761]
</p><p>29 The ideal point model assumes that lawmakers are ordered. [sent-59, score-0.888]
</p><p>30 Lawmakers to one side of the cut point are more likely to support the bill, and lawmakers to d the other side are likely to reject it. [sent-61, score-0.506]
</p><p>31 For lawmakers like Paul and Hill, this assumption is too strong because their voting behavior does not ﬁt neatly into a single ordering. [sent-62, score-0.605]
</p><p>32 Their location among the other lawmakers changes with different bills. [sent-63, score-0.464]
</p><p>33 Paul consistently votes against United States involvement in foreign military engagements, a position that contrasts with other Republicans. [sent-67, score-0.384]
</p><p>34 An issue is any federal policy area, such as “ﬁnancial regulation,” “foreign policy,” “civil liberties,” or “education,” on which lawmakers are expected to take positions. [sent-69, score-0.57]
</p><p>35 Many ideal point models use a probit function instead [1, 3]. [sent-73, score-0.424]
</p><p>36 In the issue-adjusted ideal point model, lawmakers’ ideal points change when they vote on certain issues, such as Taxation. [sent-75, score-0.903]
</p><p>37 Right: the issue-adjusted ideal point model, which models votes vud from lawmakers and legislative items. [sent-77, score-1.484]
</p><p>38 Classic item response theory models votes v using xu and ad , bd . [sent-78, score-0.633]
</p><p>39 The model we will introduce uses lawmakers’ votes and the text of bills to model deviations like this, on a variety of issues. [sent-82, score-0.619]
</p><p>40 We now describe the issue-adjusted ideal point model, a new model of lawmaker behavior that takes into account both the content of the bills and the voting patterns of the lawmakers. [sent-85, score-1.07]
</p><p>41 We build on the ideal point model so that each lawmaker’s ideal point can be adjusted for each issue. [sent-86, score-0.848]
</p><p>42 ) In our proposed model, each lawmaker is also associated with a K-vector zu ∈ RK , which describes how her ideal point changes for bills about each issue. [sent-90, score-1.049]
</p><p>43 We use these variables in a model based on the traditional ideal point model of Equation 1. [sent-91, score-0.45]
</p><p>44 As above, xu is the ideal point for lawmaker u and ad , bd are the polarity and popularity of bill d. [sent-92, score-1.258]
</p><p>45 In our model, votes are modeled with a logistic regression p(vud |ad , bd , zu , xu , wd ) = σ (zu Eq [θd |wd ] + xu )ad + bd ,  (2)  where we use an estimate Eq [θd |wd ] of the bill’s issue vector from its words wd as described below. [sent-93, score-1.041]
</p><p>46 We put standard normal priors on the ideal points, polarity, and difﬁculty variables. [sent-94, score-0.382]
</p><p>47 With a classic ideal point model, a lawmaker u’s ideal point, xu , gives his position on each issue, including Finance. [sent-100, score-1.172]
</p><p>48 With the issue-adjusted ideal point model, his effective ideal point for Finance, xu + zu,Finance , gives his position on Finance. [sent-101, score-0.969]
</p><p>49 When zu,k = 0 for all u, k, the model becomes the classic ideal point model. [sent-103, score-0.454]
</p><p>50 Given a collection of votes and a coding of bills to issues, posterior estimates of the ideal points and per-issue adjustments give us a window into voting behavior that is not available to classic ideal point models. [sent-105, score-1.716]
</p><p>51 Equation 2 adjusts a lawmaker’s ideal point by using the conditional expectation of a bill’s thematic labels θd given its words wd . [sent-107, score-0.485]
</p><p>52 Labeled LDA is a topic model, a bag-of-words model that assumes a set of themes for the collection of bills and that each bill exhibits a mixture of those themes. [sent-109, score-0.598]
</p><p>53 These subject codes describe the bills using phrases which correspond to traditional issues, such as Civil rights and National security. [sent-116, score-0.361]
</p><p>54 In the issueadjusted ideal point model (Equation 2), E [θd ] was treated as observed when estimating the posterior distribution p(xu , ad , bd , zd |E [θd |wd ] , vud ). [sent-122, score-0.779]
</p><p>55 Some political scientists have used higher-dimensional ideal points, where each legislator is attached to a vector of ideal points xu ∈ RK and each bill polarization ad takes the same dimension K [11]. [sent-127, score-1.364]
</p><p>56 The probability of a lawmaker voting “Yes” is σ(xT ad + bd ). [sent-128, score-0.542]
</p><p>57 The principal component of u ideal points explains most of the variance and explains party afﬁliation. [sent-129, score-0.45]
</p><p>58 Congressional ﬂoor debates to predict whether speeches support or oppose pending legislation [12] and predicting whether a bill will survive congressional committee by incorporating a number of features, including bill text [13]. [sent-134, score-0.647]
</p><p>59 Gerrish and Blei aimed to predict votes on bills which had not yet received any votes [14]. [sent-136, score-0.9]
</p><p>60 Their model ﬁts ad and bd using supervised topics, but the underlying voting model was one-dimensional: it could not model individual votes better than a one-dimensional ideal point model. [sent-137, score-1.056]
</p><p>61 created a Bayesian nonparametric model of votes and text over time [15]. [sent-139, score-0.329]
</p><p>62 4  4  3  Posterior estimation  The central computational challenge in this model is to uncover lawmakers’ issue preferences zu by using the their votes v and bills’ issues θd . [sent-149, score-0.623]
</p><p>63 Bayesian ideal point models are usually ﬁt with Gibbs sampling [2, 3, 5, 18]. [sent-151, score-0.424]
</p><p>64 The similarity between ideal points ﬁt with variational inference and MCMC has been demonstrated in Gerrish in Blei [14]. [sent-159, score-0.449]
</p><p>65 We compare the posterior ﬁt with this model to the same data ﬁt with traditional ideal points and validate the model quantitatively. [sent-170, score-0.462]
</p><p>66 For each congress, we considered only bills for which votes were explicitly recorded in a roll-call. [sent-187, score-0.595]
</p><p>67 We ignored votes on bills for which text was unavailable. [sent-188, score-0.619]
</p><p>68 5  Table 1: Average log-likelihood of heldout votes using six-fold cross validation. [sent-196, score-0.366]
</p><p>69 The issue-adjusted model yields higher heldout log-likelihood for all congresses in both chambers than a standard ideal point model. [sent-198, score-0.553]
</p><p>70 187  Comparison of classic and exploratory ideal points  How do classic ideal points compare with issue-adjusted ideal points? [sent-242, score-1.286]
</p><p>71 We ﬁt classic ideal points to the 111th House (2009 to 2010) to compare them with issue-adjusted ideal points xu from the ˜ same period, using regularization λ = 1. [sent-243, score-0.936]
</p><p>72 The models’ ideal points xu were very similar, correlated ˜ at 0. [sent-244, score-0.497]
</p><p>73 While traditional ideal points cleanly separate Democrats and Republicans in this period, issue-adjusted ideal points provide an even cleaner break between the parties. [sent-246, score-0.844]
</p><p>74 For each fold, we computed the average predictive log-likelihood log p(vudTest |vudTrain ) = log p(vudTest |˜u , zu , ad , ˜d , Eq [θd |w]) of the test x ˜ ˜ b votes and averaged this across folds. [sent-253, score-0.533]
</p><p>75 We compared these with the ideal point model, evaluating the latter in the same way. [sent-254, score-0.424]
</p><p>76 Note that we cannot evaluate how well this model predicts votes on a heldout bill d. [sent-257, score-0.615]
</p><p>77 As with the ideal point model, our model cannot predict ad , ˜d without votes on d. [sent-258, score-0.837]
</p><p>78 We compared the issue-adjusted model’s ability to represent heldout votes with the ideal point model. [sent-262, score-0.79]
</p><p>79 For comparison we also ﬁt an ideal point model to each of these congresses. [sent-264, score-0.424]
</p><p>80 In all Congresses and both chambers, the issue-adjusted model represents heldout votes with higher log-likelihood than an ideal point model. [sent-265, score-0.79]
</p><p>81 We used a permutation test to understand how the issue-adjusted model improves upon ideal point models. [sent-276, score-0.424]
</p><p>82 4  Analyzing issues, lawmakers, and bills  In this section we take a closer look at how issue adjustments improve on ideal points and demonstrate how the issue-adjusted ideal point model can be used to analyze speciﬁc lawmakers. [sent-296, score-1.319]
</p><p>83 We can measure the improvement by comparing the training likelihoods of votes in the issue-adjusted and traditional ideal point models. [sent-298, score-0.787]
</p><p>84 The training log-likelihood of each vote is Jud = 1{vud =Yes } p − log(1 + exp(p)), (6) T ˜d is the log-odds of a vote under the issue adjusted voting where p = (˜u + zu Eq [θd |w])˜d + b x ˜ a model. [sent-299, score-0.479]
</p><p>85 The corresponding log-likelihood Iud under the ideal point model is p = xu ad + ˜d . [sent-300, score-0.62]
</p><p>86 This is a result predicted by procedural cartel theory [23, 24, 25, 26], which posits that lawmakers will be more polarized in procedural votes (which describe how Congress will be run) than substantive votes (the issues discussed during elections). [sent-306, score-1.264]
</p><p>87 Some of most-improved lawmakers were Ron Paul and Donald Young. [sent-314, score-0.464]
</p><p>88 9% with issue offsets, placing him among the two most-improved lawmakers with this model. [sent-326, score-0.57]
</p><p>89 Young stood out in a topic used frequently in House bills about naming local landmarks. [sent-333, score-0.326]
</p><p>90 Young voted against the majority of his party (and the House in general) on a series of largely symbolic bills and resolutions. [sent-334, score-0.381]
</p><p>91 ” Young’s divergent voting was also evident in a series of votes against naming various landmarks–such as post ofﬁces–in a topic about such symbolic votes. [sent-336, score-0.454]
</p><p>92 Notice that Young’s ideal point is not particularly distinctive: using the ideal point alone, we would not recognize his unique voting behavior. [sent-337, score-0.961]
</p><p>93 The bill which decreased the most from the ideal point model in the 111th House was the Consolidated Land, Energy, and Aquatic Resources Act of 2010 (H. [sent-341, score-0.673]
</p><p>94 This bill had substantial weight in ﬁve issues, with most in Public lands and natural resources, Energy, and Land transfers, but its placement in many issues harmed our predictions. [sent-344, score-0.364]
</p><p>95 This effect—worse performance on bills about many issues—suggests that methods which represent bills more sparsely may perform better than the current model. [sent-345, score-0.58]
</p><p>96 5  Discussion  Traditional models of roll call data cannot capture how individual lawmakers deviate from their latent position on the political spectrum. [sent-346, score-0.703]
</p><p>97 In this paper, we developed a model that captures how lawmakers vary, issue by issue, and used the text of the bills to attach speciﬁc votes to speciﬁc issues. [sent-347, score-1.189]
</p><p>98 We demonstrated, across 12 years of legislative data, that this model better captures lawmaker behavior. [sent-348, score-0.415]
</p><p>99 For example, lawmakers make some (but not all) issue positions public. [sent-351, score-0.57]
</p><p>100 Dynamic ideal point estimation via Markov chain Monte Carlo for the U. [sent-375, score-0.424]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lawmakers', 0.464), ('ideal', 0.382), ('votes', 0.305), ('bills', 0.29), ('bill', 0.249), ('lawmaker', 0.215), ('legislative', 0.2), ('house', 0.158), ('political', 0.128), ('congressional', 0.125), ('zu', 0.12), ('voting', 0.113), ('ad', 0.108), ('issue', 0.106), ('bd', 0.106), ('issues', 0.092), ('vud', 0.091), ('adjustments', 0.09), ('xu', 0.088), ('congress', 0.074), ('vote', 0.07), ('congresses', 0.068), ('paul', 0.061), ('wd', 0.061), ('heldout', 0.061), ('zuk', 0.06), ('young', 0.058), ('republicans', 0.057), ('senate', 0.057), ('roll', 0.055), ('voted', 0.05), ('eq', 0.049), ('foreign', 0.046), ('democrats', 0.045), ('ethnic', 0.045), ('impk', 0.045), ('iud', 0.045), ('jud', 0.045), ('racial', 0.045), ('rights', 0.045), ('yes', 0.044), ('gerrish', 0.043), ('point', 0.042), ('party', 0.041), ('guard', 0.04), ('variational', 0.04), ('finance', 0.038), ('lda', 0.038), ('blei', 0.037), ('polarity', 0.037), ('topic', 0.036), ('ro', 0.035), ('aul', 0.034), ('procedural', 0.034), ('exceptional', 0.033), ('land', 0.033), ('position', 0.033), ('health', 0.033), ('improvement', 0.032), ('donald', 0.032), ('popularity', 0.031), ('legislators', 0.03), ('substantive', 0.03), ('classic', 0.03), ('offsets', 0.029), ('period', 0.029), ('permuted', 0.028), ('politics', 0.028), ('republican', 0.028), ('behavior', 0.028), ('topics', 0.027), ('posterior', 0.027), ('points', 0.027), ('exploratory', 0.026), ('item', 0.026), ('sessions', 0.026), ('keith', 0.026), ('gary', 0.026), ('traditional', 0.026), ('labeled', 0.026), ('ym', 0.025), ('david', 0.025), ('months', 0.025), ('ron', 0.025), ('text', 0.024), ('james', 0.023), ('deviate', 0.023), ('bush', 0.023), ('democratic', 0.023), ('ell', 0.023), ('enelow', 0.023), ('issueadjusted', 0.023), ('jes', 0.023), ('lands', 0.023), ('mathew', 0.023), ('mcc', 0.023), ('ramage', 0.023), ('themes', 0.023), ('vudtest', 0.023), ('ronald', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="154-tfidf-1" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>2 0.21180925 <a title="154-tfidf-2" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>3 0.069174871 <a title="154-tfidf-3" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>4 0.057489943 <a title="154-tfidf-4" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><p>5 0.050778538 <a title="154-tfidf-5" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>6 0.048944727 <a title="154-tfidf-6" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>7 0.045602828 <a title="154-tfidf-7" href="./nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">165 nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>8 0.042349212 <a title="154-tfidf-8" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>9 0.037919629 <a title="154-tfidf-9" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>10 0.037588749 <a title="154-tfidf-10" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>11 0.03736661 <a title="154-tfidf-11" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>12 0.036870554 <a title="154-tfidf-12" href="./nips-2012-Random_Utility_Theory_for_Social_Choice.html">286 nips-2012-Random Utility Theory for Social Choice</a></p>
<p>13 0.036674958 <a title="154-tfidf-13" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>14 0.036277965 <a title="154-tfidf-14" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>15 0.036191441 <a title="154-tfidf-15" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>16 0.034717109 <a title="154-tfidf-16" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>17 0.031443309 <a title="154-tfidf-17" href="./nips-2012-Fast_Variational_Inference_in_the_Conjugate_Exponential_Family.html">129 nips-2012-Fast Variational Inference in the Conjugate Exponential Family</a></p>
<p>18 0.030919407 <a title="154-tfidf-18" href="./nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</a></p>
<p>19 0.030006634 <a title="154-tfidf-19" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>20 0.029048854 <a title="154-tfidf-20" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.086), (1, 0.014), (2, -0.022), (3, -0.008), (4, -0.087), (5, -0.03), (6, -0.01), (7, 0.02), (8, 0.041), (9, -0.021), (10, 0.051), (11, 0.076), (12, 0.005), (13, -0.011), (14, 0.01), (15, 0.055), (16, 0.017), (17, 0.027), (18, -0.018), (19, 0.045), (20, -0.014), (21, 0.004), (22, 0.018), (23, -0.034), (24, 0.007), (25, 0.01), (26, 0.044), (27, 0.059), (28, -0.033), (29, -0.011), (30, 0.034), (31, 0.023), (32, -0.016), (33, -0.023), (34, -0.01), (35, -0.051), (36, -0.048), (37, 0.074), (38, -0.043), (39, -0.01), (40, -0.034), (41, 0.002), (42, -0.003), (43, 0.001), (44, 0.122), (45, 0.124), (46, 0.044), (47, -0.025), (48, 0.089), (49, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9029904 <a title="154-lsi-1" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>2 0.71811843 <a title="154-lsi-2" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>3 0.61035603 <a title="154-lsi-3" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>Author: Peter Krafft, Juston Moore, Bruce Desmarais, Hanna M. Wallach</p><p>Abstract: We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models. 1</p><p>4 0.5666306 <a title="154-lsi-4" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>Author: Qixia Jiang, Jun Zhu, Maosong Sun, Eric P. Xing</p><p>Abstract: An effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model. This strategy has been adopted by a number of supervised topic models, such as MedLDA, which employs max-margin posterior constraints. However, unlike the likelihoodbased supervised topic models, of which posterior inference can be carried out using the Bayes’ rule, the max-margin posterior constraints have made Monte Carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean ﬁeld assumptions. In this paper, we develop two efﬁcient Monte Carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed Gibbs sampler, respectively, in a convex dual formulation. We report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efﬁciency.</p><p>5 0.53513992 <a title="154-lsi-5" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>6 0.51568514 <a title="154-lsi-6" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>7 0.50299001 <a title="154-lsi-7" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>8 0.47581115 <a title="154-lsi-8" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>9 0.44916698 <a title="154-lsi-9" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>10 0.43297586 <a title="154-lsi-10" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>11 0.42993158 <a title="154-lsi-11" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>12 0.42342949 <a title="154-lsi-12" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>13 0.41785988 <a title="154-lsi-13" href="./nips-2012-Adaptive_Stratified_Sampling_for_Monte-Carlo_integration_of_Differentiable_functions.html">36 nips-2012-Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions</a></p>
<p>14 0.40271375 <a title="154-lsi-14" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>15 0.40235135 <a title="154-lsi-15" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>16 0.37665009 <a title="154-lsi-16" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>17 0.3713519 <a title="154-lsi-17" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>18 0.35697088 <a title="154-lsi-18" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>19 0.3513737 <a title="154-lsi-19" href="./nips-2012-Learning_Probability_Measures_with_respect_to_Optimal_Transport_Metrics.html">184 nips-2012-Learning Probability Measures with respect to Optimal Transport Metrics</a></p>
<p>20 0.33326238 <a title="154-lsi-20" href="./nips-2012-Factoring_nonnegative_matrices_with_linear_programs.html">125 nips-2012-Factoring nonnegative matrices with linear programs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.05), (17, 0.011), (21, 0.029), (31, 0.015), (38, 0.065), (39, 0.08), (42, 0.032), (54, 0.015), (55, 0.033), (74, 0.036), (76, 0.074), (80, 0.07), (92, 0.027), (98, 0.348)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73537827 <a title="154-lda-1" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>2 0.61378014 <a title="154-lda-2" href="./nips-2012-Perceptron_Learning_of_SAT.html">267 nips-2012-Perceptron Learning of SAT</a></p>
<p>Author: Alex Flint, Matthew Blaschko</p><p>Abstract: Boolean satisﬁability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will ﬁnd an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware veriﬁcation task. 1</p><p>3 0.56390268 <a title="154-lda-3" href="./nips-2012-Bayesian_Warped_Gaussian_Processes.html">55 nips-2012-Bayesian Warped Gaussian Processes</a></p>
<p>Author: Miguel Lázaro-gredilla</p><p>Abstract: Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classiﬁcation, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets.</p><p>4 0.53982288 <a title="154-lda-4" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>5 0.47043934 <a title="154-lda-5" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>Author: Aharon Birnbaum, Shai S. Shwartz</p><p>Abstract: Given α, ϵ, we study the time complexity required to improperly learn a halfspace with misclassiﬁcation error rate of at most (1 + α) L∗ + ϵ, where L∗ is the γ γ optimal γ-margin error rate. For α = 1/γ, polynomial time and sample complexity is achievable using the hinge-loss. For α = 0, Shalev-Shwartz et al. [2011] showed that poly(1/γ) time is impossible, while learning is possible in ˜ time exp(O(1/γ)). An immediate question, which this paper tackles, is what is achievable if α ∈ (0, 1/γ). We derive positive results interpolating between the polynomial time for α = 1/γ and the exponential time for α = 0. In particular, we show that there are cases in which α = o(1/γ) but the problem is still solvable in polynomial time. Our results naturally extend to the adversarial online learning model and to the PAC learning with malicious noise model. 1</p><p>6 0.46951473 <a title="154-lda-6" href="./nips-2012-Learning_Mixtures_of_Tree_Graphical_Models.html">180 nips-2012-Learning Mixtures of Tree Graphical Models</a></p>
<p>7 0.40791646 <a title="154-lda-7" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>8 0.407783 <a title="154-lda-8" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>9 0.40401816 <a title="154-lda-9" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>10 0.38861111 <a title="154-lda-10" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>11 0.38653848 <a title="154-lda-11" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>12 0.37648386 <a title="154-lda-12" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>13 0.37004873 <a title="154-lda-13" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>14 0.36703235 <a title="154-lda-14" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>15 0.36690399 <a title="154-lda-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.36684608 <a title="154-lda-16" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>17 0.36378366 <a title="154-lda-17" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>18 0.36330906 <a title="154-lda-18" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>19 0.36309847 <a title="154-lda-19" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>20 0.36053517 <a title="154-lda-20" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
