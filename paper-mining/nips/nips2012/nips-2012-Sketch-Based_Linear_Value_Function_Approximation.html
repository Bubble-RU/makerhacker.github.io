<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>313 nips-2012-Sketch-Based Linear Value Function Approximation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-313" href="#">nips2012-313</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>313 nips-2012-Sketch-Based Linear Value Function Approximation</h1>
<br/><p>Source: <a title="nips-2012-313-pdf" href="http://papers.nips.cc/paper/4540-sketch-based-linear-value-function-approximation.pdf">pdf</a></p><p>Author: Marc Bellemare, Joel Veness, Michael Bowling</p><p>Abstract: Hashing is a common method to reduce large, potentially inﬁnite feature vectors to a ﬁxed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and ﬁfty-ﬁve Atari 2600 games to highlight the superior learning performance obtained when using tug-of-war hashing. 1</p><p>Reference: <a title="nips-2012-313-reference" href="../nips2012_reference/nips-2012-Sketch-Based_Linear_Value_Function_Approximation_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hash', 0.848), ('sars', 0.236), ('gam', 0.196), ('atar', 0.183), ('qt', 0.147), ('reinforc', 0.117), ('sketch', 0.115), ('st', 0.111), ('acrobot', 0.075), ('ag', 0.072), ('mountain', 0.069), ('til', 0.067), ('exhaust', 0.047), ('episod', 0.046), ('scor', 0.044), ('veness', 0.042), ('albert', 0.042), ('maillard', 0.041), ('sutton', 0.04), ('bowl', 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="313-tfidf-1" href="./nips-2012-Sketch-Based_Linear_Value_Function_Approximation.html">313 nips-2012-Sketch-Based Linear Value Function Approximation</a></p>
<p>Author: Marc Bellemare, Joel Veness, Michael Bowling</p><p>Abstract: Hashing is a common method to reduce large, potentially inﬁnite feature vectors to a ﬁxed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and ﬁfty-ﬁve Atari 2600 games to highlight the superior learning performance obtained when using tug-of-war hashing. 1</p><p>2 0.53667474 <a title="313-tfidf-2" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>Author: Ping Li, Art Owen, Cun-hui Zhang</p><p>Abstract: Minwise hashing is a standard procedure in the context of search, for efﬁciently estimating set similarities in massive binary data such as text. Recently, b-bit minwise hashing has been applied to large-scale learning and sublinear time nearneighbor search. The major drawback of minwise hashing is the expensive preprocessing, as the method requires applying (e.g.,) k = 200 to 500 permutations on the data. This paper presents a simple solution called one permutation hashing. Conceptually, given a binary data matrix, we permute the columns once and divide the permuted columns evenly into k bins; and we store, for each data vector, the smallest nonzero location in each bin. The probability analysis illustrates that this one permutation scheme should perform similarly to the original (k-permutation) minwise hashing. Our experiments with training SVM and logistic regression conﬁrm that one permutation hashing can achieve similar (or even better) accuracies compared to the k-permutation scheme. See more details in arXiv:1208.1259.</p><p>3 0.39967886 <a title="313-tfidf-3" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>Author: Yi Zhen, Dit-Yan Yeung</p><p>Abstract: Hashing-based methods provide a very promising approach to large-scale similarity search. To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically. In this paper, we study hash function learning in the context of multimodal data. We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization framework. The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized. We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets. 1</p><p>4 0.38340926 <a title="313-tfidf-4" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>Author: Amr Ahmed, Sujith Ravi, Alex J. Smola, Shravan M. Narayanamurthy</p><p>Abstract: Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters. 1</p><p>5 0.37179813 <a title="313-tfidf-5" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>Author: Mohammad Norouzi, David M. Blei, Ruslan Salakhutdinov</p><p>Abstract: Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efﬁcient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a ﬂexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST, with promising classiﬁcation results using no more than kNN on the binary codes. 1</p><p>6 0.30311951 <a title="313-tfidf-6" href="./nips-2012-Isotropic_Hashing.html">163 nips-2012-Isotropic Hashing</a></p>
<p>7 0.2359958 <a title="313-tfidf-7" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>8 0.13563703 <a title="313-tfidf-8" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>9 0.10363745 <a title="313-tfidf-9" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>10 0.084601946 <a title="313-tfidf-10" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>11 0.081723019 <a title="313-tfidf-11" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>12 0.07423903 <a title="313-tfidf-12" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>13 0.06968797 <a title="313-tfidf-13" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>14 0.065747961 <a title="313-tfidf-14" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>15 0.064163454 <a title="313-tfidf-15" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>16 0.064058632 <a title="313-tfidf-16" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>17 0.061687242 <a title="313-tfidf-17" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>18 0.060745832 <a title="313-tfidf-18" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>19 0.059716992 <a title="313-tfidf-19" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>20 0.05849836 <a title="313-tfidf-20" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.088), (2, 0.007), (3, 0.112), (4, -0.049), (5, -0.095), (6, -0.107), (7, -0.216), (8, -0.575), (9, -0.027), (10, -0.121), (11, -0.362), (12, 0.051), (13, -0.043), (14, -0.006), (15, -0.106), (16, 0.064), (17, -0.002), (18, 0.068), (19, 0.036), (20, 0.047), (21, -0.086), (22, -0.036), (23, 0.0), (24, -0.018), (25, 0.02), (26, 0.02), (27, -0.053), (28, 0.046), (29, -0.098), (30, 0.002), (31, 0.022), (32, -0.027), (33, 0.016), (34, 0.006), (35, -0.031), (36, -0.017), (37, -0.026), (38, -0.017), (39, 0.038), (40, 0.02), (41, -0.022), (42, 0.008), (43, 0.033), (44, -0.015), (45, 0.004), (46, 0.077), (47, 0.027), (48, 0.038), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95065457 <a title="313-lsi-1" href="./nips-2012-Sketch-Based_Linear_Value_Function_Approximation.html">313 nips-2012-Sketch-Based Linear Value Function Approximation</a></p>
<p>Author: Marc Bellemare, Joel Veness, Michael Bowling</p><p>Abstract: Hashing is a common method to reduce large, potentially inﬁnite feature vectors to a ﬁxed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and ﬁfty-ﬁve Atari 2600 games to highlight the superior learning performance obtained when using tug-of-war hashing. 1</p><p>2 0.89594585 <a title="313-lsi-2" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>Author: Ping Li, Art Owen, Cun-hui Zhang</p><p>Abstract: Minwise hashing is a standard procedure in the context of search, for efﬁciently estimating set similarities in massive binary data such as text. Recently, b-bit minwise hashing has been applied to large-scale learning and sublinear time nearneighbor search. The major drawback of minwise hashing is the expensive preprocessing, as the method requires applying (e.g.,) k = 200 to 500 permutations on the data. This paper presents a simple solution called one permutation hashing. Conceptually, given a binary data matrix, we permute the columns once and divide the permuted columns evenly into k bins; and we store, for each data vector, the smallest nonzero location in each bin. The probability analysis illustrates that this one permutation scheme should perform similarly to the original (k-permutation) minwise hashing. Our experiments with training SVM and logistic regression conﬁrm that one permutation hashing can achieve similar (or even better) accuracies compared to the k-permutation scheme. See more details in arXiv:1208.1259.</p><p>3 0.83287561 <a title="313-lsi-3" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>Author: Yi Zhen, Dit-Yan Yeung</p><p>Abstract: Hashing-based methods provide a very promising approach to large-scale similarity search. To obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically. In this paper, we study hash function learning in the context of multimodal data. We propose a novel multimodal hash function learning method, called Co-Regularized Hashing (CRH), based on a boosted coregularization framework. The hash functions for each bit of the hash codes are learned by solving DC (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized. We empirically compare CRH with two state-of-the-art multimodal hash function learning methods on two publicly available data sets. 1</p><p>4 0.75898081 <a title="313-lsi-4" href="./nips-2012-Isotropic_Hashing.html">163 nips-2012-Isotropic Hashing</a></p>
<p>Author: Weihao Kong, Wu-jun Li</p><p>Abstract: Most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding. Typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (PCA). Using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information. Although this viewpoint has been widely accepted by many researchers, it is still not veriﬁed by either theory or experiment because no methods have been proposed to ﬁnd a projection with equal variances for different dimensions. In this paper, we propose a novel method, called isotropic hashing (IsoHash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances). Experimental results on real data sets show that IsoHash can outperform its counterpart with different variances for different dimensions, which veriﬁes the viewpoint that projections with isotropic variances will be better than those with anisotropic variances. 1</p><p>5 0.72152042 <a title="313-lsi-5" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>Author: Jianqiu Ji, Jianmin Li, Shuicheng Yan, Bo Zhang, Qi Tian</p><p>Abstract: Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation. In this work, we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within (0, ⇡/2]. The extensive experiments on real data well validate that given the same length of binary code, SBLSH may achieve signiﬁcant mean squared error reduction in estimating pairwise angular similarity. Moreover, SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments. 1</p><p>6 0.5894646 <a title="313-lsi-6" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>7 0.4489173 <a title="313-lsi-7" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>8 0.37134385 <a title="313-lsi-8" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>9 0.21235462 <a title="313-lsi-9" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>10 0.18848252 <a title="313-lsi-10" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>11 0.18695664 <a title="313-lsi-11" href="./nips-2012-Fast_Resampling_Weighted_v-Statistics.html">128 nips-2012-Fast Resampling Weighted v-Statistics</a></p>
<p>12 0.18564641 <a title="313-lsi-12" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>13 0.18503319 <a title="313-lsi-13" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>14 0.18404476 <a title="313-lsi-14" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>15 0.17416538 <a title="313-lsi-15" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>16 0.17112131 <a title="313-lsi-16" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>17 0.1707278 <a title="313-lsi-17" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>18 0.16922325 <a title="313-lsi-18" href="./nips-2012-Approximating_Equilibria_in_Sequential_Auctions_with_Incomplete_Information_and_Multi-Unit_Demand.html">45 nips-2012-Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand</a></p>
<p>19 0.1672271 <a title="313-lsi-19" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>20 0.15723941 <a title="313-lsi-20" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.097), (22, 0.014), (47, 0.126), (67, 0.05), (70, 0.089), (85, 0.045), (93, 0.299), (94, 0.091), (99, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78307581 <a title="313-lda-1" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>Author: Neil Burch, Marc Lanctot, Duane Szafron, Richard G. Gibson</p><p>Abstract: Counterfactual Regret Minimization (CFR) is a popular, iterative algorithm for computing strategies in extensive-form games. The Monte Carlo CFR (MCCFR) variants reduce the per iteration time cost of CFR by traversing a smaller, sampled portion of the tree. The previous most effective instances of MCCFR can still be very slow in games with many player actions since they sample every action for a given player. In this paper, we present a new MCCFR algorithm, Average Strategy Sampling (AS), that samples a subset of the player’s actions according to the player’s average strategy. Our new algorithm is inspired by a new, tighter bound on the number of iterations required by CFR to converge to a given solution quality. In addition, we prove a similar, tighter bound for AS and other popular MCCFR variants. Finally, we validate our work by demonstrating that AS converges faster than previous MCCFR algorithms in both no-limit poker and Bluff. 1</p><p>same-paper 2 0.67112732 <a title="313-lda-2" href="./nips-2012-Sketch-Based_Linear_Value_Function_Approximation.html">313 nips-2012-Sketch-Based Linear Value Function Approximation</a></p>
<p>Author: Marc Bellemare, Joel Veness, Michael Bowling</p><p>Abstract: Hashing is a common method to reduce large, potentially inﬁnite feature vectors to a ﬁxed-size table. In reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and ﬁfty-ﬁve Atari 2600 games to highlight the superior learning performance obtained when using tug-of-war hashing. 1</p><p>3 0.59811735 <a title="313-lda-3" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>Author: Katherine Chen, Michael Bowling</p><p>Abstract: Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance. One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations. In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty. Instead we focus on identifying optimization objectives for which solutions can be efﬁciently approximated. We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efﬁciently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP. 1</p><p>4 0.58807552 <a title="313-lda-4" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>Author: Trung Nguyen, Tomi Silander, Tze Y. Leong</p><p>Abstract: We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efﬁcient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without predeﬁned mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. 1</p><p>5 0.58344597 <a title="313-lda-5" href="./nips-2012-Learning_Multiple_Tasks_using_Shared_Hypotheses.html">181 nips-2012-Learning Multiple Tasks using Shared Hypotheses</a></p>
<p>Author: Koby Crammer, Yishay Mansour</p><p>Abstract: In this work we consider a setting where we have a very large number of related tasks with few examples from each individual task. Rather than either learning each task individually (and having a large generalization error) or learning all the tasks together using a single hypothesis (and suffering a potentially large inherent error), we consider learning a small pool of shared hypotheses. Each task is then mapped to a single hypothesis in the pool (hard association). We derive VC dimension generalization bounds for our model, based on the number of tasks, shared hypothesis and the VC dimension of the hypotheses class. We conducted experiments with both synthetic problems and sentiment of reviews, which strongly support our approach. 1</p><p>6 0.5822795 <a title="313-lda-6" href="./nips-2012-Gradient_Weights_help_Nonparametric_Regressors.html">145 nips-2012-Gradient Weights help Nonparametric Regressors</a></p>
<p>7 0.58103675 <a title="313-lda-7" href="./nips-2012-Active_Comparison_of_Prediction_Models.html">32 nips-2012-Active Comparison of Prediction Models</a></p>
<p>8 0.5802536 <a title="313-lda-8" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>9 0.58009404 <a title="313-lda-9" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>10 0.58007282 <a title="313-lda-10" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>11 0.58003658 <a title="313-lda-11" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>12 0.57986313 <a title="313-lda-12" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>13 0.57981336 <a title="313-lda-13" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>14 0.57838929 <a title="313-lda-14" href="./nips-2012-One_Permutation_Hashing.html">257 nips-2012-One Permutation Hashing</a></p>
<p>15 0.57831067 <a title="313-lda-15" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>16 0.57809424 <a title="313-lda-16" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>17 0.57761675 <a title="313-lda-17" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>18 0.5772531 <a title="313-lda-18" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>19 0.57718128 <a title="313-lda-19" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>20 0.57643867 <a title="313-lda-20" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
