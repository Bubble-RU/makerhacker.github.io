<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-209" href="#">nips2012-209</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</h1>
<br/><p>Source: <a title="nips-2012-209-pdf" href="http://papers.nips.cc/paper/4794-max-margin-structured-output-regression-for-spatio-temporal-action-localization.pdf">pdf</a></p><p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>Reference: <a title="nips-2012-209-reference" href="../nips2012_reference/nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. [sent-4, score-0.31]
</p><p>2 Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i. [sent-5, score-1.228]
</p><p>3 , identifying a sequence of bounding boxes that track the action in video. [sent-7, score-0.515]
</p><p>4 The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. [sent-8, score-0.587]
</p><p>5 We propose a novel structured learning approach for spatio-temporal action localization. [sent-9, score-0.483]
</p><p>6 The mapping between a video and a spatio-temporal action trajectory is learned. [sent-10, score-0.553]
</p><p>7 1  Introduction  Blaschko and Lampert have recently shown that object localization can be approached as structured regression problem [2]. [sent-13, score-0.807]
</p><p>8 Instead of modeling object localization as a binary classiﬁcation and treating every bounding box independently, their method trains a discriminant function directly for predicting the bounding boxes of objects located in images. [sent-14, score-0.922]
</p><p>9 Motivated by the successful application of structured regression in object localization [2], it is natural to ask if we can perform structured regression for action localization in videos. [sent-16, score-1.82]
</p><p>10 Although this idea looks plausible, the extension from object localization to action localization is non-trivial. [sent-17, score-1.526]
</p><p>11 Different from object localization, where a visual object can be well localized by a 2-dimensional (2D) subwindow, human actions cannot be tightly bounded in such a similar way, i. [sent-18, score-0.425]
</p><p>12 Although many current methods for action detection are based on this 3D subvolume assumption [6, 9, 20, 29], and search for video subvolumes to detect actions, such an assumption is only reasonable for “static” actions, where the subjects do not move globally e. [sent-21, score-1.099]
</p><p>13 Thus, a more accurate localization scheme that can track the actions is required for localizing dynamic actions in videos. [sent-27, score-1.01]
</p><p>14 For example, one can localize an action by a 2D bounding box in each frame, and track it as the action moves across different frames. [sent-28, score-0.909]
</p><p>15 This localization structured output generates a smooth spatio-temporal path of connected 2-D bounding boxes. [sent-29, score-0.864]
</p><p>16 Such a spatio-temporal path can tightly bound the actions in the video space and provides a more accurate spatio-temporal localization of actions. [sent-30, score-1.064]
</p><p>17 1  right  left top object bottom  a)  b)  c)  Figure 1: Complexities of object and action localization: a) Object localization is of O(n4 ). [sent-31, score-1.048]
</p><p>18 b) Action localization by subvolume search is of O(n6 ). [sent-32, score-0.738]
</p><p>19 c) Spatio-temporal action localization in a much larger search space. [sent-33, score-0.953]
</p><p>20 However, as the video space is much larger than the image space, spatio-temporal action localization has a much larger structured space compared with object localization. [sent-34, score-1.334]
</p><p>21 For a video with size w × h × n, the search space for 3D subvolumes and 2D subwindows is only O(w2 h2 n2 ) and O(w2 h2 ), respectively (Figure 1). [sent-35, score-0.372]
</p><p>22 However, the search space for possible spatio-temporal paths in the video space is exponential O(whnk n )[23] if we do not know the start and end points of the path (k is the number of incoming edges per node). [sent-36, score-0.408]
</p><p>23 Any one of these paths can be the candidates for spatiotemporal action localization, thus an exhaustive search is infeasible. [sent-37, score-0.478]
</p><p>24 This huge structured space keeps structured learning approaches from being practical to spatio-temporal action localization due to intractable inferences. [sent-38, score-1.217]
</p><p>25 This paper proposes a new approach for spatio-temporal action localization which mainly addresses the above mentioned problems. [sent-39, score-0.892]
</p><p>26 Instead of using the 3D subvolume localization scheme, we precisely locate and track the action by ﬁnding an optimal spatio-temporal path to detect and localize actions. [sent-40, score-1.329]
</p><p>27 The mapping between a video and a spatio-temporal action trajectory is learned. [sent-41, score-0.553]
</p><p>28 Being solved as structured learning problem, our method can well exploit the correlations between local dependent video features, and therefore optimizes the structured output. [sent-43, score-0.539]
</p><p>29 1  Related work  Human action detection is traditionally approached by spatio-temporal video volume matching using different features: space-time orientation [6], volumetric [9], action MACH [20], HOG3D [10]. [sent-46, score-1.143]
</p><p>30 Hu et al used multiple-instance learning to detect actions [8]. [sent-50, score-0.456]
</p><p>31 Mahadevan et al used mixtures of dynamic textures to detect anomaly events [15]. [sent-51, score-0.293]
</p><p>32 Niebles et al used a probabilistic latent semantic analysis model for recognizing actions [17]. [sent-53, score-0.35]
</p><p>33 Yuan et al extended the branch-and-bound subwindow search [11] to subvolume search for action detection [29]. [sent-55, score-0.98]
</p><p>34 Recently, Tran and Yuan relaxed the 3D bounding box constraint for detecting and localizing medium and long video events [23]. [sent-56, score-0.416]
</p><p>35 More recently, Lan et al used a latent SVM to jointly detect and recognize actions in videos [12]. [sent-66, score-0.523]
</p><p>36 However, this method requires a reliable human detector in both inference and learning, thus it is not applicable to “dynamic” actions where the human poses are signiﬁcantly varied. [sent-68, score-0.324]
</p><p>37 Moreover, because of its using HOG3D [26], it only detects actions in a sparse subset of frames where the interest points present. [sent-69, score-0.315]
</p><p>38 2  Spatio-Temporal Action Localization as Structured Output Regression  Given a video x with the size of w × h × m where w × h is the frame size and m is its length, to localize actions, one needs to predict a structured object y which is a smooth spatio-temporal path in the video space. [sent-70, score-0.857]
</p><p>39 m } where (l, t, r, b)i are respectively the left, top, right, bottom of the rectangle that bounds the action in the i-th frame. [sent-73, score-0.367]
</p><p>40 These values of (l, t, r, b) are all set to zeros when there is no action in this frame. [sent-74, score-0.336]
</p><p>41 Because of the spatio-temporal smoothness constraint, the boxes in y are necessarily smoothed over the spatio-temporal video space. [sent-75, score-0.296]
</p><p>42 Let us denote X ⊂ [0, 255]3whm as the set of color videos, and Y ⊂ R4m as the set of all smooth spatiotemporal paths in the video space. [sent-76, score-0.298]
</p><p>43 The problem of spatio-temporal action localization becomes to learn a structured prediction function of f : X → Y. [sent-77, score-1.039]
</p><p>44 We formulate the action localization problem using the structured learning as presented in [24]. [sent-86, score-1.039]
</p><p>45 F is a compatibility function which measures how compatible the localization y will be suited to the given input video x. [sent-88, score-0.773]
</p><p>46 2  The Joint Kernel Feature Map for Action Localization  Let us denote x|y as the video portion cut out from x by the path y, namely the stack of images cropped by the bounding boxes b1. [sent-114, score-0.464]
</p><p>47 We also denote ϕ(bi ) ∈ Rk as a feature map for a 2D 3  box bi . [sent-117, score-0.311]
</p><p>48 m }, where bi = (l, t, r, b)i ˆ is the ground truth box of the i-th frame. [sent-130, score-0.42]
</p><p>49 max {∆(y, y ) + w, φ(x, y) } ¯ (10) y∈Y  =  =  max y∈Y  1 m  1 max m y∈Y  m  ¯ δ(bi , bi ) + i=1 m  1 m  m  w, ϕ(bi )  (11)  i=1  ¯ δ(bi , bi ) + w, ϕ(bi )  (12)  i=1  To solve Eq. [sent-146, score-0.488]
</p><p>50 12, one needs to search for a smooth path y ∗ in the spatio-temporal video space Y which gives the maximum total score. [sent-148, score-0.352]
</p><p>51 Max-Path algorithm [23] was proposed to detect dynamic video events. [sent-155, score-0.323]
</p><p>52 It is guaranteed to obtain the best spatio-temporal path in the video space provided that the local windows’ scores can be precomputed. [sent-156, score-0.322]
</p><p>53 In testing, the trellis’s local scores are w, ϕ(bi ) where bi is the local window. [sent-158, score-0.275]
</p><p>54 In training, those values of the trellis are δ(bi , bi ) + w, ϕ(bi ) which are also ¯ computable given parameter w, feature map ϕ, and ground truth bi . [sent-160, score-0.678]
</p><p>55 ¯ ¯ b w, φ(x, y ) − w, φ(x, y) ≥ ∆(¯, y) − ξ, ∀y ∈ Y\¯ ¯ y y ⇔  1 m  m  ¯ w, ϕ(bi ) − i=1  m  w, ϕ(bi ) ≥ i=1  m  ¯ w, ϕ(bi ) −  ⇔  1 m  m  i=1  1 m  (13)  m  ¯ δ(bi , bi ) − ξ, ∀y ∈ Y\¯ y  (14)  i=1  m  ¯ δ(bi , bi ) − mξ, ∀y ∈ Y\¯ y  w, ϕ(bi ) ≥ i=1  (15)  i=1  The constraint in Eq. [sent-172, score-0.536]
</p><p>56 15 constraint ¯ ¯ w, ϕ(bi ) − w, ϕ(bi ) ≥ δ(bi , bi ) − ξ, ∀i ∈ [1. [sent-175, score-0.292]
</p><p>57 However, the important beneﬁt of using such enforcements is that instead of comparing features of two different ¯ spatio-temporal paths y and y , one can compare the features of individual box pairs (bi , bi ) of those ¯ two paths. [sent-182, score-0.441]
</p><p>58 UCFSport dataset consists of 150 video sequences of 10 different action classes. [sent-185, score-0.577]
</p><p>59 We perform the task of kiss detection and localization on this dataset. [sent-194, score-0.854]
</p><p>60 Kissing actions is more challenging compared with other action classes in this dataset due to less motion and appearance cues. [sent-195, score-0.552]
</p><p>61 As used in [12], the video localization score is measured by averaging its frame localization scores which are the overlap area divided by the union area of the predicted and truth boxes. [sent-208, score-1.603]
</p><p>62 A prediction is then considered as correct if its localization score is greater or equal to σ = 0. [sent-209, score-0.556]
</p><p>63 It is worth noting that detection evaluations are applied to both positive and negative testing examples while localization evaluations are only applied to positive ones. [sent-211, score-0.855]
</p><p>64 As a result, the detection metric is to measure the reliability of the detections (precision/recall) where the localization metric indicates the quality of detections, e. [sent-212, score-0.813]
</p><p>65 More speciﬁc, detection is to answer the question “Is there any action of interest in this video? [sent-215, score-0.533]
</p><p>66 ” while localization is to answer to “Provided that there is one action instance that appears in this video, where is it? [sent-216, score-0.892]
</p><p>67 6  Figure 2: Action detection results on UCF-Sport: detection curves of our proposed method compared with [12] and [23]. [sent-266, score-0.446]
</p><p>68 Upper plots are detection results evaluated on subset frames given by [12], while lower plots are the results of all-frame evaluations. [sent-267, score-0.32]
</p><p>69 30 Table 1: Action localization results on UCF-Sport: comparisons among our proposed method, [12], and [23]. [sent-290, score-0.556]
</p><p>70 For [23], we train a linear SVM detector for each action class using the same features as ours. [sent-299, score-0.373]
</p><p>71 The Max-Path algorithm is then applied to detect the actions of interest. [sent-300, score-0.298]
</p><p>72 According to [12], its method used HOG3D [26], so that it is only able to detect and localize actions at a sparse set of frames where the HOG3D interest points present. [sent-301, score-0.484]
</p><p>73 The ﬁrst set is applied only to the subset of frames where [12] reports detections and the second set is to take all frames into consideration. [sent-303, score-0.297]
</p><p>74 Table 1 reports the results of action localization of different methods and action classes. [sent-304, score-1.254]
</p><p>75 Our method signiﬁcantly improves over [23] for all three action classes on both subset and all-frame evaluations. [sent-310, score-0.437]
</p><p>76 However, [12] provides better detection results than ours on diving detection. [sent-312, score-0.399]
</p><p>77 This better detection is because their interest-pointbased sparse features are more suitable to deformable actions as diving. [sent-313, score-0.426]
</p><p>78 For a complete presentation, we visualze localization results of our method comapared with those of [12] and [23] on a diving sequence (Figure 3). [sent-314, score-0.786]
</p><p>79 All predicted boxes are plotted together with ground truth boxes for comparisons. [sent-315, score-0.295]
</p><p>80 2 0  0  10  20  30 Frame number  40  50  60  Figure 3: Visualization of diving localization: the plots of localization scores of different methods on a diving video sequence. [sent-320, score-1.208]
</p><p>81 Figure 4: Action detection and localization on UCF-Sport: Lan et al’s [12] results are visualized in blue, Tran and Yuan’s [23] are green, ours are read, and ground truth are black. [sent-323, score-0.952]
</p><p>82 Our method and [23] can detect multiple instances of actions (two bottom left images). [sent-324, score-0.326]
</p><p>83 Our method (red curve) localizes diving action much more accurately than [23] (green curve). [sent-326, score-0.602]
</p><p>84 [12] localizes diving action fairly good, however it is not applicable when more accurate localizations (e. [sent-327, score-0.574]
</p><p>85 Oxford-TV: we compare our method with [23] on both detection and localization tasks. [sent-330, score-0.781]
</p><p>86 For localization, besides the spatial localization (SL) metric as used in UCF dataset experiments, we also evaluate different methods by temporal localization (TL) metric. [sent-332, score-1.198]
</p><p>87 This metric is not applicable to UCF dataset because most action instances in UCF dataset start and end at the ﬁrst and last frame, respectively. [sent-333, score-0.384]
</p><p>88 Temporal localization is computed as the length Method EPR(%) AUC SL(%) TL(%) [18] 32. [sent-334, score-0.556]
</p><p>89 (measured in frames) of the intersection divided by the union of detection and ground truth. [sent-400, score-0.289]
</p><p>90 Table 2 presents detection and localization results of our proposed method compared with [23]. [sent-401, score-0.781]
</p><p>91 03% (Figure 6b) which demonstrates that our method can simultaneously detect and localize actions with high accuracy. [sent-415, score-0.396]
</p><p>92 6  Conclusions  We have proposed a novel structured learning approach for spatio-temporal action localization in videos. [sent-416, score-1.039]
</p><p>93 While most of current approaches detect actions as 3D subvolumes [6, 9, 20, 29] or a sparse subset of frames [12], our method can precisely detect and track actions in both spatial and temporal spaces. [sent-417, score-0.913]
</p><p>94 Although [23] is also applicable to spatio-temporal action detection, this method cannot be optimized over the large video space due to its independently trained detectors. [sent-418, score-0.581]
</p><p>95 Moreover, being free from people detection and background subtraction, our approach can efﬁciently handle unconstrained videos and be easily extended to detect other spatio-temporal video patterns. [sent-421, score-0.587]
</p><p>96 Efﬁcient action spotting based on a spacetime oriented structure representation. [sent-461, score-0.371]
</p><p>97 Discriminative ﬁgure-centric models for joint action localization and recognition. [sent-503, score-0.892]
</p><p>98 Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis. [sent-517, score-0.373]
</p><p>99 Unsupervised learning of human action categories using spatialtemporal words. [sent-539, score-0.388]
</p><p>100 Action mach: A spatio-temporal maximum average correlation height ﬁlter for action recognition. [sent-560, score-0.336]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('localization', 0.556), ('action', 0.336), ('bi', 0.244), ('video', 0.217), ('diving', 0.202), ('detection', 0.197), ('actions', 0.192), ('lan', 0.156), ('structured', 0.147), ('tran', 0.142), ('al', 0.125), ('subvolume', 0.121), ('detect', 0.106), ('kiss', 0.101), ('frames', 0.088), ('dive', 0.081), ('trellis', 0.081), ('ucf', 0.081), ('boxes', 0.079), ('object', 0.078), ('path', 0.074), ('localize', 0.07), ('box', 0.067), ('videos', 0.067), ('yuan', 0.067), ('ground', 0.063), ('search', 0.061), ('hof', 0.061), ('subvolumes', 0.061), ('detections', 0.06), ('visualized', 0.057), ('bounding', 0.057), ('paths', 0.056), ('roc', 0.055), ('frame', 0.054), ('human', 0.052), ('cvpr', 0.052), ('evaluations', 0.051), ('lb', 0.049), ('constraint', 0.048), ('subwindow', 0.046), ('blaschko', 0.046), ('truth', 0.046), ('area', 0.043), ('track', 0.043), ('precision', 0.042), ('epr', 0.04), ('irregularities', 0.04), ('junsong', 0.04), ('klaser', 0.04), ('nanyang', 0.04), ('bmvc', 0.038), ('improves', 0.038), ('yi', 0.037), ('features', 0.037), ('cropped', 0.037), ('curve', 0.037), ('mach', 0.036), ('localizes', 0.036), ('marszalek', 0.036), ('subset', 0.035), ('temporal', 0.035), ('oriented', 0.035), ('argmax', 0.035), ('et', 0.033), ('subwindows', 0.033), ('boiman', 0.033), ('niebles', 0.033), ('scores', 0.031), ('volumetric', 0.031), ('rectangle', 0.031), ('enforcement', 0.031), ('mahadevan', 0.031), ('intractable', 0.031), ('output', 0.03), ('anomaly', 0.029), ('divided', 0.029), ('predicted', 0.028), ('pedestrian', 0.028), ('tl', 0.028), ('lampert', 0.028), ('recall', 0.028), ('method', 0.028), ('constraints', 0.027), ('localizing', 0.027), ('finley', 0.027), ('spatial', 0.027), ('reports', 0.026), ('approached', 0.026), ('tightly', 0.025), ('spatiotemporal', 0.025), ('dataset', 0.024), ('cutting', 0.024), ('yao', 0.024), ('sl', 0.024), ('curves', 0.024), ('ijcv', 0.023), ('locate', 0.023), ('windows', 0.023), ('svm', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="209-tfidf-1" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>2 0.27038479 <a title="209-tfidf-2" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>3 0.2093845 <a title="209-tfidf-3" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>Author: Kevin Tang, Vignesh Ramanathan, Li Fei-fei, Daphne Koller</p><p>Abstract: Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest ﬁrst. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features speciﬁc to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video [2] datasets that illustrate the beneﬁt of our approach to adapt object detectors to video. 1</p><p>4 0.12468732 <a title="209-tfidf-4" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>5 0.11675564 <a title="209-tfidf-5" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>Author: Weixin Li, Nuno Vasconcelos</p><p>Abstract: In this work, we consider the problem of modeling the dynamic structure of human activities in the attributes space. A video sequence is Ä?Ĺš rst represented in a semantic feature space, where each feature encodes the probability of occurrence of an activity attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both the binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining binary observation variables with a hidden Gauss-Markov state process. In this way, it integrates the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by a popular LDS learning method from dynamic textures, is proposed. A similarity measure between BDSs, which generalizes the BinetCauchy kernel for LDS, is then introduced and used to design activity classiÄ?Ĺš ers. The proposed method is shown to outperform similar classiÄ?Ĺš ers derived from the kernel dynamic system (KDS) and state-of-the-art approaches for dynamics-based or attribute-based action recognition. 1</p><p>6 0.11539704 <a title="209-tfidf-6" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>7 0.11254552 <a title="209-tfidf-7" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>8 0.10900647 <a title="209-tfidf-8" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>9 0.1020058 <a title="209-tfidf-9" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>10 0.10194789 <a title="209-tfidf-10" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>11 0.096239462 <a title="209-tfidf-11" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>12 0.091857731 <a title="209-tfidf-12" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>13 0.087453954 <a title="209-tfidf-13" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>14 0.086176179 <a title="209-tfidf-14" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>15 0.085906766 <a title="209-tfidf-15" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>16 0.075751215 <a title="209-tfidf-16" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>17 0.07439556 <a title="209-tfidf-17" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>18 0.073904328 <a title="209-tfidf-18" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>19 0.070689686 <a title="209-tfidf-19" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>20 0.069859006 <a title="209-tfidf-20" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.097), (2, -0.165), (3, -0.031), (4, 0.116), (5, -0.091), (6, 0.007), (7, -0.048), (8, -0.005), (9, -0.013), (10, -0.068), (11, 0.036), (12, 0.111), (13, -0.102), (14, 0.113), (15, 0.164), (16, -0.021), (17, -0.053), (18, -0.058), (19, -0.004), (20, 0.015), (21, 0.023), (22, -0.06), (23, -0.044), (24, -0.027), (25, -0.066), (26, 0.022), (27, 0.054), (28, 0.024), (29, 0.009), (30, 0.065), (31, 0.073), (32, -0.001), (33, 0.029), (34, -0.137), (35, 0.021), (36, 0.033), (37, -0.015), (38, -0.049), (39, 0.038), (40, 0.058), (41, -0.021), (42, 0.022), (43, -0.007), (44, -0.099), (45, -0.073), (46, 0.026), (47, 0.049), (48, -0.076), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97078896 <a title="209-lsi-1" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>2 0.8213439 <a title="209-lsi-2" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>3 0.77531242 <a title="209-lsi-3" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>Author: Kevin Tang, Vignesh Ramanathan, Li Fei-fei, Daphne Koller</p><p>Abstract: Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest ﬁrst. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features speciﬁc to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video [2] datasets that illustrate the beneﬁt of our approach to adapt object detectors to video. 1</p><p>4 0.70306557 <a title="209-lsi-4" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>5 0.68313843 <a title="209-lsi-5" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>6 0.66701847 <a title="209-lsi-6" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>7 0.65971416 <a title="209-lsi-7" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>8 0.63942611 <a title="209-lsi-8" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>9 0.5944581 <a title="209-lsi-9" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>10 0.58609217 <a title="209-lsi-10" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>11 0.56007749 <a title="209-lsi-11" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>12 0.55365652 <a title="209-lsi-12" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>13 0.51626909 <a title="209-lsi-13" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>14 0.48654908 <a title="209-lsi-14" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>15 0.48506981 <a title="209-lsi-15" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>16 0.47981077 <a title="209-lsi-16" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>17 0.47298086 <a title="209-lsi-17" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>18 0.47124228 <a title="209-lsi-18" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>19 0.46989462 <a title="209-lsi-19" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>20 0.46912506 <a title="209-lsi-20" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (1, 0.096), (17, 0.035), (21, 0.02), (38, 0.093), (39, 0.011), (42, 0.035), (44, 0.021), (54, 0.078), (55, 0.011), (74, 0.112), (76, 0.164), (80, 0.063), (86, 0.012), (92, 0.053), (93, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90229118 <a title="209-lda-1" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>2 0.87079775 <a title="209-lda-2" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>Author: Jenna Wiens, Eric Horvitz, John V. Guttag</p><p>Abstract: A patient’s risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient’s pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient outcomes, considering only the patient’s current or aggregate state. In this paper, we represent patient risk as a time series. In doing so, patient risk stratiﬁcation becomes a time-series classiﬁcation task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must ﬁrst be extracted. Thus, we begin by deﬁning and extracting approximate risk processes, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classiﬁcation with the goal of identifying high-risk patterns. We apply the classiﬁcation to the speciﬁc task of identifying patients at risk of testing positive for hospital acquired Clostridium difﬁcile. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratiﬁcation outperforms classiﬁers that consider only a patient’s current state (p<0.05). 1</p><p>3 0.86733216 <a title="209-lda-3" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>Author: Ko-jen Hsiao, Kevin Xu, Jeff Calder, Alfred O. Hero</p><p>Abstract: We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be deﬁned, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria. 1</p><p>4 0.86397111 <a title="209-lda-4" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>5 0.86081505 <a title="209-lda-5" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>Author: Aaron Wilson, Alan Fern, Prasad Tadepalli</p><p>Abstract: We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent’s goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efﬁcient than random selection. 1</p><p>6 0.85554403 <a title="209-lda-6" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>7 0.85339433 <a title="209-lda-7" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>8 0.84416759 <a title="209-lda-8" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>9 0.84004086 <a title="209-lda-9" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>10 0.83918524 <a title="209-lda-10" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>11 0.83899677 <a title="209-lda-11" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>12 0.83741975 <a title="209-lda-12" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>13 0.83621591 <a title="209-lda-13" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>14 0.83582801 <a title="209-lda-14" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>15 0.83491027 <a title="209-lda-15" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>16 0.83437049 <a title="209-lda-16" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>17 0.83247489 <a title="209-lda-17" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>18 0.8315829 <a title="209-lda-18" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>19 0.82697225 <a title="209-lda-19" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>20 0.82673419 <a title="209-lda-20" href="./nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
