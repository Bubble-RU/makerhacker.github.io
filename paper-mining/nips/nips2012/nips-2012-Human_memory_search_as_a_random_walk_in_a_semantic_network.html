<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2012-Human memory search as a random walk in a semantic network</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-155" href="#">nips2012-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 nips-2012-Human memory search as a random walk in a semantic network</h1>
<br/><p>Source: <a title="nips-2012-155-pdf" href="http://papers.nips.cc/paper/4761-human-memory-search-as-a-random-walk-in-a-semantic-network.pdf">pdf</a></p><p>Author: Joseph L. Austerweil, Joshua T. Abbott, Thomas L. Griffiths</p><p>Abstract: The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. 1</p><p>Reference: <a title="nips-2012-155-reference" href="../nips2012_reference/nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Human memory search as a random walk in a semantic network Joseph L. [sent-1, score-0.613]
</p><p>2 Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. [sent-10, score-0.343]
</p><p>3 Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. [sent-11, score-0.306]
</p><p>4 These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. [sent-12, score-0.912]
</p><p>5 We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. [sent-13, score-0.608]
</p><p>6 This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. [sent-14, score-0.324]
</p><p>7 1  Introduction  Human memory has a vast capacity, storing all the semantic knowledge, facts, and experiences that people accrue over a lifetime. [sent-15, score-0.498]
</p><p>8 In fact, it is the same problem faced by libraries [1] and internet search engines [6] that need to efﬁciently organize information to facilitate retrieval of those items most likely to be relevant to a query. [sent-17, score-0.233]
</p><p>9 It thus becomes interesting to try to understand exactly what kind of algorithms and representations are used when people search their memory. [sent-18, score-0.179]
</p><p>10 One of the main tasks that has been used to explore memory search is the semantic ﬂuency task, in which people retrieve as many items belonging to a particular category (e. [sent-19, score-0.678]
</p><p>11 Early studies using semantic ﬂuency tasks suggested a two-part memory retrieval process: clustering, in which the production of words form semantic subcategories, and switching, in which a transition is made from one subcategory to another [13, 21]. [sent-22, score-0.842]
</p><p>12 Recently, it has been suggested that the clustering patterns observed in semantic ﬂuency tasks could reﬂect an optimal foraging strategy, with people searching for items distributed in memory in a way that is similar to animals searching for food in environments with patchy food resources [7]. [sent-24, score-1.09]
</p><p>13 The idea behind this approach is that each cluster corresponds to a “patch” and people strategically choose to leave patches when the rate at which they retrieve relevant concepts drops below their average rate of retrieval. [sent-25, score-0.242]
</p><p>14 Quantitative analyses of human data provide support for this account, ﬁnding shorter delays in retrieving relevant items after a change in clusters and a relationship between when people leave a cluster and their average retrieval time. [sent-26, score-0.495]
</p><p>15 In this paper, we argue that there may be a simpler explanation for the patterns seen in semantic ﬂuency tasks, requiring only a single cognitive process rather than separate processes for exploring a cluster and deciding to switch between clusters. [sent-27, score-0.507]
</p><p>16 We show that the results used to argue for the optimal foraging account can be reproduced by a random walk on a semantic network derived from human semantic associations. [sent-28, score-0.936]
</p><p>17 Intriguingly, this is exactly the kind of process assumed by the PageRank algorithm [12], providing a suggestive link between human memory and internet search and a new piece of evidence supporting the claim [6] that this algorithm might be relevant to understanding human semantic memory. [sent-29, score-0.646]
</p><p>18 Section 2 provides relevant background information on studies of human memory search with semantic ﬂuency tasks and outlines the retrieval phenomena predicted by an optimal foraging account. [sent-31, score-0.822]
</p><p>19 Section 3 presents the parallels between searching the internet and search in human memory, and provides a structural analysis of semantic memory. [sent-32, score-0.474]
</p><p>20 Section 4 evaluates our proposal that a random walk in a semantic network is consistent with the observed behavior in semantic ﬂuency tasks. [sent-33, score-0.739]
</p><p>21 2  Semantic ﬂuency and optimal foraging  Semantic ﬂuency tasks (also known as free recall from natural categories) are a classic methodological paradigm for examining how people recall relevant pieces of information from memory given a retrieval cue [2, 14, 19]. [sent-35, score-0.508]
</p><p>22 Asking people to retrieve as many examples of a category as possible in a limited time is a simple task to carry out in clinical settings, and semantic ﬂuency has been used to study memory deﬁcits in patients with Alzheimer’s, Parkinson’s, and Huntington’s disease [9, 20, 21, 22]. [sent-36, score-0.569]
</p><p>23 Both early and recent studies [2, 14, 21] have consistently found that clusters appear in the sequences of words that people produce, with bursts of semantically related words produced together and noticeable pauses between these bursts. [sent-37, score-0.348]
</p><p>24 [21] had people retrieve examples of animals, and divided those animals into 22 nonexclusive clusters (“pets”, “African animals”, etc. [sent-39, score-0.349]
</p><p>25 These clusters could be used to analyze patterns in people’s responses: if an item shares a cluster with the item immediately before it, it is considered part of the same cluster, otherwise, the current item deﬁnes a transition between clusters. [sent-41, score-0.498]
</p><p>26 Observing fast transitions between items within a cluster but slow transitions between clusters led to the proposal that memory search might be decomposed into separate “clustering” and “switching” processes [21]. [sent-43, score-0.39]
</p><p>27 The clusters that seem to appear in semantic memory suggest an analogy to the distribution of animal food sources in a patchy environment. [sent-44, score-0.661]
</p><p>28 When animals search for food, they must consider the costs and beneﬁts of staying within a patch as opposed to searching for a new patch. [sent-45, score-0.374]
</p><p>29 In particular, the marginal value theorem shows that a forager’s overall rate of return is optimized if it leaves a patch when the instantaneous rate (the marginal value) of ﬁnding food within the patch falls below the long-term average rate of ﬁnding food over the entire environment [3]. [sent-47, score-0.449]
</p><p>30 [7] posited that search in human semantic memory is similarly guided by an optimal foraging policy. [sent-49, score-0.664]
</p><p>31 The corresponding prediction is that people should leave a “patch” in memory (ie. [sent-50, score-0.222]
</p><p>32 [7] had people perform a semantic ﬂuency task (“Name as many animals as you can in 3 minutes”) and analyzed the search paths taken through memory 2  1. [sent-53, score-0.717]
</p><p>33 (a) The mean ratio between the inter-item response time (IRT) for an item and the participant’s long-term average IRT over the entire task, relative to the order of entry for the item (where “1” refers to the relative IRT between the ﬁrst word in a patch and the last word in the preceding patch). [sent-61, score-0.621]
</p><p>34 (c) The relationship between a participant’s deviation from the marginal value theorem policy for patch departures (horizontalaxis) and the total number of words a participant produced. [sent-64, score-0.282]
</p><p>35 in terms of the sequences of animal names produced, assessed with the predetermined animal subcategories of Troyer et al. [sent-65, score-0.295]
</p><p>36 As a ﬁrst measure of correspondence with optimal foraging theory, the ratio between inter-item response times (IRTs) of items and the long-term average IRTs for each participant were examined at different retrieval positions relative to a patch switch. [sent-67, score-0.481]
</p><p>37 The ﬁrst word in a patch (indicated by an order of entry of “1”) takes longer to produce than the overall long-term average IRT (indicated by the dotted line), and the second word in a patch (indicated by “2”) takes much less time to produce. [sent-69, score-0.481]
</p><p>38 These results are in line with the marginal value theorem where IRTs up until a patch switch should increase monotonically towards the long-term average IRT and go above this average only for patch switch IRTs since it takes extra time to ﬁnd a new patch. [sent-70, score-0.583]
</p><p>39 offered a two-part process model to account for this phenomenon: When the IRT following a word exceeds the long-term average IRT, search switches from local to global cues (e. [sent-72, score-0.194]
</p><p>40 switching between using semantic similarity or overall frequency as search cues). [sent-74, score-0.425]
</p><p>41 To formally examine how close the IRTs for words immediately preceding a patch switch were to the long-term average IRT, the per-participant average IRT for these pre-switch words was plotted against the per-participant long-term average IRT (see Figure 1 (b)). [sent-75, score-0.448]
</p><p>42 As a further analysis of these preswitch IRTs, the absolute difference between the pre-switch IRT and long-term average IRT was plotted against the number of words a participant produced along with a regression line through this data (see Figure 1 (c)). [sent-77, score-0.205]
</p><p>43 3  The structure of semantic memory  The explanation proposed by Hills et al. [sent-79, score-0.444]
</p><p>44 [7] for the patterns observed in people’s behavior in semantic ﬂuency tasks is relatively complex, assuming two separate processes and a strategic decision to switch between them. [sent-80, score-0.46]
</p><p>45 In the remainder of the paper we consider a simpler alternative explanation, based on the structure of semantic memory. [sent-81, score-0.276]
</p><p>46 Speciﬁcally, we consider the consequences of a single search process operating over a richer representation – a semantic network. [sent-82, score-0.35]
</p><p>47 A semantic network represents the relationships between words (or concepts) as a directed graph, where each word is represented as a node and nodes are connected together with edges that represent pairwise association [4]. [sent-83, score-0.441]
</p><p>48 Semantic networks derived from human behavior can be used to explore questions about the structure of human memory [5, 14, 17, 18]. [sent-84, score-0.241]
</p><p>49 We will focus on a network derived from a word association task, in which people were asked to list the words that come to mind for a particular cue. [sent-85, score-0.251]
</p><p>50 The result is a semantic network with 5018 nodes, from “a” to “zucchini”. [sent-88, score-0.312]
</p><p>51 We explored whether the distance between the nodes corresponding to different animals in the semantic network could be predicted by their cluster membership. [sent-91, score-0.548]
</p><p>52 produced 373 unique animals, of which 178 were included in the semantic network. [sent-93, score-0.345]
</p><p>53 We analyzed whether the relationship between these animals in the semantic network showed evidence of the clustering seen in semantic ﬂuency tasks, based on the clusters identiﬁed by Troyer et al. [sent-96, score-0.877]
</p><p>54 The features in the matrix F were deﬁned to be the twenty-two hand-coded subcategorization of animals from Troyer et al. [sent-100, score-0.177]
</p><p>55 The two similarity matrices contain similar block structure, which supports the hypothesis that the clusters of animals are implicitly captured by the semantic network. [sent-103, score-0.509]
</p><p>56 If the distance between animals in different clusters is greater than the distance between animals in the same cluster, as these results suggest, then a simple search process that is sensitive to this distance may be able to account for the results reported by Hills et al. [sent-104, score-0.482]
</p><p>57 (a)  (b)  Figure 2: Visualizing the similarity between pairs of animals in our semantic network (darker colors represent stronger similarities). [sent-106, score-0.483]
</p><p>58 The rows and columns of the two matrices were reordered to display animals in the clusters with largest weight ﬁrst. [sent-110, score-0.207]
</p><p>59 4  4  Random walks and semantic ﬂuency  One of the simplest processes that can operate over a semantic network is a random walk, stochastically jumping from one node to another by following edges. [sent-111, score-0.682]
</p><p>60 Intuitively, this might provide a reasonable model for searching through semantic memory, being a meandering rather than a directed search. [sent-112, score-0.308]
</p><p>61 Random walks on semantic networks have previously been proposed as a possible account of behavior on ﬂuency tasks: Grifﬁths et al. [sent-113, score-0.371]
</p><p>62 [6] argued that the responses that people produce when asked to generate a word that begins with a particular letter were consistent with the stationary distribution of a random walk on the same semantic network used in the analysis presented in the previous section. [sent-114, score-0.64]
</p><p>63 In addition to being simple, random walks on semantic networks have an interesting connection to methods used for information retrieval. [sent-115, score-0.297]
</p><p>64 The link structure of n web pages on the Internet can be characterized by an n × n matrix L, where Lij is 1 if there is a link from web page j to web page i, and 0 otherwise. [sent-118, score-0.177]
</p><p>65 If an internet user clicks uniformly at random over the outgoing links, then the probability that the user will click on page i given she is currently on page j is Lij Mij = n (2) k=1 Lkj where the denominator is the out-degree or number of web pages that page j links to. [sent-119, score-0.212]
</p><p>66 [6] is that the prominence of words in human memory can be predicted by running the PageRank algorithm on a semantic network. [sent-123, score-0.54]
</p><p>67 pointed out, multiple mechanisms exist that could produce this result, with only one possibility being that memory search is a random walk on a semantic network. [sent-125, score-0.603]
</p><p>68 Exploring whether this kind of random walk can reproduce the phenomena identiﬁed by Hills et al. [sent-126, score-0.177]
</p><p>69 We then evaluate our models of memory search by applying the analyses used by Hills et al. [sent-129, score-0.257]
</p><p>70 [7] participants were asked to produce as many unique animals as possible in three minutes. [sent-133, score-0.234]
</p><p>71 2  Computing inter-item retrieval times  Random walk simulations for the models deﬁned above will produce a list of the nodes visited at each iteration. [sent-150, score-0.234]
</p><p>72 In our analyses we consider only the ﬁrst time an animal node is visited, which we denote as τ (k) for the k th unique animal seen on the random walk (out of the K unique animals seen on the random walk). [sent-152, score-0.533]
</p><p>73 Thus, according to the random walk models, the inter-item retrieval time (IRT) between animal k and k − 1 is IRT (k) = τ (k) − τ (k − 1) + L(Xτ (k) )  (4)  where τ (k) is the ﬁrst hitting time of animal Xτ (k) and L(X) is the length of word X. [sent-159, score-0.479]
</p><p>74 The number of iterations was selected to produce a similar mean number of animals to those produced by participants in Hills et al. [sent-165, score-0.335]
</p><p>75 The left column shows the mean ratio between the inter-item retrieval time (IRT) for an item and the mean IRT over all 1750 iterations in the simulations, relative to the order of entry for the item. [sent-178, score-0.242]
</p><p>76 Here we see that the ﬁrst word starting a patch (the bar labeled “1”) has the highest overall retrieval time. [sent-179, score-0.248]
</p><p>77 as indicating the time it takes to switch clusters and generate a word from a new cluster. [sent-181, score-0.248]
</p><p>78 The 2 A slightly lower overall total number of animals is to be expected, given the limited number of animals among the words included in our semantic network. [sent-182, score-0.611]
</p><p>79 2  5  0  0 −2  −1  1  2  50 45 40 35 30 25  0  3  5  10  15  20  25  30  35  0  Mean IRT for the entry prior to switch  Order of entry relative to patch switch  2  4  6  8  10  Abs(Last item IRT − Average IRT)  50  1. [sent-198, score-0.595]
</p><p>80 4  (d)  15  35  Mean IRT for the entry prior to switch  Order of entry relative to patch switch  (c)  10  5  0  3  5  40  50  −2  20  Abs(Last item IRT − Average IRT)  Number of words produced  0. [sent-199, score-0.709]
</p><p>81 2 1  35  Mean IRT for the entry prior to switch  Order of entry relative to patch switch  (b)  40  15  0  3  45  Number of words produced  20  1  Average IRT  Item IRT / Average IRT  25 1. [sent-202, score-0.595]
</p><p>82 The right-most column displays the relationship between a simulation’s deviation from the marginal value theorem policy for patch departures (horizontal axis) and the total number of words the simulation produced. [sent-213, score-0.265]
</p><p>83 7  emergence of the same phenomenon is seen across all four of our models which suggests that the structure of semantic memory, together with a simple undirected search process, is sufﬁcient to capture this effect. [sent-214, score-0.373]
</p><p>84 [7] on the models, demonstrating that, like people, all four models take a signiﬁcantly longer amount of time for the word immediately following a patch (all t(999) > 44, p < 0. [sent-217, score-0.211]
</p><p>85 0001) and take a signiﬁcantly shorter amount of time for the second item after a patch (all t(999) < −49, p < 0. [sent-218, score-0.257]
</p><p>86 Intriguingly, all four models produce the basic phenomena taken as evidence for the use of the marginal value theorem in memory search. [sent-221, score-0.251]
</p><p>87 There is a strong correlation between the IRT at the point of a cluster switch and the mean IRT (R2 = 0. [sent-222, score-0.173]
</p><p>88 These results show that behavior consistent with following the marginal value theorem can be produced by surprisingly simple search algorithms, at least when measured along these metrics. [sent-233, score-0.194]
</p><p>89 5  Discussion  Understanding how people organize and search through information stored in memory has the potential to inform how we construct automated information retrieval systems. [sent-234, score-0.356]
</p><p>90 In this paper, we considered two different accounts for the appearance of semantically-related clusters when people retrieve a sequence of items from memory. [sent-235, score-0.274]
</p><p>91 In contrast, the proposal that memory search might just be a random walk on a semantic network [6] postulates a single, undirected process. [sent-238, score-0.636]
</p><p>92 Our results show that four random walk models qualitatively reproduce a set of results predicted by optimal foraging theory, providing an alternative explanation for clustering in semantic ﬂuency tasks. [sent-239, score-0.625]
</p><p>93 Finding that a random walk on a semantic network can account for some of the relatively complex phenomena that appear in the semantic ﬂuency task provides further support for the idea that memory search might simply be a random walk. [sent-240, score-0.948]
</p><p>94 This result helps to clarify the possible mechanisms that could account for PageRank predicting the prominence of words in semantic memory [6], since PageRank is simply the stationary distribution of the Markov chain deﬁned by this random walk. [sent-241, score-0.491]
</p><p>95 Demonstrating that the random walk models can produce behavior consistent with optimal foraging in semantic ﬂuency tasks generates some interesting directions for future research. [sent-243, score-0.6]
</p><p>96 Having two competing accounts of the same phenomena suggests that the next step in exploring semantic ﬂuency is designing an experiment that distinguishes between these accounts. [sent-244, score-0.358]
</p><p>97 Considering whether the optimal foraging account can also predict the prominence of words in semantic memory, where the random walk model is already known to succeed, is one possibility, as is exploring the predictions of the two accounts across a wider range of memory search tasks. [sent-245, score-0.866]
</p><p>98 However, one of the most intriguing directions for future research is considering how these different proposals fare in accounting for changes in semantic ﬂuency in clinical populations. [sent-246, score-0.293]
</p><p>99 Word association spaces for predicting semantic similarity effects in episodic memory. [sent-373, score-0.302]
</p><p>100 The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. [sent-379, score-0.586]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('irt', 0.706), ('uency', 0.283), ('semantic', 0.276), ('hills', 0.197), ('animals', 0.145), ('foraging', 0.144), ('irts', 0.141), ('patch', 0.123), ('animal', 0.122), ('switch', 0.121), ('memory', 0.117), ('item', 0.114), ('walk', 0.11), ('people', 0.105), ('troyer', 0.082), ('search', 0.074), ('produced', 0.069), ('word', 0.065), ('pagerank', 0.063), ('participants', 0.063), ('clusters', 0.062), ('participant', 0.06), ('retrieval', 0.06), ('cat', 0.055), ('jumping', 0.054), ('human', 0.053), ('food', 0.053), ('cluster', 0.052), ('switching', 0.049), ('dog', 0.049), ('entry', 0.048), ('psychological', 0.046), ('words', 0.045), ('abs', 0.045), ('alzheimer', 0.043), ('items', 0.043), ('transition', 0.042), ('parkinson', 0.042), ('cue', 0.039), ('web', 0.039), ('internet', 0.039), ('xn', 0.038), ('retrieve', 0.037), ('network', 0.036), ('phenomena', 0.035), ('analyses', 0.034), ('steyvers', 0.034), ('grif', 0.034), ('clustering', 0.033), ('marginal', 0.033), ('et', 0.032), ('searching', 0.032), ('ths', 0.032), ('patchy', 0.031), ('average', 0.031), ('page', 0.03), ('surfer', 0.029), ('cits', 0.029), ('prominence', 0.029), ('psychology', 0.028), ('berkeley', 0.027), ('accounts', 0.027), ('jump', 0.027), ('produce', 0.026), ('similarity', 0.026), ('tasks', 0.026), ('house', 0.025), ('jumps', 0.025), ('account', 0.024), ('outgoing', 0.024), ('displays', 0.024), ('intriguingly', 0.024), ('moscovitch', 0.024), ('four', 0.023), ('proposal', 0.023), ('responses', 0.022), ('semantically', 0.022), ('walks', 0.021), ('preceding', 0.021), ('departures', 0.021), ('links', 0.02), ('exploring', 0.02), ('relative', 0.02), ('predicted', 0.02), ('shorter', 0.02), ('nodes', 0.019), ('processes', 0.019), ('neuropsychological', 0.019), ('subcategories', 0.019), ('huntington', 0.019), ('visited', 0.019), ('simulation', 0.019), ('explanation', 0.019), ('retrieving', 0.018), ('behavior', 0.018), ('evidence', 0.017), ('clinical', 0.017), ('disease', 0.017), ('relevant', 0.017), ('reproduced', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="155-tfidf-1" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>Author: Joseph L. Austerweil, Joshua T. Abbott, Thomas L. Griffiths</p><p>Abstract: The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. 1</p><p>2 0.10649782 <a title="155-tfidf-2" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>Author: Sung J. Hwang, Kristen Grauman, Fei Sha</p><p>Abstract: When learning features for complex visual recognition problems, labeled image exemplars alone can be insufﬁcient. While an object taxonomy specifying the categories’ semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classiﬁcation task, nor does a single taxonomy capture all ties that are relevant. In light of these issues, we propose a discriminative feature learning approach that leverages multiple hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reﬂect their phylogenic ties, while another could reﬂect their habitats). For each taxonomy, we ﬁrst learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes. Then, using the resulting semantic kernel forest, we learn class-speciﬁc kernel combinations to select only those relationships relevant to recognize each object class. To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies’ structure. We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields signiﬁcant accuracy improvements.</p><p>3 0.081621729 <a title="155-tfidf-3" href="./nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">165 nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>Author: Sahand Negahban, Sewoong Oh, Devavrat Shah</p><p>Abstract: The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR’s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, ﬁnding ‘scores’ for each object (e.g. player’s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efﬁcacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the ﬁnite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]. 1</p><p>4 0.072316825 <a title="155-tfidf-4" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>Author: Andriy Mnih, Yee W. Teh</p><p>Abstract: User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative ﬁltering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difﬁcult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative ﬁltering with implicit feedback based on modelling the user’s item selection process. In the interests of scalability, we restrict our attention to treestructured distributions over items and develop a principled and efﬁcient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data. 1</p><p>5 0.06786783 <a title="155-tfidf-5" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>6 0.061691254 <a title="155-tfidf-6" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>7 0.058813743 <a title="155-tfidf-7" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>8 0.057326119 <a title="155-tfidf-8" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>9 0.055960476 <a title="155-tfidf-9" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>10 0.051155321 <a title="155-tfidf-10" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>11 0.049821232 <a title="155-tfidf-11" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>12 0.048842516 <a title="155-tfidf-12" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>13 0.048743751 <a title="155-tfidf-13" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>14 0.04816445 <a title="155-tfidf-14" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>15 0.047234699 <a title="155-tfidf-15" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>16 0.046510007 <a title="155-tfidf-16" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>17 0.04472312 <a title="155-tfidf-17" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>18 0.043904904 <a title="155-tfidf-18" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>19 0.04219972 <a title="155-tfidf-19" href="./nips-2012-Learning_with_Partially_Absorbing_Random_Walks.html">196 nips-2012-Learning with Partially Absorbing Random Walks</a></p>
<p>20 0.041638717 <a title="155-tfidf-20" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.014), (2, -0.055), (3, -0.02), (4, -0.042), (5, -0.044), (6, -0.003), (7, 0.019), (8, 0.019), (9, 0.109), (10, -0.007), (11, -0.046), (12, -0.043), (13, -0.008), (14, 0.026), (15, -0.006), (16, -0.046), (17, 0.045), (18, -0.012), (19, -0.044), (20, -0.008), (21, -0.017), (22, -0.026), (23, -0.065), (24, 0.031), (25, -0.046), (26, 0.036), (27, -0.005), (28, 0.053), (29, 0.012), (30, 0.106), (31, 0.017), (32, -0.01), (33, -0.083), (34, 0.04), (35, 0.012), (36, 0.017), (37, 0.005), (38, -0.018), (39, -0.032), (40, 0.065), (41, 0.072), (42, -0.037), (43, 0.025), (44, -0.025), (45, -0.051), (46, 0.004), (47, -0.013), (48, 0.003), (49, -0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92569596 <a title="155-lsi-1" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>Author: Joseph L. Austerweil, Joshua T. Abbott, Thomas L. Griffiths</p><p>Abstract: The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. 1</p><p>2 0.54998481 <a title="155-lsi-2" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>Author: Andriy Mnih, Yee W. Teh</p><p>Abstract: User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative ﬁltering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difﬁcult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative ﬁltering with implicit feedback based on modelling the user’s item selection process. In the interests of scalability, we restrict our attention to treestructured distributions over items and develop a principled and efﬁcient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data. 1</p><p>3 0.54702747 <a title="155-lsi-3" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>4 0.51201653 <a title="155-lsi-4" href="./nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">165 nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>Author: Sahand Negahban, Sewoong Oh, Devavrat Shah</p><p>Abstract: The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g. MSR’s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining ranking, ﬁnding ‘scores’ for each object (e.g. player’s rating) is of interest to understanding the intensity of the preferences. In this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk. The algorithm is model independent. To establish the efﬁcacy of our method, however, we consider the popular Bradley-Terry-Luce (BTL) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects. We bound the ﬁnite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. This, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm. Indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the Maximum Likelihood Estimator of the BTL model and outperforms a recently proposed algorithm by Ammar and Shah [1]. 1</p><p>5 0.49695909 <a title="155-lsi-5" href="./nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">299 nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>Author: Lloyd Elliott, Yee W. Teh</p><p>Abstract: We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of partitions. The partitions at consecutive locations in the genome are related by the splitting and merging of their clusters. Our model can be thought of as a discrete analogue of the continuous fragmentation-coagulation process [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efﬁciency while maintaining accuracies comparable to other state-of-the-art genotype imputation methods. 1</p><p>6 0.47667903 <a title="155-lsi-6" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>7 0.46530613 <a title="155-lsi-7" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>8 0.46118578 <a title="155-lsi-8" href="./nips-2012-Assessing_Blinding_in_Clinical_Trials.html">46 nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>9 0.45557594 <a title="155-lsi-9" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>10 0.44417837 <a title="155-lsi-10" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>11 0.44109991 <a title="155-lsi-11" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>12 0.42962348 <a title="155-lsi-12" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>13 0.42926183 <a title="155-lsi-13" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>14 0.42682567 <a title="155-lsi-14" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>15 0.42298356 <a title="155-lsi-15" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>16 0.41197437 <a title="155-lsi-16" href="./nips-2012-Learning_with_Partially_Absorbing_Random_Walks.html">196 nips-2012-Learning with Partially Absorbing Random Walks</a></p>
<p>17 0.4114846 <a title="155-lsi-17" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>18 0.40685633 <a title="155-lsi-18" href="./nips-2012-A_Divide-and-Conquer_Method_for_Sparse_Inverse_Covariance_Estimation.html">7 nips-2012-A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation</a></p>
<p>19 0.40369993 <a title="155-lsi-19" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>20 0.40309367 <a title="155-lsi-20" href="./nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">291 nips-2012-Reducing statistical time-series problems to binary classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.041), (17, 0.012), (21, 0.021), (36, 0.014), (38, 0.123), (39, 0.013), (42, 0.021), (54, 0.027), (55, 0.403), (74, 0.039), (76, 0.095), (80, 0.06), (92, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83793145 <a title="155-lda-1" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>2 0.83678311 <a title="155-lda-2" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>3 0.83574384 <a title="155-lda-3" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>same-paper 4 0.83459395 <a title="155-lda-4" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>Author: Joseph L. Austerweil, Joshua T. Abbott, Thomas L. Griffiths</p><p>Abstract: The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. 1</p><p>5 0.75914252 <a title="155-lda-5" href="./nips-2012-Meta-Gaussian_Information_Bottleneck.html">211 nips-2012-Meta-Gaussian Information Bottleneck</a></p>
<p>Author: Melanie Rey, Volker Roth</p><p>Abstract: We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers. 1</p><p>6 0.67959827 <a title="155-lda-6" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>7 0.66079074 <a title="155-lda-7" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>8 0.55321074 <a title="155-lda-8" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>9 0.5459857 <a title="155-lda-9" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>10 0.54016781 <a title="155-lda-10" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>11 0.52949363 <a title="155-lda-11" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>12 0.52636224 <a title="155-lda-12" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>13 0.51749998 <a title="155-lda-13" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>14 0.51472932 <a title="155-lda-14" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>15 0.50152218 <a title="155-lda-15" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>16 0.50057191 <a title="155-lda-16" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>17 0.49960417 <a title="155-lda-17" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>18 0.49706486 <a title="155-lda-18" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>19 0.49542066 <a title="155-lda-19" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>20 0.49493483 <a title="155-lda-20" href="./nips-2012-Learning_optimal_spike-based_representations.html">190 nips-2012-Learning optimal spike-based representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
