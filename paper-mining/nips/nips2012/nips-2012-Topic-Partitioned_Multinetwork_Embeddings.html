<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>345 nips-2012-Topic-Partitioned Multinetwork Embeddings</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-345" href="#">nips2012-345</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>345 nips-2012-Topic-Partitioned Multinetwork Embeddings</h1>
<br/><p>Source: <a title="nips-2012-345-pdf" href="http://papers.nips.cc/paper/4659-topic-partitioned-multinetwork-embeddings.pdf">pdf</a></p><p>Author: Peter Krafft, Juston Moore, Bruce Desmarais, Hanna M. Wallach</p><p>Abstract: We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models. 1</p><p>Reference: <a title="nips-2012-345-reference" href="../nips2012_reference/nips-2012-Topic-Partitioned_Multinetwork_Embeddings_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. [sent-6, score-1.259]
</p><p>2 Our model produces principled visualizations of email networks, i. [sent-7, score-0.839]
</p><p>3 , visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. [sent-9, score-0.273]
</p><p>4 We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. [sent-10, score-0.253]
</p><p>5 We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. [sent-11, score-1.719]
</p><p>6 We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. [sent-12, score-1.291]
</p><p>7 Finally, we advocate for principled visualization as a primary objective in the development of new network models. [sent-13, score-0.282]
</p><p>8 1  Introduction  The structures of organizational communication networks are critical to collaborative problem solving [1]. [sent-14, score-0.368]
</p><p>9 Although it is seldom possible for researchers to directly observe complete organizational communication networks, email data sets provide one means by which they can at least partially observe and reason about them. [sent-15, score-1.007]
</p><p>10 As a result—and especially in light of their rich textual detail, existing infrastructure, and widespread usage—email data sets hold the potential to answer many important scientiﬁc and practical questions within the organizational and social sciences. [sent-16, score-0.147]
</p><p>11 While some questions may be answered by studying the structure of an email network as a whole, other, more nuanced, questions can only be answered at ﬁner levels of granularity—speciﬁcally, by studying topic-speciﬁc subnetworks. [sent-17, score-0.994]
</p><p>12 For example, breaks in communication (or duplicated communication) about particular topics may indicate a need for some form of organizational restructuring. [sent-18, score-0.45]
</p><p>13 In order to facilitate the study of these kinds of questions, we present a new Bayesian admixture model intended for discovering and summarizing topic-speciﬁc communication subnetworks in email data sets. [sent-19, score-1.149]
</p><p>14 There are a number of probabilistic models that incorporate both network and text data. [sent-20, score-0.146]
</p><p>15 Although some of these models are speciﬁcally for email networks (e. [sent-21, score-0.708]
</p><p>16 ’s author–recipient– topic model [2]), most are intended for networks of documents, such as web pages and the links between them [3] or academic papers and their citations [4]. [sent-24, score-0.259]
</p><p>17 In contrast, an email network is more naturally viewed as a network of actors exchanging documents, i. [sent-25, score-1.044]
</p><p>18 , actors are associated with nodes while documents are associated with edges. [sent-27, score-0.254]
</p><p>19 In other words, an email network deﬁnes a multinetwork in which there may be multiple edges (one per email) between any pair of actors. [sent-28, score-0.937]
</p><p>20 Instead, we take a complementary approach and focus on exploratory analysis. [sent-31, score-0.075]
</p><p>21 Speciﬁcally, our goal is to discover and visualize topic-speciﬁc subnetworks. [sent-32, score-0.088]
</p><p>22 If network modeling and visualization are undertaken separately, the resultant visualizations may not directly reﬂect the model and its relationship to the observed data. [sent-34, score-0.382]
</p><p>23 Rather, these visualizations provide a view of the model and the data seen through the lens of the visualization algorithm and its associated assumptions, so any conclusions drawn from such visualizations can be biased by artifacts of the visualization algorithm. [sent-35, score-0.461]
</p><p>24 , visualizations that have precise interpretations in terms of an associated network model and its relationship to the observed data, remains an open challenge in statistical network modeling [5]. [sent-38, score-0.559]
</p><p>25 Addressing this open challenge was a primary objective in the development of our new model. [sent-39, score-0.045]
</p><p>26 In order to discover and visualize topic-speciﬁc subnetworks, our model must associate each author– recipient edge in the observed email network with a topic, as shown in Figure 1. [sent-40, score-1.029]
</p><p>27 Our model draws upon ideas from latent Dirichlet allocation (LDA) [6] to identify a set of corpus-wide topics of communication, as well as the subset of topics that best describe each observed email. [sent-41, score-0.239]
</p><p>28 We model network structure using an approach similar to that of Hoff et al. [sent-42, score-0.111]
</p><p>29 ’s latent space model (LSM) [7] so as to facilitate visualization. [sent-43, score-0.024]
</p><p>30 Given an observed network, LSM associates each actor in the network with a point in K-dimensional Euclidean space. [sent-44, score-0.211]
</p><p>31 If K = 2 or K = 3, these interaction probabilities, collectively known as a “communication pattern”, can be directly visualized in 2- or 3-dimensional space via the locations of the actor-speciﬁc points. [sent-46, score-0.044]
</p><p>32 Our model extends this idea by associating a K-dimensional Euclidean space with each topic. [sent-47, score-0.034]
</p><p>33 Observed author–recipient edges are explicitly associated with topics via the K-dimensional topic-speciﬁc communication patterns. [sent-48, score-0.42]
</p><p>34 In the next section, we present the mathematical details of our new model and outline a corresponding inference algorithm. [sent-49, score-0.023]
</p><p>35 We then introduce a new email data set: the New Hanover County (NHC) email network. [sent-50, score-1.326]
</p><p>36 Although our model is intended for exploratory analysis, we test our modeling assumptions via three validation tasks. [sent-51, score-0.146]
</p><p>37 1, we show that our model achieves better link prediction performance than three state-of-the-art network models. [sent-53, score-0.111]
</p><p>38 We also demonstrate that our model is capable of inferring topics that are as coherent as those inferred using LDA. [sent-54, score-0.106]
</p><p>39 Together, these experiments indicate that our model is an appropriate model of network structure and that modeling this structure does not compromise topic quality. [sent-55, score-0.253]
</p><p>40 As a ﬁnal validation experiment, we show that synthetic data generated using our model possesses similar network statistics to those of the NHC email network. [sent-56, score-0.774]
</p><p>41 4, we showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using the NHC network. [sent-58, score-0.393]
</p><p>42 We give an extensive analysis of these communication patterns and demonstrate that they provide accessible visualizations of emailbased collaboration while possessing precise, meaningful interpretations within the mathematical framework of our model. [sent-59, score-0.492]
</p><p>43 These ﬁndings lead us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. [sent-60, score-1.053]
</p><p>44 Finally, we advocate for principled visualization as a primary objective in the development of new network models. [sent-61, score-0.282]
</p><p>45 2  Topic-Partitioned Multinetwork Embeddings  In this section, we present our new probabilistic generative model (and associated inference algorithm) for communication networks. [sent-62, score-0.313]
</p><p>46 For concreteness, we frame our discussion of this model in 2  terms of email data, although it is generally applicable to any similarly-structured communication data. [sent-63, score-0.901]
</p><p>47 The generative process and graphical model are provided in the supplementary materials. [sent-64, score-0.032]
</p><p>48 (d)  (d)  A single email, indexed by d, is represented by a set of tokens w(d) = {wn }N that comprise the n=1 text of that email, an integer a(d) ∈ {1, . [sent-65, score-0.119]
</p><p>49 , A} indicating the identity of that email’s author, and a (d) set of binary variables y (d) = {yr }A indicating whether each of the A actors in the network is r=1 a recipient of that email. [sent-68, score-0.36]
</p><p>50 For simplicity, we assume that authors do not send emails to themselves (d) (i. [sent-69, score-0.104]
</p><p>51 Given a real-world email data set D = {{w(d) , a(d) , y (d) }}D , our d=1 model permits inference of the topics expressed in the text of the emails, a set of topic-speciﬁc K-dimensional embeddings (i. [sent-72, score-0.844]
</p><p>52 , points in K-dimensional Euclidean space) of the A actors in the network, and a partition of the full communication network into a set of topic-speciﬁc subnetworks. [sent-74, score-0.489]
</p><p>53 A symmetric Dirichlet prior with concentration parameter β is placed over Φ = {φ(1) , . [sent-76, score-0.022]
</p><p>54 To capture the relationship between the topics expressed in an email and that email’s recipients, each topic t is also associated with a “communication pattern”: an A × A (t) matrix of probabilities P (t) . [sent-80, score-0.957]
</p><p>55 Given an email about topic t, authored by actor a, element par is the probability of actor a including actor r as a recipient of that email. [sent-81, score-1.176]
</p><p>56 Inspired by LSM [7], each communication pattern P (t) is represented implicitly via a set of A points in K-dimensional Euclidean (t) (t) (t) (t) (t) space S (t) = {sa }A and a scalar bias term b(t) such that par = pra = σ(b(t) − sa − sr ) a=1 (t) 2 2 with sa ∼ N (0, σ1 I) and b(t) ∼ N (µ, σ2 ). [sent-82, score-0.618]
</p><p>57 1 If K = 2 or K = 3, this representation enables each topic-speciﬁc communication pattern to be visualized in 2- or 3-dimensional space via the locations of the points associated with the A actors. [sent-83, score-0.347]
</p><p>58 In isolation, each point sa conveys no information; however, the distance between any two points has a precise and meaningful interpretation in the generative process. [sent-85, score-0.233]
</p><p>59 Speciﬁcally, the recipients of any email associated with topic t are more likely to be those actors near to the email’s author in the Euclidean space corresponding to that topic. [sent-86, score-1.138]
</p><p>60 Each email, indexed by d, has a discrete distribution over topics θ (d) . [sent-87, score-0.106]
</p><p>61 A symmetric Dirichlet prior (d) with concentration parameter α is placed over Θ = {θ (1) , . [sent-88, score-0.022]
</p><p>62 Each token wn is associated (d) (d) (d) (d) with a topic assignment zn , such that zn ∼ θ (d) and wn ∼ φ(t) for zn = t. [sent-92, score-0.765]
</p><p>63 Our model does not include a distribution over authors; the generative process is conditioned upon their identities. [sent-93, score-0.032]
</p><p>64 (d) The email-speciﬁc binary variables y (d) = {yr }A indicate the recipients of email d and thus the r=1 presence (or absence) of email-speciﬁc edges from author a(d) to each of the A − 1 other actors. [sent-94, score-0.867]
</p><p>65 Consequently, there may be multiple edges (one per email) between any pair of actors, and D deﬁnes a multinetwork over the entire set of actors. [sent-95, score-0.163]
</p><p>66 We assume that the complete multinetwork comprises T (d) topic-speciﬁc subnetworks. [sent-96, score-0.13]
</p><p>67 In other words, each yr is associated with some topic t and therefore (t) (d) with topic-speciﬁc communication pattern P (t) such that yr ∼ Bern(par ) for a(d) = a. [sent-97, score-1.206]
</p><p>68 A better approach, advocated (d) by Blei and Jordan, is to draw a topic assignment for each yr from the empirical distribution over (d) topics deﬁned by z . [sent-100, score-0.67]
</p><p>69 By deﬁnition, the set of topics associated with edges will therefore be a subset of the topics associated with tokens. [sent-101, score-0.331]
</p><p>70 One way of simulating this generative process is to associate (d) (d) each yr with a position n = 1, . [sent-102, score-0.477]
</p><p>71 , max (1, N (d) ) and therefore with the topic assignment zn at (d) (d) that position2 by drawing a position assignment xr ∼ U(1, . [sent-105, score-0.496]
</p><p>72 This (d) (t) (d) (d) indirect procedure ensures that yr ∼ Bern(par ) for a(d) = a, xr = n, and zn = t, as desired. [sent-109, score-0.658]
</p><p>73 , N (d) = 0) convey information about the frequencies of communication between their authors and recipients. [sent-113, score-0.276]
</p><p>74 As a result, we do not omit such emails from D; instead, we (d) (d) augment each one with a single, “dummy” topic assignment z1 for which there is no associated token w1 . [sent-114, score-0.336]
</p><p>75 1  Inference  For real-world data D = {w(d) , a(d) , y (d) }D , the tokens W = {w(d) }D , authors A = d=1 d=1 {a(d) }D , and recipients Y = {y (d) }D are observed, while Φ, Θ, S = {S (t) }T , B = {b(t) }T , t=1 t=1 d=1 d=1 Z = {z (d) }D , and X = {x(d) }D are unobserved. [sent-116, score-0.175]
</p><p>76 In this section, we outline a Metropolis-within-Gibbs sampling algorithm that operates by sequentially (t) (d) (d) resampling the value of each latent variable (i. [sent-118, score-0.023]
</p><p>77 , sa , bt , zn , or xr ) from its conditional posterior. [sent-120, score-0.395]
</p><p>78 Count N (t) is the total number of tokens in W assigned to topic t by Z, of which N (v|t) are of type v and N (t|d) (d) belong to email d. [sent-122, score-0.848]
</p><p>79 New values for discrete random variable xr may be sampled directly using (t)  (d) P (x(d) = n | A, Y, S, B, zn = t, Z\d,n ) ∝ (pa(d) r ) r  (d) yr  (t)  (1 − pa(d) r )  (d) 1−yr  . [sent-123, score-0.639]
</p><p>80 (t)  New values for continuous random variables sa and b(t) cannot be sampled directly from their conditional posteriors, but may instead be obtained using the Metropolis–Hastings algorithm. [sent-124, score-0.147]
</p><p>81 With (t) (t) (t) a non-informative prior over sa (i. [sent-125, score-0.147]
</p><p>82 Likewise, with an improper, noninformative prior over b(t) (i. [sent-129, score-0.021]
</p><p>83 , b(t) ∼ N (0, ∞)), the conditional posterior over b(t) is A  P (b(t) | A, Y, S (t) , Z, X ) ∝  (p(t) ) ar  N (1|a,r,t) +N (1|r,a,t)  N (0|a,r,t) +N (0|r,a,t)  (1 − p(t) ) ar  . [sent-131, score-0.106]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('email', 0.663), ('yr', 0.391), ('communication', 0.238), ('sa', 0.147), ('visualizations', 0.14), ('actors', 0.14), ('subnetworks', 0.138), ('multinetwork', 0.13), ('zn', 0.128), ('topic', 0.121), ('xr', 0.12), ('network', 0.111), ('recipient', 0.109), ('topics', 0.106), ('recipients', 0.092), ('emails', 0.085), ('organizational', 0.085), ('author', 0.079), ('lsm', 0.078), ('nhc', 0.078), ('exploratory', 0.075), ('actor', 0.073), ('wn', 0.065), ('par', 0.064), ('tokens', 0.064), ('visualization', 0.059), ('ar', 0.053), ('assignment', 0.052), ('county', 0.052), ('desmarais', 0.052), ('intended', 0.05), ('interpretations', 0.049), ('visualize', 0.049), ('pa', 0.047), ('answered', 0.046), ('networks', 0.045), ('visualized', 0.044), ('associated', 0.043), ('bern', 0.042), ('questions', 0.042), ('dirichlet', 0.042), ('embeddings', 0.04), ('showcase', 0.04), ('hanover', 0.04), ('discover', 0.039), ('wallach', 0.036), ('admixture', 0.036), ('euclidean', 0.036), ('principled', 0.036), ('token', 0.035), ('text', 0.035), ('associating', 0.034), ('precise', 0.033), ('edges', 0.033), ('generative', 0.032), ('recommend', 0.032), ('amherst', 0.032), ('associate', 0.031), ('advocate', 0.031), ('documents', 0.028), ('patterns', 0.027), ('observed', 0.027), ('primary', 0.024), ('facilitate', 0.024), ('relationship', 0.024), ('outline', 0.023), ('massachusetts', 0.023), ('nuanced', 0.023), ('infrastructure', 0.023), ('position', 0.023), ('links', 0.022), ('producing', 0.022), ('pattern', 0.022), ('studying', 0.022), ('lda', 0.022), ('placed', 0.022), ('blei', 0.021), ('seldom', 0.021), ('noninformative', 0.021), ('citations', 0.021), ('duplicated', 0.021), ('hanna', 0.021), ('hoff', 0.021), ('conveys', 0.021), ('modeling', 0.021), ('development', 0.021), ('lens', 0.02), ('textual', 0.02), ('comprise', 0.02), ('bruce', 0.02), ('dummy', 0.02), ('count', 0.019), ('partitions', 0.019), ('indirect', 0.019), ('possessing', 0.019), ('convey', 0.019), ('collaboration', 0.019), ('csail', 0.019), ('exchanging', 0.019), ('authors', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="345-tfidf-1" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>Author: Peter Krafft, Juston Moore, Bruce Desmarais, Hanna M. Wallach</p><p>Abstract: We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models. 1</p><p>2 0.091757804 <a title="345-tfidf-2" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>3 0.089007281 <a title="345-tfidf-3" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>Author: Doina Precup, Joelle Pineau, Andre S. Barreto</p><p>Abstract: Kernel-based stochastic factorization (KBSF) is an algorithm for solving reinforcement learning tasks with continuous state spaces which builds a Markov decision process (MDP) based on a set of sample transitions. What sets KBSF apart from other kernel-based approaches is the fact that the size of its MDP is independent of the number of transitions, which makes it possible to control the trade-off between the quality of the resulting approximation and the associated computational cost. However, KBSF’s memory usage grows linearly with the number of transitions, precluding its application in scenarios where a large amount of data must be processed. In this paper we show that it is possible to construct KBSF’s MDP in a fully incremental way, thus freeing the space complexity of this algorithm from its dependence on the number of sample transitions. The incremental version of KBSF is able to process an arbitrary amount of data, which results in a model-based reinforcement learning algorithm that can be used to solve continuous MDPs in both off-line and on-line regimes. We present theoretical results showing that KBSF can approximate the value function that would be computed by conventional kernel-based learning with arbitrary precision. We empirically demonstrate the effectiveness of the proposed algorithm in the challenging threepole balancing task, in which the ability to process a large number of transitions is crucial for success. 1</p><p>4 0.075752199 <a title="345-tfidf-4" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><p>5 0.074864723 <a title="345-tfidf-5" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>Author: Charles Blundell, Jeff Beck, Katherine A. Heller</p><p>Abstract: We present a Bayesian nonparametric model that discovers implicit social structure from interaction time-series data. Social groups are often formed implicitly, through actions among members of groups. Yet many models of social networks use explicitly declared relationships to infer social structure. We consider a particular class of Hawkes processes, a doubly stochastic point process, that is able to model reciprocity between groups of individuals. We then extend the Inﬁnite Relational Model by using these reciprocating Hawkes processes to parameterise its edges, making events associated with edges co-dependent through time. Our model outperforms general, unstructured Hawkes processes as well as structured Poisson process-based models at predicting verbal and email turn-taking, and military conﬂicts among nations. 1</p><p>6 0.069354124 <a title="345-tfidf-6" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>7 0.067945167 <a title="345-tfidf-7" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>8 0.065050565 <a title="345-tfidf-8" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>9 0.064690292 <a title="345-tfidf-9" href="./nips-2012-Distributed_Non-Stochastic_Experts.html">102 nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>10 0.062716998 <a title="345-tfidf-10" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>11 0.055042353 <a title="345-tfidf-11" href="./nips-2012-Globally_Convergent_Dual_MAP_LP_Relaxation_Solvers_using_Fenchel-Young_Margins.html">143 nips-2012-Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins</a></p>
<p>12 0.054343082 <a title="345-tfidf-12" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>13 0.050894149 <a title="345-tfidf-13" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>14 0.050536133 <a title="345-tfidf-14" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>15 0.048409481 <a title="345-tfidf-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.048364189 <a title="345-tfidf-16" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>17 0.046968918 <a title="345-tfidf-17" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>18 0.040148623 <a title="345-tfidf-18" href="./nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">59 nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<p>19 0.03586074 <a title="345-tfidf-19" href="./nips-2012-On_Triangular_versus_Edge_Representations_---_Towards_Scalable_Modeling_of_Networks.html">253 nips-2012-On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks</a></p>
<p>20 0.034713555 <a title="345-tfidf-20" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (1, 0.03), (2, -0.036), (3, -0.001), (4, -0.101), (5, -0.011), (6, -0.011), (7, -0.026), (8, 0.017), (9, -0.0), (10, 0.087), (11, 0.07), (12, 0.009), (13, -0.002), (14, 0.03), (15, 0.034), (16, 0.003), (17, -0.012), (18, -0.015), (19, 0.057), (20, 0.042), (21, 0.006), (22, 0.035), (23, -0.074), (24, 0.011), (25, -0.064), (26, 0.049), (27, 0.05), (28, 0.004), (29, -0.005), (30, 0.067), (31, -0.008), (32, 0.001), (33, 0.05), (34, 0.051), (35, -0.035), (36, -0.002), (37, 0.038), (38, -0.04), (39, 0.022), (40, -0.001), (41, 0.032), (42, -0.018), (43, -0.018), (44, -0.031), (45, 0.119), (46, 0.082), (47, -0.034), (48, -0.025), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93064553 <a title="345-lsi-1" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>Author: Peter Krafft, Juston Moore, Bruce Desmarais, Hanna M. Wallach</p><p>Abstract: We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models. 1</p><p>2 0.74207002 <a title="345-lsi-2" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>Author: Kosuke Fukumasu, Koji Eguchi, Eric P. Xing</p><p>Abstract: Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. 1</p><p>3 0.6369738 <a title="345-lsi-3" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><p>4 0.63503242 <a title="345-lsi-4" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>5 0.63216716 <a title="345-lsi-5" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>6 0.59446621 <a title="345-lsi-6" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>7 0.57404208 <a title="345-lsi-7" href="./nips-2012-On_Triangular_versus_Edge_Representations_---_Towards_Scalable_Modeling_of_Networks.html">253 nips-2012-On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks</a></p>
<p>8 0.54456633 <a title="345-lsi-8" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>9 0.54058439 <a title="345-lsi-9" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>10 0.52192831 <a title="345-lsi-10" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>11 0.50652719 <a title="345-lsi-11" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>12 0.50640464 <a title="345-lsi-12" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>13 0.49732319 <a title="345-lsi-13" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>14 0.42727387 <a title="345-lsi-14" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>15 0.41854125 <a title="345-lsi-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.40942007 <a title="345-lsi-16" href="./nips-2012-Analog_readout_for_optical_reservoir_computers.html">39 nips-2012-Analog readout for optical reservoir computers</a></p>
<p>17 0.40356219 <a title="345-lsi-17" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>18 0.38708407 <a title="345-lsi-18" href="./nips-2012-Learning_to_Discover_Social_Circles_in_Ego_Networks.html">194 nips-2012-Learning to Discover Social Circles in Ego Networks</a></p>
<p>19 0.38088149 <a title="345-lsi-19" href="./nips-2012-Topology_Constraints_in_Graphical_Models.html">346 nips-2012-Topology Constraints in Graphical Models</a></p>
<p>20 0.35505572 <a title="345-lsi-20" href="./nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">132 nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.082), (17, 0.019), (21, 0.026), (38, 0.069), (39, 0.014), (41, 0.344), (42, 0.017), (54, 0.033), (55, 0.05), (74, 0.047), (76, 0.097), (80, 0.08), (92, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7025578 <a title="345-lda-1" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>Author: Peter Krafft, Juston Moore, Bruce Desmarais, Hanna M. Wallach</p><p>Abstract: We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models. 1</p><p>2 0.68872595 <a title="345-lda-2" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>Author: Yuxuan Wang, Deliang Wang</p><p>Abstract: While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random ﬁelds (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classiﬁcation allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.</p><p>3 0.49454722 <a title="345-lda-3" href="./nips-2012-Proper_losses_for_learning_from_partial_labels.html">280 nips-2012-Proper losses for learning from partial labels</a></p>
<p>Author: Jesús Cid-sueiro</p><p>Abstract: This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, we establish a necessary and sufﬁcient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. The full knowledge of this matrix is not required, and losses can be constructed that are proper for a wide set of mixing probability matrices. 1</p><p>4 0.44290078 <a title="345-lda-4" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><p>5 0.44172552 <a title="345-lda-5" href="./nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</a></p>
<p>Author: Aaron Dennis, Dan Ventura</p><p>Abstract: The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difﬁcult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture signiﬁcantly improves its performance compared to using a previously-proposed static architecture. 1</p><p>6 0.43993607 <a title="345-lda-6" href="./nips-2012-Multiresolution_Gaussian_Processes.html">233 nips-2012-Multiresolution Gaussian Processes</a></p>
<p>7 0.43458331 <a title="345-lda-7" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>8 0.43456542 <a title="345-lda-8" href="./nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">270 nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>9 0.43318114 <a title="345-lda-9" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>10 0.43128598 <a title="345-lda-10" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>11 0.42981571 <a title="345-lda-11" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>12 0.42907718 <a title="345-lda-12" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>13 0.42832598 <a title="345-lda-13" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>14 0.42808068 <a title="345-lda-14" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>15 0.42727476 <a title="345-lda-15" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>16 0.42682344 <a title="345-lda-16" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>17 0.42661932 <a title="345-lda-17" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>18 0.42589715 <a title="345-lda-18" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>19 0.422959 <a title="345-lda-19" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>20 0.42282131 <a title="345-lda-20" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
