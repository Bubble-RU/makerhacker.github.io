<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-249" href="#">nips2012-249</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</h1>
<br/><p>Source: <a title="nips-2012-249-pdf" href="http://papers.nips.cc/paper/4588-nystrom-method-vs-random-fourier-features-a-theoretical-and-empirical-comparison.pdf">pdf</a></p><p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><p>Reference: <a title="nips-2012-249-reference" href="../nips2012_reference/nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. [sent-6, score-0.368]
</p><p>2 In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. [sent-7, score-0.095]
</p><p>3 Unlike approaches based on random Fourier features where the basis functions (i. [sent-8, score-0.175]
</p><p>4 , cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. [sent-10, score-0.255]
</p><p>5 By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. [sent-11, score-0.529]
</p><p>6 One limitation of kernel methods is their high computational cost, which is at least quadratic in the number of training examples, due to the calculation of kernel matrix. [sent-15, score-0.457]
</p><p>7 , incomplete Cholesky decomposition [3]) have been used to alleviate the computational challenge of kernel methods, they still require computing the kernel matrix. [sent-18, score-0.459]
</p><p>8 Other approaches such as online learning [9] and budget learning [7] have also been developed for large-scale kernel learning, but they tend to yield performance worse performance than batch learning. [sent-19, score-0.235]
</p><p>9 To avoid computing kernel matrix, one common approach is to approximate a kernel learning problem with a linear prediction problem. [sent-20, score-0.467]
</p><p>10 It is often achieved by generating a vector representation of data that approximates the kernel similarity between any two data points. [sent-21, score-0.239]
</p><p>11 The most well known approaches in this category are random Fourier features [13, 14] and the Nystr¨ m method [20, 8]. [sent-22, score-0.169]
</p><p>12 The objective of this work is to understand the difference between these two approaches, both theoretically and empirically The theoretical foundation for random Fourier transform is that a shift-invariant kernel is the Fourier transform of a non-negative measure [15]. [sent-24, score-0.324]
</p><p>13 Analysis in [14] shows that, the generalization error bound for kernel learning based on random Fourier features is given by O(N −1/2 + m−1/2 ), where N is the number of training examples and m is the number of sampled Fourier components. [sent-26, score-0.517]
</p><p>14 1  An alternative approach for large-scale kernel classiﬁcation is the Nystr¨ m method [20, 8] that o approximates the kernel matrix by a low rank matrix. [sent-27, score-0.574]
</p><p>15 It randomly samples a subset of training examples and computes a kernel matrix K for the random samples. [sent-28, score-0.369]
</p><p>16 It then represents each data point by a vector based on its kernel similarity to the random samples and the sampled kernel matrix K. [sent-29, score-0.567]
</p><p>17 Most analysis of the Nystr¨ m method follows [8] and bounds the error in approximating the o kernel matrix. [sent-30, score-0.302]
</p><p>18 According to [8], the approximation error of the Nystr¨ m method, measured in o spectral norm 1 , is O(m−1/2 ), where m is the number of sampled training examples. [sent-31, score-0.152]
</p><p>19 Using the arguments in [6], we expected an additional error of O(m−1/2 ) in the generalization performance caused by the approximation of the Nystr¨ m method, similar to random Fourier features. [sent-32, score-0.19]
</p><p>20 o Contributions In this work, we ﬁrst establish a uniﬁed framework for both methods from the viewpoint of functional approximation. [sent-33, score-0.069]
</p><p>21 This is important because random Fourier features and the Nystr¨ m method address large-scale kernel learning very differently: random Fourier features aim o to approximate the kernel function directly while the Nystr¨ m method is designed to approximate o the kernel matrix. [sent-34, score-1.019]
</p><p>22 By exploring this difference, we show that the additional error caused by the Nystr¨ m method in the generalization performance can be o improved to O(1/m) when there is a large gap in the eigen-spectrum of the kernel matrix. [sent-36, score-0.421]
</p><p>23 , (xN , yN )} be a collection of N training examples, where xi ∈ X ⊆ Rd , yi ∈ Y. [sent-41, score-0.084]
</p><p>24 Let κ(·, ·) be a kernel function, Hκ denote the endowed Reproducing Kernel Hilbert Space, and K = [κ(xi , xj )]N ×N be the kernel matrix for the samples in D. [sent-42, score-0.518]
</p><p>25 , N be the eigenvalues and eigenvectors of K ranked in the descending order of eigenvalues. [sent-47, score-0.201]
</p><p>26 , xm } denote the randomly o sampled examples, K = [κ(xi , xj )]m×m denote the corresponding kernel matrix. [sent-55, score-0.295]
</p><p>27 According to [18], the eigenvalues of LN and Lm are λi /N, i ∈ [N ] and λi /m, i ∈ [m], respectively, and their corresponding normalized eigenfunctions ϕj , j ∈ [N ] and ϕj , j ∈ [m] are given by ϕj (·) =  1 λj  N  Vi,j κ(xi , ·), j ∈ [N ],  ϕj (·) =  1  m  Vi,j κ(xi , ·), j ∈ [m]. [sent-64, score-0.151]
</p><p>28 (2)  λj i=1  i=1  ¯ To make our discussion concrete, we focus on the RBF kernel 2 , i. [sent-65, score-0.217]
</p><p>29 Our goal is to efﬁciently learn a kernel prediction function by solving the following optimization problem: min  f ∈HD  λ f 2  2 Hκ  +  1 N  N  (f (xi ), yi ),  (3)  i=1  1  We choose the bound based on spectral norm according to the discussion in [6]. [sent-68, score-0.309]
</p><p>30 The improved bound obtained in the paper for the Nystrom method is valid for any kernel matrix that satisﬁes the eigengap condition. [sent-69, score-0.448]
</p><p>31 , κ(xN , ·)) is a span over all the training examples 3 , and (z, y) is a convex loss function with respect to z. [sent-73, score-0.08]
</p><p>32 The high computational cost of kernel learning arises from the fact that we have to search for an optimal classiﬁer f (·) in a large space HD . [sent-75, score-0.217]
</p><p>33 Given this observation, to alleviate the computational cost of kernel classiﬁcation, we can reduce space HD to a smaller space Ha , and only search for the solution f (·) ∈ Ha . [sent-76, score-0.242]
</p><p>34 Below we show that the difference between random Fourier features and the Nystr¨ m method lies in the construction of the approximate space Ha . [sent-79, score-0.209]
</p><p>35 For o each method, we begin with a description of a vector representation of data, and then connect the vector representation to the approximate large kernel machine by functional approximation. [sent-80, score-0.321]
</p><p>36 Random Fourier Features The random Fourier features are constructed by ﬁrst sampling Fourier components u1 , . [sent-81, score-0.117]
</p><p>37 , um separately, and then passing them through sine and cosine functions, i. [sent-87, score-0.101]
</p><p>38 Given the random Fourier features, we then learn a linear machine f (x) = w zf (x) by solving the following optimization problem: min 2m  w∈R  λ w 2  2 2  +  N  1 N  (w zf (xi ), yi ). [sent-93, score-0.203]
</p><p>39 (4)  i=1  To connect the linear machine (4) to the kernel machine in (3) by a functional approximation, we can f construct a functional space Ha = span(s1 (·), c1 (·), . [sent-94, score-0.339]
</p><p>40 (5)  i=1  The following proposition connects the approximate kernel machine in (5) to the linear machine in (4). [sent-99, score-0.271]
</p><p>41 Proposition 1 The approximate kernel machine in (5) is equivalent to the following linear machine min 2m  w∈R  λ 1 w (w ◦ γ) + 2 N s/c  c s c s where γ = (γ1 , γ1 , · · · , γm , γm ) and γi  N  (w zf (xi ), yi ),  (6)  i=1  = exp(σ 2 ui 2 /2). [sent-101, score-0.344]
</p><p>42 2  Comparing (6) to the linear machine based on random Fourier features in (4), we can see that other s/c than the weights {γi }m , random Fourier features can be viewed as to approximate (3) by rei=1 f stricting the solution f (·) to Ha . [sent-102, score-0.267]
</p><p>43 It is straightforward to verify that zn (xi ) zn (xj ) = [Kr ]ij . [sent-114, score-0.142]
</p><p>44 Given the vector representation zn (x), we then learn a linear machine f (x) = w zn (x) by solving the following optimization problem: λ min w w∈Rr 2 3  2 2  1 + N  N  (w zn (xi ), yi ). [sent-115, score-0.184]
</p><p>45 3  (7)  In order to see how the Nystr¨ m method can be cast into the uniﬁed framework of approximating the o large scale kernel machine by functional approximation, we construct the following functional space n Ha = span(ϕ1 , . [sent-117, score-0.374]
</p><p>46 The following proposition shows that the linear machine in (7) using the vector representation of the Nystr¨ m method is equivalent to the approximate kernel machine in (3) by restricting the o n solution f (·) to an approximate functional space Ha . [sent-124, score-0.389]
</p><p>47 In particular, the basis functions used by random Fourier features are sampled from a Gaussian distribution that is independent from the training examples. [sent-126, score-0.21]
</p><p>48 In contrast, the basis functions used by the Nystr¨ m method are sampled from the training examples and are therefore data dependent. [sent-127, score-0.153]
</p><p>49 , the ﬁrst few eigenvalues of the full kernel matrix are much larger than the remaining eigenvalues, the classiﬁcation performance is mostly determined by the top eigenvectors. [sent-131, score-0.332]
</p><p>50 Since the Nystr¨ m method uses a data dependent sampling method, it is able to discover the o subspace spanned by the top eigenvectors using a small number of samples. [sent-132, score-0.127]
</p><p>51 In contrast, since random Fourier features are drawn from a distribution independent from training data, it may require a large number of samples before it can discover this subspace. [sent-133, score-0.175]
</p><p>52 As a result, we expect a signiﬁcantly lower generalization error for the Nystr¨ m method. [sent-134, score-0.075]
</p><p>53 The σ value in the RBF kernel is chosen by cross-validation and is set to 6 for the synthetic data. [sent-141, score-0.245]
</p><p>54 According to the results shown in Figure 1(c), it is clear that the Nystr¨ m o method performs signiﬁcantly better than random Fourier features. [sent-144, score-0.074]
</p><p>55 By using only 100 samples, the Nystr¨ m method is able to make perfect prediction, while the decision made by random Fourier feao tures based method is close to random guess. [sent-145, score-0.148]
</p><p>56 To evaluate the approximation error of the functional space, we plot in Figure 1(e) and 1(f), respectively, the ﬁrst two eigenvectors of the approximate kernel matrix computed by the Nystr¨ m method and random Fourier features using 100 samples. [sent-146, score-0.61]
</p><p>57 o Compared to the eigenvectors computed from the full kernel matrix (Figure 1(d)), we can see that the Nystr¨ m method achieves a signiﬁcantly better approximation of the ﬁrst two eigenvectors than o random Fourier features. [sent-147, score-0.488]
</p><p>58 Finally, we note that although the concept of eigengap has been exploited in many studies of kernel learning [2, 12, 1, 17], to the best of our knowledge, this is the ﬁrst time it has been incorporated in the analysis for approximate large-scale kernel learning. [sent-148, score-0.578]
</p><p>59 3  Main Theoretical Result  ∗ ∗ Let fm be the optimal solution to the approximate kernel learning problem in (8), and let fN be the ∗ solution to the full version of kernel learning in (3). [sent-149, score-0.559]
</p><p>60 04 0  2000  4000  6000  8000  5  10  20  50  # random samples  100  (c) Classiﬁcation accuracy vs the number of samples  0. [sent-175, score-0.128]
</p><p>61 Let ε as the solution to ε2 = ψ(ε) where the existence and uniqueness  of ε are determined by the sub-root property of ψ(δ) [4], and  = max ε,  6 ln N N  . [sent-194, score-0.172]
</p><p>62 According  to [10], we have 2 = O(N −1/2 ), and when the eigenvalues of kernel function follow a p-power law, ∗ ∗ it is improved to 2 = O(N −p/(p+1) ). [sent-195, score-0.333]
</p><p>63 Theorem 1 For 16 2 e−2N ≤ λ ≤ 1, λr+1 = O(N/m) and 2 ln(2N 3 ) + m  (λr − λr+1 )/N = Ω(1) ≥ 3  2 ln(2N 3 ) m  ,  with a probability 1 − 3N −3 , we have ∗ Λ(fm ) ≤  ∗ 3Λ(fN ) +  1 O λ  2  +  1 m  ,  where O(·) suppresses the polynomial term of ln N . [sent-198, score-0.172]
</p><p>64 Theorem 1 shows that the additional error caused by the approximation of the Nystr¨ m method is o improved to O(1/m) when there is a large gap between λr and λr+1 . [sent-199, score-0.19]
</p><p>65 Note that the improvement √ from O(1/ m) to O(1/m) is very signiﬁcant from the theoretical viewpoint, because it is well known that the generalization error for kernel learning is O(N −1/2 ) [4]5 . [sent-200, score-0.292]
</p><p>66 As a result, to achieve a similar performance as the standard kernel learning, the number of required samples has to be 5 It is possible to achieve a better generalization error bound of O(N −p/(p+1) ) by assuming the eigenvalues of kernel matrix follow a p-power law [10]. [sent-201, score-0.712]
</p><p>67 However, large eigengap doest not immediately indicate power law distribution for eigenvalues and and consequently a better generalization error. [sent-202, score-0.267]
</p><p>68 5  √ O(N ) if the additional error caused by the kernel approximation is bounded by O(1/ m), leading to a high computational cost. [sent-203, score-0.344]
</p><p>69 On the other hand, with O(1/m) bound for the additional error caused √ by the kernel approximation, the number of required samples is reduced to N , making it more practical for large-scale kernel learning. [sent-204, score-0.572]
</p><p>70 n We also note that the improvement made for the Nystr¨ m method relies on the property that Ha ⊂ o HD and therefore requires data dependent basis functions. [sent-205, score-0.098]
</p><p>71 We ﬁrst present a theorem to show that the excessive risk bound of fm is related to the matrix approximation error K − Kr 2 . [sent-209, score-0.247]
</p><p>72 Using the eigenfunctions of Lm and LN , we deﬁne two linear operators Hr and Hr as r  Hr [f ](·) =  r  ϕi (·) ϕi , f  Hκ ,  Hr [f ](·) =  i=1  ϕi (·) ϕi , f  Hκ ,  (10)  i=1  where f ∈ Hκ . [sent-213, score-0.09]
</p><p>73 1/2  1/2  Given the result in Theorem 3, we move to bound the spectral norm of LN ∆HLN . [sent-216, score-0.067]
</p><p>74 To this end, we assume a sufﬁciently large eigengap ∆ = (λr − λr+1 )/N . [sent-217, score-0.111]
</p><p>75 The theorem below bounds 1/2 1/2 LN ∆HLN 2 using matrix perturbation theory [19]. [sent-218, score-0.084]
</p><p>76 5  Empirical Studies  To verify our theoretical ﬁndings, we evaluate the empirical performance of the Nystr¨ m method o and random Fourier features for large-scale kernel learning. [sent-228, score-0.404]
</p><p>77 Note that datasets C PU, C ENSUS, A DULT and F OREST were originally used in [13] to verify the effectiveness of random Fourier features. [sent-230, score-0.076]
</p><p>78 A RBF kernel is used for both methods and for all the datasets. [sent-237, score-0.217]
</p><p>79 , C OVTYPE and F OREST), we restrict the maximum number of random samples to 200 because of the high computational cost. [sent-244, score-0.075]
</p><p>80 We observed that for all the data sets, the Nystr¨ m method outperforms random Fourier features 6 . [sent-245, score-0.151]
</p><p>81 Moreover, except for C OVTYPE with 10 o random samples, the Nystr¨ m method performs signiﬁcantly better than random Fourier features, o according to t-tests at 95% signiﬁcance level. [sent-246, score-0.114]
</p><p>82 We ﬁnally evaluate that whether the large eigengap condition, the key assumption for our main theoretical result, holds for the data sets. [sent-247, score-0.111]
</p><p>83 Due to the large size, except for C PU, we compute the eigenvalues of kernel matrix based on 10, 000 randomly selected examples from each dataset. [sent-248, score-0.358]
</p><p>84 As shown in Figure 3 (eigenvalues are in logarithm scale), we observe that the eigenvalues drop very quickly as the rank increases, leading to a signiﬁcant gap between the top eigenvalues and the remaining eigenvalues. [sent-249, score-0.274]
</p><p>85 6  Conclusion and Discussion  We study two methods for large-scale kernel learning, i. [sent-250, score-0.217]
</p><p>86 , the Nystr¨ m method and random Fourier o features. [sent-252, score-0.074]
</p><p>87 One key difference between these two approaches is that the Nystr¨ m method uses data o 6 We note that the classiﬁcation performance of A DULT data set reported in Figure 2 does not match with the performance reported in [13]. [sent-253, score-0.077]
</p><p>88 5  COVTYPE  accuracy(%)  accuracy(%)  90  80  2  # random samples COD_RNA  100  2. [sent-268, score-0.075]
</p><p>89 5  0  50 100 200 500 1000  90  Nystrom Method Random Fourier Features  accuracy(%)  mean square error  mean square error  3 Nystrom Method Random Fourier Features  2  0  ADULT  CENSUS  2. [sent-269, score-0.096]
</p><p>90 5  20  50  100  # random samples  70 65 60  Nystrom Method Random Fourier Features 10  Nystrom Method Random Fourier Features  55  200  10  20  50  100  # random samples  200  Figure 2: Comparison of the Nymstr¨ m method and random Fourier features. [sent-270, score-0.224]
</p><p>91 6N  Figure 3: The eigenvalue distributions of kernel matrices. [sent-298, score-0.217]
</p><p>92 dependent basis functions while random Fourier features introduce data independent basis functions. [sent-300, score-0.221]
</p><p>93 This difference leads to an improved analysis for kernel learning approaches based on the Nystr¨ m o method. [sent-301, score-0.289]
</p><p>94 We show that when there is a large eigengap of kernel matrix, the approximation error of Nystr¨ m method can be improved to O(1/m), leading to a signiﬁcantly better generalization o performance than random Fourier features. [sent-302, score-0.559]
</p><p>95 As implied from our study, it is important to develop data dependent basis functions for large-scale kernel learning. [sent-304, score-0.281]
</p><p>96 One direction we plan to explore is to improve random Fourier features by making the sampling data dependent. [sent-305, score-0.117]
</p><p>97 This can be achieved by introducing a rejection procedure that rejects the sample Fourier components when they do not align well with the top eigenfunctions estimated from the sampled data. [sent-306, score-0.094]
</p><p>98 On the impact of kernel approximation on learning accuracy. [sent-343, score-0.268]
</p><p>99 On the nystrom method for approximating a gram matrix for improved kernel-based learning. [sent-355, score-0.347]
</p><p>100 Using the nystrom method to speed up kernel machines. [sent-420, score-0.486]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nystr', 0.583), ('fourier', 0.465), ('nystrom', 0.235), ('kernel', 0.217), ('kr', 0.204), ('lm', 0.177), ('ln', 0.172), ('ha', 0.17), ('eigengap', 0.111), ('hd', 0.11), ('hs', 0.107), ('hln', 0.103), ('fm', 0.092), ('hr', 0.09), ('eigenvalues', 0.087), ('features', 0.077), ('eigenvectors', 0.069), ('zf', 0.069), ('eigenfunctions', 0.064), ('eigenvector', 0.062), ('covtype', 0.059), ('fn', 0.057), ('rank', 0.056), ('zn', 0.053), ('um', 0.052), ('census', 0.052), ('functional', 0.051), ('generalization', 0.045), ('adult', 0.045), ('kb', 0.045), ('caused', 0.044), ('vr', 0.043), ('basis', 0.04), ('random', 0.04), ('dult', 0.039), ('orest', 0.039), ('ovtype', 0.039), ('spectral', 0.038), ('theorem', 0.037), ('verify', 0.036), ('xi', 0.036), ('samples', 0.035), ('puted', 0.034), ('method', 0.034), ('uni', 0.034), ('approximate', 0.033), ('rahimi', 0.032), ('span', 0.031), ('approximation', 0.031), ('sin', 0.03), ('error', 0.03), ('rbf', 0.03), ('cos', 0.03), ('forest', 0.03), ('pu', 0.03), ('sampled', 0.03), ('improved', 0.029), ('bound', 0.029), ('sine', 0.028), ('vij', 0.028), ('matrix', 0.028), ('synthetic', 0.028), ('xm', 0.027), ('examples', 0.026), ('descending', 0.026), ('owing', 0.026), ('operators', 0.026), ('cpu', 0.026), ('yi', 0.025), ('difference', 0.025), ('alleviate', 0.025), ('dependent', 0.024), ('law', 0.024), ('mohri', 0.023), ('classi', 0.023), ('training', 0.023), ('gap', 0.022), ('approximates', 0.022), ('leading', 0.022), ('dr', 0.022), ('cosine', 0.021), ('transform', 0.021), ('approximating', 0.021), ('xj', 0.021), ('libsvm', 0.021), ('rademacher', 0.021), ('proposition', 0.021), ('impact', 0.02), ('connect', 0.02), ('perturbation', 0.019), ('ranked', 0.019), ('square', 0.018), ('accuracy', 0.018), ('approaches', 0.018), ('viewpoint', 0.018), ('pling', 0.017), ('nanjing', 0.017), ('kddcup', 0.017), ('oversampling', 0.017), ('impressively', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="249-tfidf-1" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><p>2 0.10682034 <a title="249-tfidf-2" href="./nips-2012-Affine_Independent_Variational_Inference.html">37 nips-2012-Affine Independent Variational Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We consider inference in a broad class of non-conjugate probabilistic models based on minimising the Kullback-Leibler divergence between the given target density and an approximating ‘variational’ density. In particular, for generalised linear models we describe approximating densities formed from an afﬁne transformation of independently distributed latent variables, this class including many well known densities as special cases. We show how all relevant quantities can be efﬁciently computed using the fast Fourier transform. This extends the known class of tractable variational approximations and enables the ﬁtting for example of skew variational densities to the target density. 1</p><p>3 0.10202589 <a title="249-tfidf-3" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>Author: Purushottam Kar, Prateek Jain</p><p>Abstract: We address the problem of general supervised learning when data can only be accessed through an (indeﬁnite) similarity function between data points. Existing work on learning with indeﬁnite kernels has concentrated solely on binary/multiclass classiﬁcation problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classiﬁcation. We give a “goodness” criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efﬁcient algorithms for supervised learning using “good” similarity functions. We demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness deﬁnition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves signiﬁcantly higher accuracies than the baseline methods while offering reduced computational costs. 1</p><p>4 0.096362986 <a title="249-tfidf-4" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>Author: Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, Bharath K. Sriperumbudur</p><p>Abstract: Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.</p><p>5 0.085774049 <a title="249-tfidf-5" href="./nips-2012-Multiresolution_analysis_on_the_symmetric_group.html">234 nips-2012-Multiresolution analysis on the symmetric group</a></p>
<p>Author: Risi Kondor, Walter Dempsey</p><p>Abstract: There is no generally accepted way to deﬁne wavelets on permutations. We address this issue by introducing the notion of coset based multiresolution analysis (CMRA) on the symmetric group, ﬁnd the corresponding wavelet functions, and describe a fast wavelet transform for sparse signals. We discuss potential applications in ranking, sparse approximation, and multi-object tracking. 1</p><p>6 0.085468367 <a title="249-tfidf-6" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>7 0.085250489 <a title="249-tfidf-7" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>8 0.082270399 <a title="249-tfidf-8" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>9 0.080824643 <a title="249-tfidf-9" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>10 0.078471139 <a title="249-tfidf-10" href="./nips-2012-Gradient_Weights_help_Nonparametric_Regressors.html">145 nips-2012-Gradient Weights help Nonparametric Regressors</a></p>
<p>11 0.072743572 <a title="249-tfidf-11" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>12 0.069558434 <a title="249-tfidf-12" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>13 0.060894776 <a title="249-tfidf-13" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>14 0.058754481 <a title="249-tfidf-14" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>15 0.058710251 <a title="249-tfidf-15" href="./nips-2012-CPRL_--_An_Extension_of_Compressive_Sensing_to_the_Phase_Retrieval_Problem.html">63 nips-2012-CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem</a></p>
<p>16 0.057688553 <a title="249-tfidf-16" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>17 0.057318266 <a title="249-tfidf-17" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>18 0.05607025 <a title="249-tfidf-18" href="./nips-2012-Collaborative_Gaussian_Processes_for_Preference_Learning.html">74 nips-2012-Collaborative Gaussian Processes for Preference Learning</a></p>
<p>19 0.055411801 <a title="249-tfidf-19" href="./nips-2012-Learning_curves_for_multi-task_Gaussian_process_regression.html">187 nips-2012-Learning curves for multi-task Gaussian process regression</a></p>
<p>20 0.055388745 <a title="249-tfidf-20" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.019), (2, 0.03), (3, -0.034), (4, 0.061), (5, -0.005), (6, 0.001), (7, 0.105), (8, -0.01), (9, -0.084), (10, 0.023), (11, 0.006), (12, 0.015), (13, -0.026), (14, 0.06), (15, -0.073), (16, 0.051), (17, 0.029), (18, 0.026), (19, -0.029), (20, 0.042), (21, -0.03), (22, 0.065), (23, -0.093), (24, 0.0), (25, 0.102), (26, -0.047), (27, 0.046), (28, -0.085), (29, 0.067), (30, -0.02), (31, -0.037), (32, 0.052), (33, -0.083), (34, 0.027), (35, -0.067), (36, -0.123), (37, -0.067), (38, -0.077), (39, 0.056), (40, -0.011), (41, -0.064), (42, 0.061), (43, 0.025), (44, -0.027), (45, 0.054), (46, 0.073), (47, 0.035), (48, 0.086), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91140562 <a title="249-lsi-1" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><p>2 0.72075987 <a title="249-lsi-2" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>Author: Hachem Kadri, Alain Rakotomamonjy, Philippe Preux, Francis R. Bach</p><p>Abstract: Positive deﬁnite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a ﬁnite linear combination of inﬁnite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an r -norm constraint on the combination coefﬁcients (r ≥ 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of ﬁnger movement prediction in brain-computer interfaces. 1</p><p>3 0.70297551 <a title="249-lsi-3" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>Author: Alexander Lorbert, Peter J. Ramadge</p><p>Abstract: We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We report experiments using real-world, multi-subject fMRI data. 1</p><p>4 0.67540973 <a title="249-lsi-4" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>Author: Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, Bharath K. Sriperumbudur</p><p>Abstract: Given samples from distributions p and q, a two-sample test determines whether to reject the null hypothesis that p = q, based on the value of a test statistic measuring the distance between the samples. One choice of test statistic is the maximum mean discrepancy (MMD), which is a distance between embeddings of the probability distributions in a reproducing kernel Hilbert space. The kernel used in obtaining these embeddings is critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability. A means of parameter selection for the two-sample test based on the MMD is proposed. For a given test level (an upper bound on the probability of making a Type I error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a Type II error. The test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size. These properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory. In experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.</p><p>5 0.6506024 <a title="249-lsi-5" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>6 0.63303202 <a title="249-lsi-6" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>7 0.61484802 <a title="249-lsi-7" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>8 0.6008783 <a title="249-lsi-8" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>9 0.52670282 <a title="249-lsi-9" href="./nips-2012-Gradient_Weights_help_Nonparametric_Regressors.html">145 nips-2012-Gradient Weights help Nonparametric Regressors</a></p>
<p>10 0.51938462 <a title="249-lsi-10" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>11 0.51769936 <a title="249-lsi-11" href="./nips-2012-Multiresolution_analysis_on_the_symmetric_group.html">234 nips-2012-Multiresolution analysis on the symmetric group</a></p>
<p>12 0.49425015 <a title="249-lsi-12" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>13 0.47389662 <a title="249-lsi-13" href="./nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">221 nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>14 0.47147715 <a title="249-lsi-14" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>15 0.46180615 <a title="249-lsi-15" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>16 0.46063894 <a title="249-lsi-16" href="./nips-2012-Affine_Independent_Variational_Inference.html">37 nips-2012-Affine Independent Variational Inference</a></p>
<p>17 0.45389384 <a title="249-lsi-17" href="./nips-2012-Feature-aware_Label_Space_Dimension_Reduction_for_Multi-label_Classification.html">130 nips-2012-Feature-aware Label Space Dimension Reduction for Multi-label Classification</a></p>
<p>18 0.44636077 <a title="249-lsi-18" href="./nips-2012-Fast_Resampling_Weighted_v-Statistics.html">128 nips-2012-Fast Resampling Weighted v-Statistics</a></p>
<p>19 0.43675658 <a title="249-lsi-19" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>20 0.43236384 <a title="249-lsi-20" href="./nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.029), (21, 0.013), (38, 0.125), (39, 0.363), (42, 0.03), (53, 0.013), (54, 0.013), (55, 0.035), (74, 0.043), (76, 0.141), (80, 0.061), (92, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83735621 <a title="249-lda-1" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family, even if the distributions are very heavy-tailed, have inﬁnite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness. Both theories and experiments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and robustness at almost no cost. 1</p><p>2 0.80626786 <a title="249-lda-2" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>Author: Gal Elidan, Cobi Cario</p><p>Abstract: The empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances. Yet, for continuous non-Gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models. In this work we present Nonparanormal BP for performing efﬁcient inference on distributions parameterized by a Gaussian copulas network and any univariate marginals. For tree structured networks, our approach is guaranteed to be exact for this powerful class of non-Gaussian models. Importantly, the method is as efﬁcient as standard Gaussian BP, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used. 1</p><p>3 0.79897362 <a title="249-lda-3" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>Author: Xiaolong Wang, Liang Lin</p><p>Abstract: This paper studies a novel discriminative part-based model to represent and recognize object shapes with an “And-Or graph”. We deﬁne this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global veriﬁcation. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the conﬁguration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. 1</p><p>4 0.78322023 <a title="249-lda-4" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>same-paper 5 0.74446696 <a title="249-lda-5" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>Author: Tianbao Yang, Yu-feng Li, Mehrdad Mahdavi, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: Both random Fourier features and the Nystr¨ m method have been successfully o applied to efﬁcient kernel learning. In this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances. Unlike approaches based on random Fourier features where the basis functions (i.e., cosine and sine functions) are sampled from a distribution independent from the training data, basis functions used by the Nystr¨ m method are randomly sampled from the training examples and are o therefore data dependent. By exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based on the Nystr¨ m method can yield impressively better generalization error bound than o random Fourier features based approach. We empirically verify our theoretical ﬁndings on a wide range of large data sets. 1</p><p>6 0.72731256 <a title="249-lda-6" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>7 0.708148 <a title="249-lda-7" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>8 0.60475522 <a title="249-lda-8" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>9 0.56228316 <a title="249-lda-9" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>10 0.56100351 <a title="249-lda-10" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>11 0.55016387 <a title="249-lda-11" href="./nips-2012-Isotropic_Hashing.html">163 nips-2012-Isotropic Hashing</a></p>
<p>12 0.54420495 <a title="249-lda-12" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>13 0.54116118 <a title="249-lda-13" href="./nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">317 nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>14 0.53816897 <a title="249-lda-14" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>15 0.53763562 <a title="249-lda-15" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>16 0.53675133 <a title="249-lda-16" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>17 0.536111 <a title="249-lda-17" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>18 0.5323599 <a title="249-lda-18" href="./nips-2012-Semi-supervised_Eigenvectors_for_Locally-biased_Learning.html">309 nips-2012-Semi-supervised Eigenvectors for Locally-biased Learning</a></p>
<p>19 0.52969372 <a title="249-lda-19" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>20 0.52800584 <a title="249-lda-20" href="./nips-2012-Topology_Constraints_in_Graphical_Models.html">346 nips-2012-Topology Constraints in Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
