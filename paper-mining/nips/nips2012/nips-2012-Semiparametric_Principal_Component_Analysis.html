<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>310 nips-2012-Semiparametric Principal Component Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-310" href="#">nips2012-310</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>310 nips-2012-Semiparametric Principal Component Analysis</h1>
<br/><p>Source: <a title="nips-2012-310-pdf" href="http://papers.nips.cc/paper/4809-semiparametric-principal-component-analysis.pdf">pdf</a></p><p>Author: Fang Han, Han Liu</p><p>Abstract: We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>Reference: <a title="nips-2012-310-reference" href="../nips2012_reference/nips-2012-Semiparametric_Principal_Component_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose two new principal component analysis methods in this paper utilizing a semiparametric model. [sent-3, score-0.084]
</p><p>2 The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. [sent-5, score-0.025]
</p><p>3 The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. [sent-6, score-0.087]
</p><p>4 The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. [sent-7, score-0.026]
</p><p>5 We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). [sent-10, score-0.03]
</p><p>6 Given a random vector X ∈ Rd with covariance matrix Σ and n independent observations of X, the PCA reduces the dimension of the data by projecting the data onto a linear subspace spanned by the k leading eigenvectors of Σ, such that the principal modes of variations are preserved. [sent-12, score-0.103]
</p><p>7 In practice, Σ is unknown and replaced by d the sample covariance S. [sent-13, score-0.014]
</p><p>8 ≥ ωd and the corresponding orthornormal eigenvectors u1 , . [sent-17, score-0.018]
</p><p>9 PCA aims at recovering the ﬁrst k eigenvectors u1 , . [sent-21, score-0.018]
</p><p>10 [5] show that if X is multivariate Gaussian, then the distribution is centered about the principal component axes and is therefore “self-consistent” [8]. [sent-27, score-0.07]
</p><p>11 Given u1 the dominant eigenvector of S, [9] show that the angle between u1 and u1 will not converge to 0, i. [sent-30, score-0.03]
</p><p>12 lim inf n→∞ E∠(u1 , u1 ) > 0, where we denote by ∠(u1 , u1 ) the angle between the estimated and the true leading eigenvectors. [sent-32, score-0.014]
</p><p>13 The resulting estimator u1 is: u1 = arg max v T Sv subject to  v  2  = 1, card(supp(v)) ≤ s. [sent-35, score-0.014]
</p><p>14 1), a variety of algorithms are proposed: greedy algorithms [3], lasso-type methods including SCoTLASS [11], SPCA [25] and sPCA-rSVD [19], a number of power methods [12, 23, 16], the biconvex algorithm PMD [21] and the semideﬁnite relaxation DSPCA [4]. [sent-38, score-0.015]
</p><p>15 In this paper, we ﬁrst explore the use of the PCA conducted on the correlation matrix Σ0 instead of the covariance matrix Σ, and then propose a high dimensional semiparametric scale-invariant principal component analysis method, named the Copula Component Analysis (COCA). [sent-41, score-0.157]
</p><p>16 In this paper, the population version of the scale-invariant PCA is built as the estimator of the leading eigenvector of the population correlation matrix Σ0 . [sent-42, score-0.08]
</p><p>17 , Xd )T belongs to a Nonparanormal family if and only if there exists a set 0 0 0 of univariate monotone functions {fj }d such that (f1 (X1 ), . [sent-47, score-0.019]
</p><p>18 Thirdly, to estimate Σ0 robustly and efﬁciently, instead of estimating 0 0 the normal score transformation functions {fj }d as [15] did, realizing that {fj }d preserve the j=1 j=1 ranks of the data, we utilize the nonparametric correlation coefﬁcient estimator, Spearman’s rho, to estimate Σ0 . [sent-52, score-0.05]
</p><p>19 In theory, we analyze the general case that X is following the Nonparanormal and θ1 is weakly sparse, here θ1 is the leading eigenvector of Σ0 . [sent-54, score-0.032]
</p><p>20 We obtain the estimation consistency of the COCA estimator to θ1 using the Spearman’s rho correlation coefﬁcient matrix. [sent-55, score-0.089]
</p><p>21 We prove that the estimation consistency rates are close to the parametric rate under Gaussian assumption and the feature selection consistency can be achieved when d is nearly exponential to the sample size. [sent-56, score-0.032]
</p><p>22 The Copula PCA estimates the leading eigenvector of the latent covariance matrix Σ. [sent-58, score-0.062]
</p><p>23 To estimate the leading eigenvectors of Σ, instead of Σ0 , in a fast rate, we prove that extra conditions are required on the transformation functions. [sent-59, score-0.047]
</p><p>24 1  The Models of the PCA and Scale-invariant PCA  Let Σ0 be the correlation matrix of Σ, and by spectral decomposition, Σ =  d j=1  ωj uj uT and Σ0 = j  d j=1  T λj θj θj . [sent-77, score-0.034]
</p><p>25 , θd }, the eigenvectors of the sample covariance and correlation matrices S and S 0 , are the MLEs of {u1 , . [sent-97, score-0.058]
</p><p>26 xn ∼ N (µ, Σ) and Σ0 be the correlation matrix of Σ. [sent-108, score-0.04]
</p><p>27 , ud }, and the estimators of the scale-invariant PCA, {θ1 , . [sent-112, score-0.02]
</p><p>28 , σd )T is said to follow a Nonparanormal distribution N P Nd (µ, Σ, f ) if and only if there exists a set of univariate monotone transformations 2 f = {fj }d such that: f (X) = (f1 (X1 ), . [sent-153, score-0.019]
</p><p>29 Let f 0 = {fj }d be a set of monotone univariate functions and Σ0 ∈ Rd×d j=1 be a positive deﬁnite correlation matrix with diag(Σ0 ) = 1. [sent-162, score-0.053]
</p><p>30 , µd )T , Σ = [Σjk ] ∈ Rd×d such that for any 1 ≤ j, k ≤ d, E(Xj ) = µj , Var(Xj ) = Σjj and Σ0 = jk √ Σjk , and a set of monotone univariate functions f = {fj }d such that X ∼ N P Nd (µ, Σ, f ). [sent-178, score-0.03]
</p><p>31 Using the connection that fj (x) = µj + σj fj (x), for j ∈ {1, 2 . [sent-180, score-0.042]
</p><p>32 2 is more appealing because it emphasizes the correlation and hence matches the spirit of the Copula. [sent-187, score-0.026]
</p><p>33 3 Spearman’s rho Correlation and Covariance Matrices Given n data points x1 , . [sent-191, score-0.042]
</p><p>34 Because the Nonparanormal distribution preserves the rank of the data, it is natural to use the nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, to estimate the latent n 1 correlation. [sent-198, score-0.034]
</p><p>35 , xnj and rj := n i=1 rij = n+1 , ¯ 2 n (r −¯ )(r −¯ ) r r we consider the following statistics: ρjk = √ n i=1 ij 2 j nik k , and the correlation ma2 r i=1 (rij −¯j )  ·  r i=1 (rik −¯k )  trix estimator: Rjk = 2 sin( π ρjk ). [sent-202, score-0.035]
</p><p>36 1) n We denote by R := [Rjk ] the Spearman’s rho correlation coefﬁcient matrix. [sent-214, score-0.068]
</p><p>37 In the following let S := [Sjk ] = [σj σk Rjk ] be the Spearman’s rho covariance matrix. [sent-215, score-0.056]
</p><p>38 1 COCA Model We ﬁrstly present the model of the Copula Component Analysis (COCA) method, where the idea of scale-invariant PCA is exploited and we wish to estimate the leading eigenvector of the latent correlation matrix. [sent-227, score-0.066]
</p><p>39 5 and 12 0 0 the transformation functions have the form as follows: (A) f1 (x) = x3 and f2 (x) = x1/3 ; (B) 0 0 0 0 f1 (x) = sign(x)x2 and f2 (x) = x3 ; (C) f1 (x) = f2 (x) = Φ−1 (x). [sent-262, score-0.015]
</p><p>40 where θ1 is the leading eigenvectors of the latent correlation matrix Σ0 we are interested in estimating, 0 ≤ q ≤ 1 and the q ball Bq (Rq ) is deﬁned as: when q = 0,  B0 (R0 ) := {v ∈ Rd : card(supp(v)) ≤ R0 };  when 0 < q ≤ 1,  d  Bq (Rq ) := {v ∈ R : v  q q  ≤ Rq }. [sent-263, score-0.074]
</p><p>41 3)  Inspired by the model M0 (q, Rq , Σ0 , f 0 ), we consider the following COCA estimator θ1 , which maximizes the following equation with the constraint that θ1 ∈ Bq (Rq ) for some 0 ≤ q ≤ 1: θ1 = arg max v T Rv, subject to v ∈ Sd−1 ∩ Bq (Rq ). [sent-266, score-0.021]
</p><p>42 4)  v∈Rd  Here R is the estimated Spearman’s rho correlation coefﬁcient matrix. [sent-268, score-0.068]
</p><p>43 The corresponding COCA estimator θ1 can be considered as a nonlinear dimensional reduction procedure and has the potential to gain more ﬂexibility compared with the classical PCA. [sent-269, score-0.024]
</p><p>44 In Section 4 we will establish the theoretical results on the COCA estimator and will show that it can estimate the latent true dominant eigenvector θ1 in a fast rate and can achieve feature selection consistency. [sent-270, score-0.059]
</p><p>45 1 Copula PCA Model In contrast, we provide another model inspired from the classical PCA method, where we wish to estimate the leading eigenvector of the latent covariance matrix. [sent-273, score-0.054]
</p><p>46 5) u1 ∈ Sd−1 ∩ Bq (Rq ), where u1 is the leading eigenvector of the covariance matrix Σ and it is what we are interested in estimating. [sent-280, score-0.054]
</p><p>47 6)  v∈Rd  where S is the Spearman’s rho covariance coefﬁcient matrix. [sent-282, score-0.056]
</p><p>48 2 Algorithms In this section we provide three sparse PCA algorithms, where the Spearman’s rho correlation and covariance matrices R and S can be directly plugged in to obtain sparse estimators. [sent-286, score-0.1]
</p><p>49 [21] suggest using the ﬁrst leading 4  eigenvector of Γ to be the initial value of v. [sent-294, score-0.032]
</p><p>50 The main idea of the SPCA algorithm is to exploit a regression approach to PCA and then utilize lasso and elastic net [24] to calculate a sparse estimator to the leading eigenvector. [sent-299, score-0.046]
</p><p>51 [25] suggest using the ﬁrst leading eigenvector of Γ to be the initial value of v. [sent-307, score-0.032]
</p><p>52 The main idea is to utilize the power method, but truncate the vector to a 0 ball in each iteration. [sent-312, score-0.017]
</p><p>53 In detail, we utilize the classical power method, but in each iteration t we project the intermediate vector xt to the intersection of the d-dimension sphere Sd−1 and the q ball 1/q with the radius Rq . [sent-320, score-0.017]
</p><p>54 In particular, we establish the results on the max convergence rates of the Spearman’s rho correlation and covariance matrices to Σ and Σ0 . [sent-326, score-0.087]
</p><p>55 d N P Nd (µ, Σ, f ), 0 < 1/c0 < min{σj } < max{σj } < c0 < j  j  −1 2 ∞, for some constant c0 and g := {gj = fj }d satisﬁes for all j = 1, . [sent-344, score-0.021]
</p><p>56 1 claims that, under certain constraint on the transformation functions, the latent covariance matrix Σ can be recovered using the Spearman’s rho covariance matrix. [sent-352, score-0.108]
</p><p>57 For any two vectors v1 ∈ Sd−1 and v2 ∈ Sd−1 , let 21 T | sin ∠(v1 , v2 )| = 1 − (v1 v2 )2 , then we have, for any n ≥ log d + 2, 2  P sin ∠(θ1 , θ1 ) ≤  2 γq Rq  64π 2 log d · 2 (λ1 − λ2 ) n  5  2−q 2  ≥ 1 − 1/d2 ,  (4. [sent-361, score-0.016]
</p><p>58 Generally, when Rq and λ1 , λ2 do not scale with (n, d), the rate is OP ( log d )1−q/2 , which is the n parametric rate [16, 20, 18] obtain. [sent-366, score-0.02]
</p><p>59 When (n, d) goes to inﬁnity, the two dominant eigenvalues λ1 and λ2 will typically go to inﬁnity and will at least be away from zero. [sent-367, score-0.018]
</p><p>60 Similarly, we can give an upper bound for the estimation rate of the Copula PCA to the true leading eigenvalue u1 of the latent covariance matrix Σ. [sent-380, score-0.051]
</p><p>61 −1 2 If g := {gj = fj }d satisﬁes gj ∈ T F (K) for all 1 ≤ j ≤ d, and 0 < 1/c0 < minj {σj } < j=1 2  2 γq Rq  maxj {σj } < c0 < ∞, and we further have minj∈Θ |u1j | ≥ n≥  5  21 log d  + 2, P(Θ = Θ) ≥ 1 −  √ √ 4 2R0 c1 (ω1 −ω2 )  log d n ,  then for any  1 d2 . [sent-393, score-0.042]
</p><p>62 A covariance matrix Σ is ﬁrstly synthesized through the eigenvalue decomposition, where the ﬁrst two eigenvalues are given and the corresponding eigenvectors are pre-speciﬁed to be sparse. [sent-404, score-0.046]
</p><p>63 In detail, we suppose that the ﬁrst two dominant eigenvectors of Σ, u1 and u2 , are sparse in the sense that only the ﬁrst s = 10 entries of √ u1 and the second s = 10 entries of u2 are nonzero and set to be 1/ 10. [sent-405, score-0.039]
</p><p>64 The correlation matrix Σ0 is accordingly generated from Σ, with λ1 = 4, λ2 = 2. [sent-411, score-0.041]
</p><p>65 , λd ≤ 1 and the two dominant eigenvectors sparse. [sent-415, score-0.03]
</p><p>66 To sample data from the Nonparanormal, we also need the transformation 0 functions: f 0 = {fj }d . [sent-416, score-0.015]
</p><p>67 Here two types of transformation functions are considered: (1) Linear j=1 0 transformation (or no transformation): flinear = {h0 , h0 , . [sent-417, score-0.041]
</p><p>68 , h0 }, where h0 (x) := x; (2) Nonlinear transformation: there exist ﬁve univariate monotone functions h1 , h2 , . [sent-420, score-0.019]
</p><p>69 We then generate j j n = 100, 200 or 500 data points from: 0 0 [Scheme 1] X ∼ N P Nd (Σ0 , flinear ) where flinear = {h0 , h0 , . [sent-435, score-0.022]
</p><p>70 0 0 [Scheme 2] X ∼ N P Nd (Σ0 , fnonlinear ) where fnonlinear = {h1 , h2 , h3 , h4 , h5 , . [sent-439, score-0.022]
</p><p>71 The PMD, SPCA and TPower algorithms are then employed on X to computer the estimated leading eigenvector θ1 . [sent-447, score-0.032]
</p><p>72 0  FPR  Figure 2: ROC curves for the PMD, SPCA and Truncated Power method (the left two, the middle two, the right two) with linear (no) and nonlinear transformation (top, bottom) and data contamination at different levels (r = 0, 0. [sent-604, score-0.025]
</p><p>73 The raw data contain 20,248 probes and 13,182 samples belonging to 2,711 tissue types (e. [sent-610, score-0.035]
</p><p>74 There are at most 1,599 samples and at least 1 sample belonging to each tissue type. [sent-614, score-0.029]
</p><p>75 We utilize the Truncated Power method proposed by [23] to achieve the sparse estimated dominant eigenvectors. [sent-619, score-0.03]
</p><p>76 We then explore several tissue types with the largest sample size: (1) Breast tumor, 1,599 samples; (2) B cell lymphoma, 213 samples; (3) Prostate tumor, 148 samples; (4) Wilms tumor, 143 samples. [sent-624, score-0.027]
</p><p>77 b cell lymphoma, breast tumor, prostate tumor and Wilms tumor are explored (from left to right). [sent-627, score-0.102]
</p><p>78 Each black point represents a sample and each red point represents a sample belonging to the corresponding tissue type. [sent-628, score-0.029]
</p><p>79 For each tissue type listed above, we apply the COCA (Spearman) and the classic high dimensional PCA (Pearson) on the data belonging to this speciﬁc tissue type and obtain the ﬁrst two dominant sparse eigenvectors. [sent-629, score-0.087]
</p><p>80 For COCA, we do a normal score transformation on the original dataset. [sent-631, score-0.015]
</p><p>81 We subsequently project the whole dataset to the ﬁrst two principal components using the obtained eigenvectors. [sent-632, score-0.049]
</p><p>82 In Figure 3 each black point represents a sample and each red point represents a sample belonging to the corresponding tissue type. [sent-634, score-0.029]
</p><p>83 The ﬁrst phenomenon indicates that the COCA has the potential to preserve more common information shared by samples from the same tissue type. [sent-636, score-0.022]
</p><p>84 The second phenomenon indicates that the COCA has the potential to differentiate samples from different tissue types more efﬁciently. [sent-637, score-0.022]
</p><p>85 Though both papers are working on principal component analysis, the core ideas are quite different: Firstly, the analysis in [7] is based on a different distribution family called transelliptical, while COCA and Copula PCA are based on the Nonparanormal family. [sent-639, score-0.07]
</p><p>86 Secondly, by improving the modeling ﬂexibility, in [7] there does not exist a scale-variant variant since it is hard to quantify the transformation functions. [sent-640, score-0.015]
</p><p>87 In contrast, by introducing the subgaussian transformation function family, the current paper provides sufﬁcient conditions for Copula PCA to achieve parametric rates. [sent-641, score-0.029]
</p><p>88 Thirdly, the method in [7] cannot explicitly conduct data visualization, due to the fact that the latent elliptical distribution is unspeciﬁed and accordingly they cannot accurately estimate the marginal transformations. [sent-642, score-0.022]
</p><p>89 Moreover, via quantifying a sharp convergence rate in estimating the marginal transformations, we can provide the convergence rates in estimating the principal components. [sent-644, score-0.068]
</p><p>90 Finally, we recommend using the Spearman’s rho instead of the Kendall’s tau in estimating the correlation coefﬁcients provided that the Nonparanormal model holds. [sent-646, score-0.076]
</p><p>91 This is because Spearman’s rho is statistically more efﬁcient than Kendall’tau within the Nonparanormal family. [sent-647, score-0.042]
</p><p>92 High-dimensional analysis of semideﬁnite relaxations for sparse principal components. [sent-654, score-0.058]
</p><p>93 Tca: Transelliptical principal component analysis for high dimensional non-gaussian data. [sent-692, score-0.08]
</p><p>94 On consistency and sparsity for principal components analysis in high dimensions. [sent-704, score-0.056]
</p><p>95 A modiﬁed principal component technique based on the lasso. [sent-717, score-0.07]
</p><p>96 Generalized power method for sparse e a principal component analysis. [sent-724, score-0.087]
</p><p>97 Augmented sparse principal component analysis for high dimensional data. [sent-759, score-0.089]
</p><p>98 Sparse principal component analysis via regularized low rank matrix approximation. [sent-766, score-0.078]
</p><p>99 Minimax rates of estimation for sparse pca in high dimensions. [sent-772, score-0.08]
</p><p>100 A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. [sent-780, score-0.092]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qq', 0.965), ('coca', 0.117), ('spearman', 0.085), ('copula', 0.069), ('qqq', 0.067), ('pca', 0.066), ('nonparanormal', 0.059), ('spca', 0.051), ('principal', 0.049), ('pmd', 0.047), ('rq', 0.047), ('pearson', 0.042), ('rho', 0.042), ('tpr', 0.039), ('tumor', 0.036), ('fpr', 0.036), ('correlation', 0.026), ('supp', 0.023), ('tpower', 0.023), ('tissue', 0.022), ('component', 0.021), ('fj', 0.021), ('oracle', 0.018), ('eigenvector', 0.018), ('bq', 0.018), ('eigenvectors', 0.018), ('sd', 0.017), ('prostate', 0.016), ('transformation', 0.015), ('lymphoma', 0.015), ('wilms', 0.015), ('estimator', 0.014), ('ud', 0.014), ('semiparametric', 0.014), ('covariance', 0.014), ('leading', 0.014), ('gj', 0.013), ('dominant', 0.012), ('jk', 0.011), ('han', 0.011), ('monotone', 0.011), ('xd', 0.011), ('flinear', 0.011), ('fnonlinear', 0.011), ('rd', 0.011), ('dimensional', 0.01), ('contamination', 0.01), ('breast', 0.009), ('utilize', 0.009), ('truncated', 0.009), ('rij', 0.009), ('transelliptical', 0.009), ('rstly', 0.009), ('sparse', 0.009), ('card', 0.009), ('subgaussian', 0.008), ('rjk', 0.008), ('power', 0.008), ('univariate', 0.008), ('dt', 0.008), ('latent', 0.008), ('arxiv', 0.008), ('matrix', 0.008), ('minj', 0.008), ('tau', 0.008), ('sin', 0.008), ('ut', 0.008), ('belonging', 0.007), ('biconvex', 0.007), ('mles', 0.007), ('qtpm', 0.007), ('consistency', 0.007), ('named', 0.007), ('accordingly', 0.007), ('jj', 0.007), ('equation', 0.007), ('coef', 0.007), ('mij', 0.007), ('claims', 0.007), ('fd', 0.007), ('kendall', 0.007), ('marginal', 0.007), ('rate', 0.007), ('tf', 0.006), ('sjk', 0.006), ('eigenvalues', 0.006), ('parametric', 0.006), ('submatrix', 0.006), ('rik', 0.006), ('probes', 0.006), ('preprint', 0.006), ('xn', 0.006), ('genes', 0.006), ('estimators', 0.006), ('biostatistics', 0.006), ('unspeci', 0.006), ('gaussian', 0.005), ('listed', 0.005), ('rates', 0.005), ('cell', 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="310-tfidf-1" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>2 0.89288235 <a title="310-tfidf-2" href="./nips-2012-Semi-Supervised_Domain_Adaptation_with_Non-Parametric_Copulas.html">308 nips-2012-Semi-Supervised Domain Adaptation with Non-Parametric Copulas</a></p>
<p>Author: David Lopez-paz, Jose M. Hernández-lobato, Bernhard Schölkopf</p><p>Abstract: A new framework based on the theory of copulas is proposed to address semisupervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efﬁcacy of the proposed approach when compared to state-of-the-art techniques. 1</p><p>3 0.58297366 <a title="310-tfidf-3" href="./nips-2012-A_Conditional_Multinomial_Mixture_Model_for_Superset_Label_Learning.html">5 nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</a></p>
<p>Author: Liping Liu, Thomas G. Dietterich</p><p>Abstract: In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic StickBreaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSBCMM is derived from the logistic stick-breaking process. It ﬁrst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speciﬁc multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classiﬁcation predictions. 1</p><p>4 0.39949682 <a title="310-tfidf-4" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>Author: Amadou Ba, Mathieu Sinn, Yannig Goude, Pascal Pompey</p><p>Abstract: This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) ﬁlter. In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). e Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy. 1</p><p>5 0.075194478 <a title="310-tfidf-5" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family, even if the distributions are very heavy-tailed, have inﬁnite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness. Both theories and experiments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and robustness at almost no cost. 1</p><p>6 0.055739984 <a title="310-tfidf-6" href="./nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">317 nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>7 0.05569075 <a title="310-tfidf-7" href="./nips-2012-Meta-Gaussian_Information_Bottleneck.html">211 nips-2012-Meta-Gaussian Information Bottleneck</a></p>
<p>8 0.05217829 <a title="310-tfidf-8" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>9 0.033575911 <a title="310-tfidf-9" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>10 0.026957877 <a title="310-tfidf-10" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>11 0.026220024 <a title="310-tfidf-11" href="./nips-2012-On_the_Sample_Complexity_of_Robust_PCA.html">254 nips-2012-On the Sample Complexity of Robust PCA</a></p>
<p>12 0.020567171 <a title="310-tfidf-12" href="./nips-2012-Structure_estimation_for_discrete_graphical_models%3A_Generalized_covariance_matrices_and_their_inverses.html">326 nips-2012-Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses</a></p>
<p>13 0.019063398 <a title="310-tfidf-13" href="./nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">237 nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>14 0.017813275 <a title="310-tfidf-14" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>15 0.015961634 <a title="310-tfidf-15" href="./nips-2012-Latent_Coincidence_Analysis%3A_A_Hidden_Variable_Model_for_Distance_Metric_Learning.html">171 nips-2012-Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning</a></p>
<p>16 0.015212669 <a title="310-tfidf-16" href="./nips-2012-Stochastic_optimization_and_sparse_statistical_recovery%3A_Optimal_algorithms_for_high_dimensions.html">325 nips-2012-Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions</a></p>
<p>17 0.014458696 <a title="310-tfidf-17" href="./nips-2012-Semi-supervised_Eigenvectors_for_Locally-biased_Learning.html">309 nips-2012-Semi-supervised Eigenvectors for Locally-biased Learning</a></p>
<p>18 0.014398525 <a title="310-tfidf-18" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>19 0.0143839 <a title="310-tfidf-19" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>20 0.014090434 <a title="310-tfidf-20" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.051), (1, 0.034), (2, 0.031), (3, -0.024), (4, -0.008), (5, 0.004), (6, 0.913), (7, -0.065), (8, -0.048), (9, 0.024), (10, 0.07), (11, -0.041), (12, -0.042), (13, 0.036), (14, 0.022), (15, 0.031), (16, 0.016), (17, -0.009), (18, 0.014), (19, -0.011), (20, 0.019), (21, -0.015), (22, 0.011), (23, -0.028), (24, -0.004), (25, -0.004), (26, -0.008), (27, -0.005), (28, 0.014), (29, 0.015), (30, 0.003), (31, -0.025), (32, -0.016), (33, 0.022), (34, -0.002), (35, 0.011), (36, 0.001), (37, 0.012), (38, -0.007), (39, -0.002), (40, 0.018), (41, -0.003), (42, -0.005), (43, 0.006), (44, -0.017), (45, -0.006), (46, 0.003), (47, -0.005), (48, 0.016), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99634516 <a title="310-lsi-1" href="./nips-2012-Semi-Supervised_Domain_Adaptation_with_Non-Parametric_Copulas.html">308 nips-2012-Semi-Supervised Domain Adaptation with Non-Parametric Copulas</a></p>
<p>Author: David Lopez-paz, Jose M. Hernández-lobato, Bernhard Schölkopf</p><p>Abstract: A new framework based on the theory of copulas is proposed to address semisupervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efﬁcacy of the proposed approach when compared to state-of-the-art techniques. 1</p><p>same-paper 2 0.99191564 <a title="310-lsi-2" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>3 0.79227382 <a title="310-lsi-3" href="./nips-2012-A_Conditional_Multinomial_Mixture_Model_for_Superset_Label_Learning.html">5 nips-2012-A Conditional Multinomial Mixture Model for Superset Label Learning</a></p>
<p>Author: Liping Liu, Thomas G. Dietterich</p><p>Abstract: In the superset label learning problem (SLL), each training instance provides a set of candidate labels of which one is the true label of the instance. As in ordinary regression, the candidate label set is a noisy version of the true label. In this work, we solve the problem by maximizing the likelihood of the candidate label sets of training instances. We propose a probabilistic model, the Logistic StickBreaking Conditional Multinomial Model (LSB-CMM), to do the job. The LSBCMM is derived from the logistic stick-breaking process. It ﬁrst maps data points to mixture components and then assigns to each mixture component a label drawn from a component-speciﬁc multinomial distribution. The mixture components can capture underlying structure in the data, which is very useful when the model is weakly supervised. This advantage comes at little cost, since the model introduces few additional parameters. Experimental tests on several real-world problems with superset labels show results that are competitive or superior to the state of the art. The discovered underlying structures also provide improved explanations of the classiﬁcation predictions. 1</p><p>4 0.7871244 <a title="310-lsi-4" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>Author: Amadou Ba, Mathieu Sinn, Yannig Goude, Pascal Pompey</p><p>Abstract: This paper proposes an efﬁcient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) ﬁlter. In order to quickly track changes in the model and put more weight on recent data, the RLS ﬁlter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricit´ de France (EDF). e Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy. 1</p><p>5 0.12264585 <a title="310-lsi-5" href="./nips-2012-Meta-Gaussian_Information_Bottleneck.html">211 nips-2012-Meta-Gaussian Information Bottleneck</a></p>
<p>Author: Melanie Rey, Volker Roth</p><p>Abstract: We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers. 1</p><p>6 0.11629491 <a title="310-lsi-6" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>7 0.1123152 <a title="310-lsi-7" href="./nips-2012-Feature-aware_Label_Space_Dimension_Reduction_for_Multi-label_Classification.html">130 nips-2012-Feature-aware Label Space Dimension Reduction for Multi-label Classification</a></p>
<p>8 0.094729386 <a title="310-lsi-8" href="./nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">317 nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>9 0.093679689 <a title="310-lsi-9" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>10 0.07530944 <a title="310-lsi-10" href="./nips-2012-Optimal_Neural_Tuning_Curves_for_Arbitrary_Stimulus_Distributions%3A_Discrimax%2C_Infomax_and_Minimum_%24L_p%24_Loss.html">262 nips-2012-Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax, Infomax and Minimum $L p$ Loss</a></p>
<p>11 0.061535705 <a title="310-lsi-11" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>12 0.060683832 <a title="310-lsi-12" href="./nips-2012-Proper_losses_for_learning_from_partial_labels.html">280 nips-2012-Proper losses for learning from partial labels</a></p>
<p>13 0.055212285 <a title="310-lsi-13" href="./nips-2012-Feature_Clustering_for_Accelerating_Parallel_Coordinate_Descent.html">131 nips-2012-Feature Clustering for Accelerating Parallel Coordinate Descent</a></p>
<p>14 0.055035941 <a title="310-lsi-14" href="./nips-2012-Learning_from_the_Wisdom_of_Crowds_by_Minimax_Entropy.html">189 nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</a></p>
<p>15 0.054963231 <a title="310-lsi-15" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>16 0.053098783 <a title="310-lsi-16" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>17 0.053010952 <a title="310-lsi-17" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>18 0.052938677 <a title="310-lsi-18" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>19 0.052439217 <a title="310-lsi-19" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>20 0.050665677 <a title="310-lsi-20" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (11, 0.014), (21, 0.014), (24, 0.348), (38, 0.068), (39, 0.103), (42, 0.037), (54, 0.013), (55, 0.015), (64, 0.014), (74, 0.018), (76, 0.086), (80, 0.033), (92, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73927277 <a title="310-lda-1" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose two new principal component analysis methods in this paper utilizing a semiparametric model. The according methods are named Copula Component Analysis (COCA) and Copula PCA. The semiparametric model assumes that, after unspeciﬁed marginally monotone transformations, the distributions are multivariate Gaussian. The COCA and Copula PCA accordingly estimate the leading eigenvectors of the correlation and covariance matrices of the latent Gaussian distribution. The robust nonparametric rank-based correlation coefﬁcient estimator, Spearman’s rho, is exploited in estimation. We prove that, under suitable conditions, although the marginal distributions can be arbitrarily continuous, the COCA and Copula PCA estimators obtain fast estimation rates and are feature selection consistent in the setting where the dimension is nearly exponentially large relative to the sample size. Careful numerical experiments on the synthetic and real data are conducted to back up the theoretical results. We also discuss the relationship with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>2 0.52235854 <a title="310-lda-2" href="./nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">317 nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>Author: Tuo Zhao, Kathryn Roeder, Han Liu</p><p>Abstract: We introduce a new learning algorithm, named smooth-projected neighborhood pursuit, for estimating high dimensional undirected graphs. In particularly, we focus on the nonparanormal graphical model and provide theoretical guarantees for graph estimation consistency. In addition to new computational and theoretical analysis, we also provide an alternative view to analyze the tradeoff between computational efﬁciency and statistical error under a smoothing optimization framework. Numerical results on both synthetic and real datasets are provided to support our theory. 1</p><p>3 0.41935351 <a title="310-lda-3" href="./nips-2012-Transelliptical_Graphical_Models.html">352 nips-2012-Transelliptical Graphical Models</a></p>
<p>Author: Han Liu, Fang Han, Cun-hui Zhang</p><p>Abstract: We advocate the use of a new distribution family—the transelliptical—for robust inference of high dimensional graphical models. The transelliptical family is an extension of the nonparanormal family proposed by Liu et al. (2009). Just as the nonparanormal extends the normal by transforming the variables using univariate functions, the transelliptical extends the elliptical family in the same way. We propose a nonparametric rank-based regularization estimator which achieves the parametric rates of convergence for both graph recovery and parameter estimation. Such a result suggests that the extra robustness and ﬂexibility obtained by the semiparametric transelliptical modeling incurs almost no efﬁciency loss. We also discuss the relationship between this work with the transelliptical component analysis proposed by Han and Liu (2012). 1</p><p>4 0.41063625 <a title="310-lda-4" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>Author: Fang Han, Han Liu</p><p>Abstract: We propose a high dimensional semiparametric scale-invariant principle component analysis, named TCA, by utilize the natural connection between the elliptical distribution family and the principal component analysis. Elliptical distribution family includes many well-known multivariate distributions like multivariate Gaussian, t and logistic and it is extended to the meta-elliptical by Fang et.al (2002) using the copula techniques. In this paper we extend the meta-elliptical distribution family to a even larger family, called transelliptical. We prove that TCA can obtain a near-optimal s log d/n estimation consistency rate in recovering the leading eigenvector of the latent generalized correlation matrix under the transelliptical distribution family, even if the distributions are very heavy-tailed, have inﬁnite second moments, do not have densities and possess arbitrarily continuous marginal distributions. A feature selection result with explicit rate is also provided. TCA is further implemented in both numerical simulations and largescale stock data to illustrate its empirical usefulness. Both theories and experiments conﬁrm that TCA can achieve model ﬂexibility, estimation accuracy and robustness at almost no cost. 1</p><p>5 0.40917829 <a title="310-lda-5" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: We present a truncation-free stochastic variational inference algorithm for Bayesian nonparametric models. While traditional variational inference algorithms require truncations for the model or the variational distribution, our method adapts model complexity on the ﬂy. We studied our method with Dirichlet process mixture models and hierarchical Dirichlet process topic models on two large data sets. Our method performs better than previous stochastic variational inference algorithms. 1</p><p>6 0.39825782 <a title="310-lda-6" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>7 0.39607534 <a title="310-lda-7" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>8 0.39526877 <a title="310-lda-8" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>9 0.38758326 <a title="310-lda-9" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>10 0.37692007 <a title="310-lda-10" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>11 0.33767667 <a title="310-lda-11" href="./nips-2012-Isotropic_Hashing.html">163 nips-2012-Isotropic Hashing</a></p>
<p>12 0.33607733 <a title="310-lda-12" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>13 0.3330259 <a title="310-lda-13" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>14 0.33228084 <a title="310-lda-14" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>15 0.33141884 <a title="310-lda-15" href="./nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">221 nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>16 0.33113101 <a title="310-lda-16" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>17 0.33057749 <a title="310-lda-17" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>18 0.32976553 <a title="310-lda-18" href="./nips-2012-Collaborative_Gaussian_Processes_for_Preference_Learning.html">74 nips-2012-Collaborative Gaussian Processes for Preference Learning</a></p>
<p>19 0.32941273 <a title="310-lda-19" href="./nips-2012-Graphical_Models_via_Generalized_Linear_Models.html">147 nips-2012-Graphical Models via Generalized Linear Models</a></p>
<p>20 0.32787478 <a title="310-lda-20" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
