<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-229" href="#">nips2012-229</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</h1>
<br/><p>Source: <a title="nips-2012-229-pdf" href="http://papers.nips.cc/paper/4683-multimodal-learning-with-deep-boltzmann-machines.pdf">pdf</a></p><p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>Reference: <a title="nips-2012-229-reference" href="../nips2012_reference/nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The model can be used to extract a uniﬁed representation that fuses modalities together. [sent-6, score-0.348]
</p><p>2 The model works by learning a probability density over the space of multimodal inputs. [sent-8, score-0.38]
</p><p>3 The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. [sent-10, score-0.344]
</p><p>4 Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. [sent-11, score-1.147]
</p><p>5 Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. [sent-13, score-0.311]
</p><p>6 Useful representations can be learned about such data by fusing the modalities into a joint representation that captures the real-world ‘concept’ that the data corresponds to. [sent-17, score-0.377]
</p><p>7 For example, we would like a probabilistic model to correlate the occurrence of the words ‘beautiful sunset’ and the visual properties of an image of a beautiful sunset and represent them jointly, so that the model assigns high probability to one conditioned on the other. [sent-18, score-0.223]
</p><p>8 Unless we do multimodal learning, it would not be possible to discover a lot of useful information about the world (for example, ‘what do beautiful sunsets look like? [sent-22, score-0.439]
</p><p>9 In a multimodal setting, data consists of multiple input modalities, each modality having a different kind of representation and correlational structure. [sent-25, score-0.547]
</p><p>10 For example, text is usually represented as discrete sparse word count vectors, whereas an image is represented using pixel intensities or outputs of feature extractors which are real-valued and dense. [sent-26, score-0.314]
</p><p>11 This makes it much harder to discover relationships across modalities than relationships among features in the same modality. [sent-27, score-0.304]
</p><p>12 There is a lot of structure in the input but it is difﬁcult to discover the highly non-linear relationships that exist 1  Figure 1: Left: Examples of text generated from a DBM by sampling from P (vtxt |vimg , θ). [sent-28, score-0.181]
</p><p>13 A good multimodal learning model must satisfy certain properties. [sent-32, score-0.38]
</p><p>14 It should also be possible to ﬁll-in missing modalities given the observed ones. [sent-35, score-0.334]
</p><p>15 Our proposed multimodal Deep Boltzmann Machine (DBM) model satisﬁes the above desiderata. [sent-37, score-0.38]
</p><p>16 DBMs are undirected graphical models with bipartite connections between adjacent layers of hidden units [1]. [sent-38, score-0.393]
</p><p>17 The key idea is to learn a joint density model over the space of multimodal inputs. [sent-39, score-0.436]
</p><p>18 Missing modalities can then be ﬁlled-in by sampling from the conditional distributions over them given the observed ones. [sent-40, score-0.241]
</p><p>19 For example, we use a large collection of user-tagged images to learn a joint distribution over images and text P (vimg , vtxt |θ). [sent-41, score-0.391]
</p><p>20 By drawing samples from P (vtxt |vimg , θ) and from P (vimg |vtxt , θ) we can ﬁll-in missing data, thereby doing image annotation and image retrieval respectively, as shown in Fig. [sent-42, score-0.332]
</p><p>21 There have been several approaches to learning from multimodal data. [sent-44, score-0.35]
</p><p>22 [3], based on multiple kernel learning framework, further demonstrated that an additional text modality can improve the accuracy of SVMs on various object recognition tasks. [sent-48, score-0.248]
</p><p>23 However, all of these approaches are discriminative by nature and cannot make use of large amounts of unlabeled data or deal easily with missing input modalities. [sent-49, score-0.226]
</p><p>24 [4] used dual-wing harmoniums to build a joint model of images and text, which can be viewed as a linear RBM model with Gaussian hidden units together with Gaussian and Poisson visible units. [sent-51, score-0.545]
</p><p>25 However, various data modalities will typically have very different statistical properties which makes it difﬁcult to model them using shallow models. [sent-52, score-0.271]
</p><p>26 [5] that used a deep autoencoder for speech and vision fusion. [sent-54, score-0.227]
</p><p>27 First, in this work we focus on integrating together very different data modalities: sparse word count vectors and real-valued dense image features. [sent-56, score-0.187]
</p><p>28 While both approaches have lead to interesting results in several domains, using a generative model is important for applications we consider in this paper, as it allows our model to naturally handle missing data modalities. [sent-58, score-0.179]
</p><p>29 In particular, the Replicated Softmax model [6] has been shown to be effective in modeling sparse word count vectors, whereas Gaussian RBMs have been used for modeling real-valued inputs for speech and vision tasks. [sent-61, score-0.281]
</p><p>30 In this section we brieﬂy review these models, as they will serve as our building blocks for the multimodal model. [sent-62, score-0.35]
</p><p>31 1 Restricted Boltzmann Machines A Restricted Boltzmann Machine is an undirected graphical model with stochastic visible units v ∈ {0, 1}D and stochastic hidden units h ∈ {0, 1}F , with each visible unit connected to each hidden unit. [sent-64, score-0.833]
</p><p>32 The model deﬁnes the following energy function E : {0, 1}D+F → R: D  F  E(v, h; θ) = −  D  vi Wij hj − i=1 j=1  F  bi vi − i=1  aj hj j=1  where θ = {a, b, W} are the model parameters. [sent-65, score-0.292]
</p><p>33 The joint distribution over the visible and hidden units is deﬁned by: 1 exp (−E(v, h; θ)). [sent-66, score-0.425]
</p><p>34 2 Gaussian RBM Consider modeling visible real-valued units v ∈ RD , and let h ∈ {0, 1}F be binary stochastic hidden units. [sent-68, score-0.418]
</p><p>35 The energy of the state {v, h} of the Gaussian RBM is deﬁned as follows: D D F F (vi − bi )2 vi E(v, h; θ) = − Wij hj − aj hj , (2) 2 2σi σ i=1 i=1 j=1 i j=1 where θ = {a, b, W, σ} are the model parameters. [sent-69, score-0.242]
</p><p>36 3 Replicated Softmax Model The Replicated Softmax Model is useful for modeling sparse count data, such as word count vectors in a document. [sent-71, score-0.215]
</p><p>37 Let v ∈ NK be a vector of visible units where vk is the number of times word k occurs in the document with the vocabulary of size K. [sent-72, score-0.303]
</p><p>38 The energy of the state {v, h} is deﬁned as follows K  F  E(v, h; θ) = −  K  vk Wkj hj − k=1 j=1  F  bk v k − M k=1  aj hj  (3)  j=1  where θ = {a, b, W} are the model parameters and M = k vk is the total number of words in a document. [sent-74, score-0.27]
</p><p>39 We note that this replicated softmax model can also be interpreted as an RBM model that uses a single visible multinomial unit with support {1, . [sent-75, score-0.296]
</p><p>40 It contains a set of visible units v ∈ {0, 1}D , and a sequence of layers of hidden units h(1) ∈ {0, 1}F1 , h(2) ∈ {0, 1}F2 ,. [sent-82, score-0.599]
</p><p>41 There are connections only between hidden units in adjacent layers. [sent-86, score-0.291]
</p><p>42 Right: A Multimodal DBM that models the joint distribution over image and text inputs. [sent-92, score-0.264]
</p><p>43 We illustrate the construction of a multimodal DBM using an image-text bi-modal DBM as our running example. [sent-93, score-0.35]
</p><p>44 Let vm ∈ RD denote an image input and vt ∈ NK denote a text input. [sent-94, score-0.479]
</p><p>45 To form a multimodal DBM, we combine the two models by adding an additional layer of binary hidden units on top of them. [sent-104, score-0.747]
</p><p>46 The joint distribution over the multi-modal input can be written as: (2)  P (vm , vt ; θ)=  P (h(2) , ht , h(3) ) m (2)  (2)  (1)  hm ,ht ,h(3)  3. [sent-107, score-0.289]
</p><p>47 4  (a) RBM  (b) Multimodal DBN  (c) Multimodal DBM  Figure 3: Different ways of combining multimodal inputs 3. [sent-114, score-0.415]
</p><p>48 Each pathway can be pretrained separately in a completely unsupervised fashion, which allows us to leverage a large supply of unlabeled data. [sent-116, score-0.189]
</p><p>49 The type of the lower-level RBMs in each pathway could be different, accounting for different input distributions, as long as the ﬁnal hidden representations at the end of each pathway are of the same type. [sent-118, score-0.37]
</p><p>50 Each data modality has very different statistical properties which make it difﬁcult for a single hidden layer model (such as Fig. [sent-120, score-0.395]
</p><p>51 In our model, this difference is bridged by putting layers of hidden units between the modalities. [sent-122, score-0.368]
</p><p>52 3a), where the hidden layer h directly models the (1) distribution over vt and vm , the ﬁrst layer of hidden units hm in a DBM has an easier task to (2) perform - that of modeling the distribution over vm and hm . [sent-127, score-1.195]
</p><p>53 Each layer of hidden units in the DBM contributes a small part to the overall task of modeling the distribution over vm and vt . [sent-128, score-0.663]
</p><p>54 Therefore, the middle layer in the network can be seen as a (relatively) “modality-free” representation of the input as opposed to the input layers which were “modality-full”. [sent-130, score-0.314]
</p><p>55 Another way of using a deep model to combine multimodal inputs is to use a Multimodal Deep Belief Network (DBN) (Fig. [sent-131, score-0.564]
</p><p>56 In a DBN model the responsibility of the multimodal modeling falls entirely on the joint layer. [sent-135, score-0.489]
</p><p>57 The modality fusion process is distributed across all hidden units in all layers. [sent-137, score-0.412]
</p><p>58 From the generative perspective, states of low-level hidden units in one pathway can inﬂuence the states of hidden units in other pathways through the higher-level layers, which is not the case for DBNs. [sent-138, score-0.721]
</p><p>59 3  Modeling Tasks  Generating Missing Modalities: As argued in the introduction, many real-world applications will often have one or more modalities missing. [sent-140, score-0.241]
</p><p>60 The Multimodal DBM can be used to generate such missing data modalities by clamping the observed modalities at the inputs and sampling the hidden modalities from the conditional distribution by running the standard alternating Gibbs sampler [1]. [sent-141, score-1.05]
</p><p>61 For example, consider generating text conditioned on a given image1 vm . [sent-142, score-0.283]
</p><p>62 The observed modality vm is clamped at the inputs and all hidden units are initialized randomly. [sent-143, score-0.676]
</p><p>63 This fused representation is inferred by clamping the observed modalities and doing alternating Gibbs sampling to sample from P (h(3) |vm , vt ) (if both modalities are present) or from P (h(3) |vm ) (if text is missing). [sent-149, score-0.822]
</p><p>64 The activation probabilities of hidden units h(3) constitute the joint representation of the inputs. [sent-153, score-0.389]
</p><p>65 1  Generating image features conditioned on text can be done in a similar way. [sent-154, score-0.251]
</p><p>66 5  This representation can then be used to do information retrieval for multimodal or unimodal queries. [sent-155, score-0.595]
</p><p>67 Each data point in the database (whether missing some modalities or not) can be mapped to this latent space. [sent-156, score-0.356]
</p><p>68 PHOW features are bags of image words obtained by extracting dense SIFT features over multiple scales and clustering them. [sent-181, score-0.167]
</p><p>69 2  Model Architecture and Learning  The image pathway consists of a Gaussian RBM with 3857 visible units followed by 2 layers of 1024 hidden units. [sent-184, score-0.607]
</p><p>70 The text pathway consists of a Replicated Softmax Model with 2000 visible units followed by 2 layers of 1024 hidden units. [sent-185, score-0.653]
</p><p>71 For discriminative tasks, we perform 1-vs-all classiﬁcation using logistic regression on the joint hidden layer representation. [sent-192, score-0.366]
</p><p>72 3  Classiﬁcation Tasks  Multimodal Inputs: Our ﬁrst set of experiments, evaluate the DBM as a discriminative model for multimodal data. [sent-195, score-0.424]
</p><p>73 For each model that we trained, the fused representation of the data was extracted and feed to a separate logistic regression for each of the 38 topics. [sent-196, score-0.175]
</p><p>74 The text input layer in the DBM was left unclamped when the text was missing. [sent-197, score-0.394]
</p><p>75 Linear Discriminant Analysis (LDA) and Support Vector Machines (SVMs) [2] were trained using the labeled data on concatenated image and text features that did not include SIFT-based features. [sent-200, score-0.283]
</p><p>76 Right: MAP using representations from different layers of multimodal DBMs and DBNs. [sent-238, score-0.465]
</p><p>77 To measure the effect of using unlabeled data, a DBM was trained using all the unlabeled examples that had both modalities present. [sent-239, score-0.383]
</p><p>78 We compared our model to two other deep learning models: Multimodal Deep Belief Network (DBN) and a deep Autoencoder model [5]. [sent-248, score-0.298]
</p><p>79 These models were trained with the same number of layers and hidden units as the DBM. [sent-249, score-0.4]
</p><p>80 Unimodal Inputs: Next, we evaluate the ability of the model to improve classiﬁcation of unimodal inputs by ﬁlling in other modalities. [sent-260, score-0.221]
</p><p>81 For multimodal models, the text input was only used during training. [sent-261, score-0.511]
</p><p>82 4 compares the Multimodal DBM model with an SVM over image features alone (ImageSVM) [2], a DBN over image features (Image-DBN) and a DBM over image features (ImageDBM). [sent-264, score-0.402]
</p><p>83 All deep models had the same depth and same number of hidden units in each layer. [sent-265, score-0.41]
</p><p>84 In one case (DBMZeroText), the state of the joint hidden layer was inferred keeping the missing text input clamped at zero. [sent-267, score-0.597]
</p><p>85 In the other case (DBM-GenText), the text input was not clamped and the model was allowed to update the state of the text input layer when performing mean-ﬁeld updates. [sent-268, score-0.501]
</p><p>86 In doing so, the model effectively ﬁlled-in the missing text modality (some examples of which are shown in Fig. [sent-269, score-0.371]
</p><p>87 The DBM-GenText model performs better than all other models, showing that the DBM is able to generate meaningful text that serves as a plausible proxy for missing data. [sent-272, score-0.25]
</p><p>88 This suggests that learning multimodal features helps even when some modalities are absent at test time. [sent-274, score-0.665]
</p><p>89 Having multiple modalities probably regularizes the model and makes it learn much better features. [sent-275, score-0.271]
</p><p>90 As we go deeper into the model from either input layer towards the middle, the internal representations get better. [sent-282, score-0.208]
</p><p>91 The joint layer in the middle serves as the most useful feature representation. [sent-283, score-0.205]
</p><p>92 For each model, all queries and all points in the database were mapped to the joint hidden representation under that model. [sent-293, score-0.346]
</p><p>93 6 shows some examples of multimodal queries and the top 4 retrieved results. [sent-301, score-0.467]
</p><p>94 Unimodal Queries: The DBM model can also be used to query for unimodal inputs by ﬁlling in the missing modality. [sent-303, score-0.314]
</p><p>95 5b shows the precision-recall curves for the DBM model along with other unimodal models, where each model received the same image queries as input. [sent-305, score-0.355]
</p><p>96 By effectively inferring the missing text, the DBM model was able to achieve far better results than any unimodal method (MAP of 0. [sent-306, score-0.249]
</p><p>97 5  Conclusion  We proposed a Deep Boltzmann Machine model for learning multimodal data representations. [sent-310, score-0.38]
</p><p>98 Pathways for each modality can be pretrained independently and “plugged in” together for doing joint training. [sent-312, score-0.231]
</p><p>99 The model fuses multiple data modalities into a uniﬁed representation. [sent-313, score-0.306]
</p><p>100 It also works nicely when some modalities are absent and improves upon models trained on only the observed modalities. [sent-315, score-0.304]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dbm', 0.631), ('multimodal', 0.35), ('modalities', 0.241), ('vm', 0.156), ('units', 0.153), ('hidden', 0.138), ('text', 0.127), ('unimodal', 0.126), ('modality', 0.121), ('deep', 0.119), ('rbm', 0.115), ('dbn', 0.114), ('layer', 0.106), ('boltzmann', 0.1), ('missing', 0.093), ('vimg', 0.088), ('vtxt', 0.088), ('queries', 0.088), ('replicated', 0.086), ('autoencoder', 0.086), ('image', 0.081), ('vt', 0.081), ('pathway', 0.08), ('visible', 0.078), ('layers', 0.077), ('retrieval', 0.077), ('rbms', 0.074), ('hj', 0.073), ('softmax', 0.072), ('huiskes', 0.071), ('hm', 0.066), ('inputs', 0.065), ('images', 0.06), ('fused', 0.059), ('count', 0.058), ('mir', 0.057), ('joint', 0.056), ('unlabeled', 0.055), ('dbms', 0.054), ('pretrained', 0.054), ('ht', 0.052), ('word', 0.048), ('map', 0.047), ('guillaumin', 0.047), ('beautiful', 0.047), ('lling', 0.046), ('lda', 0.045), ('discriminative', 0.044), ('clamped', 0.043), ('features', 0.043), ('representation', 0.042), ('tags', 0.041), ('flickr', 0.04), ('representations', 0.038), ('dbmzerotext', 0.035), ('fuses', 0.035), ('nitish', 0.035), ('phow', 0.035), ('prec', 0.035), ('sunset', 0.035), ('vmi', 0.035), ('input', 0.034), ('pathways', 0.033), ('trained', 0.032), ('precision', 0.032), ('clamping', 0.031), ('absent', 0.031), ('multimedia', 0.031), ('model', 0.03), ('machines', 0.029), ('captions', 0.029), ('modeling', 0.029), ('retrieved', 0.029), ('wij', 0.027), ('svms', 0.026), ('generative', 0.026), ('undirected', 0.025), ('svm', 0.025), ('pretraining', 0.025), ('aj', 0.024), ('vk', 0.024), ('ngiam', 0.024), ('responsibility', 0.024), ('contrastive', 0.023), ('useful', 0.022), ('energy', 0.022), ('salakhutdinov', 0.022), ('autoencoders', 0.022), ('feed', 0.022), ('mapped', 0.022), ('logistic', 0.022), ('vision', 0.022), ('ruslan', 0.021), ('middle', 0.021), ('tasks', 0.021), ('belief', 0.021), ('stochastic', 0.02), ('vi', 0.02), ('discover', 0.02), ('classi', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="229-tfidf-1" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>2 0.50319105 <a title="229-tfidf-2" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models. 1</p><p>3 0.159141 <a title="229-tfidf-3" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>Author: Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard S. Zemel, Ruslan Salakhutdinov, Ryan P. Adams</p><p>Abstract: The Restricted Boltzmann Machine (RBM) is a popular density model that is also good for extracting features. A main source of tractability in RBM models is that, given an input, the posterior distribution over hidden variables is factorizable and can be easily computed and sampled from. Sparsity and competition in the hidden representation is beneﬁcial, and while an RBM with competition among its hidden units would acquire some of the attractive properties of sparse coding, such constraints are typically not added, as the resulting posterior over the hidden units seemingly becomes intractable. In this paper we show that a dynamic programming algorithm can be used to implement exact sparsity in the RBM’s hidden units. We also show how to pass derivatives through the resulting posterior marginals, which makes it possible to ﬁne-tune a pre-trained neural network with sparse hidden layers. 1</p><p>4 0.12609282 <a title="229-tfidf-4" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><p>5 0.11253599 <a title="229-tfidf-5" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>6 0.11028014 <a title="229-tfidf-6" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>7 0.10891981 <a title="229-tfidf-7" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>8 0.10795651 <a title="229-tfidf-8" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>9 0.1037283 <a title="229-tfidf-9" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>10 0.099758364 <a title="229-tfidf-10" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>11 0.098148517 <a title="229-tfidf-11" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>12 0.091410227 <a title="229-tfidf-12" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>13 0.087712206 <a title="229-tfidf-13" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>14 0.086729996 <a title="229-tfidf-14" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>15 0.077911623 <a title="229-tfidf-15" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>16 0.074196629 <a title="229-tfidf-16" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>17 0.066126332 <a title="229-tfidf-17" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>18 0.064841412 <a title="229-tfidf-18" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>19 0.064006627 <a title="229-tfidf-19" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>20 0.062760748 <a title="229-tfidf-20" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, 0.067), (2, -0.209), (3, 0.016), (4, 0.01), (5, -0.04), (6, -0.004), (7, -0.028), (8, -0.006), (9, -0.035), (10, 0.055), (11, 0.168), (12, -0.043), (13, 0.221), (14, -0.107), (15, -0.079), (16, 0.045), (17, -0.07), (18, 0.139), (19, -0.181), (20, 0.004), (21, -0.133), (22, -0.063), (23, 0.086), (24, 0.057), (25, 0.1), (26, 0.113), (27, 0.032), (28, 0.15), (29, -0.196), (30, -0.044), (31, -0.046), (32, -0.138), (33, 0.02), (34, -0.11), (35, -0.069), (36, -0.073), (37, -0.073), (38, -0.081), (39, 0.01), (40, -0.056), (41, 0.161), (42, 0.21), (43, 0.063), (44, -0.001), (45, -0.103), (46, -0.042), (47, 0.014), (48, -0.028), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95068181 <a title="229-lsi-1" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models. 1</p><p>same-paper 2 0.93437678 <a title="229-lsi-2" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>3 0.78486419 <a title="229-lsi-3" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>Author: Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard S. Zemel, Ruslan Salakhutdinov, Ryan P. Adams</p><p>Abstract: The Restricted Boltzmann Machine (RBM) is a popular density model that is also good for extracting features. A main source of tractability in RBM models is that, given an input, the posterior distribution over hidden variables is factorizable and can be easily computed and sampled from. Sparsity and competition in the hidden representation is beneﬁcial, and while an RBM with competition among its hidden units would acquire some of the attractive properties of sparse coding, such constraints are typically not added, as the resulting posterior over the hidden units seemingly becomes intractable. In this paper we show that a dynamic programming algorithm can be used to implement exact sparsity in the RBM’s hidden units. We also show how to pass derivatives through the resulting posterior marginals, which makes it possible to ﬁne-tune a pre-trained neural network with sparse hidden layers. 1</p><p>4 0.53140748 <a title="229-lsi-4" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>Author: Gary Huang, Marwan Mattar, Honglak Lee, Erik G. Learned-miller</p><p>Abstract: Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face veriﬁcation. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Speciﬁcally, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the speciﬁc data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned ﬁlters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face veriﬁcation compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method. 1</p><p>5 0.51503772 <a title="229-lsi-5" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>Author: Pietro D. Lena, Ken Nagata, Pierre F. Baldi</p><p>Abstract: Residue-residue contact prediction is a fundamental problem in protein structure prediction. Hower, despite considerable research efforts, contact prediction methods are still largely unreliable. Here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules. For contact prediction, the idea is implemented as a three-dimensional stack of Neural Networks NNk , where i and j index the spatial coordinates of the contact ij map and k indexes “time”. The temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process, but rather a progressive reﬁnement. Networks at level k in the stack can be trained in supervised fashion to reﬁne the predictions produced by the previous level, hence addressing the problem of vanishing gradients, typical of deep architectures. Increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction. The deep approach leads to an accuracy for difﬁcult long-range contacts of about 30%, roughly 10% above the state-of-the-art. Many variations in the architectures and the training algorithms are possible, leaving room for further improvements. Furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components. 1</p><p>6 0.46676263 <a title="229-lsi-6" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>7 0.46303859 <a title="229-lsi-7" href="./nips-2012-Large_Scale_Distributed_Deep_Networks.html">170 nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>8 0.45048767 <a title="229-lsi-8" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>9 0.42975023 <a title="229-lsi-9" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>10 0.42886013 <a title="229-lsi-10" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>11 0.39697114 <a title="229-lsi-11" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>12 0.372895 <a title="229-lsi-12" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>13 0.36434823 <a title="229-lsi-13" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>14 0.35392025 <a title="229-lsi-14" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>15 0.33781123 <a title="229-lsi-15" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>16 0.30386958 <a title="229-lsi-16" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>17 0.30185011 <a title="229-lsi-17" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>18 0.27974749 <a title="229-lsi-18" href="./nips-2012-Co-Regularized_Hashing_for_Multimodal_Data.html">71 nips-2012-Co-Regularized Hashing for Multimodal Data</a></p>
<p>19 0.27864283 <a title="229-lsi-19" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>20 0.27830598 <a title="229-lsi-20" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.041), (15, 0.023), (21, 0.028), (35, 0.198), (38, 0.078), (42, 0.028), (53, 0.024), (54, 0.024), (55, 0.052), (74, 0.067), (76, 0.11), (80, 0.12), (92, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80675894 <a title="229-lda-1" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>2 0.74541378 <a title="229-lda-2" href="./nips-2012-Minimization_of_Continuous_Bethe_Approximations%3A_A_Positive_Variation.html">213 nips-2012-Minimization of Continuous Bethe Approximations: A Positive Variation</a></p>
<p>Author: Jason Pacheco, Erik B. Sudderth</p><p>Abstract: We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions. While existing message passing algorithms deﬁne ﬁxed point iterations corresponding to stationary points of the Bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge. For continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as Gaussians with negative variance. Conversely, our approach leverages multiplier methods with well-understood convergence properties, and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random ﬁelds, showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation. 1</p><p>3 0.71888506 <a title="229-lda-3" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>Author: Michael Bryant, Erik B. Sudderth</p><p>Abstract: Variational methods provide a computationally scalable alternative to Monte Carlo methods for large-scale, Bayesian nonparametric learning. In practice, however, conventional batch and online variational methods quickly become trapped in local optima. In this paper, we consider a nonparametric topic model based on the hierarchical Dirichlet process (HDP), and develop a novel online variational inference algorithm based on split-merge topic updates. We derive a simpler and faster variational approximation of the HDP, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms. For streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</p><p>4 0.71646547 <a title="229-lda-4" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>Author: Richard Socher, Brody Huval, Bharath Bath, Christopher D. Manning, Andrew Y. Ng</p><p>Abstract: Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, ﬁxed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efﬁcient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. 1</p><p>5 0.70930743 <a title="229-lda-5" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>6 0.70814663 <a title="229-lda-6" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>7 0.69656652 <a title="229-lda-7" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>8 0.69498879 <a title="229-lda-8" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>9 0.69463146 <a title="229-lda-9" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>10 0.68934858 <a title="229-lda-10" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>11 0.68830913 <a title="229-lda-11" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>12 0.68611932 <a title="229-lda-12" href="./nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">291 nips-2012-Reducing statistical time-series problems to binary classification</a></p>
<p>13 0.68596083 <a title="229-lda-13" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>14 0.68572068 <a title="229-lda-14" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>15 0.68451709 <a title="229-lda-15" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>16 0.68217194 <a title="229-lda-16" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>17 0.68108994 <a title="229-lda-17" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>18 0.68072867 <a title="229-lda-18" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>19 0.67947036 <a title="229-lda-19" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>20 0.67645293 <a title="229-lda-20" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
