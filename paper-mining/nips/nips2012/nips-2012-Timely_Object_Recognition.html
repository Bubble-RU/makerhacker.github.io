<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>344 nips-2012-Timely Object Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-344" href="#">nips2012-344</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>344 nips-2012-Timely Object Recognition</h1>
<br/><p>Source: <a title="nips-2012-344-pdf" href="http://papers.nips.cc/paper/4712-timely-object-recognition.pdf">pdf</a></p><p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>Reference: <a title="nips-2012-344-reference" href="../nips2012_reference/nips-2012-Timely_Object_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Timely Object Recognition  Sergey Karayev UC Berkeley  Tobias Baumgartner RWTH Aachen University  Mario Fritz MPI for Informatics  Trevor Darrell UC Berkeley  Abstract In a large visual multi-class detection framework, the timeliness of results can be crucial. [sent-1, score-0.419]
</p><p>2 Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. [sent-2, score-0.488]
</p><p>3 Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. [sent-3, score-0.641]
</p><p>4 In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. [sent-4, score-0.269]
</p><p>5 Experiments are conducted on the PASCAL VOC object detection dataset. [sent-7, score-0.446]
</p><p>6 If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. [sent-8, score-0.306]
</p><p>7 Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. [sent-10, score-0.447]
</p><p>8 In large-scale detection systems, such as image search, results need to be obtained quickly per image as the number of items to process is constantly growing. [sent-13, score-0.454]
</p><p>9 The detection strategy to maximize proﬁt in such an environment has to exploit every inter-object context signal available to it, because there is not enough time to run detection for all classes. [sent-17, score-0.635]
</p><p>10 What matters in the real world is timeliness, and either not all images can be processed or not all classes can be evaluated in a detection task. [sent-18, score-0.339]
</p><p>11 Taking the task of object detection, we propose a new timeliness measure of performance vs. [sent-21, score-0.299]
</p><p>12 We present a method that treats different detectors and classiﬁers as black boxes, and uses reinforcement learning to learn a dynamic policy for selecting actions to achieve the highest performance under this evaluation. [sent-23, score-0.683]
</p><p>13 Speciﬁcally, we run scene context and object class detectors over the whole image sequentially, using the results of detection obtained so far to select the next actions. [sent-24, score-0.881]
</p><p>14 Evaluating on the PASCAL 1  Ts  Td C1  t=0  C2  C3 3  agist  scene context 2  Ts  adet1  adet2  adet3  Td C3  t = 0. [sent-25, score-0.276]
</p><p>15 1  bicycle detector  time  C1 C2 machine translation and information retrieval. [sent-26, score-0.299]
</p><p>16 While it is possible that moreback at least to the bootstrapping methods used by [38] and the highest scoring detection from each of the other complete and [35]. [sent-42, score-0.266]
</p><p>17 Their visualization show the positive features the PASCAL object detection challenge, used a single ﬁlter dimensionality of the feature vector can be constrained to be in a sparse set of locations determined weights at different orientations. [sent-61, score-0.561]
</p><p>18 In a latent SVM each example patchwork of parts model from [2] arise in the PASCAL object detection challenge and sim- ing pairs of parts. [sent-69, score-0.492]
</p><p>19 In the case of one A major innovation of the Dalal-Triggs detector was the of our star models is the concatenation of the root construction of particularly effective features. [sent-81, score-0.313]
</p><p>20 score of the root ﬁlter at the given location plus the Our second class of models represents each object sum over parts of the maximum, over placements of category by a mixture of star models. [sent-86, score-0.501]
</p><p>21 The score of one that part, of the part ﬁlter score on its location minus of our mixture models at a given position and scale a deformation cost measuring the deviation of the part is the maximum over components, of the score of that from its ideal location. [sent-87, score-0.408]
</p><p>22 case of object detection the training problem is highly unTo train models using partially labeled data we use a balanced because there is vastly more background than latent variable formulation of MI-SVM [3] that we call objects. [sent-95, score-0.446]
</p><p>23 3  person detector  adet3  Figure 1: A sample trace of our method. [sent-97, score-0.268]
</p><p>24 At each time step beginning at t = 0, potential actions are considered according to their predicted value, and the maximizing action is picked. [sent-98, score-0.511]
</p><p>25 Different actions return different observations: a detector returns a list of detections, while a scene context action simply returns its computed feature. [sent-100, score-0.898]
</p><p>26 The ﬁnal evaluation of a detection episode is the area of the AP vs. [sent-102, score-0.431]
</p><p>27 The value of an action is the expected result of ﬁnal evaluation if the action is taken and the policy continues to be followed, which allows actions without an immediate beneﬁt to be scheduled. [sent-104, score-1.11]
</p><p>28 Each object is labeled with exactly one category label k ∈ {1, . [sent-107, score-0.229]
</p><p>29 The multi-class, multi-label classiﬁcation problem asks whether I contains at least one object of class k. [sent-111, score-0.226]
</p><p>30 The detection problem is to output a list of bounding boxes (sub-images deﬁned by four coordinates), each with a real-valued conﬁdence that it encloses a single instance of an object of class k, for each k. [sent-116, score-0.656]
</p><p>31 The answer for a single class k is given by an algorithm detect(I, k), which outputs a list of sub-image bounding boxes B and their associated conﬁdences. [sent-117, score-0.255]
</p><p>32 A common measure of a correct detection is the PASCAL overlap: two bounding boxes are considered to match if they have the same class label and the ratio of their 1 intersection to their union is at least 2 . [sent-121, score-0.424]
</p><p>33 To highlight the hierarchical structure of these problems, we note that the conﬁdences for each subimage b ∈ B may be given by classify(b, k), and, more saliently for our setup, correct answer to the detection problem also answers the classiﬁcation problem. [sent-122, score-0.311]
</p><p>34 1  Related Work  Object detection The best recent performance has come from detectors that use gradient-based features to represent objects as either a collection of local patches or as object-sized windows [2, 3]. [sent-126, score-0.445]
</p><p>35 Using context The most common source of context for detection is the scene or other non-detector cues; the most common scene-level feature is the GIST [6] of the image. [sent-131, score-0.53]
</p><p>36 Inter-object context has also been shown to improve detection [7]. [sent-133, score-0.336]
</p><p>37 In a standard evaluation setup, inter-object context plays a role only in post-ﬁltering, once all detectors have been run. [sent-134, score-0.263]
</p><p>38 A critical summary of the main approaches to using context for object and scene recognition is given in [8]. [sent-136, score-0.391]
</p><p>39 Efﬁciency through cascades An early success in efﬁcient object detection of a single class uses simple, fast features to build up a cascade of classiﬁers, which then considers image regions in a sliding window regime [10]. [sent-138, score-0.834]
</p><p>40 A recent application to the problem of visual detection picks features with maximum value of information in a Hough-voting framework [12]. [sent-142, score-0.341]
</p><p>41 In contrast, we learn policies that take actions without any immediate reward. [sent-145, score-0.295]
</p><p>42 3  Multi-class Recognition Policy  Our goal is a multi-class recognition policy π that takes an image I and outputs a list of multi-class detection results by running detector and global scene actions sequentially. [sent-146, score-1.233]
</p><p>43 The policy repeatedly selects an action ai ∈ A, executes it, receiving observations oi , and then selects the next action. [sent-147, score-0.774]
</p><p>44 The set of actions A can include both classiﬁers and detectors: anything that would be useful for inferring the contents of the image. [sent-148, score-0.271]
</p><p>45 Each action ai has an expected cost c(ai ) of execution. [sent-149, score-0.337]
</p><p>46 We take the empirical approach: every executed action advances t, the time into episode, by its runtime. [sent-151, score-0.309]
</p><p>47 As shown in Figure 1, the system is given two times: the setup time Ts and deadline Td . [sent-152, score-0.257]
</p><p>48 We evaluate policies by this more robust metric and not simply by the ﬁnal performance at deadline time for the same 3  reason that Average Precision is used instead of a ﬁxed Precision vs. [sent-155, score-0.239]
</p><p>49 1  Sequential Execution  An open-loop policy, such as the common classiﬁer cascade [10], takes actions in a sequence that does not depend on observations received from previous actions. [sent-158, score-0.315]
</p><p>50 Additionally, the state records that an action ai has been taken by adding it to the initially empty set O and recording the resulting observations oi . [sent-165, score-0.472]
</p><p>51 The state also keeps track of the time into episode t, and the setup and deadline times Ts , Td . [sent-167, score-0.372]
</p><p>52 A recognition episode takes an image I and proceeds from the initial state s0 and action a0 to the next pair (s1 , a1 ), and so on until (sJ , aJ ), where J is the last step of the process with t ≤ Td . [sent-168, score-0.573]
</p><p>53 At that point, the policy is terminated, and a new episode can begin on a new image. [sent-169, score-0.377]
</p><p>54 The speciﬁc actions we consider in the following exposition are detector actions adeti , where deti is a detector class Ci , and a scene-level context action agist , which updates the probabilities of all classes. [sent-170, score-1.385]
</p><p>55 Although we avoid this in the exposition, note that our system easily handles multiple detector actions per class. [sent-171, score-0.447]
</p><p>56 2  Selecting actions  As our goal is to pick actions dynamically, we want a function Q(s, a) : S × A → R, where S is the space of all possible states, to assign a value to a potential action a ∈ A given the current state s of the decision process. [sent-173, score-0.719]
</p><p>57 We featurize the state-action pair and assume linear structure: Qπ (s, a) = θπ φ(s, a)  (2)  The policy’s performance at time t is determined by all detections that are part of the set of observations oj at the last state sj before t. [sent-175, score-0.457]
</p><p>58 Recall that detector actions returns lists of detection hypotheses. [sent-176, score-0.679]
</p><p>59 Time evaluation of an episode is a function eval(h, Ts , Td ) of the history of execution h = s0 , s1 , . [sent-178, score-0.289]
</p><p>60 Note from Figure 3b that this evaluation function is additive per action, as each action a generates observations that may raise or lower the mean AP of the results so far (∆ap) and takes a certain time (∆t). [sent-184, score-0.414]
</p><p>61 We can accordingly represent the ﬁnal evaluation eval(h, Ts , Td ) in terms of individual action J rewards: j=0 R(sj , aj ). [sent-185, score-0.331]
</p><p>62 Speciﬁcally, as shown in Figure 3b, we deﬁne the reward of an action a as 1 R(sj , a) = ∆ap(tj − ∆t) T 2  (3)  where tj is the time left until Td at state sj , and ∆t and ∆ap are the time taken and AP change T produced by the action a. [sent-186, score-0.887]
</p><p>63 3  Learning the policy  The expected value of the ﬁnal evaluation can be written recursively in terms of the value function: Qπ (sj , a) = Esj+1 [R(sj , a) + γQπ (sj+1 , π(sj+1 ))]  (4)  where γ ∈ [0, 1] is the discount value. [sent-189, score-0.322]
</p><p>64 While we can’t directly compute the expectation in (4), we can sample it by running actual episodes to gather < s, a, r, s > samples, where r is the reward obtained by taking action a in state s, and s is the following state. [sent-195, score-0.42]
</p><p>65 We then learn the optimal policy by repeatedly gathering samples with the current policy, minimizing the error between the discounted reward to the end of the episode as predicted by our current Q(sj , a) and the actual values gathered, and updating the policy with the resulting weights. [sent-196, score-0.707]
</p><p>66 To ensure sufﬁcient exploration of the state space, we implement -greedy action selection during training: with a probability that decreases with each training iteration, a random action is selected instead of following the policy. [sent-197, score-0.591]
</p><p>67 We run 15 iterations of accumulating samples by running 350 episodes, starting with a baseline policy which will be described in section 4, and cross-validating the regularization parameter at each iteration. [sent-201, score-0.267]
</p><p>68 4  Feature representation  Our policy is at its base determined by a linear function of the features of the state: π(s) = argmax θπ φ(s, ai ). [sent-205, score-0.369]
</p><p>69 H(CK |o)  The prior probability of the class that corresponds to the detector of action a (omitted for the scene-context action). [sent-212, score-0.533]
</p><p>70 To formulate learning the policy as a single regression problem, we represent the features in block form, where φ(s, a) is a vector of size F |A|, with all values set to 0 except for the F -sized block corresponding to a. [sent-222, score-0.308]
</p><p>71 Note that in the greedy learning case, this action is learned to never be taken, but it is shown to be useful in the reinforcement learning case. [sent-225, score-0.419]
</p><p>72 This allows the action-value function to learn correlations between presence of different classes, and so the policy can look for the most probable classes given the observations. [sent-231, score-0.305]
</p><p>73 4  Greedy  As an illustration, we visualize the learned weights on these features in Figure 2, reshaped such that each row shows the weights learned for an action, with the top row representing the scene context action and then next 20 rows corresponding to the PASCAL VOC class detector actions. [sent-243, score-0.805]
</p><p>74 Evaluation  We evaluate our system on the multi-class, multi-label detection task, as previously described. [sent-244, score-0.3]
</p><p>75 We evaluate on a popular detection challenge task: the PASCAL VOC 2007 dataset [1]. [sent-245, score-0.266]
</p><p>76 We learn weights on the training and validation sets, and run our policy on all images in the testing set. [sent-247, score-0.339]
</p><p>77 6  For the detector actions, we use one-vs-all cascaded deformable part-model detectors on a HOG featurization of the image [21], with linear classiﬁcation of the list of detections as described in the previous section. [sent-252, score-0.702]
</p><p>78 There are 20 classes in the PASCAL challenge task, so there are 20 detector actions. [sent-253, score-0.249]
</p><p>79 Running a detector on a PASCAL image takes about 1 second. [sent-254, score-0.305]
</p><p>80 In the ﬁrst one, the start time is immediate and execution is cut off at 20 seconds, which is enough time to run all actions. [sent-256, score-0.257]
</p><p>81 ap  t ap(tj T tj T  Ts  (a)  1 t) 2 Td  (b)  Figure 3: (a) AP vs. [sent-261, score-0.303]
</p><p>82 We establish the ﬁrst baseline for our system by selecting actions randomly at each step. [sent-265, score-0.236]
</p><p>83 As shown in Figure 3a, the Random policy results in a roughly linear gain of AP vs. [sent-266, score-0.267]
</p><p>84 This is expected: the detectors are capable of obtaining a certain level of performance; if half the detectors are run, the expected performance level is half of the maximum level. [sent-268, score-0.276]
</p><p>85 To establish an upper bound on performance, we plot the Oracle policy, obtained by re-ordering the actions at the end of each detection episode in the order of AP gains they produced. [sent-269, score-0.578]
</p><p>86 In Figure 3a, we can see that due to the dataset bias, the ﬁxed-order policy performs well at ﬁrst, as the person class is disproportionately likely to be in the image, but is signiﬁcantly overtaken by our model as execution goes on and more rare classes have to be detected. [sent-275, score-0.532]
</p><p>87 Visualizing the learned weights in Figure 2, we note that the GIST action is learned to never be taken in the greedy (γ = 0) setting, but is learned to be taken with a higher value of γ. [sent-282, score-0.38]
</p><p>88 It is additionally informative to consider the action trajectories of different policies in Figure 4. [sent-283, score-0.367]
</p><p>89 7  Figure 4: Visualizing the action trajectories of different policies. [sent-284, score-0.276]
</p><p>90 We see that the Random policy selects actions and obtains rewards randomly, while the Oracle policy obtains all rewards in the ﬁrst few actions. [sent-286, score-0.965]
</p><p>91 The Fixed Order policy selects actions in a static optimal order. [sent-287, score-0.506]
</p><p>92 Our policy does not stick a static order but selects actions dynamically to maximize the rewards obtained early on. [sent-288, score-0.558]
</p><p>93 530  Conclusion  We presented a method for learning “closed-loop” policies for multi-class object recognition, given existing object detectors and classiﬁers and a metric to optimize. [sent-306, score-0.557]
</p><p>94 The method learns the optimal policy using reinforcement learning, by observing execution traces in training. [sent-307, score-0.506]
</p><p>95 If detection on an image is cut off after only half the detectors have been run, our method does 66% better than a random ordering, and 14% better than an intelligent baseline. [sent-308, score-0.498]
</p><p>96 In particular, our method learns to take action with no intermediate reward in order to improve the overall performance of the system. [sent-309, score-0.339]
</p><p>97 Here, we derive it for the novel detection AP vs. [sent-311, score-0.266]
</p><p>98 Although computation devoted to scheduling actions is less signiﬁcant than the computation due to running the actions, the next research direction is to explicitly consider this decision-making cost; the same goes for feature computation costs. [sent-313, score-0.274]
</p><p>99 Additionally, it is interesting to consider actions deﬁned not just by object category but also by spatial region. [sent-314, score-0.431]
</p><p>100 Rapid object detection using a boosted cascade of simple features. [sent-348, score-0.509]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('action', 0.276), ('policy', 0.267), ('detection', 0.266), ('ap', 0.253), ('detector', 0.211), ('gist', 0.207), ('actions', 0.202), ('object', 0.18), ('deadline', 0.147), ('detections', 0.141), ('detectors', 0.138), ('td', 0.136), ('pascal', 0.126), ('execution', 0.124), ('agist', 0.119), ('timeliness', 0.119), ('sj', 0.117), ('episode', 0.11), ('voc', 0.101), ('lter', 0.1), ('image', 0.094), ('ck', 0.093), ('ci', 0.093), ('scene', 0.087), ('score', 0.078), ('reinforcement', 0.076), ('rl', 0.072), ('boxes', 0.07), ('context', 0.07), ('contents', 0.069), ('greedy', 0.067), ('deformable', 0.066), ('mrf', 0.064), ('reward', 0.063), ('cascade', 0.063), ('ai', 0.061), ('policies', 0.059), ('deformation', 0.058), ('lters', 0.057), ('person', 0.057), ('evaluation', 0.055), ('ers', 0.055), ('ts', 0.055), ('bicycle', 0.055), ('recognition', 0.054), ('star', 0.054), ('fixed', 0.054), ('classi', 0.053), ('list', 0.052), ('rewards', 0.052), ('hog', 0.051), ('window', 0.051), ('tj', 0.05), ('exhaustively', 0.05), ('observations', 0.05), ('category', 0.049), ('root', 0.048), ('cascades', 0.048), ('adeti', 0.048), ('eval', 0.048), ('oi', 0.046), ('class', 0.046), ('parts', 0.046), ('answer', 0.045), ('sliding', 0.045), ('obtains', 0.044), ('curve', 0.043), ('setup', 0.043), ('episodes', 0.042), ('timely', 0.042), ('lsvm', 0.042), ('sideways', 0.042), ('bounding', 0.042), ('part', 0.041), ('features', 0.041), ('traces', 0.039), ('state', 0.039), ('deva', 0.039), ('subwindows', 0.039), ('classes', 0.038), ('selects', 0.037), ('weights', 0.037), ('feature', 0.037), ('er', 0.037), ('oj', 0.036), ('images', 0.035), ('advertising', 0.035), ('scheduling', 0.035), ('varun', 0.035), ('position', 0.034), ('cvpr', 0.034), ('visual', 0.034), ('immediate', 0.034), ('oracle', 0.034), ('system', 0.034), ('time', 0.033), ('triggs', 0.033), ('labeling', 0.033), ('start', 0.033), ('additionally', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="344-tfidf-1" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>2 0.27038479 <a title="344-tfidf-2" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>3 0.25104764 <a title="344-tfidf-3" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>Author: Abdeslam Boularias, Jan R. Peters, Oliver B. Kroemer</p><p>Abstract: We use a graphical model for representing policies in Markov Decision Processes. This new representation can easily incorporate domain knowledge in the form of a state similarity graph that loosely indicates which states are supposed to have similar optimal actions. A bias is then introduced into the policy search process by sampling policies from a distribution that assigns high probabilities to policies that agree with the provided state similarity graph, i.e. smoother policies. This distribution corresponds to a Markov Random Field. We also present forward and inverse reinforcement learning algorithms for learning such policy distributions. We illustrate the advantage of the proposed approach on two problems: cart-balancing with swing-up, and teaching a robot to grasp unknown objects. 1</p><p>4 0.2492345 <a title="344-tfidf-4" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>Author: Jiarong Jiang, Adam Teichert, Jason Eisner, Hal Daume</p><p>Abstract: Users want inference to be both fast and accurate, but quality often comes at the cost of speed. The ﬁeld has experimented with approximate inference algorithms that make different speed-accuracy tradeoffs (for particular problems and datasets). We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing [12]. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the “teacher” follows a far better policy than anything in our learner’s policy space, free of the speed-accuracy tradeoff that arises when oracle information is unavailable, and thus largely insensitive to the known reward functﬁon. We propose a hybrid reinforcement/apprenticeship learning algorithm that learns to speed up an initial policy, trading off accuracy for speed according to various settings of a speed term in the loss function. 1</p><p>5 0.24188614 <a title="344-tfidf-5" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>Author: Bogdan Alexe, Nicolas Heess, Yee W. Teh, Vittorio Ferrari</p><p>Abstract: The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired. We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image, exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while achieving higher object detection performance. 1</p><p>6 0.22044422 <a title="344-tfidf-6" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>7 0.20665459 <a title="344-tfidf-7" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>8 0.19839442 <a title="344-tfidf-8" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>9 0.19241923 <a title="344-tfidf-9" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>10 0.19183123 <a title="344-tfidf-10" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>11 0.18876754 <a title="344-tfidf-11" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>12 0.17466502 <a title="344-tfidf-12" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>13 0.16561876 <a title="344-tfidf-13" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>14 0.16355281 <a title="344-tfidf-14" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>15 0.16218752 <a title="344-tfidf-15" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>16 0.15929006 <a title="344-tfidf-16" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>17 0.15777157 <a title="344-tfidf-17" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>18 0.14741832 <a title="344-tfidf-18" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>19 0.14664666 <a title="344-tfidf-19" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>20 0.14486471 <a title="344-tfidf-20" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, -0.347), (2, -0.271), (3, -0.064), (4, 0.121), (5, -0.121), (6, 0.009), (7, -0.108), (8, 0.023), (9, -0.032), (10, -0.077), (11, 0.02), (12, 0.138), (13, -0.104), (14, 0.136), (15, 0.14), (16, 0.005), (17, -0.045), (18, -0.063), (19, 0.034), (20, 0.004), (21, -0.014), (22, -0.025), (23, -0.011), (24, -0.038), (25, -0.04), (26, 0.007), (27, 0.066), (28, -0.01), (29, -0.021), (30, 0.017), (31, 0.03), (32, -0.004), (33, 0.042), (34, -0.076), (35, 0.031), (36, 0.007), (37, 0.001), (38, -0.041), (39, -0.005), (40, 0.044), (41, -0.003), (42, 0.014), (43, 0.008), (44, -0.008), (45, -0.074), (46, 0.016), (47, 0.042), (48, 0.0), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96419525 <a title="344-lsi-1" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>2 0.85720581 <a title="344-lsi-2" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>3 0.75746429 <a title="344-lsi-3" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>Author: Bogdan Alexe, Nicolas Heess, Yee W. Teh, Vittorio Ferrari</p><p>Abstract: The dominant visual search paradigm for object class detection is sliding windows. Although simple and effective, it is also wasteful, unnatural and rigidly hardwired. We propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations. Our strategies adapt to the class being searched and to the content of a particular test image, exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. In addition to being more elegant than sliding windows, we demonstrate experimentally on the PASCAL VOC 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while achieving higher object detection performance. 1</p><p>4 0.72610581 <a title="344-lsi-4" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>5 0.70698893 <a title="344-lsi-5" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>6 0.6808024 <a title="344-lsi-6" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>7 0.67987764 <a title="344-lsi-7" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>8 0.67888016 <a title="344-lsi-8" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>9 0.65680176 <a title="344-lsi-9" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>10 0.64886087 <a title="344-lsi-10" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>11 0.63987327 <a title="344-lsi-11" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>12 0.63650793 <a title="344-lsi-12" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>13 0.63304734 <a title="344-lsi-13" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>14 0.62728065 <a title="344-lsi-14" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>15 0.61390108 <a title="344-lsi-15" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>16 0.60244828 <a title="344-lsi-16" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>17 0.5948087 <a title="344-lsi-17" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>18 0.5895201 <a title="344-lsi-18" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>19 0.5798161 <a title="344-lsi-19" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>20 0.56871271 <a title="344-lsi-20" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (21, 0.04), (38, 0.108), (42, 0.031), (54, 0.305), (55, 0.013), (74, 0.118), (76, 0.157), (80, 0.081), (92, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94692653 <a title="344-lda-1" href="./nips-2012-Symbolic_Dynamic_Programming_for_Continuous_State_and_Observation_POMDPs.html">331 nips-2012-Symbolic Dynamic Programming for Continuous State and Observation POMDPs</a></p>
<p>Author: Zahra Zamani, Scott Sanner, Pascal Poupart, Kristian Kersting</p><p>Abstract: Point-based value iteration (PBVI) methods have proven extremely effective for ﬁnding (approximately) optimal dynamic programming solutions to partiallyobservable Markov decision processes (POMDPs) when a set of initial belief states is known. However, no PBVI work has provided exact point-based backups for both continuous state and observation spaces, which we tackle in this paper. Our key insight is that while there may be an inﬁnite number of observations, there are only a ﬁnite number of continuous observation partitionings that are relevant for optimal decision-making when a ﬁnite, ﬁxed set of reachable belief states is considered. To this end, we make two important contributions: (1) we show how previous exact symbolic dynamic programming solutions for continuous state MDPs can be generalized to continuous state POMDPs with discrete observations, and (2) we show how recently developed symbolic integration methods allow this solution to be extended to PBVI for continuous state and observation POMDPs with potentially correlated, multivariate continuous observation spaces. 1</p><p>2 0.89826995 <a title="344-lda-2" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>Author: Grégoire Montavon, Katja Hansen, Siamac Fazli, Matthias Rupp, Franziska Biegler, Andreas Ziehe, Alexandre Tkatchenko, Anatole V. Lilienfeld, Klaus-Robert Müller</p><p>Abstract: The accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design. The inherently graph-like, non-vectorial nature of molecular data gives rise to a unique and difﬁcult machine learning problem. In this paper, we adopt a learning-from-scratch approach where quantum-mechanical molecular energies are predicted directly from the raw molecular geometry. The study suggests a beneﬁt from setting ﬂexible priors and enforcing invariance stochastically rather than structurally. Our results improve the state-of-the-art by a factor of almost three, bringing statistical methods one step closer to chemical accuracy. 1</p><p>same-paper 3 0.87960213 <a title="344-lda-3" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>4 0.86641544 <a title="344-lda-4" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>Author: Paul Vernaza, Drew Bagnell</p><p>Abstract: Maximum entropy (MaxEnt) modeling is a popular choice for sequence analysis in applications such as natural language processing, where the sequences are embedded in discrete, tractably-sized spaces. We consider the problem of applying MaxEnt to distributions over paths in continuous spaces of high dimensionality— a problem for which inference is generally intractable. Our main contribution is to show that this intractability can be avoided as long as the constrained features possess a certain kind of low dimensional structure. In this case, we show that the associated partition function is symmetric and that this symmetry can be exploited to compute the partition function efﬁciently in a compressed form. Empirical results are given showing an application of our method to learning models of high-dimensional human motion capture data. 1</p><p>5 0.85370636 <a title="344-lda-5" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>Author: Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, Erkki Oja</p><p>Abstract: Nonnegative Matrix Factorization (NMF) is a promising relaxation technique for clustering analysis. However, conventional NMF methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples. Here we propose a new NMF clustering method which replaces the approximated matrix with its smoothed version using random walk. Our method can thus accommodate farther relationships between data samples. Furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering. The new learning objective is optimized by a multiplicative Majorization-Minimization algorithm with a scalable implementation for learning the factorizing matrix. Extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity. 1</p><p>6 0.84333593 <a title="344-lda-6" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>7 0.75500411 <a title="344-lda-7" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>8 0.74902803 <a title="344-lda-8" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>9 0.72996587 <a title="344-lda-9" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>10 0.72500074 <a title="344-lda-10" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>11 0.72460383 <a title="344-lda-11" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>12 0.72160679 <a title="344-lda-12" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>13 0.71676475 <a title="344-lda-13" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>14 0.71441442 <a title="344-lda-14" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>15 0.71004367 <a title="344-lda-15" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>16 0.70437628 <a title="344-lda-16" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>17 0.70196313 <a title="344-lda-17" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>18 0.69697714 <a title="344-lda-18" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>19 0.69661093 <a title="344-lda-19" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>20 0.69545293 <a title="344-lda-20" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
