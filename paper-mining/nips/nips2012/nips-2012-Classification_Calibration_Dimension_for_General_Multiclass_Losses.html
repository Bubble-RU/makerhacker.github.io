<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 nips-2012-Classification Calibration Dimension for General Multiclass Losses</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-67" href="#">nips2012-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 nips-2012-Classification Calibration Dimension for General Multiclass Losses</h1>
<br/><p>Source: <a title="nips-2012-67-pdf" href="http://papers.nips.cc/paper/4528-classification-calibration-dimension-for-general-multiclass-losses.pdf">pdf</a></p><p>Author: Harish G. Ramaswamy, Shivani Agarwal</p><p>Abstract: We study consistency properties of surrogate loss functions for general multiclass classiﬁcation problems, deﬁned by a general loss matrix. We extend the notion of classiﬁcation calibration, which has been studied for binary and multiclass 0-1 classiﬁcation problems (and for certain other speciﬁc learning problems), to the general multiclass setting, and derive necessary and sufﬁcient conditions for a surrogate loss to be classiﬁcation calibrated with respect to a loss matrix in this setting. We then introduce the notion of classiﬁcation calibration dimension of a multiclass loss matrix, which measures the smallest ‘size’ of a prediction space for which it is possible to design a convex surrogate that is classiﬁcation calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al. (2010) for analyzing the difﬁculty of designing ‘low-dimensional’ convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classiﬁcation calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems. 1</p><p>Reference: <a title="nips-2012-67-reference" href="../nips2012_reference/nips-2012-Classification_Calibration_Dimension_for_General_Multiclass_Losses_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 in  Abstract We study consistency properties of surrogate loss functions for general multiclass classiﬁcation problems, deﬁned by a general loss matrix. [sent-5, score-1.139]
</p><p>2 We then introduce the notion of classiﬁcation calibration dimension of a multiclass loss matrix, which measures the smallest ‘size’ of a prediction space for which it is possible to design a convex surrogate that is classiﬁcation calibrated with respect to the loss matrix. [sent-7, score-1.923]
</p><p>3 We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. [sent-8, score-0.249]
</p><p>4 (2010) for analyzing the difﬁculty of designing ‘low-dimensional’ convex surrogates that are consistent with respect to pairwise subset ranking losses. [sent-10, score-0.44]
</p><p>5 We anticipate the classiﬁcation calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems. [sent-11, score-1.184]
</p><p>6 Such ﬁnite-output problems can all be viewed as instances of a general multiclass learning problem, whose structure is deﬁned by a loss function, or equivalently, by a loss matrix. [sent-13, score-0.666]
</p><p>7 While the studies above have contributed to the understanding of learning problems corresponding to certain forms of loss matrices, a framework for analyzing consistency properties for a general multiclass learning problem, deﬁned by a general loss matrix, has remained elusive. [sent-14, score-0.856]
</p><p>8 In this paper, we analyze consistency of surrogate losses for general multiclass learning problems, building on the results of [3, 5–7] and others. [sent-15, score-0.886]
</p><p>9 We start in Section 2 with some background and examples that will be used as running examples to illustrate concepts throughout the paper, and formalize the notion of classiﬁcation calibration with respect to a general loss matrix. [sent-16, score-0.61]
</p><p>10 In Section 3, we derive both necessary and sufﬁcient conditions for classiﬁcation calibration with respect to general multiclass losses; these are both of independent interest and useful in our later results. [sent-17, score-0.635]
</p><p>11 Section 4 introduces the notion of classiﬁcation calibration dimension of a loss matrix, a fundamental quantity that measures the smallest ‘size’ of a prediction space for which it is possible to design a convex surrogate that is classiﬁcation calibrated with respect to the loss matrix. [sent-18, score-1.723]
</p><p>12 We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. [sent-19, score-0.249]
</p><p>13 [10] for analyzing the difﬁculty of designing ‘low-dimensional’ convex surrogates that are consistent with respect to certain pairwise subset ranking losses. [sent-21, score-0.482]
</p><p>14 , k} of target labels in which predictions are to be made, and a loss function � : Y × T →[0, ∞), where �(y, t) denotes the loss incurred on predicting t ∈ T when the label is y ∈ Y. [sent-37, score-0.636]
</p><p>15 We will ﬁnd it convenient to represent the loss function � as a loss matrix L ∈ Rn×k (here R+ = + [0, ∞)), and for each y ∈ [n], t ∈ [k], will denote by �yt the (y, t)-th element of L, �yt = (L)yt = �(y, t), and by �t the t-th column of L, �t = (�1t , . [sent-41, score-0.442]
</p><p>16 Here Y = T = [n], and the loss incurred is 1 if the predicted label t is different from the actual class label y, and 0 otherwise: �0-1 (y, t) = 1(t �= y) , where 1(·) is 1 if the argument is true and 0 otherwise. [sent-46, score-0.324]
</p><p>17 The loss matrix L0-1 for n = 3 is shown in Figure 1(a). [sent-47, score-0.221]
</p><p>18 The loss matrix Lord for n = 3 is shown in Figure 1(b). [sent-52, score-0.221]
</p><p>19 Here Y = T = [2r ] for some r ∈ N, and the loss incurred on predicting t when the actual class label is y is the number of bit-positions in which the r-bit binary �r representations of t − 1 and y − 1 differ: �Ham (y, t) = i=1 1((t − 1)i �= (y − 1)i ) , where for any r z ∈ {0, . [sent-54, score-0.293]
</p><p>20 The loss matrix LHam for r = 2 is shown in Figure 1(c). [sent-58, score-0.221]
</p><p>21 This loss is used in sequence labeling tasks [16]. [sent-59, score-0.221]
</p><p>22 One possible loss function in this setting assigns a loss of 1 to incorrect predictions in [n], 0 to correct predictions, and 1 for abstaining: �(? [sent-62, score-0.469]
</p><p>23 In particular, the goal is to learn a function with �-risk close to the optimal �-risk, deﬁned as �  er�,∗ = D  inf  h:X →[k]  er� [h] = D  inf  h:X →[k]  EX p(X)� �h(X) = EX min p(X)� �t . [sent-70, score-0.18]
</p><p>24 (3) D y=1  � The learned function f : X →T is then used to make predictions in [k] via some transformation pred : � →[k]: the prediction on a new instance x ∈ X is given by pred(f (x)), and the �-risk incurred is T er� [pred ◦ f ]. [sent-72, score-0.28]
</p><p>25 As an example, several algorithms for multiclass classiﬁcation with respect to 0-1 loss D learn a function of the form f : X →Rn and predict according to pred(f (x)) = argmaxt∈[n] ft (x). [sent-73, score-0.495]
</p><p>26 Below we will ﬁnd it useful to represent the surrogate loss function ψ via n real-valued functions � t) t) ψy : T →R+ deﬁned as ψy (ˆ = ψ(y, ˆ for y ∈ [n], or equivalently, as a vector-valued function � t) t), t)) ψ : T →Rn deﬁned as ψ(ˆ = (ψ1 (ˆ . [sent-74, score-0.586]
</p><p>27 We will also deﬁne the sets + � � � � Rψ = ψ(ˆ : ˆ ∈ T t) t � and Sψ = conv(Rψ ) , (4) where for any A ⊆ Rn , conv(A) denotes the convex hull of A. [sent-78, score-0.134]
</p><p>28 to converge (in probability) to the optimal ψ-risk, deﬁned as �  erψ,∗ = D  inf erψ [f ] = D  � f :X →T  inf EX p(X)� ψ(f (X)) = EX inf p(X)� z = EX inf p(X)� z . [sent-84, score-0.36]
</p><p>29 z∈Rψ  � f :X →T  z∈Sψ  (5) This raises the natural question of whether, for a given loss �, there are surrogate losses ψ for which consistency with respect to the ψ-risk also guarantees consistency with respect to the �-risk, i. [sent-85, score-1.116]
</p><p>30 This question has been studied in detail for the 0-1 loss, and for square losses of the form �(y, t) = ay 1(t �= y), which can be analyzed similarly to the 0-1 loss [6, 7]. [sent-89, score-0.41]
</p><p>31 In this paper, we consider this question for general multiclass losses � : [n] × [k]→R+ , including rectangular losses with k �= n. [sent-90, score-0.602]
</p><p>32 The notion of classiﬁcation calibration will be central to our study; as Theorem 3 below shows, classiﬁcation calibration of a surrogate loss ψ w. [sent-94, score-1.232]
</p><p>33 A surrogate loss function ψ : [n] × T →R+ is said to be classiﬁcation calibrated with respect to a loss function � : [n] × [k]→R+ over P ⊆ Δn if there exists � a function pred : T →[k] such that p� ψ(ˆ > inf p� ψ(ˆ . [sent-106, score-1.478]
</p><p>34 t) t) ∀p ∈ P : inf ˆ T t∈ �  ˆ T :pred(ˆ ∈argmint p� �t t∈ � t) /  � Lemma 2. [sent-107, score-0.09]
</p><p>35 Then ψ is classiﬁcation calibrated with respect to � over P ⊆ Δn iff there exists a function pred� : Sψ →[k] such that ∀p ∈ P :  inf  z∈Sψ :pred� (z)∈argmint p� �t /  p� z >  inf p� z . [sent-109, score-0.569]
</p><p>36 Then ψ is classiﬁcation calibrated with � →[k] such that for all distributions D on X × [n] and respect to � over Δn iff ∃ a function pred : T � all sequences of random (vector) functions fm : X →T (depending on (X1 , Y1 ), . [sent-112, score-0.644]
</p><p>37 Let ψ : [n] × T normals at z is deﬁned as4 � � � NSψ (z) = p ∈ Δn : p� (z − z� ) ≤ 0 ∀z� ∈ Sψ . [sent-118, score-0.133]
</p><p>38 For each t ∈ [k], the set of trigger probabilities of t with respect to � is deﬁned as � � � � � Q� = p ∈ Δn : p� (�t − �t� ) ≤ 0 ∀t� ∈ [k] = p ∈ Δn : t ∈ argmint� ∈[k] p� �t� . [sent-121, score-0.13]
</p><p>39 t Examples of trigger probability sets for various losses are shown in Figure 2. [sent-122, score-0.291]
</p><p>40 2  Here Δn denotes the probability simplex in Rn , Δn = {p ∈ Rn : pi ≥ 0 ∀ i ∈ [n], P 3 Here − denotes convergence in probability. [sent-123, score-0.102]
</p><p>41 → 4 The set of positive normals is non-empty only at points z in the boundary of Sψ . [sent-124, score-0.154]
</p><p>42 ) Q4  = {p ∈ Δ3 : p1 ≥ = {p ∈ Δ3 : p2 ≥ = {p ∈ Δ3 : p3 ≥  1 2} 1 2} 1 2}  = {p ∈ Δ3 : max(p1 , p2 , p3 ) ≤  1 2}  (b) (c) Figure 2: Trigger probability sets for (a) 0-1 loss �0-1 ; (b) ordinal regression loss �ord ; and (c) ‘abstain’ loss �(? [sent-130, score-0.8]
</p><p>43 3  Necessary and Sufﬁcient Conditions for Classiﬁcation Calibration  We start by giving a necessary condition for classiﬁcation calibration of a surrogate loss ψ with respect to any multiclass loss � over Δn , which requires the positive normals of all points z ∈ Sψ to be ‘well-behaved’ w. [sent-134, score-1.589]
</p><p>44 � and generalizes the ‘admissibility’ condition used for 0-1 loss in [7]. [sent-137, score-0.221]
</p><p>45 Let ψ : [n] × T →R+ be classiﬁcation calibrated with respect to � : [n] × [k]→R+ over Δn . [sent-140, score-0.369]
</p><p>46 t  We note that, as in [7], it is possible to give a necessary and sufﬁcient condition for classiﬁcation calibration in terms of a similar property holding for positive normals associated with projections of Sψ in lower dimensions. [sent-142, score-0.463]
</p><p>47 Instead, below we give a different sufﬁcient condition that will be helpful in showing classiﬁcation calibration of certain surrogates. [sent-143, score-0.374]
</p><p>48 In particular, we show that for a surrogate loss ψ to be classiﬁcation calibrated with respect to � over Δn , it is sufﬁcient for the above property of positive normals to hold only at a ﬁnite number of points in Rψ , as long as their positive normal sets jointly cover Δn : � Theorem 7. [sent-144, score-1.131]
</p><p>49 Then t ψ is classiﬁcation calibrated with respect to � over Δn . [sent-150, score-0.369]
</p><p>50 The conditions in the above results both involve the sets of positive normals NSψ (z) at various points z ∈ Sψ . [sent-152, score-0.207]
</p><p>51 Thus in order to use the above results to show that a surrogate ψ is (or is not) classiﬁcation calibrated with respect to a loss �, one needs to be able to compute or characterize the sets NSψ (z). [sent-153, score-0.977]
</p><p>52 Here we give a method for computing these sets for certain surrogate losses ψ and points z ∈ Sψ . [sent-154, score-0.639]
</p><p>53 ¯ Recall � the subdifferential of a convex function φ : Rd →� + at a point u0 ∈ Rd is deﬁned as that R ∂φ(u0 ) = w ∈ Rd : φ(u) − φ(u0 ) ≥ w� (u − u0 ) ∀u ∈ Rd and is a convex set in Rd (e. [sent-185, score-0.195]
</p><p>54 6  4  We give an example illustrating the use of Theorem 7 and Lemma 8 to show classiﬁcation calibration of a certain surrogate loss with respect to the ordinal regression loss �ord deﬁned in Example 2: Example 5 (Classiﬁcation calibrated surrogate for ordinal regression loss). [sent-188, score-2.12]
</p><p>55 Consider the ordinal � regression loss �ord deﬁned in Example 2 for n = 3. [sent-189, score-0.336]
</p><p>56 Observe � � ˆ that T here is a convex set and ψ : T →R3 is a convex function. [sent-194, score-0.17]
</p><p>57 0 0 0 1 This gives NSψ (z1 )  = = =  Figure 3: The surrogate ψ � p ∈ Δ3 : p = (q1 + q2 , q3 , q4 ) for some q ∈ Δ4 , q1 − q2 − q3 − q4 = 0 � � p ∈ Δ3 : p = (q1 + q2 , q3 , q4 ) for some q ∈ Δ4 , q1 = 1 2 � � p ∈ Δ3 : p1 ≥ 1 2 �  = Qord . [sent-198, score-0.365]
</p><p>58 Thus, by Theorem 7, we get that 2 3 ψ is classiﬁcation calibrated with respect to �ord over Δ3 . [sent-200, score-0.369]
</p><p>59 4  Classiﬁcation Calibration Dimension  We now turn to the study of a fundamental quantity associated with the property of classiﬁcation calibration with respect to a general multiclass loss �. [sent-202, score-0.826]
</p><p>60 Speciﬁcally, in the above example, we saw that to develop a classiﬁcation calibrated surrogate loss w. [sent-203, score-0.905]
</p><p>61 the ordinal regression loss for n = 3, � it was sufﬁcient to consider a surrogate target space T = R, with dimension d = 1; in addition, this yielded a convex surrogate ψ : R→R3 which can be used in developing computationally efﬁcient + algorithms. [sent-206, score-1.296]
</p><p>62 In fact the same surrogate target space with d = 1 can be used to develop a similar convex, classiﬁcation calibrated surrogate loss w. [sent-207, score-1.316]
</p><p>63 However not all losses � have such ‘low-dimensional’ surrogates. [sent-211, score-0.189]
</p><p>64 This raises the natural question of what is the smallest dimension d that supports a convex classiﬁcation calibrated surrogate for a given multiclass loss �, and leads us to the following deﬁnition: Deﬁnition 9 (Classiﬁcation calibration dimension). [sent-212, score-1.645]
</p><p>65 Deﬁne the classiﬁcation calibration dimension (CC dimension) of � as � � � � CCdim(�) = min d ∈ N : ∃ a convex set T ⊆ Rd and a convex surrogate ψ : T →Rn + � that is classiﬁcation calibrated w. [sent-214, score-1.26]
</p><p>66 In the following, we will be interested in developing an understanding of the CC dimension for general losses �, and in particular in deriving upper and lower bounds on this. [sent-219, score-0.316]
</p><p>67 1  Upper Bounds on the Classiﬁcation Calibration Dimension  We start with a simple result that establishes that the CC dimension of any multiclass loss � is ﬁnite, and in fact is strictly smaller than the number of class labels n. [sent-221, score-0.588]
</p><p>68 j∈[n−1],j�=y  Then ψ is classiﬁcation calibrated with respect to � over Δn . [sent-225, score-0.369]
</p><p>69 It may appear surprising that the convex surrogate ψ in the above lemma is classiﬁcation calibrated with respect to all multiclass losses � on n classes. [sent-227, score-1.29]
</p><p>70 Minimizing the above surrogate effectively corresponds to such class probability estimation. [sent-229, score-0.365]
</p><p>71 Indeed, the above lemma can be shown to hold for any surrogate that is a strictly proper composite multiclass loss [21]. [sent-230, score-0.868]
</p><p>72 Next we give a different upper bound on the CC dimension that depends on the loss �, and for certain losses, can be signiﬁcantly tighter than the general bound above. [sent-232, score-0.444]
</p><p>73 Then CCdim(�) ≤ rank(L), the rank of the loss matrix L. [sent-235, score-0.246]
</p><p>74 We will construct a convex classiﬁcation calibrated surrogate loss ψ for � � with surrogate target space T ⊆ Rd . [sent-238, score-1.401]
</p><p>75 , �k }); from the deﬁnitions of positive normals and trigger probabilities, it then follows that NSψ (zt ) = NSψ (�t ) = Q� for all t ∈ [k]. [sent-266, score-0.213]
</p><p>76 Thus by Theorem 7, ψ is classiﬁcation calibrated w. [sent-267, score-0.319]
</p><p>77 Consider the Hamming loss �Ham deﬁned in Example 3, for n = 2r . [sent-272, score-0.221]
</p><p>78 Then the loss matrix LHam satisﬁes  r  LHam =  r � 1� ee − σi σi � , 2 2 i=1  where e is the n × 1 all ones vector. [sent-274, score-0.221]
</p><p>79 6  We note that the upper bound of Theorem 11 need not always be tight: for example, for the ordinal regression loss, for which we already know CCdim(�ord ) = 1, the theorem actually gives an upper bound of n, which is even weaker than that implied by Lemma 10. [sent-277, score-0.26]
</p><p>80 2  Lower Bound on the Classiﬁcation Calibration Dimension  In this section we give a lower bound on the CC dimension of a loss function � and illustrate it by using it to calculate the CC dimension of the 0-1 loss. [sent-279, score-0.446]
</p><p>81 Section 5 we will explore consequences of the lower bound for classiﬁcation calibrated surrogates for certain types of ranking losses. [sent-280, score-0.565]
</p><p>82 The feasible subspace dimension of a convex set C at p ∈ C, denoted by µC (p), is deﬁned as the dimension of the subspace FC (p) ∩ (−FC (p)), where FC (p) is the cone of feasible directions of C at p. [sent-282, score-0.395]
</p><p>83 7 The following gives a lower bound on the CC dimension of a loss � in terms of the feasible subspace dimension of the trigger probability sets Q� at certain points p ∈ Q� : t t  Theorem 13. [sent-283, score-0.667]
</p><p>84 t The proof requires extensions of the deﬁnition of positive normals and the necessary condition of Theorem 6 to sequences of points in Sψ and is quite technical. [sent-288, score-0.177]
</p><p>85 In the appendix, we provide a proof in the special case when p ∈ relint(Δn ) is such that inf z∈Sψ p� z is achieved in Sψ , which does not require these extensions. [sent-289, score-0.09]
</p><p>86 Both the proof of the lower bound and its applications make use of the following lemma, which gives a method to calculate the feasible subspace dimension for certain convex sets C and points p ∈ C: � � Lemma 14. [sent-291, score-0.352]
</p><p>87 Let p ∈ C be such that � 1� �� 1 �� A A , the dimension of the null space of . [sent-293, score-0.139]
</p><p>88 Then µC (p) = nullity 3 A A3 The above lower bound allows us to calculate precisely the CC dimension of the 0-1 loss: Example 7 (CC dimension of 0-1 loss). [sent-295, score-0.331]
</p><p>89 , n} � � � = q ∈ Rn : −en−1 In−1 q ≤ 0, −q ≤ 0, e� q = 1} , n  where en−1 , en denote the (n − 1) × 1 and n × 1 all ones vectors, respectively, and In−1 denotes � � the (n − 1) × (n − 1) identity matrix. [sent-305, score-0.096]
</p><p>90 [10] showed recently that for certain pairwise subset ranking losses �, ﬁnding a predictor that minimizes the �-risk is an NP-hard problem. [sent-329, score-0.389]
</p><p>91 Here we provide an alternative route to analyzing the difﬁculty of obtaining consistent surrogates for such pairwise subset ranking problems using the classiﬁcation calibration dimension. [sent-333, score-0.624]
</p><p>92 Speciﬁcally, we will show that even for a simple setting of such problems, the classiﬁcation calibration dimension � of the underlying loss � is greater than r, and therefore no convex surrogate operating on T ⊆ Rr can be classiﬁcation calibrated w. [sent-334, score-1.462]
</p><p>93 Consider now the following simple pairwise loss �pair : Y × T →R+ : � � �pair (g(i,j) , σ) = 1 σ(i) > σ(j) . [sent-342, score-0.26]
</p><p>94 Therefore, by Lemma 14, we get σ1 σt �� �� � µQpair (p) = nullity (�pair − �pair ), . [sent-363, score-0.106]
</p><p>95 It is not hard to see that the set {�pair : σ ∈ T } spans a σ � � r(r−1) dimensional space, and hence the nullity of the above matrix is at most r(r−1)− r(r−1) −1 . [sent-368, score-0.106]
</p><p>96 An interesting direction would be to develop a generic procedure for designing consistent convex surrogates operating on a ‘minimal’ surrogate target space according to the classiﬁcation calibration dimension of the loss matrix. [sent-372, score-1.296]
</p><p>97 On the bayes-risk consistency of regularized boosting a methods. [sent-378, score-0.108]
</p><p>98 Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. [sent-384, score-0.193]
</p><p>99 Learning scoring e e functions with order-preserving losses and standardized supervision. [sent-412, score-0.189]
</p><p>100 How to compare different loss functions and their risks. [sent-421, score-0.221]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('surrogate', 0.365), ('calibrated', 0.319), ('calibration', 0.307), ('ccdim', 0.254), ('multiclass', 0.224), ('loss', 0.221), ('pred', 0.212), ('losses', 0.189), ('ns', 0.184), ('cc', 0.137), ('normals', 0.133), ('ord', 0.131), ('classi', 0.131), ('qord', 0.127), ('relint', 0.127), ('consistency', 0.108), ('lham', 0.106), ('nullity', 0.106), ('conv', 0.103), ('rn', 0.1), ('dimension', 0.099), ('ranking', 0.096), ('inf', 0.09), ('ordinal', 0.088), ('convex', 0.085), ('argmint', 0.085), ('qpair', 0.085), ('ex', 0.083), ('cation', 0.083), ('surrogates', 0.081), ('trigger', 0.08), ('abstain', 0.069), ('en', 0.069), ('rd', 0.068), ('operating', 0.066), ('ham', 0.064), ('wsy', 0.064), ('lemma', 0.058), ('pair', 0.057), ('er', 0.057), ('py', 0.052), ('respect', 0.05), ('simplex', 0.048), ('rr', 0.046), ('target', 0.046), ('fc', 0.044), ('fm', 0.043), ('byj', 0.042), ('harish', 0.042), ('listwise', 0.042), ('duchi', 0.042), ('certain', 0.042), ('incurred', 0.041), ('analyzing', 0.04), ('null', 0.04), ('pairwise', 0.039), ('route', 0.038), ('ingo', 0.037), ('lord', 0.037), ('tong', 0.036), ('theorem', 0.035), ('nicolas', 0.035), ('notion', 0.032), ('conditions', 0.031), ('ambuj', 0.031), ('label', 0.031), ('hamming', 0.03), ('sy', 0.03), ('india', 0.03), ('tewari', 0.028), ('upper', 0.028), ('feasible', 0.028), ('subspace', 0.028), ('nition', 0.028), ('regression', 0.027), ('bound', 0.027), ('denotes', 0.027), ('predictions', 0.027), ('preference', 0.026), ('ut', 0.026), ('designing', 0.026), ('rank', 0.025), ('helpful', 0.025), ('subdifferential', 0.025), ('raises', 0.025), ('giving', 0.024), ('quantity', 0.024), ('ym', 0.023), ('documents', 0.023), ('subset', 0.023), ('yt', 0.023), ('necessary', 0.023), ('tj', 0.022), ('labels', 0.022), ('establishes', 0.022), ('nitions', 0.022), ('sets', 0.022), ('points', 0.021), ('iff', 0.02), ('zj', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="67-tfidf-1" href="./nips-2012-Classification_Calibration_Dimension_for_General_Multiclass_Losses.html">67 nips-2012-Classification Calibration Dimension for General Multiclass Losses</a></p>
<p>Author: Harish G. Ramaswamy, Shivani Agarwal</p><p>Abstract: We study consistency properties of surrogate loss functions for general multiclass classiﬁcation problems, deﬁned by a general loss matrix. We extend the notion of classiﬁcation calibration, which has been studied for binary and multiclass 0-1 classiﬁcation problems (and for certain other speciﬁc learning problems), to the general multiclass setting, and derive necessary and sufﬁcient conditions for a surrogate loss to be classiﬁcation calibrated with respect to a loss matrix in this setting. We then introduce the notion of classiﬁcation calibration dimension of a multiclass loss matrix, which measures the smallest ‘size’ of a prediction space for which it is possible to design a convex surrogate that is classiﬁcation calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al. (2010) for analyzing the difﬁculty of designing ‘low-dimensional’ convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classiﬁcation calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems. 1</p><p>2 0.20642208 <a title="67-tfidf-2" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>Author: Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-jeacques Slotine</p><p>Abstract: In this paper we discuss a novel framework for multiclass learning, deﬁned by a suitable coding/decoding strategy, namely the simplex coding, that allows us to generalize to multiple classes a relaxation approach commonly used in binary classiﬁcation. In this framework, we develop a relaxation error analysis that avoids constraints on the considered hypotheses class. Moreover, using this setting we derive the ﬁrst provably consistent regularized method with training/tuning complexity that is independent to the number of classes. We introduce tools from convex analysis that can be used beyond the scope of this paper. 1</p><p>3 0.17827564 <a title="67-tfidf-3" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>Author: Joseph Wang, Venkatesh Saligrama</p><p>Abstract: We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-speciﬁc classiﬁers. We formulate an empirical risk minimization problem that incorporates both partitioning and classiﬁcation in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classiﬁers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-ﬁtting and generalization error. We train locally linear classiﬁers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classiﬁcation techniques on benchmark datasets. We also show improved robustness to label noise.</p><p>4 0.17022654 <a title="67-tfidf-4" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>Author: Yanyan Lan, Jiafeng Guo, Xueqi Cheng, Tie-yan Liu</p><p>Abstract: This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justiﬁcations of some empirical ﬁndings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications.</p><p>5 0.1360103 <a title="67-tfidf-5" href="./nips-2012-Proper_losses_for_learning_from_partial_labels.html">280 nips-2012-Proper losses for learning from partial labels</a></p>
<p>Author: Jesús Cid-sueiro</p><p>Abstract: This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, we establish a necessary and sufﬁcient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. The full knowledge of this matrix is not required, and losses can be constructed that are proper for a wide set of mixing probability matrices. 1</p><p>6 0.12820809 <a title="67-tfidf-6" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>7 0.12820198 <a title="67-tfidf-7" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>8 0.12236422 <a title="67-tfidf-8" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>9 0.10445837 <a title="67-tfidf-9" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>10 0.097113319 <a title="67-tfidf-10" href="./nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>11 0.094690017 <a title="67-tfidf-11" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>12 0.087379634 <a title="67-tfidf-12" href="./nips-2012-Multiclass_Learning_Approaches%3A_A_Theoretical_Comparison_with_Implications.html">226 nips-2012-Multiclass Learning Approaches: A Theoretical Comparison with Implications</a></p>
<p>13 0.083511762 <a title="67-tfidf-13" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>14 0.081976786 <a title="67-tfidf-14" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>15 0.078021981 <a title="67-tfidf-15" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>16 0.07516674 <a title="67-tfidf-16" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>17 0.071070731 <a title="67-tfidf-17" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>18 0.070606008 <a title="67-tfidf-18" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>19 0.070453994 <a title="67-tfidf-19" href="./nips-2012-Confusion-Based_Online_Learning_and_a_Passive-Aggressive_Scheme.html">80 nips-2012-Confusion-Based Online Learning and a Passive-Aggressive Scheme</a></p>
<p>20 0.068736874 <a title="67-tfidf-20" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.179), (1, 0.016), (2, 0.083), (3, -0.042), (4, 0.131), (5, -0.035), (6, -0.007), (7, 0.218), (8, -0.066), (9, 0.057), (10, 0.047), (11, 0.1), (12, 0.02), (13, 0.046), (14, 0.057), (15, -0.04), (16, 0.002), (17, -0.019), (18, -0.014), (19, 0.09), (20, -0.072), (21, 0.057), (22, -0.124), (23, 0.197), (24, -0.042), (25, 0.003), (26, -0.069), (27, 0.027), (28, -0.095), (29, -0.095), (30, 0.099), (31, -0.007), (32, 0.082), (33, 0.012), (34, 0.129), (35, 0.056), (36, 0.028), (37, -0.04), (38, -0.046), (39, -0.061), (40, 0.01), (41, -0.043), (42, -0.009), (43, -0.044), (44, -0.027), (45, -0.043), (46, -0.005), (47, 0.018), (48, 0.019), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96447963 <a title="67-lsi-1" href="./nips-2012-Classification_Calibration_Dimension_for_General_Multiclass_Losses.html">67 nips-2012-Classification Calibration Dimension for General Multiclass Losses</a></p>
<p>Author: Harish G. Ramaswamy, Shivani Agarwal</p><p>Abstract: We study consistency properties of surrogate loss functions for general multiclass classiﬁcation problems, deﬁned by a general loss matrix. We extend the notion of classiﬁcation calibration, which has been studied for binary and multiclass 0-1 classiﬁcation problems (and for certain other speciﬁc learning problems), to the general multiclass setting, and derive necessary and sufﬁcient conditions for a surrogate loss to be classiﬁcation calibrated with respect to a loss matrix in this setting. We then introduce the notion of classiﬁcation calibration dimension of a multiclass loss matrix, which measures the smallest ‘size’ of a prediction space for which it is possible to design a convex surrogate that is classiﬁcation calibrated with respect to the loss matrix. We derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices. In particular, as one application, we provide a different route from the recent result of Duchi et al. (2010) for analyzing the difﬁculty of designing ‘low-dimensional’ convex surrogates that are consistent with respect to pairwise subset ranking losses. We anticipate the classiﬁcation calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems. 1</p><p>2 0.80683076 <a title="67-lsi-2" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>Author: Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-jeacques Slotine</p><p>Abstract: In this paper we discuss a novel framework for multiclass learning, deﬁned by a suitable coding/decoding strategy, namely the simplex coding, that allows us to generalize to multiple classes a relaxation approach commonly used in binary classiﬁcation. In this framework, we develop a relaxation error analysis that avoids constraints on the considered hypotheses class. Moreover, using this setting we derive the ﬁrst provably consistent regularized method with training/tuning complexity that is independent to the number of classes. We introduce tools from convex analysis that can be used beyond the scope of this paper. 1</p><p>3 0.78801107 <a title="67-lsi-3" href="./nips-2012-Proper_losses_for_learning_from_partial_labels.html">280 nips-2012-Proper losses for learning from partial labels</a></p>
<p>Author: Jesús Cid-sueiro</p><p>Abstract: This paper discusses the problem of calibrating posterior class probabilities from partially labelled data. Each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true. We generalize the concept of proper loss to this scenario, we establish a necessary and sufﬁcient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss. The problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels. The full knowledge of this matrix is not required, and losses can be constructed that are proper for a wide set of mixing probability matrices. 1</p><p>4 0.71351385 <a title="67-lsi-4" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>Author: Yanyan Lan, Jiafeng Guo, Xueqi Cheng, Tie-yan Liu</p><p>Abstract: This paper is concerned with the statistical consistency of ranking methods. Recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (WPDL), which can be viewed as the true loss of ranking, even in a low-noise setting. This result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice. In this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used. We give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (RDPS), and prove that the pairwise ranking methods become consistent with WPDL under this assumption. What is especially inspiring is that RDPS is actually not stronger than but similar to the low-noise setting. Our studies provide theoretical justiﬁcations of some empirical ﬁndings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications.</p><p>5 0.71231169 <a title="67-lsi-5" href="./nips-2012-Mixability_in_Statistical_Learning.html">217 nips-2012-Mixability in Statistical Learning</a></p>
<p>Author: Tim V. Erven, Peter Grünwald, Mark D. Reid, Robert C. Williamson</p><p>Abstract: Statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions. Mapping out their relations and transferring ideas is an active area of investigation. We provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability. Just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning. We show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and Bayesian inference. In the case of 0/1-loss, it reduces to the margin condition of Mammen and Tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability. 1</p><p>6 0.70733392 <a title="67-lsi-6" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>7 0.63613361 <a title="67-lsi-7" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>8 0.63226992 <a title="67-lsi-8" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>9 0.62686336 <a title="67-lsi-9" href="./nips-2012-Volume_Regularization_for_Binary_Classification.html">361 nips-2012-Volume Regularization for Binary Classification</a></p>
<p>10 0.60141855 <a title="67-lsi-10" href="./nips-2012-Multiclass_Learning_Approaches%3A_A_Theoretical_Comparison_with_Implications.html">226 nips-2012-Multiclass Learning Approaches: A Theoretical Comparison with Implications</a></p>
<p>11 0.59597892 <a title="67-lsi-11" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>12 0.58671916 <a title="67-lsi-12" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>13 0.57926726 <a title="67-lsi-13" href="./nips-2012-Feature-aware_Label_Space_Dimension_Reduction_for_Multi-label_Classification.html">130 nips-2012-Feature-aware Label Space Dimension Reduction for Multi-label Classification</a></p>
<p>14 0.5677864 <a title="67-lsi-14" href="./nips-2012-Pointwise_Tracking_the_Optimal_Regression_Function.html">271 nips-2012-Pointwise Tracking the Optimal Regression Function</a></p>
<p>15 0.56617171 <a title="67-lsi-15" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>16 0.56018192 <a title="67-lsi-16" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>17 0.51063395 <a title="67-lsi-17" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>18 0.50303 <a title="67-lsi-18" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>19 0.49958915 <a title="67-lsi-19" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<p>20 0.49258277 <a title="67-lsi-20" href="./nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">141 nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.016), (38, 0.109), (42, 0.028), (54, 0.011), (74, 0.033), (76, 0.124), (80, 0.539), (92, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97182614 <a title="67-lda-1" href="./nips-2012-Slice_Normalized_Dynamic_Markov_Logic_Networks.html">314 nips-2012-Slice Normalized Dynamic Markov Logic Networks</a></p>
<p>Author: Tivadar Papai, Henry Kautz, Daniel Stefankovic</p><p>Abstract: Markov logic is a widely used tool in statistical relational learning, which uses a weighted ﬁrst-order logic knowledge base to specify a Markov random ﬁeld (MRF) or a conditional random ﬁeld (CRF). In many applications, a Markov logic network (MLN) is trained in one domain, but used in a different one. This paper focuses on dynamic Markov logic networks, where the size of the discretized time-domain typically varies between training and testing. It has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an MLN. We show that in addition to this problem, the standard way of unrolling a Markov logic theory into a MRF may result in time-inhomogeneity of the underlying Markov chain. Furthermore, even if these representational problems are not signiﬁcant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random ﬁeld for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample. We propose a new discriminative model, slice normalized dynamic Markov logic networks (SN-DMLN), that suffers from none of these issues. It supports efﬁcient online inference, and can directly model inﬂuences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., DBNs). Experimental results show an improvement in accuracy over previous approaches to online inference in dynamic Markov logic networks. 1</p><p>2 0.9709425 <a title="67-lda-2" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>Author: James Scott, Jonathan W. Pillow</p><p>Abstract: Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses. The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability. Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latentvariable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals. This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efﬁcient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models. We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains. 1</p><p>3 0.97057217 <a title="67-lda-3" href="./nips-2012-MAP_Inference_in_Chains_using_Column_Generation.html">204 nips-2012-MAP Inference in Chains using Column Generation</a></p>
<p>Author: David Belanger, Alexandre Passos, Sebastian Riedel, Andrew McCallum</p><p>Abstract: Linear chains and trees are basic building blocks in many applications of graphical models, and they admit simple exact maximum a-posteriori (MAP) inference algorithms based on message passing. However, in many cases this computation is prohibitively expensive, due to quadratic dependence on variables’ domain sizes. The standard algorithms are inefﬁcient because they compute scores for hypotheses for which there is strong negative local evidence. For this reason there has been signiﬁcant previous interest in beam search and its variants; however, these methods provide only approximate results. This paper presents new exact inference algorithms based on the combination of column generation and pre-computed bounds on terms of the model’s scoring function. While we do not improve worst-case performance, our method substantially speeds real-world, typical-case inference in chains and trees. Experiments show our method to be twice as fast as exact Viterbi for Wall Street Journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task. Our algorithm is also extendable to new techniques for approximate inference, to faster 0/1 loss oracles, and new opportunities for connections between inference and learning. We encourage further exploration of high-level reasoning about the optimization problem implicit in dynamic programs. 1</p><p>4 0.96984547 <a title="67-lda-4" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>Author: Ashish Kapoor, Raajay Viswanathan, Prateek Jain</p><p>Abstract: In this paper, we present a Bayesian framework for multilabel classiďŹ cation using compressed sensing. The key idea in compressed sensing for multilabel classiďŹ cation is to ďŹ rst project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efďŹ cient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key beneďŹ ts of the model are that a) it can naturally handle datasets that have missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show signiďŹ cant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model. 1</p><p>5 0.94383341 <a title="67-lda-5" href="./nips-2012-Discriminative_Learning_of_Sum-Product_Networks.html">100 nips-2012-Discriminative Learning of Sum-Product Networks</a></p>
<p>Author: Robert Gens, Pedro Domingos</p><p>Abstract: Sum-product networks are a new deep architecture that can perform fast, exact inference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the ﬁrst discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an efﬁcient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably using “hard” gradient descent, where marginal inference is replaced by MPE inference (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classiﬁcation tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architecture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset. 1</p><p>same-paper 6 0.93800271 <a title="67-lda-6" href="./nips-2012-Classification_Calibration_Dimension_for_General_Multiclass_Losses.html">67 nips-2012-Classification Calibration Dimension for General Multiclass Losses</a></p>
<p>7 0.8380723 <a title="67-lda-7" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>8 0.8176353 <a title="67-lda-8" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>9 0.78806502 <a title="67-lda-9" href="./nips-2012-Provable_ICA_with_Unknown_Gaussian_Noise%2C_with_Implications_for_Gaussian_Mixtures_and_Autoencoders.html">281 nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</a></p>
<p>10 0.77986103 <a title="67-lda-10" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>11 0.77193463 <a title="67-lda-11" href="./nips-2012-Latent_Coincidence_Analysis%3A_A_Hidden_Variable_Model_for_Distance_Metric_Learning.html">171 nips-2012-Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning</a></p>
<p>12 0.77105021 <a title="67-lda-12" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>13 0.77098167 <a title="67-lda-13" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>14 0.7486257 <a title="67-lda-14" href="./nips-2012-Mandatory_Leaf_Node_Prediction_in_Hierarchical_Multilabel_Classification.html">207 nips-2012-Mandatory Leaf Node Prediction in Hierarchical Multilabel Classification</a></p>
<p>15 0.73899209 <a title="67-lda-15" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>16 0.73897338 <a title="67-lda-16" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>17 0.73529232 <a title="67-lda-17" href="./nips-2012-Bayesian_active_learning_with_localized_priors_for_fast_receptive_field_characterization.html">56 nips-2012-Bayesian active learning with localized priors for fast receptive field characterization</a></p>
<p>18 0.72897279 <a title="67-lda-18" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>19 0.7204752 <a title="67-lda-19" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>20 0.7158637 <a title="67-lda-20" href="./nips-2012-Supervised_Learning_with_Similarity_Functions.html">330 nips-2012-Supervised Learning with Similarity Functions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
