<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-90" href="#">nips2012-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</h1>
<br/><p>Source: <a title="nips-2012-90-pdf" href="http://papers.nips.cc/paper/4730-deep-learning-of-invariant-features-via-simulated-fixations-in-video.pdf">pdf</a></p><p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>Reference: <a title="nips-2012-90-reference" href="../nips2012_reference/nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com 1  Abstract We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. [sent-8, score-0.34]
</p><p>2 With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. [sent-9, score-1.451]
</p><p>3 The network encodes invariance which are increasingly complex with hierarchy. [sent-10, score-0.235]
</p><p>4 Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. [sent-11, score-0.435]
</p><p>5 During their development, training stimuli are not incoherent sequences of images, but natural visual streams modulated by ﬁxations [1]. [sent-15, score-0.296]
</p><p>6 Likewise, we expect a machine vision system to learn from coherent image sequences extracted from the natural environment. [sent-16, score-0.318]
</p><p>7 Through this learning process, it is desired that features become robust to temporal transfromations and perform signiﬁcantly better in recognition. [sent-17, score-0.355]
</p><p>8 However, it remains unclear to what extent sparsity and subspace pooling [3, 4] could produce invariance exhibited in higher levels of visual systems. [sent-20, score-0.468]
</p><p>9 Another approach to learning invariance is temporal slowness [1, 5, 6, 7]. [sent-21, score-1.053]
</p><p>10 Experimental evidence suggests that high-level visual representations become slow-changing and tolerant towards non-trivial transformations, by associating low-level features which appear in a coherent sequence [5]. [sent-22, score-0.275]
</p><p>11 To learn features using slowness, a key observation is that during our visual ﬁxations, moving objects remain in visual focus for a sustained amount of time through smooth pursuit eye movements. [sent-23, score-0.463]
</p><p>12 At these feature locations, we apply local contrast normalization [8], template matching [9] to ﬁnd local correspondences between successive video frames. [sent-27, score-0.269]
</p><p>13 In prior work [10, 11, 12], a single layer of features learned using temporal slowness results in translation-invariant edge detectors, reminiscent of complex-cells. [sent-30, score-1.348]
</p><p>14 However, it remains unclear whether higher levels of invariances [1], such as ones exhibited in IT, can be learned using temporal 1  Figure 1: Simulating smooth pursuit eye movements. [sent-31, score-0.481]
</p><p>15 Using temporal slowness, the ﬁrst layer units become locally translational invariant, similar to subspace or spatial pooling; the second layer units can then encode more complex invariances such as out-of-plane transformations and non-linear warping. [sent-37, score-0.964]
</p><p>16 Using this approach, we show a surprising result that despite being trained on videos, our features encode complex invariances which translate to recognition performance on still images. [sent-38, score-0.414]
</p><p>17 We ﬁrst learn a set of features using simulated ﬁxations in unlabeled videos, and then apply the learned features to classiﬁcation tasks. [sent-40, score-0.459]
</p><p>18 The learned features improve accuracy by a signiﬁcant 4% to 5% across four still image recognition datasets. [sent-41, score-0.386]
</p><p>19 Finally, we quantify the invariance learned using temporal slowness and simulated ﬁxations by a set of control experiments. [sent-43, score-1.146]
</p><p>20 2  Related work  Unsupervised learning image features from pixels is a relatively new approach in computer vision. [sent-44, score-0.259]
</p><p>21 [20] showed that temporal slowness could improve recognition on a video-like COIL-100 dataset. [sent-53, score-0.922]
</p><p>22 Despite being one of the ﬁrst to apply temporal slowness in deep architectures, the authors trained a fully supervised convolutional network and used temporal slowness as a regularizing step in the optimization procedure. [sent-54, score-1.949]
</p><p>23 The inﬂuential work of Slow Feature Analysis (SFA) [7] was an early example of unsupervised algorithm using temporal slowness. [sent-55, score-0.269]
</p><p>24 SFA solves a constrained problem and optimizes for temporal slowness by mapping data into a quadratic expansion and performing eigenvector decomposition. [sent-56, score-0.861]
</p><p>25 [12] proposed to train deep architectures with temporal slowness and decorrelation, and illustrated training a ﬁrst layer on MNIST digits. [sent-61, score-1.216]
</p><p>26 [24] trained a two-layer algorithm to learn visual transformations in videos, with limited emphasis on temporal slowness. [sent-65, score-0.407]
</p><p>27 The computer vision literature has a number of works which, similar to us, use the idea of video tracking to learn invariant features. [sent-66, score-0.417]
</p><p>28 [25] show improvement in performance when SIFT/HOG parameters are optimized using tracked image patch sequences in speciﬁc application domains. [sent-69, score-0.361]
</p><p>29 In contrast to these recent examples, our algorithm learns features directly from raw image pixels, and adapts to pixel-level image statistics—in particular, it does not rely on hand-designed preprocessing such as SIFT/HOG. [sent-76, score-0.295]
</p><p>30 In particular, our learning modules use a combination of temporal slowness and a non-degeneracy principle similar to orthogonality [30, 31]. [sent-79, score-0.934]
</p><p>31 To learn invariant features with temporal slowness, we use a two layer network, where the ﬁrst layer is convolutional and replicates neurons with local receptive ﬁeld across dense grid locations, and the second (non-convolutional) layer is fully connected. [sent-82, score-1.335]
</p><p>32 This pooling mechanism is implemented by a subspace pooling matrix H with a group size of two [30]. [sent-87, score-0.282]
</p><p>33 ) Although the algorithm is driven by temporal slowness, sparsity also helps to obtain good features from natural images. [sent-94, score-0.42]
</p><p>34 This basic algorithm trained on the Hans van Hateren’s natural video repository [24] produced oriented edge ﬁlters. [sent-96, score-0.329]
</p><p>35 The learned features are highly invariant to local translations. [sent-97, score-0.322]
</p><p>36 The reason for this is that temporal slowness requires hidden features to be slow-changing across time. [sent-98, score-1.054]
</p><p>37 2  Stacked Architecture  The ﬁrst layer modules described in the last section are trained on a smaller patch size (16x16 pixels) of locally tracked video sequences. [sent-102, score-0.801]
</p><p>38 To construct the set of inputs to the second stacked layer, ﬁrst layer features are replicated on a dense grid in a larger scale (32x32 pixels). [sent-103, score-0.455]
</p><p>39 The input to layer two is extracted after L2 pooling. [sent-104, score-0.283]
</p><p>40 This architecture produces an over-complete number of local 16x16 features across the larger feature area. [sent-105, score-0.299]
</p><p>41 Due to the high dimensionality of the ﬁrst layer outputs, we apply PCA to reduce their dimensions for the second layer algorithm. [sent-107, score-0.504]
</p><p>42 avi  3  Figure 2: Neural network architecture of the basic learning module  Figure 3: Translational invariance in ﬁrst layer features; columns correspond to interpolation angle θ at multiples of 45 degrees  a fully connected module is trained with temporal slowness on the output of PCA. [sent-111, score-1.663]
</p><p>43 The stacked architecture learns features in a signﬁcantly larger 2-D area than the ﬁrst layer algorithm, and able to learn invariance to larger-scale transformations seen in videos. [sent-112, score-0.777]
</p><p>44 Figure 4: Two-layer architecture of our algorithm used to learn invariance from videos. [sent-113, score-0.283]
</p><p>45 3  Invariance Visualization  After unsupervised training with video sequences, we visualize the features learned by the two layer network. [sent-115, score-0.796]
</p><p>46 On the left of Figure 5, we show the optimal stimuli which maximally activates each of the ﬁrst layer pooling units. [sent-116, score-0.456]
</p><p>47 The optimal stimuli for units learned without slowness are shown at the top, and appears to give high frequency grating-like patterns. [sent-118, score-0.831]
</p><p>48 At the bottom, we show the optimal stimuli for features learned with slowness; here, the optimal stimuli appear much smoother because the pairs of Gabor-like features being pooled over are usually a quadrature pair. [sent-119, score-0.58]
</p><p>49 The second layer features are learned on top of the pooled ﬁrst layer features. [sent-121, score-0.791]
</p><p>50 We visualize the second layer features by plotting linear combinations of the ﬁrst layer features’ optimal stimuli (as shown on the left of Figure 5), and varying the interpolation angle as in [24]. [sent-122, score-0.796]
</p><p>51 Each row corresponds to a motion sequence to which we would expect the second layer features to be roughly invariant. [sent-124, score-0.419]
</p><p>52 A video animation of this visualization is also available online2 . [sent-126, score-0.259]
</p><p>53 avi  4  Figure 5: (Left) Comparison of optimal stimuli of ﬁrst layer pooling units (patch size 16x16) learned without (top) and with (bottom) temporal slowness. [sent-130, score-0.739]
</p><p>54 (Right) visualization of second layer features (patch size 32x32), with each row corresponding to one pooling unit. [sent-131, score-0.622]
</p><p>55 The learned features are then used to classify single images in each of four datasets. [sent-135, score-0.295]
</p><p>56 1  Training with Tracked Sequences  To extract data from the Hans van Hateren natural video repository, we apply spatial-temporal Difference-of-Gaussian blob detector and select areas of high response to simulate visual ﬁxations. [sent-138, score-0.365]
</p><p>57 After the initial frame is selected, the image patch is tracked across 20 frames using a tracker we built and customized for this task. [sent-139, score-0.366]
</p><p>58 The ﬁrst layer algorithm is learned on 16x16 patches with 128 features (pooled from 256 linear bases). [sent-141, score-0.528]
</p><p>59 The second layer learns 150 features (pooled from 300 linear bases). [sent-144, score-0.419]
</p><p>60 The videos we trained on to obtain the temporal slowness features were based on the van Hataren videos, and were thus unrelated to COIL-100. [sent-148, score-1.241]
</p><p>61 COIL-100 (unrelated video) Method VTU [32] ConvNet regularized with video [20] Our results without video Our results using video Performance increase by training on video  Acc. [sent-157, score-0.819]
</p><p>62 0%  Method Two-layer ConvNet [36] ScSPM [37] Hierarchical sparse-coding [38] Macrofeatures [39] Our results without video Our results using video Performance increase with video  Ave. [sent-163, score-0.591]
</p><p>63 STL-10 Method Reconstruction ICA [31] Sparse Filtering [40] SC features, K-means encoding [16] SC features, SC encoding [16] Local receptive ﬁeld selection [19] Our result without video Our result using video Performance increase with video  Ave. [sent-174, score-0.628]
</p><p>64 PubFig faces Method Our result without video Our result using video Performance increase with video  Acc. [sent-185, score-0.591]
</p><p>65 3  Test Pipeline  On still images, we apply our trained network to extract features at dense grid locations. [sent-196, score-0.28]
</p><p>66 A linear SVM classiﬁer is trained on features from both ﬁrst and second layers. [sent-197, score-0.237]
</p><p>67 For Caltech 101, we use a three layer spatial pyramid. [sent-201, score-0.285]
</p><p>68 However, performance is not particularly sensitive to the weighting between temporal slowness objective compared to reconstruction objective in Equation 1, as we will illustrate in Section 4. [sent-205, score-0.933]
</p><p>69 For each dataset, we compare results using features trained with and without the temporal slowness objective term in Equation 1. [sent-208, score-1.098]
</p><p>70 Despite the feature being learned from natural videos and then being transferred to different recognition tasks (i. [sent-209, score-0.329]
</p><p>71 The application of temporal slowness increases recognition accuracy consistently by 4% to 5%, bringing our results to be competitive with the state-of-the-art. [sent-212, score-0.922]
</p><p>72 As shown on the left of Figure 6, training on tracked sequences reduces the translation invariance learned in the second layer. [sent-217, score-0.569]
</p><p>73 In 6  comparison to other forms of invariances, translation is less useful because it is easy to encode with spatial pooling [17]. [sent-218, score-0.259]
</p><p>74 Instead, the features encode other invariance such as different forms of nonlinear warping. [sent-219, score-0.386]
</p><p>75 The advantage of using tracked data is reﬂected in object recognition performance on the STL-10 dataset. [sent-220, score-0.237]
</p><p>76 Shown on the right of Figure 6, recognition accuracy is increased by a considerable margin by training on tracked sequences. [sent-221, score-0.224]
</p><p>77 Figure 6: (Left) Comparison of second layer invariance visualization when training data was obtained with tracking and without; (Right) Ave. [sent-222, score-0.601]
</p><p>78 on STL-10 with features trained on tracked sequences compared to non-tracked; λ in this plot is slowness weighting parameter from Equation 1 . [sent-224, score-1.169]
</p><p>79 2 Importance of Temporal Slowness to Recognition Performance To understand how much the slowness principle helps to learn good features, we vary the slowness parameter across a range of values to observe its effect on recognition accuracy. [sent-227, score-1.465]
</p><p>80 Figure 7 shows recognition accuracy on STL-10, plotted as a function of a slowness weighting parameter λ in the ﬁrst and second layers. [sent-228, score-0.773]
</p><p>81 Figure 7: Performance on STL-10 versus the amount of temporal slowness, on the ﬁrst layer (left) and second layer (right); in these plots λ is the slowness weighting parameter from Equation 1; different colored curves are shown for different λ values in the other layer. [sent-231, score-1.404]
</p><p>82 3  Invariance Tests  We quantify invariance encoded in the unsupservised learned features with invariance tests. [sent-234, score-0.619]
</p><p>83 In this experiment, we take the approach described in [4] and measure the change in features as input image undergoes transformations. [sent-235, score-0.231]
</p><p>84 A patch is extracted from a natural image, and transformed through tranlation, rotation and zoom. [sent-236, score-0.21]
</p><p>85 We measure the Mean Squared Error (MSE) between the L2 normalized feature vector of the transformed patch and the feature vector of the original patch 3 . [sent-237, score-0.248]
</p><p>86 Results of invariance tests are 3 MSE is normalized against feature dimensions, and averaged across 100 randomly sampled patches. [sent-239, score-0.301]
</p><p>87 Our features trained with temporal slowness have better invariance properties compared to features learned only using sparity, and SIFT 5 . [sent-243, score-1.525]
</p><p>88 Speciﬁcally, as shown on the left of Figure 8, feature tracking reduces translation invariance in agreement with our analysis in Section 4. [sent-245, score-0.361]
</p><p>89 At the same time, middle and right plots of Figure 8 show that feature tracking increases the non-trivial rotation and zoom invariance in the second layer of our temporal slowness features. [sent-248, score-1.536]
</p><p>90 Figure 8: Invariance tests comparing our temporal slowness features using tracked and non-tracked sequences, against SIFT and features trained only with sparsity, shown for different transformations: Translation (left), Rotation (middle) and Zoom (right). [sent-249, score-1.433]
</p><p>91 5  Conclusion  We have described an unsupervised learning algorithm for learning invariant features from video using the temporal slowness principle. [sent-250, score-1.393]
</p><p>92 The system is improved by using simulated ﬁxations and smooth pursuit to generate the video sequences provided to the learning algorithm. [sent-251, score-0.394]
</p><p>93 We illustrate by virtual of visualization and invariance tests, that the learned features are invariant to a collection of non-trivial transformations. [sent-252, score-0.576]
</p><p>94 With concrete recognition experiments, we show that the features learned from natural videos not only apply to still images, but also give competitive results on a number of object recognition benchmarks. [sent-253, score-0.554]
</p><p>95 Unsupervised natural experience rapidly alters invariant object representation in visual cortex. [sent-259, score-0.245]
</p><p>96 Unsupervised learning of visual features through spike timing dependent plasticity. [sent-291, score-0.245]
</p><p>97 4 Translation test is performed with 16x16 patches and ﬁrst layer features, rotation and zoom tests are performed with 32x32 patches and second layer features. [sent-315, score-0.742]
</p><p>98 Temporal coherence, natural image sequences and the visual cortex. [sent-321, score-0.266]
</p><p>99 An analysis of single layer networks in unsupervised feature learning. [sent-340, score-0.38]
</p><p>100 Learning optimized features for hierarchical models of invariant object recogo nition. [sent-471, score-0.298]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('slowness', 0.673), ('layer', 0.252), ('video', 0.197), ('invariance', 0.192), ('temporal', 0.188), ('features', 0.167), ('pooling', 0.141), ('tracked', 0.132), ('xations', 0.126), ('videos', 0.117), ('pubfig', 0.102), ('invariances', 0.089), ('sequences', 0.088), ('invariant', 0.087), ('sfa', 0.082), ('unsupervised', 0.081), ('visual', 0.078), ('patch', 0.077), ('modules', 0.073), ('deep', 0.072), ('trained', 0.07), ('learned', 0.068), ('caltech', 0.066), ('rotation', 0.066), ('tracking', 0.064), ('image', 0.064), ('stimuli', 0.063), ('hateren', 0.062), ('module', 0.062), ('visualization', 0.062), ('recognition', 0.061), ('images', 0.06), ('architecture', 0.059), ('translation', 0.058), ('pursuit', 0.054), ('zoom', 0.054), ('pooled', 0.052), ('cadieu', 0.05), ('hyvarinen', 0.05), ('feature', 0.047), ('hans', 0.047), ('coates', 0.046), ('hurri', 0.044), ('object', 0.044), ('network', 0.043), ('mse', 0.042), ('convolutional', 0.042), ('cvpr', 0.041), ('ngiam', 0.041), ('convnet', 0.041), ('mobahi', 0.041), ('patches', 0.041), ('sc', 0.04), ('frames', 0.039), ('weighting', 0.039), ('transformations', 0.039), ('vision', 0.037), ('receptive', 0.037), ('ica', 0.036), ('stacked', 0.036), ('natural', 0.036), ('vlfeat', 0.036), ('tests', 0.036), ('interpolation', 0.035), ('reconstruction', 0.033), ('slow', 0.033), ('spatial', 0.033), ('learn', 0.032), ('bergstra', 0.031), ('koh', 0.031), ('leistner', 0.031), ('training', 0.031), ('extracted', 0.031), ('smooth', 0.03), ('translational', 0.03), ('coherent', 0.03), ('bases', 0.029), ('sparsity', 0.029), ('tracker', 0.028), ('warping', 0.028), ('boureau', 0.028), ('xation', 0.028), ('sift', 0.028), ('levels', 0.028), ('pixels', 0.028), ('simulate', 0.028), ('angle', 0.027), ('bilinear', 0.027), ('topographic', 0.027), ('encode', 0.027), ('units', 0.027), ('le', 0.027), ('van', 0.026), ('across', 0.026), ('kavukcuoglu', 0.026), ('coding', 0.025), ('simulated', 0.025), ('correspondences', 0.025), ('layers', 0.024), ('eye', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="90-tfidf-1" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>2 0.20606898 <a title="90-tfidf-2" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>Author: Falk Lieder, Thomas Griffiths, Noah Goodman</p><p>Abstract: Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we ﬁnd that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to signiﬁcant global distortions like large translations and scale. 1</p><p>3 0.20606898 <a title="90-tfidf-3" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>Author: Adam Coates, Andrej Karpathy, Andrew Y. Ng</p><p>Abstract: Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we ﬁnd that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to signiﬁcant global distortions like large translations and scale. 1</p><p>4 0.19017397 <a title="90-tfidf-4" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>5 0.18366411 <a title="90-tfidf-5" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>6 0.17258602 <a title="90-tfidf-6" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>7 0.15035118 <a title="90-tfidf-7" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>8 0.14167561 <a title="90-tfidf-8" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>9 0.14113933 <a title="90-tfidf-9" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>10 0.096239462 <a title="90-tfidf-10" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>11 0.095356554 <a title="90-tfidf-11" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>12 0.095238827 <a title="90-tfidf-12" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>13 0.087848224 <a title="90-tfidf-13" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>14 0.086729996 <a title="90-tfidf-14" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>15 0.084950276 <a title="90-tfidf-15" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>16 0.079062924 <a title="90-tfidf-16" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>17 0.078476898 <a title="90-tfidf-17" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>18 0.075793378 <a title="90-tfidf-18" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>19 0.072837502 <a title="90-tfidf-19" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>20 0.070411958 <a title="90-tfidf-20" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.17), (1, 0.06), (2, -0.3), (3, 0.029), (4, 0.156), (5, 0.009), (6, -0.004), (7, -0.094), (8, 0.038), (9, -0.064), (10, -0.014), (11, 0.055), (12, -0.137), (13, 0.052), (14, -0.05), (15, -0.093), (16, 0.064), (17, -0.015), (18, 0.058), (19, -0.022), (20, -0.003), (21, -0.035), (22, -0.008), (23, -0.024), (24, 0.03), (25, -0.026), (26, -0.014), (27, 0.007), (28, 0.023), (29, 0.003), (30, 0.04), (31, 0.086), (32, 0.037), (33, -0.069), (34, -0.0), (35, 0.015), (36, 0.036), (37, 0.025), (38, 0.028), (39, 0.034), (40, 0.011), (41, -0.027), (42, 0.011), (43, 0.038), (44, -0.048), (45, -0.032), (46, 0.029), (47, -0.007), (48, -0.027), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96637791 <a title="90-lsi-1" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>2 0.85675937 <a title="90-lsi-2" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>Author: Gary Huang, Marwan Mattar, Honglak Lee, Erik G. Learned-miller</p><p>Abstract: Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face veriﬁcation. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Speciﬁcally, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the speciﬁc data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned ﬁlters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face veriﬁcation compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method. 1</p><p>3 0.82424188 <a title="90-lsi-3" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>Author: Richard Socher, Brody Huval, Bharath Bath, Christopher D. Manning, Andrew Y. Ng</p><p>Abstract: Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, ﬁxed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efﬁcient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. 1</p><p>4 0.77240515 <a title="90-lsi-4" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>5 0.76438653 <a title="90-lsi-5" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>Author: Ryan Kiros, Csaba Szepesvári</p><p>Abstract: The task of image auto-annotation, namely assigning a set of relevant tags to an image, is challenging due to the size and variability of tag vocabularies. Consequently, most existing algorithms focus on tag assignment and ﬁx an often large number of hand-crafted features to describe image characteristics. In this paper we introduce a hierarchical model for learning representations of standard sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection for annotation. We benchmark our model on the STL-10 recognition dataset, achieving state-of-the-art performance. When our features are combined with TagProp (Guillaumin et al.), we compete with or outperform existing annotation approaches that use over a dozen distinct handcrafted image descriptors. Furthermore, using 256-bit codes and Hamming distance for training TagProp, we exchange only a small reduction in performance for efﬁcient storage and fast comparisons. Self-taught learning is used in all of our experiments and deeper architectures always outperform shallow ones. 1</p><p>6 0.7119379 <a title="90-lsi-6" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>7 0.68557084 <a title="90-lsi-7" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>8 0.6846388 <a title="90-lsi-8" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>9 0.6846388 <a title="90-lsi-9" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>10 0.65133405 <a title="90-lsi-10" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>11 0.6474579 <a title="90-lsi-11" href="./nips-2012-Large_Scale_Distributed_Deep_Networks.html">170 nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>12 0.63784438 <a title="90-lsi-12" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>13 0.61184567 <a title="90-lsi-13" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>14 0.59616417 <a title="90-lsi-14" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>15 0.56427759 <a title="90-lsi-15" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>16 0.55885094 <a title="90-lsi-16" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>17 0.55523896 <a title="90-lsi-17" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>18 0.54503626 <a title="90-lsi-18" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>19 0.53663605 <a title="90-lsi-19" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>20 0.53138733 <a title="90-lsi-20" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (17, 0.015), (20, 0.184), (21, 0.043), (38, 0.083), (42, 0.045), (54, 0.027), (55, 0.081), (74, 0.082), (76, 0.139), (80, 0.062), (92, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83139104 <a title="90-lda-1" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>2 0.80733317 <a title="90-lda-2" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>Author: Felipe Trevizan, Manuela Veloso</p><p>Abstract: Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artiﬁcial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP always ﬁnishes and is asymptotically optimal under sufﬁcient conditions on the structure of short-sighted SSPs. We empirically compare SSiPP using trajectorybased short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states. 1</p><p>3 0.72872138 <a title="90-lda-3" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>Author: Ren Xiaofeng, Liefeng Bo</p><p>Abstract: Finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition. At the core of contour detection technologies are a set of hand-designed gradient features, used by most approaches including the state-of-the-art Global Pb (gPb) operator. In this work, we show that contour detection accuracy can be significantly improved by computing Sparse Code Gradients (SCG), which measure contrast using patch representations automatically learned through sparse coding. We use K-SVD for dictionary learning and Orthogonal Matching Pursuit for computing sparse codes on oriented local neighborhoods, and apply multi-scale pooling and power transforms before classifying them with linear SVMs. By extracting rich representations from pixels and avoiding collapsing them prematurely, Sparse Code Gradients effectively learn how to measure local contrasts and ﬁnd contours. We improve the F-measure metric on the BSDS500 benchmark to 0.74 (up from 0.71 of gPb contours). Moreover, our learning approach can easily adapt to novel sensor data such as Kinect-style RGB-D cameras: Sparse Code Gradients on depth maps and surface normals lead to promising contour detection using depth and depth+color, as veriﬁed on the NYU Depth Dataset. 1</p><p>4 0.72528189 <a title="90-lda-4" href="./nips-2012-Scaled_Gradients_on_Grassmann_Manifolds_for_Matrix_Completion.html">301 nips-2012-Scaled Gradients on Grassmann Manifolds for Matrix Completion</a></p>
<p>Author: Thanh Ngo, Yousef Saad</p><p>Abstract: This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods signiﬁcantly improve canonical gradient methods, especially on ill-conditioned matrices, while maintaining established global convegence and exact recovery guarantees. A connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established. The proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods. 1</p><p>5 0.72332251 <a title="90-lda-5" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>6 0.72312516 <a title="90-lda-6" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>7 0.72279078 <a title="90-lda-7" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>8 0.72004575 <a title="90-lda-8" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>9 0.71772534 <a title="90-lda-9" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>10 0.71395993 <a title="90-lda-10" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>11 0.7134735 <a title="90-lda-11" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>12 0.71287233 <a title="90-lda-12" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>13 0.71211427 <a title="90-lda-13" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>14 0.71150339 <a title="90-lda-14" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>15 0.71116555 <a title="90-lda-15" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>16 0.709571 <a title="90-lda-16" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>17 0.70898938 <a title="90-lda-17" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>18 0.70833647 <a title="90-lda-18" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>19 0.7079106 <a title="90-lda-19" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>20 0.70701158 <a title="90-lda-20" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
