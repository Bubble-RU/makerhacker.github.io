<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-276" href="#">nips2012-276</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</h1>
<br/><p>Source: <a title="nips-2012-276-pdf" href="http://papers.nips.cc/paper/4856-probabilistic-event-cascades-for-alzheimers-disease.pdf">pdf</a></p><p>Author: Jonathan Huang, Daniel Alexander</p><p>Abstract: Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. We introduce the ALPACA (Alzheimer’s disease Probabilistic Cascades) model, a generative model linking latent Alzheimer’s progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a ﬁxed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. We describe efﬁcient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer’s patients from the Alzheimer’s Disease Neuroimaging Initiative. 1</p><p>Reference: <a title="nips-2012-276-reference" href="../nips2012_reference/nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. [sent-6, score-0.539]
</p><p>2 We introduce the ALPACA (Alzheimer’s disease Probabilistic Cascades) model, a generative model linking latent Alzheimer’s progression dynamics to observable biomarker data. [sent-7, score-0.446]
</p><p>3 In contrast with previous works which model disease progression as a ﬁxed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. [sent-8, score-0.956]
</p><p>4 1  Introduction  Models of disease progression are among the core tools of modern medicine for early disease diagnosis, treatment determination and for explaining symptoms to patients. [sent-10, score-0.698]
</p><p>5 Despite their utility, traditional models of disease progression [3, 17] have largely been limited to coarse symptomatic staging which divides patients into a small number of groups by thresholding a crude clinical score of how far the disease has progressed. [sent-14, score-1.036]
</p><p>6 The models are thus only as precise as these crude clinical scores — although providing insight into disease mechanisms, they provide little beneﬁt for early diagnosis or accurate patient staging. [sent-15, score-0.704]
</p><p>7 The recent availability of cross sectional datasets such as the Alzheimer’s Disease Neuroimaging Initiative data has generated intense speculation in the neurology community about the nature of the cascade of events in AD and the ordering in which biomarkers show abnormality. [sent-17, score-0.57]
</p><p>8 Beckett [2] was the ﬁrst, nearly two decades ago, to propose a data driven model of disease progression using a distribution over orderings of clinical events. [sent-20, score-0.565]
</p><p>9 Examples of events in the model of [8] include (but are not limited to) clinical events (such as a transition from Presymptomatic Alzheimer’s to Mild Cognitive Impairment) and the onset of atrophy (reduction of tissue volume). [sent-24, score-0.597]
</p><p>10 By assuming a single universal ordering of events within the disease progression, the method of [8] is able to scale to much larger collections of events, thus achieving much more detailed characterizations of disease progression compared to that of [2]. [sent-25, score-1.16]
</p><p>11 1  The assumption made in [8] of a universal ordering common to all patients within a disease cohort, is a major oversimpliﬁcation of reality, however, where the event ordering can vary considerably among patients even if it is consistent enough to distinguish different diseases. [sent-26, score-1.173]
</p><p>12 In practice, the assumption of a universal ordering within the model means we cannot recover the diversity of orderings over population groups and can make ﬁtting the model to patient data unstable. [sent-27, score-0.631]
</p><p>13 Additionally, like [8], our method can achieve the scalability that is required to produced ﬁne-grained disease progression models. [sent-31, score-0.424]
</p><p>14 • We propose the Alzheimer’s disease Probabilistic Cascades (ALPACA) model, a probabilistic model of disease cascades, allowing for patients to have distinct event orderings. [sent-33, score-0.857]
</p><p>15 • We develop efﬁcient probabilistic inference and learning algorithms for ALPACA, including a novel patient “staging” method, which predicts a patient’s full trajectory through clinical and atrophy events from sparse and noisy measurement data. [sent-34, score-0.8]
</p><p>16 2  Preliminaries: Snapshots of neurodegenerative disease cascades  We model a neurodegenerative disease cascade as an ordering of a discrete set of N events, {e1 , . [sent-36, score-0.991]
</p><p>17 These events represent changes in patient state, such as a sufﬁciently low score on a memory test for a clinical diagnosis of AD, or the ﬁrst measurement of tissue pathology, such as signiﬁcant atrophy in the hippocampus (memory related brain area). [sent-40, score-0.844]
</p><p>18 An ordering over events is represented as a permutation σ which corresponds events to the positions within the ordering at which they occur. [sent-41, score-0.882]
</p><p>19 In practice, the ordering σ for a particular patient can only be observed indirectly via snapshots which probe at a particular point in time whether each event has occurred or not. [sent-46, score-0.953]
</p><p>20 We denote a snapshot by a vector of N measurements z = (ze1 , . [sent-47, score-0.49]
</p><p>21 , zen ), where each zei is a real valued measurement reﬂecting a noisy diagnosis as to whether event i of the disease progression has occurred prior to measuring z. [sent-50, score-0.825]
</p><p>22 1 Were it not for noise within the measurement process, a single snapshot z would partition the event set into two disjoint subsets: events that have occurred already (e. [sent-51, score-1.035]
</p><p>23 Where prior models [8] considered data in which a patient is only associated with a single snapshot (taken at a single time point), we allow for multiple snapshots of a patient to be taken spaced throughout that patient’s disease cascade. [sent-62, score-1.511]
</p><p>24 For example, if σ = e3 |e1 |e4 |e5 |e6 |e2 , then k = 2 snapshots might partition the event ordering into sets X1 = {e1 , e3 }, X2 = {e4 , e5 }, X3 = {e2 , e6 }, reﬂecting that events in X1 occur before events in X2 , which occur before events in X3 . [sent-64, score-1.278]
</p><p>25 Such partitions can also be thought of as partial rankings over the events (and indeed, we will exploit recent methods for learning with partial rankings in our own approach, [11]). [sent-65, score-0.4]
</p><p>26 To denote partial rankings, we again use vertical bars, separating the events that occur between snapshots. [sent-66, score-0.307]
</p><p>27 Instead of reasoning with continuous snapshot times, we use the fact that many distinct snapshot times can result in the same partial ranking, to reason instead with discrete snapshot sets. [sent-70, score-1.446]
</p><p>28 By snapshot set, we refer to the collection of positions in the full event ordering just before each snapshot is taken. [sent-71, score-1.337]
</p><p>29 In our running example, the snapshot set is τ = {2, 4}. [sent-72, score-0.464]
</p><p>30 Given a full ordering σ, the partial ranking which arises from snapshot data (assuming no noise) is fully determined by τ . [sent-73, score-0.842]
</p><p>31 3  ALPACA: the Alzheimer’s disease Probabilistic Cascades model  We now present ALPACA, a generative model of noisy snapshots in which the event ordering for each patient is a latent variable. [sent-76, score-1.142]
</p><p>32 ALPACA makes two main assumptions: (1), that the measured outcomes for each patient are independent of each other and (2), conditioned on the event ordering of each 1 For notational simplicity, we assume that measurements corresponding to each event are scalar valued. [sent-77, score-0.957]
</p><p>33 2  patient and the time at which a snapshot is taken, the measurements for each event are independent. [sent-79, score-0.99]
</p><p>34 In contrast with [8], we do not assume that multiple snapshot vectors for the same patient are independent of each other. [sent-80, score-0.778]
</p><p>35 Draw an ordering of the events σ (j) from a Mallows distribution P (σ; σ0 , λ) over orderings. [sent-86, score-0.441]
</p><p>36 Draw a snapshot set τ (j) from a uniform distribution P (τ ) over subsets of the event set. [sent-88, score-0.65]
</p><p>37 For each element of the snapshot set, τi = τ1 , . [sent-90, score-0.464]
</p><p>38 , if event e has occurred prior to time τi ), draw zi,e  ∼  (j)  N (µoccurred , coccurred ). [sent-98, score-0.332]
</p><p>39 e e e e (j)  In the above basic model, each entry of a snapshot vector, zi,e , is generated by sampling from a univariate measurement model (assumed in this case to be Gaussian). [sent-100, score-0.574]
</p><p>40 If event e has already occurred, (j) (j) the observation zi,e is sampled from the distribution N (µoccurred , coccurred ) — otherwise zi,e is e e sampled from a measurement distribution estimated from a control population of healthy individuals, N (µhealthy , chealthy ). [sent-101, score-0.395]
</p><p>41 For notational simplicity, we denote the collection of snapshots for patient e e (j) (j) j by z·,· = {zi,e }i=1,. [sent-102, score-0.459]
</p><p>42 The Kendall’s tau distance penalizes the number of inversions, or pairs of events for which σ and σ0 disagree over relative ordering. [sent-112, score-0.306]
</p><p>43 Our algorithms are thus applicable for more general classes of distributions over orderings as well as snapshot sets. [sent-117, score-0.537]
</p><p>44 With respect to the event-based characterization of disease progression, a critical problem is that of patient staging, the problem of determining the extent to which a disease has progressed for a particular patient given corresponding measurement data. [sent-119, score-1.258]
</p><p>45 ALPACA offers a simple and natural formulation of the patient staging problem as a probabilistic inference query. [sent-120, score-0.465]
</p><p>46 In particular, given the measurements corresponding to a particular patient, we perform patient staging by: (1) computing a posterior distribution over the event ordering σ (j) , then (2) computing a posterior distribution over the most recent element of the snapshot set τ (j) . [sent-121, score-1.423]
</p><p>47 To visualize the posterior distribution over the event ordering σ (j) , we plot a simple “ﬁrst-order staging diagram”, displaying the probability that event e has occurred (or will occur) in position q according to the posterior. [sent-122, score-0.87]
</p><p>48 Two major features differentiate ALPACA from traditional patient staging approaches, in which patients are binned into a small number of imprecisely deﬁned stages. [sent-123, score-0.563]
</p><p>49 In particular, our method is more ﬁne-grained, allowing for a detailed picture of what the patient has undergone as well as a prediction of what is to come next. [sent-124, score-0.314]
</p><p>50 Given a collection of K (j) snapshots for a patient j, the critical inference problem that we must solve is that of computing a posterior distribution over the latent event order and snapshot set for that patient. [sent-129, score-1.176]
</p><p>51 One reason is that it is not obvious how to tractably sample the event ordering σ conditioned on its Markov blanket, given that the corresponding likelihood function is not conjugate prior to the Mallows model. [sent-134, score-0.431]
</p><p>52 This augmented model is equivalent to the original model, but has the advantage that it reduces the sampling step for the event ordering σ to a well understood problem (described below). [sent-138, score-0.437]
</p><p>53 1)  Observe that since the snapshot set τ is fully determined by the partial ranking γ, it is not necessary to condition on τ in Equation 4. [sent-141, score-0.619]
</p><p>54 1 (right), since γ is fully determined given both the event ordering σ and the snapshot set τ , one can sample τ ﬁrst, and deterministically reconstruct γ. [sent-144, score-0.873]
</p><p>55 We now turn to the problem of sampling a snapshot set τ (j) of size K (j) from Equation 4. [sent-157, score-0.492]
</p><p>56 In the following, we present a dynamic programming algorithm for sampling snapshot sets with running time much lower than the exhaustive setting (even for small K (j) ). [sent-161, score-0.492]
</p><p>57 Our core insight is to exploit conditional independence relations within the posterior distribution over snapshot sets. [sent-162, score-0.506]
</p><p>58 As we show in the following, however, we can bijectively associate each snapshot set with a trajectory through a certain grid. [sent-164, score-0.464]
</p><p>59 With respect to this grid-based representation of snapshot sets, we then show that the posterior distribution can be viewed as that of a particular hidden Markov model (HMM). [sent-165, score-0.506]
</p><p>60 , given a snapshot set τ , there is a unique staircase walk pτ mapping to τ ). [sent-186, score-0.669]
</p><p>61 (b): Grid structured state space G for sampling snapshot sets with edges labeled with transition probabilities according to Equation 4. [sent-188, score-0.492]
</p><p>62 The example path (highlighted) is p = ((2, 3), (2, 2), (1, 2), (1, 1), (0, 1), (0, 0)), corresponding to the snapshot set τ = {4, 2}. [sent-191, score-0.464]
</p><p>63 Conditioned on σ = σ (j) , the posterior probability P (τ = τ (j) | σ = σ (j) , z·,· ) is equal to the posterior probability of the staircase walk pτ (j) under the hidden Markov model deﬁned by Equations 4. [sent-206, score-0.289]
</p><p>64 To sample a snapshot set from the conditional distribution in Equation 4. [sent-209, score-0.464]
</p><p>65 2, we therefore sample staircase walks from the above HMM and convert the resulting samples to snapshot sets. [sent-210, score-0.673]
</p><p>66 We claim that the complexity of sampling a snapshot set is also O(N 2 ). [sent-214, score-0.492]
</p><p>67 The one exception is when the size of the snapshot set is one less than the number of events (K (j) = N − 1). [sent-225, score-0.682]
</p><p>68 2  Note that to have so many snapshots for a single patient would be rare indeed. [sent-230, score-0.459]
</p><p>69 5  Even when K (j) < N − 1, mixing times for the chain can be longer for larger snapshot sets (where K (j) is close to N − 1). [sent-231, score-0.503]
</p><p>70 For example, when K (j) = N − 2, it is possible to show that the T th ordering in the Gibbs chain can differ from the (T + 1)th ordering by at most an adjacent swap. [sent-232, score-0.446]
</p><p>71 For each patient in the cohort, use the inference algorithm described in Section 4. [sent-244, score-0.339]
</p><p>72 Note that the sampled snapshot sets ({τ (j) }) do not play a role in the M-step described here, but can be used to estimate parameters for the more complex snapshot set distributions described in Section 5. [sent-252, score-0.928]
</p><p>73 In clinical datasets, it is more conceivable that different biomarkers within a disease cascade change over different timescales, thus leading to higher positional variance for certain events and lower positional variance for others. [sent-259, score-0.756]
</p><p>74 ([11]), in particular, proved that these hierarchical rifﬂe independent models form a natural conjugate prior family for partial ranking likelihood functions and introduced efﬁcient algorithms for conditioning on partial ranking observations. [sent-264, score-0.31]
</p><p>75 It is ﬁnally interesting to note that it would not be trivial to use traditional Markov chains to capture the dependencies in the event sequence due to the fact that observations come in snapshot form instead of being indexed by time as they would be in an ordinary hidden Markov model. [sent-265, score-0.65]
</p><p>76 (Left) ﬁrst order staging diagram, the (e, q)th entry is the probability that event e has/will occur in position q. [sent-287, score-0.369]
</p><p>77 (Right) posterior probability distribution over the position in the event ordering at which the patient snapshot was taken. [sent-288, score-1.251]
</p><p>78 Setting 0 ≤ α < 1/2, however, reﬂects a prior bias for snapshots to have been taken earlier in the disease cascade, while setting 1/2 < α ≤ 1 reﬂects a prior bias for snapshots to have been taken later in the disease cascade. [sent-291, score-0.838]
</p><p>79 Since we are interested in the ability of the model to recover the true central ranking, we evaluate based on the Kendall’s tau distance between the ground truth central ranking and the central rankings learned by our algorithms. [sent-294, score-0.331]
</p><p>80 2(a) illustrates the results on a problem with N = 10 events and 250 patients (with K (j) set to be 1, 2, or 3 randomly for each patient) as cM AX varies between [0. [sent-297, score-0.341]
</p><p>81 2(b) shows, as expected, that recovery rates for the central ordering improve as the number of patients increases. [sent-307, score-0.381]
</p><p>82 2(c) shows example Gibbs trace plots with N = 20 events and varying sizes of the snapshot set, K (j) . [sent-312, score-0.682]
</p><p>83 [8] (which assumes that all patients follow a single ordering σ ∗ ) by searching exhaustively over the collection of all 7! [sent-320, score-0.346]
</p><p>84 We use a single Gaussian for each of the healthy and occurred measurement distributions (as described in [8]), assuming that all patients in the control group are healthy. [sent-330, score-0.341]
</p><p>85 3 The results show that by allowing for the event ordering σ to vary across patients, the ALPACA model signiﬁcantly outperforms the single ordering model (shown in the σ ∗ column) in BIC score with respect to all of the tried settings of α. [sent-331, score-0.653]
</p><p>86 The optimal central ordering inferred by the Fonteijn model is: σ ∗ = adas|hippovol|hippoatrophy|brainatrophy|abeta|tau|brainvol, while ALPACA infers the central ordering: σ0 = adas|hippovol|abeta|hippoatrophy|tau|brainatrophy|brainvol. [sent-334, score-0.293]
</p><p>87 Observe that the two event orderings are largely in agreement with each other with CSF Aβ42 and CSF tau events shifted to being earlier in the event ordering, which is more consistent with current thinking in neurology [12, 5, 1], which places the two CSF events ﬁrst. [sent-335, score-1.004]
</p><p>88 It is surprising that the hippocampal volume and atrophy events are inferred in both models to occur before the CSF events [13], but we believe that this may be due to the signiﬁcant proportion of misdiagnosed patients in the data. [sent-337, score-0.741]
</p><p>89 2(e) shows the patient staging result for an example patient from the ADNI data. [sent-341, score-0.754]
</p><p>90 The left matrix visualizes the probability that each event will occur in each position of the event ordering given snapshot data from this patient, while the right histogram visualizes where in the event ordering the patient was situated when the snapshot was taken. [sent-342, score-2.303]
</p><p>91 7  Conclusions  We have developed the Alzheimer’s disease Probabilistic Cascades model for event ordering within the Alzheimer’s disease cascade. [sent-343, score-0.957]
</p><p>92 In its most basic form, ALPACA is a simple model with generative semantics, allowing one to learn the central ordering of events that occur within a disease progression as well as to quantify the variance of this ordering across patients. [sent-344, score-1.158]
</p><p>93 Our preliminary results show that relaxing the notion that a single ordering over events exists for all patients allows ALPACA to achieve a much better ﬁt to snapshot data from a cohort of Alzheimer’s patients. [sent-345, score-1.117]
</p><p>94 One of our main contributions is to show how the combinatorial structure of event ordering models can be exploited for algorithmic efﬁciency. [sent-346, score-0.409]
</p><p>95 There may exist biomarkers for Alzheimer’s which are more effective than those considered in our current work for the purposes of patient staging. [sent-348, score-0.381]
</p><p>96 Identifying such biomarker events remains an open question crucial to the success of data-driven models of disease cascades. [sent-349, score-0.514]
</p><p>97 We have discussed several such possible extensions, from more general measurement models to more general rifﬂe independent ordering models. [sent-351, score-0.305]
</p><p>98 The alzheimer’s disease neuroimaging initiative: progress report and future plans. [sent-376, score-0.318]
</p><p>99 s disease biomarkers in the alzheimer’s disease neuroimaging initiative cohort. [sent-394, score-0.682]
</p><p>100 An event-based model for disease progression and its application in familial alzheimer’s disease and huntington’s disease. [sent-415, score-0.723]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('snapshot', 0.464), ('patient', 0.314), ('alpaca', 0.304), ('disease', 0.274), ('ordering', 0.223), ('alzheimer', 0.221), ('events', 0.218), ('event', 0.186), ('staircase', 0.164), ('progression', 0.15), ('snapshots', 0.145), ('staging', 0.126), ('mallows', 0.123), ('patients', 0.123), ('ranking', 0.101), ('atrophy', 0.093), ('cohort', 0.089), ('csf', 0.089), ('tau', 0.088), ('occurred', 0.085), ('measurement', 0.082), ('gibbs', 0.074), ('orderings', 0.073), ('clinical', 0.068), ('adni', 0.068), ('biomarkers', 0.067), ('neurodegenerative', 0.067), ('adas', 0.063), ('cascades', 0.059), ('partial', 0.054), ('healthy', 0.051), ('fonteijn', 0.051), ('positional', 0.051), ('rif', 0.051), ('diagnosis', 0.048), ('jonathan', 0.048), ('walks', 0.045), ('neuroimaging', 0.044), ('posterior', 0.042), ('walk', 0.041), ('hmm', 0.041), ('mixing', 0.039), ('grid', 0.039), ('kendall', 0.039), ('abeta', 0.038), ('brainatrophy', 0.038), ('chealthy', 0.038), ('clifford', 0.038), ('coccurred', 0.038), ('hippoatrophy', 0.038), ('hippovol', 0.038), ('sampler', 0.037), ('rankings', 0.037), ('central', 0.035), ('neurology', 0.035), ('occur', 0.035), ('pg', 0.034), ('aisen', 0.033), ('shaw', 0.033), ('ronald', 0.032), ('bic', 0.032), ('yt', 0.032), ('petersen', 0.031), ('leslie', 0.029), ('jack', 0.029), ('alzheimers', 0.029), ('hippocampal', 0.029), ('sampling', 0.028), ('cascade', 0.027), ('measurements', 0.026), ('huang', 0.025), ('brainvol', 0.025), ('familial', 0.025), ('jagust', 0.025), ('laurel', 0.025), ('misdiagnosed', 0.025), ('oversimpli', 0.025), ('rachael', 0.025), ('sarah', 0.025), ('scahill', 0.025), ('schott', 0.025), ('tgibbs', 0.025), ('trojanowski', 0.025), ('inference', 0.025), ('draw', 0.023), ('initiative', 0.023), ('xt', 0.023), ('conditioned', 0.022), ('neurological', 0.022), ('biomarker', 0.022), ('pathology', 0.022), ('position', 0.022), ('ecting', 0.022), ('john', 0.022), ('universal', 0.021), ('worst', 0.021), ('subjects', 0.021), ('michael', 0.021), ('score', 0.021), ('gracefully', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="276-tfidf-1" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>Author: Jonathan Huang, Daniel Alexander</p><p>Abstract: Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. We introduce the ALPACA (Alzheimer’s disease Probabilistic Cascades) model, a generative model linking latent Alzheimer’s progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a ﬁxed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. We describe efﬁcient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer’s patients from the Alzheimer’s Disease Neuroimaging Initiative. 1</p><p>2 0.22731018 <a title="276-tfidf-2" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>Author: Jenna Wiens, Eric Horvitz, John V. Guttag</p><p>Abstract: A patient’s risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient’s pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient outcomes, considering only the patient’s current or aggregate state. In this paper, we represent patient risk as a time series. In doing so, patient risk stratiﬁcation becomes a time-series classiﬁcation task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must ﬁrst be extracted. Thus, we begin by deﬁning and extracting approximate risk processes, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classiﬁcation with the goal of identifying high-risk patterns. We apply the classiﬁcation to the speciﬁc task of identifying patients at risk of testing positive for hospital acquired Clostridium difﬁcile. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratiﬁcation outperforms classiﬁers that consider only a patient’s current state (p<0.05). 1</p><p>3 0.13040648 <a title="276-tfidf-3" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>Author: Hua Wang, Feiping Nie, Heng Huang, Jingwen Yan, Sungeun Kim, Shannon Risacher, Andrew Saykin, Li Shen</p><p>Abstract: Alzheimer’s disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in imaging and cognitive data by structured sparsity-inducing norms. The sparsity of the model enables the selection of a small number of imaging measures while maintaining high prediction accuracy. The empirical studies, using the longitudinal imaging and cognitive data of the ADNI cohort, have yielded promising results.</p><p>4 0.12959923 <a title="276-tfidf-4" href="./nips-2012-Bayesian_Pedigree_Analysis_using_Measure_Factorization.html">53 nips-2012-Bayesian Pedigree Analysis using Measure Factorization</a></p>
<p>Author: Bonnie Kirkpatrick, Alexandre Bouchard-côté</p><p>Abstract: Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease. With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites. Some pedigrees number in the thousands of individuals. Meanwhile, analysis methods have remained limited to pedigrees of < 100 individuals which limits analyses to many small independent pedigrees. Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited. This is because linkage analysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order. LODs are difﬁcult to interpret and nontrivial to extend to consider interactions among sites. These developments and difﬁculties call for the creation of modern methods of pedigree analysis. Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models. We show that these disease models can be turned into accurate and computationally efﬁcient estimators. The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models. This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction. 1</p><p>5 0.10507084 <a title="276-tfidf-5" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>Author: Uri Maoz, Shengxuan Ye, Ian Ross, Adam Mamelak, Christof Koch</p><p>Abstract: The ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientiﬁc study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action as well as for brain-machine interfaces. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a “matching-pennies” game against an opponent. In each trial, subjects were given a 5 s countdown, after which they had to raise their left or right hand immediately as the “go” signal appeared on a computer screen. They won a ﬁxed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The question we here studied was the extent to which neural precursors of the subjects’ decisions can be detected in intracranial local ﬁeld potentials (LFP) prior to the onset of the action. We found that combined low-frequency (0.1–5 Hz) LFP signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5 s before the go signal with 68±3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 electrodes simultaneously, and tested it on retrospective data from 7 patients. On average, we could predict the correct hand choice in 83% of the trials, which rose to 92% if we let the system drop 3/10 of the trials on which it was less conﬁdent. Our system demonstrates— for the ﬁrst time—the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings, well before the action occurs. 1 1</p><p>6 0.099608473 <a title="276-tfidf-6" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>7 0.096671395 <a title="276-tfidf-7" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>8 0.088683881 <a title="276-tfidf-8" href="./nips-2012-Learning_Networks_of_Heterogeneous_Influence.html">182 nips-2012-Learning Networks of Heterogeneous Influence</a></p>
<p>9 0.087713473 <a title="276-tfidf-9" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>10 0.083590977 <a title="276-tfidf-10" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>11 0.074872874 <a title="276-tfidf-11" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>12 0.071542658 <a title="276-tfidf-12" href="./nips-2012-Iterative_ranking_from_pair-wise_comparisons.html">165 nips-2012-Iterative ranking from pair-wise comparisons</a></p>
<p>13 0.064872779 <a title="276-tfidf-13" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>14 0.063849851 <a title="276-tfidf-14" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>15 0.059958443 <a title="276-tfidf-15" href="./nips-2012-Statistical_Consistency_of_Ranking_Methods_in_A_Rank-Differentiable_Probability_Space.html">323 nips-2012-Statistical Consistency of Ranking Methods in A Rank-Differentiable Probability Space</a></p>
<p>16 0.059189178 <a title="276-tfidf-16" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>17 0.058788404 <a title="276-tfidf-17" href="./nips-2012-Random_Utility_Theory_for_Social_Choice.html">286 nips-2012-Random Utility Theory for Social Choice</a></p>
<p>18 0.057587411 <a title="276-tfidf-18" href="./nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">141 nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<p>19 0.057221662 <a title="276-tfidf-19" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>20 0.054826468 <a title="276-tfidf-20" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.009), (2, 0.008), (3, 0.057), (4, -0.048), (5, -0.056), (6, 0.001), (7, 0.057), (8, -0.024), (9, 0.079), (10, -0.043), (11, -0.006), (12, -0.014), (13, -0.04), (14, 0.012), (15, -0.081), (16, 0.012), (17, -0.011), (18, 0.012), (19, -0.113), (20, 0.074), (21, 0.067), (22, -0.199), (23, -0.067), (24, 0.11), (25, -0.186), (26, -0.012), (27, 0.13), (28, -0.098), (29, 0.089), (30, 0.122), (31, -0.117), (32, -0.064), (33, 0.069), (34, -0.045), (35, -0.059), (36, 0.114), (37, -0.095), (38, -0.019), (39, -0.049), (40, -0.075), (41, 0.152), (42, -0.096), (43, 0.058), (44, 0.193), (45, -0.055), (46, -0.077), (47, 0.007), (48, 0.055), (49, -0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93230665 <a title="276-lsi-1" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>Author: Jonathan Huang, Daniel Alexander</p><p>Abstract: Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. We introduce the ALPACA (Alzheimer’s disease Probabilistic Cascades) model, a generative model linking latent Alzheimer’s progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a ﬁxed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. We describe efﬁcient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer’s patients from the Alzheimer’s Disease Neuroimaging Initiative. 1</p><p>2 0.64847147 <a title="276-lsi-2" href="./nips-2012-Bayesian_Pedigree_Analysis_using_Measure_Factorization.html">53 nips-2012-Bayesian Pedigree Analysis using Measure Factorization</a></p>
<p>Author: Bonnie Kirkpatrick, Alexandre Bouchard-côté</p><p>Abstract: Pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease. With the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites. Some pedigrees number in the thousands of individuals. Meanwhile, analysis methods have remained limited to pedigrees of < 100 individuals which limits analyses to many small independent pedigrees. Disease models, such those used for the linkage analysis log-odds (LOD) estimator, have similarly been limited. This is because linkage analysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order. LODs are difﬁcult to interpret and nontrivial to extend to consider interactions among sites. These developments and difﬁculties call for the creation of modern methods of pedigree analysis. Drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models. We show that these disease models can be turned into accurate and computationally efﬁcient estimators. The technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models. This method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction. 1</p><p>3 0.61134905 <a title="276-lsi-3" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>Author: Jenna Wiens, Eric Horvitz, John V. Guttag</p><p>Abstract: A patient’s risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient’s pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient outcomes, considering only the patient’s current or aggregate state. In this paper, we represent patient risk as a time series. In doing so, patient risk stratiﬁcation becomes a time-series classiﬁcation task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must ﬁrst be extracted. Thus, we begin by deﬁning and extracting approximate risk processes, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classiﬁcation with the goal of identifying high-risk patterns. We apply the classiﬁcation to the speciﬁc task of identifying patients at risk of testing positive for hospital acquired Clostridium difﬁcile. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratiﬁcation outperforms classiﬁers that consider only a patient’s current state (p<0.05). 1</p><p>4 0.52924621 <a title="276-lsi-4" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>Author: Hua Wang, Feiping Nie, Heng Huang, Jingwen Yan, Sungeun Kim, Shannon Risacher, Andrew Saykin, Li Shen</p><p>Abstract: Alzheimer’s disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions. Regression analysis has been studied to relate neuroimaging measures to cognitive status. However, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in AD research. We propose a novel high-order multi-task learning model to address this issue. The proposed model explores the temporal correlations existing in imaging and cognitive data by structured sparsity-inducing norms. The sparsity of the model enables the selection of a small number of imaging measures while maintaining high prediction accuracy. The empirical studies, using the longitudinal imaging and cognitive data of the ADNI cohort, have yielded promising results.</p><p>5 0.49502882 <a title="276-lsi-5" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>Author: Won H. Kim, Deepti Pachauri, Charles Hatt, Moo. K. Chung, Sterling Johnson, Vikas Singh</p><p>Abstract: Hypothesis testing on signals deﬁned on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in Neuroscience. The goal here is to identify regions that exhibit changes as a function of the clinical condition under study. As the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify. Indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive. In contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex. Our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-Euclidean settings (i.e., irregular weighted graphs). We provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results. Other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere. 1</p><p>6 0.45940971 <a title="276-lsi-6" href="./nips-2012-Assessing_Blinding_in_Clinical_Trials.html">46 nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>7 0.44853488 <a title="276-lsi-7" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>8 0.41615447 <a title="276-lsi-8" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>9 0.3880077 <a title="276-lsi-9" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>10 0.38041151 <a title="276-lsi-10" href="./nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">299 nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>11 0.37449431 <a title="276-lsi-11" href="./nips-2012-Learning_Networks_of_Heterogeneous_Influence.html">182 nips-2012-Learning Networks of Heterogeneous Influence</a></p>
<p>12 0.34341162 <a title="276-lsi-12" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>13 0.34205946 <a title="276-lsi-13" href="./nips-2012-Random_Utility_Theory_for_Social_Choice.html">286 nips-2012-Random Utility Theory for Social Choice</a></p>
<p>14 0.33976024 <a title="276-lsi-14" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>15 0.29925585 <a title="276-lsi-15" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>16 0.29655594 <a title="276-lsi-16" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>17 0.28323203 <a title="276-lsi-17" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>18 0.28256723 <a title="276-lsi-18" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>19 0.27595615 <a title="276-lsi-19" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>20 0.26752126 <a title="276-lsi-20" href="./nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">141 nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.031), (11, 0.02), (17, 0.011), (21, 0.025), (38, 0.106), (39, 0.011), (42, 0.02), (54, 0.032), (55, 0.014), (74, 0.041), (76, 0.165), (80, 0.083), (92, 0.039), (94, 0.327)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96290219 <a title="276-lda-1" href="./nips-2012-Waveform_Driven_Plasticity_in_BiFeO3_Memristive_Devices%3A_Model_and_Implementation.html">362 nips-2012-Waveform Driven Plasticity in BiFeO3 Memristive Devices: Model and Implementation</a></p>
<p>Author: Christian Mayr, Paul Stärke, Johannes Partzsch, Love Cederstroem, Rene Schüffny, Yao Shuai, Nan Du, Heidemarie Schmidt</p><p>Abstract: Memristive devices have recently been proposed as efﬁcient implementations of plastic synapses in neuromorphic systems. The plasticity in these memristive devices, i.e. their resistance change, is deﬁned by the applied waveforms. This behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms. However, learning in memristive devices has so far been approached mostly on a pragmatic technological level. The focus seems to be on ﬁnding any waveform that achieves spike-timing-dependent plasticity (STDP), without regard to the biological veracity of said waveforms or to further important forms of plasticity. Bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced BiFeO3 memristive material. Based on this approach, we show STDP for the ﬁrst time for this material, with learning window replication superior to previous memristor-based STDP implementations. We also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity. To the best of our knowledge, this is the ﬁrst implementations of triplet plasticity on any physical memristive device. 1</p><p>same-paper 2 0.78553462 <a title="276-lda-2" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>Author: Jonathan Huang, Daniel Alexander</p><p>Abstract: Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. We introduce the ALPACA (Alzheimer’s disease Probabilistic Cascades) model, a generative model linking latent Alzheimer’s progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a ﬁxed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. We describe efﬁcient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer’s patients from the Alzheimer’s Disease Neuroimaging Initiative. 1</p><p>3 0.71589231 <a title="276-lda-3" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>Author: Xaq Pitkow</p><p>Abstract: This paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression. The representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals. The compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables. When these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling. Since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain. Interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons. If we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a highdimensional joint distribution implicitly, even without accounting for any noise correlations. This comprises a novel hypothesis for how neurons could encode probabilities in the brain. 1</p><p>4 0.69922972 <a title="276-lda-4" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>Author: Shulin Yang, Liefeng Bo, Jue Wang, Linda G. Shapiro</p><p>Abstract: Fine-grained recognition refers to a subordinate level of recognition, such as recognizing different species of animals and plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape and structure shared cross different categories, and the differences are in the details of object parts. We suggest that the key to identifying the ﬁne-grained differences lies in ﬁnding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the cooccurrence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classiﬁcation. Learning of the template model is efﬁcient, and the recognition results we achieve signiﬁcantly outperform the stateof-the-art algorithms. 1</p><p>5 0.67416567 <a title="276-lda-5" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>Author: Nan Li, Longin J. Latecki</p><p>Abstract: We formulate clustering aggregation as a special instance of Maximum-Weight Independent Set (MWIS) problem. For a given dataset, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters. The vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation. The edges connect the vertices whose corresponding clusters overlap. Intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together. We formalize this intuition as the MWIS problem on the attributed graph, i.e., ﬁnding the heaviest subset of mutually non-adjacent vertices. This MWIS problem exhibits a special structure. Since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (MIS) in the attributed graph. We propose a variant of simulated annealing method that takes advantage of this special structure. Our algorithm starts from each MIS, which is close to a distinct local optimum of the MWIS problem, and utilizes a local search heuristic to explore its neighborhood in order to ﬁnd the MWIS. Extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings. 1</p><p>6 0.63966542 <a title="276-lda-6" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>7 0.59799403 <a title="276-lda-7" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>8 0.59486365 <a title="276-lda-8" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>9 0.59112549 <a title="276-lda-9" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>10 0.58307225 <a title="276-lda-10" href="./nips-2012-CPRL_--_An_Extension_of_Compressive_Sensing_to_the_Phase_Retrieval_Problem.html">63 nips-2012-CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem</a></p>
<p>11 0.56293768 <a title="276-lda-11" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>12 0.5617187 <a title="276-lda-12" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>13 0.56166381 <a title="276-lda-13" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<p>14 0.55980802 <a title="276-lda-14" href="./nips-2012-Spectral_learning_of_linear_dynamics_from_generalised-linear_observations_with_application_to_neural_population_data.html">321 nips-2012-Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</a></p>
<p>15 0.55967712 <a title="276-lda-15" href="./nips-2012-Proper_losses_for_learning_from_partial_labels.html">280 nips-2012-Proper losses for learning from partial labels</a></p>
<p>16 0.55965781 <a title="276-lda-16" href="./nips-2012-Collaborative_Gaussian_Processes_for_Preference_Learning.html">74 nips-2012-Collaborative Gaussian Processes for Preference Learning</a></p>
<p>17 0.55965322 <a title="276-lda-17" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>18 0.55962288 <a title="276-lda-18" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>19 0.55928862 <a title="276-lda-19" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>20 0.55913824 <a title="276-lda-20" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
