<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-349" href="#">nips2012-349</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</h1>
<br/><p>Source: <a title="nips-2012-349-pdf" href="http://papers.nips.cc/paper/4832-training-sparse-natural-image-models-with-a-fast-gibbs-sampler-of-an-extended-state-space.pdf">pdf</a></p><p>Author: Lucas Theis, Jascha Sohl-dickstein, Matthias Bethge</p><p>Abstract: We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. 1</p><p>Reference: <a title="nips-2012-349-reference" href="../nips2012_reference/nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Training sparse natural image models with a fast Gibbs sampler of an extended state space  Jascha Sohl-Dickstein Redwood Center for Theoretical Neuroscience jascha@berkeley. [sent-1, score-0.351]
</p><p>2 org  Abstract We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. [sent-4, score-1.01]
</p><p>3 Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. [sent-5, score-0.612]
</p><p>4 Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. [sent-6, score-0.303]
</p><p>5 Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. [sent-7, score-0.549]
</p><p>6 When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. [sent-8, score-0.266]
</p><p>7 We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. [sent-9, score-1.234]
</p><p>8 In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. [sent-10, score-1.117]
</p><p>9 1  Introduction  Here we study learning and inference in the overcomplete linear model given by x = As,  p(s) =  fi (si ),  (1)  i  where A ∈ RM ×N , N ≥ M , and each marginal source distribution fi may depend on additional parameters. [sent-11, score-0.891]
</p><p>10 Most of the literature on overcomplete linear models assumes observations corrupted by additive Gaussian noise, that is, x = As + ε for a Gaussian distributed random variable ε. [sent-13, score-0.529]
</p><p>11 When the observations are image patches, the source distributions fi (si ) are typically assumed to be sparse or leptokurtotic [e. [sent-15, score-0.502]
</p><p>12 A large family of leptokurtotic distributions which also contains 1  A  s2  p(z|x)  p(s|x) A  B  s1  λ  s A x  B z  Figure 1: A: In the noiseless overcomplete linear model, the posterior distribution over hidden sources s lives on a linear subspace. [sent-19, score-0.958]
</p><p>13 For sparse source distributions, the posterior will generally be heavy-tailed and multimodal, as can be seen on the right. [sent-21, score-0.333]
</p><p>14 B: A graphical model representation of the overcomplete linear model extended by two sets of auxiliary variables (Equation 2 and 3). [sent-22, score-0.529]
</p><p>15 We perform blocked Gibbs sampling between λ and z to sample from the posterior distribution over all latent variables given an observation x. [sent-23, score-0.452]
</p><p>16 For a given λ, the posterior over z becomes Gaussian while for given z, the posterior over λ becomes factorial and is thus easy to sample from. [sent-24, score-0.261]
</p><p>17 the aforementioned distributions as a special case is formed by Gaussian scale mixtures (GSMs), ∞  fi (si ) = 0  gi (λi )N (si ; 0, λ−1 ) dλi , i  (2)  where gi (λi ) is a univariate density over precisions λi . [sent-25, score-0.406]
</p><p>18 In the following, we will concentrate on linear models whose marginal source distributions can be represented as GSMs. [sent-26, score-0.289]
</p><p>19 In particular, the posterior distribution over sources p(s | x) is constrained to a linear subspace and can have multiple modes with heavy tails (Figure 1A). [sent-29, score-0.263]
</p><p>20 Inference can be simpliﬁed by assuming additive Gaussian noise, constraining the source distributions to be log-concave or making crude approximations to the posterior. [sent-30, score-0.273]
</p><p>21 On this account, we use Markov chain Monte Carlo (MCMC) methods to obtain samples with which we represent the posterior distribution. [sent-32, score-0.207]
</p><p>22 In the following, we will describe an efﬁcient blocked Gibbs sampler which exploits the speciﬁc structure of the sparse linear model. [sent-38, score-0.53]
</p><p>23 2  Sampling and inference  In this section, we ﬁrst review the nullspace sampling algorithm of Chen and Wu [4], which solves the problem of sampling from a linear subspace in the noiseless case of the overcomplete linear model. [sent-39, score-1.093]
</p><p>24 We then introduce an additional set of auxiliary variables which leads to an efﬁcient blocked Gibbs sampler. [sent-40, score-0.234]
</p><p>25 1  Nullspace sampling  The basic idea behind the nullspace sampling algorithm is to extend the overcomplete linear model by an additional set of variables z which essentially makes it complete (Figure 1B), x z  A B  =  s,  (3)  where B ∈ R(N −M )×N and square brackets denote concatenation. [sent-42, score-1.038]
</p><p>26 If the rows of A and B are orthogonal, AB = 0, or, in other words, B spans the nullspace of A, we have s = A+ x + B + z,  (4)  where A+ and B + are the pseudoinverses [24] of A and B, respectively. [sent-44, score-0.223]
</p><p>27 An orthogonal basis spanning the nullspace of A can be obtained from A’s singular value decomposition [4]. [sent-46, score-0.3]
</p><p>28 2  Blocked Gibbs sampling  The fact that the marginals fi (si ) are expressed as Gaussian mixtures (Equation 2) can be used to derive an efﬁcient blocked Gibbs sampler. [sent-51, score-0.535]
</p><p>29 The Gibbs sampler alternately samples nullspace representations z and precisions of the source marginals λ. [sent-52, score-0.913]
</p><p>30 The key observation here is that given the precisions λ, the distribution over x and z becomes Gaussian which makes sampling from the posterior distribution tractable. [sent-53, score-0.356]
</p><p>31 A similar idea was pursued by Olshausen and Millman [21], who modeled the source distributions with mixtures of Gaussians and conditionally Gibbs sampled precisions one by one. [sent-54, score-0.45]
</p><p>32 In contrast, here we update all precision variables in parallel by conditioning on the nullspace representation z. [sent-56, score-0.223]
</p><p>33 Using a ﬁnite number of precisions ϑik with prior probabilities πik , for example, the posterior probability of λi being ϑij becomes N (si ; 0, ϑ−1 )πij ij  p(λi = ϑij | x, z) =  k  N (si ; 0, ϑ−1 )πik ik  . [sent-59, score-0.331]
</p><p>34 We use the following computationally xx xx efﬁcient method to conditionally sample Gaussian distributions [8, 14]: x z  ∼ N (0, Σ),  z = z + Σxz Σ−1 (x − x ). [sent-63, score-0.187]
</p><p>35 Together, equations 7 and 9 implement a rapidly mixing blocked Gibbs sampler. [sent-65, score-0.234]
</p><p>36 We empirically show in the results section that for natural image patches the beneﬁts of blocked Gibbs sampling outweigh its computational costs. [sent-67, score-0.507]
</p><p>37 A closely related sampling algorithm was proposed by Park and Casella [23] for implementing Bayesian inference in the linear regression model with Laplace prior. [sent-68, score-0.19]
</p><p>38 The main differences here are that we also consider the noiseless case by exploiting the nullspace representation, that instead of using a ﬁxed Laplace prior we will use the sampler to learn the distribution over source variables, and that we apply the algorithm in the context of image modeling. [sent-69, score-0.706]
</p><p>39 3  Learning  In the following, we describe a learning strategy for the overcomplete linear model based on the idea of persistent Markov chains [26, 32, 36], which already has led to improved learning strategies for a number of different models [e. [sent-72, score-0.713]
</p><p>40 Following Girolami [11] and others, we use expectation maximization (EM) [7] to maximize the likelihood of the overcomplete linear model. [sent-75, score-0.529]
</p><p>41 Instead of a variational approximation, here we use the blocked Gibbs sampler to sample a hidden state z for every data point x in the E-step. [sent-76, score-0.461]
</p><p>42 Despite our efforts to make sampling efﬁcient, running the Markov chain till convergence can still be a costly operation due to the generally large number of data points and high dimensionality of posterior samples. [sent-79, score-0.317]
</p><p>43 To further reduce computational costs, we developed a learning strategy which makes use of persistent Markov chains and only requires a few sampling steps in every iteration. [sent-80, score-0.294]
</p><p>44 Second, if updating the Markov chain has only a small effect on the posterior samples z, also the distribution of the complete data (x, z) will change very little. [sent-85, score-0.273]
</p><p>45 This causes an inefﬁcient Markov chain to automatically slow down the learning process, so that the posterior samples will always be close to the stationary distribution. [sent-87, score-0.207]
</p><p>46 Interestingly, improving the lower bound F with respect to q can be accomplished by driving the Markov chain with our Gibbs sampler or some other transition operator [26]. [sent-93, score-0.292]
</p><p>47 5 0  0  Time [s]  Image model  Toy model  5  10  15  Time [s]  0  20  40  60  80  Time [s]  Figure 2: A: The average energy of posterior samples for different sampling methods after deterministic initialization. [sent-101, score-0.284]
</p><p>48 This stands in contrast to other contexts where persistent Markov chains have been successful but training can diverge [10]. [sent-110, score-0.184]
</p><p>49 4  Results  We trained several linear models on log-transformed, centered and symmetrically whitened image patches extracted from van Hateren’s dataset of natural images [33]. [sent-113, score-0.274]
</p><p>50 We explicitly modeled the DC component of the whitened image patches using a mixture of Gaussians and constrained the remaining components of the linear basis to be orthogonal to the DC component. [sent-114, score-0.321]
</p><p>51 For faster convergence, we initialized the linear basis with the sparse coding algorithm of Olshausen and Field [19], which corresponds to learning with MAP inference and ﬁxed marginal source distributions. [sent-115, score-0.382]
</p><p>52 After initialization, we optimized the basis using L-BFGS [3] during each M-step and updated the representation of the posterior using 2 steps of Gibbs sampling in each E-step. [sent-116, score-0.295]
</p><p>53 To represent the source marginals, we used ﬁnite GSMs (Equation 8) with 10 precisions ϑij each and equal prior weights, that is, πij = 0. [sent-117, score-0.309]
</p><p>54 The source marginals were initialized by ﬁtting them to samples from the Laplace distribution and later optimized using 10 iterations of standard EM at the beginning of each M-step. [sent-119, score-0.271]
</p><p>55 1  Performance of the blocked Gibbs sampler  We compared the sampling performance of our Gibbs sampler to MALA sampling—as used by Chen and Wu [4]—as well as HMC sampling [9], which is a generalization of MALA. [sent-121, score-0.84]
</p><p>56 The HMC sampler has two parameters: a step width and a number of so called leap frog steps. [sent-122, score-0.193]
</p><p>57 25 0  64  128  192  256  Basis coefﬁcient, i  Figure 3: We trained models with up to four times overcomplete representations using either Laplace marginals or GSM marginals. [sent-128, score-0.604]
</p><p>58 A four times overcomplete basis set is shown in the center. [sent-129, score-0.557]
</p><p>59 Basis vectors were normalized so that the corresponding source distributions had unit variance. [sent-130, score-0.24]
</p><p>60 The right panel shows log-densities of the source distributions corresponding to basis vectors inside the dashed rectangle. [sent-134, score-0.317]
</p><p>61 The algorithms were tested on one toy model and one two times overcomplete model trained on 8 × 8 image patches. [sent-136, score-0.602]
</p><p>62 Figure 2 shows trace plots and autocorrelation functions for the different sampling methods. [sent-141, score-0.236]
</p><p>63 Autocorrelation functions were estimated from single Markov chain runs of equal duration for each sampler and data point. [sent-143, score-0.258]
</p><p>64 All Markov chains were initialized using 100 burn-in steps of Gibbs sampling, independent of the sampler used to generate the autocorrelation functions. [sent-144, score-0.377]
</p><p>65 Still, even the best HMC sampler produced more correlated samples than the blocked Gibbs sampler. [sent-148, score-0.461]
</p><p>66 While the best HMC sampler reached an autocorrelation of 0. [sent-149, score-0.319]
</p><p>67 05 after about 64 seconds, it took only about 26 seconds with the blocked Gibbs sampler (right-hand side of Figure 2B). [sent-150, score-0.427]
</p><p>68 [2] found that even for very sparse choices of the Student-t prior, the representations learned by the linear model are barely overcomplete if a variational approximation to the posterior is used. [sent-155, score-0.784]
</p><p>69 Consistent with the study of Seeger [28], if we ﬁx the source distributions to be Laplacian, our algorithm learns representations which are only slightly overcomplete (Figure 3). [sent-160, score-0.778]
</p><p>70 However, much more overcomplete representations were obtained when the source distributions were learned from the data. [sent-161, score-0.778]
</p><p>71 This is in line with the results of Olshausen and Millman [21], who used mixtures of two 6  Log-likelihood ± SEM [bit/pixel]  16 × 16 image patches  8 × 8 image patches 1. [sent-162, score-0.319]
</p><p>72 While using overcomplete representations (OLM) yields substantial improvements over the complete linear model (LM), it still cannot compete with other models of natural image patches. [sent-189, score-0.757]
</p><p>73 and three Gaussians as source distributions and obtained two times overcomplete representations for 8 × 8 image patches. [sent-193, score-0.856]
</p><p>74 Figure 3 suggests that with GSMs as source distributions, the model can make use of three and up to four times overcomplete representations. [sent-194, score-0.651]
</p><p>75 Our quantitative evaluations conﬁrmed a substantial improvement of the two-times overcomplete model over the complete model. [sent-195, score-0.546]
</p><p>76 The source distributions discovered by our algorithm were extremely sparse and resembled spikeand-slab distributions, generating mostly values close to zero with the occasional outlier. [sent-197, score-0.294]
</p><p>77 3  Model comparison  To compare the performance of the overcomplete linear model to the complete linear model and other image models, we would like to evaluate the overcomplete linear models’ log-likelihood on a test set of images. [sent-200, score-1.251]
</p><p>78 q(z | x)  (13)  We can then estimate the above integral by sampling states zn from q(z | x) and averaging over p(x, zn )/q(zn | x), a technique called importance sampling. [sent-203, score-0.205]
</p><p>79 A procedure for constructing distributions q(z | x) from transition operators such as our Gibbs sampling operator is annealed importance sampling (AIS) [16]. [sent-205, score-0.404]
</p><p>80 Here, we used our Gibbs sampler and constructed intermediate distributions by interpolating between a Gaussian distribution and the overcomplete linear model. [sent-208, score-0.791]
</p><p>81 For the four-times overcomplete model, we used 300 intermediate distributions and 300 importance samples to estimate the density of each data point. [sent-209, score-0.612]
</p><p>82 We ﬁnd that the overcomplete linear model is still worse than, for example, a single multivariate GSM with separately modeled DC component (Figure 4; see also Supplementary Section 3). [sent-210, score-0.556]
</p><p>83 7  An alternative overcomplete generalization of the complete linear model is the family of products of experts (PoE) [13]. [sent-211, score-0.639]
</p><p>84 Instead of introducing additional source variables, a PoE can have more factors than visible units, s = W x,  p(x) ∝  fi (si ),  (14)  i  where W ∈ RN ×M and each factor is also called an expert. [sent-212, score-0.284]
</p><p>85 In contrast to the overcomplete linear model, the prior over hidden sources s here is in general not factorial. [sent-214, score-0.641]
</p><p>86 A popular choice of PoE in the context of natural images is the product of Student-t (PoT) distributions, in which experts have the form fi (si ) = (1 + s2 )−αi [35]. [sent-215, score-0.181]
</p><p>87 We ﬁnd that the PoT is better suited for modeling the statistics of natural images and takes better advantage of overcomplete representations (Figure 4). [sent-218, score-0.595]
</p><p>88 While both the estimator for the PoT and the estimator for the overcomplete linear model are consistent, the former tends to overestimate and the latter tends to underestimate the average loglikelihood. [sent-219, score-0.529]
</p><p>89 5  Discussion  We have shown how to efﬁciently perform inference, training and evaluation in the sparse overcomplete linear model. [sent-221, score-0.583]
</p><p>90 While general purpose sampling algorithms such as MALA or HMC have the advantage of being more widely applicable, we showed that blocked Gibbs sampling can be much faster when the source distributions are sparse, as for natural images. [sent-222, score-0.72]
</p><p>91 Another advantage of our sampler is that it is parameter free. [sent-223, score-0.193]
</p><p>92 Choosing suboptimal parameters for the HMC sampler can lead to extremely poor performance. [sent-224, score-0.193]
</p><p>93 We showed that by training a model with a persistent variant of Monte Carlo EM, even the number of sampling steps performed in each E-step becomes much less crucial for the success of training. [sent-227, score-0.265]
</p><p>94 Optimizing and evaluating the likelihood of overcomplete linear models is a challenging problem. [sent-228, score-0.529]
</p><p>95 To our knowledge, our study is the ﬁrst to show a clear advantage of the overcomplete linear model over its complete counterpart on natural images. [sent-229, score-0.621]
</p><p>96 At the same time, we demonstrated that with the assumptions of a factorial prior, the overcomplete linear model underperforms other generalizations of the complete linear model. [sent-230, score-0.739]
</p><p>97 Code for training and evaluating overcomplete linear models is available at http://bethgelab. [sent-233, score-0.529]
</p><p>98 A variational method for learning sparse and overcomplete representations. [sent-305, score-0.534]
</p><p>99 Sparse coding with an overcomplete basis set: A strategy employed by V1? [sent-356, score-0.557]
</p><p>100 Hamiltonian annealed importance sampling for partition function estimation, 2012. [sent-409, score-0.191]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('overcomplete', 0.48), ('hmc', 0.234), ('blocked', 0.234), ('gibbs', 0.228), ('nullspace', 0.223), ('sampler', 0.193), ('gsm', 0.182), ('mala', 0.175), ('source', 0.171), ('pot', 0.149), ('precisions', 0.138), ('xz', 0.131), ('persistent', 0.126), ('autocorrelation', 0.126), ('sampling', 0.11), ('posterior', 0.108), ('laplace', 0.103), ('gsms', 0.099), ('poe', 0.099), ('olshausen', 0.096), ('overcompleteness', 0.095), ('fi', 0.08), ('sources', 0.078), ('image', 0.078), ('basis', 0.077), ('papandreou', 0.074), ('em', 0.074), ('si', 0.071), ('distributions', 0.069), ('dkl', 0.066), ('complete', 0.066), ('marginals', 0.066), ('chain', 0.065), ('markov', 0.062), ('ais', 0.061), ('patches', 0.059), ('xx', 0.059), ('representations', 0.058), ('chains', 0.058), ('sparse', 0.054), ('annealed', 0.052), ('jascha', 0.05), ('leptokurtotic', 0.05), ('millman', 0.05), ('olm', 0.05), ('reichardt', 0.05), ('underperforms', 0.05), ('linear', 0.049), ('gaussian', 0.047), ('ik', 0.046), ('mixtures', 0.045), ('factorial', 0.045), ('experts', 0.044), ('toy', 0.044), ('sem', 0.044), ('andrews', 0.044), ('integrative', 0.044), ('werner', 0.044), ('dc', 0.043), ('mcmc', 0.041), ('noiseless', 0.041), ('ij', 0.039), ('heess', 0.038), ('lucas', 0.038), ('hateren', 0.038), ('langevin', 0.038), ('gi', 0.037), ('gaussians', 0.036), ('wu', 0.035), ('barely', 0.035), ('hamiltonian', 0.035), ('transition', 0.034), ('monte', 0.034), ('ow', 0.034), ('convergence', 0.034), ('hidden', 0.034), ('samples', 0.034), ('zn', 0.033), ('crude', 0.033), ('zz', 0.033), ('visible', 0.033), ('lm', 0.032), ('berkes', 0.032), ('energy', 0.032), ('images', 0.031), ('whitened', 0.031), ('inference', 0.031), ('alternately', 0.03), ('seeger', 0.029), ('equation', 0.029), ('importance', 0.029), ('variant', 0.029), ('carlo', 0.029), ('matthias', 0.029), ('tails', 0.028), ('chen', 0.028), ('schmidt', 0.027), ('maxima', 0.027), ('modeled', 0.027), ('natural', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="349-tfidf-1" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>Author: Lucas Theis, Jascha Sohl-dickstein, Matthias Bethge</p><p>Abstract: We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. 1</p><p>2 0.22229247 <a title="349-tfidf-2" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>Author: Yichuan Zhang, Zoubin Ghahramani, Amos J. Storkey, Charles A. Sutton</p><p>Abstract: Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difﬁcult discrete systems. We demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems. 1</p><p>3 0.13628662 <a title="349-tfidf-3" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>Author: Deepak Venugopal, Vibhav Gogate</p><p>Abstract: First-order probabilistic models combine the power of ﬁrst-order logic, the de facto tool for handling relational structure, with probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the accuracy and scalability of existing graphical models’ inference algorithms by exploiting symmetry in the ﬁrst-order representation. In this paper, we consider blocked Gibbs sampling, an advanced MCMC scheme, and lift it to the ﬁrst-order level. We propose to achieve this by partitioning the ﬁrst-order atoms in the model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing the clusters and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy, scalability and convergence.</p><p>4 0.12733965 <a title="349-tfidf-4" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable’s marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufﬁciently certain about any individual decision. Whenever this is the case, we dynamically prune the variables we are conﬁdent about from the underlying factor graph. Consequently, at any time only samples of variables whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classiﬁcation and image inpainting, show that adaptive sampling can drastically accelerate MMP without sacriﬁcing prediction accuracy. 1</p><p>5 0.12039321 <a title="349-tfidf-5" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>Author: Daniel Zoran, Yair Weiss</p><p>Abstract: Simple Gaussian Mixture Models (GMMs) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images. Here we provide an in depth analysis of this simple yet rich model. We show that such a GMM model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality. We provide an analysis of what such a model learns from natural images as a function of number of mixture components including covariance structure, contrast variation and intricate structures such as textures, boundaries and more. Finally, we show that the salient properties of the GMM learned from natural images can be derived from a simplified Dead Leaves model which explicitly models occlusion, explaining its surprising success relative to other models. 1 GMMs and natural image statistics models Many models for the statistics of natural image patches have been suggested in recent years. Finding good models for natural images is important to many different research areas - computer vision, biological vision and neuroscience among others. Recently, there has been a growing interest in comparing different aspects of models for natural images such as log-likelihood and multi-information reduction performance, and much progress has been achieved [1,2, 3,4,5, 6]. Out of these results there is one which is particularly interesting: simple, unconstrained Gaussian Mixture Models (GMMs) with a relatively small number of mixture components learned from image patches are extraordinarily good in modeling image statistics [6, 4]. This is a surprising result due to the simplicity of GMMs and their ubiquity. Another surprising aspect of this result is that many of the current models may be thought of as GMMs with an exponential or infinite number of components, having different constraints on the covariance structure of the mixture components. In this work we study the nature of GMMs learned from natural image patches. We start with a thorough comparison to some popular and cutting edge image models. We show that indeed, GMMs are excellent performers in modeling natural image patches. We then analyze what properties of natural images these GMMs capture, their dependence on the number of components in the mixture and their relation to the structure of the world around us. Finally, we show that the learned GMM suggests a strong connection between natural image statistics and a simple variant of the dead leaves model [7, 8] , explicitly modeling occlusions and explaining some of the success of GMMs in modeling natural images. 1 3.5 .,...- ••.......-.-.. -..---'-. 1 ~~6\8161·· -.. .-.. --...--.-- ---..-.- -. --------------MII+··+ilIl ..... .. . . ~ '[25 . . . ---- ] B'II 1_ -- ~2 ;t:: fI 1 - --- ,---- ._.. : 61.5 ..... '</p><p>6 0.11385672 <a title="349-tfidf-6" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>7 0.11378959 <a title="349-tfidf-7" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>8 0.11062238 <a title="349-tfidf-8" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>9 0.10468809 <a title="349-tfidf-9" href="./nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">365 nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<p>10 0.10404634 <a title="349-tfidf-10" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>11 0.093728036 <a title="349-tfidf-11" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>12 0.092792422 <a title="349-tfidf-12" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>13 0.092379406 <a title="349-tfidf-13" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>14 0.078245655 <a title="349-tfidf-14" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>15 0.077799082 <a title="349-tfidf-15" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>16 0.077349037 <a title="349-tfidf-16" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>17 0.073125459 <a title="349-tfidf-17" href="./nips-2012-Affine_Independent_Variational_Inference.html">37 nips-2012-Affine Independent Variational Inference</a></p>
<p>18 0.072011583 <a title="349-tfidf-18" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>19 0.071068853 <a title="349-tfidf-19" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>20 0.068129092 <a title="349-tfidf-20" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.045), (2, -0.055), (3, 0.024), (4, -0.151), (5, -0.033), (6, -0.009), (7, -0.04), (8, 0.072), (9, -0.048), (10, -0.031), (11, -0.048), (12, -0.039), (13, -0.018), (14, -0.055), (15, -0.14), (16, -0.025), (17, -0.162), (18, 0.064), (19, -0.079), (20, 0.106), (21, 0.003), (22, -0.083), (23, 0.069), (24, -0.079), (25, 0.041), (26, 0.001), (27, 0.014), (28, -0.023), (29, -0.003), (30, -0.085), (31, 0.027), (32, -0.017), (33, -0.02), (34, -0.024), (35, -0.068), (36, -0.061), (37, -0.043), (38, 0.045), (39, -0.1), (40, 0.068), (41, -0.053), (42, -0.03), (43, -0.087), (44, -0.021), (45, 0.109), (46, 0.033), (47, 0.06), (48, -0.046), (49, -0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94835043 <a title="349-lsi-1" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>Author: Lucas Theis, Jascha Sohl-dickstein, Matthias Bethge</p><p>Abstract: We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. 1</p><p>2 0.81729627 <a title="349-lsi-2" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>Author: Yichuan Zhang, Zoubin Ghahramani, Amos J. Storkey, Charles A. Sutton</p><p>Abstract: Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difﬁcult discrete systems. We demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems. 1</p><p>3 0.68717676 <a title="349-lsi-3" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable’s marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufﬁciently certain about any individual decision. Whenever this is the case, we dynamically prune the variables we are conﬁdent about from the underlying factor graph. Consequently, at any time only samples of variables whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classiﬁcation and image inpainting, show that adaptive sampling can drastically accelerate MMP without sacriﬁcing prediction accuracy. 1</p><p>4 0.64752316 <a title="349-lsi-4" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>Author: Deepak Venugopal, Vibhav Gogate</p><p>Abstract: First-order probabilistic models combine the power of ﬁrst-order logic, the de facto tool for handling relational structure, with probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the accuracy and scalability of existing graphical models’ inference algorithms by exploiting symmetry in the ﬁrst-order representation. In this paper, we consider blocked Gibbs sampling, an advanced MCMC scheme, and lift it to the ﬁrst-order level. We propose to achieve this by partitioning the ﬁrst-order atoms in the model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing the clusters and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy, scalability and convergence.</p><p>5 0.64595103 <a title="349-lsi-5" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>Author: Maksims Volkovs, Richard S. Zemel</p><p>Abstract: Bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision. Exact inference in realworld applications of these problems is intractable, making efﬁcient approximation methods essential for learning and inference. In this paper we propose a novel sequential matching sampler based on a generalization of the PlackettLuce model, which can effectively make large moves in the space of matchings. This allows the sampler to match the difﬁcult target distributions common in these problems: highly multimodal distributions with well separated modes. We present experimental results with bipartite matching problems—ranking and image correspondence—which show that the sequential matching sampler efﬁciently approximates the target distribution, signiﬁcantly outperforming other sampling approaches. 1</p><p>6 0.64424503 <a title="349-lsi-6" href="./nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">365 nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<p>7 0.61612672 <a title="349-lsi-7" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>8 0.61575526 <a title="349-lsi-8" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>9 0.61410302 <a title="349-lsi-9" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>10 0.5943656 <a title="349-lsi-10" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>11 0.5912739 <a title="349-lsi-11" href="./nips-2012-Slice_sampling_normalized_kernel-weighted_completely_random_measure_mixture_models.html">315 nips-2012-Slice sampling normalized kernel-weighted completely random measure mixture models</a></p>
<p>12 0.58875185 <a title="349-lsi-12" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>13 0.56298065 <a title="349-lsi-13" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>14 0.55871999 <a title="349-lsi-14" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>15 0.53305441 <a title="349-lsi-15" href="./nips-2012-A_nonparametric_variable_clustering_model.html">26 nips-2012-A nonparametric variable clustering model</a></p>
<p>16 0.5265066 <a title="349-lsi-16" href="./nips-2012-Nonparanormal_Belief_Propagation_%28NPNBP%29.html">248 nips-2012-Nonparanormal Belief Propagation (NPNBP)</a></p>
<p>17 0.51716781 <a title="349-lsi-17" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>18 0.51176596 <a title="349-lsi-18" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>19 0.50716603 <a title="349-lsi-19" href="./nips-2012-The_Coloured_Noise_Expansion_and_Parameter_Estimation_of_Diffusion_Processes.html">336 nips-2012-The Coloured Noise Expansion and Parameter Estimation of Diffusion Processes</a></p>
<p>20 0.49625444 <a title="349-lsi-20" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (17, 0.01), (21, 0.022), (38, 0.098), (39, 0.013), (42, 0.015), (54, 0.017), (55, 0.016), (74, 0.032), (76, 0.143), (80, 0.101), (92, 0.434)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9174484 <a title="349-lda-1" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>Author: Richard Socher, Brody Huval, Bharath Bath, Christopher D. Manning, Andrew Y. Ng</p><p>Abstract: Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, ﬁxed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efﬁcient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. 1</p><p>2 0.90353876 <a title="349-lda-2" href="./nips-2012-Approximating_Concavely_Parameterized_Optimization_Problems.html">44 nips-2012-Approximating Concavely Parameterized Optimization Problems</a></p>
<p>Author: Joachim Giesen, Jens Mueller, Soeren Laue, Sascha Swiercy</p><p>Abstract: We consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the √ parameter can always be approximated with accuracy ε > 0 by a set of size O(1/ ε). A √ lower bound of size Ω(1/ ε) shows that the upper bound is tight up to a constant factor. We also devise an algorithm that calls a step-size oracle and computes an √ approximate path of size O(1/ ε). Finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-deﬁnite program for matrix completion. 1</p><p>3 0.90059495 <a title="349-lda-3" href="./nips-2012-Continuous_Relaxations_for_Discrete_Hamiltonian_Monte_Carlo.html">82 nips-2012-Continuous Relaxations for Discrete Hamiltonian Monte Carlo</a></p>
<p>Author: Yichuan Zhang, Zoubin Ghahramani, Amos J. Storkey, Charles A. Sutton</p><p>Abstract: Continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference. Here we show that a general form of the Gaussian Integral Trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems. The continuous representation allows the use of gradient-based Hamiltonian Monte Carlo for inference, results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difﬁcult discrete systems. We demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems. 1</p><p>4 0.89620245 <a title="349-lda-4" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>Author: Yunchao Gong, Sanjiv Kumar, Vishal Verma, Svetlana Lazebnik</p><p>Abstract: This paper focuses on the problem of learning binary codes for efﬁcient retrieval of high-dimensional non-negative data that arises in vision and text applications where counts or frequencies are used as features. The similarity of such feature vectors is commonly measured using the cosine of the angle between them. In this work, we introduce a novel angular quantization-based binary coding (AQBC) technique for such data and analyze its properties. In its most basic form, AQBC works by mapping each non-negative feature vector onto the vertex of the binary hypercube with which it has the smallest angle. Even though the number of vertices (quantization landmarks) in this scheme grows exponentially with data dimensionality d, we propose a method for mapping feature vectors to their smallest-angle binary vertices that scales as O(d log d). Further, we propose a method for learning a linear transformation of the data to minimize the quantization error, and show that it results in improved binary codes. Experiments on image and text datasets show that the proposed AQBC method outperforms the state of the art. 1</p><p>same-paper 5 0.86975896 <a title="349-lda-5" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>Author: Lucas Theis, Jascha Sohl-dickstein, Matthias Bethge</p><p>Abstract: We present a new learning strategy based on an efﬁcient blocked Gibbs sampler for sparse overcomplete linear models. Particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations. Our Gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters. Using the Gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data. When applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions. We evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model. In contrast to previous claims, we ﬁnd that overcomplete representations lead to signiﬁcant improvements, but that the overcomplete linear model still underperforms other models. 1</p><p>6 0.8494221 <a title="349-lda-6" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>7 0.67851359 <a title="349-lda-7" href="./nips-2012-Super-Bit_Locality-Sensitive_Hashing.html">329 nips-2012-Super-Bit Locality-Sensitive Hashing</a></p>
<p>8 0.67366362 <a title="349-lda-8" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>9 0.65887535 <a title="349-lda-9" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>10 0.63115454 <a title="349-lda-10" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>11 0.62971419 <a title="349-lda-11" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>12 0.62942839 <a title="349-lda-12" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>13 0.62809825 <a title="349-lda-13" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>14 0.62493455 <a title="349-lda-14" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>15 0.62261176 <a title="349-lda-15" href="./nips-2012-Isotropic_Hashing.html">163 nips-2012-Isotropic Hashing</a></p>
<p>16 0.61837596 <a title="349-lda-16" href="./nips-2012-Delay_Compensation_with_Dynamical_Synapses.html">94 nips-2012-Delay Compensation with Dynamical Synapses</a></p>
<p>17 0.61811864 <a title="349-lda-17" href="./nips-2012-A_mechanistic_model_of_early_sensory_processing_based_on_subtracting_sparse_representations.html">24 nips-2012-A mechanistic model of early sensory processing based on subtracting sparse representations</a></p>
<p>18 0.61149466 <a title="349-lda-18" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>19 0.61024433 <a title="349-lda-19" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>20 0.60973865 <a title="349-lda-20" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
