<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>304 nips-2012-Selecting Diverse Features via Spectral Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-304" href="#">nips2012-304</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>304 nips-2012-Selecting Diverse Features via Spectral Regularization</h1>
<br/><p>Source: <a title="nips-2012-304-pdf" href="http://papers.nips.cc/paper/4689-selecting-diverse-features-via-spectral-regularization.pdf">pdf</a></p><p>Author: Abhimanyu Das, Anirban Dasgupta, Ravi Kumar</p><p>Abstract: We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability, robustness to noise, etc. We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized by efﬁcient greedy and local search algorithms, with provable guarantees. We compare our algorithms to traditional greedy and 1 -regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations. 1</p><p>Reference: <a title="nips-2012-304-reference" href="../nips2012_reference/nips-2012-Selecting_Diverse_Features_via_Spectral_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. [sent-6, score-0.622]
</p><p>2 We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. [sent-8, score-1.008]
</p><p>3 These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized by efﬁcient greedy and local search algorithms, with provable guarantees. [sent-9, score-0.625]
</p><p>4 We compare our algorithms to traditional greedy and 1 -regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations. [sent-10, score-0.511]
</p><p>5 In the ﬁrst, features are greedily selected one by one up to the pre-speciﬁed budget k; the Forward or Backward greedy methods[19] fall into this type. [sent-18, score-0.32]
</p><p>6 In the second, the feature selection process is intimately coupled with the regression objective itself by adding a (usually convex) regularizer. [sent-19, score-0.239]
</p><p>7 For example, the Lasso [20] uses the 1 -norm of the coefﬁcients as a regularizer to promote sparsity. [sent-20, score-0.218]
</p><p>8 In this work we consider the feature selection problem of choosing the best set of features for predicting a speciﬁed target, coupled with the desire to choose as “diverse” features as possible; our goal will be to construct a regularizer that can capture diversity. [sent-21, score-0.543]
</p><p>9 Secondly, as we show, the right notion of diversity can also make the feature selection task resistant to noise in the data. [sent-26, score-0.462]
</p><p>10 1  Unfortunately, the traditional greedy and 1 -relaxation approaches to feature-selection do not explictly address feature diversity1 . [sent-32, score-0.258]
</p><p>11 In this paper, we address this problem of diverse feature selection using an approach that falls between that of greedy methods and convex-regularization methods. [sent-33, score-0.502]
</p><p>12 In particular, we construct regularizers that capture a notion of diversity — unlike regularizers such as Lasso, our regularizers are set functions as opposed to functions of the regression coefﬁcient vector. [sent-34, score-0.972]
</p><p>13 We then design provable approximation algorithms for such objectives using a combination of greedy and local search techniques. [sent-36, score-0.31]
</p><p>14 By deﬁning diversity to be a carefully chosen function of the spectrum of the chosen features, we tap into notions of submodularity and consequently into the rich literature for maximizing submodular functions [5, 9, 14]. [sent-38, score-0.878]
</p><p>15 Our contributions are as follows: (i) We formulate an optimization problem for diverse feature selection and construct a family of submodular spectral regularizers that capture diversity notions. [sent-39, score-1.196]
</p><p>16 (ii) We use a novel approach of combining the diversity regularizers with the optimization objective to obtain (approximately) submodular maximization problems, and optimize them using greedy and local search algorithms with provable guarantees. [sent-40, score-1.087]
</p><p>17 2  Related work  Feature selection and the closely related problems of sparse approximation/recovery have been extensively studied using two broad classes of methods: greedy [5, 19, 21, 11, 24] and convex relaxation [20, 25, 3, 22, 8]. [sent-42, score-0.271]
</p><p>18 None of these methods, however, takes feature diversity into the account during selection. [sent-43, score-0.333]
</p><p>19 However, they do not incorporate any notion of feature diversity; they also require monotonicity, which does not hold for several regularizers we construct. [sent-45, score-0.311]
</p><p>20 [12], who address the unstable behavior of Lasso in the presence of correlated features and propose adding a trace norm regularizer to the error objective. [sent-47, score-0.359]
</p><p>21 Previous work on diverse feature selection includes greedy heuristics for trading-off informationtheoretic feature relevance and feature redundancy criteria when selecting features [7, 23]. [sent-50, score-0.794]
</p><p>22 There has been some work on selecting a diverse set of features to maximize the mutual information or the entropy of a set of variables [13, 17]. [sent-52, score-0.342]
</p><p>23 But, the problem deﬁnition in these works does not specify a target prediction vector or variable; the goal instead is to select diverse features regardless of whether the features are relevant for predicting a particular target variable. [sent-53, score-0.423]
</p><p>24 On the other hand, our work requires us to simultaneously optimize for both feature selection and diversity objectives. [sent-54, score-0.418]
</p><p>25 For a n-dimensional Gaussian random vector v with covariance matrix C, we use H(v) = 1 n 2 log((2πe) det(C)) to denote the differential entropy of v. [sent-71, score-0.184]
</p><p>26 Note that diversity is not a uniquely-deﬁned notion, however, we call a regularizer f to be diversitypromoting if the following two conditions are satisﬁed: for a ﬁxed k, f (S) is maximized when S is an orthogonal set of vectors and is minimized when S has the lowest rank, where |S| ≤ k. [sent-75, score-0.523]
</p><p>27 Das and Kempe [5] introduced the notion of submodularity ratio for a general set function to capture how close is the function to being submodular. [sent-84, score-0.274]
</p><p>28 submodularity ratio of f with respect to a set U and a parameter k ≥ x∈S f (L ∪ {x}) − f (L) . [sent-86, score-0.23]
</p><p>29 In particular, [5] deﬁnes the submodularity ratio for the R2 function and relates it to the smallest eigenvalue of the covariance matrix of the data. [sent-88, score-0.284]
</p><p>30 They also show that, in practice, the submodularity ratio for R2 is often quite close to 1, and hence a greedy algorithm is a good approximation to maximizing R2 subject to a cardinality constraint. [sent-89, score-0.507]
</p><p>31 1  Robustness to perturbations  As mentioned earlier, in addition to providing better interpretability, another beneﬁt of diverse feature selection is robustness to feature and label perturbations. [sent-93, score-0.454]
</p><p>32 Given a selected subset S, we now obtain a connection between the robustness of the estimated regression coefﬁcients and the spectrum of CS , in the presence of noise. [sent-94, score-0.253]
</p><p>33 We show the following perturbation result relating the differential entropy of the perturbation error in the regression coefﬁcients to the spectrum of CS . [sent-107, score-0.456]
</p><p>34 k  Thus the perturbation error entropy is minimized by maximizing i=1 log(λi (CS )), which motivates the smoothed differential-entropy regularizer used in Section 5. [sent-114, score-0.5]
</p><p>35 We can also show (supplementary material) that the two-norm of the perturbation error in the regression coefﬁcients is also related to the spectrum of CS : the expected noise in the regression −1 1 coefﬁcients depends on the sum of the eigenvalues of CS . [sent-116, score-0.367]
</p><p>36 This suggests the use of − i λi (CS ) as a diversity-promoting regularizer in Deﬁnition 1. [sent-117, score-0.218]
</p><p>37 Unfortunately, this regularization function is not submodular and is thus hard to use directly. [sent-118, score-0.371]
</p><p>38 3, there are other related spectral functions that are indeed submodular and can thus be used as efﬁcient regularizers. [sent-121, score-0.424]
</p><p>39 4  Algorithms  In this section we present a greedy and local-search based (GLS) approximation algorithm for solving (1) when f (S) is a non-negative (but not necessarily monotone) submodular function (w. [sent-122, score-0.573]
</p><p>40 Next, we modify a local-search based algorithm for unconstrained submodular maximization to give an approximation of argmaxS g(S) (Theorem 7). [sent-129, score-0.449]
</p><p>41 The greedy Forward Regression (FR) algorithm is the following. [sent-131, score-0.186]
</p><p>42 Theorem 5 For any set T such that |T | ≤ k, the set S selected by the greedy FR algorithm satisﬁes γS,2k 2 g(S) = RZ (S) + f (S) ≥ (1 − e− 2 )g(S ∪ T ). [sent-139, score-0.236]
</p><p>43 n  We now present the greedy and local search (GLS) algorithm for solving (1) for any submodular, non-monotone, non-negative regularizer. [sent-161, score-0.21]
</p><p>44 Using the submodularity of f and the monotonicity of RZ (S), we obtain g(S1 ∪C)+g(S2 ∪ 2 2 2 C ) = RZ (S1 ∪ C) + RZ (S2 ∪ C ) + f (S1 ∪ C) + f (S2 ∪ C ) ≥ RZ (C) + f (S1 ∪ S2 ∪ C) + f (C ). [sent-172, score-0.243]
</p><p>45 Theorem 9 If f is non-negative and submodular and < gives a  γ˜ S,2k 1−e− 2 γ˜ S,2k 2+(1−e− 2 )(4+4  ≥ /n)  γ˜ S,2k 1−e− 2  7  n ˜ 4 , the set S selected by the GLS algorithm  approximation for solving argmaxS:|S|≤k g(S). [sent-175, score-0.437]
</p><p>46 2 When f (S) is a monotone, non-negative, submodular function, the problem becomes much easier due to the proposition below that follows directly from the deﬁnition of the submodularity ratio. [sent-186, score-0.53]
</p><p>47 2 Proposition 10 For any submodular set function f (S), the function g(S) = RZ (S)+f (S) satisﬁes γU,k (g) ≥ γU,k (R2 ) for any U and k. [sent-187, score-0.337]
</p><p>48 Thus, since g(S) is monotone and approximately submodular, we can directly apply [4, Theorem 3] ˜ to show that the greedy FR algorithm gives a (1 − e−γS,k (f ) )-approximation. [sent-188, score-0.327]
</p><p>49 5  Spectral regularizers for diversity  In this section we propose a number of diversity-promoting regularizers for the feature selection problem. [sent-189, score-0.808]
</p><p>50 We then prove that our algorithms in the previous section can obtain provable guarantees for each of the corresponding regularized feature selection problems. [sent-190, score-0.229]
</p><p>51 Most of our analysis requires the notion of operator antitone function [1] and its connection with submodularity that was recently obtained by Friedland and Gaubert [10]. [sent-191, score-0.379]
</p><p>52 If the derivative of f is operator antitone on the interior of Γ, then for every n × n Hermitian matrix C with n spectrum in Γ, the set function (from 2n −→ R) tr(f (S)) = i=1 f (λi (CS )) is submodular. [sent-194, score-0.244]
</p><p>53 1  Smoothed differential entropy regularizer  For any set S with the corresponding covariance matrix CS , we deﬁne the smoothed differential |S| entropy regularizer as fde (S) = i=1 log2 (δ + λi (CS )) − 3k log2 δ, where δ > 0 is the smoothing constant. [sent-199, score-1.032]
</p><p>54 This is a smoothed version of the log-determinant function fld (S) = log(det(CS )) = |S| i=1 log(λi (CS )), that is also normalized by an additive term of 3k log2 δ in order to make the regularizer non-negative 3 . [sent-200, score-0.367]
</p><p>55 As shown in Lemma 4, this regularizer also helps improve the robustness of the regression model to noise since maximizing fld (S) minimizes the entropy of the perturbation error. [sent-201, score-0.627]
</p><p>56 For a multivariate Gaussian distribution, fld (S) also equivalent (up to an additive |S| factor) to the differential entropy of S. [sent-202, score-0.218]
</p><p>57 However, fld (S) is undeﬁned if S is rank-deﬁcient and might also take negative values; the smoothed version fde (S) overcomes these issues. [sent-203, score-0.37]
</p><p>58 It is also easy to show that fde (S) is a diversitypromoting regularizer. [sent-204, score-0.265]
</p><p>59 We now show that the GLS algorithm to solve (1) with f (S) = fde (S) gives a constant-factor approximation algorithm. [sent-205, score-0.271]
</p><p>60 −  γ˜ S,2k  ˜ Theorem 14 The set S selected by the GLS algorithm gives a 1−e 7 2 multiplicative approximation guarantee for (1) using the smoothed differential entropy regularizer fde (S). [sent-206, score-0.793]
</p><p>61 We ﬁrst prove that fde (S) is non-negative and submodular. [sent-208, score-0.243]
</p><p>62 Since h(t) is the derivative of f (t), a straightforward application of Theorem 12 gives us that fde (S) is submodular. [sent-216, score-0.247]
</p><p>63 By Proposition 10, we obtain that g(S) is approximately submodular, with submodularity ratio at least γS,k (R2 ) . [sent-217, score-0.258]
</p><p>64 Notice that since fde (S) is not monotone in general [13], we cannot use Theorem 3. [sent-219, score-0.334]
</p><p>65 However, in the case when δ ≥ 1, a simple application of Lemma 13 shows that fde (S) becomes monotonically increasing and we can then use Theorem 3 to obtain a tighter approximation bound. [sent-220, score-0.298]
</p><p>66 2  Generalized rank regularizer  For any set S with covariance matrix CS , and constant α such that 0 ≤ α ≤ 1, we deﬁne the gen|S| eralized rank regularizer as fgr (S) = i=1 λi (CS )α . [sent-222, score-0.805]
</p><p>67 The rank function however, does not discriminate between a full-rank matrix and an orthogonal matrix, and hence we deﬁne fgr (S) as a generalization of the rank function. [sent-224, score-0.345]
</p><p>68 It is easy to show that fgr (S) is diversity-promoting. [sent-225, score-0.199]
</p><p>69 We prove that fgr (S) is also monotone and submodular, and hence obtain approximation guarantees for the greedy FR algorithm for (1) with f (S) = fgr (S). [sent-226, score-0.769]
</p><p>70 2 ˜ ˜ Theorem 15 The set S selected by the greedy FR algorithm gives a (1 − e−γS,k (R ) ) multiplicative approximation guarantee for (1) using the generalized rank regularizer fgr (S). [sent-227, score-0.824]
</p><p>71 3 we need this regularizer to be non-negative for sets of size up to 3k, because of the use of f (S1 ∪ S2 ∪ C) in the proof of Lemma 8  6  ˜ Proof. [sent-228, score-0.218]
</p><p>72 Thus, Theorem 12 gives us that fgr (S) is submodular. [sent-231, score-0.199]
</p><p>73 the derivative of f Hence, by applying Lemma 10, we obtain that g(S) is an approximately submodular function, with ˜ submodularity ratio at least γS,k (R2 ) . [sent-232, score-0.621]
</p><p>74 ˜ Thus, using Lemma 13, we get that fgr (S) and consequently g(S) is a monotonically increasing set function. [sent-234, score-0.226]
</p><p>75 3  Spectral variance regularizer  For a set S with covariance matrix CS , we deﬁne the spectral variance regularizer as |S| − i=1 (λi (CS ) − 1)2 . [sent-237, score-0.623]
</p><p>76 For non-negativity, we add a constant 9k 2 term4 to the regularizer and deﬁne fsv (S) = |S| 9k 2 − i=1 (λi (CS ) − 1)2 . [sent-239, score-0.284]
</p><p>77 As with fde (S), we can show (proof relegated to the supplementary material) that fsv (S) is submodular, but it is not monotonically increasing in general. [sent-240, score-0.314]
</p><p>78 γ˜ S,2k  − ˜ Theorem 16 The set S selected by the GLS algorithm gives a 1−e 7 2 tion guarantee for (1) using the spectral variance regularizer fsv (S). [sent-242, score-0.474]
</p><p>79 We compare our approach against two baselines: Lasso and greedy FR. [sent-244, score-0.186]
</p><p>80 We use two baselines: lasso and no-reg, the greedy FR with no regularizer. [sent-253, score-0.347]
</p><p>81 If S is the set of features selected, then the η −1 T unperturbed regression coefﬁcients are deﬁned as α = CS XS Z, and the perturbed coefﬁcients as −1 T α = CS XS Z . [sent-264, score-0.236]
</p><p>82 Here again, the perturbed regression coefﬁcients −1  T  are α = CS XS y where CS = (XS )T XS and the error is measured as α − α 2 . [sent-269, score-0.176]
</p><p>83 For clarity of presentation, we have only shown the results of greedy FR for monotone regularizers (ld and gr) and GLS for nonmonotone (ld-0. [sent-274, score-0.494]
</p><p>84 55 10  90  lasso no−reg logdet 20  30  Number of features selected  0. [sent-304, score-0.439]
</p><p>85 65  lasso no−reg spec−var 20  30  40  50  60  Number of features selected  70  80  90  10  lasso no−reg gen−rank 20  30  40  50  60  Number of features selected  Figure 1: All plots on mnist data. [sent-326, score-0.633]
</p><p>86 We chose the regularization constant (ν) to be the maximum subject to the condition that the R2 value for that solution should be greater than that obtained by the lasso solution with corresponding sparsity. [sent-339, score-0.195]
</p><p>87 This ensures we are not sacriﬁcing diversity for solution quality. [sent-340, score-0.261]
</p><p>88 As is obvious from the ﬁgure, the coefﬁcient vector obtained by lasso is very susceptible to perturbation, and the effect of perturbation increases with the number of features used by lasso. [sent-342, score-0.332]
</p><p>89 This indicates that as lasso starts incorporating more features, it does not ensure that the features are diverse enough so as to be robust to perturbation. [sent-343, score-0.404]
</p><p>90 Greedy with no regularization seems more stable than lasso but still shows an increasing trend. [sent-344, score-0.195]
</p><p>91 On the other hand, the errors obtained by perturbing is much less for any of the regularizers, and is only very mildly increasing with k: it does not seem to matter which regularizer we employ. [sent-345, score-0.282]
</p><p>92 In both cases, using any of the regularizers we are able to pick a set of features that are more robust to perturbation. [sent-347, score-0.279]
</p><p>93 Figures 1(c)- 1(f) show that our features are also more diverse than the ones obtained by both lasso and no-reg. [sent-348, score-0.404]
</p><p>94 Since there is no one deﬁnition of diversity, in each of the plots, we take one of the deﬁnitions of diversity value corresponding to the four regularizers we use. [sent-349, score-0.456]
</p><p>95 In order to be able to compare, the regularizer values for each k are normalized by the maximum value possible for that k. [sent-350, score-0.218]
</p><p>96 For each of the plots we show the values of the diversity value for solutions at different levels of sparsity. [sent-351, score-0.261]
</p><p>97 It is obvious that we get more diverse solutions than both lasso and no-reg. [sent-352, score-0.32]
</p><p>98 7  Conclusions  In this paper we proposed submodular spectral regularizers for diverse feature selection and obtained efﬁcient approximation algorithms using greedy and local search algorithms. [sent-354, score-1.195]
</p><p>99 Minimum redundancy feature selection from microarray gene expression data. [sent-396, score-0.219]
</p><p>100 Adaptive forward-backward greedy algorithm for sparse learning with linear models. [sent-494, score-0.186]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cs', 0.341), ('submodular', 0.337), ('diversity', 0.261), ('fde', 0.221), ('regularizer', 0.218), ('fgr', 0.199), ('regularizers', 0.195), ('submodularity', 0.193), ('greedy', 0.186), ('rz', 0.186), ('gls', 0.177), ('lasso', 0.161), ('diverse', 0.159), ('logdet', 0.144), ('fr', 0.118), ('monotone', 0.113), ('ls', 0.102), ('argmaxs', 0.097), ('gen', 0.097), ('reg', 0.096), ('antitone', 0.088), ('fld', 0.088), ('perturbation', 0.087), ('spectral', 0.087), ('selection', 0.085), ('features', 0.084), ('regression', 0.082), ('opt', 0.079), ('spec', 0.078), ('feature', 0.072), ('perturbed', 0.07), ('entropy', 0.069), ('fsv', 0.066), ('perturbing', 0.064), ('hermitian', 0.064), ('differential', 0.061), ('smoothed', 0.061), ('lemma', 0.061), ('das', 0.059), ('rank', 0.058), ('cients', 0.056), ('xs', 0.055), ('coef', 0.055), ('operator', 0.054), ('theorem', 0.051), ('provable', 0.05), ('monotonicity', 0.05), ('selected', 0.05), ('approximation', 0.05), ('interpretability', 0.049), ('target', 0.048), ('det', 0.046), ('eigenvalues', 0.046), ('spectrum', 0.046), ('notion', 0.044), ('clash', 0.044), ('cpam', 0.044), ('diversitypromoting', 0.044), ('mnist', 0.043), ('robustness', 0.042), ('maximizing', 0.041), ('kempe', 0.039), ('friedland', 0.039), ('grave', 0.039), ('bs', 0.037), ('ratio', 0.037), ('anirban', 0.036), ('redundancy', 0.034), ('regularization', 0.034), ('maximization', 0.034), ('correlated', 0.033), ('multiplicative', 0.033), ('subset', 0.033), ('perturb', 0.032), ('selecting', 0.03), ('matrix', 0.03), ('guarantee', 0.03), ('approximately', 0.028), ('xj', 0.028), ('mountain', 0.028), ('microarray', 0.028), ('unconstrained', 0.028), ('monotonically', 0.027), ('ld', 0.026), ('gr', 0.026), ('nition', 0.026), ('semide', 0.026), ('derivative', 0.026), ('sv', 0.026), ('material', 0.025), ('si', 0.025), ('perturbations', 0.024), ('error', 0.024), ('yahoo', 0.024), ('search', 0.024), ('covariance', 0.024), ('cov', 0.023), ('var', 0.023), ('variance', 0.023), ('prove', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="304-tfidf-1" href="./nips-2012-Selecting_Diverse_Features_via_Spectral_Regularization.html">304 nips-2012-Selecting Diverse Features via Spectral Regularization</a></p>
<p>Author: Abhimanyu Das, Anirban Dasgupta, Ravi Kumar</p><p>Abstract: We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability, robustness to noise, etc. We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized by efﬁcient greedy and local search algorithms, with provable guarantees. We compare our algorithms to traditional greedy and 1 -regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations. 1</p><p>2 0.22361772 <a title="304-tfidf-2" href="./nips-2012-Near-Optimal_MAP_Inference_for_Determinantal_Point_Processes.html">236 nips-2012-Near-Optimal MAP Inference for Determinantal Point Processes</a></p>
<p>Author: Jennifer Gillenwater, Alex Kulesza, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) have recently been proposed as computationally efﬁcient probabilistic models of diverse sets for a variety of applications, including document summarization, image search, and pose estimation. Many DPP inference operations, including normalization and sampling, are tractable; however, ﬁnding the most likely conﬁguration (MAP), which is often required in practice for decoding, is NP-hard, so we must resort to approximate inference. This optimization problem, which also arises in experimental design and sensor placement, involves ﬁnding the largest principal minor of a positive semideﬁnite matrix. Because the objective is log-submodular, greedy algorithms have been used in the past with some empirical success; however, these methods only give approximation guarantees in the special case of monotone objectives, which correspond to a restricted class of DPPs. In this paper we propose a new algorithm for approximating the MAP problem based on continuous techniques for submodular function maximization. Our method involves a novel continuous relaxation of the log-probability function, which, in contrast to the multilinear extension used for general submodular functions, can be evaluated and differentiated exactly and efﬁciently. We obtain a practical algorithm with a 1/4-approximation guarantee for a more general class of non-monotone DPPs; our algorithm also extends to MAP inference under complex polytope constraints, making it possible to combine DPPs with Markov random ﬁelds, weighted matchings, and other models. We demonstrate that our approach outperforms standard and recent methods on both synthetic and real-world data. 1</p><p>3 0.2000989 <a title="304-tfidf-3" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>Author: Aaron Defazio, Tibério S. Caetano</p><p>Abstract: A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov´ sz extension to obtain a convex relaxation. For tractable classes a such as Gaussian graphical models, this leads to a convex optimization problem that can be efﬁciently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.</p><p>4 0.19883104 <a title="304-tfidf-4" href="./nips-2012-Submodular-Bregman_and_the_Lov%C3%A1sz-Bregman_Divergences_with_Applications.html">328 nips-2012-Submodular-Bregman and the Lovász-Bregman Divergences with Applications</a></p>
<p>Author: Rishabh Iyer, Jeff A. Bilmes</p><p>Abstract: We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular-Bregman divergences. We consider two kinds, deﬁned either from tight modular upper or tight modular lower bounds of a submodular function. We show that the properties of these divergences are analogous to the (standard continuous) Bregman divergence. We demonstrate how they generalize many useful divergences, including the weighted Hamming distance, squared weighted Hamming, weighted precision, recall, conditional mutual information, and a generalized KL-divergence on sets. We also show that the generalized Bregman divergence on the Lov´ sz extension of a submodular function, which we a call the Lov´ sz-Bregman divergence, is a continuous extension of a submodular a Bregman divergence. We point out a number of applications, and in particular show that a proximal algorithm deﬁned through the submodular Bregman divergence provides a framework for many mirror-descent style algorithms related to submodular function optimization. We also show that a generalization of the k-means algorithm using the Lov´ sz Bregman divergence is natural in clustering scenarios where a ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efﬁcient unlike other order based distance measures. 1</p><p>5 0.10308515 <a title="304-tfidf-5" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>6 0.10041688 <a title="304-tfidf-6" href="./nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">214 nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>7 0.099266768 <a title="304-tfidf-7" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>8 0.09449178 <a title="304-tfidf-8" href="./nips-2012-CPRL_--_An_Extension_of_Compressive_Sensing_to_the_Phase_Retrieval_Problem.html">63 nips-2012-CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem</a></p>
<p>9 0.084749252 <a title="304-tfidf-9" href="./nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">141 nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<p>10 0.083783254 <a title="304-tfidf-10" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>11 0.083247669 <a title="304-tfidf-11" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>12 0.08095821 <a title="304-tfidf-12" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>13 0.074028097 <a title="304-tfidf-13" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<p>14 0.072327837 <a title="304-tfidf-14" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>15 0.067755565 <a title="304-tfidf-15" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>16 0.065943733 <a title="304-tfidf-16" href="./nips-2012-Convergence_Rate_Analysis_of_MAP_Coordinate_Minimization_Algorithms.html">84 nips-2012-Convergence Rate Analysis of MAP Coordinate Minimization Algorithms</a></p>
<p>17 0.065908588 <a title="304-tfidf-17" href="./nips-2012-Feature_Clustering_for_Accelerating_Parallel_Coordinate_Descent.html">131 nips-2012-Feature Clustering for Accelerating Parallel Coordinate Descent</a></p>
<p>18 0.065832011 <a title="304-tfidf-18" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>19 0.063635632 <a title="304-tfidf-19" href="./nips-2012-Calibrated_Elastic_Regularization_in_Matrix_Completion.html">64 nips-2012-Calibrated Elastic Regularization in Matrix Completion</a></p>
<p>20 0.063518435 <a title="304-tfidf-20" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.046), (2, 0.078), (3, -0.108), (4, 0.059), (5, 0.069), (6, -0.018), (7, -0.034), (8, -0.018), (9, -0.014), (10, -0.014), (11, 0.011), (12, -0.021), (13, 0.072), (14, 0.136), (15, 0.018), (16, 0.005), (17, -0.026), (18, -0.044), (19, -0.058), (20, 0.171), (21, 0.132), (22, 0.087), (23, 0.004), (24, 0.308), (25, 0.122), (26, -0.113), (27, -0.005), (28, -0.008), (29, -0.035), (30, -0.028), (31, 0.057), (32, 0.059), (33, 0.036), (34, -0.13), (35, 0.151), (36, -0.008), (37, 0.006), (38, -0.022), (39, 0.045), (40, 0.034), (41, -0.062), (42, -0.034), (43, -0.039), (44, 0.021), (45, -0.018), (46, 0.008), (47, -0.042), (48, -0.016), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91124618 <a title="304-lsi-1" href="./nips-2012-Selecting_Diverse_Features_via_Spectral_Regularization.html">304 nips-2012-Selecting Diverse Features via Spectral Regularization</a></p>
<p>Author: Abhimanyu Das, Anirban Dasgupta, Ravi Kumar</p><p>Abstract: We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability, robustness to noise, etc. We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized by efﬁcient greedy and local search algorithms, with provable guarantees. We compare our algorithms to traditional greedy and 1 -regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations. 1</p><p>2 0.8477686 <a title="304-lsi-2" href="./nips-2012-Near-Optimal_MAP_Inference_for_Determinantal_Point_Processes.html">236 nips-2012-Near-Optimal MAP Inference for Determinantal Point Processes</a></p>
<p>Author: Jennifer Gillenwater, Alex Kulesza, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) have recently been proposed as computationally efﬁcient probabilistic models of diverse sets for a variety of applications, including document summarization, image search, and pose estimation. Many DPP inference operations, including normalization and sampling, are tractable; however, ﬁnding the most likely conﬁguration (MAP), which is often required in practice for decoding, is NP-hard, so we must resort to approximate inference. This optimization problem, which also arises in experimental design and sensor placement, involves ﬁnding the largest principal minor of a positive semideﬁnite matrix. Because the objective is log-submodular, greedy algorithms have been used in the past with some empirical success; however, these methods only give approximation guarantees in the special case of monotone objectives, which correspond to a restricted class of DPPs. In this paper we propose a new algorithm for approximating the MAP problem based on continuous techniques for submodular function maximization. Our method involves a novel continuous relaxation of the log-probability function, which, in contrast to the multilinear extension used for general submodular functions, can be evaluated and differentiated exactly and efﬁciently. We obtain a practical algorithm with a 1/4-approximation guarantee for a more general class of non-monotone DPPs; our algorithm also extends to MAP inference under complex polytope constraints, making it possible to combine DPPs with Markov random ﬁelds, weighted matchings, and other models. We demonstrate that our approach outperforms standard and recent methods on both synthetic and real-world data. 1</p><p>3 0.81927639 <a title="304-lsi-3" href="./nips-2012-Submodular-Bregman_and_the_Lov%C3%A1sz-Bregman_Divergences_with_Applications.html">328 nips-2012-Submodular-Bregman and the Lovász-Bregman Divergences with Applications</a></p>
<p>Author: Rishabh Iyer, Jeff A. Bilmes</p><p>Abstract: We introduce a class of discrete divergences on sets (equivalently binary vectors) that we call the submodular-Bregman divergences. We consider two kinds, deﬁned either from tight modular upper or tight modular lower bounds of a submodular function. We show that the properties of these divergences are analogous to the (standard continuous) Bregman divergence. We demonstrate how they generalize many useful divergences, including the weighted Hamming distance, squared weighted Hamming, weighted precision, recall, conditional mutual information, and a generalized KL-divergence on sets. We also show that the generalized Bregman divergence on the Lov´ sz extension of a submodular function, which we a call the Lov´ sz-Bregman divergence, is a continuous extension of a submodular a Bregman divergence. We point out a number of applications, and in particular show that a proximal algorithm deﬁned through the submodular Bregman divergence provides a framework for many mirror-descent style algorithms related to submodular function optimization. We also show that a generalization of the k-means algorithm using the Lov´ sz Bregman divergence is natural in clustering scenarios where a ordering is important. A unique property of this algorithm is that computing the mean ordering is extremely efﬁcient unlike other order based distance measures. 1</p><p>4 0.63407069 <a title="304-lsi-4" href="./nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">214 nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>Author: Andrew Delong, Olga Veksler, Anton Osokin, Yuri Boykov</p><p>Abstract: Inference in high-order graphical models has become important in recent years. Several approaches are based, for example, on generalized message-passing, or on transformation to a pairwise model with extra ‘auxiliary’ variables. We focus on a special case where a much more efﬁcient transformation is possible. Instead of adding variables, we transform the original problem into a comparatively small instance of submodular vertex-cover. These vertex-cover instances can then be attacked by existing algorithms (e.g. belief propagation, QPBO), where they often run 4–15 times faster and ﬁnd better solutions than when applied to the original problem. We evaluate our approach on synthetic data, then we show applications within a fast hierarchical clustering and model-ﬁtting framework. 1</p><p>5 0.61635464 <a title="304-lsi-5" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>Author: Aaron Defazio, Tibério S. Caetano</p><p>Abstract: A key problem in statistics and machine learning is the determination of network structure from data. We consider the case where the structure of the graph to be reconstructed is known to be scale-free. We show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their Lov´ sz extension to obtain a convex relaxation. For tractable classes a such as Gaussian graphical models, this leads to a convex optimization problem that can be efﬁciently solved. We show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data. We also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.</p><p>6 0.49974272 <a title="304-lsi-6" href="./nips-2012-CPRL_--_An_Extension_of_Compressive_Sensing_to_the_Phase_Retrieval_Problem.html">63 nips-2012-CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem</a></p>
<p>7 0.43178809 <a title="304-lsi-7" href="./nips-2012-Spectral_Learning_of_General_Weighted_Automata_via_Constrained_Matrix_Completion.html">320 nips-2012-Spectral Learning of General Weighted Automata via Constrained Matrix Completion</a></p>
<p>8 0.42337477 <a title="304-lsi-8" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>9 0.40451664 <a title="304-lsi-9" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>10 0.39644754 <a title="304-lsi-10" href="./nips-2012-Multi-Stage_Multi-Task_Feature_Learning.html">221 nips-2012-Multi-Stage Multi-Task Feature Learning</a></p>
<p>11 0.39437261 <a title="304-lsi-11" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>12 0.38253438 <a title="304-lsi-12" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>13 0.36561489 <a title="304-lsi-13" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>14 0.36545563 <a title="304-lsi-14" href="./nips-2012-Convex_Multi-view_Subspace_Learning.html">86 nips-2012-Convex Multi-view Subspace Learning</a></p>
<p>15 0.36252829 <a title="304-lsi-15" href="./nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<p>16 0.35966742 <a title="304-lsi-16" href="./nips-2012-Variational_Inference_for_Crowdsourcing.html">359 nips-2012-Variational Inference for Crowdsourcing</a></p>
<p>17 0.3529869 <a title="304-lsi-17" href="./nips-2012-A_new_metric_on_the_manifold_of_kernel_matrices_with_application_to_matrix_geometric_means.html">25 nips-2012-A new metric on the manifold of kernel matrices with application to matrix geometric means</a></p>
<p>18 0.34518543 <a title="304-lsi-18" href="./nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">230 nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<p>19 0.34339949 <a title="304-lsi-19" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>20 0.34208196 <a title="304-lsi-20" href="./nips-2012-A_Scalable_CUR_Matrix_Decomposition_Algorithm%3A_Lower_Time_Complexity_and_Tighter_Bound.html">17 nips-2012-A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.059), (3, 0.223), (21, 0.034), (38, 0.168), (39, 0.022), (42, 0.036), (54, 0.019), (55, 0.016), (68, 0.011), (74, 0.082), (76, 0.135), (80, 0.081), (92, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81488299 <a title="304-lda-1" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>Author: Aditya Khosla, Jianxiong Xiao, Antonio Torralba, Aude Oliva</p><p>Abstract: While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. 1</p><p>same-paper 2 0.81290281 <a title="304-lda-2" href="./nips-2012-Selecting_Diverse_Features_via_Spectral_Regularization.html">304 nips-2012-Selecting Diverse Features via Spectral Regularization</a></p>
<p>Author: Abhimanyu Das, Anirban Dasgupta, Ravi Kumar</p><p>Abstract: We study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective. Diversity is useful for several reasons such as interpretability, robustness to noise, etc. We propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions. These regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized by efﬁcient greedy and local search algorithms, with provable guarantees. We compare our algorithms to traditional greedy and 1 -regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations. 1</p><p>3 0.73981237 <a title="304-lda-3" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>Author: Vasiliy Karasev, Alessandro Chiuso, Stefano Soatto</p><p>Abstract: We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of “visual search” of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a “passive” agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an “omnipotent” agent, capable of inﬁnite control authority, can achieve arbitrarily good performance (asymptotically). In between these limiting cases, the tradeoff can be characterized empirically. 1</p><p>4 0.73838764 <a title="304-lda-4" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>Author: Odalric-ambrym Maillard</p><p>Abstract: This paper aims to take a step forwards making the term “intrinsic motivation” from reinforcement learning theoretically well founded, focusing on curiositydriven learning. To that end, we consider the setting where, a ﬁxed partition P of a continuous space X being given, and a process ν deﬁned on X being unknown, we are asked to sequentially decide which cell of the partition to select as well as where to sample ν in that cell, in order to minimize a loss function that is inspired from previous work on curiosity-driven learning. The loss on each cell consists of one term measuring a simple worst case quadratic sampling error, and a penalty term proportional to the range of the variance in that cell. The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region, and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem. The resulting procedure, called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a ﬁnite-time regret analysis. 1</p><p>5 0.73694092 <a title="304-lda-5" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>Author: Andriy Mnih, Yee W. Teh</p><p>Abstract: User preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories. Research in collaborative ﬁltering has concentrated on explicit feedback, resulting in the development of accurate and scalable models. However, since explicit feedback is often difﬁcult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback. We introduce a probabilistic approach to collaborative ﬁltering with implicit feedback based on modelling the user’s item selection process. In the interests of scalability, we restrict our attention to treestructured distributions over items and develop a principled and efﬁcient algorithm for learning item trees from data. We also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data. 1</p><p>6 0.73525178 <a title="304-lda-6" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>7 0.7335034 <a title="304-lda-7" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>8 0.73342043 <a title="304-lda-8" href="./nips-2012-Near-Optimal_MAP_Inference_for_Determinantal_Point_Processes.html">236 nips-2012-Near-Optimal MAP Inference for Determinantal Point Processes</a></p>
<p>9 0.7329064 <a title="304-lda-9" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>10 0.73281825 <a title="304-lda-10" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>11 0.73252016 <a title="304-lda-11" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>12 0.73150492 <a title="304-lda-12" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>13 0.73088777 <a title="304-lda-13" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>14 0.72981662 <a title="304-lda-14" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>15 0.72847331 <a title="304-lda-15" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>16 0.72818899 <a title="304-lda-16" href="./nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</a></p>
<p>17 0.72810972 <a title="304-lda-17" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>18 0.72717106 <a title="304-lda-18" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>19 0.72693199 <a title="304-lda-19" href="./nips-2012-Learning_curves_for_multi-task_Gaussian_process_regression.html">187 nips-2012-Learning curves for multi-task Gaussian process regression</a></p>
<p>20 0.72610068 <a title="304-lda-20" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
