<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 nips-2012-Learning about Canonical Views from Internet Image Collections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-185" href="#">nips2012-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 nips-2012-Learning about Canonical Views from Internet Image Collections</h1>
<br/><p>Source: <a title="nips-2012-185-pdf" href="http://papers.nips.cc/paper/4827-learning-about-canonical-views-from-internet-image-collections.pdf">pdf</a></p><p>Author: Elad Mezuman, Yair Weiss</p><p>Abstract: Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or “canonical” view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views? We start by manually ﬁnding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to ﬁnd the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories. 1</p><p>Reference: <a title="nips-2012-185-reference" href="../nips2012_reference/nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 il/~mezuman  Abstract Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or “canonical” view of objects. [sent-9, score-0.576]
</p><p>2 This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. [sent-10, score-0.889]
</p><p>3 Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. [sent-11, score-1.156]
</p><p>4 In this paper we ask: Can we use Internet image collections to learn more about canonical views? [sent-12, score-0.598]
</p><p>5 We start by manually ﬁnding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. [sent-13, score-0.62]
</p><p>6 Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. [sent-14, score-0.712]
</p><p>7 We also present a simple method to ﬁnd the most likely view in an image collection and apply it to hundreds of categories. [sent-15, score-0.466]
</p><p>8 Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories. [sent-16, score-0.978]
</p><p>9 Although ideally object recognition should be viewpoint invariant, much research in human perception indicates that certain views are privileged, or “canonical”. [sent-18, score-0.657]
</p><p>10 1 presents different views of a horse used in their experiments and the average goodness rating given by human subjects. [sent-22, score-0.616]
</p><p>11 For the horse, the canonical view is a slightly off-axis sideways view, while the least favored view is from above. [sent-23, score-0.939]
</p><p>12 The preference for side views of horses is very robust and can be reliably demonstrated in simple classroom experiments [6]. [sent-25, score-0.506]
</p><p>13 36  Figure 1: When people are asked to rate images of the same object from different views some views consistently get better grades than others. [sent-40, score-1.2]
</p><p>14 The view that gets the best grade is called the canonical view. [sent-41, score-0.673]
</p><p>15 The ﬁrst one, called the frequency hypothesis argues that the canonical view is the one from which we most often see the object. [sent-45, score-0.735]
</p><p>16 The second one, called the maximal information hypothesis argues that the canonical view is the view that gives the most information about the 3D structure of the object. [sent-46, score-1.001]
</p><p>17 If we have access to the statistics with which we view certain objects, we can compute the most frequent view and given the 3D shape of an object we can automatically compute the most stable view [7, 8, 9]. [sent-51, score-1.201]
</p><p>18 Both of these formal theories have been shown to be insufﬁcient to predict the canonical views preferred by human observers; Palmer et al. [sent-52, score-1.128]
</p><p>19 One reason for the relative vagueness of theories of canonical views may be the lack of data: the number of objects for which canonical views have been tested in the lab is at most a few dozens. [sent-55, score-1.832]
</p><p>20 In this paper, we seek to dramatically increase the number of examples for canonical views using Internet search engines and computer vision tools. [sent-56, score-0.984]
</p><p>21 We expect that since the canonical view of an object corresponds to what people perceive as the "best" photograph, when people include a photograph of an object in their web page, they are most likely to choose a photograph from the canonical view. [sent-57, score-1.4]
</p><p>22 In other words, we expect the canonical view to be the most frequent view in the set of images retrieved by a search engine when queried for the object. [sent-58, score-1.573]
</p><p>23 We start by manually validating our hypothesis and showing that indeed the most frequent view in Internet image collections often corresponds to the cognitive canonical view. [sent-59, score-1.286]
</p><p>24 We then present an automatic method for ﬁnding the most frequent view in a large dataset of images. [sent-60, score-0.614]
</p><p>25 Rather than trying to map images to views and then ﬁnding the most frequent view, we ﬁnd it by analyzing the density of global image descriptors. [sent-61, score-1.129]
</p><p>26 Using images for which we have ground truth, we verify that our automatic method indeed ﬁnds the most frequent view in a large percentage of the cases. [sent-62, score-0.836]
</p><p>27 We next apply this method to images retrieved by search engines and ﬁnd the canonical view for hundreds of categories. [sent-63, score-1.104]
</p><p>28 Finally we use the canonical views we ﬁnd to present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories. [sent-64, score-1.824]
</p><p>29 2  Figure 2: The four most frequent views (frequencies speciﬁed) manually found in images returned by Google images (second-ﬁfth rows) often corresponds to the canonical view found in psychophysical experiments (ﬁrst row). [sent-65, score-2.027]
</p><p>30 2  Manual experiments with Internet image collections  We ﬁrst asked whether Internet image collections will show the same view biases as reported in psychophysical experiments. [sent-66, score-0.794]
</p><p>31 In order to answer this question, we downloaded images of the twelve categories used by Palmer et al. [sent-67, score-0.477]
</p><p>32 To download these images we simply queried Google Image search with the object and retrieved the top returned images. [sent-69, score-0.459]
</p><p>33 For each category we manually sorted the images into bins corresponding to similar views (each category could have a different number of bins), counted the number of images in each bin and found the most frequent view. [sent-70, score-1.385]
</p><p>34 We used 400 images for the four categories presented in Figure 2 and 100 images for the other eight categories. [sent-71, score-0.634]
</p><p>35 Figure 2 shows the bins with the highest frequencies along with their frequencies and the cognitive canonical view for car, horse, shoe, and steaming iron categories. [sent-72, score-0.788]
</p><p>36 The results of this manual experiment are clear cut: for 11 out of the 12 categories, the most frequent view in Google images is the canonical view found by Palmer et al. [sent-73, score-1.504]
</p><p>37 The only exception is the horse category for which the most frequent view is the one that received the second best ratings in the psychophysical experiments (see ﬁgure 1). [sent-75, score-0.837]
</p><p>38 This study validates our hypothesis that when humans decide which view of an object to embed in a web page, they exhibit a very similar view bias as is seen in psychophysical experiments. [sent-76, score-0.832]
</p><p>39 This result now gives us the possibility to harness the huge numbers of images available on the Internet to study these view biases in many categories. [sent-77, score-0.54]
</p><p>40 3  Can we ﬁnd the most frequent view automatically? [sent-78, score-0.569]
</p><p>41 [10] showed how clustering Internet photographs of tourist sites can ﬁnd several "canonical" views of the site. [sent-83, score-0.472]
</p><p>42 Clustering on images from the Internet is also used to ﬁnd canonical views (or iconic images) in other works e. [sent-84, score-1.142]
</p><p>43 [13] uses similarity measure between images to ﬁnd a small subset of canonical images to a larger set of images. [sent-88, score-0.9]
</p><p>44 Deselaers and Ferrari [14] present a simpler method that ﬁnds the image in the center of the GIST image descriptor [15] space to select the prototype image for categories in ImageNet [16]. [sent-92, score-0.629]
</p><p>45 We experimented with this method and found that often the prototypical image did not correspond to the most frequent view. [sent-93, score-0.457]
</p><p>46 [17] suggest a method to ﬁnd a single most representative image (canonical image) for a category relying on similarities between images based on local invariant features. [sent-95, score-0.413]
</p><p>47 Since they use invariant features the view of the object in the image has no role in the selection of the canonical image. [sent-96, score-0.901]
</p><p>48 Weyand and Leibe [18] use mode estimation to ﬁnd iconic images for many images of a single scene using a distance measure based on calculating a homography between the images and measuring the overlap. [sent-97, score-0.841]
</p><p>49 Our method to ﬁnd the most frequent view is based on estimating the density of views using the Parzen window method, and simply choosing the modes of the density as the most frequent views. [sent-99, score-1.458]
</p><p>50 In other words, if we have an image descriptor so that the distance between descriptors for two images approximates the similarity of views between the objects, we can calculate the Parzen density without ever computing the views. [sent-108, score-0.93]
</p><p>51 We hypothesize that despite this sensitivity to the background, the maximum of the Parzen density when we use GIST similarity between images will serve as a useful proxy for the maximum of the Parzen density when we use view similarity. [sent-112, score-0.641]
</p><p>52 1  Our method  In summary, given an object category our algorithm automatically ﬁnds the modes of the GIST distribution in images of that object. [sent-114, score-0.458]
</p><p>53 Our method therefore also includes a manual phase which requires a human to view the output of the algorithm and to verify whether or not this mode in the GIST distribution actually corresponds to a mode in the view distribution. [sent-116, score-0.815]
</p><p>54 The ﬁrst mode is simply the most frequent image in the GIST space and its k closest neighbors. [sent-120, score-0.505]
</p><p>55 The second mode is the most frequent image that is not a close neighbor of the ﬁrst most frequent image (e. [sent-121, score-0.936]
</p><p>56 We found that when a human veriﬁes that a set of images that are modes in the GIST space are indeed of the same view, then in almost all cases these images are indeed the modes in view space. [sent-129, score-0.916]
</p><p>57 4  (a)  (b)  (c)  (d)  Figure 3: By using Parzen density estimation on GIST features, we are able to ﬁnd the most frequent view without calculating the view for a given image. [sent-131, score-0.872]
</p><p>58 (a) Distribution of views for 715 images of Notre Dame Cathedral in Paris, adapted from [22]. [sent-132, score-0.661]
</p><p>59 The image from the most frequent view (c) is the same image of the most frequent GIST descriptor (d). [sent-134, score-1.183]
</p><p>60 This is much less painful than requiring a human to look at all retrieved images which can take a few hours (the automatic part of the method, that ﬁnds the modes in GIST space, takes a few seconds of computer time). [sent-136, score-0.44]
</p><p>61 In the ﬁrst experiment, we ran our automatic method on the same images that we manually sorted into views in the previous section: images downloaded from Google image search for the twelve categories used by Palmer et al. [sent-140, score-1.388]
</p><p>62 We ﬁnd that in 10 out of 12 categories our automatic method found the same most frequent view as we found manually. [sent-143, score-0.804]
</p><p>63 On this dataset, we calculated the most frequent view using Parzen density estimation in two different ways (1) using the similarity between the camera’s rotation matrices and (2) using the GIST similarity between images. [sent-147, score-0.704]
</p><p>64 As shown in ﬁgure 3 the most frequent view calculated using the two methods was identical. [sent-148, score-0.569]
</p><p>65 3  Control  As can be seen in ﬁgure 4, the most frequent view chosen by our method often has a white, or uniform background. [sent-150, score-0.569]
</p><p>66 Will a method that simply chooses images with uniform background can also ﬁnd canonical views? [sent-151, score-0.629]
</p><p>67 The ImageNet images were collected by querying various Internet search engines with the desired object, and the resulting set of images was then “cleaned up” by humans. [sent-156, score-0.541]
</p><p>68 It is important to note that the humans were not instructed to choose particular views but rather to verify that the image contained the desired object. [sent-157, score-0.611]
</p><p>69 For a subset of the images, ImageNet also supplies bounding boxes around the object of interest; we cropped the objects from the images and considered it as a fourth dataset. [sent-158, score-0.494]
</p><p>70 We saw that our method ﬁnds preferred views also in the other datasets and that these preferred views are usually the cognitive canonical views. [sent-160, score-1.502]
</p><p>71 One example of this improvement is the horse category for which we did not ﬁnd the most frequent view using the the full images but did ﬁnd it when we used the cropped images. [sent-162, score-0.983]
</p><p>72 5  Random  Palmer  First mode  Figure 4: Results on categories we downloaded from Google for which the canonical view was found in Palmer et al. [sent-164, score-0.972]
</p><p>73 4  What can we learn from hundreds of canonical views? [sent-168, score-0.479]
</p><p>74 To summarize our validation experiments: although we use GIST similarity as a proxy for view similarity, our method often ﬁnds the canonical view. [sent-169, score-0.752]
</p><p>75 We used our method to ﬁnd canonical views for two groups of object categories: (1) 54 categories inspired by the work of Rosch et al. [sent-171, score-1.136]
</p><p>76 (2) 552 categories of mammals (all the categories of mammals in ImageNet [16] for which there are bounding boxes around the objects), for these categories we used the cropped objects. [sent-173, score-0.772]
</p><p>77 For every object category tested we downloaded all corresponding images (in average more than 1,200 images, out of them around 300 with bounding boxes) from ImageNet. [sent-174, score-0.452]
</p><p>78 For Rosch’s categories we used full images since for some of them bounding boxes are not supplied, for the mammals we used cropped images. [sent-178, score-0.574]
</p><p>79 For most of the categories the modes found by our algorithm were indeed veriﬁed by a human observer as representing a true mode in view space. [sent-179, score-0.663]
</p><p>80 Thus while our method does not succeed in ﬁnding preferred views for all categories, by focusing only on the categories for which humans veriﬁed that preferred views were found, we still have canonical views for hundreds of categories. [sent-180, score-2.21]
</p><p>81 [2] raised two basic theories to explain the phenomenon of canonical views: (1) the frequency hypothesis and (2) the maximal information hypothesis. [sent-185, score-0.532]
</p><p>82 We ﬁnd canonical views of animals that are from the animals’ height rather than ours (ﬁg. [sent-187, score-0.891]
</p><p>83 5a-b); dogs, for example, are usually seen from above while many of the canonical views we ﬁnd for dogs are from their height. [sent-188, score-0.873]
</p><p>84 The canonical views of vehicles are another counter-example for the frequency hypothesis, we usually see vehicles from the side (as pedestrians) or from behind (as drivers), but the canonical views we ﬁnd are the “perfect” off-axis view (ﬁg. [sent-189, score-2.014]
</p><p>85 As a third family of examples we have the tools; we usually see them when we use them, this is not the canonical view we ﬁnd (ﬁg. [sent-191, score-0.673]
</p><p>86 While for 20% of the categories we ﬁnd off-axis canonical views that give the most information about the shape of the object, for more than 60% of the categories we ﬁnd canonical views that are either side-views (ﬁg. [sent-194, score-2.072]
</p><p>87 5f,i) or frontal views (especially views of the face - ﬁg. [sent-195, score-0.878]
</p><p>88 Not only do these views not give us the full information about the 3D structure of the object, they are also accidental, i. [sent-197, score-0.439]
</p><p>89 a small change in the view will cause a big change of the appearance of the object; for example in some of the side-views we see only two legs out of four, a small change in the view will reveal the two other legs. [sent-199, score-0.562]
</p><p>90 2  Constraints for new theories  We believe that our experiments reveal several robust features of canonical views that every future theory should take into considerations. [sent-201, score-0.966]
</p><p>91 The ﬁrst aspect is that there are several preferred views for a given object. [sent-202, score-0.529]
</p><p>92 Sometimes these several views are related to symmetry (e. [sent-203, score-0.439]
</p><p>93 a mirror image of the preferred view is also preferred) but in other cases they are different views that are just slightly less preferred than the canonical view (e. [sent-205, score-1.686]
</p><p>94 Finally, the view biases are most pronounced for basic and subordinate level categories and less so for superordinate categories (e. [sent-214, score-0.671]
</p><p>95 5  Conclusion  In this work we revisited a cognitive phenomenon that was discovered over 30 years ago: a preference by human observers for particular "canonical" views of objects. [sent-219, score-0.596]
</p><p>96 We showed that a nearly identical view bias can be observed in the results of Internet image search engines, suggesting that when humans decide which image to embed in a web page, they prefer the same canonical view that is assigned highest goodness in laboratory experiments. [sent-220, score-1.302]
</p><p>97 We presented an automatic method to discover the most likely view in an image collection and used this algorithm to obtain canonical views for hundreds of object categories. [sent-221, score-1.457]
</p><p>98 Our results provide strong counter-examples for the two formal hypotheses of canonical views; we hope they will serve as a basis for a computational explanation for this fascinating effect. [sent-222, score-0.449]
</p><p>99 Canonical views of scenes depend on the shape of the space. [sent-256, score-0.439]
</p><p>100 Discovering favorite views of popular places with iconoid shift. [sent-347, score-0.439]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('views', 0.439), ('canonical', 0.407), ('frequent', 0.303), ('view', 0.266), ('gist', 0.255), ('images', 0.222), ('categories', 0.19), ('palmer', 0.154), ('image', 0.128), ('psychophysical', 0.121), ('internet', 0.113), ('rosch', 0.111), ('parzen', 0.108), ('object', 0.1), ('preferred', 0.09), ('theories', 0.09), ('horse', 0.084), ('imagenet', 0.077), ('mode', 0.074), ('iconic', 0.074), ('modes', 0.073), ('hundreds', 0.072), ('engines', 0.067), ('category', 0.063), ('collections', 0.063), ('photograph', 0.06), ('human', 0.06), ('viewpoint', 0.058), ('google', 0.057), ('collages', 0.055), ('descriptor', 0.055), ('objects', 0.05), ('similarity', 0.049), ('manually', 0.047), ('boxes', 0.045), ('animals', 0.045), ('automatic', 0.045), ('dame', 0.045), ('notre', 0.045), ('cropped', 0.045), ('humans', 0.044), ('formal', 0.042), ('vision', 0.041), ('mammals', 0.04), ('manual', 0.04), ('retrieved', 0.04), ('queried', 0.039), ('nd', 0.038), ('density', 0.037), ('cathedral', 0.037), ('denton', 0.037), ('edmond', 0.037), ('horses', 0.037), ('lthoff', 0.037), ('mezuman', 0.037), ('perceiver', 0.037), ('raguram', 0.037), ('safra', 0.037), ('weyand', 0.037), ('cognitive', 0.037), ('nds', 0.036), ('phase', 0.035), ('downloaded', 0.035), ('hypothesis', 0.035), ('goodness', 0.033), ('snavely', 0.033), ('blanz', 0.033), ('collage', 0.033), ('lily', 0.033), ('photographs', 0.033), ('bounding', 0.032), ('camera', 0.031), ('preference', 0.03), ('reveal', 0.03), ('azimuth', 0.03), ('workshops', 0.03), ('jing', 0.03), ('twelve', 0.03), ('observers', 0.03), ('proxy', 0.03), ('search', 0.03), ('deselaers', 0.028), ('vehicles', 0.028), ('download', 0.028), ('berg', 0.028), ('scene', 0.027), ('photo', 0.027), ('argues', 0.027), ('ehinger', 0.027), ('harness', 0.027), ('vague', 0.027), ('elevation', 0.027), ('dogs', 0.027), ('veri', 0.027), ('page', 0.027), ('bins', 0.026), ('frequencies', 0.026), ('prototypical', 0.026), ('ago', 0.026), ('biases', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="185-tfidf-1" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>Author: Elad Mezuman, Yair Weiss</p><p>Abstract: Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or “canonical” view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views? We start by manually ﬁnding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to ﬁnd the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories. 1</p><p>2 0.13192078 <a title="185-tfidf-2" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>Author: Trung Nguyen, Tomi Silander, Tze Y. Leong</p><p>Abstract: We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efﬁcient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without predeﬁned mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. 1</p><p>3 0.12777238 <a title="185-tfidf-3" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>4 0.10314551 <a title="185-tfidf-4" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>Author: Mohsen Hejrati, Deva Ramanan</p><p>Abstract: We present an approach to detecting and analyzing the 3D conﬁguration of objects in real-world images with heavy occlusion and clutter. We focus on the application of ﬁnding and analyzing cars. We do so with a two-stage model; the ﬁrst stage reasons about 2D shape and appearance variation due to within-class variation (station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then reﬁned by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset. 1</p><p>5 0.099839412 <a title="185-tfidf-5" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>Author: Jinfeng Yi, Rong Jin, Shaili Jain, Tianbao Yang, Anil K. Jain</p><p>Abstract: One of the main challenges in data clustering is to deﬁne an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by deﬁning the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called semi-crowdsourced clustering that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects and from the manual annotations of only a small portion of the data to be clustered. One difﬁculty in learning the pairwise similarity measure is that there is a signiﬁcant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difﬁculty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efﬁciency. 1</p><p>6 0.096816286 <a title="185-tfidf-6" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>7 0.088969655 <a title="185-tfidf-7" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>8 0.086341202 <a title="185-tfidf-8" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>9 0.08584667 <a title="185-tfidf-9" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>10 0.08261203 <a title="185-tfidf-10" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>11 0.080163933 <a title="185-tfidf-11" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>12 0.079596244 <a title="185-tfidf-12" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>13 0.075487979 <a title="185-tfidf-13" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>14 0.072912186 <a title="185-tfidf-14" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>15 0.072909527 <a title="185-tfidf-15" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>16 0.07097023 <a title="185-tfidf-16" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>17 0.06966152 <a title="185-tfidf-17" href="./nips-2012-Convex_Multi-view_Subspace_Learning.html">86 nips-2012-Convex Multi-view Subspace Learning</a></p>
<p>18 0.069408335 <a title="185-tfidf-18" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>19 0.068703443 <a title="185-tfidf-19" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>20 0.068703443 <a title="185-tfidf-20" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, 0.028), (2, -0.178), (3, -0.029), (4, 0.087), (5, -0.086), (6, -0.005), (7, -0.027), (8, 0.03), (9, 0.006), (10, -0.013), (11, -0.034), (12, 0.003), (13, -0.072), (14, 0.051), (15, 0.102), (16, 0.032), (17, -0.023), (18, -0.04), (19, -0.067), (20, 0.004), (21, -0.023), (22, -0.015), (23, 0.011), (24, 0.002), (25, 0.021), (26, 0.059), (27, 0.042), (28, 0.06), (29, 0.054), (30, -0.016), (31, -0.029), (32, 0.053), (33, -0.059), (34, 0.05), (35, 0.097), (36, -0.025), (37, -0.045), (38, 0.055), (39, -0.052), (40, 0.042), (41, -0.014), (42, 0.058), (43, -0.044), (44, 0.083), (45, -0.017), (46, 0.159), (47, 0.049), (48, -0.023), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98035073 <a title="185-lsi-1" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>Author: Elad Mezuman, Yair Weiss</p><p>Abstract: Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or “canonical” view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views? We start by manually ﬁnding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to ﬁnd the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories. 1</p><p>2 0.75286239 <a title="185-lsi-2" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>Author: Aditya Khosla, Jianxiong Xiao, Antonio Torralba, Aude Oliva</p><p>Abstract: While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. 1</p><p>3 0.73324674 <a title="185-lsi-3" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>Author: Shulin Yang, Liefeng Bo, Jue Wang, Linda G. Shapiro</p><p>Abstract: Fine-grained recognition refers to a subordinate level of recognition, such as recognizing different species of animals and plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape and structure shared cross different categories, and the differences are in the details of object parts. We suggest that the key to identifying the ﬁne-grained differences lies in ﬁnding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the cooccurrence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classiﬁcation. Learning of the template model is efﬁcient, and the recognition results we achieve signiﬁcantly outperform the stateof-the-art algorithms. 1</p><p>4 0.59684008 <a title="185-lsi-4" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>Author: Tatsuya Harada, Yasuo Kuniyoshi</p><p>Abstract: This paper proposes a novel image representation called a Graphical Gaussian Vector (GGV), which is a counterpart of the codebook and local feature matching approaches. We model the distribution of local features as a Gaussian Markov Random Field (GMRF) which can efﬁciently represent the spatial relationship among local features. Using concepts of information geometry, proper parameters and a metric from the GMRF can be obtained. Then we deﬁne a new image feature by embedding the proper metric into the parameters, which can be directly applied to scalable linear classiﬁers. We show that the GGV obtains better performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset. 1</p><p>5 0.59347731 <a title="185-lsi-5" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>Author: Mohsen Hejrati, Deva Ramanan</p><p>Abstract: We present an approach to detecting and analyzing the 3D conﬁguration of objects in real-world images with heavy occlusion and clutter. We focus on the application of ﬁnding and analyzing cars. We do so with a two-stage model; the ﬁrst stage reasons about 2D shape and appearance variation due to within-class variation (station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then reﬁned by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset. 1</p><p>6 0.58959323 <a title="185-lsi-6" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>7 0.57571608 <a title="185-lsi-7" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>8 0.56094748 <a title="185-lsi-8" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>9 0.55892444 <a title="185-lsi-9" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>10 0.55460238 <a title="185-lsi-10" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>11 0.54510224 <a title="185-lsi-11" href="./nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">2 nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<p>12 0.5225029 <a title="185-lsi-12" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>13 0.52119666 <a title="185-lsi-13" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>14 0.5181638 <a title="185-lsi-14" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>15 0.51526397 <a title="185-lsi-15" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>16 0.50082791 <a title="185-lsi-16" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>17 0.50023609 <a title="185-lsi-17" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>18 0.50023198 <a title="185-lsi-18" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>19 0.49779344 <a title="185-lsi-19" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>20 0.48796573 <a title="185-lsi-20" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (17, 0.016), (21, 0.018), (37, 0.223), (38, 0.109), (39, 0.032), (42, 0.022), (54, 0.014), (55, 0.043), (60, 0.015), (74, 0.164), (76, 0.158), (80, 0.033), (92, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84496921 <a title="185-lda-1" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>Author: Elad Mezuman, Yair Weiss</p><p>Abstract: Although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or “canonical” view of objects. This phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally. Moreover, the explanation for why humans prefer the canonical view over other views remains elusive. In this paper we ask: Can we use Internet image collections to learn more about canonical views? We start by manually ﬁnding the most common view in the results returned by Internet search engines when queried with the objects used in psychophysical experiments. Our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments. We also present a simple method to ﬁnd the most likely view in an image collection and apply it to hundreds of categories. Using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories. 1</p><p>2 0.7374934 <a title="185-lda-2" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>3 0.73673701 <a title="185-lda-3" href="./nips-2012-The_Lov%C3%A1sz_%CF%91_function%2C_SVMs_and_finding_large_dense_subgraphs.html">337 nips-2012-The Lovász ϑ function, SVMs and finding large dense subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: The Lov´ sz ϑ function of a graph, a fundamental tool in combinatorial optimizaa tion and approximation algorithms, is computed by solving a SDP. In this paper we establish that the Lov´ sz ϑ function is equivalent to a kernel learning problem a related to one class SVM. This interesting connection opens up many opportunities bridging graph theoretic algorithms and machine learning. We show that there exist graphs, which we call SVM − ϑ graphs, on which the Lov´ sz ϑ function a can be approximated well by a one-class SVM. This leads to novel use of SVM techniques for solving algorithmic problems in large graphs e.g. identifying a √ 1 planted clique of size Θ( n) in a random graph G(n, 2 ). A classic approach for this problem involves computing the ϑ function, however it is not scalable due to SDP computation. We show that the random graph with a planted clique is an example of SVM − ϑ graph. As a consequence a SVM based approach easily identiﬁes the clique in large graphs and is competitive with the state-of-the-art. We introduce the notion of common orthogonal labelling and show that it can be computed by solving a Multiple Kernel learning problem. It is further shown that such a labelling is extremely useful in identifying a large common dense subgraph in multiple graphs, which is known to be a computationally difﬁcult problem. The proposed algorithm achieves an order of magnitude scalability compared to state of the art methods. 1</p><p>4 0.73518181 <a title="185-lda-4" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>Author: Kei Wakabayashi, Takao Miura</p><p>Abstract: Hierarchical Hidden Markov Models (HHMMs) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data. However, existing HHMM parameter estimation methods require large computations of time complexity O(T N 2D ) at least for model inference, where D is the depth of the hierarchy, N is the number of states in each level, and T is the sequence length. In this paper, we propose a new inference method of HHMMs for which the time complexity is O(T N D+1 ). A key idea of our algorithm is application of the forward-backward algorithm to state activation probabilities. The notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of HHMMs, enables us to conduct model inference efﬁciently. We present some experiments to demonstrate that our proposed method works more efﬁciently to estimate HHMM parameters than do some existing methods such as the ﬂattening method and Gibbs sampling method. 1</p><p>5 0.7334438 <a title="185-lda-5" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>Author: Aaron Wilson, Alan Fern, Prasad Tadepalli</p><p>Abstract: We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent’s goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efﬁcient than random selection. 1</p><p>6 0.73326832 <a title="185-lda-6" href="./nips-2012-Locally_Uniform_Comparison_Image_Descriptor.html">202 nips-2012-Locally Uniform Comparison Image Descriptor</a></p>
<p>7 0.73271334 <a title="185-lda-7" href="./nips-2012-The_Time-Marginalized_Coalescent_Prior_for_Hierarchical_Clustering.html">339 nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</a></p>
<p>8 0.73141533 <a title="185-lda-8" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>9 0.72672063 <a title="185-lda-9" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>10 0.72324187 <a title="185-lda-10" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>11 0.71975404 <a title="185-lda-11" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>12 0.71433431 <a title="185-lda-12" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>13 0.71075863 <a title="185-lda-13" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>14 0.7098912 <a title="185-lda-14" href="./nips-2012-Collaborative_Ranking_With_17_Parameters.html">75 nips-2012-Collaborative Ranking With 17 Parameters</a></p>
<p>15 0.6985873 <a title="185-lda-15" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>16 0.69588023 <a title="185-lda-16" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>17 0.69396222 <a title="185-lda-17" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>18 0.6933887 <a title="185-lda-18" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>19 0.69218105 <a title="185-lda-19" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>20 0.6892966 <a title="185-lda-20" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
