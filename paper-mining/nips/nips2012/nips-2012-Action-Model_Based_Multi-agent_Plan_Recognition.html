<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 nips-2012-Action-Model Based Multi-agent Plan Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-31" href="#">nips2012-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 nips-2012-Action-Model Based Multi-agent Plan Recognition</h1>
<br/><p>Source: <a title="nips-2012-31-pdf" href="http://papers.nips.cc/paper/4812-action-model-based-multi-agent-plan-recognition.pdf">pdf</a></p><p>Author: Hankz H. Zhuo, Qiang Yang, Subbarao Kambhampati</p><p>Abstract: Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. However, collecting a library of team plans to ensure adequate coverage is often difﬁcult and costly. In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. We assume instead that a set of action models are available. Such models are often already created to describe domain physics; i.e., the preconditions and effects of effects actions. We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. We encode the resulting MAPR problem as a satisﬁability problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. Our approach also allows for incompleteness in the observed plan traces. Our empirical studies demonstrate that our algorithm is both effective and efﬁcient in comparison to state-of-the-art MAPR methods based on plan libraries. 1</p><p>Reference: <a title="nips-2012-31-reference" href="../nips2012_reference/nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. [sent-7, score-2.395]
</p><p>2 Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. [sent-8, score-0.917]
</p><p>3 However, collecting a library of team plans to ensure adequate coverage is often difﬁcult and costly. [sent-9, score-1.324]
</p><p>4 In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. [sent-10, score-1.137]
</p><p>5 We assume instead that a set of action models are available. [sent-11, score-0.121]
</p><p>6 Such models are often already created to describe domain physics; i. [sent-12, score-0.019]
</p><p>7 We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. [sent-15, score-2.047]
</p><p>8 We encode the resulting MAPR problem as a satisﬁability problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. [sent-16, score-0.025]
</p><p>9 Our approach also allows for incompleteness in the observed plan traces. [sent-17, score-0.243]
</p><p>10 Our empirical studies demonstrate that our algorithm is both effective and efﬁcient in comparison to state-of-the-art MAPR methods based on plan libraries. [sent-18, score-0.214]
</p><p>11 1  Introduction  Multi-Agent Plan Recognition (MAPR) seeks an explanation of observed team-action traces. [sent-19, score-0.029]
</p><p>12 From the activity sequences of a set of agents, MAPR aims to identify the dynamic team structures and team behaviors of agents. [sent-20, score-1.572]
</p><p>13 The MAPR problem has important applications in analyzing data from automated monitoring, situation awareness, intelligence surveillance and analysis [4]. [sent-21, score-0.022]
</p><p>14 Many approaches have been proposed in the past to automatically recognize team plans given an observed team trace as input. [sent-22, score-2.046]
</p><p>15 They solved MAPR problems using a ﬁrst-cut approach, provided that a fully observed team trace and a library of full team plans were given as input. [sent-25, score-2.004]
</p><p>16 To relax the full observability constraint, Zhuo and Li [19] proposed a MARS system to recognize team plans based on partially observed team traces and libraries of partial team plans. [sent-26, score-2.901]
</p><p>17 1  Despite the success of these previous approaches, they all assume that a library of team plans has been collected beforehand and provided as input. [sent-27, score-1.256]
</p><p>18 However, there are many applications where collecting and maintaining a library of team plans is difﬁcult and costly. [sent-28, score-1.267]
</p><p>19 For example, in military operations, it is difﬁcult and expensive to collect team plans, since activities of team-mates may consume lots of resources such as ammunition and human labor. [sent-29, score-0.856]
</p><p>20 Collecting a smaller library is not an option since it is infeasible to recognize team plans if they are not covered by the library. [sent-30, score-1.377]
</p><p>21 It is thus useful to design approaches for solving the MAPR problem where we do not require libraries of team plans to be known. [sent-31, score-1.207]
</p><p>22 In this paper, we advocate replacing the plan library with a compact action model of the domain. [sent-32, score-0.46]
</p><p>23 In contrast to plan libraries, action models are easier to specify (in terms of preconditions and effects of each type of activity). [sent-33, score-0.469]
</p><p>24 Moreover, in principle action models provide full coverage to recognize any team plans. [sent-34, score-0.983]
</p><p>25 The speciﬁc algorithmic framework we develop is called DARE, which stands for Domain- model based multi-Agent REcognition, to recognize multi-agent plans. [sent-35, score-0.137]
</p><p>26 DARE does not require plan libraries to be given as input. [sent-36, score-0.306]
</p><p>27 Instead, DARE takes as input a team trace and a set of action models. [sent-37, score-0.871]
</p><p>28 DARE also allows the observed traces to be incomplete, i. [sent-38, score-0.113]
</p><p>29 , there can be To ﬁll these gaps, DARE leverages all possible constraints both from the plan traces and from its knowledge of how a plan works in terms of its causal structure. [sent-40, score-0.538]
</p><p>30 To do this, DARE ﬁrst builds a set of hard constraints that encode the correctness property of the team plans, and a set of soft constraints that encode the optimal utility property of team plans based on the input team trace and action models. [sent-41, score-2.778]
</p><p>31 After that, it solves all these constraints using a state-of-the-art weighted MAX-SAT solver, such as MaxSatz [10], and converts the solution to a set of team plans as output. [sent-42, score-1.159]
</p><p>32 In the next section, we ﬁrst introduce the related work including single agent plan recognition and multi-agent plan recognition, and then give our formulation of the MAPR problem. [sent-44, score-0.534]
</p><p>33 2  Related work  The plan recognition problem has been addressed by many researchers. [sent-47, score-0.281]
</p><p>34 Kautz and Allen proposed an approach to recognize plans based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free rule in an “action grammar” [9]. [sent-48, score-0.62]
</p><p>35 presented approaches to probabilistic plan recognition problems [5, 7]. [sent-50, score-0.263]
</p><p>36 Instead of using a library of plans, Ramrez and Geffner [12] proposed an approach to solving the plan recognition problem using slightly modiﬁed planning algorithms, assuming the action models were given as input. [sent-51, score-0.494]
</p><p>37 Note that action models can be created by experts or learnt by previous systems, such as ARMS [18] and LAMP [20]. [sent-52, score-0.14]
</p><p>38 Singla and Mooney proposed an approach to abductive reasoning using a ﬁrst-order probabilistic logic to recognize plans [15]. [sent-53, score-0.577]
</p><p>39 Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories [1]. [sent-54, score-0.369]
</p><p>40 Ramirez and Geffner exploited off-the-shelf classical planners to recognize probabilistic plans [13]. [sent-55, score-0.583]
</p><p>41 Despite the success of these systems, a limitation is that they all focus only on single agent plans. [sent-56, score-0.072]
</p><p>42 For multi-agent plan recognition, Sukthankar and Sycara presented an approach that leveraged several types of agent resource dependencies and temporal ordering constraints in the plan library to prune the size of the plan library considered for each observation trace [16]. [sent-57, score-1.053]
</p><p>43 Avrahami-Zilberbrand and Kaminka preferred a library of single agent plans to team plans, but identiﬁed dynamic teams based on the assumption that all agents in a team executing the same plan under the temporal constraints of that plan [2]. [sent-58, score-2.565]
</p><p>44 The constraint on activities of the agents that can form a team can be severely limiting when team-mates can execute coordinated but different behaviors. [sent-59, score-0.977]
</p><p>45 proposed a probabilistic model based on conditional random ﬁelds to automatically recognize the composition of teams and team activities in relation to a plan [11]. [sent-61, score-1.178]
</p><p>46 In these systems, although coordinated activities can be recognized, they either assume there is a set of real-world GPS data available, or assume that team traces and team plans can be fully observed. [sent-62, score-2.043]
</p><p>47 In this paper, we allow that: (1) agents can execute coordinated different activities in a team, (2) team traces can be partial, and (3) neither GPS data nor team plans are needed. [sent-63, score-2.176]
</p><p>48 2  3  Problem Deﬁnition  We ﬁrst deﬁne a team trace. [sent-64, score-0.69]
</p><p>49 , n } be a set of agents, and O = [otj ] be an observed team trace. [sent-68, score-0.719]
</p><p>50 Let otj be the observed activity executed by agent j at time step t, where 0 < t  T and 0 < j  n. [sent-69, score-0.25]
</p><p>51 A team trace O is partial, if some elements in O are empty (denoted by null), i. [sent-70, score-0.771]
</p><p>52 In the STRIPS language [6], an action model is a tuple ha, Pre(a), Add(a), Del(a)i, where a is an action name with zero or more parameters, Pre(a) is a list of preconditions of a, Add(a) is a list of add effects, and Del(a) is a list of deleting effects. [sent-74, score-0.485]
</p><p>53 An action name with zero of more parameters is called an activity. [sent-76, score-0.141]
</p><p>54 An observed activity otj in a partial team trace O is either an instantiated action of A or noop or null, where noop is an empty activity that does nothing. [sent-77, score-1.299]
</p><p>55 An initial state s0 is a set of propositions that describes a closed world state from which the team trace O starts to be observed. [sent-78, score-0.826]
</p><p>56 In other words, activities at time step t = 0 can be applied in the initial state s0 . [sent-79, score-0.102]
</p><p>57 When we say an activity can be applied in a state, we mean the activity’s preconditions are satisﬁed by the state. [sent-80, score-0.19]
</p><p>58 A set of goals G, each of which is a set of propositions, describes the probable targets of the team trace. [sent-81, score-0.706]
</p><p>59 We assume s0 and G can both be observed by sensing devices. [sent-82, score-0.029]
</p><p>60 A team is composed of a subset of agents 0 = { j1 , j2 , . [sent-83, score-0.779]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('team', 0.69), ('plans', 0.425), ('mapr', 0.279), ('plan', 0.214), ('dare', 0.203), ('recognize', 0.137), ('action', 0.121), ('library', 0.11), ('preconditions', 0.102), ('libraries', 0.092), ('agents', 0.089), ('activity', 0.088), ('activities', 0.087), ('traces', 0.084), ('otj', 0.076), ('coordinated', 0.067), ('trace', 0.06), ('agent', 0.057), ('geffner', 0.051), ('noop', 0.051), ('recognition', 0.049), ('kautz', 0.045), ('zhuo', 0.045), ('execute', 0.044), ('behaviors', 0.042), ('collecting', 0.042), ('del', 0.035), ('teams', 0.035), ('arizona', 0.035), ('coverage', 0.035), ('effects', 0.032), ('gps', 0.032), ('kong', 0.031), ('propositions', 0.03), ('hong', 0.029), ('pre', 0.029), ('sequences', 0.029), ('observed', 0.029), ('recognizing', 0.029), ('list', 0.028), ('constraints', 0.026), ('encode', 0.025), ('partial', 0.024), ('null', 0.024), ('atk', 0.022), ('adequate', 0.022), ('ammunition', 0.022), ('bui', 0.022), ('guangzhou', 0.022), ('lamp', 0.022), ('masato', 0.022), ('sadilek', 0.022), ('singla', 0.022), ('strips', 0.022), ('sukthankar', 0.022), ('surveillance', 0.022), ('relax', 0.022), ('empty', 0.021), ('ark', 0.021), ('consume', 0.021), ('gal', 0.021), ('planners', 0.021), ('name', 0.02), ('deleting', 0.019), ('jm', 0.019), ('tempe', 0.019), ('created', 0.019), ('allen', 0.018), ('noah', 0.018), ('observability', 0.018), ('aims', 0.018), ('add', 0.018), ('military', 0.018), ('awareness', 0.018), ('converts', 0.018), ('lots', 0.018), ('addressed', 0.018), ('laboratories', 0.017), ('amir', 0.017), ('leveraged', 0.017), ('qiang', 0.016), ('gaps', 0.016), ('organize', 0.016), ('beforehand', 0.016), ('grammar', 0.016), ('monitoring', 0.016), ('prune', 0.016), ('describes', 0.016), ('roles', 0.016), ('dynamic', 0.015), ('arms', 0.015), ('logic', 0.015), ('automatically', 0.015), ('success', 0.015), ('advocate', 0.015), ('covered', 0.015), ('resource', 0.015), ('state', 0.015), ('recognized', 0.015), ('ha', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="31-tfidf-1" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>Author: Hankz H. Zhuo, Qiang Yang, Subbarao Kambhampati</p><p>Abstract: Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. However, collecting a library of team plans to ensure adequate coverage is often difﬁcult and costly. In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. We assume instead that a set of action models are available. Such models are often already created to describe domain physics; i.e., the preconditions and effects of effects actions. We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. We encode the resulting MAPR problem as a satisﬁability problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. Our approach also allows for incompleteness in the observed plan traces. Our empirical studies demonstrate that our algorithm is both effective and efﬁcient in comparison to state-of-the-art MAPR methods based on plan libraries. 1</p><p>2 0.061499603 <a title="31-tfidf-2" href="./nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">141 nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<p>Author: Jingrui He, Hanghang Tong, Qiaozhu Mei, Boleslaw Szymanski</p><p>Abstract: Diversiﬁed ranking is a fundamental task in machine learning. It is broadly applicable in many real world problems, e.g., information retrieval, team assembling, product search, etc. In this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples. We formulate it as an optimization problem and show that in general it is NP-hard. Then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to ﬁnd the (1 − 1/e) near-optimal solution. Experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.</p><p>3 0.059623092 <a title="31-tfidf-3" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>Author: Trung Nguyen, Tomi Silander, Tze Y. Leong</p><p>Abstract: We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efﬁcient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without predeﬁned mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. 1</p><p>4 0.052967887 <a title="31-tfidf-4" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>5 0.050230514 <a title="31-tfidf-5" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>Author: Weixin Li, Nuno Vasconcelos</p><p>Abstract: In this work, we consider the problem of modeling the dynamic structure of human activities in the attributes space. A video sequence is Ä?Ĺš rst represented in a semantic feature space, where each feature encodes the probability of occurrence of an activity attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both the binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining binary observation variables with a hidden Gauss-Markov state process. In this way, it integrates the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by a popular LDS learning method from dynamic textures, is proposed. A similarity measure between BDSs, which generalizes the BinetCauchy kernel for LDS, is then introduced and used to design activity classiÄ?Ĺš ers. The proposed method is shown to outperform similar classiÄ?Ĺš ers derived from the kernel dynamic system (KDS) and state-of-the-art approaches for dynamics-based or attribute-based action recognition. 1</p><p>6 0.049182065 <a title="31-tfidf-6" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>7 0.037332103 <a title="31-tfidf-7" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>8 0.036542505 <a title="31-tfidf-8" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>9 0.031356052 <a title="31-tfidf-9" href="./nips-2012-Matrix_reconstruction_with_the_local_max_norm.html">208 nips-2012-Matrix reconstruction with the local max norm</a></p>
<p>10 0.030508783 <a title="31-tfidf-10" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>11 0.029510545 <a title="31-tfidf-11" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>12 0.027918825 <a title="31-tfidf-12" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>13 0.026994804 <a title="31-tfidf-13" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>14 0.02688797 <a title="31-tfidf-14" href="./nips-2012-Rational_inference_of_relative_preferences.html">288 nips-2012-Rational inference of relative preferences</a></p>
<p>15 0.024622317 <a title="31-tfidf-15" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>16 0.024459507 <a title="31-tfidf-16" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>17 0.024312923 <a title="31-tfidf-17" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>18 0.023404753 <a title="31-tfidf-18" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>19 0.023365853 <a title="31-tfidf-19" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>20 0.020738576 <a title="31-tfidf-20" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.05), (1, -0.052), (2, -0.026), (3, 0.003), (4, -0.004), (5, -0.003), (6, 0.005), (7, -0.003), (8, 0.002), (9, 0.023), (10, -0.01), (11, 0.011), (12, 0.021), (13, -0.019), (14, 0.027), (15, 0.017), (16, 0.002), (17, 0.034), (18, 0.025), (19, -0.031), (20, 0.023), (21, -0.008), (22, -0.072), (23, 0.001), (24, 0.018), (25, -0.015), (26, 0.009), (27, 0.011), (28, 0.003), (29, 0.045), (30, 0.04), (31, 0.008), (32, 0.017), (33, 0.015), (34, -0.023), (35, 0.01), (36, 0.003), (37, -0.053), (38, 0.0), (39, -0.01), (40, 0.019), (41, -0.031), (42, 0.018), (43, 0.046), (44, -0.056), (45, -0.004), (46, 0.049), (47, 0.017), (48, -0.021), (49, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95299858 <a title="31-lsi-1" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>Author: Hankz H. Zhuo, Qiang Yang, Subbarao Kambhampati</p><p>Abstract: Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. However, collecting a library of team plans to ensure adequate coverage is often difﬁcult and costly. In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. We assume instead that a set of action models are available. Such models are often already created to describe domain physics; i.e., the preconditions and effects of effects actions. We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. We encode the resulting MAPR problem as a satisﬁability problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. Our approach also allows for incompleteness in the observed plan traces. Our empirical studies demonstrate that our algorithm is both effective and efﬁcient in comparison to state-of-the-art MAPR methods based on plan libraries. 1</p><p>2 0.59340167 <a title="31-lsi-2" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>Author: Trung Nguyen, Tomi Silander, Tze Y. Leong</p><p>Abstract: We study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning. We address the challenges of transfer learning in heterogeneous environments with varying tasks. We present an efﬁcient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks. Without predeﬁned mapping strategies, we introduce a general approach to support transfer learning across different state spaces. We demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains. 1</p><p>3 0.54142731 <a title="31-lsi-3" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>4 0.4858681 <a title="31-lsi-4" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>Author: Weixin Li, Nuno Vasconcelos</p><p>Abstract: In this work, we consider the problem of modeling the dynamic structure of human activities in the attributes space. A video sequence is Ä?Ĺš rst represented in a semantic feature space, where each feature encodes the probability of occurrence of an activity attribute at a given time. A generative model, denoted the binary dynamic system (BDS), is proposed to learn both the distribution and dynamics of different activities in this space. The BDS is a non-linear dynamic system, which extends both the binary principal component analysis (PCA) and classical linear dynamic systems (LDS), by combining binary observation variables with a hidden Gauss-Markov state process. In this way, it integrates the representation power of semantic modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes. An algorithm for learning BDS parameters, inspired by a popular LDS learning method from dynamic textures, is proposed. A similarity measure between BDSs, which generalizes the BinetCauchy kernel for LDS, is then introduced and used to design activity classiÄ?Ĺš ers. The proposed method is shown to outperform similar classiÄ?Ĺš ers derived from the kernel dynamic system (KDS) and state-of-the-art approaches for dynamics-based or attribute-based action recognition. 1</p><p>5 0.48565701 <a title="31-lsi-5" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>Author: Dongho Kim, Kee-eung Kim, Pascal Poupart</p><p>Abstract: In this paper, we consider Bayesian reinforcement learning (BRL) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected longterm total reward. In order to formalize cost-sensitive exploration, we use the constrained Markov decision process (CMDP) as the model of the environment, in which we can naturally encode exploration requirements using the cost function. We extend BEETLE, a model-based BRL method, for learning in the environment with cost constraints. We demonstrate the cost-sensitive exploration behaviour in a number of simulated problems. 1</p><p>6 0.46042839 <a title="31-lsi-6" href="./nips-2012-Bayesian_Hierarchical_Reinforcement_Learning.html">51 nips-2012-Bayesian Hierarchical Reinforcement Learning</a></p>
<p>7 0.44415653 <a title="31-lsi-7" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>8 0.44113195 <a title="31-lsi-8" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>9 0.42617288 <a title="31-lsi-9" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>10 0.41201735 <a title="31-lsi-10" href="./nips-2012-How_Prior_Probability_Influences_Decision_Making%3A_A_Unifying_Probabilistic_Model.html">153 nips-2012-How Prior Probability Influences Decision Making: A Unifying Probabilistic Model</a></p>
<p>11 0.40182659 <a title="31-lsi-11" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>12 0.39038229 <a title="31-lsi-12" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>13 0.39034736 <a title="31-lsi-13" href="./nips-2012-A_latent_factor_model_for_highly_multi-relational_data.html">22 nips-2012-A latent factor model for highly multi-relational data</a></p>
<p>14 0.37600523 <a title="31-lsi-14" href="./nips-2012-Approximating_Equilibria_in_Sequential_Auctions_with_Incomplete_Information_and_Multi-Unit_Demand.html">45 nips-2012-Approximating Equilibria in Sequential Auctions with Incomplete Information and Multi-Unit Demand</a></p>
<p>15 0.37596914 <a title="31-lsi-15" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>16 0.37379241 <a title="31-lsi-16" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>17 0.37218249 <a title="31-lsi-17" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>18 0.36838475 <a title="31-lsi-18" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>19 0.3650701 <a title="31-lsi-19" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>20 0.35717848 <a title="31-lsi-20" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (15, 0.416), (21, 0.025), (36, 0.022), (38, 0.057), (39, 0.012), (42, 0.048), (54, 0.048), (55, 0.015), (74, 0.042), (76, 0.098), (80, 0.051), (92, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97913337 <a title="31-lda-1" href="./nips-2012-Semi-Supervised_Domain_Adaptation_with_Non-Parametric_Copulas.html">308 nips-2012-Semi-Supervised Domain Adaptation with Non-Parametric Copulas</a></p>
<p>Author: David Lopez-paz, Jose M. Hernández-lobato, Bernhard Schölkopf</p><p>Abstract: A new framework based on the theory of copulas is proposed to address semisupervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efﬁcacy of the proposed approach when compared to state-of-the-art techniques. 1</p><p>same-paper 2 0.72182292 <a title="31-lda-2" href="./nips-2012-Action-Model_Based_Multi-agent_Plan_Recognition.html">31 nips-2012-Action-Model Based Multi-agent Plan Recognition</a></p>
<p>Author: Hankz H. Zhuo, Qiang Yang, Subbarao Kambhampati</p><p>Abstract: Multi-Agent Plan Recognition (MAPR) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents. Previous MAPR approaches required a library of team activity sequences (team plans) be given as input. However, collecting a library of team plans to ensure adequate coverage is often difﬁcult and costly. In this paper, we relax this constraint, so that team plans are not required to be provided beforehand. We assume instead that a set of action models are available. Such models are often already created to describe domain physics; i.e., the preconditions and effects of effects actions. We propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans. We encode the resulting MAPR problem as a satisﬁability problem and solve the problem using a state-of-the-art weighted MAX-SAT solver. Our approach also allows for incompleteness in the observed plan traces. Our empirical studies demonstrate that our algorithm is both effective and efﬁcient in comparison to state-of-the-art MAPR methods based on plan libraries. 1</p><p>3 0.53111851 <a title="31-lda-3" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>Author: Geoffrey E. Hinton, Ruslan Salakhutdinov</p><p>Abstract: We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models. 1</p><p>4 0.43356571 <a title="31-lda-4" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>Author: Odalric-ambrym Maillard</p><p>Abstract: This paper aims to take a step forwards making the term “intrinsic motivation” from reinforcement learning theoretically well founded, focusing on curiositydriven learning. To that end, we consider the setting where, a ﬁxed partition P of a continuous space X being given, and a process ν deﬁned on X being unknown, we are asked to sequentially decide which cell of the partition to select as well as where to sample ν in that cell, in order to minimize a loss function that is inspired from previous work on curiosity-driven learning. The loss on each cell consists of one term measuring a simple worst case quadratic sampling error, and a penalty term proportional to the range of the variance in that cell. The corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region, and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem. The resulting procedure, called Hierarchical Optimistic Region SElection driven by Curiosity (HORSE.C) is provided together with a ﬁnite-time regret analysis. 1</p><p>5 0.33687004 <a title="31-lda-5" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>6 0.32983392 <a title="31-lda-6" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>7 0.32696733 <a title="31-lda-7" href="./nips-2012-Semiparametric_Principal_Component_Analysis.html">310 nips-2012-Semiparametric Principal Component Analysis</a></p>
<p>8 0.32674265 <a title="31-lda-8" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>9 0.32487142 <a title="31-lda-9" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>10 0.32432199 <a title="31-lda-10" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>11 0.323524 <a title="31-lda-11" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>12 0.32104641 <a title="31-lda-12" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>13 0.32101458 <a title="31-lda-13" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>14 0.32077569 <a title="31-lda-14" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>15 0.3186411 <a title="31-lda-15" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>16 0.31853974 <a title="31-lda-16" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>17 0.3181704 <a title="31-lda-17" href="./nips-2012-Non-linear_Metric_Learning.html">242 nips-2012-Non-linear Metric Learning</a></p>
<p>18 0.31791312 <a title="31-lda-18" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>19 0.31780037 <a title="31-lda-19" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>20 0.31770912 <a title="31-lda-20" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
