<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-332" href="#">nips2012-332</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</h1>
<br/><p>Source: <a title="nips-2012-332-pdf" href="http://papers.nips.cc/paper/4583-symmetric-correspondence-topic-models-for-multilingual-text-analysis.pdf">pdf</a></p><p>Author: Kosuke Fukumasu, Koji Eguchi, Eric P. Xing</p><p>Abstract: Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. 1</p><p>Reference: <a title="nips-2012-332-reference" href="../nips2012_reference/nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. [sent-12, score-0.753]
</p><p>2 Other topic models that were originally proposed for structured data are also applicable to multilingual documents. [sent-13, score-0.535]
</p><p>3 Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. [sent-14, score-0.738]
</p><p>4 We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. [sent-15, score-0.67]
</p><p>5 We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. [sent-16, score-0.947]
</p><p>6 In topic modeling, each document is represented as a mixture of topics, where each topic is represented as a word distribution. [sent-18, score-0.497]
</p><p>7 Most topic models assume that texts are monolingual; however, some can capture statistical dependencies between multiple classes of representations and can be used for multilingual parallel or comparable documents. [sent-20, score-0.629]
</p><p>8 Here, a parallel document is a merged document consisting of multiple language parts that are translations from one language to another, sometimes including sentence-to-sentence or word-to-word alignments. [sent-21, score-0.691]
</p><p>9 A comparable document is a merged document consisting of multiple language parts that are not translations of each other but instead describe similar concepts and events. [sent-22, score-0.477]
</p><p>10 Recently published multilingual topic models [3, 4], which are the equivalent of Conditionally Independent LDA (CI-LDA) [5, 6], can discover latent topics among parallel or comparable documents. [sent-23, score-0.753]
</p><p>11 It can control the proportions of languages in each multilingual topic. [sent-25, score-0.588]
</p><p>12 However, both CI-LDA and SwitchLDA preserve dependencies between languages only by sharing per-document multinomial distributions over latent topics, and accordingly the resulting dependencies are relatively weak. [sent-26, score-0.346]
</p><p>13 In this sense, visual features can be said to be the pivot in modeling annotated image data. [sent-31, score-0.528]
</p><p>14 The pivot language selected is sensitive to the quality of the multilingual topics estimated with CorrLDA. [sent-34, score-1.278]
</p><p>15 For example, a translation of a Japanese book into English would presumably have a pivot to the Japanese book, but a set of international news stories would have pivots that diﬀer based on the country an article is about. [sent-35, score-0.621]
</p><p>16 It is often diﬃcult to appropriately select the pivot language. [sent-36, score-0.521]
</p><p>17 To address this problem, which we call the pivot problem, we propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control the pivot language, in an extension of CorrLDA. [sent-37, score-1.176]
</p><p>18 Our SymCorrLDA addresses the problem of CorrLDA and can select an appropriate pivot language by inference from the data. [sent-38, score-0.738]
</p><p>19 , CI-LDA, SwitchLDA, CorrLDA, and our SymCorrLDA, as well as LDA, using comparable articles in diﬀerent languages (English, Japanese, and Spanish) extracted from Wikipedia. [sent-41, score-0.313]
</p><p>20 We ﬁrst demonstrate through experiments that CorrLDA outperforms the other existing multilingual topic models mentioned, and then show that our SymCorrLDA works more eﬀectively than CorrLDA in any case of selecting a pivot language. [sent-42, score-1.041]
</p><p>21 2 Multilingual Topic Models with Multilingual Comparable Documents Bilingual topic models for bilingual parallel documents that have word-to-word alignments have been developed, such as those by [8]. [sent-43, score-0.439]
</p><p>22 In contrast, we focus on analyzing dependencies among languages by modeling multilingual comparable documents, each of which consists of multiple language parts that are not translations of each other but instead describe similar concepts and events. [sent-45, score-0.909]
</p><p>23 The target documents can be parallel documents, but word-to-word alignments are not taken into account in the topic modeling. [sent-46, score-0.299]
</p><p>24 Some other researchers explored diﬀerent types of multilingual topic models that are based on the premise of using multilingual dictionaries or WordNet [9, 10, 11]. [sent-47, score-0.906]
</p><p>25 In contrast, CI-LDA and SwitchLDA only require multilingual comparable documents that can be easily obtained, such as from Wikipedia, when we use those models for multilingual text analysis. [sent-48, score-0.888]
</p><p>26 Below, we introduce LDA-style topic models that handle multiple classes and can be applied to multilingual comparable documents for the above-mentioned purposes. [sent-50, score-0.669]
</p><p>27 The CI-LDA framework was used to model multilingual parallel or comparable documents by [3] and [4]. [sent-53, score-0.528]
</p><p>28 D, T , and Nd respectively indicate the number of documents, number of topics, and number of word tokens that appear in a speciﬁc language part in a document d. [sent-55, score-0.423]
</p><p>29 The superscript ‘(·)’ indicates the variables corresponding to a speciﬁc language part in a document d. [sent-56, score-0.333]
</p><p>30 For all D documents, sample θd ∼ Dirichlet(α) For all T topics and for all L languages, sample ϕ(ℓ) ∼ Dirichlet(β (ℓ) ) t (ℓ) For each of the Nd words w(ℓ) in language ℓ (ℓ ∈ {1, · · · , L}) of document d: i a. [sent-61, score-0.518]
</p><p>31 CI-LDA preserves dependencies between languages only by sharing the multinomial distributions with parameters θd . [sent-64, score-0.287]
</p><p>32 Accordingly, there are substantial chances that some topics are assigned only to a speciﬁc language part in each document, and the resulting dependencies are relatively weak. [sent-65, score-0.417]
</p><p>33 2  SwitchLDA  Similarly to CI-LDA, SwitchLDA [6] can be applied to multilingual comparable documents. [sent-67, score-0.412]
</p><p>34 However, diﬀerent from CI-LDA, SwitchLDA can adjust the proportions of multiple diﬀerent languages for each topic, according to a binomial distribution for bilingual data or a multinomial distribution for data of more than three languages. [sent-68, score-0.429]
</p><p>35 Sample a word wi ∼ Multinomial(ϕ(si ) ) zi  Here, ψt indicates a multinomial parameter to adjust the proportions of L diﬀerent languages for topic t. [sent-79, score-0.573]
</p><p>36 Therefore, SwitchLDA may represent multilingual topics more ﬂexibly; however, it still has the drawback that the dependencies between languages are relatively weak. [sent-82, score-0.738]
</p><p>37 3  Correspondence LDA (CorrLDA)  CorrLDA [7] can also be applied to multilingual comparable documents. [sent-84, score-0.412]
</p><p>38 In the multilingual setting, this model ﬁrst generates topics for one language part of a document. [sent-85, score-0.755]
</p><p>39 For the other languages, the model then uses the topics that were already generated in the pivot language. [sent-87, score-0.644]
</p><p>40 Figure 2(a) shows a graphical model representation of CorrLDA assuming L (ℓ) languages, when p is the pivot language that is speciﬁed in advance. [sent-88, score-0.751]
</p><p>41 Here, Nd (ℓ ∈ {p, 2, · · · , L}) denotes the number of words in language ℓ in document d. [sent-89, score-0.35]
</p><p>42 (p) For all D documents’ pivot language parts, sample θd ∼ Dirichlet(α(p) ) For all T topics and for all L languages (including the pivot language), sample ϕ(ℓ) ∼ Dirichlet(β (ℓ) ) t (p) For each of the Nd words w(p) in the pivot language p of document d: i (p) a. [sent-93, score-2.467]
</p><p>43 (ℓ) For each of the Nd words w(ℓ) in language ℓ (ℓ ∈ {2, · · · , L}) of document d: i ( ) a. [sent-96, score-0.35]
</p><p>44 Sample a word w(ℓ) ∼ Multinomial(ϕ(ℓ) ) (ℓ) i  Nd  yi  This model can capture more direct dependencies between languages, due to the constraints that topics have to be selected from the topics selected in the pivot language parts. [sent-98, score-1.161]
</p><p>45 However, when CorrLDA is applied to multilingual documents, a pivot language must be speciﬁed in advance. [sent-99, score-1.109]
</p><p>46 Moreover, the pivot language selected is sensitive to the quality of the multilingual topics estimated with CorrLDA. [sent-100, score-1.278]
</p><p>47 3 Symmetric Correspondence Topic Models When CorrLDA is applied to parallel or comparable documents, this model ﬁrst generates topics for one language part of a document, which we refer to this language as a pivot language. [sent-101, score-1.186]
</p><p>48 For the other languages, the model then uses the topics that were already generated in the pivot language. [sent-102, score-0.644]
</p><p>49 Since the pivot language may diﬀer based on the subject, such as the country a document is about, it is often diﬃcult to appropriately select the pivot language. [sent-104, score-1.356]
</p><p>50 This model generates a ﬂag that speciﬁes a pivot language for each word, adjusting the probability of being pivot languages in each language part of a document according to a binomial distribution for bilingual data or a multinomial distribution for data of more than three languages. [sent-106, score-1.985]
</p><p>51 In other words, SymCorrLDA estimates from the data the best pivot language at the word level in each document. [sent-107, score-0.823]
</p><p>52 The pivot language ﬂags may be assigned to the words in the originally written portions in each language, since the original portions may be described conﬁdently and with rich vocabulary. [sent-108, score-0.789]
</p><p>53 For all D documents: (ℓ) For all L languages, sample θd ∼ Dirichlet(α(ℓ) ) Sample πd ∼ Dirichlet(γ) For all T topics and for all L languages, sample ϕ(ℓ) ∼ Dirichlet(β (ℓ) ) t (ℓ) For each of the Nd words w(ℓ) in language ℓ (ℓ ∈ {1, · · · , L}) of document d: i a. [sent-111, score-0.518]
</p><p>54 Sample a pivot language ﬂag xi(ℓ) ∼ Multinomial(πd ) (ℓ) b. [sent-112, score-0.738]
</p><p>55 zi  i  yi  i  The pivot language ﬂag xi(ℓ) = ℓ for an arbitrary language ℓ indicates that the pivot language for the word wi(ℓ) is its own language ℓ, and xi(ℓ) = m indicates that the pivot language for w(ℓ) is another i language m diﬀerent from its own language ℓ. [sent-119, score-3.28]
</p><p>56 W (·) and Nd respectively indicate the total number of i i vocabulary words (word types) in the speciﬁed language, and the number of word tokens that appear in the speciﬁed language part of document d. [sent-131, score-0.457]
</p><p>57 ndℓ and ndm are the number of times, for an arbitrary (·) word i ∈ {1, · · · , Nd } in an arbitrary language j ∈ {1, · · · , L} of document d, the ﬂags xi( j) = ℓ and T (·) xi( j) = m respectively are allocated to document d. [sent-132, score-0.546]
</p><p>58 CtdD indicates the (t, d) element of a T × D topic-document count matrix, meaning the number of times topic t is allocated to the document d’s W (·) language part speciﬁed in parentheses. [sent-133, score-0.514]
</p><p>59 Cwt T indicates the (w, t) element of a W (·) × T word-topic count matrix, meaning the number of times topic t is allocated to word w in the language speciﬁed in parentheses. [sent-134, score-0.515]
</p><p>60 In this model, non-pivot topics are dependent on the distribution behind the pivot topics, not dependent directly on the pivot topics as in the original SymCorrLDA. [sent-138, score-1.288]
</p><p>61 We will show through experiments how the modiﬁcation aﬀects the quality of the estimated multilingual topics, in the following section. [sent-143, score-0.386]
</p><p>62 4  Experiments  In this section, we demonstrate some examples with SymCorrLDA, and then we compare multilingual topic models using various evaluation methods. [sent-144, score-0.535]
</p><p>63 For the evaluation, we use held-out loglikelihood using two datasets, the task of ﬁnding an English article that is on the same topic as that of a Japanese article, and a task with the languages reversed. [sent-145, score-0.426]
</p><p>64 1  Settings  The datasets used in this work are two collections of Wikipedia articles: one is in English and Japanese, the other is in English, Japanese, and Spanish, and articles in each collection are connected across languages via inter-language links, as of November 2, 2009. [sent-147, score-0.272]
</p><p>65 0  rui (species) shu (species) karada (body) konchu̅ (insect) dobutsu (animal)  Topic 13 japan osaka kyoto hughes japanese  osaka kyoto shi (city) nen (year) kobe  0. [sent-166, score-0.397]
</p><p>66 three) languages as a comparable document that consists of two (or three) language parts. [sent-170, score-0.556]
</p><p>67 To carry out the evaluation in the task of ﬁnding counterpart articles that we will describe later, we randomly divided the Wikipedia document collection at the document level into 80% training documents and 20% test documents. [sent-171, score-0.362]
</p><p>68 In addition, we estimated a special implementation of SymCorrLDA, setting πd in a simple way for comparison, where the pivot language ﬂag for each word is randomly selected according to the proportion of the length of each language part (‘SymCorrLDA-rand’). [sent-174, score-1.086]
</p><p>69 2 Pivot assignments Figure 3 demonstrates how the frequency distribution of the pivot language-ﬂag (binomial) parameter πd,1 for the Japanese language with the bilingual dataset5 in SymCorrLDA changes while in iterations of collapsed Gibbs sampling. [sent-184, score-0.901]
</p><p>70 This ﬁgure shows that the pivot language ﬂag is randomly assigned at the initial state, and then it converges to an appropriate bias for each document as the iterations proceed. [sent-185, score-0.839]
</p><p>71 We next demonstrate how the pivot language ﬂags are assigned to each document. [sent-186, score-0.755]
</p><p>72 Figure 4(a) shows the titles of eight documents and the corresponding πd when using the bilingual data (T = 500). [sent-187, score-0.255]
</p><p>73 Therefore, a pivot is assigned considering the language biases of the articles. [sent-190, score-0.755]
</p><p>74 Here, πd,1 , πd,2 , and πd,3 respectively indicate the pivot language-ﬂag (multinomial) parameters corresponding to Japanese, English, and Spanish parts in each document. [sent-259, score-0.528]
</p><p>75 We further demonstrate the proportions of pivot assignments at the topic level. [sent-260, score-0.688]
</p><p>76 Figure 5 shows the content of 6 topics through 10 words with the highest probability for each language and for each topic when using the bilingual data (T = 500), some of which are biased to Japanese (Topics 13 and 59) or English (Topics 201 and 251), while the others have almost no bias. [sent-261, score-0.708]
</p><p>77 It can be seen that the pivot bias to speciﬁc languages can be interpreted. [sent-262, score-0.705]
</p><p>78 In this work, we estimated multilingual topic models with the training set and computed the log-likelihood of generating the held-out set that was mentioned in Section 4. [sent-266, score-0.55]
</p><p>79 Table 3 shows the held-out log-likelihood of each multilingual topic model estimated with the bilingual dataset when T = 500 and 1000. [sent-268, score-0.69]
</p><p>80 Hereafter, CorrLDA1 refers to the CorrLDA model that was estimated when Japanese was the pivot language. [sent-272, score-0.521]
</p><p>81 3, the CorrLDA model ﬁrst generates topics for the pivot language part of a document, and for the other language parts of the document, the model then uses the topics that were already generated in the pivot language. [sent-274, score-1.788]
</p><p>82 CorrLDA2 refers to the CorrLDA model when English was the pivot language. [sent-275, score-0.506]
</p><p>83 This is because CorrLDA can capture direct dependencies between languages, due to the constraints that topics have to be selected from the topics selected in the pivot language parts. [sent-277, score-1.076]
</p><p>84 This is probably because SymCorrLDA estimates the pivot language appropriately adjusted for each word in each document. [sent-284, score-0.838]
</p><p>85 This is because the constraints in SymCorrLDA-alt are relaxed so that topics do not always have to be selected from the topics selected for the words with the pivot language ﬂags. [sent-287, score-1.08]
</p><p>86 These results reﬂect the fact that the performance of SymCorrLDA in its full form is inherently aﬀected by the nature of the language biases in the multilingual comparable documents, rather than merely being aﬀected by the language part length. [sent-291, score-0.876]
</p><p>87 Here, CorrLDA3 refers to the CorrLDA model that was estimated when Spanish was the pivot language. [sent-293, score-0.521]
</p><p>88 SymCorrLDA can estimate the pivot language appropriately adjusted for each word in each document in the trilingual data, as with the bilingual data. [sent-295, score-1.124]
</p><p>89 On clock time, SymCorrLDA does pay some extra, such as around 40% of the time for CorrLDA in the case of the bilingual data, for allocating the pivot language ﬂags. [sent-299, score-0.878]
</p><p>90 4  Finding counterpart articles  Given an article, we can ﬁnd its unseen counterpart articles in other languages using a multilingual topic model. [sent-301, score-0.936]
</p><p>91 We estimated document-topic distributions of test documents for each language, using the topic-word distributions that were estimated by each multilingual topic model with training documents. [sent-303, score-0.658]
</p><p>92 For estimating the document-topic distributions of test documents, we used re-sampling of LDA using the topic-word distribution estimated beforehand by each multilingual topic model [15]. [sent-305, score-0.55]
</p><p>93 Each held-out English-Japanese article pair connected via an inter-language link is considered to be on the same topic; therefore, JS divergence of such an article pair is expected to be small if the latent topic estimation is accurate. [sent-307, score-0.306]
</p><p>94 We ﬁrst assumed each held-out Japanese article to be a query and the corresponding English article to be relevant, and evaluated the ranking of all the test articles of English in ascending order of the JS divergence; then we conducted the task with the languages reversed. [sent-308, score-0.398]
</p><p>95 Therefore, it is clear that SymCorrLDA estimates multilingual topics the most successfully in this experiment. [sent-339, score-0.509]
</p><p>96 5  Conclusions  In this paper, we compared the performance of various topic models that can be applied to multilingual documents, not using multilingual dictionaries, in terms of held-out log-likelihood and in the task of cross-lingual link detection. [sent-340, score-0.906]
</p><p>97 We demonstrated through experiments that CorrLDA works signiﬁcantly more eﬀectively than CI-LDA, which was used in prior work on multilingual topic models. [sent-341, score-0.535]
</p><p>98 Furthermore, we proposed a new topic model, SymCorrLDA, that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. [sent-342, score-0.67]
</p><p>99 SymCorrLDA has an advantage in that it does not require a pivot language to be speciﬁed in advance, while CorrLDA does. [sent-343, score-0.738]
</p><p>100 We demonstrated that SymCorrLDA is more eﬀective than CorrLDA and the other topic models, through experiments with Wikipedia datasets using held-out log-likelihood and in the task of ﬁnding counterpart articles in other languages. [sent-344, score-0.265]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pivot', 0.506), ('multilingual', 0.371), ('symcorrlda', 0.353), ('corrlda', 0.291), ('japanese', 0.291), ('language', 0.232), ('languages', 0.199), ('switchlda', 0.185), ('topic', 0.164), ('english', 0.145), ('bilingual', 0.14), ('topics', 0.138), ('documents', 0.093), ('word', 0.085), ('document', 0.084), ('lda', 0.075), ('articles', 0.073), ('article', 0.063), ('trilingual', 0.062), ('multinomial', 0.058), ('spanish', 0.049), ('dirichlet', 0.048), ('ndm', 0.044), ('comparable', 0.041), ('wikipedia', 0.039), ('ags', 0.039), ('di', 0.034), ('ag', 0.034), ('words', 0.034), ('erent', 0.033), ('dependencies', 0.03), ('correspondence', 0.029), ('counterpart', 0.028), ('ctdd', 0.026), ('kobe', 0.026), ('collapsed', 0.023), ('parallel', 0.023), ('md', 0.023), ('annotated', 0.022), ('parts', 0.022), ('ectively', 0.022), ('nen', 0.022), ('titles', 0.022), ('tokens', 0.022), ('translation', 0.021), ('js', 0.02), ('ireland', 0.02), ('david', 0.019), ('alignments', 0.019), ('zi', 0.019), ('proportions', 0.018), ('castle', 0.018), ('eguchi', 0.018), ('fukumasu', 0.018), ('hideyoshi', 0.018), ('horyu', 0.018), ('league', 0.018), ('nobunaga', 0.018), ('oda', 0.018), ('pivots', 0.018), ('truck', 0.018), ('vocab', 0.018), ('generative', 0.017), ('symmetric', 0.017), ('allocated', 0.017), ('assigned', 0.017), ('year', 0.017), ('indicates', 0.017), ('selected', 0.016), ('retrieval', 0.016), ('latent', 0.016), ('boldface', 0.016), ('osaka', 0.016), ('estimated', 0.015), ('appropriately', 0.015), ('sample', 0.015), ('species', 0.015), ('mrr', 0.014), ('dublin', 0.014), ('orm', 0.014), ('unaligned', 0.014), ('hanna', 0.014), ('generates', 0.014), ('removed', 0.014), ('translations', 0.014), ('binomial', 0.014), ('gibbs', 0.014), ('ective', 0.014), ('scotland', 0.014), ('graphical', 0.013), ('accordingly', 0.013), ('country', 0.013), ('kyoto', 0.013), ('wilcoxon', 0.013), ('wi', 0.013), ('wallach', 0.012), ('montreal', 0.012), ('canada', 0.012), ('nd', 0.012), ('text', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="332-tfidf-1" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>Author: Kosuke Fukumasu, Koji Eguchi, Eric P. Xing</p><p>Abstract: Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. 1</p><p>2 0.19865397 <a title="332-tfidf-2" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>Author: Elad Hazan, Zohar Karnin</p><p>Abstract: We present a simplex algorithm for linear programming in a linear classiﬁcation formulation. The paramount complexity parameter in linear classiﬁcation problems is called the margin. We prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear. This is in contrast to general linear programming, for which no sub-polynomial pivot rule is known. 1</p><p>3 0.16326125 <a title="332-tfidf-3" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>4 0.11071597 <a title="332-tfidf-4" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><p>5 0.11024596 <a title="332-tfidf-5" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Anima Anandkumar, Yi-kai Liu, Daniel J. Hsu, Dean P. Foster, Sham M. Kakade</p><p>Abstract: Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efﬁcient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k × k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space. 1</p><p>6 0.10628379 <a title="332-tfidf-6" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>7 0.079519533 <a title="332-tfidf-7" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>8 0.077918015 <a title="332-tfidf-8" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>9 0.076358959 <a title="332-tfidf-9" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>10 0.074232228 <a title="332-tfidf-10" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>11 0.07380303 <a title="332-tfidf-11" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>12 0.060159974 <a title="332-tfidf-12" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>13 0.058514208 <a title="332-tfidf-13" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>14 0.054834507 <a title="332-tfidf-14" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>15 0.054242909 <a title="332-tfidf-15" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>16 0.050536133 <a title="332-tfidf-16" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>17 0.045535196 <a title="332-tfidf-17" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>18 0.043054126 <a title="332-tfidf-18" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>19 0.034734752 <a title="332-tfidf-19" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>20 0.029990604 <a title="332-tfidf-20" href="./nips-2012-A_latent_factor_model_for_highly_multi-relational_data.html">22 nips-2012-A latent factor model for highly multi-relational data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.078), (1, 0.038), (2, -0.03), (3, -0.011), (4, -0.122), (5, -0.034), (6, -0.012), (7, 0.007), (8, 0.069), (9, -0.02), (10, 0.182), (11, 0.15), (12, 0.038), (13, 0.034), (14, 0.048), (15, 0.055), (16, 0.033), (17, 0.035), (18, 0.022), (19, 0.111), (20, 0.006), (21, 0.039), (22, -0.06), (23, -0.05), (24, 0.006), (25, -0.021), (26, -0.004), (27, 0.068), (28, 0.074), (29, -0.043), (30, 0.105), (31, -0.006), (32, 0.05), (33, 0.055), (34, 0.034), (35, -0.004), (36, -0.011), (37, 0.087), (38, -0.04), (39, -0.039), (40, -0.007), (41, 0.07), (42, 0.051), (43, -0.026), (44, -0.073), (45, 0.033), (46, 0.055), (47, -0.017), (48, 0.031), (49, -0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94721872 <a title="332-lsi-1" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>Author: Kosuke Fukumasu, Koji Eguchi, Eric P. Xing</p><p>Abstract: Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. 1</p><p>2 0.76891208 <a title="332-lsi-2" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>Author: Hugo Larochelle, Stanislas Lauly</p><p>Abstract: We describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents. This model is inspired by the recently proposed Replicated Softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations. Speciﬁcally, we take inspiration from the conditional mean-ﬁeld recursive equations of the Replicated Softmax in order to deﬁne a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words. This paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words. The end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the Replicated Softmax. Our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm. 1</p><p>3 0.73711765 <a title="332-lsi-3" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>4 0.66859102 <a title="332-lsi-4" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>Author: Peter Krafft, Juston Moore, Bruce Desmarais, Hanna M. Wallach</p><p>Abstract: We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks—speciﬁcally, the discovery and visualization of topic-speciﬁc subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of our model and its relationship to the observed data. We validate our modeling assumptions by demonstrating that our model achieves better link prediction performance than three state-of-the-art network models and exhibits topic coherence comparable to that of latent Dirichlet allocation. We showcase our model’s ability to discover and visualize topic-speciﬁc communication patterns using a new email data set: the New Hanover County email network. We provide an extensive analysis of these communication patterns, leading us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models. 1</p><p>5 0.64647514 <a title="332-lsi-5" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Anima Anandkumar, Yi-kai Liu, Daniel J. Hsu, Dean P. Foster, Sham M. Kakade</p><p>Abstract: Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efﬁcient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k × k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space. 1</p><p>6 0.57856441 <a title="332-lsi-6" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>7 0.56996864 <a title="332-lsi-7" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>8 0.56367731 <a title="332-lsi-8" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>9 0.47591004 <a title="332-lsi-9" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>10 0.45159775 <a title="332-lsi-10" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>11 0.44829911 <a title="332-lsi-11" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>12 0.4075571 <a title="332-lsi-12" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>13 0.39497003 <a title="332-lsi-13" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>14 0.32879558 <a title="332-lsi-14" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>15 0.31497157 <a title="332-lsi-15" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>16 0.3083415 <a title="332-lsi-16" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>17 0.30226409 <a title="332-lsi-17" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>18 0.29167071 <a title="332-lsi-18" href="./nips-2012-On_Triangular_versus_Edge_Representations_---_Towards_Scalable_Modeling_of_Networks.html">253 nips-2012-On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks</a></p>
<p>19 0.26832029 <a title="332-lsi-19" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>20 0.26032871 <a title="332-lsi-20" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (21, 0.017), (38, 0.05), (39, 0.014), (42, 0.021), (54, 0.017), (55, 0.014), (68, 0.013), (74, 0.038), (76, 0.074), (80, 0.081), (92, 0.026), (93, 0.368)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73954368 <a title="332-lda-1" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>Author: Kosuke Fukumasu, Koji Eguchi, Eric P. Xing</p><p>Abstract: Topic modeling is a widely used approach to analyzing large text collections. A small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in Wikipedia. Other topic models that were originally proposed for structured data are also applicable to multilingual documents. Correspondence Latent Dirichlet Allocation (CorrLDA) is one such model; however, it requires a pivot language to be speciﬁed in advance. We propose a new topic model, Symmetric Correspondence LDA (SymCorrLDA), that incorporates a hidden variable to control a pivot language, in an extension of CorrLDA. We experimented with two multilingual comparable datasets extracted from Wikipedia and demonstrate that SymCorrLDA is more eﬀective than some other existing multilingual topic models. 1</p><p>2 0.44502926 <a title="332-lda-2" href="./nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</a></p>
<p>Author: Aaron Dennis, Dan Ventura</p><p>Abstract: The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difﬁcult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture signiﬁcantly improves its performance compared to using a previously-proposed static architecture. 1</p><p>3 0.43447369 <a title="332-lda-3" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>4 0.42594886 <a title="332-lda-4" href="./nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">270 nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>Author: Hyunsin Park, Sungrack Yun, Sanghyuk Park, Jongmin Kim, Chang D. Yoo</p><p>Abstract: For phoneme classiﬁcation, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and nonparametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classiﬁcation performance, on the synthetic and benchmark datasets. 1</p><p>5 0.42504603 <a title="332-lda-5" href="./nips-2012-Multiresolution_Gaussian_Processes.html">233 nips-2012-Multiresolution Gaussian Processes</a></p>
<p>Author: David B. Dunson, Emily B. Fox</p><p>Abstract: We propose a multiresolution Gaussian process to capture long-range, nonMarkovian dependencies while allowing for abrupt changes and non-stationarity. The multiresolution GP hierarchically couples a collection of smooth GPs, each deﬁned over an element of a random nested partition. Long-range dependencies are captured by the top-level GP while the partition points deﬁne the abrupt changes. Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the marginal likelihood of the observations given the partition tree. This property allows for efﬁcient inference of the partition itself, for which we employ graph-theoretic techniques. We apply the multiresolution GP to the analysis of magnetoencephalography (MEG) recordings of brain activity.</p><p>6 0.42426962 <a title="332-lda-6" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>7 0.40584752 <a title="332-lda-7" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>8 0.394835 <a title="332-lda-8" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>9 0.391325 <a title="332-lda-9" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>10 0.38834655 <a title="332-lda-10" href="./nips-2012-A_Divide-and-Conquer_Method_for_Sparse_Inverse_Covariance_Estimation.html">7 nips-2012-A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation</a></p>
<p>11 0.38393113 <a title="332-lda-11" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>12 0.37323079 <a title="332-lda-12" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>13 0.36812684 <a title="332-lda-13" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>14 0.36682534 <a title="332-lda-14" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>15 0.36466733 <a title="332-lda-15" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>16 0.36362553 <a title="332-lda-16" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>17 0.36331999 <a title="332-lda-17" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>18 0.36173055 <a title="332-lda-18" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>19 0.36169437 <a title="332-lda-19" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>20 0.36095884 <a title="332-lda-20" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
