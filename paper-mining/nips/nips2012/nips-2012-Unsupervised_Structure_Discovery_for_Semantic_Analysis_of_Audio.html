<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-356" href="#">nips2012-356</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</h1>
<br/><p>Source: <a title="nips-2012-356-pdf" href="http://papers.nips.cc/paper/4661-unsupervised-structure-discovery-for-semantic-analysis-of-audio.pdf">pdf</a></p><p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>Reference: <a title="nips-2012-356-reference" href="../nips2012_reference/nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. [sent-5, score-0.491]
</p><p>2 Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. [sent-8, score-0.643]
</p><p>3 1  Introduction  Automatic semantic analysis of multimedia content has been an active area of research due to potential implications for indexing and retrieval [1–7]. [sent-10, score-0.318]
</p><p>4 In this paper, we limit ourselves to the analysis of the audio component of multimedia data only. [sent-11, score-0.546]
</p><p>5 Early approaches for semantic indexing of audio relied on automatic speech recognition techniques to generate semantically relevant keywords [2]. [sent-12, score-0.678]
</p><p>6 Subsequently, supervised approaches were developed for detecting speciﬁc (potentially semantically relevant) sounds in audio streams [6, 8–10], e. [sent-13, score-0.721]
</p><p>7 , and using the detected sounds to characterize the audio ﬁles. [sent-16, score-0.639]
</p><p>8 While this approach has been shown to be effective on certain datasets, it requires data for each of the various sounds expected in the dataset. [sent-17, score-0.174]
</p><p>9 audio libraries are studio-quality, while user-generated Youtube-style content are noisy. [sent-20, score-0.434]
</p><p>10 In order to avoid the issues that arise with using supervised, detection-based systems, unsupervised approaches were developed to learn sound dictionaries from the data [7, 11, 12],. [sent-21, score-0.186]
</p><p>11 Typically, these methods use clustering techniques on ﬁxed length audio segments to learn a dictionary, and then characterize new data using this dictionary. [sent-22, score-0.554]
</p><p>12 However, characterizing audio data with elements from an audio dictionary (supervised or unsupervised) for semantic analysis involves an implicit assumption that the acoustics map directly to semantics. [sent-23, score-1.058]
</p><p>13 In reality, we expect the mapping to be more complex, because acoustically similar sounds can be produced by very different sources. [sent-24, score-0.269]
</p><p>14 Further, since most audio datasets do not contain detailed hierarchical labels that our framework would require, we present unsupervised formulations for two layers in this hierarchical framework, building on previous work for the ﬁrst layer, and developing a model for the second. [sent-27, score-0.597]
</p><p>15 Instead, we use features derived from this structure to characterize audio, and evaluate these characterizations in a large-scale audio retrieval task with semantic categories, where our model signiﬁcantly improves over state-of-the-art baselines. [sent-29, score-0.671]
</p><p>16 A further beneﬁt of the induced structure is that the generated segments may be used for annotation by humans, thus removing the need for the annotator to scan the audio to identify and mark segment boundaries, making the annotation process much faster [13]. [sent-30, score-0.546]
</p><p>17 In Section 2, we introduce a novel framework for mapping acoustics to semantics for deeper analysis of audio. [sent-31, score-0.189]
</p><p>18 Section 3 describes the process of learning the lower-level acoustic units in the framework, while Section 4 describes a generative model that automatically identiﬁes patterns over and segments these acoustic units. [sent-32, score-0.937]
</p><p>19 Thus, changes in real-world scenes are sequential by nature and the human brain can perceive this sequentiality and use it to learn semantic relationships between the various events to analyze scenes; e. [sent-35, score-0.373]
</p><p>20 In this section, we present a hierarchical model that maps observed scene characteristics to semantics in a hierarchical fashion. [sent-38, score-0.196]
</p><p>21 Traditional detection-based approaches, that assign each frame or a sequence of frames of prespeciﬁed length to sound categories/clusters, are severely limited in their ability to account for context. [sent-42, score-0.186]
</p><p>22 In addition to context, we need to consider the possibility of polysemy in sounds– semantically different sounds may be acoustically similar; e. [sent-43, score-0.308]
</p><p>23 a dull metallic sound may be produced by a hammer striking an object, a baseball bat hitting a ball, or a car collision. [sent-45, score-0.222]
</p><p>24 The sound alone doesn’t provide us with sufﬁcient information to infer the semantic context. [sent-46, score-0.274]
</p><p>25 However, if the sound is followed by applause, we guess the context to be baseball, screams or sirens suggest an accident, while monotonic repetitions of the metallic sound suggest someone using a hammer. [sent-47, score-0.285]
</p><p>26 The grey circles closest to the observed audio represent short-duration lower-level acoustic units which produce sounds that human ears can perceive, such as the clink of glass, thump produced by footsteps, etc. [sent-50, score-1.036]
</p><p>27 These units have acoustic characteristics, but no clear associated semantics since the semantics may be context dependent. [sent-51, score-0.616]
</p><p>28 Sequences of these units, however, will have interpretable semantics– we refer to these as events marked by grey rectangles in Figure 1a. [sent-52, score-0.249]
</p><p>29 Further, these events themselves likely inﬂuence future events, shown by the arrows, e. [sent-54, score-0.224]
</p><p>30 the loud cheering in the audio clip is because a hitter hit a home run. [sent-56, score-0.539]
</p><p>31 The event layer in Figure 1b has been further divided into 2, where the lower level (indexed by v) correspond to observable events (e. [sent-59, score-0.495]
</p><p>32 hit-ball, cheering), whereas the higher level (e) corresponds to a semantic event (e. [sent-61, score-0.356]
</p><p>33 battingin-run), and the root node represents the semantic category (baseball, in this case). [sent-63, score-0.149]
</p><p>34 Typically, audio datasets contain only a category or genre label for each audio ﬁle. [sent-65, score-0.868]
</p><p>35 This framework for semantic analysis of audio is the ﬁrst effort to extract deeper semantic structure, to the best of our knowledge. [sent-67, score-0.786]
</p><p>36 We build on previous work to automatically learn lower level units unsupervised from audio data [14]. [sent-69, score-0.615]
</p><p>37 We then develop a generative model to learn event patterns over the lower-level units, which correspond to the second layer in Figure 1b. [sent-70, score-0.346]
</p><p>38 We represent the audio as a sequence of 39-dimensional feature vectors, each comprising 13 Mel-Frequency Cepstral Coefﬁcients and 13-dimensional∆ and ∆∆ features. [sent-71, score-0.468]
</p><p>39 2  Figure 1: Conceptual representation of the proposed hierarchical framework (a) Left ﬁgure: Conceptualizing increasingly complex semantic analysis; (b) Right ﬁgure: An example semantic parse for baseball . [sent-72, score-0.411]
</p><p>40 3  Unsupervised Learning of the Acoustic Unit Lexicon  At the the lowest level of the hierarchical structure speciﬁed by the model of Figure 1a is a sequence of atomic acoustic units, as described earlier. [sent-73, score-0.425]
</p><p>41 In reality, the number of such acoustic units is very large, possibly even inﬁnite. [sent-74, score-0.428]
</p><p>42 For the task of learning a lexicon of lower-level acoustic units, we leverage an unsupervised learning framework proposed in [14], which employs a the generative model shown in Figure 2 to describe audio recordings. [sent-76, score-0.959]
</p><p>43 We deﬁne a ﬁnite set of audio symbols A, and corresponding to each symbol a ∈ A, we deﬁne an acoustic model λa , and we refer to the set of all acoustic models as Λ. [sent-77, score-1.16]
</p><p>44 Thereafter, for each symbol at in T , a variable-length audio segment Dat is generated in accordance with λat . [sent-79, score-0.434]
</p><p>45 The ﬁnal audio D comprises the concatenation of the audio segments corresponding to all the symbols in T . [sent-80, score-0.976]
</p><p>46 Similar to [14], we represent each acoustic unit as a 5-state HMM with gaussian mixture output densities. [sent-81, score-0.34]
</p><p>47 The learnt parameters λa for each symbol a ∈ A allow us to decode any new audio ﬁle in terms of the set of symbols. [sent-83, score-0.507]
</p><p>48 While these symbols are not guaranteed to have any semantic interpretations, we expect them to capture acoustically consistent phenomena, and we see later that they do so in Figure 7. [sent-84, score-0.29]
</p><p>49 The symbols may hence be interpreted as representing generalized acoustic units (representing clusters of basic sound units). [sent-85, score-0.599]
</p><p>50 H and Λ are the language model and acoustic model parameters, T is the latent transcript and D is the observed data. [sent-89, score-0.405]
</p><p>51 1  A Generative Model for Inducing Patterns over AUDs  As discussed in Section 2, we expect that audio data are composed of a sequence of semantically meaningful events which manifest themselves in various acoustic forms, depending on the context. [sent-92, score-1.121]
</p><p>52 The acoustic unit (AUD) lexicon described in Section 3 automatically learns the various acoustic manifestations from a dataset but do not have interpretable semantic meaning. [sent-93, score-0.963]
</p><p>53 In this section, we introduce a generative model for the second layer in Fig 1a where the semantically interpretable acoustic events generate lower level AUDs (and thus, the observed audio). [sent-95, score-0.764]
</p><p>54 The distribution of AUDs for a speciﬁc event will be stochastic in nature (e. [sent-96, score-0.207]
</p><p>55 segments for a cheering event may contain any or all of claps, shouts, speech, music), and the distribution of the events themselves are stochastic and category-dependent. [sent-98, score-0.563]
</p><p>56 Again, while the number of such events can be expected to be very large, we assume that for a given dataset, a limited number of events can describe the event space fairly well. [sent-99, score-0.655]
</p><p>57 Further, we expect that the distribution of naturally occurring events in audio will follow the power law properties typically found in natural distributions [15, 16]. [sent-100, score-0.683]
</p><p>58 Events, drawn from this distribution, then generate lower level acoustic units (AUDs) corresponding to the sounds that are to be produced. [sent-102, score-0.602]
</p><p>59 Because this process is stochastic, different occurrences of the same event may produce different sequences of AUDs, which are variants of a common underlying pattern. [sent-103, score-0.283]
</p><p>60 We assume K audio events in the vocabulary, and M distinct AUD tokens, and we can generate a corpus of D documents as follows: for each document d, we ﬁrst draw a unigram distribution U for the events based on a power-law prior µ. [sent-105, score-0.925]
</p><p>61 We then draw Nd event tokens from the distribution for the events. [sent-106, score-0.25]
</p><p>62 Each event token can generate a sequence of AUDs of variable length n, where n is drawn from an event speciﬁc distribution α. [sent-107, score-0.534]
</p><p>63 n AUDs (cn ) 1 are now drawn from the multinomial AUD-emission distribution for the event Φevent . [sent-108, score-0.207]
</p><p>64 Thus, in this model, each audio document is a bag of events and each occurrence of an event is a bag of AUDs; the events themselves are distributions over AUDs. [sent-109, score-1.171]
</p><p>65 We can then use these parameters to estimate the latent events present in audio based on an observed AUD stream (the AUD stream is obtained by decoding audio as described in Section 3). [sent-113, score-1.194]
</p><p>66 Unlike those, however, we model each event as a bag of AUDs as opposed to an AUD sequence for two reasons. [sent-115, score-0.269]
</p><p>67 First, AUD sequences (and indeed, the observed audio) for different instances of the same event will have innate variations. [sent-116, score-0.242]
</p><p>68 Second, in the case of the audio, presence of multiple sounds may result in noisy AUD streams so that text character streams which are usually clean are not directly analogous; instead, noisy, badly spelt text might be a better analogy. [sent-117, score-0.3]
</p><p>69 1  Latent Variable Estimation in the Learning Framework  Figure 4: An example automaton for a word of maximum length 3. [sent-129, score-0.191]
</p><p>70 Figure 5: An automaton with the K word automatons in parallel for decoding a token stream  We construct an automaton for each of the K events– an example is shown in Figure 4. [sent-131, score-0.395]
</p><p>71 Based on these, states can skip to the ﬁnal state, thus accounting for variable lengths of events in terms of number of AUDs. [sent-136, score-0.256]
</p><p>72 We deﬁne S as the set of all start states for events, so that Si =start state of event i. [sent-137, score-0.207]
</p><p>73 Since we model event occurrences as bags of AUDs, AUD emission probabilities are shared by all states for a given event. [sent-138, score-0.288]
</p><p>74 The automatons for the events are now put together as shown in Figure 5– the black circle represents a dummy start state, and terminal states for each event can transition to this start state. [sent-139, score-0.53]
</p><p>75 Pd (wi ) represents the probability of the event wi given the unigram distribution for the document d. [sent-140, score-0.301]
</p><p>76 Thus, if for word i, we have a set of m occurrences in these paths of lengths n1 , n2 , . [sent-152, score-0.151]
</p><p>77 Then, we present results using the 2-level hierarchical model on the event kit of the 2011 TRECVID Multimedia Event Detection (MED) task [22]. [sent-164, score-0.258]
</p><p>78 Again, the learner automatically identiﬁed the most frequent word to be one which had highest emission probabilities for {‘. [sent-176, score-0.18]
</p><p>79 ’, ‘c’, ‘o’, ‘m’} and the second most frequent word with {‘h’, ‘t’, ‘p’, ‘/’, ‘:’, ‘w’} characters having high probabilities. [sent-177, score-0.153]
</p><p>80 board trick, feeding an animal– full list at [22]), and was to be used to build detectors for each semantic category, so that given a new ﬁle, it can predict whether it belongs to any of those categories or not. [sent-182, score-0.194]
</p><p>81 All our reported results use 8-fold cross validation on the entire event kit. [sent-183, score-0.207]
</p><p>82 While the AUDs are not required to have clear semantic interpretations, listening to the concatenated instances shows that the AUD on the left primarily spans music segments while the right consists primarily of speech– speech formant structures are visible in the image. [sent-188, score-0.355]
</p><p>83 We then use the decoded AUD sequences as character streams to learn parameters for the second layer of observable acoustic events spanning local AUD sequences. [sent-189, score-0.77]
</p><p>84 Since there are no annotations available, these events 7  Table 1: Performance summary across MED11 dataset (lower is better) System Average AUC Best Performance in #categories #SSI-over-VQ #SSI-over-FOLEY #SSI-over-AUD-FREQ #SSI-over-EVENT-FREQ  VQ 0. [sent-191, score-0.264]
</p><p>85 One such event consists of sequences of sounds that relate to crowds with loud cheering and a babble of voices in a party being subsumed within the same event. [sent-197, score-0.521]
</p><p>86 We use the decoded AUDs and event sequences for each ﬁle to characterize the MED11 data, and evaluate the effect of using the AUDs layer and the event layer individually (AUD-FREQ and EVENT-FREQ, respectively) and together (COMB). [sent-198, score-0.638]
</p><p>87 The ﬁrst is a VQ baseline (VQ) where a set of audio words is learned unsupervised by applying K-Means on the data at the frame level. [sent-202, score-0.495]
</p><p>88 The second uses an audio library to create a supervised sound library from the 480 sound types in the Foley Sound Library [23], and we characterize each ﬁle using occurrence information of these sounds in the ﬁle (FOLEY). [sent-203, score-0.915]
</p><p>89 We used the best performing lexicon size for the various systems– 4096 clusters for the VQ, 480 Foley audio events, 1024 AUDs, and 128 acoustic events. [sent-206, score-0.851]
</p><p>90 This work presents an initial approach to extracting such deeper semantic features from audio based on local patterns of low-level acoustic units. [sent-213, score-1.005]
</p><p>91 Since the discovered latent events and acoustic units do not have true labels, we would like to explore ways to leverage tags, knowledge bases and human annotators to induce labels. [sent-215, score-0.652]
</p><p>92 In such settings, we would like to explore non-parametric techniques that can grow the event set based on data. [sent-216, score-0.207]
</p><p>93 Finally, we would like to use such event structure to study co-occurrences and dependencies of acoustic event types that might allow us to predict sounds in the future based on the context. [sent-218, score-0.928]
</p><p>94 Content-based audio classication and retrieval by support vector machines. [sent-240, score-0.537]
</p><p>95 Mixture of probability experts for audio retrieval and indexing. [sent-244, score-0.491]
</p><p>96 Classication of sound clips by two schemes: using onomatopoeia and semantic labels. [sent-264, score-0.274]
</p><p>97 Classiﬁcation of tv programs based on audio information using hidden markov model. [sent-270, score-0.464]
</p><p>98 Unsupervised learning of acoustic unit descriptors for audio content representation and classiﬁcation. [sent-306, score-0.774]
</p><p>99 Bayesian unsupervised word segmentation with nested pitmanyor language modeling. [sent-320, score-0.217]
</p><p>100 Improving nonparametric bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. [sent-338, score-0.183]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('audio', 0.434), ('aud', 0.385), ('auds', 0.35), ('acoustic', 0.34), ('events', 0.224), ('event', 0.207), ('sounds', 0.174), ('semantic', 0.149), ('sound', 0.125), ('multimedia', 0.112), ('semantics', 0.094), ('units', 0.088), ('automaton', 0.086), ('word', 0.078), ('lexicon', 0.077), ('acoustically', 0.07), ('cheering', 0.07), ('foley', 0.07), ('layer', 0.064), ('semantically', 0.064), ('segments', 0.062), ('baseball', 0.062), ('trecvid', 0.062), ('unsupervised', 0.061), ('token', 0.059), ('retrieval', 0.057), ('deeper', 0.054), ('sundaram', 0.053), ('tir', 0.053), ('le', 0.052), ('hierarchical', 0.051), ('wi', 0.051), ('stream', 0.051), ('auc', 0.049), ('streams', 0.049), ('vq', 0.049), ('generative', 0.047), ('symbols', 0.046), ('classication', 0.046), ('characters', 0.045), ('categories', 0.045), ('segmentation', 0.044), ('unigram', 0.043), ('music', 0.043), ('tokens', 0.043), ('occurrences', 0.041), ('acoustics', 0.041), ('concatenated', 0.041), ('comb', 0.04), ('annotations', 0.04), ('emission', 0.04), ('learnt', 0.038), ('ni', 0.038), ('cn', 0.037), ('terminal', 0.037), ('sequences', 0.035), ('emitted', 0.035), ('decode', 0.035), ('phenomena', 0.035), ('automatons', 0.035), ('bhiksha', 0.035), ('efi', 0.035), ('expo', 0.035), ('georgiou', 0.035), ('loud', 0.035), ('metallic', 0.035), ('morphological', 0.035), ('sequence', 0.034), ('di', 0.034), ('language', 0.034), ('med', 0.033), ('lengths', 0.032), ('automatically', 0.032), ('speech', 0.031), ('characterize', 0.031), ('transcript', 0.031), ('decoded', 0.03), ('tv', 0.03), ('frequent', 0.03), ('technologies', 0.029), ('zipf', 0.029), ('listening', 0.029), ('eqn', 0.029), ('urls', 0.029), ('character', 0.028), ('bag', 0.028), ('patterns', 0.028), ('ij', 0.028), ('length', 0.027), ('binomial', 0.027), ('dummy', 0.027), ('linguistics', 0.026), ('occurrence', 0.026), ('backward', 0.026), ('count', 0.026), ('interpretable', 0.025), ('detection', 0.025), ('expect', 0.025), ('oracle', 0.025), ('annotation', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="356-tfidf-1" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>2 0.22306231 <a title="356-tfidf-2" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<p>Author: Yan Karklin, Chaitanya Ekanadham, Eero P. Simoncelli</p><p>Abstract: Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important ﬁrst step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The ﬁrst layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of ﬁrst layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while ﬁne-scale structure is captured by recurrent interactions within the ﬁrst layer. When ﬁt to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a signiﬁcant improvement over standard methods. 1</p><p>3 0.10756665 <a title="356-tfidf-3" href="./nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">270 nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>Author: Hyunsin Park, Sungrack Yun, Sanghyuk Park, Jongmin Kim, Chang D. Yoo</p><p>Abstract: For phoneme classiﬁcation, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and nonparametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classiﬁcation performance, on the synthetic and benchmark datasets. 1</p><p>4 0.099608473 <a title="356-tfidf-4" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>Author: Jonathan Huang, Daniel Alexander</p><p>Abstract: Accurate and detailed models of neurodegenerative disease progression are crucially important for reliable early diagnosis and the determination of effective treatments. We introduce the ALPACA (Alzheimer’s disease Probabilistic Cascades) model, a generative model linking latent Alzheimer’s progression dynamics to observable biomarker data. In contrast with previous works which model disease progression as a ﬁxed event ordering, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed progression models. We describe efﬁcient learning algorithms for ALPACA and discuss promising experimental results on a real cohort of Alzheimer’s patients from the Alzheimer’s Disease Neuroimaging Initiative. 1</p><p>5 0.098553315 <a title="356-tfidf-5" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>Author: Hiroki Terashima, Masato Okada</p><p>Abstract: The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efﬁcient coding strategy and that the disorder in A1 reﬂects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reﬂected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.</p><p>6 0.076605499 <a title="356-tfidf-6" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>7 0.076048881 <a title="356-tfidf-7" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>8 0.073914349 <a title="356-tfidf-8" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>9 0.072983406 <a title="356-tfidf-9" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>10 0.06786783 <a title="356-tfidf-10" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>11 0.065313719 <a title="356-tfidf-11" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>12 0.065003231 <a title="356-tfidf-12" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>13 0.064841412 <a title="356-tfidf-13" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>14 0.064045981 <a title="356-tfidf-14" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>15 0.063944325 <a title="356-tfidf-15" href="./nips-2012-Learning_Networks_of_Heterogeneous_Influence.html">182 nips-2012-Learning Networks of Heterogeneous Influence</a></p>
<p>16 0.063907862 <a title="356-tfidf-16" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>17 0.06267184 <a title="356-tfidf-17" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>18 0.057978459 <a title="356-tfidf-18" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>19 0.05478818 <a title="356-tfidf-19" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>20 0.053199172 <a title="356-tfidf-20" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.03), (2, -0.112), (3, 0.02), (4, -0.028), (5, -0.011), (6, -0.003), (7, 0.016), (8, -0.011), (9, 0.007), (10, 0.035), (11, 0.058), (12, 0.008), (13, 0.004), (14, -0.022), (15, -0.041), (16, -0.023), (17, 0.029), (18, 0.037), (19, -0.098), (20, 0.001), (21, 0.019), (22, -0.098), (23, -0.146), (24, 0.047), (25, -0.029), (26, 0.047), (27, -0.079), (28, 0.013), (29, 0.003), (30, 0.233), (31, 0.025), (32, 0.034), (33, -0.032), (34, -0.076), (35, -0.004), (36, 0.165), (37, -0.02), (38, 0.134), (39, 0.087), (40, 0.073), (41, 0.203), (42, -0.162), (43, -0.007), (44, -0.04), (45, 0.062), (46, -0.065), (47, 0.117), (48, -0.001), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9476853 <a title="356-lsi-1" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>2 0.76737261 <a title="356-lsi-2" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<p>Author: Yan Karklin, Chaitanya Ekanadham, Eero P. Simoncelli</p><p>Abstract: Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important ﬁrst step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The ﬁrst layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of ﬁrst layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while ﬁne-scale structure is captured by recurrent interactions within the ﬁrst layer. When ﬁt to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a signiﬁcant improvement over standard methods. 1</p><p>3 0.56708598 <a title="356-lsi-3" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>Author: Yuxuan Wang, Deliang Wang</p><p>Abstract: While human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison. We show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction. To account for temporal dynamics in speech, we employ conditional random ﬁelds (CRFs) to classify speech dominance within each time-frequency unit for a sound mixture. To capture complex, nonlinear relationship between input and output, both state and transition feature functions in CRFs are learned by deep neural networks. The formulation of the problem as classiﬁcation allows us to directly optimize a measure that is well correlated with human speech intelligibility. The proposed system substantially outperforms existing ones in a variety of noises.</p><p>4 0.56666154 <a title="356-lsi-4" href="./nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">270 nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>Author: Hyunsin Park, Sungrack Yun, Sanghyuk Park, Jongmin Kim, Chang D. Yoo</p><p>Abstract: For phoneme classiﬁcation, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and nonparametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classiﬁcation performance, on the synthetic and benchmark datasets. 1</p><p>5 0.49008635 <a title="356-lsi-5" href="./nips-2012-Modelling_Reciprocating_Relationships_with_Hawkes_Processes.html">219 nips-2012-Modelling Reciprocating Relationships with Hawkes Processes</a></p>
<p>Author: Charles Blundell, Jeff Beck, Katherine A. Heller</p><p>Abstract: We present a Bayesian nonparametric model that discovers implicit social structure from interaction time-series data. Social groups are often formed implicitly, through actions among members of groups. Yet many models of social networks use explicitly declared relationships to infer social structure. We consider a particular class of Hawkes processes, a doubly stochastic point process, that is able to model reciprocity between groups of individuals. We then extend the Inﬁnite Relational Model by using these reciprocating Hawkes processes to parameterise its edges, making events associated with edges co-dependent through time. Our model outperforms general, unstructured Hawkes processes as well as structured Poisson process-based models at predicting verbal and email turn-taking, and military conﬂicts among nations. 1</p><p>6 0.47621486 <a title="356-lsi-6" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>7 0.4720929 <a title="356-lsi-7" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>8 0.46929932 <a title="356-lsi-8" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>9 0.44463098 <a title="356-lsi-9" href="./nips-2012-Probabilistic_Event_Cascades_for_Alzheimer%27s_disease.html">276 nips-2012-Probabilistic Event Cascades for Alzheimer's disease</a></p>
<p>10 0.44114175 <a title="356-lsi-10" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>11 0.39358613 <a title="356-lsi-11" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>12 0.35133037 <a title="356-lsi-12" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>13 0.34215054 <a title="356-lsi-13" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>14 0.3289237 <a title="356-lsi-14" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>15 0.32764462 <a title="356-lsi-15" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>16 0.32200974 <a title="356-lsi-16" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>17 0.31850591 <a title="356-lsi-17" href="./nips-2012-Effective_Split-Merge_Monte_Carlo_Methods_for_Nonparametric_Models_of_Sequential_Data.html">107 nips-2012-Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data</a></p>
<p>18 0.31558648 <a title="356-lsi-18" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>19 0.31381142 <a title="356-lsi-19" href="./nips-2012-Learning_Networks_of_Heterogeneous_Influence.html">182 nips-2012-Learning Networks of Heterogeneous Influence</a></p>
<p>20 0.31326529 <a title="356-lsi-20" href="./nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">2 nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.076), (17, 0.018), (21, 0.034), (38, 0.079), (39, 0.013), (42, 0.026), (54, 0.017), (55, 0.052), (74, 0.04), (76, 0.157), (77, 0.018), (80, 0.073), (92, 0.053), (98, 0.254)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87035263 <a title="356-lda-1" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>same-paper 2 0.78894985 <a title="356-lda-2" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>3 0.78217876 <a title="356-lda-3" href="./nips-2012-Perceptron_Learning_of_SAT.html">267 nips-2012-Perceptron Learning of SAT</a></p>
<p>Author: Alex Flint, Matthew Blaschko</p><p>Abstract: Boolean satisﬁability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will ﬁnd an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware veriﬁcation task. 1</p><p>4 0.78077608 <a title="356-lda-4" href="./nips-2012-Bayesian_Warped_Gaussian_Processes.html">55 nips-2012-Bayesian Warped Gaussian Processes</a></p>
<p>Author: Miguel Lázaro-gredilla</p><p>Abstract: Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classiﬁcation, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets.</p><p>5 0.70262861 <a title="356-lda-5" href="./nips-2012-Learning_Mixtures_of_Tree_Graphical_Models.html">180 nips-2012-Learning Mixtures of Tree Graphical Models</a></p>
<p>Author: Anima Anandkumar, Furong Huang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: We consider unsupervised estimation of mixtures of discrete graphical models, where the class variable is hidden and each mixture component can have a potentially different Markov graph structure and parameters over the observed variables. We propose a novel method for estimating the mixture components with provable guarantees. Our output is a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. The sample and computational requirements for our method scale as poly(p, r), for an r-component mixture of pvariate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs. Keywords: Graphical models, mixture models, spectral methods, tree approximation.</p><p>6 0.67481256 <a title="356-lda-6" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>7 0.63356167 <a title="356-lda-7" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>8 0.62779486 <a title="356-lda-8" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>9 0.62675363 <a title="356-lda-9" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>10 0.62404299 <a title="356-lda-10" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>11 0.62324226 <a title="356-lda-11" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>12 0.62197137 <a title="356-lda-12" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>13 0.62001014 <a title="356-lda-13" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>14 0.61967784 <a title="356-lda-14" href="./nips-2012-Multiresolution_Gaussian_Processes.html">233 nips-2012-Multiresolution Gaussian Processes</a></p>
<p>15 0.61912256 <a title="356-lda-15" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>16 0.61869609 <a title="356-lda-16" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>17 0.61764961 <a title="356-lda-17" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>18 0.61747891 <a title="356-lda-18" href="./nips-2012-Spectral_learning_of_linear_dynamics_from_generalised-linear_observations_with_application_to_neural_population_data.html">321 nips-2012-Spectral learning of linear dynamics from generalised-linear observations with application to neural population data</a></p>
<p>19 0.61685592 <a title="356-lda-19" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>20 0.61663365 <a title="356-lda-20" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
