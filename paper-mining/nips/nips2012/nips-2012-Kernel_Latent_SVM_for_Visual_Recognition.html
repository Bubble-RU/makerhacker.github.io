<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2012-Kernel Latent SVM for Visual Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-168" href="#">nips2012-168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 nips-2012-Kernel Latent SVM for Visual Recognition</h1>
<br/><p>Source: <a title="nips-2012-168-pdf" href="http://papers.nips.cc/paper/4590-kernel-latent-svm-for-visual-recognition.pdf">pdf</a></p><p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>Reference: <a title="nips-2012-168-reference" href="../nips2012_reference/nips-2012-Kernel_Latent_SVM_for_Visual_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. [sent-11, score-0.752]
</p><p>2 The person detection algorithm in [2] is an example of the success of linear classiﬁers in computer vision. [sent-21, score-0.236]
</p><p>3 The ﬁrst one is to introduce latent variables into the linear model. [sent-29, score-0.292]
</p><p>4 DPM captures shape and pose variations of an object class with a root template covering the whole object and several part templates. [sent-31, score-0.335]
</p><p>5 Learning a DPM involves solving a latent 1  Without loss of generality, we assume linear models without the bias term. [sent-33, score-0.26]
</p><p>6 1  SVM (LSVM) [5, 17] – an extension of regular linear SVM for handling latent variables. [sent-34, score-0.285]
</p><p>7 For example, in object detection, the training data are weakly labeled because we are only given the bounding boxes of the objects without the detailed annotation for each part. [sent-36, score-0.38]
</p><p>8 In addition to modeling part deformation, another popular application of LSVM is to use it as a mixture model where the mixture component is represented as a latent variable [5, 6, 16]. [sent-37, score-0.263]
</p><p>9 A limitation of kernel methods is that the learning is more expensive than linear classiﬁers on large datasets, although efﬁcient algorithms exist for certain types of kernels (e. [sent-40, score-0.245]
</p><p>10 The latent variables in LSVM can often have some intuitive and semantic meanings. [sent-47, score-0.283]
</p><p>11 Examples of latent variables in the literature include part locations in object detection [5], subcategories in video annotation [16], object localization in image classiﬁcation [8], etc. [sent-49, score-0.726]
</p><p>12 Since the number of support vectors can vary depending on the training data, kernel methods can adapt their model complexity to ﬁt the data. [sent-54, score-0.211]
</p><p>13 In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. [sent-55, score-0.752]
</p><p>14 On one hand, the latent variables in KLSVM can be something intuitive and semantically meaningful. [sent-57, score-0.283]
</p><p>15 We demonstrate KLSVM on three applications in visual recognition: 1) object classiﬁcation with latent localization; 2) object classiﬁcation with latent subcategories; 3) recognition of object interactions. [sent-59, score-0.906]
</p><p>16 2  Preliminaries  In this section, we introduce some background on latent SVM and on the dual form of SVMs used for deriving kernel SVMs. [sent-60, score-0.431]
</p><p>17 Each instance is also associated with a latent variable h that captures some unobserved information about the data. [sent-64, score-0.312]
</p><p>18 To simplify the notation, we also assume the latent variable h takes its value from a discrete set of labels h ∈ H. [sent-74, score-0.263]
</p><p>19 In latent SVM, the scoring function of sample x is deﬁned as fw (x) = maxh w φ(x, h), where φ(x, h) is the feature vector deﬁned for the pair of (x, h). [sent-81, score-0.332]
</p><p>20 For example, in the “car model” example, φ(x, h) can be a feature vector extracted from the image patch at location h of the image x. [sent-82, score-0.252]
</p><p>21 However, the learning problem becomes convex once the latent variable h is ﬁxed for positive examples. [sent-85, score-0.29]
</p><p>22 More formally, we are given 2  M positive samples {xi , hi }M , and N negative samples {xj }M +N . [sent-93, score-0.297]
</p><p>23 αi + i  j  h  1 βj,h − || 2  βj,h φ(xj , h)||2 (2a)  αi φ(xi , hi ) − i  j  h  0 ≤ αi ≤ C1 , ∀i; 0 ≤ βj,h ≤ C2 , ∀j, ∀h ∈ H ∗  (2b) ∗  ∗  The optimal primal parameters w for Eq. [sent-117, score-0.27]
</p><p>24 2 are related as follows: w∗ =  ∗ αi φ(xi , hi ) −  ∗ βj,h φ(xj , h)  i  j  (3)  h  Let us deﬁne λ to be the concatenations of {αi : ∀i} and {βj,h : ∀j, ∀h ∈ H}, so |λ| = M +N ×|H|. [sent-119, score-0.27]
</p><p>25 Ψ is obtained by stacking together {φ(xi , hi ) : ∀i} and {−φ(xj , h) : ∀j, ∀h ∈ H}. [sent-121, score-0.27]
</p><p>26 The scoring function for the testing images xnew can be kernelized as follows: f (xnew ) = maxhnew  i  ∗ αi k(φ(xi , hi ), φ(xnew , hnew )) −  j  h  ∗ βj,h k(φ(xj , h), φ(xnew , hnew )) . [sent-128, score-0.54]
</p><p>27 In the next section, we will exploit this fact to develop the kernel latent support vector machines. [sent-132, score-0.399]
</p><p>28 In this section, we propose kernel latent SVM (KLSVM) – a new latent variable learning method that only requires a kernel function K(x, h, x , h ) between a pair of (x, h) and (x , h ). [sent-136, score-0.787]
</p><p>29 0 ≤ αi ≤ C1 , ∀i;  h  1 βj,h − || 2  βj,h φ(xj , h)||2 (5b)  αi φ(xi , hi ) − i  j  h  0 ≤ βj,h ≤ C2 , ∀j, ∀h ∈ H  (5c)  The most straightforward way of solving Eq. [sent-146, score-0.27]
</p><p>30 When hi takes its value from a discrete set of K possible choices (i. [sent-148, score-0.27]
</p><p>31 8(b) as follows h∗ = arg max αt αt k(φ(xt , ht ), φ(xt , ht )) + 2 t ht  αi αt k(φ(xi , hi ), φ(xt , ht )) i:i=t  −2  βj,h αt k(φ(xj , h), φ(xt , ht )) j  (9)  h  It is interesting to notice that if the t-th example is not a support vector (i. [sent-168, score-1.742]
</p><p>32 For other positive examples (non-support vectors), we can simply set their latent variables the same 4  (7)  as the previous iteration. [sent-174, score-0.313]
</p><p>33 8 becomes: h∗ = arg max ||αt φ(xt , ht )||2 + 2 w − αt φ(xt , hold ) t t  αt φ(xt , ht )  (10a)  ht  1 2 2 ⇔ arg max αt w φ(xt , ht ) + αt ||φ(xt , ht )||2 − αt φ(xt , hold ) φ(xt , ht ) t 2 ht  (10b)  where hold is the value of latent variable of the t-th example in the previous iteration. [sent-181, score-2.501]
</p><p>34 In this case, αt φ(xt , ht ) φ(xt , ht ) is a constant, and we have φ(xt , hold ) φ(xt , hold ) > φ(xt , hold ) φ(xt , ht ) if ht = hold . [sent-183, score-1.384]
</p><p>35 10 is equivalent to: t t t t h∗ = arg max w φ(xt , ht ) − αt φ(xt , hold ) φ(xt , ht ) t t  (11)  ht  Eq. [sent-185, score-0.946]
</p><p>36 , h∗ = arg maxht w φ(xt , ht ), but t with an extra term αt φ(xt , hold ) φ(xt , ht ) which penalizes the choice of ht for being the same t value as previous iteration hold . [sent-188, score-1.035]
</p><p>37 If the t-th positive t example is a support vector, the latent variable hold from previous iteration causes this example to lie very close to (or even on the wrong side) the decision boundary, i. [sent-190, score-0.4]
</p><p>38 The amount of penalty depends on the magnitudes of αt and φ(xt , hold ) φ(xt , ht ). [sent-195, score-0.346]
</p><p>39 We can interpret αt as how t “bad” hold is, and φ(xt , hold ) φ(xt , ht ) as how close ht is to hold . [sent-196, score-0.755]
</p><p>40 2  Composite Kernels  So far we have assumed that the latent variable h takes its value from a discrete set of labels. [sent-200, score-0.263]
</p><p>41 First of all, it allows us to exploit structural information in the latent variables. [sent-206, score-0.228]
</p><p>42 In the following, we will show how to compose a new kernel for the “person riding a bike” classiﬁer from those components. [sent-221, score-0.28]
</p><p>43 We denote the latent variable using h to emphasize that now it is a vector instead of a single discrete value. [sent-222, score-0.263]
</p><p>44 For the structured latent variable, it is assumed that there are certain dependencies between some pairs of (zu , zv ). [sent-227, score-0.28]
</p><p>45 We can use an undirected graph G = (V, E) to capture the structure of the latent variable, where a vertex u ∈ V corresponds to the label zu , and an edge (u, v) ∈ E corresponds to the dependency between zu and zv . [sent-228, score-0.66]
</p><p>46 The latent variable in this case has two components h = (zperson , zbike ) corresponding to the location of person and bike, respectively. [sent-230, score-0.509]
</p><p>47 On the training data, we have access to the ground-truth bounding box of “person riding a bike” as a whole, but not the exact location of “person” or “bike” within the bounding box. [sent-231, score-0.463]
</p><p>48 Suppose we already have kernel functions corresponding to the vertices and edges in the graph, we can then deﬁne the composite kernel as the summation of the kernels over all the vertices and edges. [sent-237, score-0.367]
</p><p>49 5  Figure 1: Visualization of how the latent variable (i. [sent-238, score-0.263]
</p><p>50 The red bounding box corresponds to the initial object location. [sent-241, score-0.264]
</p><p>51 The blue bounding box corresponds to the object location after the learning. [sent-242, score-0.32]
</p><p>52 Method BOF + linear SVM BOF + kernel SVM linear LSVM KLSVM Acc (%) 45. [sent-243, score-0.212]
</p><p>53 K(Φ(x, h), Φ(x , h )) =  ku (φ(x, zu ), φ(x , zu )) + u∈V  kuv (ψ(x, zu , zv ), ψ(x , zu , zv )) (12) (u,v)∈E  When the latent variable h forms a tree structure, there exist efﬁcient inference algorithms for solving Eq. [sent-253, score-1.179]
</p><p>54 Each application has a different type of latent variables. [sent-258, score-0.228]
</p><p>55 Our training data only have image-level labels indicating the presence/absence of each object category in an image. [sent-263, score-0.219]
</p><p>56 The exact object location in the image is not provided and is considered as the latent variable h in our formulation. [sent-264, score-0.508]
</p><p>57 We deﬁne the feature vector φ(x, h) as the HOG feature extracted from the image at location h. [sent-265, score-0.221]
</p><p>58 We assume the object size is the same for the images of the same category, which is a reasonable assumption for this dataset. [sent-270, score-0.204]
</p><p>59 To demonstrate the beneﬁt of using latent variables, we also compare with two simple baselines using linear and kernel SVMs based on bag-offeatures (BOF) extracted from the whole image (i. [sent-273, score-0.561]
</p><p>60 We use the histogram intersection kernel (HIK) [10] since it has been proved to be successful for vision applications, and efﬁcient learning/inference algorithms exist for this kernel. [sent-278, score-0.215]
</p><p>61 In each round, we randomly split the images from each category into training and testing sets. [sent-280, score-0.201]
</p><p>62 For both linear LSVM and KLSVM, we initialize the latent variable at the center location of each image and we set C1 = C2 = 1. [sent-281, score-0.416]
</p><p>63 So BOF feature without latent variables cannot capture the subtle differences between each category. [sent-287, score-0.294]
</p><p>64 1 shows examples of how the latent variables change on some training images during the learning of the KLSVM. [sent-290, score-0.406]
</p><p>65 For each training image, the location of the object (latent variable h) is initialized to the center of the image. [sent-291, score-0.255]
</p><p>66 After the learning algorithm terminates, the latent variables accurately locate the objects. [sent-292, score-0.291]
</p><p>67 Method non-latent linear SVM linear LSVM non-latent kernel SVM KLSVM Acc (%) 50. [sent-296, score-0.212]
</p><p>68 But here we consider a different type of latent variable. [sent-309, score-0.228]
</p><p>69 Here we use the latent variable h to indicate the subcategory an image belongs to. [sent-317, score-0.439]
</p><p>70 If a training image belongs to the class c, its subcategory label h takes value from a set Hc of subcategory labels corresponding to the c-th class. [sent-318, score-0.327]
</p><p>71 Note that subcategories are latent on the training data, so they may or may not have semantic meanings. [sent-319, score-0.327]
</p><p>72 Results: Again we compare with three baselines: linear LSVM, non-latent linear SVM, non-latent kernel SVM. [sent-328, score-0.212]
</p><p>73 For non-latent approaches, we simply feed feature vector φ(x) to SVMs without using any latent variable. [sent-330, score-0.262]
</p><p>74 01 for all the experiments and initialize the subcategory labels of training images by k-means clustering. [sent-334, score-0.231]
</p><p>75 It is interesting to note that both linear LSVM and KLSVM outperform their non-latent counterparts, which demonstrates the effectiveness of using latent subcategories in object classiﬁcation. [sent-337, score-0.443]
</p><p>76 3  Recognition of Object Interaction  Problem and Dataset: Finally, we consider an application where the latent variable is more complex and requires the composite kernel introduced in Sec. [sent-343, score-0.441]
</p><p>77 Each image is only associated with one of the four object interaction label. [sent-351, score-0.225]
</p><p>78 Our approach: We treat the locations of objects as latent variables. [sent-354, score-0.298]
</p><p>79 For example, when learning the model for “person riding a bicycle”, we treat the locations of “person” and “bicycle” as latent variables. [sent-355, score-0.401]
</p><p>80 In this example, each image is associated with latent variables h = (z1 , z2 ), where z1 denotes the location of the “person” and z2 denotes the location of the “bicycle”. [sent-356, score-0.437]
</p><p>81 To reduce the search space of inference, we ﬁrst apply off-the-shelf “person” and “bicycle” detectors [5] on 7  Method BOF + linear SVM BOF + kernel SVM linear LSVM KLSVM Acc(%) 42. [sent-357, score-0.212]
</p><p>82 For the approaches using latent variables, we show the mean/std of classiﬁcation accuracies over ﬁve folds of experiments. [sent-364, score-0.259]
</p><p>83 Figure 3: Visualization of how latent variables (i. [sent-365, score-0.26]
</p><p>84 The left image is from the “person riding a bicycle” category, and the right image is from the “person next to a car” category. [sent-368, score-0.262]
</p><p>85 Yellow bounding boxes corresponds to the initial object locations. [sent-369, score-0.281]
</p><p>86 The blue bounding boxes correspond to the object locations after the learning. [sent-370, score-0.322]
</p><p>87 Then the kernel between two images can be deﬁned as follows: K(Φ(x, h), Φ(x , h )) =  ku (φ(x, zu ), φ(x , zu )) + kp (ψ(z1 , z2 ), ψ(z1 , z2 ))  (13)  u={1,2}  We deﬁne φ(x, zu ) as the bag-of-features (BOF) extracted from the bounding box zu in the image x. [sent-377, score-1.332]
</p><p>88 The kernel kp (·) captures the spatial relationship between z1 and z2 such as above, below, overlapping, next-to, near, and far. [sent-383, score-0.227]
</p><p>89 Note that this is a strong baseline since [3] has shown that a similar pyramid feature representation with kernel SVM achieves top performances on the task of person-object interaction recognition. [sent-391, score-0.218]
</p><p>90 We run the experiments for ﬁve rounds for approaches using latent variables. [sent-396, score-0.256]
</p><p>91 The kernel latent SVM that uses HIK for ku (·) achieves the best performance. [sent-399, score-0.428]
</p><p>92 3 shows examples of how the latent variables change on some training images during the learning of the KLSVM. [sent-401, score-0.406]
</p><p>93 For each training image, both latent variables z1 and z2 are randomly initialized to one of ﬁve candidate bounding boxes. [sent-402, score-0.395]
</p><p>94 As we can see, the initial bounding boxes can accurately locate the target objects but their spatial relations are different to ground-truth labels. [sent-403, score-0.217]
</p><p>95 After learning algorithm terminates, the latent variables not only locate the target objects, but more importantly they also capture the correct spatial relationship between objects. [sent-404, score-0.291]
</p><p>96 5  Conclusion  We have proposed kernel latent SVM – a new learning framework that combines the beneﬁts of LSVM and kernel methods. [sent-405, score-0.524]
</p><p>97 The latent variables can not only be a single discrete value, but also be more complex values with interdependent structures. [sent-407, score-0.26]
</p><p>98 Our experimental results on three different applications in visual recognition demonstrate that KLSVM outperforms using LSVM or using kernel methods alone. [sent-408, score-0.226]
</p><p>99 Classiﬁcation using intersection kernel support vector machines is efﬁcient. [sent-476, score-0.194]
</p><p>100 Discriminative tag learning on youtube videos with latent sub-tags. [sent-518, score-0.254]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsvm', 0.476), ('klsvm', 0.378), ('ht', 0.283), ('hi', 0.27), ('latent', 0.228), ('zu', 0.19), ('bof', 0.175), ('person', 0.154), ('kernel', 0.148), ('svm', 0.143), ('riding', 0.132), ('hik', 0.127), ('object', 0.124), ('xt', 0.121), ('subcategory', 0.111), ('bicycle', 0.11), ('bike', 0.102), ('bounding', 0.095), ('images', 0.08), ('car', 0.075), ('bird', 0.068), ('svms', 0.067), ('image', 0.065), ('xnew', 0.063), ('hold', 0.063), ('boxes', 0.062), ('classi', 0.062), ('subcategories', 0.059), ('hog', 0.058), ('location', 0.056), ('category', 0.055), ('kp', 0.055), ('dual', 0.055), ('fraser', 0.054), ('zv', 0.052), ('ku', 0.052), ('dpm', 0.048), ('xj', 0.046), ('ve', 0.045), ('box', 0.045), ('vision', 0.044), ('visual', 0.043), ('mammal', 0.041), ('kernels', 0.041), ('locations', 0.041), ('training', 0.04), ('fw', 0.038), ('hc', 0.038), ('template', 0.037), ('acc', 0.036), ('hnew', 0.036), ('lsvms', 0.036), ('zbike', 0.036), ('zperson', 0.036), ('interaction', 0.036), ('variable', 0.035), ('recognition', 0.035), ('arg', 0.034), ('feature', 0.034), ('extracted', 0.032), ('linear', 0.032), ('variables', 0.032), ('maxh', 0.032), ('mori', 0.032), ('locate', 0.031), ('ers', 0.031), ('accuracies', 0.031), ('composite', 0.03), ('weakly', 0.03), ('baselines', 0.03), ('objects', 0.029), ('visually', 0.029), ('kernelized', 0.029), ('simon', 0.028), ('rounds', 0.028), ('nonlinear', 0.028), ('labelings', 0.028), ('boat', 0.028), ('localization', 0.027), ('positive', 0.027), ('detector', 0.027), ('whole', 0.026), ('testing', 0.026), ('youtube', 0.026), ('examples', 0.026), ('detection', 0.026), ('penalizes', 0.026), ('unobserved', 0.025), ('margin', 0.025), ('handling', 0.025), ('decision', 0.024), ('computer', 0.024), ('enumerating', 0.024), ('xi', 0.024), ('limitation', 0.024), ('captures', 0.024), ('intersection', 0.023), ('support', 0.023), ('intuitive', 0.023), ('cation', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="168-tfidf-1" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>2 0.17182815 <a title="168-tfidf-2" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>Author: Marius Pachitariu, Maneesh Sahani</p><p>Abstract: We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a speciﬁc pattern of recurrent connections learned by the model. 1</p><p>3 0.15132993 <a title="168-tfidf-3" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>4 0.14664666 <a title="168-tfidf-4" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>5 0.13024627 <a title="168-tfidf-5" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>6 0.12357195 <a title="168-tfidf-6" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>7 0.12336375 <a title="168-tfidf-7" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>8 0.11085517 <a title="168-tfidf-8" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>9 0.10621398 <a title="168-tfidf-9" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>10 0.10492373 <a title="168-tfidf-10" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>11 0.09788432 <a title="168-tfidf-11" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>12 0.095745601 <a title="168-tfidf-12" href="./nips-2012-Stochastic_Gradient_Descent_with_Only_One_Projection.html">324 nips-2012-Stochastic Gradient Descent with Only One Projection</a></p>
<p>13 0.089822128 <a title="168-tfidf-13" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>14 0.088769548 <a title="168-tfidf-14" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>15 0.087238744 <a title="168-tfidf-15" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>16 0.086176179 <a title="168-tfidf-16" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>17 0.08585266 <a title="168-tfidf-17" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>18 0.085665718 <a title="168-tfidf-18" href="./nips-2012-Non-linear_Metric_Learning.html">242 nips-2012-Non-linear Metric Learning</a></p>
<p>19 0.085624762 <a title="168-tfidf-19" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>20 0.082854815 <a title="168-tfidf-20" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.212), (1, 0.033), (2, -0.122), (3, 0.05), (4, 0.13), (5, -0.119), (6, -0.009), (7, -0.025), (8, -0.035), (9, -0.101), (10, 0.002), (11, 0.064), (12, 0.177), (13, -0.032), (14, 0.08), (15, 0.045), (16, -0.028), (17, 0.068), (18, -0.009), (19, 0.071), (20, -0.008), (21, -0.071), (22, 0.036), (23, -0.164), (24, -0.064), (25, 0.025), (26, -0.031), (27, -0.036), (28, 0.001), (29, 0.026), (30, -0.138), (31, -0.048), (32, -0.01), (33, 0.01), (34, -0.003), (35, -0.016), (36, -0.07), (37, -0.04), (38, 0.112), (39, -0.06), (40, -0.048), (41, 0.056), (42, -0.002), (43, 0.071), (44, 0.025), (45, -0.01), (46, -0.061), (47, -0.057), (48, -0.023), (49, -0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9515698 <a title="168-lsi-1" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>2 0.72882962 <a title="168-lsi-2" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>3 0.63687754 <a title="168-lsi-3" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>4 0.63125587 <a title="168-lsi-4" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>Author: Shulin Yang, Liefeng Bo, Jue Wang, Linda G. Shapiro</p><p>Abstract: Fine-grained recognition refers to a subordinate level of recognition, such as recognizing different species of animals and plants. It differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape and structure shared cross different categories, and the differences are in the details of object parts. We suggest that the key to identifying the ﬁne-grained differences lies in ﬁnding the right alignment of image regions that contain the same object parts. We propose a template model for the purpose, which captures common shape patterns of object parts, as well as the cooccurrence relation of the shape patterns. Once the image regions are aligned, extracted features are used for classiﬁcation. Learning of the template model is efﬁcient, and the recognition results we achieve signiﬁcantly outperform the stateof-the-art algorithms. 1</p><p>5 0.61240703 <a title="168-lsi-5" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>Author: Sung J. Hwang, Kristen Grauman, Fei Sha</p><p>Abstract: When learning features for complex visual recognition problems, labeled image exemplars alone can be insufﬁcient. While an object taxonomy specifying the categories’ semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classiﬁcation task, nor does a single taxonomy capture all ties that are relevant. In light of these issues, we propose a discriminative feature learning approach that leverages multiple hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reﬂect their phylogenic ties, while another could reﬂect their habitats). For each taxonomy, we ﬁrst learn a tree of semantic kernels, where each node has a Mahalanobis kernel optimized to distinguish between the classes in its children nodes. Then, using the resulting semantic kernel forest, we learn class-speciﬁc kernel combinations to select only those relationships relevant to recognize each object class. To learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies’ structure. We demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields signiﬁcant accuracy improvements.</p><p>6 0.61193436 <a title="168-lsi-6" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>7 0.61022758 <a title="168-lsi-7" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>8 0.5649249 <a title="168-lsi-8" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>9 0.55570185 <a title="168-lsi-9" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>10 0.54762304 <a title="168-lsi-10" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>11 0.54534334 <a title="168-lsi-11" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>12 0.54247499 <a title="168-lsi-12" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>13 0.53471041 <a title="168-lsi-13" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>14 0.5323177 <a title="168-lsi-14" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>15 0.53157908 <a title="168-lsi-15" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>16 0.52088624 <a title="168-lsi-16" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>17 0.50855869 <a title="168-lsi-17" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>18 0.50490898 <a title="168-lsi-18" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>19 0.47297207 <a title="168-lsi-19" href="./nips-2012-Selective_Labeling_via_Error_Bound_Minimization.html">305 nips-2012-Selective Labeling via Error Bound Minimization</a></p>
<p>20 0.47002652 <a title="168-lsi-20" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (8, 0.2), (17, 0.011), (21, 0.047), (38, 0.097), (39, 0.014), (42, 0.047), (54, 0.011), (55, 0.028), (74, 0.137), (76, 0.132), (80, 0.117), (92, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82687867 <a title="168-lda-1" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>2 0.76822168 <a title="168-lda-2" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>Author: James T. Kwok, Ryan P. Adams</p><p>Abstract: Probabilistic latent variable models are one of the cornerstones of machine learning. They offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference. Such models are useful for exploratory analysis and visualization, for building density models of data, and for providing features that can be used for later discriminative tasks. A signiﬁcant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d. assumptions on internal parameters. For example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-occurring words only appear in a small number of topics. In this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the underlying i.i.d. prior with a determinantal point process (DPP). The DPP allows us to specify a preference for diversity in our latent variables using a positive deﬁnite kernel function. Using a kernel between probability distributions, we are able to deﬁne a DPP on probability measures. We show how to perform MAP inference with DPP priors in latent Dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model. 1</p><p>3 0.75484031 <a title="168-lda-3" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>Author: Aaron Wilson, Alan Fern, Prasad Tadepalli</p><p>Abstract: We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent’s goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efﬁcient than random selection. 1</p><p>4 0.74858987 <a title="168-lda-4" href="./nips-2012-The_Time-Marginalized_Coalescent_Prior_for_Hierarchical_Clustering.html">339 nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</a></p>
<p>Author: Levi Boyles, Max Welling</p><p>Abstract: We introduce a new prior for use in Nonparametric Bayesian Hierarchical Clustering. The prior is constructed by marginalizing out the time information of Kingman’s coalescent, providing a prior over tree structures which we call the Time-Marginalized Coalescent (TMC). This allows for models which factorize the tree structure and times, providing two beneﬁts: more ﬂexible priors may be constructed and more efﬁcient Gibbs type inference can be used. We demonstrate this on an example model for density estimation and show the TMC achieves competitive experimental results. 1</p><p>5 0.74574924 <a title="168-lda-5" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>Author: Aditya Khosla, Jianxiong Xiao, Antonio Torralba, Aude Oliva</p><p>Abstract: While long term human visual memory can store a remarkable amount of visual information, it tends to degrade over time. Recent works have shown that image memorability is an intrinsic property of an image that can be reliably estimated using state-of-the-art image features and machine learning algorithms. However, the class of features and image information that is forgotten has not been explored yet. In this work, we propose a probabilistic framework that models how and which local regions from an image may be forgotten using a data-driven approach that combines local and global images features. The model automatically discovers memorability maps of individual images without any human annotation. We incorporate multiple image region attributes in our algorithm, leading to improved memorability prediction of images as compared to previous works. 1</p><p>6 0.74283439 <a title="168-lda-6" href="./nips-2012-A_Generative_Model_for_Parts-based_Object_Segmentation.html">8 nips-2012-A Generative Model for Parts-based Object Segmentation</a></p>
<p>7 0.74282342 <a title="168-lda-7" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>8 0.7378574 <a title="168-lda-8" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>9 0.73319238 <a title="168-lda-9" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>10 0.73202211 <a title="168-lda-10" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>11 0.72951257 <a title="168-lda-11" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>12 0.72241449 <a title="168-lda-12" href="./nips-2012-The_Lov%C3%A1sz_%CF%91_function%2C_SVMs_and_finding_large_dense_subgraphs.html">337 nips-2012-The Lovász ϑ function, SVMs and finding large dense subgraphs</a></p>
<p>13 0.72061551 <a title="168-lda-13" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>14 0.72021145 <a title="168-lda-14" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>15 0.72017342 <a title="168-lda-15" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>16 0.72003615 <a title="168-lda-16" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>17 0.71872872 <a title="168-lda-17" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>18 0.71871179 <a title="168-lda-18" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>19 0.7178331 <a title="168-lda-19" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>20 0.71762305 <a title="168-lda-20" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
