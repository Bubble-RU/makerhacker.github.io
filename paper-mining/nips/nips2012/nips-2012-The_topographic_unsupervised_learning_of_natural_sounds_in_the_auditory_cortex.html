<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-341" href="#">nips2012-341</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</h1>
<br/><p>Source: <a title="nips-2012-341-pdf" href="http://papers.nips.cc/paper/4703-the-topographic-unsupervised-learning-of-natural-sounds-in-the-auditory-cortex.pdf">pdf</a></p><p>Author: Hiroki Terashima, Masato Okada</p><p>Abstract: The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efﬁcient coding strategy and that the disorder in A1 reﬂects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reﬂected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.</p><p>Reference: <a title="nips-2012-341-reference" href="../nips2012_reference/nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The topographic unsupervised learning of natural sounds in the auditory cortex  Hiroki Terashima The University of Tokyo / JSPS Tokyo, Japan teratti@teratti. [sent-1, score-0.991]
</p><p>2 jp  Abstract The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. [sent-5, score-0.621]
</p><p>3 Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. [sent-6, score-0.761]
</p><p>4 This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efﬁcient coding strategy and that the disorder in A1 reﬂects natural sound statistics. [sent-7, score-0.535]
</p><p>5 To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. [sent-8, score-0.256]
</p><p>6 In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reﬂected in the disordered map. [sent-9, score-0.833]
</p><p>7 The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. [sent-10, score-1.21]
</p><p>8 1 Introduction Despite the anatomical and functional similarities between the primary auditory cortex (A1) and the primary visual cortex (V1), the computational modelling of A1 has proven to be less fruitful than V1, primarily because the responses of A1 cells are more disorganized. [sent-12, score-0.943]
</p><p>9 For instance, the receptive ﬁelds of V1 cells are localized within a small portion of the ﬁeld of view [1], whereas certain A1 cells have receptive ﬁelds that are not localized, as these A1 cells demonstrate signiﬁcant responses to multiple distant frequencies [2, 3]. [sent-13, score-1.123]
</p><p>10 , the retinotopy of V1 and the tonotopy of A1; these structures had long been considered to be quite similar, but studies on a microscopic scale have demonstrated that in mice, the tonotopy of A1 is much more disordered [4, 5] than the retinotopy of V1 [6, 7]. [sent-16, score-1.342]
</p><p>11 A number of computational modelling studies have emphasized the close associations between V1 cells and natural image statistics, which suggests that the V1 adopts an unsupervised, efﬁcient coding strategy [10]. [sent-19, score-0.375]
</p><p>12 For instance, the receptive ﬁelds of V1 simple cells were reproduced by either sparse coding [11] or the independent component analysis [12] of natural images. [sent-20, score-0.434]
</p><p>13 Similar efforts to address A1 have been attempted by only a few studies, which demonstrated that the efﬁcient coding of natural, harmonic sounds, such as human voices or piano 1  recordings, can explain the basic receptive ﬁelds of A1 cells [16, 17] and their harmony-related responses [18, 19]. [sent-22, score-0.728]
</p><p>14 In an integrated and computational manner, the present paper attempts to explain why the tonotopy of A1 is more disordered than the retinotopy of V1. [sent-24, score-0.703]
</p><p>15 We hypothesized that V1 and A1 still share an efﬁcient coding strategy, and we therefore proposed that the distant correlations in natural sounds would be responsible for the relative disorder in A1. [sent-25, score-0.94]
</p><p>16 To test this hypothesis, we ﬁrst demonstrated the signiﬁcant differences between natural images and natural sounds. [sent-26, score-0.303]
</p><p>17 Natural images and natural sounds were then each used as inputs for topographic independent component analysis, a model that had previously been proposed for the smooth topography of V1, and maps were generated for these images and sounds. [sent-27, score-0.973]
</p><p>18 Due to the distant correlations of natural sounds, greater disorder was observed in the learned map that had been adapted to natural sounds than in the analogous map that had been adapted to images. [sent-28, score-1.002]
</p><p>19 For natural sounds, this model not only predicted harmonic relationships between neighbouring cells but also demonstrated nonlinear responses that appeared similar to the responses of the pitch-selective cells that were recently found in A1. [sent-29, score-1.167]
</p><p>20 These results suggest that the apparently dissimilar topographies of V1 and A1 may reﬂect statistical differences between natural images and natural sounds; however, these two regions may employ a common adaptive strategy. [sent-30, score-0.289]
</p><p>21 1  Topographic independent component analysis  Herein, we discuss an unsupervised learning model termed topographic independent component analysis (TICA), which was originally proposed for the study of V1 topography [13, 14]. [sent-32, score-0.338]
</p><p>22 This model comprises two layers: the ﬁrst layer of N units models the linear responses of V1 simple cells, whereas the second layer of N units models the nonlinear responses of V1 complex cells, and the connections between the layers deﬁne a topography. [sent-33, score-0.718]
</p><p>23 Given a whitened input vector I(x) ∈ Rd (here, d = N ), the input is reconstructed by the linear superposition of a basis ai ∈ Rd , each of which corresponds to the ﬁrst-layer units ∑ I= si ai (1) i  where si ∈ R are activity levels of the units or model neurons. [sent-34, score-0.617]
</p><p>24 Using the activities of the ﬁrst layer, the activities of the second-layer units ci ∈ R can be deﬁned as follows: ∑ ci = h(i, j)s2 (2) j j  where h(i, j) is the neighbourhood function that takes the value of 1 if i and j are neighbours and is 0 otherwise. [sent-36, score-0.391]
</p><p>25 (B) The correlation matrix of the human voice spectra (right) demonstrated not only local correlation but also off-diagonal distant correlations produced by harmonics. [sent-48, score-0.504]
</p><p>26 2  The discontinuity index for topographic representation  To compare the degrees of disorder in topographies of different modalities, we deﬁned a discontinuity index (DI) for each point i of the maps. [sent-55, score-0.51]
</p><p>27 1  Results Correlations of natural images and natural sounds  Given that V1 is supposed to adapt to natural images and that A1 is supposed to adapt to natural sounds, the ﬁrst analysis in this study simply compared statistics for natural images and natural sounds. [sent-63, score-1.041]
</p><p>28 6k [Hz]  F  CF [Hz]  0  Tonotopy  CF [Hz]  Number of units  B  C  Tonotopy  90  90  200 [ms]  Figure 2: The ordered retinotopy and disordered tonotopy. [sent-71, score-0.581]
</p><p>29 (A) The topography of units adapted to natural images. [sent-72, score-0.459]
</p><p>30 (C) The topography of spectro-temporal units that have been adapted to natural sounds. [sent-75, score-0.459]
</p><p>31 (D-F) The retinotopy of the visual map (D) is smooth, whereas the tonotopy of the auditory map (F) is more disordered, although global tonotopy still exists (E). [sent-76, score-1.259]
</p><p>32 For natural sounds, we used human narratives from the Handbook of the International Phonetic Association [21], as efﬁcient representations of human voices have been successful in facilitating studies of various components of the auditory system [22, 23], including A1 [16, 17]. [sent-80, score-0.531]
</p><p>33 After these sounds were downsampled to 4 kHz, their spectrograms were generated using the NSL toolbox [24] to approximate peripheral auditory processing. [sent-81, score-0.76]
</p><p>34 The most prominent off-diagonal correlation, which was just 1 octave away from the main diagonal, corresponded to the second harmonic of a sound, i. [sent-85, score-0.27]
</p><p>35 These distant correlations represent relatively typical results for natural sounds and differ greatly from the strictly local correlations observed for natural images. [sent-91, score-0.871]
</p><p>36 2  Greater disorder for the tonotopy than the retinotopy  To test the hypothesis that V1 and A1 share a learning strategy, the TICA model was applied to natural images and natural sounds, which exhibit different statistical proﬁles, as discussed above. [sent-93, score-0.94]
</p><p>37 4  Strength of distant correlation Vision-like  Audition-like  Figure 3: The correlation between discontinuity and input “auditoriness”. [sent-98, score-0.343]
</p><p>38 Figure 2A illustrates the visual topographic map obtained from this analysis, a small square of which constitutes a basis vector ai . [sent-105, score-0.309]
</p><p>39 As previously observed in the original TICA study [13, 14], each unit was localized, oriented, and bandpassed; thus, these units appeared to be organized similarly to the receptive ﬁelds of V1 simple cells. [sent-106, score-0.338]
</p><p>40 Figure 2B graphically indicates that the obtained DI values were quite low, which is consistent with the smooth retinotopy illustrated in Figure 2D. [sent-109, score-0.276]
</p><p>41 Next, another TICA model was applied to natural sounds to create an auditory topographic map that could be compared to the visual topography. [sent-110, score-0.998]
</p><p>42 As detailed in the previous section, spectrograms of human voices (sampled at 8 kHz) were generated using the NSL toolbox to approximate peripheral auditory processing. [sent-111, score-0.498]
</p><p>43 Figure 2C shows the resulting auditory topographic map, which is composed of spectro-temporal units of ai that are represented by small squares. [sent-114, score-0.76]
</p><p>44 The units were localized temporally and spectrally, and some units demonstrated multiple, harmonic peaks; thus, these units appeared to reasonably represent the typical spectro-temporal receptive ﬁelds of A1 cells [16, 3]. [sent-115, score-1.108]
</p><p>45 The frequency to which an auditory neuron responds most signiﬁcantly is called its characteristic frequency (CF) [2]. [sent-116, score-0.518]
</p><p>46 Within local regions, the tonotopy was not necessarily smooth, i. [sent-121, score-0.303]
</p><p>47 However, at a global level, a smooth tonotopy was observed (Figure 2E). [sent-124, score-0.339]
</p><p>48 The distribution of tonotopic DI values is shown in Figure 2B, which clearly demonstrates that the tonotopy was more disordered than the retinotopy (p < 0. [sent-126, score-0.767]
</p><p>49 For this purpose, we generated artiﬁcial inputs (d = 16) with a parameter pa ∈ [0, 1] that regulates the degree of distant correlations. [sent-130, score-0.274]
</p><p>50 4 Distance of two units  Figure 4: The harmonic relationships between CFs of neighbouring units. [sent-147, score-0.555]
</p><p>51 There were three peaks that indicate harmonic relationships between neighbouring units. [sent-150, score-0.409]
</p><p>52 The topography was also set as a one-dimensional torus of 16 units with a neighbourhood window size of 5. [sent-157, score-0.459]
</p><p>53 If the input only demonstrated local correlations like visual stimuli (pa ∼ 0), then its learned topography was smooth (i. [sent-161, score-0.41]
</p><p>54 The DI values generally increased as distant correlations appeared more frequently, i. [sent-164, score-0.314]
</p><p>55 Thus, the topographic disorder of auditory maps results from distant correlations presented by natural auditory signals. [sent-167, score-1.419]
</p><p>56 4  The harmonic relationship among neighbouring units  Several experiments [4, 5] have reported that the CFs of neighbouring cells can differ by up to 4 octaves, although these studies have failed to provide additional detail regarding the local spatial patterns of the CF distributions. [sent-169, score-0.928]
</p><p>57 However, if the auditory topography is representative of natural stimulus statistics, the topographic map is likely to possess certain additional spatial features that reﬂect the statistical characteristics of natural sounds. [sent-170, score-0.864]
</p><p>58 To enable a detailed investigation of the CF distribution, we employed a model that had been adapted to ﬁner frequency spectra of natural sounds, and this model was then used throughout the remainder of the study. [sent-171, score-0.265]
</p><p>59 As the temporal structure of the auditory receptive ﬁelds was less dominant than their spectral structure (Figure 2C), we focused solely on the spectral domain and did not attempt to address temporal information. [sent-172, score-0.449]
</p><p>60 Therefore, the inputs for the new model (n = 100,000) were short-time frequency spectra of 128 pixels each (24 pixels = 1 octave). [sent-173, score-0.285]
</p><p>61 The CFs of even neighbouring units differed by up to ∼ 4 octaves, which is consistent with recent experimental ﬁndings [4, 5]. [sent-181, score-0.363]
</p><p>62 2  90  0 0 f0  4*f0 8*f0 Frequency [Hz]  12*f0  8k [Hz] CF  Normalized activity  B  Harmonic composition of MFs  A  f0 2 4 6 8 10 Lowest harmonic present  Pitch selective units  Figure 5: Nonlinear responses similar to pitch selectivity. [sent-188, score-0.597]
</p><p>63 (C) The distribution of pitch-selective units on the smoothed tonotopy in a single session. [sent-191, score-0.484]
</p><p>64 These examples indicate that CFs of neighbouring units did not differ randomly, but tended to be harmonically related. [sent-197, score-0.395]
</p><p>65 Thus, this prediction of a harmonic relationship in neighbouring CFs will need to be examined in more detailed investigations. [sent-200, score-0.34]
</p><p>66 ), then its pitch is the frequency of the lowest harmonic, which is called the fundamental frequency f0 . [sent-206, score-0.312]
</p><p>67 The perception of pitch is known to remain constant even if the sound lacks power at lower harmonics; in fact, pitch at f0 can be perceived from a sound that lacks f0 , a phenomenon known as “missing fundamental” [26]. [sent-207, score-0.355]
</p><p>68 For each unit, responses were calculated to complex tones termed missing fundamental complex tones (MFs) [27]. [sent-211, score-0.271]
</p><p>69 The MFs were composed of three consecutive harmonics sharing a single f0 ; the lowest frequency for these consecutive harmonics varied from the fundamental frequency (f0 ) to the tenth harmonic (10f0 ), as shown in Figure 5A. [sent-212, score-0.53]
</p><p>70 Figure 5B illustrates the response proﬁles of the pitch-selective units, which demonstrated sustained activity for MFs with a lowest harmonic below the sixth harmonic (6f0 ), and this result is similar to previously published data [27]. [sent-219, score-0.499]
</p><p>71 Additionally, these units were located in a low-frequency region of the global tonotopy, as shown in Figure 5C, and this feature of pitch-selective units is also consistent with previous ﬁndings [27]. [sent-220, score-0.362]
</p><p>72 The second layer of the TICA model, which contained the pitch-selective units, was originally designed to represent the layer of V1 complex cells, which have nonlinear responses that can be modelled by a summation of “energies” of neighbouring simple cells [13, 14, 15]. [sent-221, score-0.626]
</p><p>73 7  B Natural sounds  Correlation 0  Retinal position  0  No correlation (different objects)  Retinal distance 2:3 1:2  Smooth V1 retinotopy Cortical position  1:3  Disordered A1 tonotopy  CF  Natural images  Correlation  A  1. [sent-223, score-1.036]
</p><p>74 4  Discussion  Using a single model, we have provided a computational account explaining why the tonotopy of A1 is more disordered than the retinotopy of V1. [sent-225, score-0.703]
</p><p>75 First, we demonstrated that there are signiﬁcant differences between natural images and natural sounds; in particular, the latter evince distant correlations, whereas the former do not. [sent-226, score-0.488]
</p><p>76 The topographic independent component analysis therefore generated a disordered tonotopy for these sounds, whereas the retinotopy adapted to natural images was locally organized throughout. [sent-227, score-1.078]
</p><p>77 Detailed analyses of the TICA model predicted harmonic relationships among neighbouring neurons; furthermore, these analyses successfully replicated pitch selectivity, a nonlinear response of actual cells, using a mechanism that was designed to model V1 complex cells. [sent-228, score-0.56]
</p><p>78 The results suggest that A1 and V1 may share an adaptive strategy, and the dissimilar topographies of visual and auditory maps may therefore reﬂect signiﬁcant differences in the natural stimuli. [sent-229, score-0.546]
</p><p>79 Natural images correlate only locally, which produces a smooth retinotopy through an efﬁcient coding strategy (Figure 6A). [sent-231, score-0.412]
</p><p>80 By contrast, natural sounds exhibit additional distant correlations (primarily correlations among harmonics), which produce the topographic disorganization observed for A1 (Figure 6B). [sent-232, score-0.968]
</p><p>81 Our ﬁnal result suggested that a common mechanism may underlie the complex cells of V1 and the pitch-selective cells of A1. [sent-235, score-0.418]
</p><p>82 Additional support for this notion was provided by recent evidence indicating that the pitch-selective cells are most commonly found in the supragranular layer [27], and V1 complex cells display a similar tendency. [sent-236, score-0.461]
</p><p>83 To the best of our knowledge, no previous studies in the literature have attempted to use this analogy of V1 complex cells to explain A1 pitch-selective cells (however, other potential analogues have been mentioned [31, 32]). [sent-238, score-0.452]
</p><p>84 Another issue that must be addressed is what functional roles the other units in the second layer play. [sent-240, score-0.252]
</p><p>85 Modular organization of frequency integration in primary auditory cortex. [sent-257, score-0.526]
</p><p>86 Dichotomy of functional organization in the mouse auditory cortex. [sent-271, score-0.458]
</p><p>87 Functional organization and population dynamics in the mouse primary auditory cortex. [sent-277, score-0.484]
</p><p>88 The spatial distribution of unit characteristic frequency in the primary auditory cortex of the cat. [sent-301, score-0.575]
</p><p>89 Functional architecture in cat primary auditory cortex: tonotopic organization. [sent-310, score-0.474]
</p><p>90 A two-layer sparse coding model learns simple and complex cell receptive a ﬁelds and topography from natural images. [sent-338, score-0.435]
</p><p>91 A hierarchical generative model for overcomplete topographic representations in natural images. [sent-350, score-0.306]
</p><p>92 Unsupervised learning models of primary cortical receptive ﬁelds and receptive ﬁeld plasticity. [sent-368, score-0.275]
</p><p>93 Sparse codes of harmonic natural sounds and their modulatory interactions. [sent-373, score-0.561]
</p><p>94 Sparse coding of harmonic vocalization in monkey auditory cortex. [sent-381, score-0.579]
</p><p>95 Independent component ﬁlters of natural images compared with simple cells in primary visual cortex. [sent-392, score-0.458]
</p><p>96 Measurement of absolute auditory thresholds in the common marmoset (callithrix jacchus). [sent-423, score-0.356]
</p><p>97 The neuronal representation of pitch in primate auditory cortex. [sent-435, score-0.478]
</p><p>98 Experimentally induced visual projections into auditory thalamus and cortex. [sent-443, score-0.413]
</p><p>99 Brainstem inputs to the ferret medial geniculate nucleus and the a effect of early deafferentation on novel retinal projections to the auditory thalamus. [sent-449, score-0.437]
</p><p>100 Nonlinearity of coding in primary auditory cortex of the awake ferret. [sent-461, score-0.525]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('auditory', 0.356), ('sounds', 0.318), ('tonotopy', 0.303), ('retinotopy', 0.24), ('cells', 0.191), ('distant', 0.185), ('neighbouring', 0.182), ('topographic', 0.182), ('units', 0.181), ('disordered', 0.16), ('harmonic', 0.158), ('topography', 0.156), ('disorder', 0.156), ('tica', 0.144), ('cf', 0.141), ('pitch', 0.122), ('octave', 0.112), ('responses', 0.103), ('correlations', 0.099), ('mfs', 0.096), ('receptive', 0.093), ('harmonics', 0.091), ('di', 0.088), ('natural', 0.085), ('cfs', 0.085), ('frequency', 0.081), ('auditoriness', 0.08), ('images', 0.071), ('octaves', 0.07), ('coding', 0.065), ('bandpassed', 0.064), ('tonotopic', 0.064), ('torus', 0.064), ('demonstrated', 0.062), ('discontinuity', 0.062), ('spectra', 0.062), ('neighbourhood', 0.058), ('visual', 0.057), ('voices', 0.056), ('primary', 0.054), ('inputs', 0.054), ('elds', 0.052), ('hz', 0.051), ('cortex', 0.05), ('si', 0.05), ('spectrograms', 0.049), ('phonetic', 0.049), ('correlation', 0.048), ('nsl', 0.048), ('terashima', 0.048), ('tones', 0.048), ('topographies', 0.048), ('ci', 0.045), ('frequencies', 0.045), ('pixels', 0.044), ('layer', 0.043), ('ai', 0.041), ('sound', 0.041), ('whitened', 0.04), ('mouse', 0.039), ('tokyo', 0.039), ('overcomplete', 0.039), ('peripheral', 0.037), ('khz', 0.037), ('hateren', 0.037), ('retinotopic', 0.037), ('neuroscience', 0.037), ('adapted', 0.037), ('smooth', 0.036), ('complex', 0.036), ('peaks', 0.035), ('organization', 0.035), ('cortical', 0.035), ('pa', 0.035), ('unit', 0.034), ('studies', 0.034), ('relationships', 0.034), ('activity', 0.033), ('hypothesized', 0.032), ('angelucci', 0.032), ('harmonically', 0.032), ('multipeaked', 0.032), ('nnb', 0.032), ('patches', 0.032), ('activities', 0.031), ('hyv', 0.031), ('rinen', 0.031), ('published', 0.031), ('localized', 0.031), ('appeared', 0.03), ('illustrates', 0.029), ('perceived', 0.029), ('lowest', 0.028), ('okada', 0.028), ('harmony', 0.028), ('nonlinear', 0.028), ('position', 0.028), ('functional', 0.028), ('retinal', 0.027), ('gabor', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="341-tfidf-1" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>Author: Hiroki Terashima, Masato Okada</p><p>Abstract: The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efﬁcient coding strategy and that the disorder in A1 reﬂects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reﬂected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.</p><p>2 0.19617154 <a title="341-tfidf-2" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<p>Author: Yan Karklin, Chaitanya Ekanadham, Eero P. Simoncelli</p><p>Abstract: Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important ﬁrst step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The ﬁrst layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of ﬁrst layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while ﬁne-scale structure is captured by recurrent interactions within the ﬁrst layer. When ﬁt to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a signiﬁcant improvement over standard methods. 1</p><p>3 0.16895367 <a title="341-tfidf-3" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>Author: Falk Lieder, Thomas Griffiths, Noah Goodman</p><p>Abstract: Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we ﬁnd that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to signiﬁcant global distortions like large translations and scale. 1</p><p>4 0.16895367 <a title="341-tfidf-4" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>Author: Adam Coates, Andrej Karpathy, Andrew Y. Ng</p><p>Abstract: Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we ﬁnd that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to signiﬁcant global distortions like large translations and scale. 1</p><p>5 0.12200215 <a title="341-tfidf-5" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>Author: Jaldert Rombouts, Pieter Roelfsema, Sander M. Bohte</p><p>Abstract: A key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use. Neurons in association cortex play an important role in this process: by learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity [1]. It is however not well known how such neurons acquire these task-relevant working memories. Here we introduce a biologically plausible learning scheme grounded in Reinforcement Learning (RL) theory [2] that explains how neurons become selective for relevant information by trial and error learning. The model has memory units which learn useful internal state representations to solve working memory tasks by transforming partially observable Markov decision problems (POMDP) into MDPs. We propose that synaptic plasticity is guided by a combination of attentional feedback signals from the action selection stage to earlier processing levels and a globally released neuromodulatory signal. Feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping. The neuromodulatory signal interacts with tagged synapses to determine the sign and strength of plasticity. The learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards. It explains how neurons in association cortex learn to 1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks [1, 3, 4] and 2) learn to optimally integrate probabilistic evidence for perceptual decision making [5, 6]. 1</p><p>6 0.12182651 <a title="341-tfidf-6" href="./nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">23 nips-2012-A lattice filter model of the visual pathway</a></p>
<p>7 0.11514773 <a title="341-tfidf-7" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>8 0.098553315 <a title="341-tfidf-8" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>9 0.097271457 <a title="341-tfidf-9" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>10 0.076053567 <a title="341-tfidf-10" href="./nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">114 nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>11 0.070575885 <a title="341-tfidf-11" href="./nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">365 nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<p>12 0.068889737 <a title="341-tfidf-12" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>13 0.066126332 <a title="341-tfidf-13" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>14 0.06430839 <a title="341-tfidf-14" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>15 0.062302701 <a title="341-tfidf-15" href="./nips-2012-Training_sparse_natural_image_models_with_a_fast_Gibbs_sampler_of_an_extended_state_space.html">349 nips-2012-Training sparse natural image models with a fast Gibbs sampler of an extended state space</a></p>
<p>16 0.062149651 <a title="341-tfidf-16" href="./nips-2012-A_mechanistic_model_of_early_sensory_processing_based_on_subtracting_sparse_representations.html">24 nips-2012-A mechanistic model of early sensory processing based on subtracting sparse representations</a></p>
<p>17 0.059614867 <a title="341-tfidf-17" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>18 0.058659863 <a title="341-tfidf-18" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>19 0.057755239 <a title="341-tfidf-19" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>20 0.056198306 <a title="341-tfidf-20" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.051), (2, -0.183), (3, 0.08), (4, 0.038), (5, 0.109), (6, -0.007), (7, -0.046), (8, 0.004), (9, -0.013), (10, -0.005), (11, -0.036), (12, -0.149), (13, 0.033), (14, 0.005), (15, -0.089), (16, 0.034), (17, 0.007), (18, 0.008), (19, 0.077), (20, 0.048), (21, 0.041), (22, -0.029), (23, -0.022), (24, 0.02), (25, 0.067), (26, 0.004), (27, -0.115), (28, 0.006), (29, 0.048), (30, 0.086), (31, -0.005), (32, -0.039), (33, -0.006), (34, -0.016), (35, -0.101), (36, 0.088), (37, -0.001), (38, 0.033), (39, 0.025), (40, 0.041), (41, 0.117), (42, -0.021), (43, -0.002), (44, -0.001), (45, 0.11), (46, -0.038), (47, 0.077), (48, -0.0), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95004511 <a title="341-lsi-1" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>Author: Hiroki Terashima, Masato Okada</p><p>Abstract: The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efﬁcient coding strategy and that the disorder in A1 reﬂects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reﬂected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.</p><p>2 0.69866419 <a title="341-lsi-2" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>Author: Brett Vintch, Andrew Zaharia, J Movshon, Hhmi) Hhmi), Eero P. Simoncelli</p><p>Abstract: Many visual and auditory neurons have response properties that are well explained by pooling the rectiﬁed responses of a set of spatially shifted linear ﬁlters. These ﬁlters cannot be estimated using spike-triggered averaging (STA). Subspace methods such as spike-triggered covariance (STC) can recover multiple ﬁlters, but require substantial amounts of data, and recover an orthogonal basis for the subspace in which the ﬁlters reside rather than the ﬁlters themselves. Here, we assume a linear-nonlinear–linear-nonlinear (LN-LN) cascade model in which the ﬁrst linear stage is a set of shifted (‘convolutional’) copies of a common ﬁlter, and the ﬁrst nonlinear stage consists of rectifying scalar nonlinearities that are identical for all ﬁlter outputs. We refer to these initial LN elements as the ‘subunits’ of the receptive ﬁeld. The second linear stage then computes a weighted sum of the responses of the rectiﬁed subunits. We present a method for directly ﬁtting this model to spike data, and apply it to both simulated and real neuronal data from primate V1. The subunit model signiﬁcantly outperforms STA and STC in terms of cross-validated accuracy and efﬁciency. 1</p><p>3 0.67669582 <a title="341-lsi-3" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<p>Author: Yan Karklin, Chaitanya Ekanadham, Eero P. Simoncelli</p><p>Abstract: Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important ﬁrst step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The ﬁrst layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of ﬁrst layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while ﬁne-scale structure is captured by recurrent interactions within the ﬁrst layer. When ﬁt to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a signiﬁcant improvement over standard methods. 1</p><p>4 0.66387045 <a title="341-lsi-4" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>Author: Falk Lieder, Thomas Griffiths, Noah Goodman</p><p>Abstract: Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we ﬁnd that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to signiﬁcant global distortions like large translations and scale. 1</p><p>5 0.66387045 <a title="341-lsi-5" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>Author: Adam Coates, Andrej Karpathy, Andrew Y. Ng</p><p>Abstract: Recent work in unsupervised feature learning has focused on the goal of discovering high-level features from unlabeled images. Much progress has been made in this direction, but in most cases it is still standard to use a large amount of labeled data in order to construct detectors sensitive to object classes or other complex patterns in the data. In this paper, we aim to test the hypothesis that unsupervised feature learning methods, provided with only unlabeled data, can learn high-level, invariant features that are sensitive to commonly-occurring objects. Though a handful of prior results suggest that this is possible when each object class accounts for a large fraction of the data (as in many labeled datasets), it is unclear whether something similar can be accomplished when dealing with completely unlabeled data. A major obstacle to this test, however, is scale: we cannot expect to succeed with small datasets or with small numbers of learned features. Here, we propose a large-scale feature learning system that enables us to carry out this experiment, learning 150,000 features from tens of millions of unlabeled images. Based on two scalable clustering algorithms (K-means and agglomerative clustering), we ﬁnd that our simple system can discover features sensitive to a commonly occurring object class (human faces) and can also combine these into detectors invariant to signiﬁcant global distortions like large translations and scale. 1</p><p>6 0.58652461 <a title="341-lsi-6" href="./nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">23 nips-2012-A lattice filter model of the visual pathway</a></p>
<p>7 0.55619502 <a title="341-lsi-7" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>8 0.53271073 <a title="341-lsi-8" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>9 0.47283709 <a title="341-lsi-9" href="./nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">365 nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<p>10 0.45302448 <a title="341-lsi-10" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>11 0.44822565 <a title="341-lsi-11" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>12 0.44310743 <a title="341-lsi-12" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>13 0.43861899 <a title="341-lsi-13" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>14 0.4178226 <a title="341-lsi-14" href="./nips-2012-A_mechanistic_model_of_early_sensory_processing_based_on_subtracting_sparse_representations.html">24 nips-2012-A mechanistic model of early sensory processing based on subtracting sparse representations</a></p>
<p>15 0.36732614 <a title="341-lsi-15" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>16 0.36079022 <a title="341-lsi-16" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>17 0.35541505 <a title="341-lsi-17" href="./nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">114 nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>18 0.35399652 <a title="341-lsi-18" href="./nips-2012-Neuronal_Spike_Generation_Mechanism_as_an_Oversampling%2C_Noise-shaping_A-to-D_converter.html">239 nips-2012-Neuronal Spike Generation Mechanism as an Oversampling, Noise-shaping A-to-D converter</a></p>
<p>19 0.35336584 <a title="341-lsi-19" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>20 0.34965321 <a title="341-lsi-20" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (17, 0.042), (21, 0.069), (38, 0.067), (39, 0.016), (42, 0.031), (44, 0.029), (54, 0.02), (55, 0.031), (74, 0.039), (76, 0.096), (80, 0.051), (87, 0.33), (92, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75257689 <a title="341-lda-1" href="./nips-2012-The_topographic_unsupervised_learning_of_natural_sounds_in_the_auditory_cortex.html">341 nips-2012-The topographic unsupervised learning of natural sounds in the auditory cortex</a></p>
<p>Author: Hiroki Terashima, Masato Okada</p><p>Abstract: The computational modelling of the primary auditory cortex (A1) has been less fruitful than that of the primary visual cortex (V1) due to the less organized properties of A1. Greater disorder has recently been demonstrated for the tonotopy of A1 that has traditionally been considered to be as ordered as the retinotopy of V1. This disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both A1 and V1 would adopt an efﬁcient coding strategy and that the disorder in A1 reﬂects natural sound statistics. To provide a computational model of the tonotopic disorder in A1, we used a model that was originally proposed for the smooth V1 map. In contrast to natural images, natural sounds exhibit distant correlations, which were learned and reﬂected in the disordered map. The auditory model predicted harmonic relationships among neighbouring A1 cells; furthermore, the same mechanism used to model V1 complex cells reproduced nonlinear responses similar to the pitch selectivity. These results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.</p><p>2 0.68025887 <a title="341-lda-2" href="./nips-2012-Learning_Networks_of_Heterogeneous_Influence.html">182 nips-2012-Learning Networks of Heterogeneous Influence</a></p>
<p>Author: Nan Du, Le Song, Ming Yuan, Alex J. Smola</p><p>Abstract: Information, disease, and inﬂuence diffuse over networks of entities in both natural systems and human society. Analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting future events. However, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen. In this paper, we address the challenging problem of uncovering the hidden network only from the cascades. The structure discovery problem is complicated by the fact that the inﬂuence between networked entities is heterogeneous, which can not be described by a simple parametric model. Therefore, we propose a kernelbased method which can capture a diverse range of different types of inﬂuence without any prior assumption. In both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the transmission functions among networked entities. 1</p><p>3 0.6038416 <a title="341-lda-3" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>Author: Junyuan Xie, Linli Xu, Enhong Chen</p><p>Abstract: We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method’s performance in the image denoising task is comparable to that of KSVD which is a widely used sparse coding technique. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Speciﬁcally, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning. 1</p><p>4 0.47435579 <a title="341-lda-4" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<p>Author: Yan Karklin, Chaitanya Ekanadham, Eero P. Simoncelli</p><p>Abstract: Natural sounds exhibit complex statistical regularities at multiple scales. Acoustic events underlying speech, for example, are characterized by precise temporal and frequency relationships, but they can also vary substantially according to the pitch, duration, and other high-level properties of speech production. Learning this structure from data while capturing the inherent variability is an important ﬁrst step in building auditory processing systems, as well as understanding the mechanisms of auditory perception. Here we develop Hierarchical Spike Coding, a two-layer probabilistic generative model for complex acoustic structure. The ﬁrst layer consists of a sparse spiking representation that encodes the sound using kernels positioned precisely in time and frequency. Patterns in the positions of ﬁrst layer spikes are learned from the data: on a coarse scale, statistical regularities are encoded by a second-layer spiking representation, while ﬁne-scale structure is captured by recurrent interactions within the ﬁrst layer. When ﬁt to speech data, the second layer acoustic features include harmonic stacks, sweeps, frequency modulations, and precise temporal onsets, which can be composed to represent complex acoustic events. Unlike spectrogram-based methods, the model gives a probability distribution over sound pressure waveforms. This allows us to use the second-layer representation to synthesize sounds directly, and to perform model-based denoising, on which we demonstrate a signiﬁcant improvement over standard methods. 1</p><p>5 0.43587339 <a title="341-lda-5" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>Author: Gary Huang, Marwan Mattar, Honglak Lee, Erik G. Learned-miller</p><p>Abstract: Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face veriﬁcation. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Speciﬁcally, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the speciﬁc data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned ﬁlters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face veriﬁcation compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method. 1</p><p>6 0.42992204 <a title="341-lda-6" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>7 0.42443648 <a title="341-lda-7" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>8 0.4231475 <a title="341-lda-8" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>9 0.4231475 <a title="341-lda-9" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>10 0.42156026 <a title="341-lda-10" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>11 0.42098373 <a title="341-lda-11" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>12 0.42012757 <a title="341-lda-12" href="./nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">23 nips-2012-A lattice filter model of the visual pathway</a></p>
<p>13 0.41800949 <a title="341-lda-13" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>14 0.41685534 <a title="341-lda-14" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>15 0.41623983 <a title="341-lda-15" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>16 0.41528252 <a title="341-lda-16" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>17 0.41523486 <a title="341-lda-17" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>18 0.41387102 <a title="341-lda-18" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>19 0.41292012 <a title="341-lda-19" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>20 0.41276661 <a title="341-lda-20" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
