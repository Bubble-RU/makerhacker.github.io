<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-18" href="#">nips2012-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</h1>
<br/><p>Source: <a title="nips-2012-18-pdf" href="http://papers.nips.cc/paper/4548-a-simple-and-practical-algorithm-for-differentially-private-data-release.pdf">pdf</a></p><p>Author: Moritz Hardt, Katrina Ligett, Frank Mcsherry</p><p>Abstract: We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques. 1</p><p>Reference: <a title="nips-2012-18-reference" href="../nips2012_reference/nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. [sent-5, score-0.378]
</p><p>2 1  Introduction  Sensitive statistical data on individuals are ubiquitous, and publishable analysis of such private data is an important objective. [sent-7, score-0.227]
</p><p>3 When releasing statistics or synthetic data based on sensitive data sets, one must balance the inherent tradeoff between the usefulness of the released information and the privacy of the affected individuals. [sent-8, score-0.415]
</p><p>4 Against this backdrop, differential privacy [1, 2, 3] has emerged as a compelling privacy deﬁnition that allows one to understand this tradeoff via formal, provable guarantees. [sent-9, score-0.799]
</p><p>5 In recent years, the theoretical literature on differential privacy has provided a large repertoire of techniques for achieving the deﬁnition in a variety of settings (see, e. [sent-10, score-0.459]
</p><p>6 However, data analysts have found that several algorithms for achieving differential privacy add unacceptable levels of noise. [sent-13, score-0.459]
</p><p>7 In this work we develop a broadly applicable, simple, and easy-to-implement algorithm, capable of substantially improving the performance of linear queries on many realistic datasets. [sent-14, score-0.334]
</p><p>8 Linear queries are equivalent to statistical queries (in the sense of [6]) and can serve as the basis of a wide range of data analysis and learning algorithms (see [7] for some examples). [sent-15, score-0.59]
</p><p>9 We present experimental results for differentially private data release for a variety of problems studied in prior work: range queries as studied by [11, 12], contingency table release across a collection of statistical benchmarks as in [13], and datacube release as studied by [14]. [sent-18, score-1.333]
</p><p>10 We empirically evaluate the accuracy of the differentially private data produced by MWEM using the same query class and accuracy metric proposed by each of the corresponding prior works, improving on all. [sent-19, score-0.623]
</p><p>11 Beyond empirical improvements in these settings, MWEM matches the best known and nearly optimal theoretical accuracy guarantees for differentially private data analysis with linear queries. [sent-20, score-0.404]
</p><p>12 1  Finally, we describe a scalable implementation of MWEM capable of processing datasets of substantial complexity. [sent-23, score-0.065]
</p><p>13 Producing synthetic data for the classes of queries we consider is known to be computationally hard in the worst-case [15, 16]. [sent-24, score-0.312]
</p><p>14 We repeatedly improve the accuracy of this approximation with respect to the private dataset and the desired query set by selecting and posing a query poorly served by our approximation and improving the approximation to better reﬂect the true answer to this query. [sent-29, score-0.641]
</p><p>15 We select and pose queries using the Exponential [10] and Laplace Mechanisms [3], whose deﬁnitions and privacy properties we review in Subsection 2. [sent-30, score-0.621]
</p><p>16 1  Differential Privacy and Mechanisms  Differential privacy is a constraint on a randomized computation that the computation should not reveal speciﬁcs of individual records present in the input. [sent-35, score-0.441]
</p><p>17 It places this constraint by requiring the mechanism to behave almost identically on any two datasets that are sufﬁciently close. [sent-36, score-0.209]
</p><p>18 Imagine a dataset A whose records are drawn from some abstract domain D, and which is described as a function from D to the natural numbers N, with A(x) indicating the frequency (number of occurrences) of x in the dataset. [sent-37, score-0.16]
</p><p>19 We use kA Bk to indicate the sum of the absolute values of difference in frequencies (how many records would have to be added or removed to change A to B). [sent-38, score-0.101]
</p><p>20 A mechanism M mapping datasets to distributions over an output space R provides (", )-differential privacy if for every S ✓ R and for all data sets A, B where kA Bk  1, P r[M (A) 2 S]  e" Pr[M (B) 2 S] + . [sent-41, score-0.549]
</p><p>21 The Exponential Mechanism [10] is an "-differentially private mechanism that can be used to select among the best of a discrete set of alternatives, where “best” is deﬁned by a function relating each alternative to the underlying secret data. [sent-43, score-0.394]
</p><p>22 Intuitively, the mechanism selects result r biased exponentially by its quality score. [sent-48, score-0.188]
</p><p>23 A linear query (also referred to as counting query or statistical query) is speciﬁed by a function q mapping data records to the interval [ 1, +1]. [sent-50, score-0.479]
</p><p>24 The answer of a linear query on a data set D, denoted P q(B), is the sum x2D q(x) ⇥ B(x). [sent-51, score-0.168]
</p><p>25 The Laplace Mechanism is an "-differentially private mechanism which reports approximate sums of bounded functions across a dataset. [sent-52, score-0.394]
</p><p>26 2  Inputs: Data set B over a universe D; Set Q of linear queries; Number of iterations T 2 N; Privacy parameter " > 0; Number of records n. [sent-55, score-0.142]
</p><p>27 Exponential Mechanism: Select a query qi 2 Q using the Exponential Mechanism parameterized with epsilon value "/2T and the score function si (B, q) = |q(Ai  1)  q(B)| . [sent-61, score-0.299]
</p><p>28 Note that these bounds are worst-case bounds, over adversarially chosen data and query sets. [sent-69, score-0.168]
</p><p>29 The algorithm is embarrassingly parallel: query evaluation can be conducted independently, implemented using modern database technology; the only required serialization is that the T steps must proceed in sequence, but within each step essentially all work is parallelizable. [sent-74, score-0.168]
</p><p>30 [17] show that for worst case data, producing differentially private synthetic data for a set of counting queries requires time |D|0. [sent-76, score-0.732]
</p><p>31 Moreover, Ullman and Vadhan [16] showed that similar lower bounds also hold for more basic query classes such as we consider in Section 3. [sent-78, score-0.168]
</p><p>32 Despite these hardness results, we provide an alternate implementation of our algorithm in Section 4 and demonstrate that its running time is acceptable on real-world data even in cases where |D| is as large as 277 , and on simple synthetic input datasets where |D| is as large as 21000 . [sent-80, score-0.123]
</p><p>33 Second, in each iteration we apply the multiplicative weights update rule for all measuments taken, multiple times; as long as any measurements do not agree with the approximating distribution (within error) we can improve the result. [sent-85, score-0.195]
</p><p>34 Finally, it is occasionally helpful to initialize A0 by performing a noisy count for each element of the domain; this consumes from the privacy budget and lessens the accuracy of subsequent queries, but is often a good trade-off. [sent-86, score-0.366]
</p><p>35 , polynomial in |D|) as well as algorithms that work in the interactive query setting. [sent-93, score-0.211]
</p><p>36 The latest of these results is the private Multiplicative Weights method of Hardt and Rothblum [8] p which achieves error rates of O( n log(|Q|)) for (", )-differential privacy (which is the same dependence achieved by applying k-fold adaptive composition [19] and optimizing T in our Theorem 2. [sent-94, score-0.59]
</p><p>37 While their algorithm works in the interactive setting, it can also be used non-interactively to produce synthetic data, albeit at a computational overhead of O(n). [sent-96, score-0.074]
</p><p>38 Prior work on linear queries includes Fienberg et al. [sent-99, score-0.31]
</p><p>39 [22] on range queries (and substantial related work [23, 24, 22, 11, 12, 25] which Li and Miklau [11, 25] show can all be seen as instances of the matrix mechanism of [22]); and Ding et al. [sent-102, score-0.528]
</p><p>40 3  Experimental Evaluation  We evaluate MWEM across a variety of query classes, datasets, and metrics as explored by prior work, demonstrating improvement in the quality of approximation (often signiﬁcant) in each case. [sent-106, score-0.189]
</p><p>41 The problems we consider are: (1) range queries under the total squared error metric, (2) binary 4  contingency table release under the relative entropy metric, and (3) datacube release under the average absolute error metric. [sent-107, score-0.918]
</p><p>42 Although contingency table release and datacube release are very similar, prior work on the two have had different focuses: small datasets over many binary attributes vs. [sent-108, score-0.691]
</p><p>43 large datasets over few categorical attributes, low-order marginals vs. [sent-109, score-0.093]
</p><p>44 Our general conclusion is that intelligently selecting the queries to measure can result in signiﬁcant accuracy improvements, in settings where accuracy is a scare resource. [sent-112, score-0.333]
</p><p>45 When the privacy parameters are very lax, or the query set very simple, direct measurement of all queries yields better results than expending some fraction of the privacy budget determining what to measure. [sent-113, score-1.148]
</p><p>46 On the other hand, in the more challenging case of restrictions on privacy for complex data and query sets, MWEM can substantially out-perform previous algorithms. [sent-114, score-0.536]
</p><p>47 1  Range Queries  A range query over a domain D = {1, . [sent-116, score-0.228]
</p><p>48 , N } is a counting query speciﬁed by the indicator function of an interval I ✓ D. [sent-119, score-0.21]
</p><p>49 Dd a range query is deﬁned by the product of indicator functions. [sent-123, score-0.196]
</p><p>50 Differentially private algorithms for range queries were speciﬁcally considered by [18, 23, 24, 22, 11, 12, 25]. [sent-124, score-0.536]
</p><p>51 As noted in [11, 25], all previously implemented algorithms for range queries can be seen as instances of the matrix mechanism of [22]. [sent-125, score-0.476]
</p><p>52 Moreover, [11, 25] show a lower bound on the total squared error achieved by the matrix mechanism in terms of the singular values of a matrix associated with the set of queries. [sent-126, score-0.19]
</p><p>53 The y-axis measures the average squared error per query, averaged over 5 independent repetitions of the experiment, as epsilon varies. [sent-168, score-0.125]
</p><p>54 The improvement is most signiﬁcant for small epsilon, diminishing as epsilon increases. [sent-169, score-0.102]
</p><p>55 We chose these data sets as they feature numerical attributes of suitable size. [sent-171, score-0.145]
</p><p>56 In Figure 2, we compare the performance of MWEM on sets of randomly chosen range queries against the SVD lower bound proved by [11, 25], varying " while keeping the number of queries ﬁxed. [sent-172, score-0.59]
</p><p>57 The SVD lower bound holds for algorithms achieving the strictly weaker guarantee of (", )differential privacy with > 0, permitting some probability of unbounded disclosure. [sent-173, score-0.34]
</p><p>58 9  1  Figure 3: Relative entropy (y-axis) as a function of epsilon (x-axis) for the mildew, rochdale, and czech datasets, respectively. [sent-215, score-0.14]
</p><p>59 The solid black horizontal line is the stated relative entropy values from Fienberg et al. [sent-219, score-0.088]
</p><p>60 bound depends on ; in our experiments we ﬁxed = 1/n when instantiating the SVD bound, as any larger value of permits mechanisms capable of exact release of individual records. [sent-221, score-0.17]
</p><p>61 [21] describe an approach to differentially private contingency table release using linear queries deﬁned by the Hadamard matrix. [sent-225, score-0.954]
</p><p>62 Importantly, all k-dimensional marginals d can be exactly recovered by examination of relatively few such queries: roughly k out of the possible 2d , improving over direct measurement of the marginals by a factor of 2k . [sent-226, score-0.146]
</p><p>63 x2D  We use several statistical datasets from Fienberg et al. [sent-232, score-0.071]
</p><p>64 [21] which combines its observations using multiplicative weights (we ﬁnd that without this modiﬁcation, [21] is terrible with respect to relative entropy). [sent-234, score-0.177]
</p><p>65 These experiments are therefore largely assessing the selective choice of measurements to take, rather than the efﬁcacy of multiplicative weights. [sent-235, score-0.152]
</p><p>66 Our ﬁndings here are fairly uniform across the datasets: the ability to measure only those queries that are informative about the dataset results in substantial savings over taking all possible measurements. [sent-237, score-0.331]
</p><p>67 For our initial settings, maintaining all three-way marginals, we see similar behavior as above: the ability to choose the measurements that are important allows substantially higher accuracy on those that matter. [sent-241, score-0.093]
</p><p>68 The red (dashed) curve represents Barak et al, and the multiple blue (solid) curves represent MWEM, with 20, 30, and 40 queries (top to bottom, respectively). [sent-292, score-0.31]
</p><p>69 As before, the xaxis is the value of epsilon guaranteed, and the y-axis is the relative entropy between the produced distribution and actual dataset. [sent-294, score-0.161]
</p><p>70 3  Data Cubes  We now change our terminology and objectives, shifting our view of contingency tables to one of datacubes. [sent-298, score-0.176]
</p><p>71 The two concepts are interchangeable, a contingency table corresponding to the datacube, and a marginal corresponding to its cuboids. [sent-299, score-0.153]
</p><p>72 Although MWEM is deﬁned with respect to a single query at a time, it generalizes to sets of counting queries, as reﬂected in a cuboid. [sent-302, score-0.21]
</p><p>73 The Exponential Mechanism can select a cuboid to measure using a quality score function summing the absolute values of the errors within the cells of the cuboid. [sent-303, score-0.116]
</p><p>74 We also (heuristically) subtract the number of cells from the score of a cuboid to bias the selection away from cuboids with many cells, which would collect Laplace error in each cell. [sent-304, score-0.147]
</p><p>75 An entire cuboid can be measured with a single differentially private query, as any record contributes to at most one cell (this is a generalization of the Laplace Mechanism to multiple dimensions, from [3]). [sent-306, score-0.504]
</p><p>76 Finally, Multiplicative Weights works unmodiﬁed, increasing and decreasing weights based on the over- or under-estimation of the count to which the record contributes. [sent-307, score-0.074]
</p><p>77 5  2  MWEM (T = 10)  Figure 5: Comparison of MWEM with the custom approaches from [14], varying epsilon through the reported values from [14]. [sent-314, score-0.102]
</p><p>78 Each cuboid (marginal) is assessed by its average error, and either the average or maximum over all 256 marginals is taken to evaluate the technique. [sent-315, score-0.146]
</p><p>79 As the number of attributes grows, the universe D grows exponentially, and it can quickly become infeasible to track the distribution explicitly. [sent-321, score-0.186]
</p><p>80 Recall that the heart of MWEM maintains a distribution Ai over D that is then used in the Exponential Mechanism to select queries poorly approximated by the current distribution. [sent-323, score-0.304]
</p><p>81 From the deﬁnition of the Multiplicative Weights distribution, we see that the weight Ai (x) can be determined from the history Hi = {(qj , mj ) : j  i}: 0 1 X Ai (x) / exp @ qj (x) ⇥ (mj qj (Aj 1 ))/2nA . [sent-324, score-0.154]
</p><p>82 ji  We explicitly record the scaling factors lj = mj qj (Aj {(qj , mj , lj ) : j  i}, to remove the dependence on prior Aj . [sent-325, score-0.191]
</p><p>83 If we partition these attributes into disjoint parts D1 , D2 , . [sent-327, score-0.145]
</p><p>84 Dk so that no query in Hi involves attributes from more than one part, then the distribution produced by Multiplicative Weights is a product distribution over D1 ⇥D2 ⇥. [sent-330, score-0.313]
</p><p>85 For query classes that factorize over the attributes of the domain (for example, range queries, marginal queries, and cuboid queries) we can rewrite and efﬁciently perform the integration over D using 0 1 X Y X @ q(x) ⇥ Ai (x) = q(xj ) ⇥ Aj (xj )A . [sent-334, score-0.468]
</p><p>86 is a mini Multiplicative Weights over attributes in part Dj , using only the relevant queries So long as the measurements taken reﬂect modest groups of independent attributes, the integration can be efﬁciently performed. [sent-339, score-0.465]
</p><p>87 Experimentally, we are able to process a binarized form of the Adult dataset with 27 attributes efﬁciently (taking 80 seconds to process completely), and the addition of 50 new independent binary attributes, corresponding to a domain of size 277 , results in neglible performance impact. [sent-342, score-0.204]
</p><p>88 5  Conclusions  We introduced MWEM, a simple algorithm for releasing data maintaining a high ﬁdelity to the protected source data, as well as differential privacy with respect to the records. [sent-344, score-0.503]
</p><p>89 An analyst does not require a complicated mathematical understanding of the nature of the queries (as the community has for linear algebra [11] and the Hadamard transform [21]), but rather only needs to enumerate those measurements that should be preserved. [sent-348, score-0.32]
</p><p>90 The promise of differential privacy: A tutorial on algorithmic techniques. [sent-370, score-0.119]
</p><p>91 A multiplicative weights mechanism for interactive privacy-preserving data analysis. [sent-382, score-0.366]
</p><p>92 An adaptive mechanism for accurate query answering under differential privacy. [sent-395, score-0.48]
</p><p>93 Differential privacy and the risk-utility tradeoff for multi-dimensional contingency tables. [sent-399, score-0.493]
</p><p>94 Differentially private data cubes: optimizing noise sources and consistency. [sent-402, score-0.227]
</p><p>95 On the complexity of differentially private data release: efﬁcient algorithms and hardness results. [sent-407, score-0.428]
</p><p>96 PCPs and the hardness of generating private synthetic data. [sent-411, score-0.308]
</p><p>97 On the complexity of differentially private data release: efﬁcient algorithms and hardness results. [sent-420, score-0.428]
</p><p>98 The median mechanism: Interactive and efﬁcient privacy with multiple queries. [sent-429, score-0.34]
</p><p>99 Privacy, accuracy, and consistency too: a holistic solution to contingency table release. [sent-438, score-0.153]
</p><p>100 Measuring the achievable error of query sets under differential privacy. [sent-455, score-0.31]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mwem', 0.584), ('privacy', 0.34), ('queries', 0.281), ('private', 0.227), ('query', 0.168), ('mechanism', 0.167), ('contingency', 0.153), ('differentially', 0.151), ('attributes', 0.145), ('release', 0.142), ('barak', 0.122), ('differential', 0.119), ('multiplicative', 0.113), ('dwork', 0.102), ('epsilon', 0.102), ('records', 0.101), ('cuboid', 0.095), ('cynthia', 0.088), ('laplace', 0.077), ('mcsherry', 0.077), ('svd', 0.072), ('datacube', 0.067), ('gerome', 0.067), ('hardt', 0.067), ('adult', 0.064), ('fienberg', 0.063), ('rothblum', 0.059), ('ai', 0.052), ('pods', 0.051), ('marginals', 0.051), ('katrina', 0.05), ('miklau', 0.05), ('salil', 0.05), ('tcc', 0.05), ('transfusion', 0.05), ('qj', 0.05), ('hardness', 0.05), ('ligett', 0.044), ('releasing', 0.044), ('interactive', 0.043), ('weights', 0.043), ('datasets', 0.042), ('counting', 0.042), ('stoc', 0.042), ('chao', 0.041), ('moritz', 0.041), ('universe', 0.041), ('measurements', 0.039), ('exponential', 0.039), ('aj', 0.038), ('entropy', 0.038), ('focs', 0.037), ('ka', 0.035), ('mj', 0.034), ('hay', 0.033), ('kobbi', 0.033), ('reingold', 0.033), ('guy', 0.032), ('hadamard', 0.032), ('domain', 0.032), ('synthetic', 0.031), ('aaron', 0.031), ('record', 0.031), ('hi', 0.031), ('frank', 0.029), ('avrim', 0.029), ('cubes', 0.029), ('cuboids', 0.029), ('monetary', 0.029), ('rastogi', 0.029), ('ullman', 0.029), ('et', 0.029), ('qi', 0.029), ('mechanisms', 0.028), ('substantially', 0.028), ('range', 0.028), ('footprint', 0.027), ('recency', 0.027), ('blum', 0.027), ('experimentally', 0.027), ('dataset', 0.027), ('roth', 0.027), ('accuracy', 0.026), ('answering', 0.026), ('capital', 0.026), ('naor', 0.026), ('improving', 0.025), ('tables', 0.023), ('error', 0.023), ('delity', 0.023), ('substantial', 0.023), ('maintains', 0.023), ('li', 0.021), ('quality', 0.021), ('lj', 0.021), ('relative', 0.021), ('shaded', 0.02), ('ding', 0.02), ('history', 0.02), ('measurement', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="18-tfidf-1" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>Author: Moritz Hardt, Katrina Ligett, Frank Mcsherry</p><p>Abstract: We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques. 1</p><p>2 0.36220768 <a title="18-tfidf-2" href="./nips-2012-Privacy_Aware_Learning.html">275 nips-2012-Privacy Aware Learning</a></p>
<p>Author: Martin J. Wainwright, Michael I. Jordan, John C. Duchi</p><p>Abstract: We study statistical risk minimization problems under a version of privacy in which the data is kept conﬁdential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator. 1</p><p>3 0.36087579 <a title="18-tfidf-3" href="./nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">237 nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>Author: Kamalika Chaudhuri, Anand Sarwate, Kaushik Sinha</p><p>Abstract: Principal components analysis (PCA) is a standard tool for identifying good lowdimensional approximations to data sets in high dimension. Many current data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We demonstrate that on real data, there is a large performance gap between the existing method and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. 1</p><p>4 0.091430791 <a title="18-tfidf-4" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>Author: Aaron Wilson, Alan Fern, Prasad Tadepalli</p><p>Abstract: We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the agent presents an expert with short runs of a pair of policies originating from the same state and the expert indicates which trajectory is preferred. The agent’s goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efﬁcient than random selection. 1</p><p>5 0.07409855 <a title="18-tfidf-5" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>6 0.061321992 <a title="18-tfidf-6" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>7 0.054096315 <a title="18-tfidf-7" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>8 0.051615272 <a title="18-tfidf-8" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>9 0.050225683 <a title="18-tfidf-9" href="./nips-2012-A_Linear_Time_Active_Learning_Algorithm_for_Link_Classification.html">10 nips-2012-A Linear Time Active Learning Algorithm for Link Classification</a></p>
<p>10 0.048957929 <a title="18-tfidf-10" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>11 0.046329368 <a title="18-tfidf-11" href="./nips-2012-Query_Complexity_of_Derivative-Free_Optimization.html">285 nips-2012-Query Complexity of Derivative-Free Optimization</a></p>
<p>12 0.043183636 <a title="18-tfidf-12" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>13 0.041521978 <a title="18-tfidf-13" href="./nips-2012-Variational_Inference_for_Crowdsourcing.html">359 nips-2012-Variational Inference for Crowdsourcing</a></p>
<p>14 0.039656162 <a title="18-tfidf-14" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>15 0.038867291 <a title="18-tfidf-15" href="./nips-2012-Affine_Independent_Variational_Inference.html">37 nips-2012-Affine Independent Variational Inference</a></p>
<p>16 0.036913037 <a title="18-tfidf-16" href="./nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>17 0.036349386 <a title="18-tfidf-17" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>18 0.03440538 <a title="18-tfidf-18" href="./nips-2012-Probabilistic_n-Choose-k_Models_for_Classification_and_Ranking.html">278 nips-2012-Probabilistic n-Choose-k Models for Classification and Ranking</a></p>
<p>19 0.034027677 <a title="18-tfidf-19" href="./nips-2012-Minimizing_Sparse_High-Order_Energies_by_Submodular_Vertex-Cover.html">214 nips-2012-Minimizing Sparse High-Order Energies by Submodular Vertex-Cover</a></p>
<p>20 0.033277858 <a title="18-tfidf-20" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, 0.005), (2, -0.008), (3, -0.032), (4, 0.006), (5, 0.033), (6, 0.002), (7, 0.085), (8, -0.011), (9, -0.014), (10, 0.013), (11, -0.033), (12, -0.072), (13, -0.078), (14, 0.004), (15, 0.08), (16, 0.037), (17, -0.439), (18, 0.109), (19, 0.098), (20, -0.28), (21, 0.009), (22, -0.008), (23, -0.181), (24, 0.252), (25, -0.012), (26, 0.027), (27, -0.047), (28, 0.049), (29, -0.042), (30, -0.003), (31, -0.007), (32, 0.075), (33, -0.013), (34, 0.079), (35, -0.051), (36, -0.048), (37, -0.014), (38, -0.056), (39, -0.061), (40, -0.021), (41, 0.064), (42, 0.005), (43, 0.061), (44, 0.014), (45, -0.003), (46, -0.016), (47, -0.015), (48, 0.031), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94263184 <a title="18-lsi-1" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>Author: Moritz Hardt, Katrina Ligett, Frank Mcsherry</p><p>Abstract: We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques. 1</p><p>2 0.87867165 <a title="18-lsi-2" href="./nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">237 nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>Author: Kamalika Chaudhuri, Anand Sarwate, Kaushik Sinha</p><p>Abstract: Principal components analysis (PCA) is a standard tool for identifying good lowdimensional approximations to data sets in high dimension. Many current data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We demonstrate that on real data, there is a large performance gap between the existing method and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. 1</p><p>3 0.75857681 <a title="18-lsi-3" href="./nips-2012-Privacy_Aware_Learning.html">275 nips-2012-Privacy Aware Learning</a></p>
<p>Author: Martin J. Wainwright, Michael I. Jordan, John C. Duchi</p><p>Abstract: We study statistical risk minimization problems under a version of privacy in which the data is kept conﬁdential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator. 1</p><p>4 0.35447887 <a title="18-lsi-4" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>Author: Sejong Yoon, Vladimir Pavlovic</p><p>Abstract: Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. 1</p><p>5 0.2479071 <a title="18-lsi-5" href="./nips-2012-Assessing_Blinding_in_Clinical_Trials.html">46 nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>Author: Ognjen Arandjelovic</p><p>Abstract: The interaction between the patient’s expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufﬁciently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants’ feedback. 1</p><p>6 0.23066697 <a title="18-lsi-6" href="./nips-2012-Query_Complexity_of_Derivative-Free_Optimization.html">285 nips-2012-Query Complexity of Derivative-Free Optimization</a></p>
<p>7 0.22922343 <a title="18-lsi-7" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>8 0.19709666 <a title="18-lsi-8" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>9 0.18645851 <a title="18-lsi-9" href="./nips-2012-On_the_Sample_Complexity_of_Robust_PCA.html">254 nips-2012-On the Sample Complexity of Robust PCA</a></p>
<p>10 0.18640435 <a title="18-lsi-10" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>11 0.18610831 <a title="18-lsi-11" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>12 0.18100613 <a title="18-lsi-12" href="./nips-2012-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">76 nips-2012-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>13 0.17945881 <a title="18-lsi-13" href="./nips-2012-Variational_Inference_for_Crowdsourcing.html">359 nips-2012-Variational Inference for Crowdsourcing</a></p>
<p>14 0.17889968 <a title="18-lsi-14" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>15 0.17010367 <a title="18-lsi-15" href="./nips-2012-Scalable_imputation_of_genetic_data_with_a_discrete_fragmentation-coagulation_process.html">299 nips-2012-Scalable imputation of genetic data with a discrete fragmentation-coagulation process</a></p>
<p>16 0.16619973 <a title="18-lsi-16" href="./nips-2012-Bayesian_Pedigree_Analysis_using_Measure_Factorization.html">53 nips-2012-Bayesian Pedigree Analysis using Measure Factorization</a></p>
<p>17 0.15959597 <a title="18-lsi-17" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>18 0.15785801 <a title="18-lsi-18" href="./nips-2012-Mixability_in_Statistical_Learning.html">217 nips-2012-Mixability in Statistical Learning</a></p>
<p>19 0.15708563 <a title="18-lsi-19" href="./nips-2012-Distributed_Non-Stochastic_Experts.html">102 nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>20 0.1570607 <a title="18-lsi-20" href="./nips-2012-A_Linear_Time_Active_Learning_Algorithm_for_Link_Classification.html">10 nips-2012-A Linear Time Active Learning Algorithm for Link Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (17, 0.011), (21, 0.16), (28, 0.071), (38, 0.105), (39, 0.01), (42, 0.038), (53, 0.012), (54, 0.027), (55, 0.031), (74, 0.077), (76, 0.104), (80, 0.055), (86, 0.012), (92, 0.046), (93, 0.119)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84374541 <a title="18-lda-1" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>2 0.83508754 <a title="18-lda-2" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>Author: Marius Pachitariu, Maneesh Sahani</p><p>Abstract: We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a speciﬁc pattern of recurrent connections learned by the model. 1</p><p>same-paper 3 0.81299096 <a title="18-lda-3" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>Author: Moritz Hardt, Katrina Ligett, Frank Mcsherry</p><p>Abstract: We present a new algorithm for differentially private data release, based on a simple combination of the Multiplicative Weights update rule with the Exponential Mechanism. Our MWEM algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques. 1</p><p>4 0.78944981 <a title="18-lda-4" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>5 0.78101516 <a title="18-lda-5" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable’s marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufﬁciently certain about any individual decision. Whenever this is the case, we dynamically prune the variables we are conﬁdent about from the underlying factor graph. Consequently, at any time only samples of variables whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classiﬁcation and image inpainting, show that adaptive sampling can drastically accelerate MMP without sacriﬁcing prediction accuracy. 1</p><p>6 0.77930129 <a title="18-lda-6" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>7 0.7600913 <a title="18-lda-7" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<p>8 0.75970364 <a title="18-lda-8" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>9 0.73438811 <a title="18-lda-9" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>10 0.72268134 <a title="18-lda-10" href="./nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">23 nips-2012-A lattice filter model of the visual pathway</a></p>
<p>11 0.71676576 <a title="18-lda-11" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>12 0.71430469 <a title="18-lda-12" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>13 0.70429456 <a title="18-lda-13" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>14 0.70406646 <a title="18-lda-14" href="./nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">114 nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>15 0.70264727 <a title="18-lda-15" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>16 0.69944811 <a title="18-lda-16" href="./nips-2012-Delay_Compensation_with_Dynamical_Synapses.html">94 nips-2012-Delay Compensation with Dynamical Synapses</a></p>
<p>17 0.69820863 <a title="18-lda-17" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>18 0.6975916 <a title="18-lda-18" href="./nips-2012-Learning_optimal_spike-based_representations.html">190 nips-2012-Learning optimal spike-based representations</a></p>
<p>19 0.69702268 <a title="18-lda-19" href="./nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">237 nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>20 0.69591391 <a title="18-lda-20" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
