<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2012-Learning the Dependency Structure of Latent Factors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-192" href="#">nips2012-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 nips-2012-Learning the Dependency Structure of Latent Factors</h1>
<br/><p>Source: <a title="nips-2012-192-pdf" href="http://papers.nips.cc/paper/4636-learning-the-dependency-structure-of-latent-factors.pdf">pdf</a></p><p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><p>Reference: <a title="nips-2012-192-reference" href="../nips2012_reference/nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract In this paper, we study latent factor models with dependency structure in the latent space. [sent-6, score-0.518]
</p><p>2 We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. [sent-7, score-0.303]
</p><p>3 A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. [sent-8, score-0.435]
</p><p>4 The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. [sent-9, score-0.522]
</p><p>5 To enable the efﬁcient processing of large data collections, latent factor models (LFMs) have been proposed to ﬁnd concise descriptions of the members of a data collection. [sent-13, score-0.305]
</p><p>6 A random vector x ∈ RM is assumed to be generated by a linear combination of a set of basis vectors, i. [sent-14, score-0.104]
</p><p>7 , BK ] stores the set of unknown basis vectors and “factor” si (i ∈ {1, . [sent-19, score-0.158]
</p><p>8 The i-th  In this paper, we consider the problem of learning hidden dependency structure of latent factors in complex data sets. [sent-24, score-0.408]
</p><p>9 Our goal includes two main aspects: (1) to learn the interpretable lowerdimensional representations hidden in a set of data samples, and (2) to simultaneously model the pairwise interaction of latent factors. [sent-25, score-0.417]
</p><p>10 The statistical structure captured by LFM methods, such as Principal Component Analysis (PCA) are limited in interpretability, due to their anti-correlation assumption on the latent factors. [sent-27, score-0.222]
</p><p>11 For example, when a face image is represented as a linear super-position of PCA bases with uncorrelated coefﬁcients learned by PCA, there exist complex cancellations between the basis images [14]. [sent-28, score-0.231]
</p><p>12 Methods that theoretically assume independence of components like ICA [10] or sparse coding [15] fail to generate independent representations in practice. [sent-29, score-0.103]
</p><p>13 1  Instead of imposing this unrealistic assumption, more recent works [18, 25, 27] propose to allow correlated latent factors, which shows to be helpful in obtaining better performance on various tasks. [sent-33, score-0.24]
</p><p>14 Particularly, the sparse structure of the latent factor network is often preferred but has been never been explicitly explored in the learning process [2, 8, 23]. [sent-37, score-0.34]
</p><p>15 For example, when mining the enormous on-line news-text documents, a method discovering semantically meaningful latent topics and a concise graph connecting the topics will greatly assist intelligent browsing, organizing and accessing of these documents. [sent-38, score-0.4]
</p><p>16 The main contribution in this paper is a general LFM method that models the pairwise relationships between latent factors by sparse graphical models. [sent-39, score-0.573]
</p><p>17 By introducing a generalized Tikhonov regularization, we enforce the interaction of latent factors to have an inﬂuence on learning latent factors and basis vectors. [sent-40, score-0.867]
</p><p>18 As a result, we learn meaningful latent factors and simultaneously obtain a graph where the nodes represent hidden groups and the edges represent their pairwise relationships. [sent-41, score-0.516]
</p><p>19 This graphical representation helps us analyze collections of complex data samples in a much more structured and organized way. [sent-42, score-0.09]
</p><p>20 The latent representations of data samples obtained from our model capture deeper signals hidden in the data which produce the useful features for discriminative task and in-depth analysis, e. [sent-43, score-0.273]
</p><p>21 To learn the hidden factors for generating x, the natural parameter η is assumed to be represented by a linear combination of basis vectors, i. [sent-51, score-0.291]
</p><p>22 Let G = (V, E) denote a graph with K nodes, corresponding to the K latent factors {s1 , . [sent-59, score-0.393]
</p><p>23 (5)  Since θij = 0 indicates that latent factor si and latent factor sj are conditionally independent given other latent factors, the graph G presents an illustrative view of the statistical dependencies between latent factors. [sent-63, score-1.093]
</p><p>24 With such a hierarchical and ﬂexible model, there would be signiﬁcant risk of over-ﬁtting, especially when we consider all possible interactions between K latent factors. [sent-64, score-0.245]
</p><p>25 , x(N ) }, the Maximum a Posteriori (MAP) estimates of the basis matrix B, the latent factors in S = [s(1) , . [sent-74, score-0.5]
</p><p>26 , s(N ) ] and the parameters {µ, Θ} of the latent factor network are therefore the solution of the following problem: 1 min {− log h(x(i) ) + A(Bs(i) ) − s(i) B T (x(i) )} B,S,Θ N i 1 1 1 µ S1N + tr(S ΘS) + ρ Θ N 2N 2 ≤ 1, k = 1, . [sent-77, score-0.275]
</p><p>27 Subproblem (9) is easy to solve for real-valued s(i) but generally hard when the latent factors only admit discrete values. [sent-89, score-0.359]
</p><p>28 The subproblem (11) is minimizing the sum of a differentiable convex function and an L1 regularization term, for which a few recently developed methods can be very efﬁcient, such as variants of ADMM [6]. [sent-91, score-0.123]
</p><p>29 (8) when x follows a multivariate normal distribution and s follows a sparse Gaussian graphical model (SGGM). [sent-94, score-0.125]
</p><p>30 We name our model under this default setting as “structured latent factor analysis” (SLFA) and compare 1 it to related works. [sent-95, score-0.275]
</p><p>31 Assume p(x|η) = (2π)−M/2 exp(− 2σ2 x − η 2 ) and s ∼ N (µ, Φ−1 ), with 3  sparse precision matrix Φ (inverse covariance). [sent-96, score-0.19]
</p><p>32 If Φi,j > 0, minimizing the objective function will avoid si and sj to be simultaneously large, and we say the i-th factor and the j-th factor are negatively related. [sent-110, score-0.244]
</p><p>33 If Φi,j < 0, the solution is likely to have si and sj of the same sign, and we say the i-th factor and the j-th factor are positively related. [sent-111, score-0.215]
</p><p>34 If Φi,j = 0, the regularization doesn’t induce interaction between si and sj in the objective function. [sent-112, score-0.155]
</p><p>35 Therefore, this regularization term makes SLFA produce a collaborative reconstruction based on the conditional dependencies between latent factors. [sent-113, score-0.299]
</p><p>36 On one hand, the collaborative nature makes SLFA capture deeper statistical structure hidden in the data set, compared to the matrix factorization problem with the Tikhonov regularization S 2 or sparse F coding with the sparsity-inducing regularization such as S 1 . [sent-114, score-0.333]
</p><p>37 On the other hand, SLFA encourages sparse interactions which is very different from previous works such as correlated topic Model [2] and latent Gaussian model [18], where the latent factors are densely related. [sent-115, score-0.748]
</p><p>38 As summarized in Algorithm 1, at each iteration, we randomly fetch a mini-batch of observations simultaneously, compute their latent factor vector s. [sent-121, score-0.275]
</p><p>39 Then the latent factor vectors are used to update the basis matrix B in stochastic gradient descent fashion with projections on the constraint set. [sent-122, score-0.439]
</p><p>40 , x(N ) ], initial guess of basis matrix B, initial precision matrix Φ = I, number of iterations T , parameters σ 2 and ρ, step-size γ, mini-batch size N . [sent-128, score-0.266]
</p><p>41 – Compute the latent factor vectors Sbatch = (B B + σ 2 Φ)−1 Xbatch . [sent-133, score-0.298]
</p><p>42 – Update the basis matrix B using a gradient descent step: γ B ← B − N [BSbatch − Xbatch ]Sbatch . [sent-134, score-0.141]
</p><p>43 – Solve the subproblem (13) to update the sparse inverse covariance matrix Φ using all available latent factor vectors in S. [sent-138, score-0.498]
</p><p>44 A large ρ will result in a diagonal precision matrix Φ, indicating that the latent factors are conditionally independent. [sent-140, score-0.484]
</p><p>45 Alternatively, for visual analysis of latent factors, we can select multiple values of ρ to obtain Φ with desired sparsity. [sent-150, score-0.222]
</p><p>46 Relationship to Sparse Gaussian Graphical Model: We can also see SLFA as a generalization of sparse Gaussian graphical model. [sent-151, score-0.125]
</p><p>47 In this sense, SLFA could be seen as the sparse Gaussian graphical model of s = Wx, i. [sent-155, score-0.125]
</p><p>48 A few recent efforts [3, 24] also combined the model of SGGM and with latent factor models. [sent-158, score-0.275]
</p><p>49 Different from our SLFA, these methods still aim at modeling the interaction between the original features and doesn’t consider interaction in the latent factor space. [sent-160, score-0.365]
</p><p>50 Instead, SLFA is a hierarchical model and the learned pairwise relationships are on the latent factor level. [sent-161, score-0.397]
</p><p>51 SLFA, however, can dramatically reduce the problem to learning a 50 × 50 sparse precision matrix and the corresponding graph of 50 nodes. [sent-163, score-0.224]
</p><p>52 Intuitively, sparse coding based works (such as [7]) try to remove the redundancy in the representation of data while SLFA encourages a (sparse) collaborative reconstruction of the data from the latent bases. [sent-167, score-0.383]
</p><p>53 [12] proposed a method that can learn latent factors with given tree structure. [sent-169, score-0.381]
</p><p>54 (14), but uses a different regularization term which imposes the overlapped group sparsity of factors. [sent-173, score-0.088]
</p><p>55 Differently, SLFA can learn a more general graphical structure among latent factors and doesn’t assume that data sample maps to a sparse combination of basis vectors. [sent-174, score-0.61]
</p><p>56 The model of SLFA has similar hierarchy with correlated topic model [2] and latent Gaussian model [18]. [sent-175, score-0.275]
</p><p>57 Besides the key difference of sparsity, SLFA directly use precision matrix to learn latent factor networks while the other two works learn the covariance matrix by Bayesian methods. [sent-176, score-0.501]
</p><p>58 1 Synthetic Data I: Four Different Graphical Relationships The ﬁrst experiment uses randomly generated synthetic data with different graphical structures of latent factors. [sent-179, score-0.304]
</p><p>59 It aims to test if SLFA can ﬁnd true latent factors and the true relationships among latent factors and to study the effect of the parameter ρ on the results. [sent-180, score-0.807]
</p><p>60 We use four special cases of Sparse Gaussian Graphical Model to generate the latent factors. [sent-181, score-0.242]
</p><p>61 The underlying graph is either a ring, a grid, a tree or a random sparse graph, which are shown in Figure 1. [sent-182, score-0.099]
</p><p>62 2  0  2  4  −log2(rho)  6  (g) F-score (tree)  8  0 −2  0  2  4  −log2(rho)  6  8  (h) F-score (random)  Figure 1: Recovering structured latent factors from data. [sent-199, score-0.389]
</p><p>63 On the upper row are four different underlying graphical model of latent factors. [sent-200, score-0.302]
</p><p>64 Red edge means the two latent factors are positively related (Φ∗ < 0), blue edge implies the two latent factors are negatively related (Φ∗ > 0). [sent-201, score-0.81]
</p><p>65 We compare SLFA to other four methods for learning the basis matrix B and the precision matrix Φ from the data. [sent-221, score-0.286]
</p><p>66 The ﬁrst one is NMF, where we learn nonnegative basis B from the data and then learn the sparse precision matrix Φ for the corresponding factor vectors (non nonnegative constraint on factors) by SGGM. [sent-222, score-0.414]
</p><p>67 The second one is an ideal case where we have the “oracle” of the true basis B∗ , then after ﬁt the data to be true basis we learn the sparse precision matrix Φ by SGGM. [sent-223, score-0.468]
</p><p>68 In all cases except the oracle method, we have a non-convex problem so that after we obtain the learned basis vectors we use Hungarian algorithm to align them to with the true basis vectors based on the cosine similarity. [sent-226, score-0.333]
</p><p>69 We compute the precision and recall rates for recovering the relationship between latent factors by comparing the learned Φ with the true precision matrix Φ∗ . [sent-227, score-0.691]
</p><p>70 We can see that for all four cases, our proposed method SLFA is as good as the “oracle” method at recovering the pairwise relationship between latent factors. [sent-232, score-0.352]
</p><p>71 NMF most probably fails to ﬁnd the right basis since it does consider any higher level information about the interactions between basis elements, hence SGGM can’t ﬁnd meaningful relationship between the factors obtained from NMF. [sent-233, score-0.406]
</p><p>72 Since latent factors have dense interactions in L2 version of SLFA, combining it with a postprocessing by SGGM improves the performance signiﬁcantly, however it still performs worse compared to SLFA. [sent-235, score-0.382]
</p><p>73 This experiment also conﬁrms that the idea of performing an integrated learning of the bases together with a regularized precision matrix is essential for recovering the true structure in the data. [sent-236, score-0.231]
</p><p>74 2  Synthetic Data II: Parts-based Images  The second experiment also utilizes a simulated data set based on images to compare SLFA with popular latent factor models. [sent-238, score-0.311]
</p><p>75 For Φ(i, j) > 0, Bi and Bj are negatively related (exclusive), for Φ(i, j) < 0, Bi and Bj are positively related (supportive). [sent-254, score-0.092]
</p><p>76 of which is essentially a linear combination of ﬁve latent parts shown in Figure 2a. [sent-255, score-0.222]
</p><p>77 Given 37 basis images, we ﬁrst randomly select one of the ﬁve big circles as the body of the “bugs”. [sent-256, score-0.136]
</p><p>78 Each shape of body is associated with four positions where the legs of the bug is located. [sent-257, score-0.16]
</p><p>79 We combine the selected ﬁve latent parts with random coefﬁcients that are sampled from the uniform distribution and multiplied by −1 with probability 0. [sent-260, score-0.222]
</p><p>80 Finally, we add a randomly selected basis with small random coefﬁcients plus Gaussian random noise to the image to introduce the noise and confusion in the data set. [sent-262, score-0.104]
</p><p>81 The generating process (Figure 2b) indicates positive relationship between one type of body and its associates legs, as well as negative relationship between the pair of circle and square that is located at the same position. [sent-264, score-0.108]
</p><p>82 Using SLFA and other two baseline algorithms, PCA and NMF, we learn a set of latent bases and compare the result of three methods in Figures 2e. [sent-265, score-0.302]
</p><p>83 We can see that the basis images generated by SLFA is almost exactly same as the true latent bases. [sent-266, score-0.386]
</p><p>84 This is due to the fact that SLFA accounts for the sparse interaction between factors in the joint optimization problem and encourages collaborative reconstruction. [sent-267, score-0.305]
</p><p>85 NMF basis (shown in supplementary material due to space considerations) in this case also turns out to be similar to true basis, however, one can still observe that many components contain mixed structures since it can not capture the true data generation process. [sent-268, score-0.152]
</p><p>86 More importantly, SLFA provides the convenience of analyzing the relationship between the bases using the precision matrix Φ. [sent-271, score-0.221]
</p><p>87 In Figure 2d, we analyze the relational structure learned in the precision matrix Φ. [sent-272, score-0.158]
</p><p>88 The most negatively related (exclusive) pairs (the i and j entries with highest positive entries in Φ) are circular and square legs which conforms fully to the generation process, since only one of them is chosen for any given location. [sent-273, score-0.115]
</p><p>89 Accordingly, the most positively related pairs are a body shape and one of its associated legs since every bug has a body and four legs with ﬁxed positions. [sent-274, score-0.303]
</p><p>90 In ﬁgure 3, we plot a graph of topics (standing-alone topics removed) with positive interaction between each other and present the top 5 keywords for each topic. [sent-280, score-0.193]
</p><p>91 Each edge corresponds to a negative element in the sparse precision matrix Φ. [sent-308, score-0.19]
</p><p>92 This data set contains the gene expression values of 8, 141 genes for 295 breast cancer tumor samples. [sent-324, score-0.126]
</p><p>93 Lasso-overlapped-group, which is a logistic regression approach with the graph-guided sparsity enforced, uses a known biological network as the graphical (overlapped group) regularization on the lasso regression. [sent-327, score-0.182]
</p><p>94 Gene groups that usually correspond to biological processes or pathways, exhibit diverse pairwise dependency relationships among each other. [sent-336, score-0.139]
</p><p>95 SLFA discovers these relationships while learning the latent representation of each data sample at the same time. [sent-337, score-0.263]
</p><p>96 The learned structural information and latent gene groups also get conﬁrmed by the biological function analysis in supplementary document. [sent-339, score-0.332]
</p><p>97 5  Conclusion  In this paper we have introduced a novel structured latent factor model that simultaneously learns latent factors and their pairwise relationships. [sent-340, score-0.737]
</p><p>98 The learned sparse interaction between latent factors is crucial for understanding complex data sets and to visually analyze them. [sent-342, score-0.502]
</p><p>99 SLFA model is also a hierarchical extension of Sparse Gaussian Graphical Model by generalizing the application of precision matrix from the original variable space to the latent factor space and optimizing the bases together with the precision matrix simultaneously. [sent-343, score-0.583]
</p><p>100 : Exponential family sparse coding with applications to selftaught learning. [sent-412, score-0.103]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('slfa', 0.841), ('latent', 0.222), ('sggm', 0.184), ('factors', 0.137), ('basis', 0.104), ('precision', 0.088), ('bs', 0.086), ('nmf', 0.082), ('subproblem', 0.078), ('legs', 0.067), ('sparse', 0.065), ('graphical', 0.06), ('bases', 0.058), ('bic', 0.058), ('topics', 0.057), ('factor', 0.053), ('pairwise', 0.048), ('negatively', 0.048), ('gene', 0.048), ('pca', 0.047), ('bk', 0.047), ('xbatch', 0.046), ('interaction', 0.045), ('regularization', 0.045), ('positively', 0.044), ('relationships', 0.041), ('rho', 0.041), ('bug', 0.041), ('coding', 0.038), ('relationship', 0.038), ('matrix', 0.037), ('images', 0.036), ('topic', 0.035), ('exclusive', 0.035), ('cancer', 0.035), ('sj', 0.034), ('graph', 0.034), ('learned', 0.033), ('det', 0.032), ('collaborative', 0.032), ('body', 0.032), ('glasso', 0.031), ('tikhonov', 0.031), ('doesn', 0.031), ('bugs', 0.031), ('lfm', 0.031), ('obd', 0.031), ('sbatch', 0.031), ('yanjun', 0.031), ('si', 0.031), ('structured', 0.03), ('concise', 0.03), ('biological', 0.029), ('jenatton', 0.029), ('microarray', 0.029), ('ij', 0.028), ('hidden', 0.028), ('scaled', 0.027), ('lowerdimensional', 0.027), ('lasso', 0.027), ('totally', 0.027), ('encourages', 0.026), ('gaussian', 0.026), ('schmidt', 0.025), ('simultaneously', 0.025), ('obs', 0.025), ('koray', 0.025), ('arxiv', 0.025), ('kronecker', 0.024), ('true', 0.024), ('recovering', 0.024), ('deeper', 0.023), ('tumor', 0.023), ('georgia', 0.023), ('tr', 0.023), ('vectors', 0.023), ('interactions', 0.023), ('learn', 0.022), ('overlapped', 0.022), ('nec', 0.022), ('ring', 0.022), ('oracle', 0.022), ('svm', 0.022), ('synthetic', 0.022), ('bj', 0.022), ('sparsity', 0.021), ('dependency', 0.021), ('score', 0.021), ('factorization', 0.02), ('four', 0.02), ('bi', 0.02), ('controller', 0.02), ('breast', 0.02), ('raina', 0.02), ('covariance', 0.02), ('hsieh', 0.019), ('preprint', 0.018), ('correlated', 0.018), ('ss', 0.018), ('labs', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="192-tfidf-1" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><p>2 0.12149269 <a title="192-tfidf-2" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>Author: Anima Anandkumar, Ragupathyraj Valluvan</p><p>Abstract: Graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model. We consider a challenging instance of this problem when some of the nodes are latent or hidden. We characterize conditions for tractable graph estimation and develop efﬁcient methods with provable guarantees. We consider the class of Ising models Markov on locally tree-like graphs, which are in the regime of correlation decay. We propose an efﬁcient method for graph estimation, and establish its structural consistency −δη(η+1)−2 when the number of samples n scales as n = Ω(θmin log p), where θmin is the minimum edge potential, δ is the depth (i.e., distance from a hidden node to the nearest observed nodes), and η is a parameter which depends on the minimum and maximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides ﬂexibility to control the number of latent variables and the cycle lengths in the output graph. We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound on sample requirements. Keywords: Graphical model selection, latent variables, quartet methods, locally tree-like graphs. 1</p><p>3 0.085782617 <a title="192-tfidf-3" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>4 0.077814169 <a title="192-tfidf-4" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>Author: Anima Anandkumar, Yi-kai Liu, Daniel J. Hsu, Dean P. Foster, Sham M. Kakade</p><p>Abstract: Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efﬁcient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k × k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space. 1</p><p>5 0.075414129 <a title="192-tfidf-5" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>6 0.074677527 <a title="192-tfidf-6" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>7 0.073114716 <a title="192-tfidf-7" href="./nips-2012-Structure_estimation_for_discrete_graphical_models%3A_Generalized_covariance_matrices_and_their_inverses.html">326 nips-2012-Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses</a></p>
<p>8 0.072515294 <a title="192-tfidf-8" href="./nips-2012-Fully_Bayesian_inference_for_neural_models_with_negative-binomial_spiking.html">138 nips-2012-Fully Bayesian inference for neural models with negative-binomial spiking</a></p>
<p>9 0.068493821 <a title="192-tfidf-9" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>10 0.066130735 <a title="192-tfidf-10" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>11 0.065424494 <a title="192-tfidf-11" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>12 0.064340688 <a title="192-tfidf-12" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>13 0.063432336 <a title="192-tfidf-13" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>14 0.063089669 <a title="192-tfidf-14" href="./nips-2012-Graphical_Models_via_Generalized_Linear_Models.html">147 nips-2012-Graphical Models via Generalized Linear Models</a></p>
<p>15 0.061987057 <a title="192-tfidf-15" href="./nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">327 nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<p>16 0.061614972 <a title="192-tfidf-16" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>17 0.060493052 <a title="192-tfidf-17" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>18 0.05869738 <a title="192-tfidf-18" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>19 0.057244264 <a title="192-tfidf-19" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>20 0.055971142 <a title="192-tfidf-20" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, 0.059), (2, -0.029), (3, -0.034), (4, -0.059), (5, 0.009), (6, 0.0), (7, -0.066), (8, 0.001), (9, -0.008), (10, 0.03), (11, 0.05), (12, 0.008), (13, 0.04), (14, 0.036), (15, 0.058), (16, 0.091), (17, 0.051), (18, 0.023), (19, 0.017), (20, -0.011), (21, -0.001), (22, -0.039), (23, -0.052), (24, 0.016), (25, -0.017), (26, 0.057), (27, -0.059), (28, -0.006), (29, 0.055), (30, -0.066), (31, 0.015), (32, 0.013), (33, 0.003), (34, -0.008), (35, -0.041), (36, 0.01), (37, 0.06), (38, 0.099), (39, -0.083), (40, 0.012), (41, -0.03), (42, 0.054), (43, -0.032), (44, -0.007), (45, 0.104), (46, -0.07), (47, 0.058), (48, 0.006), (49, 0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93741351 <a title="192-lsi-1" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><p>2 0.67376643 <a title="192-lsi-2" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>Author: Xianxing Zhang, Lawrence Carin</p><p>Abstract: A new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns. The documents are modeled with a focused topic model, inferring interpretable latent binary features for each document. A new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint. The matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each. The model is applied to roll-call data, with the associated documents deﬁned by the legislation. Advantages of the proposed model are demonstrated for prediction of votes on a new piece of legislation, based only on the observed text of legislation. The coupling of the text and legislation is also shown to yield insight into the properties of the matrix decomposition for roll-call data. 1</p><p>3 0.66379458 <a title="192-lsi-3" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>Author: Lei Shi</p><p>Abstract: For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Brieﬂy, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two low-dimensional features, which distribute in the row-wise and column-wise latent subspaces respectively. In consequence, PCSA captures the dependencies among entries intricately, and is able to handle non-Gaussian and heteroscedastic densities. By formulating the posterior updating into the task of solving Sylvester equations, we propose an efﬁcient variational inference algorithm. Furthermore, PCSA is extended to tackling and ﬁlling missing values, to adapting model sparseness, and to modelling tensor data. In comparison with several state-of-art methods, experiments demonstrate the effectiveness and efﬁciency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and ﬁlling missing values.</p><p>4 0.65936321 <a title="192-lsi-4" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>Author: Anima Anandkumar, Ragupathyraj Valluvan</p><p>Abstract: Graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model. We consider a challenging instance of this problem when some of the nodes are latent or hidden. We characterize conditions for tractable graph estimation and develop efﬁcient methods with provable guarantees. We consider the class of Ising models Markov on locally tree-like graphs, which are in the regime of correlation decay. We propose an efﬁcient method for graph estimation, and establish its structural consistency −δη(η+1)−2 when the number of samples n scales as n = Ω(θmin log p), where θmin is the minimum edge potential, δ is the depth (i.e., distance from a hidden node to the nearest observed nodes), and η is a parameter which depends on the minimum and maximum node and edge potentials in the Ising model. The proposed method is practical to implement and provides ﬂexibility to control the number of latent variables and the cycle lengths in the output graph. We also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound on sample requirements. Keywords: Graphical model selection, latent variables, quartet methods, locally tree-like graphs. 1</p><p>5 0.64164543 <a title="192-lsi-5" href="./nips-2012-A_latent_factor_model_for_highly_multi-relational_data.html">22 nips-2012-A latent factor model for highly multi-relational data</a></p>
<p>Author: Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes, Guillaume R. Obozinski</p><p>Abstract: Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efﬁcient and semantically meaningful verb representations. 1</p><p>6 0.63606215 <a title="192-lsi-6" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>7 0.63026083 <a title="192-lsi-7" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>8 0.62329072 <a title="192-lsi-8" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>9 0.59680748 <a title="192-lsi-9" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>10 0.58704954 <a title="192-lsi-10" href="./nips-2012-Why_MCA%3F_Nonlinear_sparse_coding_with_spike-and-slab_prior_for_neurally_plausible_image_encoding.html">365 nips-2012-Why MCA? Nonlinear sparse coding with spike-and-slab prior for neurally plausible image encoding</a></p>
<p>11 0.58702004 <a title="192-lsi-11" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>12 0.55489618 <a title="192-lsi-12" href="./nips-2012-Monte_Carlo_Methods_for_Maximum_Margin_Supervised_Topic_Models.html">220 nips-2012-Monte Carlo Methods for Maximum Margin Supervised Topic Models</a></p>
<p>13 0.55170625 <a title="192-lsi-13" href="./nips-2012-Simultaneously_Leveraging_Output_and_Task_Structures_for_Multiple-Output_Regression.html">312 nips-2012-Simultaneously Leveraging Output and Task Structures for Multiple-Output Regression</a></p>
<p>14 0.54626483 <a title="192-lsi-14" href="./nips-2012-Structure_estimation_for_discrete_graphical_models%3A_Generalized_covariance_matrices_and_their_inverses.html">326 nips-2012-Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses</a></p>
<p>15 0.54509372 <a title="192-lsi-15" href="./nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">327 nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<p>16 0.54426783 <a title="192-lsi-16" href="./nips-2012-Learning_Mixtures_of_Tree_Graphical_Models.html">180 nips-2012-Learning Mixtures of Tree Graphical Models</a></p>
<p>17 0.53672045 <a title="192-lsi-17" href="./nips-2012-Coupling_Nonparametric_Mixtures_via_Latent_Dirichlet_Processes.html">89 nips-2012-Coupling Nonparametric Mixtures via Latent Dirichlet Processes</a></p>
<p>18 0.5349921 <a title="192-lsi-18" href="./nips-2012-Smooth-projected_Neighborhood_Pursuit_for_High-dimensional_Nonparanormal_Graph_Estimation.html">317 nips-2012-Smooth-projected Neighborhood Pursuit for High-dimensional Nonparanormal Graph Estimation</a></p>
<p>19 0.53163248 <a title="192-lsi-19" href="./nips-2012-Topology_Constraints_in_Graphical_Models.html">346 nips-2012-Topology Constraints in Graphical Models</a></p>
<p>20 0.52648145 <a title="192-lsi-20" href="./nips-2012-Causal_discovery_with_scale-mixture_model_for_spatiotemporal_variance_dependencies.html">66 nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.356), (17, 0.017), (21, 0.03), (36, 0.013), (38, 0.11), (42, 0.019), (54, 0.024), (55, 0.022), (74, 0.047), (76, 0.136), (80, 0.076), (92, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95385814 <a title="192-lda-1" href="./nips-2012-Factorial_LDA%3A_Sparse_Multi-Dimensional_Text_Models.html">124 nips-2012-Factorial LDA: Sparse Multi-Dimensional Text Models</a></p>
<p>Author: Michael Paul, Mark Dredze</p><p>Abstract: Latent variable models can be enriched with a multi-dimensional structure to consider the many latent factors in a text corpus, such as topic, author perspective and sentiment. We introduce factorial LDA, a multi-dimensional model in which a document is inﬂuenced by K different factors, and each word token depends on a K-dimensional vector of latent variables. Our model incorporates structured word priors and learns a sparse product of factors. Experiments on research abstracts show that our model can learn latent factors such as research topic, scientiﬁc discipline, and focus (methods vs. applications). Our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors. 1</p><p>2 0.92304689 <a title="192-lda-2" href="./nips-2012-Learning_the_Architecture_of_Sum-Product_Networks_Using_Clustering_on_Variables.html">191 nips-2012-Learning the Architecture of Sum-Product Networks Using Clustering on Variables</a></p>
<p>Author: Aaron Dennis, Dan Ventura</p><p>Abstract: The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difﬁcult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture signiﬁcantly improves its performance compared to using a previously-proposed static architecture. 1</p><p>3 0.88369435 <a title="192-lda-3" href="./nips-2012-Multiresolution_Gaussian_Processes.html">233 nips-2012-Multiresolution Gaussian Processes</a></p>
<p>Author: David B. Dunson, Emily B. Fox</p><p>Abstract: We propose a multiresolution Gaussian process to capture long-range, nonMarkovian dependencies while allowing for abrupt changes and non-stationarity. The multiresolution GP hierarchically couples a collection of smooth GPs, each deﬁned over an element of a random nested partition. Long-range dependencies are captured by the top-level GP while the partition points deﬁne the abrupt changes. Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the marginal likelihood of the observations given the partition tree. This property allows for efﬁcient inference of the partition itself, for which we employ graph-theoretic techniques. We apply the multiresolution GP to the analysis of magnetoencephalography (MEG) recordings of brain activity.</p><p>4 0.87949806 <a title="192-lda-4" href="./nips-2012-Phoneme_Classification_using_Constrained_Variational_Gaussian_Process_Dynamical_System.html">270 nips-2012-Phoneme Classification using Constrained Variational Gaussian Process Dynamical System</a></p>
<p>Author: Hyunsin Park, Sungrack Yun, Sanghyuk Park, Jongmin Kim, Chang D. Yoo</p><p>Abstract: For phoneme classiﬁcation, this paper describes an acoustic model based on the variational Gaussian process dynamical system (VGPDS). The nonlinear and nonparametric acoustic model is adopted to overcome the limitations of classical hidden Markov models (HMMs) in modeling speech. The Gaussian process prior on the dynamics and emission functions respectively enable the complex dynamic structure and long-range dependency of speech to be better represented than that by an HMM. In addition, a variance constraint in the VGPDS is introduced to eliminate the sparse approximation error in the kernel matrix. The effectiveness of the proposed model is demonstrated with three experimental results, including parameter estimation and classiﬁcation performance, on the synthetic and benchmark datasets. 1</p><p>same-paper 5 0.87210667 <a title="192-lda-5" href="./nips-2012-Learning_the_Dependency_Structure_of_Latent_Factors.html">192 nips-2012-Learning the Dependency Structure of Latent Factors</a></p>
<p>Author: Yunlong He, Yanjun Qi, Koray Kavukcuoglu, Haesun Park</p><p>Abstract: In this paper, we study latent factor models with dependency structure in the latent space. We propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors. A novel latent factor model SLFA is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction. The main beneﬁt (novelty) of the model is that we can simultaneously learn the lowerdimensional representation for data and model the pairwise relationships between latent factors explicitly. An on-line learning algorithm is devised to make the model feasible for large-scale learning problems. Experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data, and the learned representations achieve the state-of-the-art classiﬁcation performance. 1</p><p>6 0.86990565 <a title="192-lda-6" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>7 0.82706177 <a title="192-lda-7" href="./nips-2012-A_Divide-and-Conquer_Method_for_Sparse_Inverse_Covariance_Estimation.html">7 nips-2012-A Divide-and-Conquer Method for Sparse Inverse Covariance Estimation</a></p>
<p>8 0.78050435 <a title="192-lda-8" href="./nips-2012-A_Neural_Autoregressive_Topic_Model.html">12 nips-2012-A Neural Autoregressive Topic Model</a></p>
<p>9 0.77355498 <a title="192-lda-9" href="./nips-2012-Symmetric_Correspondence_Topic_Models_for_Multilingual_Text_Analysis.html">332 nips-2012-Symmetric Correspondence Topic Models for Multilingual Text Analysis</a></p>
<p>10 0.73291957 <a title="192-lda-10" href="./nips-2012-The_variational_hierarchical_EM_algorithm_for_clustering_hidden_Markov_models.html">342 nips-2012-The variational hierarchical EM algorithm for clustering hidden Markov models</a></p>
<p>11 0.68910807 <a title="192-lda-11" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>12 0.67502248 <a title="192-lda-12" href="./nips-2012-A_Spectral_Algorithm_for_Latent_Dirichlet_Allocation.html">19 nips-2012-A Spectral Algorithm for Latent Dirichlet Allocation</a></p>
<p>13 0.67299652 <a title="192-lda-13" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>14 0.6706292 <a title="192-lda-14" href="./nips-2012-Compressive_Sensing_MRI_with_Wavelet_Tree_Sparsity.html">78 nips-2012-Compressive Sensing MRI with Wavelet Tree Sparsity</a></p>
<p>15 0.66313016 <a title="192-lda-15" href="./nips-2012-Joint_Modeling_of_a_Matrix_with_Associated_Text_via_Latent_Binary_Features.html">166 nips-2012-Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a></p>
<p>16 0.66190588 <a title="192-lda-16" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>17 0.66150957 <a title="192-lda-17" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>18 0.66099 <a title="192-lda-18" href="./nips-2012-Dual-Space_Analysis_of_the_Sparse_Linear_Model.html">104 nips-2012-Dual-Space Analysis of the Sparse Linear Model</a></p>
<p>19 0.65734577 <a title="192-lda-19" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>20 0.65720654 <a title="192-lda-20" href="./nips-2012-Hierarchical_spike_coding_of_sound.html">150 nips-2012-Hierarchical spike coding of sound</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
