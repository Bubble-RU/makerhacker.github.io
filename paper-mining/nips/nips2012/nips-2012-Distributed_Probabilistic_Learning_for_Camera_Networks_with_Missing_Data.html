<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-103" href="#">nips2012-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</h1>
<br/><p>Source: <a title="nips-2012-103-pdf" href="http://papers.nips.cc/paper/4629-distributed-probabilistic-learning-for-camera-networks-with-missing-data.pdf">pdf</a></p><p>Author: Sejong Yoon, Vladimir Pavlovic</p><p>Abstract: Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. 1</p><p>Reference: <a title="nips-2012-103-reference" href="../nips2012_reference/nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. [sent-5, score-0.401]
</p><p>2 However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. [sent-6, score-0.16]
</p><p>3 Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. [sent-7, score-0.497]
</p><p>4 In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. [sent-8, score-0.297]
</p><p>5 In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. [sent-9, score-0.661]
</p><p>6 We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. [sent-10, score-0.204]
</p><p>7 Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. [sent-11, score-0.783]
</p><p>8 1  Introduction  Traditional computer vision algorithms, particularly those that exploit various probabilistic and learning-based approaches, are often formulated in centralized settings. [sent-12, score-0.464]
</p><p>9 A scene or an object is observed by a single camera with all acquired information centrally processed and stored in a single knowledge base (e. [sent-13, score-0.332]
</p><p>10 Even if the problem setting relies on multiple cameras, as may be the case in multi-view or structure from motion (SfM) tasks, all collected information is still processed and organized in a centralized fashion. [sent-16, score-0.491]
</p><p>11 Nevertheless, the overall goal of such distributed device (camera) networks may still be to exchange information and form a consensus interpretation of the visual scene. [sent-18, score-0.304]
</p><p>12 For instance, even if a camera observes a limited set of object views, one would like its local computational model to reﬂect a general 3D appearance of the object visible by other cameras in the network. [sent-19, score-0.548]
</p><p>13 A number of distributed algorithms have been proposed to address the problems such as calibration, pose estimation, tracking, object and activity recognition in large camera networks [1–3]. [sent-20, score-0.497]
</p><p>14 In order to deal with high dimensionality of vision problems, distributed latent space search such as decentralized variants of PCA have been studied in [4, 5]. [sent-21, score-0.282]
</p><p>15 A more general framework using distributed least squares [6] based on distributed averaging of sensor fusions [7] was introduced for PCA, triangulation, pose estimation and SfM. [sent-22, score-0.394]
</p><p>16 Similar approaches have been extended to settings such as the distributed object tracking and activity interpretation [8,9]. [sent-23, score-0.248]
</p><p>17 Even though the methods such as PCA or Kalman ﬁltering have their well-known probabilistic counterparts, the aforementioned approaches do not use probabilistic formulation when dealing with the distributed setting. [sent-24, score-0.308]
</p><p>18 One critical challenge in distributed data analysis includes dealing with missing data. [sent-25, score-0.332]
</p><p>19 In camera networks, different nodes will only have access to a partial set of data features because of varying camera views or object movement. [sent-26, score-0.647]
</p><p>20 For instance, object points used for SfM may be visible only 1  in some cameras and only in particular object poses. [sent-27, score-0.318]
</p><p>21 As a consequence, different nodes will be frequently exposed to missing data. [sent-28, score-0.18]
</p><p>22 However, most current distributed data analysis methods are algebraic in nature and cannot seamlessly handle such missing data. [sent-29, score-0.31]
</p><p>23 In this work we propose a distributed consensus learning approach for parametric probabilistic models with latent variables that can effectively deal with missing data. [sent-30, score-0.566]
</p><p>24 Furthermore, we assume that some of the data features may be missing across different nodes. [sent-34, score-0.15]
</p><p>25 The goal of the network of sensors is to learn a single consensus probabilistic model (e. [sent-35, score-0.214]
</p><p>26 , 3D object structure) without ever resorting to a centralized data pooling and centralized computation. [sent-37, score-0.787]
</p><p>27 We will demonstrate that this task can be accomplished in a principled manner by local probabilistic models and in-network information sharing, implemented as recursive distributed probabilistic learning. [sent-38, score-0.337]
</p><p>28 In particular, we focus on probabilistic PCA (PPCA) as a prototypical example and derive its distributed version, the D-PPCA. [sent-39, score-0.223]
</p><p>29 We then suggest how missing data can be handled in this setting using a missing-data PPCA and apply this model to solve the distributed SfM task in a camera network. [sent-40, score-0.563]
</p><p>30 Our model is inspired by the consensus-based distributed Expectation-Maximization (EM) algorithm for Gaussian mixtures [10], which we extend to deal with generalized linear Gaussian models [11]. [sent-41, score-0.187]
</p><p>31 These assumptions are reasonably applicable to many real world camera network settings. [sent-44, score-0.285]
</p><p>32 In Section 2, we ﬁrst explain the general distributed probabilistic model. [sent-45, score-0.223]
</p><p>33 Section 3 shows how DPPCA can be formulated as a special case of the probabilistic framework and propose the means for handling missing data. [sent-46, score-0.213]
</p><p>34 2  Distributed Probabilistic Model  We start our discussion by ﬁrst considering a general parametric probabilistic model in a centralized setting and then we show how to derive its distributed form. [sent-50, score-0.608]
</p><p>35 Our model is a joint density deﬁned on (xn , zn ) with a global parameter θ (xn , zn ) ∼ p(xn , zn |θ), with p(X, Z|θ) = n p(xn , zn |θ), as depicted in Fig. [sent-56, score-0.25]
</p><p>36 It is important to point out that each posterior density estimate at point n depends solely on the corresponding measurement xn and does not depend on any other xk , k = n. [sent-60, score-0.105]
</p><p>37 , Ni }, where xin ∈ RD is n-th measurement vector and Ni is the number of samples collected in i-th node. [sent-70, score-0.207]
</p><p>38 2  (a) Centralized  (b) Distributed  (c) Augmented  Figure 1: Centralized, distributed and augmented models for probabilistic PCA. [sent-75, score-0.264]
</p><p>39 Still, the centralized model can be equivalently deﬁned using the set of local parameters, with an additional constraint on their consensus, θ1 = θ2 = · · · = θ|V | . [sent-78, score-0.384]
</p><p>40 The simple consensus tying can be more conveniently deﬁned using a set of auxiliary variables ρij , one for each edge eij (Fig. [sent-81, score-0.159]
</p><p>41 This now leads to the ﬁnal distributed consensus learning formulation, similar to [10]: ˆ θ = arg min − log p(X|θ, G) s. [sent-83, score-0.279]
</p><p>42 The last term (modulated by η) is not strictly necessary for consensus but introduces additional regularization. [sent-91, score-0.119]
</p><p>43 This classic (ﬁrst introduced in 1970s) meta decompose algorithm can be used to devise a distributed counterpart for any centralized problem that attempts to maximize a global log likehood function over a connected network. [sent-94, score-0.555]
</p><p>44 3  Distributed Probabilistic PCA (D-PPCA)  We now apply the general distributed probabilistic learning explained above to the speciﬁc case of distributed PPCA. [sent-95, score-0.383]
</p><p>45 Traditional centralized formulation of probabilistic PCA (PPCA) [17] assumes that latent variable zin ∼ N (zin |0, I), with a generative relation xin = Wi zin + µi + where  i  i,  (3)  ∼ N ( i |0, a−1 I) and ai is the noise precision. [sent-96, score-1.345]
</p><p>46 i  T where Li = Wi Wi + We can ﬁnd optimal parameters Wi , µi , ai by ﬁnding the maximum likelihood estimates of the marginal data likelihood or by applying the EM algorithm on expected complete data log likelihood with respect to the posterior density p(Zi |Xi ). [sent-98, score-0.143]
</p><p>47 1  Distributed Formulation  The distributed algorithm developed in Section 2 can be directly applied to this PPCA model. [sent-100, score-0.16]
</p><p>48 The local parameter i estimates are then computed using the consensus updates that combine local summary data statistics with the information about the model conveyed through neighboring network nodes. [sent-103, score-0.191]
</p><p>49 Let Θi = {Wi , µi , ai } be the set of parameters for each node i. [sent-105, score-0.159]
</p><p>50 The global constrained consensus optimization now becomes Wi = ρij , ρij = Wj , i ∈ V, j ∈ Bi , µi = φij , φij = µj , i ∈ V, j ∈ Bi , ai = ψij , ψij = aj , i ∈ V, j ∈ Bi  min{Wi ,µi ,ai :i∈V } −F (Θi ) s. [sent-106, score-0.26]
</p><p>51 Exploiting the posterior density in (4), we compute the expected mean and variance of latent variables in each node as T E[zin ] = L−1 Wi (xin − µi ), i  E[zin zT ] = a−1 L−1 + E[zin ]E[zin ]T . [sent-114, score-0.152]
</p><p>52 (12)  j∈Bi  For ai , we solve the quadratic equation 0=−  +  Ni D (t+1) 2 (t+1) + 2η|Bi |ai + ai · 2 1 2  Ni (t)  (t)  2βi − η  (t)  ai + aj  T E[zin ]T Wi (xin − µi )  − n=1  j∈Bi  Ni T ||xin − µi ||2 + tr E[zin zT ]Wi Wi in  . [sent-116, score-0.339]
</p><p>53 (13)  n=1  The overall distributed EM algorithm for D-PPCA is summarized in Algorithm 1. [sent-117, score-0.16]
</p><p>54 4  ,  Algorithm 1 Distributed Probabilistic PCA (D-PPCA) (0)  (0)  (0)  (0)  Require: For every node i initialize Wi , µi , ai randomly and set λi for t = 0, 1, 2, . [sent-119, score-0.159]
</p><p>55 Hence, we adopt D-PPCA as a method to deal with missing data in a distributed consensus setting. [sent-132, score-0.456]
</p><p>56 Generalization to missing data D-PPCA from D-PPCA is straightforward and follows [18]. [sent-133, score-0.15]
</p><p>57 4  D-PPCA for Structure from Motion (SfM)  In this section, we consider a speciﬁc formulation of the modiﬁed distributed probabilistic PCA for application in afﬁne SfM. [sent-141, score-0.223]
</p><p>58 In SfM, our goal is to estimate the 3D location of N points on a rigid object based on corresponding 2-D points observed from multiple cameras (or views). [sent-142, score-0.278]
</p><p>59 The dimension D of our measurement matrix is thus twice the number of frames each camera observed. [sent-143, score-0.334]
</p><p>60 Given a 2D (image coordinate) measurement matrix X, of size 2 · #f rames × #points, the matrix is factorized into a 2 · #f rames × 3 motion matrix M and the 3 × #points 3D structure matrix S. [sent-145, score-0.337]
</p><p>61 In the centralized setting this can be easily computed using SVD on X. [sent-146, score-0.364]
</p><p>62 Equivalently, the estimates of M and S can be found using inference and learning in a centralized PPCA, where M is treated as the PPCA parameter and S is the latent structure. [sent-147, score-0.414]
</p><p>63 However, the above deﬁned (2 · #f rames × #points) data structure of X is not amenable to distribution of different views (cameras, nodes), as considered in Section 3 of D-PPCA. [sent-149, score-0.167]
</p><p>64 The latent D-PPCA variables will model the unknown and uncertain motion of each camera (and/or object in its view). [sent-158, score-0.441]
</p><p>65 One should note that we have implicitly assumed, in a standard D-PPCA manner, that each column of Zi is iid and distributed as N (0, I). [sent-160, score-0.186]
</p><p>66 However, each pair of subsequent Zi columns represents one 3 × 2 afﬁne motion matrix. [sent-161, score-0.103]
</p><p>67 The reason is that occlusions, the main source of missing data, cannot be treated as a random process. [sent-168, score-0.174]
</p><p>68 However, as we demonstrate in experiments this assumption does not adversely affect SfM when the number of missing points is within a reasonable range. [sent-171, score-0.221]
</p><p>69 1  Empirical Convergence Analysis  Using synthetic data generated from Gaussian distribution, we observed that D-PPCA works well regardless of the number of network nodes, topology, choice of the parameter η or even with missing values in both MAR and MNAR cases. [sent-175, score-0.229]
</p><p>70 2  Afﬁne Structure from Motion  We now show that the modiﬁed D-PPCA can be used as an effective framework for distributed afﬁne SfM. [sent-178, score-0.16]
</p><p>71 We assume that correspondences across frames and cameras are known. [sent-180, score-0.186]
</p><p>72 For the missing values of MNAR case, we either used the actual occlusions to induce missing points or simulated consistently missing points over several frames. [sent-181, score-0.57]
</p><p>73 1  Synthetic Data (Cube)  We ﬁrst generated synthetic data with a rotating unit cube and 5 cameras facing the cube in a 3D space, similar to synthetic experiments in [6]. [sent-184, score-0.436]
</p><p>74 We extracted 8 cube points projected on each camera view every 6◦ , i. [sent-186, score-0.37]
</p><p>75 For all synthetic and real SfM experiments, we picked η = 10 and initialized Wi matrix with feature point coordinates of the ﬁrst frame visible in the i-th camera with some small noise. [sent-191, score-0.355]
</p><p>76 To measure the performance, we computed maximum subspace angle between the ground truth 3D coordinates and our estimated 3D structure matrix. [sent-193, score-0.264]
</p><p>77 Red circles are camera locations and blue arrows indicate each camera’s facing direction. [sent-212, score-0.28]
</p><p>78 Green and red crosses in the right plot are outliers for centralized SVD-based SfM and D-PPCA for SfM, respectively. [sent-213, score-0.364]
</p><p>79 The mean subspace angle tends to be slightly larger than that estimated by the centralized SVD SfM, however both reside within the overlapping conﬁdence intervals. [sent-215, score-0.526]
</p><p>80 66◦ for 20% missing points averaged over 10 different missing point samples. [sent-217, score-0.343]
</p><p>81 Intuitively, this is because the missing points in the scene are naturally not random. [sent-219, score-0.213]
</p><p>82 However, we argue that D-PPCA can still handle missing points given the evidence below. [sent-220, score-0.193]
</p><p>83 The dataset provides various objects rotating on a turntable under different lighting conditions. [sent-224, score-0.124]
</p><p>84 The views of most objects were taken every 5◦ which make it challenging to extract feature points with correspondence across frames. [sent-225, score-0.13]
</p><p>85 Due to the lack of the ground truth 3D coordinates, we compared the subspace angles between the structure inferred using the traditional centralized SVD-based SfM and the D-PPCA-based SfM. [sent-235, score-0.549]
</p><p>86 Experimenal results indicate existance of differences between the reconstructions obtained by centralized factorization approach and that of D-PPCA. [sent-238, score-0.401]
</p><p>87 Moreover, re-projecting back to the camera coordinate space resulted in close matching with the tracked feature points, as shown in videos provided in supplementary materials. [sent-241, score-0.277]
</p><p>88 We collected 135 single-object sequences containing image coordinates of points and we simulated multi-camera setting by partitioning the frames sequentially and almost equally for 5 nodes and the network was connected using ring topology. [sent-244, score-0.22]
</p><p>89 Again, we computed maximum subspace angle between centralized SVD-based SfM and distributed D-PPCA for SfM. [sent-245, score-0.686]
</p><p>90 MAR results provide variances over both various initializations and missing value settings. [sent-254, score-0.15]
</p><p>91 Object BallSander BoxStuff Rooster Standing StorageBin  # Points # Frames  62 67 189 310 102 30 30 30 30 30 Subspace angle b/w centralized SVD SfM and D-PPCA (degree) Mean 1. [sent-255, score-0.454]
</p><p>92 2002 Subspace angle b/w fully observable centralized PPCA SfM and D-PPCA with MAR (degree) Mean 6. [sent-265, score-0.454]
</p><p>93 0444 Subspace angle b/w fully observable centralized PPCA SfM and D-PPCA with MNAR (degree) Mean 3. [sent-282, score-0.454]
</p><p>94 Moreover, more than 53% of all objects yielded the subspace angle below 1◦ , 77% of them below 5◦ and more than 94% were less than 15◦ . [sent-296, score-0.226]
</p><p>95 6  Discussion and Future Work  In this work we introduced a general approach for learning parameters of traditional centralized probabilistic models, such as PPCA, in a distributed setting. [sent-301, score-0.629]
</p><p>96 Our synthetic data experiments showed that the proposed algorithm is robust to choices of initial parameters and, more importantly, is not adversely affected by variations in network size, topology or missing values. [sent-302, score-0.282]
</p><p>97 In the SfM problems, the algorithm can be effectively used to distribute computation of 3D structure and motion in camera networks, while retaining the probabilistic nature of the original model. [sent-303, score-0.464]
</p><p>98 In particular, we assume the independence of the afﬁne motion matrix parameters in (15). [sent-305, score-0.103]
</p><p>99 The assumption is clearly inconsistent with the modeling of motion on the SE(3) manifold. [sent-306, score-0.103]
</p><p>100 Shape and motion from image streams under orthography: a factorization method. [sent-423, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sfm', 0.517), ('centralized', 0.364), ('zin', 0.307), ('camera', 0.253), ('ppca', 0.182), ('xin', 0.179), ('distributed', 0.16), ('missing', 0.15), ('bi', 0.15), ('wi', 0.143), ('ij', 0.14), ('cameras', 0.133), ('mnar', 0.128), ('consensus', 0.119), ('motion', 0.103), ('ai', 0.099), ('ni', 0.094), ('rames', 0.091), ('angle', 0.09), ('mar', 0.08), ('sensor', 0.074), ('cube', 0.074), ('subspace', 0.072), ('probabilistic', 0.063), ('pca', 0.062), ('node', 0.06), ('object', 0.059), ('zn', 0.057), ('turntable', 0.055), ('frames', 0.053), ('views', 0.052), ('rene', 0.048), ('tron', 0.048), ('synthetic', 0.047), ('roberto', 0.045), ('af', 0.044), ('points', 0.043), ('wj', 0.042), ('traditional', 0.042), ('aj', 0.042), ('augmented', 0.041), ('lagrangian', 0.04), ('eij', 0.04), ('svd', 0.039), ('wireless', 0.038), ('ijk', 0.038), ('vision', 0.037), ('factorization', 0.037), ('forero', 0.037), ('smart', 0.037), ('objects', 0.035), ('rotating', 0.034), ('occlusions', 0.034), ('xn', 0.033), ('zi', 0.033), ('em', 0.033), ('network', 0.032), ('takeo', 0.032), ('cano', 0.032), ('decentralized', 0.032), ('wiesel', 0.032), ('coordinates', 0.031), ('decomposable', 0.031), ('connected', 0.031), ('accomplished', 0.031), ('principal', 0.03), ('zt', 0.03), ('nodes', 0.03), ('tracking', 0.029), ('yielded', 0.029), ('measurement', 0.028), ('adversely', 0.028), ('tomasi', 0.028), ('april', 0.027), ('deal', 0.027), ('facing', 0.027), ('latent', 0.026), ('iid', 0.026), ('networks', 0.025), ('topology', 0.025), ('rutgers', 0.025), ('treated', 0.024), ('visible', 0.024), ('truth', 0.024), ('admm', 0.024), ('tracked', 0.024), ('caltech', 0.024), ('structure', 0.024), ('vladimir', 0.023), ('ground', 0.023), ('density', 0.022), ('dealing', 0.022), ('variance', 0.022), ('posterior', 0.022), ('distribute', 0.021), ('kalman', 0.021), ('parametric', 0.021), ('local', 0.02), ('scene', 0.02), ('utility', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="103-tfidf-1" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>Author: Sejong Yoon, Vladimir Pavlovic</p><p>Abstract: Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. 1</p><p>2 0.13832577 <a title="103-tfidf-2" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>Author: Mohsen Hejrati, Deva Ramanan</p><p>Abstract: We present an approach to detecting and analyzing the 3D conﬁguration of objects in real-world images with heavy occlusion and clutter. We focus on the application of ﬁnding and analyzing cars. We do so with a two-stage model; the ﬁrst stage reasons about 2D shape and appearance variation due to within-class variation (station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then reﬁned by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset. 1</p><p>3 0.11553983 <a title="103-tfidf-3" href="./nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">237 nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>Author: Kamalika Chaudhuri, Anand Sarwate, Kaushik Sinha</p><p>Abstract: Principal components analysis (PCA) is a standard tool for identifying good lowdimensional approximations to data sets in high dimension. Many current data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We demonstrate that on real data, there is a large performance gap between the existing method and our method. We show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. 1</p><p>4 0.11152562 <a title="103-tfidf-4" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>Author: S. D. Babacan, Shinichi Nakajima, Minh Do</p><p>Abstract: In this paper, we consider the problem of clustering data points into lowdimensional subspaces in the presence of outliers. We pose the problem using a density estimation formulation with an associated generative model. Based on this probability model, we ﬁrst develop an iterative expectation-maximization (EM) algorithm and then derive its global solution. In addition, we develop two Bayesian methods based on variational Bayesian (VB) approximation, which are capable of automatic dimensionality selection. While the ﬁrst method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in VB matrix factorization leading to fast and effective estimation. Both methods are extended to handle sparse outliers for robustness and can handle missing values. Experimental results suggest that proposed methods are very effective in subspace clustering and identifying outliers. 1</p><p>5 0.065572046 <a title="103-tfidf-5" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>6 0.064504892 <a title="103-tfidf-6" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>7 0.0630043 <a title="103-tfidf-7" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>8 0.060420651 <a title="103-tfidf-8" href="./nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">2 nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<p>9 0.059702739 <a title="103-tfidf-9" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>10 0.059641294 <a title="103-tfidf-10" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>11 0.059504744 <a title="103-tfidf-11" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>12 0.056379944 <a title="103-tfidf-12" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>13 0.056344897 <a title="103-tfidf-13" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>14 0.052317616 <a title="103-tfidf-14" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>15 0.050894465 <a title="103-tfidf-15" href="./nips-2012-Angular_Quantization-based_Binary_Codes_for_Fast_Similarity_Search.html">42 nips-2012-Angular Quantization-based Binary Codes for Fast Similarity Search</a></p>
<p>16 0.049606908 <a title="103-tfidf-16" href="./nips-2012-Gradient_Weights_help_Nonparametric_Regressors.html">145 nips-2012-Gradient Weights help Nonparametric Regressors</a></p>
<p>17 0.04947326 <a title="103-tfidf-17" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>18 0.048355006 <a title="103-tfidf-18" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>19 0.046604734 <a title="103-tfidf-19" href="./nips-2012-Discriminative_Learning_of_Sum-Product_Networks.html">100 nips-2012-Discriminative Learning of Sum-Product Networks</a></p>
<p>20 0.045565687 <a title="103-tfidf-20" href="./nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">327 nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, 0.039), (2, -0.049), (3, -0.024), (4, 0.001), (5, -0.029), (6, -0.004), (7, -0.033), (8, -0.017), (9, -0.005), (10, -0.022), (11, -0.018), (12, -0.001), (13, -0.061), (14, 0.033), (15, 0.128), (16, 0.005), (17, -0.113), (18, 0.027), (19, -0.021), (20, -0.069), (21, 0.018), (22, -0.011), (23, -0.019), (24, 0.051), (25, 0.002), (26, 0.053), (27, 0.036), (28, 0.027), (29, 0.024), (30, -0.074), (31, 0.034), (32, 0.071), (33, 0.032), (34, -0.044), (35, -0.052), (36, 0.022), (37, -0.023), (38, -0.044), (39, 0.012), (40, -0.016), (41, 0.061), (42, -0.009), (43, -0.16), (44, -0.039), (45, 0.049), (46, 0.053), (47, -0.0), (48, 0.015), (49, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91765666 <a title="103-lsi-1" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>Author: Sejong Yoon, Vladimir Pavlovic</p><p>Abstract: Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. 1</p><p>2 0.60733289 <a title="103-lsi-2" href="./nips-2012-Bayesian_Probabilistic_Co-Subspace_Addition.html">54 nips-2012-Bayesian Probabilistic Co-Subspace Addition</a></p>
<p>Author: Lei Shi</p><p>Abstract: For modeling data matrices, this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Brieﬂy, PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two low-dimensional features, which distribute in the row-wise and column-wise latent subspaces respectively. In consequence, PCSA captures the dependencies among entries intricately, and is able to handle non-Gaussian and heteroscedastic densities. By formulating the posterior updating into the task of solving Sylvester equations, we propose an efﬁcient variational inference algorithm. Furthermore, PCSA is extended to tackling and ﬁlling missing values, to adapting model sparseness, and to modelling tensor data. In comparison with several state-of-art methods, experiments demonstrate the effectiveness and efﬁciency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and ﬁlling missing values.</p><p>3 0.56382799 <a title="103-lsi-3" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>Author: Mohsen Hejrati, Deva Ramanan</p><p>Abstract: We present an approach to detecting and analyzing the 3D conﬁguration of objects in real-world images with heavy occlusion and clutter. We focus on the application of ﬁnding and analyzing cars. We do so with a two-stage model; the ﬁrst stage reasons about 2D shape and appearance variation due to within-class variation (station wagons look different than sedans) and changes in viewpoint. Rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates. We use this model to propose candidate detections and 2D estimates of shape. These estimates are then reﬁned by our second stage, using an explicit 3D model of shape and viewpoint. We use a morphable model to capture 3D within-class variation, and use a weak-perspective camera model to capture viewpoint. We learn all model parameters from 2D annotations. We demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3D shape reconstruction on challenging images from the PASCAL VOC 2011 dataset. 1</p><p>4 0.56234336 <a title="103-lsi-4" href="./nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">2 nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<p>Author: Hyun S. Park, Eakta Jain, Yaser Sheikh</p><p>Abstract: A gaze concurrence is a point in 3D where the gaze directions of two or more people intersect. It is a strong indicator of social saliency because the attention of the participating group is focused on that point. In scenes occupied by large groups of people, multiple concurrences may occur and transition over time. In this paper, we present a method to construct a 3D social saliency ďŹ eld and locate multiple gaze concurrences that occur in a social scene from videos taken by head-mounted cameras. We model the gaze as a cone-shaped distribution emanating from the center of the eyes, capturing the variation of eye-in-head motion. We calibrate the parameters of this distribution by exploiting the ďŹ xed relationship between the primary gaze ray and the head-mounted camera pose. The resulting gaze model enables us to build a social saliency ďŹ eld in 3D. We estimate the number and 3D locations of the gaze concurrences via provably convergent modeseeking in the social saliency ďŹ eld. Our algorithm is applied to reconstruct multiple gaze concurrences in several real world scenes and evaluated quantitatively against motion-captured ground truth. 1</p><p>5 0.53636795 <a title="103-lsi-5" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>Author: Jianxiong Xiao, Bryan Russell, Antonio Torralba</p><p>Abstract: In this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes. In contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3D cuboid model. Our model copes with different 3D viewpoints and aspect ratios and is able to detect cuboids across many different object categories. We introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database. Our model out-performs baseline detectors that use 2D constraints alone on the task of localizing cuboid corners. 1</p><p>6 0.52990782 <a title="103-lsi-6" href="./nips-2012-Near-optimal_Differentially_Private_Principal_Components.html">237 nips-2012-Near-optimal Differentially Private Principal Components</a></p>
<p>7 0.52340758 <a title="103-lsi-7" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>8 0.52084798 <a title="103-lsi-8" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>9 0.50867754 <a title="103-lsi-9" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>10 0.49933708 <a title="103-lsi-10" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>11 0.49579602 <a title="103-lsi-11" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>12 0.49518186 <a title="103-lsi-12" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>13 0.48394001 <a title="103-lsi-13" href="./nips-2012-CPRL_--_An_Extension_of_Compressive_Sensing_to_the_Phase_Retrieval_Problem.html">63 nips-2012-CPRL -- An Extension of Compressive Sensing to the Phase Retrieval Problem</a></p>
<p>14 0.47625294 <a title="103-lsi-14" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>15 0.46907866 <a title="103-lsi-15" href="./nips-2012-Learning_to_Discover_Social_Circles_in_Ego_Networks.html">194 nips-2012-Learning to Discover Social Circles in Ego Networks</a></p>
<p>16 0.46385902 <a title="103-lsi-16" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>17 0.46182725 <a title="103-lsi-17" href="./nips-2012-Factoring_nonnegative_matrices_with_linear_programs.html">125 nips-2012-Factoring nonnegative matrices with linear programs</a></p>
<p>18 0.45072761 <a title="103-lsi-18" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>19 0.44331613 <a title="103-lsi-19" href="./nips-2012-Topology_Constraints_in_Graphical_Models.html">346 nips-2012-Topology Constraints in Graphical Models</a></p>
<p>20 0.43816581 <a title="103-lsi-20" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (21, 0.043), (38, 0.111), (39, 0.013), (42, 0.027), (54, 0.021), (55, 0.019), (73, 0.227), (74, 0.076), (76, 0.161), (80, 0.113), (92, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83519524 <a title="103-lda-1" href="./nips-2012-Augment-and-Conquer_Negative_Binomial_Processes.html">47 nips-2012-Augment-and-Conquer Negative Binomial Processes</a></p>
<p>Author: Mingyuan Zhou, Lawrence Carin</p><p>Abstract: By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efﬁcient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. 1</p><p>same-paper 2 0.82604843 <a title="103-lda-2" href="./nips-2012-Distributed_Probabilistic_Learning_for_Camera_Networks_with_Missing_Data.html">103 nips-2012-Distributed Probabilistic Learning for Camera Networks with Missing Data</a></p>
<p>Author: Sejong Yoon, Vladimir Pavlovic</p><p>Abstract: Probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points. However, many problems in wide-area surveillance can beneﬁt from distributed modeling, either because of physical or computational constraints. Most distributed models to date use algebraic approaches (such as distributed SVD) and as a result cannot explicitly deal with missing data. In this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing. In particular, we show how traditional centralized models, such as probabilistic PCA and missing-data PPCA, can be learned when the data is distributed across a network of sensors. We demonstrate the utility of this approach on the problem of distributed afﬁne structure from motion. Our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations. 1</p><p>3 0.78385168 <a title="103-lda-3" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>Author: Vasiliy Karasev, Alessandro Chiuso, Stefano Soatto</p><p>Abstract: We describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process. We focus on the problem of “visual search” of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density). We show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions. We show that a “passive” agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an “omnipotent” agent, capable of inﬁnite control authority, can achieve arbitrarily good performance (asymptotically). In between these limiting cases, the tradeoff can be characterized empirically. 1</p><p>4 0.73111689 <a title="103-lda-4" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>Author: Weilong Yang, Yang Wang, Arash Vahdat, Greg Mori</p><p>Abstract: Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) – a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning. 1</p><p>5 0.72943908 <a title="103-lda-5" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>6 0.72388989 <a title="103-lda-6" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>7 0.72356367 <a title="103-lda-7" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>8 0.72296184 <a title="103-lda-8" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>9 0.72226894 <a title="103-lda-9" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>10 0.72212493 <a title="103-lda-10" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>11 0.71954256 <a title="103-lda-11" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<p>12 0.71915042 <a title="103-lda-12" href="./nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">232 nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>13 0.71827251 <a title="103-lda-13" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>14 0.71802604 <a title="103-lda-14" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>15 0.71789461 <a title="103-lda-15" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>16 0.71744996 <a title="103-lda-16" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>17 0.7174105 <a title="103-lda-17" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<p>18 0.71714705 <a title="103-lda-18" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>19 0.71691799 <a title="103-lda-19" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>20 0.71569335 <a title="103-lda-20" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
