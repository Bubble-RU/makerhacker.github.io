<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>300 nips-2012-Scalable nonconvex inexact proximal splitting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-300" href="#">nips2012-300</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>300 nips-2012-Scalable nonconvex inexact proximal splitting</h1>
<br/><p>Source: <a title="nips-2012-300-pdf" href="http://papers.nips.cc/paper/4785-scalable-nonconvex-inexact-proximal-splitting.pdf">pdf</a></p><p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>Reference: <a title="nips-2012-300-reference" href="../nips2012_reference/nips-2012-Scalable_nonconvex_inexact_proximal_splitting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Scalable nonconvex inexact proximal splitting  Suvrit Sra Max Planck Institute for Intelligent Systems 72076 T¨ bigen, Germany u suvrit@tuebingen. [sent-1, score-0.521]
</p><p>2 de  Abstract We study a class of large-scale, nonsmooth, and nonconvex optimization problems. [sent-3, score-0.227]
</p><p>3 In particular, we focus on nonconvex problems with composite objectives. [sent-4, score-0.32]
</p><p>4 This class includes the extensively studied class of convex composite objective problems as a subclass. [sent-5, score-0.184]
</p><p>5 To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. [sent-6, score-0.465]
</p><p>6 Within our new framework we derive both batch and incremental proximal splitting algorithms. [sent-7, score-0.321]
</p><p>7 To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. [sent-8, score-0.475]
</p><p>8 We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. [sent-9, score-0.102]
</p><p>9 (2)  Problem (1) is a natural but far-reaching generalization of composite objective convex problems, which enjoy tremendous importance in machine learning; see e. [sent-14, score-0.184]
</p><p>10 Although, convex formulations are extremely useful, for many difﬁcult problems a nonconvex formulation is natural. [sent-17, score-0.279]
</p><p>11 Our framework solves (1) by “splitting” the task into smooth (gradient) and nonsmooth (proximal) parts. [sent-21, score-0.102]
</p><p>12 This capability proves critical to obtaining a scalable, incremental-gradient variant of NIPS, which, to our knowledge, is the ﬁrst incremental proximal-splitting method for nonconvex problems. [sent-23, score-0.363]
</p><p>13 Notably, it does not require the errors to vanish in the limit, which is a more realistic assumption as often one has limited to no control over computational errors inherent to a complex system. [sent-25, score-0.111]
</p><p>14 In accord with the errors, NIPS also does not require stepsizes (learning rates) to shrink to zero. [sent-26, score-0.084]
</p><p>15 In contrast, most incremental-gradient methods [5] and stochastic gradient algorithms [16] do assume that the computational errors and stepsizes decay to zero. [sent-27, score-0.201]
</p><p>16 1  Our analysis builds on the remarkable work of Solodov [29], who studied the simpler setting of differentiable nonconvex problems (which correspond with h ≡ 0 in (1)). [sent-29, score-0.267]
</p><p>17 NIPS is strictly more general: unlike [29] it solves a non-differentiable problem by allowing a nonsmooth regularizer h ≡ 0, and this h is tackled by invoking proximal-splitting [8]. [sent-30, score-0.102]
</p><p>18 It retains the simplicity of gradient-projection while handling the nonsmooth regularizer h via its proximity operator. [sent-32, score-0.167]
</p><p>19 This approach is especially attractive because for several important choices of h, efﬁcient implementations of the associated proximity operators exist [2, 22, 23]. [sent-33, score-0.093]
</p><p>20 For convex problems, an alternative to proximal splitting is the subgradient method; similarly, for nonconvex problems one may use a generalized subgradient method [7, 12]. [sent-34, score-0.532]
</p><p>21 However, as in the convex case, the use of subgradients has drawbacks: it fails to exploit the composite structure, and even when using sparsity promoting regularizers it does not generate intermediate sparse iterates [11]. [sent-35, score-0.235]
</p><p>22 Among batch nonconvex splitting methods, an early paper is [14]. [sent-36, score-0.325]
</p><p>23 More recently, in his pioneering paper on convex composite minimization, Nesterov [26] also brieﬂy discussed nonconvex problems. [sent-37, score-0.372]
</p><p>24 [1] have introduced a generic method for nonconvex nonsmooth problems based on Kurdyka-Łojasiewicz theory, but their entire framework too hinges on descent. [sent-40, score-0.329]
</p><p>25 In general, the insistence on strict descent and exact gradients makes many of the methods unsuitable for incremental, stochastic, or online variants, all of which usually lead to a nonmonotone objective values especially due to inexact gradients. [sent-42, score-0.224]
</p><p>26 Among nonmonotonic methods that apply to (1), we are aware of the generalized gradient-type algorithms of [31] and the stochastic generalized gradient methods of [12]. [sent-43, score-0.15]
</p><p>27 Both methods, however, are analogous to the usual subgradient-based algorithms that fail to exploit the composite objective structure, unlike proximal-splitting methods. [sent-44, score-0.132]
</p><p>28 But proximal-splitting methods do not apply out-of-the-box to (1): nonconvexity raises signiﬁcant obstructions, especially because nonmonotonic descent in the objective function values is allowed and inexact gradient might be used. [sent-45, score-0.256]
</p><p>29 Overcoming these obstructions to achieve a scalable non-descent based method that allows inexact gradients is what makes our NIPS framework novel. [sent-46, score-0.157]
</p><p>30 The proximity operator for g, indexed by η > 0, is the nonlinear map [see e. [sent-52, score-0.095]
</p><p>31 It is also key to Rockafellar’s classic proximal point algorithm [27], and it arises in a host of proximal-splitting methods [2, 3, 8, 11], most notably in forward-backward splitting (FBS) [8]. [sent-58, score-0.189]
</p><p>32 It minimizes convex composite objective functions by alternating between “forward” (gradient) steps and “backward” (proximal) steps. [sent-60, score-0.184]
</p><p>33 Therefore, to tackle nonconvex f we must take a different approach. [sent-66, score-0.227]
</p><p>34 Thus, the key challenge is: how to retain the algorithmic simplicity of FBS and allow nonconvex losses, without sacriﬁcing scalability? [sent-69, score-0.227]
</p><p>35 We address this challenge by introducing the following inexact proximal-splitting iteration: g xk+1 = Pηk (xk − ηk f (xk ) + ηk e(xk )),  k = 0, 1, . [sent-70, score-0.105]
</p><p>36 ,  (7)  where e(xk ) models the computational errors in computing the gradient f (xk ). [sent-73, score-0.097]
</p><p>37 (8)  Condition (8) is weaker than the typical vanishing error requirements k  η e(xk ) < ∞,  lim η e(xk ) = 0,  k→∞  which are stipulated by most analyses of methods with gradient errors [4, 5]. [sent-75, score-0.159]
</p><p>38 We will, however, show that the iterates produced by (7) do progress towards reasonable inexact stationary points. [sent-77, score-0.159]
</p><p>39 We note in passing that even if we assume the simpler case of vanishing errors, NIPS is still the ﬁrst nonconvex proximalsplitting framework that does not insist on monotonicity, which complicates convergence analysis but ultimately proves crucial to scalability. [sent-78, score-0.393]
</p><p>40 Algorithm 1 Inexact Nonconvex Proximal Splitting (NIPS) g Input: Operator Pη , and a sequence {ηk } satisfying  c ≤ lim inf k ηk ,  lim supk ηk ≤ min {1, 2/L − c} ,  0 < c < 1/L. [sent-79, score-0.097]
</p><p>41 1  f (xk ) − e(xk )  Convergence analysis  We begin by characterizing inexact stationarity. [sent-81, score-0.105]
</p><p>42 (11)  This equation helps us deﬁne a measure of inexact stationarity: the proximal residual g ρ(x) := x − P1 (x − ∗  f (x)). [sent-84, score-0.252]
</p><p>43 Deﬁne the functions g g 1 pg (η) := η Pη (y − ηz) − y , and qg (η) := Pη (y − ηz) − y . [sent-105, score-0.139]
</p><p>44 (16) Then, pg (η) is a decreasing function of η, and qg (η) an increasing function of η. [sent-106, score-0.139]
</p><p>45 Consider the “deﬂected” proximal objective 1 mg (x, η; y, z) := z, x − y + 2η x − y 2 + g(x), for some y, z ∈ X . [sent-110, score-0.193]
</p><p>46 (17) Associate to objective mg the deﬂected Moreau-envelope Eg (η) := inf mg (x, η; y, z),  (18)  x∈X  g whose inﬁmum is attained at the unique point Pη (y − ηz). [sent-111, score-0.133]
</p><p>47 Similarly, deﬁne eg (γ) := Eg (1/γ); this function is concave in γ as it is a pointwise inﬁmum (indexed by x) of functions linear g 1 in γ [see e. [sent-116, score-0.091]
</p><p>48 Thus, its derivative eg (γ) = 2 P1/γ (x − γ −1 y) − x 2 = qg (1/γ), is a ˆ decreasing function of γ. [sent-121, score-0.206]
</p><p>49 Set η = 1/γ to conclude the argument about qg (η). [sent-122, score-0.097]
</p><p>50 Let xk+1 , xk , ηk , and X be as in (7), and that ηk e(xk ) ≤ (xk ) holds. [sent-127, score-0.813]
</p><p>51 Then, Φ(xk ) − Φ(xk+1 )  2−Lηk 2ηk  ≥  xk+1 − xk  2  −  1 ηk  (xk ) xk+1 − xk . [sent-128, score-1.626]
</p><p>52 (21)  in (21), and rearrange to obtain  k+1  f (xk ), xk+1 − xk +  − xk ) + sk+1 , xk − xk+1 . [sent-131, score-2.439]
</p><p>53 4  Next we further bound (20) by deriving two-sided bounds on xk+1 − xk . [sent-135, score-0.813]
</p><p>54 Let xk+1 , xk , and (xk ) be as before; also let c and ηk satisfy (9). [sent-137, score-0.813]
</p><p>55 Then, c ρ(xk ) − (xk ) ≤ xk+1 − xk ≤ ρ(xk ) + (xk ). [sent-138, score-0.813]
</p><p>56 First observe that from Lemma 4 that for ηk > 0 it holds that if 1 ≤ ηk then q(1) ≤ qg (ηk ),  and if ηk ≤ 1 then pg (1) ≤ pg (ηk ) =  1 ηk qg (ηk ). [sent-140, score-0.278]
</p><p>57 (25)  Using (25), the triangle inequality, and Lemma 3, we have min {1, ηk } qg (1) = min {1, ηk } ρ(xk )  ≤  g Pηk (xk − ηk f (xk )) − xk  ≤  g xk+1 − xk + xk+1 − Pηk (xk − ηk f (xk ))  ≤  xk+1 − xk + ηk e(xk )  ≤  xk+1 − xk + (xk ). [sent-141, score-3.349]
</p><p>58 From (9) it follows that for sufﬁciently large k we have xk+1 − xk ≥ c ρ(xk ) − (xk ). [sent-142, score-0.813]
</p><p>59 For the upper bound note that g g xk+1 − xk ≤ xk − Pηk (xk − ηk f (xk )) + Pηk (xk − ηk f (xk )) − xk+1  ≤ max {1, ηk } ρ(xk ) + ηk e(xk )  ≤  ρ(xk ) + (xk ). [sent-143, score-1.626]
</p><p>60 Let xk , xk+1 , ηk , and c be as above and k sufﬁciently large so that c and ηk satisfy (9). [sent-146, score-0.813]
</p><p>61 There exists a limit point x∗ of the sequence xk , and a constant K > 0, such that ρ(x∗ ) ≤ K (x∗ ). [sent-154, score-0.813]
</p><p>62 If Φ(xk ) converges, then for every limit point x∗ of xk it holds that ρ(x∗ ) ≤ K (x∗ ). [sent-155, score-0.813]
</p><p>63 The statement of the theorem is written in a conditional form, because nonvanishing errors e(x) prevent us from making a stronger statement. [sent-162, score-0.156]
</p><p>64 In particular, once the iterates enter a region where the residual norm falls below the error threshold, the behavior of xk may be arbitrary. [sent-163, score-0.89]
</p><p>65 This, however, is a small price to pay for having the added ﬂexibility of nonvanishing errors. [sent-164, score-0.11]
</p><p>66 Under the stronger assumption of vanishing errors (and diminishing stepsizes), we can also obtain guarantees to exact stationary points. [sent-165, score-0.107]
</p><p>67 3  Scaling up NIPS: incremental variant  We now apply NIPS to the large-scale setting, where we have composite objectives of the form Φ(x) :=  T t=1  ft (x) + g(x),  (27)  1 where each ft : Rn → R is a CLt (X ) function. [sent-166, score-0.594]
</p><p>68 It is well-known that for such decomposable objectives it can be advantageous to replace the full gradient t ft (x) by an incremental gradient fσ(t) (x), where σ(t) is some suitable index. [sent-168, score-0.406]
</p><p>69 5  Nonconvex incremental methods for differentiable problems have been extensively analyzed, e. [sent-169, score-0.147]
</p><p>70 However, when g(x) = 0, the only incremental methods that we are aware of are stochastic generalized gradient methods of [12] or the generalized gradient methods of [31]. [sent-172, score-0.277]
</p><p>71 As previously mentioned, both of these fail to exploit the composite structure of the objective function, a disadvantage even in the convex case [11]. [sent-173, score-0.184]
</p><p>72 In stark contrast, we do exploit the composite structure of (27). [sent-174, score-0.093]
</p><p>73 Formally, we propose the following incremental nonconvex proximal-splitting iteration: T  xk+1 = M xk − ηk xk,1 = xk ,  t=1  ft (xk,t ) ,  k = 0, 1, . [sent-175, score-2.157]
</p><p>74 ,  xk,t+1 = O(xk,t − ηk ft (xk,t )),  (28)  t = 1, . [sent-178, score-0.197]
</p><p>75 For example, when X = Rn , g(x) ≡ 0, M = O = Id, and ηk → 0, then (28) reduces to the classic incremental gradient method (IGM) [4], and to the IGM of [30], if lim ηk = η > 0. [sent-182, score-0.185]
</p><p>76 If X a closed ¯ convex set, g(x) ≡ 0, M is orthogonal projection onto X , O = Id, and ηk → 0, then iteration (28) reduces to (projected) IGM [4, 5]. [sent-183, score-0.097]
</p><p>77 We begin by rewriting (28) in a form that matches the main iteration (7): g xk+1 = Pη xk − ηk g = Pη xk − ηk g = Pη xk − ηk  T  ft (xk,t )  t=1 T t=1 t  T  ft (xk ) + ηk  t=1  ft (xk ) − ft (xk,t )  (29)  ft (xk ) + ηk e(xk ) . [sent-190, score-3.45]
</p><p>78 From the deﬁnition of a proximity operator (5), we have the inequality  =⇒  1 2 1 2  xk,t+1 − xk,t + ηk ft (xk,t ) xk,t+1 − xk,t  2  ≤ ηk  2  + ηk g(xk,t+1 ) ≤  1 2  ηk ft (xk,t )  2  + ηk g(xk,t ),  ft (xk,t ), xk,t − xk,t+1 + ηk (g(xk,t ) − g(xk,t+1 )). [sent-197, score-0.686]
</p><p>79 Therefore, 1 2  xk,t+1 − xk,t  2  ≤ ηk st , xk,t − xk,t+1 + ≤ ηk st +  =⇒  ft (xk,t ), xk,t − xk,t+1  ft (xk,t ) xk,t − xk,t+1  xk,t+1 − xk,t ≤ 2ηk  Lemma 9 proves helpful in bounding the overall error. [sent-199, score-0.501]
</p><p>80 If for all xk ∈ X , ft (xk ) ≤ M and ∂g(xk ) ≤ G, then there exists a constant K1 > 0 such that e(xk ) ≤ K1 . [sent-202, score-1.01]
</p><p>81 To bound the error of using xk,t instead of xk ﬁrst deﬁne the term := ft (xk,t ) − ft (xk ) , t = 1, . [sent-204, score-1.207]
</p><p>82 (32)  = 0, (32) then leads to the bound  1  t−1  j=1  T −1  (1 + 2ηk L)t−1−j βj = 2ηk L  (1 + 2ηk L)T −t βt  ≤  t=1 T −1  (1 + 2ηk L)T −1  t=1  T −t−1  βt  j=0  (1 + 2ηk L)j  ft (x) + st  ≤ C1 (T − 1)(M + G) =: K1 . [sent-209, score-0.236]
</p><p>83 Thus, the error norm e(xk ) is bounded from above by a constant, whereby it satisﬁes the requirement (8), making the incremental NIPS method (28) a special case of the general NIPS framework. [sent-210, score-0.143]
</p><p>84 We do, however, provide an illustrative application of NIPS to a challenging nonconvex problem: sparsity regularized low-rank matrix factorization min  X,A≥0  1 2  Y − XA  2 F  T  + ψ0 (X) +  t=1  ψt (at ),  (33)  where Y ∈ Rm×T , X ∈ Rm×K and A ∈ RK×T , with a1 , . [sent-213, score-0.292]
</p><p>85 Problem (33) generalizes the well-known nonnegative matrix factorization (NMF) problem of [20] by permitting arbitrary Y (not necessarily nonnegative), and adding regularizers on X and A. [sent-217, score-0.091]
</p><p>86 A related class of problems was studied in [23], but with a crucial difference: the formulation in [23] does not allow nonsmooth regularizers on X. [sent-218, score-0.146]
</p><p>87 On a more theoretical note, [23] considered stochastic-gradient like methods whose analysis requires computational errors and stepsizes to vanish, whereas our method is deterministic and allows nonvanishing stepsizes and errors. [sent-220, score-0.324]
</p><p>88 We eliminate A and consider minX  φ(X) :=  T t=1  ft (X) + g(X),  where g(X) := ψ0 (X) + δ(X|≥ 0),  (34)  and where each ft (X) for 1 ≤ t ≤ T is deﬁned as ft (X) := mina  1 2  yt − Xa  2  + gt (a),  (35) 1  where gt (a) := ψt (a) + δ(a|≥ 0). [sent-222, score-0.591]
</p><p>89 For simplicity, assume that (35) attains its unique minimum, say a∗ , then ft (X) is differentiable and we have X ft (X) = (Xa∗ − yt )(a∗ )T . [sent-223, score-0.434]
</p><p>90 Figure 2 shows numerical results comparing the stochastic generalized gradient (SGGD) algorithm of [12] against NIPS, when started at the same point. [sent-249, score-0.095]
</p><p>91 5  Discussion  We presented a new framework called NIPS, which solves a broad class of nonconvex composite objective problems. [sent-266, score-0.359]
</p><p>92 NIPS permits nonvanishing computational errors, which can be practically useful. [sent-267, score-0.11]
</p><p>93 We specialized NIPS to also obtain a scalable incremental version. [sent-268, score-0.128]
</p><p>94 For example, batch and incremental convex FBS, convex and nonconvex gradient projection, the proximalpoint algorithm, among others. [sent-271, score-0.514]
</p><p>95 Theoretically, however, the most exciting open problem resulting from this paper is: extend NIPS in a scalable way when even the nonsmooth part is nonconvex. [sent-272, score-0.123]
</p><p>96 Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. [sent-279, score-0.146]
</p><p>97 Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization. [sent-359, score-0.424]
</p><p>98 A generalized proximal point algorithm for certain non-convex minimization problems. [sent-377, score-0.14]
</p><p>99 Convergence properties of backpropagation for neural nets via theory of stochastic gradient methods. [sent-391, score-0.089]
</p><p>100 Incremental gradient algorithms with stepsizes bounded away from zero. [sent-488, score-0.135]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xk', 0.813), ('nonconvex', 0.227), ('ft', 0.197), ('proximal', 0.116), ('nonvanishing', 0.11), ('sggd', 0.11), ('incremental', 0.107), ('inexact', 0.105), ('nonsmooth', 0.102), ('spams', 0.097), ('qg', 0.097), ('composite', 0.093), ('eg', 0.091), ('stepsizes', 0.084), ('fbs', 0.083), ('nmf', 0.073), ('splitting', 0.073), ('nips', 0.071), ('proximity', 0.065), ('convex', 0.052), ('gradient', 0.051), ('factorization', 0.047), ('igm', 0.047), ('errors', 0.046), ('lemma', 0.046), ('pg', 0.042), ('cl', 0.041), ('differentiable', 0.04), ('id', 0.04), ('st', 0.039), ('stepsize', 0.039), ('objective', 0.039), ('lsc', 0.038), ('mg', 0.038), ('atlab', 0.036), ('cbcl', 0.036), ('vanishing', 0.035), ('rn', 0.035), ('yale', 0.033), ('stationarity', 0.031), ('attouch', 0.031), ('dmg', 0.031), ('gafni', 0.031), ('insistence', 0.031), ('nonmonotonic', 0.031), ('obstructions', 0.031), ('proximalsplitting', 0.031), ('solodov', 0.031), ('residual', 0.031), ('descent', 0.03), ('operator', 0.03), ('convergence', 0.03), ('xa', 0.029), ('rand', 0.029), ('penalized', 0.029), ('proves', 0.029), ('iterates', 0.028), ('operators', 0.028), ('moreau', 0.028), ('rockafellar', 0.028), ('variants', 0.027), ('lim', 0.027), ('iteration', 0.026), ('regularizers', 0.026), ('stationary', 0.026), ('suvrit', 0.025), ('fukushima', 0.025), ('supk', 0.025), ('batch', 0.025), ('combettes', 0.024), ('invoke', 0.024), ('generalized', 0.024), ('sk', 0.023), ('complicates', 0.023), ('obviously', 0.022), ('ected', 0.022), ('scalable', 0.021), ('seconds', 0.021), ('stochastic', 0.02), ('subgradient', 0.02), ('unconstrained', 0.02), ('projection', 0.019), ('sra', 0.019), ('vanish', 0.019), ('strict', 0.019), ('mum', 0.019), ('minx', 0.019), ('crucial', 0.018), ('sparsity', 0.018), ('nonnegative', 0.018), ('inf', 0.018), ('derivative', 0.018), ('whereby', 0.018), ('sparse', 0.018), ('norm', 0.018), ('monotonicity', 0.018), ('backpropagation', 0.018), ('plots', 0.018), ('nesterov', 0.017), ('mairal', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="300-tfidf-1" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>2 0.48830867 <a title="300-tfidf-2" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><p>3 0.37017182 <a title="300-tfidf-3" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>4 0.26524007 <a title="300-tfidf-4" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>5 0.26324677 <a title="300-tfidf-5" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>Author: Nicholas Ruozzi</p><p>Abstract: Sudderth, Wainwright, and Willsky conjectured that the Bethe approximation corresponding to any ﬁxed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function. In this work, we resolve this conjecture in the afﬁrmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the Bethe partition function always lower bounds the true partition function. The proof of this result follows from a new variant of the “four functions” theorem that may be of independent interest. 1</p><p>6 0.15180667 <a title="300-tfidf-6" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>7 0.14907669 <a title="300-tfidf-7" href="./nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">20 nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<p>8 0.14384542 <a title="300-tfidf-8" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>9 0.12979063 <a title="300-tfidf-9" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>10 0.12517273 <a title="300-tfidf-10" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>11 0.11803151 <a title="300-tfidf-11" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>12 0.10689329 <a title="300-tfidf-12" href="./nips-2012-Minimization_of_Continuous_Bethe_Approximations%3A_A_Positive_Variation.html">213 nips-2012-Minimization of Continuous Bethe Approximations: A Positive Variation</a></p>
<p>13 0.10410269 <a title="300-tfidf-13" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>14 0.097904302 <a title="300-tfidf-14" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>15 0.094233342 <a title="300-tfidf-15" href="./nips-2012-Nonconvex_Penalization_Using_Laplace_Exponents_and_Concave_Conjugates.html">244 nips-2012-Nonconvex Penalization Using Laplace Exponents and Concave Conjugates</a></p>
<p>16 0.080742002 <a title="300-tfidf-16" href="./nips-2012-Transelliptical_Component_Analysis.html">351 nips-2012-Transelliptical Component Analysis</a></p>
<p>17 0.079630025 <a title="300-tfidf-17" href="./nips-2012-A_Convex_Formulation_for_Learning_Scale-Free_Networks_via_Submodular_Relaxation.html">6 nips-2012-A Convex Formulation for Learning Scale-Free Networks via Submodular Relaxation</a></p>
<p>18 0.073623747 <a title="300-tfidf-18" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>19 0.072076783 <a title="300-tfidf-19" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>20 0.070970766 <a title="300-tfidf-20" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.163), (1, 0.052), (2, 0.204), (3, -0.085), (4, 0.133), (5, 0.13), (6, 0.032), (7, -0.246), (8, 0.391), (9, 0.119), (10, -0.197), (11, 0.188), (12, 0.152), (13, -0.082), (14, -0.172), (15, -0.104), (16, -0.073), (17, 0.098), (18, -0.036), (19, 0.012), (20, -0.187), (21, -0.025), (22, -0.015), (23, -0.041), (24, 0.066), (25, -0.014), (26, -0.03), (27, -0.065), (28, 0.02), (29, -0.088), (30, 0.004), (31, -0.06), (32, 0.034), (33, 0.077), (34, 0.066), (35, 0.025), (36, 0.036), (37, -0.011), (38, 0.017), (39, -0.029), (40, 0.011), (41, -0.023), (42, 0.025), (43, -0.055), (44, 0.045), (45, 0.051), (46, -0.012), (47, 0.042), (48, 0.004), (49, -0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99094737 <a title="300-lsi-1" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>2 0.88025194 <a title="300-lsi-2" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><p>3 0.83448875 <a title="300-lsi-3" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>4 0.77457494 <a title="300-lsi-4" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>5 0.60194749 <a title="300-lsi-5" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>6 0.59350693 <a title="300-lsi-6" href="./nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">20 nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<p>7 0.59237319 <a title="300-lsi-7" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>8 0.43504718 <a title="300-lsi-8" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>9 0.40823817 <a title="300-lsi-9" href="./nips-2012-Adaptive_Learning_of_Smoothing_Functions%3A_Application_to_Electricity_Load_Forecasting.html">35 nips-2012-Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting</a></p>
<p>10 0.38967618 <a title="300-lsi-10" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>11 0.38436064 <a title="300-lsi-11" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>12 0.34073597 <a title="300-lsi-12" href="./nips-2012-Convergence_and_Energy_Landscape_for_Cheeger_Cut_Clustering.html">85 nips-2012-Convergence and Energy Landscape for Cheeger Cut Clustering</a></p>
<p>13 0.3283577 <a title="300-lsi-13" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>14 0.32230774 <a title="300-lsi-14" href="./nips-2012-Minimization_of_Continuous_Bethe_Approximations%3A_A_Positive_Variation.html">213 nips-2012-Minimization of Continuous Bethe Approximations: A Positive Variation</a></p>
<p>15 0.32210961 <a title="300-lsi-15" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<p>16 0.2687901 <a title="300-lsi-16" href="./nips-2012-Optimal_Regularized_Dual_Averaging_Methods_for_Stochastic_Optimization.html">263 nips-2012-Optimal Regularized Dual Averaging Methods for Stochastic Optimization</a></p>
<p>17 0.26493505 <a title="300-lsi-17" href="./nips-2012-Newton-Like_Methods_for_Sparse_Inverse_Covariance_Estimation.html">240 nips-2012-Newton-Like Methods for Sparse Inverse Covariance Estimation</a></p>
<p>18 0.25583327 <a title="300-lsi-18" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>19 0.25000161 <a title="300-lsi-19" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<p>20 0.23778148 <a title="300-lsi-20" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.086), (21, 0.324), (38, 0.143), (42, 0.026), (54, 0.03), (55, 0.017), (64, 0.013), (74, 0.02), (76, 0.116), (80, 0.085), (92, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92191941 <a title="300-lda-1" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>Author: Marius Pachitariu, Maneesh Sahani</p><p>Abstract: We present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated Gaussian variables. Trained on sequences of images, the model learns to represent different movement directions in different variables. We use an online approximate inference scheme that can be mapped to the dynamics of networks of neurons. Probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex. Most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed. We show how these computations are enabled by a speciﬁc pattern of recurrent connections learned by the model. 1</p><p>same-paper 2 0.87792677 <a title="300-lda-2" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>3 0.87135738 <a title="300-lda-3" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>Author: Sanja Fidler, Sven Dickinson, Raquel Urtasun</p><p>Abstract: This paper addresses the problem of category-level 3D object detection. Given a monocular image, our aim is to localize the objects in 3D by enclosing them with tight oriented 3D bounding boxes. We propose a novel approach that extends the well-acclaimed deformable part-based model [1] to reason in 3D. Our model represents an object class as a deformable 3D cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3D box. We model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint. Our model reasons about face visibility patters called aspects. We train the cuboid model jointly and discriminatively and share weights across all aspects to attain efﬁciency. Inference then entails sliding and rotating the box in 3D and scoring object hypotheses. While for inference we discretize the search space, the variables are continuous in our model. We demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach signiﬁcantly outperforms the stateof-the-art in both 2D [1] and 3D object detection [2]. 1</p><p>4 0.82711673 <a title="300-lda-4" href="./nips-2012-Bayesian_nonparametric_models_for_ranked_data.html">60 nips-2012-Bayesian nonparametric models for ranked data</a></p>
<p>Author: Francois Caron, Yee W. Teh</p><p>Abstract: We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an inﬁnite number of choice items. Our framework is based on the theory of random atomic measures, with the prior speciﬁed by a gamma process. We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation. We develop a time-varying extension of our model, and apply it to the New York Times lists of weekly bestselling books. 1</p><p>5 0.80315816 <a title="300-lda-5" href="./nips-2012-Dynamic_Pruning_of_Factor_Graphs_for_Maximum_Marginal_Prediction.html">105 nips-2012-Dynamic Pruning of Factor Graphs for Maximum Marginal Prediction</a></p>
<p>Author: Christoph H. Lampert</p><p>Abstract: We study the problem of maximum marginal prediction (MMP) in probabilistic graphical models, a task that occurs, for example, as the Bayes optimal decision rule under a Hamming loss. MMP is typically performed as a two-stage procedure: one estimates each variable’s marginal probability and then forms a prediction from the states of maximal probability. In this work we propose a simple yet effective technique for accelerating MMP when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable. This allows us to identify the point of time when we are sufﬁciently certain about any individual decision. Whenever this is the case, we dynamically prune the variables we are conﬁdent about from the underlying factor graph. Consequently, at any time only samples of variables whose decision is still uncertain need to be created. Experiments in two prototypical scenarios, multi-label classiﬁcation and image inpainting, show that adaptive sampling can drastically accelerate MMP without sacriﬁcing prediction accuracy. 1</p><p>6 0.79440826 <a title="300-lda-6" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>7 0.76646972 <a title="300-lda-7" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<p>8 0.71760118 <a title="300-lda-8" href="./nips-2012-A_lattice_filter_model_of_the_visual_pathway.html">23 nips-2012-A lattice filter model of the visual pathway</a></p>
<p>9 0.70891345 <a title="300-lda-9" href="./nips-2012-A_Simple_and_Practical_Algorithm_for_Differentially_Private_Data_Release.html">18 nips-2012-A Simple and Practical Algorithm for Differentially Private Data Release</a></p>
<p>10 0.69741064 <a title="300-lda-10" href="./nips-2012-Efficient_and_direct_estimation_of_a_neural_subunit_model_for_sensory_coding.html">113 nips-2012-Efficient and direct estimation of a neural subunit model for sensory coding</a></p>
<p>11 0.67947042 <a title="300-lda-11" href="./nips-2012-Learning_optimal_spike-based_representations.html">190 nips-2012-Learning optimal spike-based representations</a></p>
<p>12 0.66361821 <a title="300-lda-12" href="./nips-2012-Efficient_coding_provides_a_direct_link_between_prior_and_likelihood_in_perceptual_Bayesian_inference.html">114 nips-2012-Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference</a></p>
<p>13 0.66169137 <a title="300-lda-13" href="./nips-2012-Delay_Compensation_with_Dynamical_Synapses.html">94 nips-2012-Delay Compensation with Dynamical Synapses</a></p>
<p>14 0.6604135 <a title="300-lda-14" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>15 0.65453911 <a title="300-lda-15" href="./nips-2012-Homeostatic_plasticity_in_Bayesian_spiking_networks_as_Expectation_Maximization_with_posterior_constraints.html">152 nips-2012-Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints</a></p>
<p>16 0.65339452 <a title="300-lda-16" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>17 0.64687127 <a title="300-lda-17" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>18 0.64385659 <a title="300-lda-18" href="./nips-2012-A_mechanistic_model_of_early_sensory_processing_based_on_subtracting_sparse_representations.html">24 nips-2012-A mechanistic model of early sensory processing based on subtracting sparse representations</a></p>
<p>19 0.64058137 <a title="300-lda-19" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>20 0.63706577 <a title="300-lda-20" href="./nips-2012-Neuronal_Spike_Generation_Mechanism_as_an_Oversampling%2C_Noise-shaping_A-to-D_converter.html">239 nips-2012-Neuronal Spike Generation Mechanism as an Oversampling, Noise-shaping A-to-D converter</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
