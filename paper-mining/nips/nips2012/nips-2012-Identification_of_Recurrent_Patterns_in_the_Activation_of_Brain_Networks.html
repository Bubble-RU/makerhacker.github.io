<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-157" href="#">nips2012-157</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</h1>
<br/><p>Source: <a title="nips-2012-157-pdf" href="http://papers.nips.cc/paper/4771-identification-of-recurrent-patterns-in-the-activation-of-brain-networks.pdf">pdf</a></p><p>Author: Firdaus Janoos, Weichang Li, Niranjan Subrahmanya, Istvan Morocz, William Wells</p><p>Abstract: Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) deďŹ ning a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series. In this paper, we present a network-aware feature-space to represent the states of a general network, that enables comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efďŹ cient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efďŹ ciency of the approximation, especially for large problems. 1</p><p>Reference: <a title="nips-2012-157-reference" href="../nips2012_reference/nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Wells (III) a Â´ o Harvard Medical School Boston, MA 02115  Abstract Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. [sent-3, score-0.654]
</p><p>2 This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? [sent-6, score-0.484]
</p><p>3 1  Introduction  In addition to functional localization and integration, mapping the neural correlates of â&euro;&oelig;mental statesâ&euro;? [sent-9, score-0.121]
</p><p>4 the distinct cognitive, affective or perceptive states of the human mind) is an important research topic for understanding the connection between mind and brain [2]. [sent-13, score-0.265]
</p><p>5 In functional neuroimaging, this problem is equivalent to identifying recurrent spatial patterns from the recorded activation of neural circuits and relating them with the mental state of the subject. [sent-14, score-0.484]
</p><p>6 In contrast to clustering voxels based on the similarity of their functional activity (i. [sent-16, score-0.302]
</p><p>7 along the spatial dimension) [15], the problem of clustering fMRI data along the temporal dimension has not been widely explored in literature, primarily because of the following challenges: a) Lack of â&circ;&mdash;  Corresponding Author. [sent-18, score-0.096]
</p><p>8 com  1  a physiologically meaningful metric to compare the difference between the spatial distribution of recorded brain activity (i. [sent-21, score-0.404]
</p><p>9 brain states) at two different time-points; b) Problems that arise because the number of voxels (i. [sent-23, score-0.255]
</p><p>10 The dimensionality problem in fMRI has been typically addressed through PCA [16], ICA[3] or by selection of a subset of voxels either manually or via regression against the stimulus [18, 11]. [sent-29, score-0.114]
</p><p>11 On the other hand, supervised featurespaces are inherently biased towards the experimental variables against which they were selected or by the investigatorâ&euro;&trade;s expectations, and may not capture unexpected patterns in the data. [sent-31, score-0.112]
</p><p>12 Intuitively, this network-aware metric assesses the distance between two states zt1 , zt2 that differ mainly on proximally connected nodes to be less than the distance between states zt1 , zt2 that differ on unconnected nodes. [sent-35, score-0.485]
</p><p>13 In the context of neuroimaging, where the network measures the functional connectivity [4] between brain regions, this implies that two brain activation patterns that differ mainly on functionally similar regions are functionally closer than two that differ on functionally unrelated regions. [sent-38, score-0.923]
</p><p>14 For example, zt1 and zt2 that zt1 zt3 activated mainly in the cingulo-opercular network would be functionally more similar with each other than with zt3 that exhibited activity mainly in the fronto-parietal network. [sent-39, score-0.287]
</p><p>15 Such network awareness is provided by the zt2 Kantorovich metric[20], also called the transportation distance (TD), which measures the minimum ďŹ&sbquo;ow of â&euro;&oelig;massâ&euro;? [sent-40, score-0.39]
</p><p>16 over the network to Figure 1: Shown are zt1 , zt2 and zt3 , three states make zt1 at time t1 match zt2 that at t2 . [sent-41, score-0.127]
</p><p>17 Here, zt1 and cost of this ďŹ&sbquo;ow is encoded by the weights of zt2 activate on more proximal regions of the graph and the edges of the graph. [sent-43, score-0.118]
</p><p>18 tance (EMD), closely related to the transportation distance, is widely used for clustering and retrieval in computer vision, medical imaging, bio-informatics and data-mining [21, 22, 7]. [sent-46, score-0.279]
</p><p>19 The TD, however, has the following limitations: Firstly, it is computationally expensive with worstcase complexity of O(NV 3 log NV ) where NV is the number of nodes in the graph [17]. [sent-48, score-0.134]
</p><p>20 2  The second contribution of this paper is to address these issues through the development of a linear feature-space that provides a good approximation of the transportation distance. [sent-56, score-0.28]
</p><p>21 This featureâ&euro;&ldquo;space is motivated by spherical relaxation [14] of the dual polytope of the transportation problem, as described in Section 2. [sent-57, score-0.46]
</p><p>22 The network function zG is then embedded into an Euclidean space via a similarity transformation such that the the transportation distance is well-approximated by the 2 distance in this space, as elucidated in Section 3. [sent-58, score-0.519]
</p><p>23 In contrast to existing linear approximations, the feature-space developed here has a very simple form closely related to the graph Laplacian [6]. [sent-59, score-0.086]
</p><p>24 Theoretical bounds to the error the approximation are developed and the accuracy of the method is validated empirically in Section 4. [sent-60, score-0.095]
</p><p>25 2  Transportation Distance and Spherical Relaxation  Let zt1 and zt2 denote the states of zG at time-points t1 , t2 on the graph G = (V, E), with nodes V = {1 . [sent-66, score-0.203]
</p><p>26 The symmetric distance matrix WG [i, j] â&circ;&circ; R+ encodes the cost of transport between nodes i and j . [sent-70, score-0.176]
</p><p>27 Also, deďŹ ne the difference between two states as dz = zt1 â&circ;&rsquo; zt2 , and assume iâ&circ;&circ;V dz[i] = 0 without loss of generality 1 . [sent-71, score-0.309]
</p><p>28 over the network to convert zt1 into zt2 , is posed as the following linear program (LP): TD(zt1 , zt2 ) = min f  subject to  f [i, j]WG [i, j],  f [i, j] â&circ;&rsquo; j  iâ&circ;&circ;V bâ&circ;&circ;V  f [j, i] = dz[i]. [sent-73, score-0.093]
</p><p>29 (1)  j  The corresponding TP dual, formulated in the unrestricted dual variables g : V â&dagger;&rsquo; R, is: TD(zt1 , zt2 ) = max g, dz g ďŁŽ 1 â&circ;&rsquo;1 ďŁŻ1 0 ďŁŻ ďŁŻ . [sent-74, score-0.274]
</p><p>30 The feasible set of the dual is a convex polytope formed by the intersection of the half-spaces speciďŹ ed by the constraints {ai,j , i = 1 : NV , j = 1 . [sent-129, score-0.146]
</p><p>31 These constraints which form normals to the hyper-planes bounding this polytope, are symmetrically distributed in the +i Ă&mdash; â&circ;&rsquo;j quadrant of RNV for each combination of i and j . [sent-133, score-0.106]
</p><p>32 Moreover, A is totally uni-modular [5], and has rank of NV â&circ;&rsquo; 1 with the LP polytope lying in an NV â&circ;&rsquo; 1 dimensional space orthogonal to 1NV , the 1 â&euro;&ldquo;vector in RNV . [sent-134, score-0.112]
</p><p>33 Here, subject to  TD(zt1 , zt2 ) = max < g, dz > g  Ag â&permil;¤ 1NV Ă&mdash;(NV â&circ;&rsquo;1) . [sent-140, score-0.275]
</p><p>34 (3)  â&circ;&scaron; Each hyper-plane of the LP polytope is at distance 1/ 2 from the origin and the maximum inscribed â&circ;&scaron;  hyper-sphere, with center at the origin and radius 1/ 2 touches all the polytopeâ&euro;&trade;s hyper-planes. [sent-141, score-0.328]
</p><p>35 Relaxing the feasible set of the TP dual from the convex polytope to this hyper-sphere, eqn. [sent-144, score-0.146]
</p><p>36 3  Linear Feature Space Embedding  In the case of an arbitrary distance matrix WG , however, the polytope loses its regular structure, and has a variable number of extreme points. [sent-146, score-0.213]
</p><p>37 Also, in general, the maximal inscribed hyper-sphere does not touch all the bounding hyper-planes, resulting in a very poor approximation [14]. [sent-147, score-0.11]
</p><p>38 Therefore, to use the spherical relaxation for the general problem, we apply a similarity transformation M, such that A Âˇ M = diag{wG }â&circ;&rsquo;1 A and M positive semi-deďŹ nite. [sent-148, score-0.111]
</p><p>39 (2) in terms of a new variable Îž Mg, we see that the general problem: TD(zt1 , zt2 ) = max < g, dz > g  such that  Ag â&permil;¤ wG  (6)  is equivalent to the special case given by eqn. [sent-150, score-0.24]
</p><p>40 (3), in a transformed space, as per: TD(zt1 , zt2 ) = max < Mâ&circ;&rsquo; Îž, dz >  such that  Îž  AÎž â&permil;¤ 1NV Ă&mdash;(NV â&circ;&rsquo;1) ,  (7)  where Mâ&circ;&rsquo; is the (pseudo-)inverse of M. [sent-151, score-0.24]
</p><p>41 2 2  (8)  Consequently, the transportation distance can be approximated by a 2 metric through a similarity transformation of the original space. [sent-157, score-0.429]
</p><p>42 In this case the error of the approximation is O(Îťâ&circ;&rsquo;1 ||dz||2 ) min (Â§Theorem 1 of the Supplemental), which implies that the approximation improves as the smallest eigenvalue of the graph Laplacian increases. [sent-158, score-0.23]
</p><p>43 Also, notice that the eigenvector vNV of LG corresponding to the smallest eigenvalue ÎťNV = 0 is a constant vector, and therefore vNV , dz = 0 by the requirement that iâ&circ;&circ;V dz[i] = 0 , thereby automatically reducing the dimension of the projected space to NV â&circ;&rsquo; 1 . [sent-159, score-0.24]
</p><p>44 k=1 Îťk /  4  Results  First, we start by providing an empirical validation of the approximation to the transportation distance in Section 4. [sent-165, score-0.381]
</p><p>45 1 And then the feature-space is used to ďŹ nd representative patterns (i. [sent-166, score-0.112]
</p><p>46 brain states) in the dynamically changing activations of the brain during a visuo-motor task in Section 4. [sent-168, score-0.334]
</p><p>47 1  Validation  To validate the linear approximation to the transportation distance on networks, like the brain, that exhibit a scale-free property [1], we simulated random graphs of NV vertices using the following procedure: a) Create an edge between nodes i and j with probability â&circ;? [sent-171, score-0.429]
</p><p>48 For 1 each instance G(n) of the graph, a set of T = 100 states zt : V(i) â&dagger;&rsquo; R, t = 1 . [sent-173, score-0.119]
</p><p>49 The transportation problem was solved using network simplex [17] in the IBM CPLEX optimization package, while the linear approximation was implemented in Matlab . [sent-181, score-0.338]
</p><p>50 This is because the approximation error for an arbitrary graph is O(Îťâ&circ;&rsquo;1 ||dz||2 ) , while min for random graphs satisfying basic regularity conditions the eigenvalues of the graph Laplacian increase as O(NV ) [8]. [sent-189, score-0.295]
</p><p>51 In comparison, the Euclidean metric ||zt1 â&circ;&rsquo; zt2 ||2 starts with a much higher relative error with respect to the transportation distance, and although its error also reduces with graph size, the trend is slower. [sent-190, score-0.478]
</p><p>52 Secondly, the variance of the error is much higher than the linear embedding proposed here. [sent-191, score-0.093]
</p><p>53 2(c), we observe that for data-points that are relatively close to each other, the ordering relationships are preserved with very high accuracy and it reduces as the relative distance between the points increases. [sent-200, score-0.147]
</p><p>54 The formulation has the effect of normalizing the distance between zt1 , zt2 with respect to the local neighborhood of zt1 . [sent-206, score-0.101]
</p><p>55 2(d) that the approximation error according to this measure is extremely low and almost constant with respect to NV for points that are close to each other. [sent-208, score-0.095]
</p><p>56 2  Neuroimaging Data  Clustering using the feature-space described in this paper was applied to a data-set of ďŹ fteen subjects performing a visuo-motor task during functional MR imaging to discover salient patterns of recurrent brain activation. [sent-211, score-0.42]
</p><p>57 The subjects were visually exposed to oriented wedges ďŹ lled with highcontrast random noise patterns and displayed randomly in one of four quadrants. [sent-212, score-0.152]
</p><p>58 They were asked to focus on a center dot and to perform a ďŹ nger-tapping motion with the right or left hand when the visual wedge was active in the upper right or lower left quadrants, respectively. [sent-213, score-0.243]
</p><p>59 Block length of each visual wedge stimulation varied from 5 to 15s and noise patterns changed at a frequency of 5Hz. [sent-214, score-0.389]
</p><p>60 ˇ approx  20% 0% 128  256  128  512 1024 2048 4096 8192 16384  Number of nodes  Number of nodes  60%  25%-ile Tđ? [sent-220, score-0.125]
</p><p>61 (a) shows the (amortized) per-comparison running time in seconds for the transportation distance TD and its approximation TD with respect to with respect to graph size NV . [sent-223, score-0.467]
</p><p>62 (b) the relative approximation error (TD â&circ;&rsquo; TD)/TD (Âą1 std. [sent-225, score-0.095]
</p><p>63 The error for an Euclidean approximation ||zt1 â&circ;&rsquo; zt2 ||2 is also shown for comparison. [sent-228, score-0.095]
</p><p>64 The set {zt2 } is divided into quartiles according to their distance TD(zt1 , zt2 ) from zt1 , where the 25 percentile is set of the ďŹ rst 25% closest points to zt1 (similarly for the 50 and 75%-iles). [sent-236, score-0.101]
</p><p>65 Also shown is the ordering error of the Euclidean metric with respect to TD. [sent-237, score-0.161]
</p><p>66 Fig (d) shows the quartile-wise approximation error normalized by the average distance of its 10 nearest neighbors. [sent-239, score-0.196]
</p><p>67 The dashed line shows the un-normalized approximation error (Â§ Fig. [sent-240, score-0.095]
</p><p>68 The fMRI scans were motion corrected using linear registration and co- registered with the structural scans using SPM8 [16]. [sent-245, score-0.149]
</p><p>69 The design matrix included a regressor for the presentation of the wedge in each quadrant, convolved with a canonical hemodynamic response function. [sent-249, score-0.247]
</p><p>70 The functional networks for a subject were computed by estimating the correlations between voxels using the method described in Supplemental Section C, that is sparse, consistent and computationally efďŹ cient. [sent-254, score-0.205]
</p><p>71 The distance matrix of the functional connectivity graph was constructed as WG [i, j] = â&circ;&rsquo; log(|Ď [i, j]|/Ď&bdquo; ), where Ď [i, j] is the correlation between voxels i and j and Ď&bdquo; is a user-deďŹ ned scale parameter (typically set to 10). [sent-255, score-0.404]
</p><p>72 A multinomial logistic classiďŹ er (MLC) was then trained to predict the wedge position at time t from Ď&euro;t . [sent-270, score-0.214]
</p><p>73 It should be noted here that identiďŹ cation of patterns of recorded brain activity was performed in a purely unsupervised manner. [sent-272, score-0.363]
</p><p>74 Only model selection and model interpretation was done, post hoc, using observable correlates of the unobservable mental state of the subject. [sent-273, score-0.245]
</p><p>75 Spatial maps for each wedge orientation were computed as an average of cluster centroids weighted by the MLC weights for that orientation. [sent-274, score-0.299]
</p><p>76 3(c) shows the distribution of state probabilities for one subject corresponding to a sequence of wedges oriented in each quadrant for 4Ă&mdash;TRs each. [sent-278, score-0.202]
</p><p>77 Here, we see that the probability of a particular state is highly structured with respect to the orientation of the wedge. [sent-279, score-0.096]
</p><p>78 For example, at the start of the presentation with the wedge in the lower-right quadrant, state 1 is most probable. [sent-280, score-0.264]
</p><p>79 However, as this orientation is maintained, the probability distribution peaks about state 4 and remains stable. [sent-283, score-0.096]
</p><p>80 The spatial maps reconstructed from these two feature-spaces (not shown here) exhibited task-speciďŹ c activation patterns, although the foci were much weaker and much more diffused as compared to those of the TD feature-space. [sent-287, score-0.092]
</p><p>81 The error of the MLC in predicting the stimulus at time t from the state probability vector Ď&euro;t , which reďŹ&sbquo;ects the modelâ&euro;&trade;s ability to capture patterns in the data related to the mental state of the subject, for these three feature spaces is listed in Table 1. [sent-288, score-0.404]
</p><p>82 Due to the random presentation of wedge orientations, the chance level prediction error varied between 68% â&circ;&rsquo; â&circ;&rsquo;81% for each subject. [sent-320, score-0.294]
</p><p>83 (a): Group-level maximum intensity projections of signiďŹ cantly activated voxels (p < 0. [sent-326, score-0.115]
</p><p>84 05, FWE corrected) at the four orientations of the wedge and the hand motor actions, computed using SPM8 Fig. [sent-327, score-0.289]
</p><p>85 (b): Group-level z-maps showing the activity for each orientation of the wedge computed as an average of cluster centroids weighted by the MLC weights. [sent-328, score-0.383]
</p><p>86 8 during the display of the wedge in lower right, lower left, upper left and upper right quadrants for 4TRs each. [sent-336, score-0.25]
</p><p>87 5  Conclusion  In this paper, we have presented an approach to compare and identify patterns of brain activation during a mental process using a distance metric that is aware of the connectivity structure of the underlying brain networks. [sent-338, score-0.827]
</p><p>88 This distance metric is obtained by an Euclidean approximation of the transportation distance between patterns via a spherical relaxation of the linear-programming dual polytope. [sent-339, score-0.78]
</p><p>89 The embedding is achieved by a transformation of the original space of the function with the graph Laplacian of the network. [sent-340, score-0.161]
</p><p>90 We provided theoretical bounds on the quality of the approximation and through empirical validation demonstrated low error that, importantly, decreases as the size of the problem increases. [sent-342, score-0.095]
</p><p>91 We also showed the superior ability of this distance metric to identify salient patterns of brain activity, related to the internal mental state of the subject, from an fMRI study of visuo-motor tasks. [sent-343, score-0.619]
</p><p>92 The framework presented here is applicable to the more general problem of identifying patterns in time-varying measurements distributed over a network that has an intrinsic notion of distance and proximity, such as social, sensor, communication, transportation, energy and other similar networks. [sent-344, score-0.304]
</p><p>93 Future work would include assessing the quality of the approximation for sparse, restricted topology, small-world and scale-free networks that arise in many real world cases, and applying the method for detecting patterns and outliers in these types of networks. [sent-345, score-0.161]
</p><p>94 : A resilient, low-frequency, small-world human brain functional network with highly connected association cortical hubs. [sent-351, score-0.307]
</p><p>95 : Stateâ&euro;&ldquo;space models of mental processes o from fMRI (2011) 2, 7 [14] Khachiyan, L. [sent-411, score-0.12]
</p><p>96 : Categories and functional units: An inďŹ nite hierarchical model for brain activations. [sent-419, score-0.249]
</p><p>97 : Theoretical, statistical, and practical perspectives on pattern-based classiďŹ cation approaches to the analysis of functional neuroimaging data. [sent-440, score-0.167]
</p><p>98 : Segmentation of brain electrical activity into microstates: model estimation and validation. [sent-446, score-0.251]
</p><p>99 : Mass transportation problems: Volume I: Theory (probability and its applications) (March 1998) 2 [21] Rubner, Y. [sent-450, score-0.231]
</p><p>100 s distance as a metric for image retrieval (1998) 2 [22] Shirdhonkar, S. [sent-455, score-0.17]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nv', 0.428), ('td', 0.368), ('wg', 0.324), ('dz', 0.24), ('transportation', 0.231), ('wedge', 0.214), ('brain', 0.167), ('fmri', 0.163), ('mlc', 0.121), ('mental', 0.12), ('patterns', 0.112), ('polytope', 0.112), ('distance', 0.101), ('voxels', 0.088), ('laplacian', 0.088), ('graph', 0.086), ('neuroimaging', 0.085), ('activity', 0.084), ('rnv', 0.082), ('lg', 0.082), ('functional', 0.082), ('quadrant', 0.077), ('states', 0.069), ('metric', 0.069), ('euclidean', 0.067), ('ssm', 0.066), ('supplemental', 0.063), ('zg', 0.062), ('functionally', 0.062), ('correig', 0.061), ('inscribed', 0.061), ('janoos', 0.061), ('network', 0.058), ('scans', 0.056), ('spherical', 0.053), ('state', 0.05), ('zt', 0.05), ('approximation', 0.049), ('clustering', 0.048), ('spatial', 0.048), ('nodes', 0.048), ('connectivity', 0.047), ('embedding', 0.047), ('volumes', 0.046), ('orientation', 0.046), ('error', 0.046), ('ordering', 0.046), ('ag', 0.045), ('jul', 0.044), ('activation', 0.044), ('pca', 0.041), ('earth', 0.041), ('rees', 0.04), ('rocz', 0.04), ('scanner', 0.04), ('vnv', 0.04), ('wedges', 0.04), ('ztn', 0.04), ('mri', 0.039), ('correlates', 0.039), ('cluster', 0.039), ('motor', 0.039), ('ow', 0.037), ('corrected', 0.037), ('orientations', 0.036), ('physiologically', 0.036), ('quadrants', 0.036), ('unobservable', 0.036), ('wells', 0.036), ('subject', 0.035), ('neurosci', 0.034), ('varied', 0.034), ('dual', 0.034), ('amortized', 0.033), ('deteriorate', 0.033), ('hemodynamic', 0.033), ('kantorovich', 0.033), ('mover', 0.033), ('intrinsic', 0.033), ('regions', 0.032), ('imaging', 0.031), ('masked', 0.031), ('emd', 0.031), ('relaxation', 0.03), ('symmetrically', 0.029), ('approx', 0.029), ('visual', 0.029), ('mind', 0.029), ('recurrent', 0.028), ('mainly', 0.028), ('glm', 0.028), ('artifacts', 0.028), ('transformation', 0.028), ('eigenvalues', 0.028), ('preserve', 0.027), ('lp', 0.027), ('activated', 0.027), ('transport', 0.027), ('origin', 0.027), ('stimulus', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="157-tfidf-1" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>Author: Firdaus Janoos, Weichang Li, Niranjan Subrahmanya, Istvan Morocz, William Wells</p><p>Abstract: Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) deďŹ ning a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series. In this paper, we present a network-aware feature-space to represent the states of a general network, that enables comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efďŹ cient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efďŹ ciency of the approximation, especially for large problems. 1</p><p>2 0.14584194 <a title="157-tfidf-2" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>Author: Bo Liu, Sridhar Mahadevan, Ji Liu</p><p>Abstract: We present a novel l1 regularized off-policy convergent TD-learning method (termed RO-TD), which is able to learn sparse representations of value functions with low computational complexity. The algorithmic framework underlying ROTD integrates two key ideas: off-policy convergent gradient TD methods, such as TDC, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables ﬁrst-order solvers and feature selection using online convex regularization. A detailed theoretical and experimental analysis of RO-TD is presented. A variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the RO-TD algorithm. 1</p><p>3 0.12103223 <a title="157-tfidf-3" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>Author: Francisco Pereira, Matthew Botvinick</p><p>Abstract: This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure. 1</p><p>4 0.0851807 <a title="157-tfidf-4" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>Author: Jinfeng Yi, Rong Jin, Shaili Jain, Tianbao Yang, Anil K. Jain</p><p>Abstract: One of the main challenges in data clustering is to deﬁne an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by deﬁning the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called semi-crowdsourced clustering that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects and from the manual annotations of only a small portion of the data to be clustered. One difﬁculty in learning the pairwise similarity measure is that there is a signiﬁcant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difﬁculty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efﬁciency. 1</p><p>5 0.079552315 <a title="157-tfidf-5" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>6 0.076380171 <a title="157-tfidf-6" href="./nips-2012-Semi-supervised_Eigenvectors_for_Locally-biased_Learning.html">309 nips-2012-Semi-supervised Eigenvectors for Locally-biased Learning</a></p>
<p>7 0.074553862 <a title="157-tfidf-7" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>8 0.073517196 <a title="157-tfidf-8" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>9 0.069596745 <a title="157-tfidf-9" href="./nips-2012-A_Geometric_take_on_Metric_Learning.html">9 nips-2012-A Geometric take on Metric Learning</a></p>
<p>10 0.066061355 <a title="157-tfidf-10" href="./nips-2012-Parametric_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">265 nips-2012-Parametric Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>11 0.065849215 <a title="157-tfidf-11" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>12 0.06461636 <a title="157-tfidf-12" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>13 0.062837772 <a title="157-tfidf-13" href="./nips-2012-Learning_Probability_Measures_with_respect_to_Optimal_Transport_Metrics.html">184 nips-2012-Learning Probability Measures with respect to Optimal Transport Metrics</a></p>
<p>14 0.061824802 <a title="157-tfidf-14" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>15 0.06135264 <a title="157-tfidf-15" href="./nips-2012-Compressive_neural_representation_of_sparse%2C_high-dimensional_probabilities.html">79 nips-2012-Compressive neural representation of sparse, high-dimensional probabilities</a></p>
<p>16 0.059899766 <a title="157-tfidf-16" href="./nips-2012-Non-linear_Metric_Learning.html">242 nips-2012-Non-linear Metric Learning</a></p>
<p>17 0.059518151 <a title="157-tfidf-17" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>18 0.058519043 <a title="157-tfidf-18" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>19 0.057743173 <a title="157-tfidf-19" href="./nips-2012-Learning_visual_motion_in_recurrent_neural_networks.html">195 nips-2012-Learning visual motion in recurrent neural networks</a></p>
<p>20 0.057312842 <a title="157-tfidf-20" href="./nips-2012-Delay_Compensation_with_Dynamical_Synapses.html">94 nips-2012-Delay Compensation with Dynamical Synapses</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, 0.027), (2, -0.044), (3, -0.028), (4, 0.02), (5, 0.036), (6, -0.01), (7, -0.013), (8, -0.048), (9, 0.051), (10, 0.032), (11, -0.143), (12, 0.029), (13, 0.011), (14, 0.042), (15, 0.01), (16, -0.023), (17, 0.072), (18, 0.035), (19, 0.007), (20, 0.007), (21, -0.027), (22, -0.069), (23, -0.045), (24, 0.086), (25, -0.05), (26, -0.073), (27, 0.139), (28, 0.062), (29, 0.07), (30, 0.01), (31, 0.004), (32, -0.03), (33, 0.012), (34, 0.059), (35, -0.046), (36, 0.081), (37, -0.034), (38, -0.053), (39, 0.024), (40, -0.05), (41, 0.014), (42, 0.064), (43, -0.085), (44, 0.04), (45, 0.036), (46, 0.059), (47, 0.093), (48, -0.021), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92595422 <a title="157-lsi-1" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>Author: Firdaus Janoos, Weichang Li, Niranjan Subrahmanya, Istvan Morocz, William Wells</p><p>Abstract: Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) deďŹ ning a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series. In this paper, we present a network-aware feature-space to represent the states of a general network, that enables comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efďŹ cient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efďŹ ciency of the approximation, especially for large problems. 1</p><p>2 0.62163115 <a title="157-lsi-2" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>Author: Won H. Kim, Deepti Pachauri, Charles Hatt, Moo. K. Chung, Sterling Johnson, Vikas Singh</p><p>Abstract: Hypothesis testing on signals deﬁned on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in Neuroscience. The goal here is to identify regions that exhibit changes as a function of the clinical condition under study. As the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify. Indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive. In contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex. Our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-Euclidean settings (i.e., irregular weighted graphs). We provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results. Other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere. 1</p><p>3 0.55627567 <a title="157-lsi-3" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>Author: Jinfeng Yi, Rong Jin, Shaili Jain, Tianbao Yang, Anil K. Jain</p><p>Abstract: One of the main challenges in data clustering is to deﬁne an appropriate similarity measure between two objects. Crowdclustering addresses this challenge by deﬁning the pairwise similarity based on the manual annotations obtained through crowdsourcing. Despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available. To address this limitation, we propose a new approach for clustering, called semi-crowdsourced clustering that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing. The key idea is to learn an appropriate similarity measure, based on the low-level features of objects and from the manual annotations of only a small portion of the data to be clustered. One difﬁculty in learning the pairwise similarity measure is that there is a signiﬁcant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing. We address this difﬁculty by developing a metric learning algorithm based on the matrix completion method. Our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efﬁciency. 1</p><p>4 0.54436243 <a title="157-lsi-4" href="./nips-2012-Parametric_Local_Metric_Learning_for_Nearest_Neighbor_Classification.html">265 nips-2012-Parametric Local Metric Learning for Nearest Neighbor Classification</a></p>
<p>Author: Jun Wang, Alexandros Kalousis, Adam Woznica</p><p>Abstract: We study the problem of learning local metrics for nearest neighbor classiﬁcation. Most previous works on local metric learning learn a number of local unrelated metrics. While this ”independence” approach delivers an increased ﬂexibility its downside is the considerable risk of overﬁtting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics deﬁned on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several largescale classiﬁcation problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a signiﬁcant manner. 1</p><p>5 0.53215057 <a title="157-lsi-5" href="./nips-2012-Assessing_Blinding_in_Clinical_Trials.html">46 nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>Author: Ognjen Arandjelovic</p><p>Abstract: The interaction between the patient’s expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects. Thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e. to blind it. Yet, in practice perfect blinding is impossible to ensure or even verify. The current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial. If the estimated extent of blinding exceeds a threshold the trial is deemed sufﬁciently blinded; otherwise, the trial is deemed to have failed. In this paper we make several important contributions. Firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures. Secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials. We too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed. Unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants’ feedback. 1</p><p>6 0.50784713 <a title="157-lsi-6" href="./nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">132 nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<p>7 0.50657618 <a title="157-lsi-7" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>8 0.49724329 <a title="157-lsi-8" href="./nips-2012-Forging_The_Graphs%3A_A_Low_Rank_and_Positive_Semidefinite_Graph_Learning_Approach.html">135 nips-2012-Forging The Graphs: A Low Rank and Positive Semidefinite Graph Learning Approach</a></p>
<p>9 0.49549234 <a title="157-lsi-9" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>10 0.49180797 <a title="157-lsi-10" href="./nips-2012-Semi-supervised_Eigenvectors_for_Locally-biased_Learning.html">309 nips-2012-Semi-supervised Eigenvectors for Locally-biased Learning</a></p>
<p>11 0.49098566 <a title="157-lsi-11" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>12 0.48589313 <a title="157-lsi-12" href="./nips-2012-3D_Social_Saliency_from_Head-mounted_Cameras.html">2 nips-2012-3D Social Saliency from Head-mounted Cameras</a></p>
<p>13 0.47984272 <a title="157-lsi-13" href="./nips-2012-The_Perturbed_Variation.html">338 nips-2012-The Perturbed Variation</a></p>
<p>14 0.47917062 <a title="157-lsi-14" href="./nips-2012-Non-linear_Metric_Learning.html">242 nips-2012-Non-linear Metric Learning</a></p>
<p>15 0.47326663 <a title="157-lsi-15" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>16 0.47210416 <a title="157-lsi-16" href="./nips-2012-Learning_with_Partially_Absorbing_Random_Walks.html">196 nips-2012-Learning with Partially Absorbing Random Walks</a></p>
<p>17 0.46261835 <a title="157-lsi-17" href="./nips-2012-Delay_Compensation_with_Dynamical_Synapses.html">94 nips-2012-Delay Compensation with Dynamical Synapses</a></p>
<p>18 0.45631018 <a title="157-lsi-18" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>19 0.45201206 <a title="157-lsi-19" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>20 0.45154157 <a title="157-lsi-20" href="./nips-2012-On_Triangular_versus_Edge_Representations_---_Towards_Scalable_Modeling_of_Networks.html">253 nips-2012-On Triangular versus Edge Representations --- Towards Scalable Modeling of Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (11, 0.021), (21, 0.025), (38, 0.092), (39, 0.018), (42, 0.049), (54, 0.02), (55, 0.04), (61, 0.011), (74, 0.065), (76, 0.151), (80, 0.068), (92, 0.031), (96, 0.273)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80139548 <a title="157-lda-1" href="./nips-2012-Causal_discovery_with_scale-mixture_model_for_spatiotemporal_variance_dependencies.html">66 nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</a></p>
<p>Author: Zhitang Chen, Kun Zhang, Laiwan Chan</p><p>Abstract: In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves. However, in many real world problems, there are signiﬁcant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a speciﬁc type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR). We prove the identiﬁability of this model under the non-Gaussian assumption on the innovation processes. We also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthetic and real world data are conducted to show the applicability of the proposed model and algorithms.</p><p>same-paper 2 0.7953611 <a title="157-lda-2" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>Author: Firdaus Janoos, Weichang Li, Niranjan Subrahmanya, Istvan Morocz, William Wells</p><p>Abstract: Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) deďŹ ning a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series. In this paper, we present a network-aware feature-space to represent the states of a general network, that enables comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efďŹ cient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efďŹ ciency of the approximation, especially for large problems. 1</p><p>3 0.66528058 <a title="157-lda-3" href="./nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">206 nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>Author: Tony Jebara, Anna Choromanska</p><p>Abstract: The partition function plays a key role in probabilistic modeling including conditional random ﬁelds, graphical models, and maximum likelihood estimation. To optimize partition functions, this article introduces a quadratic variational upper bound. This inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems. Such bounds remain efﬁcient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings. For large-scale problems, low-rank versions of the bound are provided and outperform LBFGS as well as ﬁrst-order methods. Several learning applications are shown and reduce to fast and convergent update rules. Experimental results show advantages over state-of-the-art optimization methods. This supplement presents additional details in support of the full article. These include the application of the majorization method to maximum entropy problems. It also contains proofs of the various theorems, in particular, a guarantee that the bound majorizes the partition function. In addition, a proof is provided guaranteeing convergence on (non-latent) maximum conditional likelihood problems. The supplement also contains supporting lemmas that show the bound remains applicable in constrained optimization problems. The supplement then proves the soundness of the junction tree implementation of the bound for graphical models with large n. It also proves the soundness of the low-rank implementation of the bound for problems with large d. Finally, the supplement contains additional experiments and ﬁgures to provide further empirical support for the majorization methodology. Supplement for Section 2 Proof of Theorem 1 Rewrite the partition function as a sum over the integer index j = 1, . . . , n under the random ordering π : Ω → {1, . . . , n}. This deﬁnes j∑ π(y) and associates h and f with = n hj = h(π −1 (j)) and fj = f (π −1 (j)). Next, write Z(θ) = j=1 αj exp(λ⊤ fj ) by introducing ˜ ˜ λ = θ − θ and αj = hj exp(θ ⊤ fj ). Deﬁne the partition function over only the ﬁrst i components ∑i as Zi (θ) = j=1 αj exp(λ⊤ fj ). When i = 0, a trivial quadratic upper bound holds ( ) Z0 (θ) ≤ z0 exp 1 λ⊤ Σ0 λ + λ⊤ µ0 2 with the parameters z0 → 0+ , µ0 = 0, and Σ0 = z0 I. Next, add one term to the current partition function Z1 (θ) = Z0 (θ) + α1 exp(λ⊤ f1 ). Apply the current bound Z0 (θ) to obtain 1 Z1 (θ) ≤ z0 exp( 2 λ⊤ Σ0 λ + λ⊤ µ0 ) + α1 exp(λ⊤ f1 ). Consider the following change of variables u γ 1/2 −1/2 = Σ0 λ − Σ0 = α1 z0 exp( 1 (f1 2 (f1 − µ0 )) − µ0 )⊤ Σ−1 (f1 − µ0 )) 0 and rewrite the logarithm of the bound as log Z1 (θ) ( ) 1 ≤ log z0 − 1 (f1 − µ0 )⊤ Σ−1 (f1 − µ0 ) + λ⊤ f1 + log exp( 2 ∥u∥2 ) + γ . 0 2 Apply Lemma 1 (cf. [32] p. 100) to the last term to get ( (1 ) ) log Z1 (θ) ≤ log z0 − 1 (f1 − µ0 )⊤ Σ−1 (f1 − µ0 ) + λ⊤ f1 + log exp 2 ∥v∥2 +γ 0 2 ( ) v⊤ (u − v) 1 + + (u − v)⊤ I + Γvv⊤ (u − v) 1+γ exp(− 1 ∥v∥2 ) 2 2 10 where Γ = 1 1 tanh( 2 log(γ exp(− 2 ∥v∥2 ))) . 1 2 log(γ exp(− 2 ∥v∥2 )) The bound in [32] is tight when u = v. To achieve tightness −1/2 ˜ when θ = θ or, equivalently, λ = 0, we choose v = Σ0 (µ0 − f1 ) yielding ( ) Z1 (θ) ≤ z1 exp 1 λ⊤ Σ1 λ + λ⊤ µ1 2 where we have z1 = z0 + α1 z0 α1 = µ0 + f1 z0 + α1 z0 + α1 1 tanh( 2 log(α1 /z0 )) = Σ0 + (µ0 − f1 )(µ0 − f1 )⊤ . 2 log(α1 /z0 ) µ1 Σ1 This rule updates the bound parameters z0 , µ0 , Σ0 to incorporate an extra term in the sum over i in Z(θ). The process is iterated n times (replacing 1 with i and 0 with i − 1) to produce an overall bound on all terms. Lemma 1 (See [32] p. 100) ( ( ) ) For all u ∈ Rd , any v ∈ Rd and any γ ≥ 0, the bound log exp 1 ∥u∥2 + γ ≤ 2 ( ( ) ) log exp 1 ∥v∥2 + γ + 2 ( ) v⊤ (u − v) 1 + (u − v)⊤ I + Γvv⊤ (u − v) 1 1 + γ exp(− 2 ∥v∥2 ) 2 holds when the scalar term Γ = 1 tanh( 2 log(γ exp(−∥v∥2 /2))) . 2 log(γ exp(−∥v∥2 /2)) Equality is achieved when u = v. Proof of Lemma 1 The proof is provided in [32]. Supplement for Section 3 Maximum entropy problem We show here that partition functions arise naturally in maximum ∑ p(y) entropy estimation or minimum relative entropy RE(p∥h) = y p(y) log h(y) estimation. Consider the following problem: ∑ ∑ min RE(p∥h) s.t. p(y)f (y) = 0, p(y)g(y) ≥ 0. p y y d′ Here, assume that f : Ω → R and g : Ω → R are arbitrary (non-constant) vector-valued functions ( ) over the sample space. The solution distribution p(y) = h(y) exp θ ⊤ f (y) + ϑ⊤ g(y) /Z(θ, ϑ) is recovered by the dual optimization ∑ ( ) arg θ, ϑ = max − log h(y) exp θ ⊤ f (y) + ϑ⊤ g(y) d ϑ≥0,θ y ′ where θ ∈ Rd and ϑ ∈ Rd . These are obtained by minimizing Z(θ, ϑ) or equivalently by maximizing its negative logarithm. Algorithm 1 permits variational maximization of the dual via the quadratic program min 1 (β ϑ≥0,θ 2 ˜ ˜ − β)⊤ Σ(β − β) + β ⊤ µ ′ where β ⊤ = [θ ⊤ ϑ⊤ ]. Note that any general convex hull of constraints β ∈ Λ ⊆ Rd+d could be imposed without loss of generality. Proof of Theorem 2 We begin by proving a lemma that will be useful later. Lemma 2 If κΨ ≽ Φ ≻ 0 for Φ, Ψ ∈ Rd×d , then 1 ˜ ˜ ˜ L(θ) = − 2 (θ − θ)⊤ Φ(θ − θ) − (θ − θ)⊤ µ ˜ ˜ ˜ U (θ) = − 1 (θ − θ)⊤ Ψ(θ − θ) − (θ − θ)⊤ µ 2 satisfy supθ∈Λ L(θ) ≥ 1 κ ˜ supθ∈Λ U (θ) for any convex Λ ⊆ Rd , θ ∈ Λ, µ ∈ Rd and κ ∈ R+ . 11 Proof of Lemma 2 Deﬁne the primal problems of interest as PL = supθ∈Λ L(θ) and PU = supθ∈Λ U (θ). The constraints θ ∈ Λ can be summarized by a set of linear inequalities Aθ ≤ b where A ∈ Rk×d and b ∈ Rk for some (possibly inﬁnite) k ∈ Z. Apply the change of variables ˜ ˜ ˜ ˜ ˜ ˜ z = θ− θ. The constraint A(z+ θ) ≤ b simpliﬁes into Az ≤ b where b = b−Aθ. Since θ ∈ Λ, it 1 ⊤ ˜ ≥ 0. We obtain the equivalent primal problems PL = sup is easy to show that b ˜ − z Φz − Az≤b z⊤ µ and PU = supAz≤b − 1 z⊤ Ψz − z⊤ µ. The corresponding dual problems are ˜ 2 2 ⊤ −1 y⊤AΦ−1A⊤y ˜ µ Φ µ +y⊤AΦ−1µ+y⊤b+ y≥0 2 2 y⊤AΨ−1 A⊤y µ⊤Ψ−1µ ˜ DU = inf +y⊤AΨ−1µ+y⊤b+ . y≥0 2 2 DL = inf ˜ Due to strong duality, PL = DL and PU = DU . Apply the inequalities Φ ≼ κΨ and y⊤ b > 0 as ⊤ −1 y⊤AΨ−1 A⊤y y⊤AΨ−1 µ κ ˜ µΨ µ + + y⊤b + PL ≥ sup − z⊤ Ψz − z⊤ µ = inf y≥0 2 2κ κ 2κ ˜ Az≤b 1 1 ≥ DU = PU . κ κ 1 This proves that PL ≥ κ PU . We will use the above to prove Theorem 2. First, we will upper-bound (in the Loewner ordering sense) the matrices Σj in Algorithm 2. Since ∥fxj (y)∥2 ≤ r for all y ∈ Ωj and since µj in Algorithm 1 is a convex combination of fxj (y), the outer-product terms in the update for Σj satisfy (fxj (y) − µ)(fxj (y) − µ)⊤ ≼ 4r2 I. Thus, Σj ≼ F(α1 , . . . , αn )4r2 I holds where α 1 F(α1 , . . . , αn ) = i n ∑ tanh( 2 log( ∑i−1 αk )) k=1 αi 2 log( ∑i−1 α ) i=2 k k=1 using the deﬁnition of α1 , . . . , αn in the proof of Theorem 1. The formula for F starts at i = 2 since z0 → 0+ . Assume permutation π is sampled uniformly at random. The expected value of F is then α π(i) 1 n 1 ∑ ∑ tanh( 2 log( ∑i−1 απ(k) )) k=1 . Eπ [F(α1 , . . . , αn )] = απ(i) n! π i=2 ) 2 log( ∑i−1 k=1 απ(k) We claim that the expectation is maximized when all αi = 1 or any positive constant. Also, F is invariant under uniform scaling of its arguments. Write the expected value of F as E for short. ∂E ∂E ∂E Next, consider ∂αl at the setting αi = 1, ∀i. Due to the expectation over π, we have ∂αl = ∂αo for any l, o. Therefore, the gradient vector is constant when all αi = 1. Since F(α1 , . . . , αn ) is invariant to scaling, the gradient vector must therefore be the all zeros vector. Thus, the point ∂ ∂E when all αi = 1 is an extremum or a saddle. Next, consider ∂αo ∂αl for any l, o. At the setting 2 ∂ ∂E αi = 1, ∂ E = −c(n) and, ∂αo ∂αl = c(n)/(n − 1) for some non-negative constant function ∂α2 l c(n). Thus, the αi = 1 extremum is locally concave and is a maximum. This establishes that Eπ [F(α1 , . . . , αn )] ≤ Eπ [F(1, . . . , 1)] and yields the Loewner bound ) ( n−1 ∑ tanh(log(i)/2) 2 I = ωI. Σj ≼ 2r log(i) i=1 Apply this bound to each Σj in the lower bound on J(θ) and also note a corresponding upper bound ∑ ˜ ˜ ˜ J(θ) ≥ J(θ)−tω+tλ ∥θ − θ∥2− (θ − θ)⊤(µj −fxj (yj )) 2 j ∑ ˜ ˜ ˜ J(θ) ≤ J(θ)−tλ ∥θ − θ∥2− (θ − θ)⊤(µj −fxj (yj )) 2 j 12 ˜ which follows from Jensen’s inequality. Deﬁne the current θ at time τ as θτ and denote by Lτ (θ) the above lower bound and by Uτ (θ) the above upper bound at time τ . Clearly, Lτ (θ) ≤ J(θ) ≤ Uτ (θ) with equality when θ = θτ . Algorithm 2 maximizes J(θ) after initializing at θ0 and performing an update by maximizing a lower bound based on Σj . Since Lτ (θ) replaces the deﬁnition of Σj with ωI ≽ Σj , Lτ (θ) is a looser bound than the one used by Algorithm 2. Thus, performing θτ +1 = arg maxθ∈Λ Lτ (θ) makes less progress than a step of Algorithm 1. Consider computing the slower update at each iteration τ and returning θτ +1 = arg maxθ∈Λ Lτ (θ). Setting Φ = (tω +tλ)I, Ψ = tλI and κ = ω+λ allows us to apply Lemma 2 as follows λ sup Lτ (θ) − Lτ (θτ ) = θ∈Λ 1 sup Uτ (θ) − Uτ (θτ ). κ θ∈Λ Since Lτ (θτ ) = J(θτ ) = Uτ (θτ ), J(θτ +1 ) ≥ supθ∈Λ Lτ (θ) and supθ∈Λ Uτ (θ) ≥ J(θ ∗ ), we obtain ( ) 1 J(θτ +1 ) − J(θ ∗ ) ≥ 1− (J(θτ ) − J(θ ∗ )) . κ Iterate the above inequality starting at t = 0 to obtain ( )τ 1 ∗ J(θτ ) − J(θ ) ≥ 1− (J(θ0 ) − J(θ ∗ )) . κ ( ) 1 τ κ A solution within a multiplicative factor of ϵ implies that ϵ = 1 − κ or log(1/ϵ) = τ log κ−1 . ⌉ ⌈ Inserting the deﬁnition for κ shows that the number of iterations τ is at most log(1/ϵ) or κ log κ−1 ⌉ ⌈ log(1/ϵ) log(1+λ/ω) . Inserting the deﬁnition for ω gives the bound. Y12,0 Y11,1 Y21,1 Y31,1 ··· 1,1 Ym1,1 Figure 3: Junction tree of depth 2. Algorithm 5 SmallJunctionTree ˜ Input Parameters θ and h(u), f (u) ∀u ∈ Y12,0 and zi , Σi , µi ∀i = 1, . . . , m1,1 + Initialize z → 0 , µ = 0, Σ = zI For each conﬁguration u ∈ Y12,0 { ∏m1,1 ∑m1,1 ∏m1,1 ˜ ˜ ˜ α = h(u)( ∑ zi exp(−θ ⊤ µi )) exp(θ ⊤ (f (u) + i=1 µi )) = h(u) exp(θ ⊤ f (u)) i=1 zi i=1 m1,1 l = f (u) + i=1 µi − µ 1 ∑m1,1 tanh( 2 log(α/z)) ⊤ Σ + = i=1 Σi + ll 2 log(α/z) α µ + = z+α l z += α } Output z, µ, Σ Supplement for Section 5 Proof of correctness for Algorithm 3 Consider a simple junction tree of depth 2 shown on Figure 3. The notation Yca,b refers to the cth tree node located at tree level a (ﬁrst level is considered as the one with∑ leaves) whose parent is the bth from the higher tree level (the root has no parent so b = 0). tree ∑ Let Y a1 ,b1 refer to the sum over all conﬁgurations of variables in Yca1 ,b1 and Y a1 ,b1 \Y a2 ,b2 1 c1 c1 c2 refers to the sum over all conﬁgurations of variables that are in Yca1 ,b1 but not in Yca2 ,b2 . Let ma,b 1 2 denote the number of children of the bth node located at tree level a + 1. For short-hand, use ψ(Y ) = h(Y ) exp(θ ⊤ f (Y )). The partition function can be expressed as: 13 Y13,0 Y12,1 ··· Y11,1 Y21,1 ··· Y22,1 1,1 Ym1,1 ··· Y11,2 Y21,2 1,2 Ym1,2 2,1 Ym2,1 1,m2,1 Y1 ··· 1,m2,1 Y2 1,m 2,1 Ym1,m2,1 Figure 4: Junction tree of depth 3. Z(θ) = ∑ 2,0 u∈Y1 ≤ ∑ 2,0 u∈Y1 = ∑  ψ(u) ∏ m1,1 i=1 [ ∏ m1,1 ψ(u) i=1 [    ∑ ψ(v) 2,0 v∈Yi1,1 \Y1 ) 1( ˜ ˜ ˜ zi exp( θ − θ)⊤ Σi (θ − θ) + (θ − θ)⊤ µi 2 ∏ ( m1,1 ⊤ h(u) exp(θ f (u)) 2,0 u∈Y1 zi exp i=1 ] 1 ˜ ˜ ˜ (θ − θ)⊤ Σi (θ − θ) + (θ − θ)⊤ µi 2 )] ∑ where the upper-bound is obtained by applying Theorem 1 to each of the terms v∈Y 1,1 \Y 2,0 ψ(v). 1 i By simply rearranging terms we get: ) ( ( [ (m1,1 )) m1,1 ∏ ∑ ∑ ˜ zi exp(−θ ⊤ µi ) exp θ ⊤ f (u) + µi h(u) Z(θ) ≤ 2,0 u∈Y1 i=1 ( exp 1 ˜ (θ − θ)⊤ 2 (m1,1 ∑ ) Σi i=1 ˜ (θ − θ) )] . i=1 One ( can prove that this ) expression can be upper-bounded by 1 ˆ ⊤ Σ(θ − θ) + (θ − θ)⊤ µ where z, Σ and µ can be computed using Algoˆ ˆ z exp 2 (θ − θ) rithm 5 (a simpliﬁcation of Algorithm 3). We will call this result Lemma A. The proof is similar to the proof of Theorem 1 so is not repeated here. Consider enlarging the tree to a depth 3 as shown on Figure 4. The partition function is now      m2,1 m1,i ∑  ∏ ∑ ∏ ∑   Z(θ) = ψ(w) . ψ(u)  ψ(v)  3,0 u∈Y1 i=1 3,0 v∈Yi2,1 \Y1 j=1 w∈Yj1,i \Yi2,1 ( )) ∏m1,i (∑ ∑ term By Lemma A we can upper bound each v∈Y 2,1 \Y 3,0 ψ(v) j=1 w∈Yj1,i \Yi2,1 ψ(w) 1 i ( ) ˆ ˆ ˆ by the expression zi exp 1 (θ − θ)⊤ Σi (θ − θ) + (θ − θ)⊤ µi . This yields 2 [ )] ( m2,1 ∑ ∏ 1 ˜ ˜ ˜ Z(θ) ≤ ψ(u) (θ − θ)⊤ Σi (θ − θ) + (θ − θ)⊤ µi . zi exp 2 3,0 i=1 u∈Y1 2,1 2,1 2,1 This process can be viewed as collapsing the sub-trees S1 , S2 , . . ., Sm2,1 to super-nodes that are represented by bound parameters, zi , Σi and µi , i = {1, 2, · · · , m2,1 }, where the sub-trees are 14 deﬁned as: 2,1 S1 = 1,1 {Y12,1 , Y11,1 , Y21,1 , Y31,1 , . . . , Ym1,1 } 2,1 S2 = 1,2 {Y22,1 , Y11,2 , Y21,2 , Y31,2 , . . . , Ym1,2 } = 2,1 {Ym2,1 , Y1 . . . 2,1 Sm2,1 1,m2,1 1,m2,1 , Y2 1,m2,1 , Y3 1,m2,1 , . . . , Ym1,m2,1 }. Notice that the obtained expression can be further upper bounded using again Lemma A (induction) ( ) ˆ ˆ ˆ yielding a bound of the form: z exp 1 (θ − θ)⊤ Σ(θ − θ) + (θ − θ)⊤ µ . 2 Finally, for a general tree, follow the same steps described above, starting from leaves and collapsing nodes to super-nodes, each represented by bound parameters. This procedure effectively yields Algorithm 3 for the junction tree under consideration. Supplement for Section 6 Proof of correctness for Algorithm 4 We begin by proving a lemma that will be useful later. Lemma 3 For all x ∈ Rd and for all l ∈ Rd , 2  d d 2 ∑ ∑ l(i)  . x(i)2 l(i)2 ≥  x(i) √∑ d l(j)2 i=1 i=1 j=1 Proof of Lemma 3 By Jensen’s inequality, 2  ( d )2 d d ∑ x(i)l(i)2 ∑ ∑ x(i)l(i)2  . √∑ x(i)2 ∑d ⇐⇒ x(i)2 l(i)2 ≥  ≥ ∑d d l(j)2 l(j)2 l(j)2 j=1 j=1 i=1 i=1 i=1 i=1 d ∑ l(i)2 j=1 Now we prove the correctness of Algorithm 4. At the ith iteration, the algorithm stores Σi using ⊤ a low-rank representation Vi Si Vi + Di where Vi ∈ Rk×d is orthonormal, Si ∈ Rk×k positive d×d semi-deﬁnite and Di ∈ R is non-negative diagonal. The diagonal terms D are initialized to tλI where λ is the regularization term. To mimic Algorithm 1 we must increment the Σ matrix by a rank one update of the form Σi = Σi−1 + ri r⊤ . By projecting ri onto each eigenvector in V, we i ∑k ⊤ can decompose it as ri = j=1 Vi−1 (j, ·)ri Vi−1 (j, ·)⊤ + g = Vi−1 Vi−1 ri + g where g is the remaining residue. Thus the update rule can be rewritten as: Σi ⊤ ⊤ ⊤ = Σi−1 + ri r⊤ = Vi−1 Si−1 Vi−1 + Di−1 + (Vi−1 Vi−1 ri + g)(Vi−1 Vi−1 ri + g)⊤ i ′ ′ ′ ⊤ ⊤ ⊤ = Vi−1 (Si−1 + Vi−1 ri r⊤ Vi−1 )Vi−1 + Di−1 + gg⊤ = Vi−1 Si−1 Vi−1 + gg⊤ + Di−1 i ′ where we deﬁne Vi−1 = Qi−1 Vi−1 and deﬁned Qi−1 in terms of the singular value decomposition, ′ ′ ⊤ Q⊤ Si−1 Qi−1 = svd(Si−1 + Vi−1 ri r⊤ Vi−1 ). Note that Si−1 is diagonal and nonnegative by i−1 i construction. The current formula for Σi shows that we have a rank (k + 1) system (plus diagonal term) which needs to be converted back to a rank k system (plus diagonal term) which we denote by ′ Σi . We have two options as follows. Case 1) Remove g from Σi to obtain ′ ′ ′ ′ ⊤ Σi = Vi−1 Si−1 Vi−1 + Di−1 = Σi − gg⊤ = Σi − cvv⊤ 1 where c = ∥g∥2 and v = ∥g∥ g. th Case 2) Remove the m (smallest) eigenvalue in S′ and its corresponding eigenvector: i−1 ′ Σi ′ ′ ′ ′ ′ ′ ⊤ = Vi−1 Si−1 Vi−1 + Di−1 + gg⊤ − S (m, m)V (m, ·)⊤ V (m, ·) = Σi − cvv⊤ ′ ′ where c = S (m, m) and v = V(m, ·) . 15 ′ Clearly, both cases can be written as an update of the form Σi = Σi + cvv⊤ where c ≥ 0 and v⊤ v = 1. We choose the case with smaller c value to minimize the change as we drop from a system of order (k + 1) to order k. Discarding the smallest singular value and its corresponding eigenvector would violate the bound. Instead, consider absorbing this term into the diagonal component to ′′ ′ preserve the bound. Formally, we look for a diagonal matrix F such that Σi = Σi + F which also ′′ maintains x⊤ Σi x ≥ x⊤ Σi x for all x ∈ Rd . Thus, we want to satisfy: ( d )2 d ∑ ∑ ⊤ ′′ ⊤ ⊤ ⊤ ⊤ x Σi x ≥ x Σi x ⇐⇒ x cvv x ≤ x Fx ⇐⇒ c x(i)v(i) ≤ x(i)2 F(i) i=1 i=1 where, for ease of notation, we take F(i) = F(i, i). ′ where w = v⊤ 1. Consider the case where v ≥ 0 though we will soon get rid of (∑ )2 ∑d d this assumption. We need an F such that i=1 x(i)2 F(i) ≥ c i=1 x(i)v(i) . Equivalently, we (∑ ) ∑d ′ 2 ′ d need i=1 x(i)2 F(i) ≥ . Deﬁne F(i) = F(i) for all i = 1, . . . , d. So, we need 2 i=1 x(i)v(i) cw cw2 (∑ ) ∑d ′ ′ ′ 2 d an F such that i=1 x(i)2 F(i) ≥ . Using Lemma 3 it is easy to show that we i=1 x(i)v(i) Deﬁne v = 1 wv ′ ′ ′ may choose F (i) = v(i) . Thus, we obtain F(i) = cw2 F(i) = cwv(i). Therefore, for all x ∈ Rd , ∑d all v ≥ 0, and for F(i) = cv(i) j=1 v(j) we have d ∑ ( x(i) F(i) ≥ c 2 i=1 d ∑ )2 x(i)v(i) . (3) i=1 To generalize the inequality to hold for all vectors v ∈ Rd with potentially negative entries, it is ∑d sufﬁcient to set F(i) = c|v(i)| j=1 |v(j)|. To verify this, consider ﬂipping the sign of any v(i). The left side of the Inequality 3 does not change. For the right side of this inequality, ﬂipping the sign of v(i) is equivalent to ﬂipping the sign of x(i) and not changing the sign of v(i). However, in this case the inequality holds as shown before (it holds for any x ∈ Rd ). Thus for all x, v ∈ Rd and ∑d for F(i) = c|v(i)| j=1 |v(j)|, Inequality 3 holds. Supplement for Section 7 Small scale experiments In additional small-scale experiments, we compared Algorithm 2 with steepest descent (SD), conjugate gradient (CG), BFGS and Newton-Raphson. Small-scale problems may be interesting in real-time learning settings, for example, when a website has to learn from a user’s uploaded labeled data in a split second to perform real-time retrieval. We considered logistic regression on ﬁve UCI data sets where missing values were handled via mean-imputation. A range of regularization settings λ ∈ {100 , 102 , 104 } was explored and all algorithms were initialized from the same ten random start-points. Table 3 shows the average number of seconds each algorithm needed to achieve the same solution that BFGS converged to (all algorithms achieve the same solution due to concavity). The bound is the fastest algorithm as indicated in bold. data|λ BFGS SD CG Newton Bound a|100 1.90 1.74 0.78 0.31 0.01 a|102 0.89 0.92 0.83 0.25 0.01 a|104 2.45 1.60 0.85 0.22 0.01 b|100 3.14 2.18 0.70 0.43 0.07 b|102 2.00 6.17 0.67 0.37 0.04 b|104 1.60 5.83 0.83 0.35 0.04 c|100 4.09 1.92 0.65 0.39 0.07 c|102 1.03 0.64 0.64 0.34 0.02 c|104 1.90 0.56 0.72 0.32 0.02 d|100 5.62 12.04 1.36 0.92 0.16 d|102 2.88 1.27 1.21 0.63 0.09 d|104 3.28 1.94 1.23 0.60 0.07 e|100 2.63 2.68 0.48 0.35 0.03 e|102 2.01 2.49 0.55 0.26 0.03 e|104 1.49 1.54 0.43 0.20 0.03 Table 3: Convergence time in seconds under various regularization levels for a) Bupa (t = 345, dim = 7), b) Wine (t = 178, dim = 14), c) Heart (t = 187, dim = 23), d) Ion (t = 351, dim = 34), and e) Hepatitis (t = 155, dim = 20) data sets. Inﬂuence of rank k on bound performance in large scale experiments We also examined the inﬂuence of k on bound performance and compared it with LBFGS, SD and CG. Several choices 16 of k were explored. Table 4 shows results for the SRBCT data-set. In general, the bound performs best but slows down for superﬂuously large values of k. Steepest descent and conjugate gradient are slow yet obviously do not vary with k. Note that each iteration takes less time with smaller k for the bound. However, we are reporting overall runtime which is also a function of the number of iterations. Therefore, total runtime (a function of both) may not always decrease/increase with k. k LBFGS SD CG Bound 1 1.37 8.80 4.39 0.56 2 1.32 8.80 4.39 0.56 4 1.39 8.80 4.39 0.67 8 1.35 8.80 4.39 0.96 16 1.46 8.80 4.39 1.34 32 1.40 8.80 4.39 2.11 64 1.54 8.80 4.39 4.57 Table 4: Convergence time in seconds as a function of k. Additional latent-likelihood results For completeness, Figure 5 depicts two additional data-sets to complement Figure 2. Similarly, Table 5 shows all experimental settings explored in order to provide the summary Table 2 in the main article. bupa wine −19 0 −5 −log(J(θ)) −log(J(θ)) −20 −21 −22 Bound Newton BFGS Conjugate gradient Steepest descent −15 −23 −24 −5 −10 0 5 log(Time) [sec] 10 −20 −4 −2 0 2 4 log(Time) [sec] 6 8 Figure 5: Convergence of test latent log-likelihood for bupa and wine data-sets. Data-set Algorithm BFGS SD CG Newton Bound ion m=1 m=2m=3m=4 -4.96 -5.55 -5.88 -5.79 -11.80 -9.92 -5.56 -8.59 -5.47 -5.81 -5.57 -5.22 -5.95 -5.95 -5.95 -5.95 -6.08 -4.84 -4.18 -5.17 Data-set Algorithm BFGS SD CG Newton Bound bupa m=1 m=2 m=3 m=4 -22.07 -21.78 -21.92 -21.87 -21.76 -21.74 -21.73 -21.83 -21.81 -21.81 -21.81 -21.81 -21.85 -21.85 -21.85 -21.85 -21.85 -19.95 -20.01 -19.97 wine m=1m=2m=3m=4 -0.90 -0.91 -1.79 -1.35 -1.61 -1.60 -1.37 -1.63 -0.51 -0.78 -0.95 -0.51 -0.71 -0.71 -0.71 -0.71 -0.51 -0.51 -0.48 -0.51 hepatitis m=1m=2m=3m=4 -4.42 -5.28 -4.95 -4.93 -4.93 -5.14 -5.01 -5.20 -4.84 -4.84 -4.84 -4.84 -5.50 -5.50 -5.50 -4.50 -5.47 -4.40 -4.75 -4.92 SRBCT m=1m=2m=3m=4 -5.99 -6.17 -6.09 -6.06 -5.61 -5.62 -5.62 -5.61 -5.62 -5.49 -5.36 -5.76 -5.54 -5.54 -5.54 -5.54 -5.31 -5.31 -4.90 -0.11 Table 5: Test latent log-likelihood at convergence for different values of m ∈ {1, 2, 3, 4} on ion, bupa, hepatitis, wine and SRBCT data-sets. 17</p><p>4 0.64761007 <a title="157-lda-4" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>5 0.60555249 <a title="157-lda-5" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>6 0.60519391 <a title="157-lda-6" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>7 0.60258287 <a title="157-lda-7" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>8 0.60108906 <a title="157-lda-8" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>9 0.5955773 <a title="157-lda-9" href="./nips-2012-Repulsive_Mixtures.html">294 nips-2012-Repulsive Mixtures</a></p>
<p>10 0.59531164 <a title="157-lda-10" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>11 0.59483421 <a title="157-lda-11" href="./nips-2012-Truly_Nonparametric_Online_Variational_Inference_for_Hierarchical_Dirichlet_Processes.html">354 nips-2012-Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes</a></p>
<p>12 0.59464329 <a title="157-lda-12" href="./nips-2012-Clustering_Aggregation_as_Maximum-Weight_Independent_Set.html">68 nips-2012-Clustering Aggregation as Maximum-Weight Independent Set</a></p>
<p>13 0.59446186 <a title="157-lda-13" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>14 0.5943675 <a title="157-lda-14" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>15 0.59436238 <a title="157-lda-15" href="./nips-2012-Probabilistic_Low-Rank_Subspace_Clustering.html">277 nips-2012-Probabilistic Low-Rank Subspace Clustering</a></p>
<p>16 0.59410191 <a title="157-lda-16" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>17 0.59317291 <a title="157-lda-17" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>18 0.59300983 <a title="157-lda-18" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>19 0.59204328 <a title="157-lda-19" href="./nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">232 nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>20 0.59198958 <a title="157-lda-20" href="./nips-2012-Collaborative_Gaussian_Processes_for_Preference_Learning.html">74 nips-2012-Collaborative Gaussian Processes for Preference Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
