<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>295 nips-2012-Risk-Aversion in Multi-armed Bandits</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-295" href="#">nips2012-295</a> knowledge-graph by maker-knowledge-mining</p><h1>295 nips-2012-Risk-Aversion in Multi-armed Bandits</h1>
<br/><p>Source: <a title="nips-2012-295-pdf" href="http://papers.nips.cc/paper/4753-risk-aversion-in-multi-armed-bandits.pdf">pdf</a></p><p>Author: Amir Sani, Alessandro Lazaric, Rémi Munos</p><p>Abstract: Stochastic multi–armed bandits solve the Exploration–Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk–aversion where the objective is to compete against the arm with the best risk–return trade–off. This setting proves to be more difﬁcult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we deﬁne two algorithms, investigate their theoretical guarantees, and report preliminary empirical results. 1</p><p>Reference: <a title="nips-2012-295-reference" href="../nips2012_reference/nips-2012-Risk-Aversion_in_Multi-armed_Bandits_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arm', 0.746), ('regret', 0.408), ('bandit', 0.246), ('risk', 0.237), ('expexp', 0.197), ('pul', 0.17), ('reward', 0.136), ('mvi', 0.089), ('round', 0.078), ('avert', 0.063), ('mult', 0.053), ('trad', 0.052), ('worst', 0.048), ('return', 0.047), ('audibert', 0.037), ('mvn', 0.036), ('dilemm', 0.035), ('horizon', 0.032), ('suff', 0.031), ('suboptim', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="295-tfidf-1" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>2 0.66222733 <a title="295-tfidf-2" href="./nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>3 0.24881068 <a title="295-tfidf-3" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>4 0.21985923 <a title="295-tfidf-4" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>5 0.15282154 <a title="295-tfidf-5" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>6 0.13015111 <a title="295-tfidf-6" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>7 0.12738016 <a title="295-tfidf-7" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>8 0.12397028 <a title="295-tfidf-8" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>9 0.12012748 <a title="295-tfidf-9" href="./nips-2012-Active_Comparison_of_Prediction_Models.html">32 nips-2012-Active Comparison of Prediction Models</a></p>
<p>10 0.11746194 <a title="295-tfidf-10" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>11 0.11622493 <a title="295-tfidf-11" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>12 0.11512981 <a title="295-tfidf-12" href="./nips-2012-Distributed_Non-Stochastic_Experts.html">102 nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>13 0.10102668 <a title="295-tfidf-13" href="./nips-2012-On_Multilabel_Classification_and_Ranking_with_Partial_Feedback.html">252 nips-2012-On Multilabel Classification and Ranking with Partial Feedback</a></p>
<p>14 0.09721338 <a title="295-tfidf-14" href="./nips-2012-Online_allocation_and_homogeneous_partitioning_for_piecewise_constant_mean-approximation.html">261 nips-2012-Online allocation and homogeneous partitioning for piecewise constant mean-approximation</a></p>
<p>15 0.089761674 <a title="295-tfidf-15" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>16 0.08128684 <a title="295-tfidf-16" href="./nips-2012-Relax_and_Randomize_%3A_From_Value_to_Algorithms.html">293 nips-2012-Relax and Randomize : From Value to Algorithms</a></p>
<p>17 0.080679595 <a title="295-tfidf-17" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>18 0.078272671 <a title="295-tfidf-18" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>19 0.069519393 <a title="295-tfidf-19" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>20 0.06618616 <a title="295-tfidf-20" href="./nips-2012-Finite_Sample_Convergence_Rates_of_Zero-Order_Stochastic_Optimization_Methods.html">134 nips-2012-Finite Sample Convergence Rates of Zero-Order Stochastic Optimization Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.121), (1, 0.163), (2, -0.01), (3, -0.1), (4, -0.053), (5, 0.054), (6, -0.004), (7, -0.032), (8, -0.033), (9, -0.057), (10, -0.186), (11, 0.195), (12, 0.252), (13, 0.165), (14, -0.204), (15, -0.237), (16, 0.163), (17, 0.225), (18, -0.118), (19, 0.049), (20, 0.027), (21, -0.078), (22, -0.06), (23, -0.227), (24, -0.008), (25, -0.009), (26, -0.166), (27, -0.003), (28, 0.075), (29, 0.106), (30, 0.012), (31, 0.16), (32, -0.104), (33, 0.057), (34, 0.057), (35, -0.01), (36, -0.101), (37, 0.061), (38, -0.019), (39, -0.024), (40, 0.056), (41, 0.118), (42, -0.019), (43, 0.018), (44, -0.123), (45, -0.021), (46, -0.231), (47, -0.072), (48, 0.017), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97104812 <a title="295-lsi-1" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>2 0.93801713 <a title="295-lsi-2" href="./nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>3 0.53740269 <a title="295-lsi-3" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>4 0.48014233 <a title="295-lsi-4" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>5 0.45517361 <a title="295-lsi-5" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<p>6 0.41236675 <a title="295-lsi-6" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>7 0.39773995 <a title="295-lsi-7" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>8 0.36387357 <a title="295-lsi-8" href="./nips-2012-Online_allocation_and_homogeneous_partitioning_for_piecewise_constant_mean-approximation.html">261 nips-2012-Online allocation and homogeneous partitioning for piecewise constant mean-approximation</a></p>
<p>9 0.35599867 <a title="295-lsi-9" href="./nips-2012-Distributed_Non-Stochastic_Experts.html">102 nips-2012-Distributed Non-Stochastic Experts</a></p>
<p>10 0.35468981 <a title="295-lsi-10" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>11 0.31289256 <a title="295-lsi-11" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>12 0.27095613 <a title="295-lsi-12" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>13 0.25493091 <a title="295-lsi-13" href="./nips-2012-Active_Comparison_of_Prediction_Models.html">32 nips-2012-Active Comparison of Prediction Models</a></p>
<p>14 0.21373841 <a title="295-lsi-14" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>15 0.2102863 <a title="295-lsi-15" href="./nips-2012-Minimax_Multi-Task_Learning_and_a_Generalized_Loss-Compositional_Paradigm_for_MTL.html">212 nips-2012-Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL</a></p>
<p>16 0.20396093 <a title="295-lsi-16" href="./nips-2012-Nonparametric_Bayesian_Inverse_Reinforcement_Learning_for_Multiple_Reward_Functions.html">245 nips-2012-Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions</a></p>
<p>17 0.20244263 <a title="295-lsi-17" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>18 0.19961959 <a title="295-lsi-18" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>19 0.19834042 <a title="295-lsi-19" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>20 0.19596671 <a title="295-lsi-20" href="./nips-2012-Pointwise_Tracking_the_Optimal_Regression_Function.html">271 nips-2012-Pointwise Tracking the Optimal Regression Function</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(11, 0.109), (47, 0.082), (67, 0.023), (70, 0.351), (85, 0.055), (89, 0.106), (94, 0.078), (99, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93274724 <a title="295-lda-1" href="./nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>2 0.92195314 <a title="295-lda-2" href="./nips-2012-Mixability_in_Statistical_Learning.html">217 nips-2012-Mixability in Statistical Learning</a></p>
<p>3 0.92188305 <a title="295-lda-3" href="./nips-2012-GenDeR%3A_A_Generic_Diversified_Ranking_Algorithm.html">141 nips-2012-GenDeR: A Generic Diversified Ranking Algorithm</a></p>
<p>4 0.91959083 <a title="295-lda-4" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>5 0.91075671 <a title="295-lda-5" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>6 0.88979906 <a title="295-lda-6" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>same-paper 7 0.85868579 <a title="295-lda-7" href="./nips-2012-Risk-Aversion_in_Multi-armed_Bandits.html">295 nips-2012-Risk-Aversion in Multi-armed Bandits</a></p>
<p>8 0.84936553 <a title="295-lda-8" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>9 0.82051063 <a title="295-lda-9" href="./nips-2012-Fiedler_Random_Fields%3A_A_Large-Scale_Spectral_Approach_to_Statistical_Network_Modeling.html">132 nips-2012-Fiedler Random Fields: A Large-Scale Spectral Approach to Statistical Network Modeling</a></p>
<p>10 0.80425948 <a title="295-lda-10" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>11 0.78280717 <a title="295-lda-11" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<p>12 0.78139943 <a title="295-lda-12" href="./nips-2012-Robustness_and_risk-sensitivity_in_Markov_decision_processes.html">297 nips-2012-Robustness and risk-sensitivity in Markov decision processes</a></p>
<p>13 0.77974498 <a title="295-lda-13" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>14 0.77602828 <a title="295-lda-14" href="./nips-2012-No-Regret_Algorithms_for_Unconstrained_Online_Convex_Optimization.html">241 nips-2012-No-Regret Algorithms for Unconstrained Online Convex Optimization</a></p>
<p>15 0.77221543 <a title="295-lda-15" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>16 0.76930332 <a title="295-lda-16" href="./nips-2012-Putting_Bayes_to_sleep.html">283 nips-2012-Putting Bayes to sleep</a></p>
<p>17 0.76869434 <a title="295-lda-17" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>18 0.7652601 <a title="295-lda-18" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>19 0.76284969 <a title="295-lda-19" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>20 0.75585818 <a title="295-lda-20" href="./nips-2012-Efficient_Reinforcement_Learning_for_High_Dimensional_Linear_Quadratic_Systems.html">110 nips-2012-Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems</a></p>
<br/><br/><br/></body>
</html>
