<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2012-Learning with Recursive Perceptual Representations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-197" href="#">nips2012-197</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 nips-2012-Learning with Recursive Perceptual Representations</h1>
<br/><p>Source: <a title="nips-2012-197-pdf" href="http://papers.nips.cc/paper/4747-learning-with-recursive-perceptual-representations.pdf">pdf</a></p><p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>Reference: <a title="nips-2012-197-reference" href="../nips2012_reference/nips-2012-Learning_with_Recursive_Perceptual_Representations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. [sent-3, score-0.655]
</p><p>2 Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. [sent-4, score-0.376]
</p><p>3 Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. [sent-5, score-0.149]
</p><p>4 This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. [sent-6, score-0.14]
</p><p>5 The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. [sent-7, score-0.26]
</p><p>6 The Support Vector Machine (SVM) has been a popular method for multimodal classiﬁcation tasks since its introduction, and one of its main advantages is the simplicity of training a linear model. [sent-9, score-0.136]
</p><p>7 In addition, ﬁnding the “oracle” kernel for a speciﬁc task remains an open problem, especially in applications such as vision and speech. [sent-11, score-0.139]
</p><p>8 Our aim is to design a classiﬁer that combines the simplicity of the linear Support Vector Machine (SVM) with the power derived from deep architectures. [sent-12, score-0.239]
</p><p>9 the framework of building layer-by-layer architectures, and is motivated by the recent success of a convex stacking architecture which uses a simpliﬁed form of neural network with closed-form, convex learning [10]. [sent-15, score-0.429]
</p><p>10 Speciﬁcally, we propose a new stacking technique for building a deep architecture, using a linear SVM as the base building block, and a random projection as its core stacking element. [sent-16, score-0.959]
</p><p>11 The key element in our convex learning of each layer is to randomly project the predictions of the previous layer SVM back to the original feature space. [sent-18, score-0.547]
</p><p>12 As we will show in the paper, this could be seen as recursively transforming the original data manifold so that data from different classes are moved apart, leading to better linear separability in the subsequent layers. [sent-19, score-0.233]
</p><p>13 Starting from data manifolds that are not linearly separable, our method transforms the data manifolds in a stacked way to ﬁnd a linear separating hyperplane in the high layers, which corresponds to non-linear separating hyperplanes in the lower layers. [sent-22, score-0.538]
</p><p>14 Non-linear classiﬁcation is achieved without kernelization, using a recursive architecture. [sent-23, score-0.146]
</p><p>15 model does not require any complex learning techniques other than training linear SVMs, while canonical deep architectures usually require carefully designed pre-training and ﬁne-tuning steps, which often depend on speciﬁc applications. [sent-24, score-0.347]
</p><p>16 Using linear SVMs as building blocks our model scales in the same way as the linear SVM does, enabling fast computation during both training and testing time. [sent-25, score-0.316]
</p><p>17 From a kernel based perspective, our method could be viewed as a special non-linear SVM, with the beneﬁt that the non-linear kernel naturally emerges from the stacked structure instead of being deﬁned as in conventional algorithms. [sent-27, score-0.251]
</p><p>18 Our ﬁndings suggest that the proposed model, while keeping the simplicity and efﬁciency of training a linear SVM, can exploit non-linear dependencies with the proposed deep architecture, as suggested by the results on two well known vision and speech datasets. [sent-30, score-0.504]
</p><p>19 it exhibits better generalization gap), which is a desirable property inherited from the linear model used in the architecture presented in the paper. [sent-33, score-0.216]
</p><p>20 2  Previous Work  There has been a trend on object, acoustic and image classiﬁcation to move the complexity from the classiﬁer to the feature extraction step. [sent-34, score-0.164]
</p><p>21 In [4], the authors note that the choice of codebook does not seem to impact performance signiﬁcantly, and encoding via an inner product plus a non-linearity can effectively replace sparse coding, making testing signiﬁcantly simpler and faster. [sent-40, score-0.308]
</p><p>22 A disturbing issue with sparse coding + linear classiﬁcation is that with a limited codebook size, linear separability might be an overly strong statement, undermining the use of a single linear classiﬁer. [sent-41, score-0.654]
</p><p>23 This has been empirically veriﬁed: as we increase the codebook size, the performance keeps improving [4], indicating that such representations may not be able to fully exploit the complexity 2  of the data [2]. [sent-42, score-0.25]
</p><p>24 In fact, recent success on PASCAL VOC could partially be attributed to a huge codebook [25]. [sent-43, score-0.22]
</p><p>25 While this is theoretically valid, the practical advantage of linear models diminishes quickly, as the computation cost of feature generation, as well as training a high-dimensional classiﬁer (despite linear), can make it as expensive as classical non-linear classiﬁers. [sent-44, score-0.195]
</p><p>26 Despite this trend to rely on linear classiﬁers and overcomplete feature representations, sparse coding is still a ﬂat model, and efforts have been made to add ﬂexibility to the features. [sent-45, score-0.311]
</p><p>27 Our approach can be seen as an extension to sparse coding used in a stacked architecture. [sent-47, score-0.314]
</p><p>28 The method presented in this paper is a new stacking technique that has close connections to several stacking methods developed in the literature, which are brieﬂy surveyed in this section. [sent-49, score-0.556]
</p><p>29 In [23], the concept of stacking was proposed where simple modules of functions or classiﬁers are “stacked” on top of each other in order to learn complex functions or classiﬁers. [sent-50, score-0.326]
</p><p>30 Since then, various ways of implementing stacking operations have been developed, and they can be divided into two general categories. [sent-51, score-0.278]
</p><p>31 In the ﬁrst category, stacking is performed in a layer-by-layer fashion and typically involves no supervised information. [sent-52, score-0.278]
</p><p>32 This gives rise to multiple layers in unsupervised feature learning, as exempliﬁed in Deep Belief Networks [14, 13, 9], layered Convolutional Neural Networks [15], Deep Auto-encoder [14, 9], etc. [sent-53, score-0.228]
</p><p>33 Applications of such stacking methods includes object recognition [15, 26, 4], speech recognition [20], etc. [sent-54, score-0.517]
</p><p>34 In the second category of techniques, stacking is carried out using supervised information. [sent-55, score-0.312]
</p><p>35 The modules of the stacking architectures are typically simple classiﬁers. [sent-56, score-0.371]
</p><p>36 The new features for the stacked classiﬁer at a higher level of the hierarchy come from concatenation of the classiﬁer output of lower modules and the raw input features. [sent-57, score-0.378]
</p><p>37 Cohen and de Carvalho [5] developed a stacking architecture where the simple module is a Conditional Random Field. [sent-58, score-0.416]
</p><p>38 Another successful stacking architecture reported in [10, 11] uses supervised information for stacking where the basic module is a simpliﬁed form of multilayer perceptron where the output units are linear and the hidden units are sigmoidal nonlinear. [sent-59, score-0.977]
</p><p>39 The linearity in the output units permits highly efﬁcient, closed-form estimation (results of convex optimization) for the output network weights given the hidden units’ outputs. [sent-60, score-0.128]
</p><p>40 Stacked context has also been used in [3], where a set of classiﬁer scores are stacked to produce a more reliable detection. [sent-61, score-0.167]
</p><p>41 Our proposed method will build a stacked architecture where each layer is an SVM, which has proven to be a very successful classiﬁer for computer vision applications. [sent-62, score-0.546]
</p><p>42 Speciﬁcally, we consider a training set that contains N pairs of tuples (d(i) , y (i) ), where d(i) ∈ RD is the feature vector, and y (i) ∈ {1, . [sent-64, score-0.15]
</p><p>43 As depicted in Figure 2(b), the model is built by multiple layers of blocks, which we call Random SVMs, that each learns a linear SVM classiﬁer and transforms the data based on a random projection of previous layers SVM outputs. [sent-68, score-0.415]
</p><p>44 1  Recursive Transform of Input Features  Figure 2(b) visualizes one typical layer in the pipeline of our algorithm. [sent-73, score-0.304]
</p><p>45 Each layer takes the output of the previous layer, (starting from x1 = d for the ﬁrst layer as our initial input), and feeds it to a standard linear SVM that gives the output o1 . [sent-74, score-0.549]
</p><p>46 (a) The model is built with layers of Random SVM blocks, which are based on simple linear SVMs. [sent-78, score-0.204]
</p><p>47 (b) For each random SVM layer, we train a linear SVM using the transformed data manifold by combining the original features and random projections of previous layers’ predictions. [sent-80, score-0.197]
</p><p>48 The sigmoid function controls the scale of the resulting features, and at the same time prevents the random projection to be “too conﬁdent” on some data points, as the prediction of the lower-layer is still imperfect. [sent-85, score-0.188]
</p><p>49 ol = ec , where ec is the one-hot encoding representing class c), so the fact that they are approximately orthogonal means that (with high probability) they are pushing the per-class manifolds apart. [sent-89, score-0.178]
</p><p>50 Following [10], for each layer we use the outputs from all lower modules, instead of only the immediately lower module. [sent-92, score-0.203]
</p><p>51 A chief difference of our proposed method from previous approaches is that, instead of concatenating predictions with the raw input data to form the new expanded input data, we use the predictions to modify the features in the original space with a non-linear transformation. [sent-93, score-0.254]
</p><p>52 The following Lemma illustrates the fact that, if we are given an oracle prediction of the labels, it is possible to 4  add an offset to each class to “pull” the manifolds apart with this new architecture, and to guarantee an improvement on the training set if we assume perfect labels. [sent-98, score-0.182]
</p><p>53 1 would work for any monotonically decreasing loss function (in particular, for the hinge loss of SVM), and motivates our search for a transform of the original features to achieve linear separability, under the guidance of SVM predictions. [sent-114, score-0.138]
</p><p>54 1 (which degenerates due to imperfect predictions), or alternative stacking strategies such as concatenation as in [10]. [sent-121, score-0.349]
</p><p>55 In general, we aim to avoid supervision in the projection parameters, as trying to optimize the weights jointly would defeat the purpose of having a computationally efﬁcient method, and would, perhaps, increase training accuracy at the expense of over-ﬁtting. [sent-123, score-0.176]
</p><p>56 The risk of over-ﬁtting is also lower in this way, as we do not increase the dimensionality of the input space, and we do not learn the matrices Wl , which means we pass a weak signal from layer to layer. [sent-124, score-0.277]
</p><p>57 The ﬁrst layer of our approach is identical to the linear SVM, which is not able to separate the data well. [sent-130, score-0.276]
</p><p>58 However, when classiﬁers are recursively stacked in our approach, the classiﬁcation hyperplane is able to adapt to the nonlinear characteristics of the two classes. [sent-131, score-0.25]
</p><p>59 5  (a)  (b)  (c)  (d)  (e)  (f)  Figure 3: Classiﬁcation hyperplane from different stages of our algorithm: ﬁrst layer, second layer, and ﬁnal layer outputs. [sent-133, score-0.257]
</p><p>60 (b) Accuracy versus codebook size on CIFAR-10 for linear SVM, RBF SVM, and our proposed method. [sent-139, score-0.293]
</p><p>61 TIMIT is a speech database that contains two orders of magnitude more training samples than the other datasets, and the largest output label space. [sent-142, score-0.233]
</p><p>62 Recall that our method relies on two parameters: β, which is the factor that controls how much to shift the original feature space, and C, the regularization parameter of the linear SVM trained at 1 each layer. [sent-143, score-0.197]
</p><p>63 C controls the regularization of each layer, and is an important parameter – setting it too high will yield overﬁtting as the number of layers is increased. [sent-145, score-0.161]
</p><p>64 As a result, even if the training and testing sets are ﬁxed, randomness still exists in our algorithm. [sent-148, score-0.127]
</p><p>65 For this dataset, we follow the standard pipeline deﬁned in [4]: dense 6x6 local patches with ZCA whitening are extracted with stride 1, and thresholding coding with α = 0. [sent-153, score-0.187]
</p><p>66 As have been shown in Figure 4(b), the performance is almost monotonically increasing as we stack more layers in R2 SVM. [sent-158, score-0.131]
</p><p>67 Also, stacks of SVMs by concatenation of output and input feature space does not yield much gain above 1 layer (which is a linear SVM), and neither does a deterministic 6  Table 2: Results on CIFAR-10, with 25 training data per class. [sent-159, score-0.562]
</p><p>68 Table 1: Results on CIFAR-10, with different codebook sizes (hence feature dimensions). [sent-160, score-0.279]
</p><p>69 7%  version of recursive SVM where a projection matrix as in the proof for Lemma 3. [sent-183, score-0.226]
</p><p>70 Note that training each layer involves training a linear SVM, so the computational complexity is simply linear to the depth of our model. [sent-186, score-0.475]
</p><p>71 In contrast to this, the difﬁculty of training deep learning models based on many hidden layers may be signiﬁcantly harder, partially due to the lack of supervised information for its hidden layers. [sent-187, score-0.414]
</p><p>72 Figure 4(b) shows the effect that the feature dimensionality (controlled by the codebook size of OMP-1) has on the performance of the linear and non-linear classiﬁers, and Table 1 provides representative numerical results. [sent-188, score-0.399]
</p><p>73 In particular, when the codebook size is low, the assumption that we can approximate the non-linear function f as a globally linear classiﬁer fails, and in those cases the R2 SVM and RBF SVM clearly outperform the linear SVM. [sent-189, score-0.366]
</p><p>74 Moreover, as the codebook size grows, non-linear classiﬁers, represented by RBF SVM in our experiments, suffer from the curse of dimensionality partially due to the large dimensionality of the over-complete feature representation. [sent-190, score-0.411]
</p><p>75 For linear SVM, increasing the codebook size makes it perform better with respect to non-linear classiﬁers, but additional gains can still be consistently obtained by the Random Recursive SVM method. [sent-192, score-0.293]
</p><p>76 Also note how our model outperforms DCN, another stacking architecture proposed in [10]. [sent-193, score-0.387]
</p><p>77 Similar to the change of codebook sizes, it is interesting to experiment with the number of training examples per class. [sent-194, score-0.283]
</p><p>78 This again suggests that our proposed method may generalize better than RBF, which is a desirable property when the number of training examples is small with respect to the dimensionality of the feature space, which are cases of interest to many computer vision applications. [sent-196, score-0.236]
</p><p>79 In general, our method is able to combine the advantages of both linear and nonlinear SVM: it has higher representation power than linear SVM, providing consistent performance gains, and at the same time has a better robustness against overﬁtting. [sent-197, score-0.146]
</p><p>80 It is also worth pointing out again that R2 SVM is highly efﬁcient, since each layer is a simple linear SVM that can be carried out by simple matrix multiplication. [sent-198, score-0.31]
</p><p>81 TIMIT Finally, we report our experiments using the popular speech database TIMIT. [sent-200, score-0.135]
</p><p>82 The speech data is analyzed using a 25-ms Hamming window with a 10-ms ﬁxed frame rate. [sent-201, score-0.135]
</p><p>83 We represent the speech using ﬁrst- to 12th-order Mel frequency cepstral coefﬁcients (MFCCs) and energy, along with their ﬁrst and second temporal derivatives. [sent-202, score-0.135]
</p><p>84 In Table 3 we also report recent work on this dataset [10], which uses multi-layer perceptron with a hidden layer and linear output, and stacks each block on top of each other. [sent-224, score-0.373]
</p><p>85 In their experiments, the representation used from the speech signal is not sparse, and uses instead Restricted Boltzman Machine, which is more time consuming to learn. [sent-225, score-0.135]
</p><p>86 Second, it can offer a better generalization ability over nonlinear SVMs, especially when the ratio of dimensionality to the number of training data is large. [sent-231, score-0.174]
</p><p>87 These advantages, combined with the fact that R2 SVM is efﬁcient in both training and testing, suggests that it could be adopted as an improvement over the existing classiﬁcation pipeline in general. [sent-232, score-0.189]
</p><p>88 We also note that in the current work we have not employed techniques of ﬁne tuning similar to the one employed in the architecture of [10]. [sent-233, score-0.139]
</p><p>89 Fine tuning of the latter architecture has accounted for between 10% to 20% error reduction, and reduces the need for having large depth in order to achieve a ﬁxed level of recognition accuracy. [sent-234, score-0.176]
</p><p>90 We combined the simplicity of linear SVMs with the power derived from deep architectures, and proposed a new stacking technique for building a better classiﬁer, using linear SVM as the base building blocks and emplying a random non-linear projection to add ﬂexibility to the model. [sent-238, score-0.79]
</p><p>91 Our work is partially motivated by the recent trend of using coding techniques as feature representation with relatively large dictionaries. [sent-239, score-0.208]
</p><p>92 The chief advantage of our method lies in the fact that it learns non-linear classiﬁers without the need of kernel design, while keeping the efﬁciency of linear SVMs. [sent-240, score-0.156]
</p><p>93 Experimental results on vision and speech datasets showed that the method provides consistent improvement over linear baselines, even with no learning of the model parameters. [sent-241, score-0.302]
</p><p>94 The importance of encoding versus training with sparse coding and vector quantization. [sent-253, score-0.239]
</p><p>95 Binary coding of speech spectrograms using a deep auto-encoder. [sent-268, score-0.418]
</p><p>96 Deep convex network: A scalable architecture for deep learning. [sent-271, score-0.275]
</p><p>97 What is the best multi-stage architecture for object recognition? [sent-286, score-0.139]
</p><p>98 Investigation of full-sequence training of deep belief networks for speech recognition. [sent-304, score-0.393]
</p><p>99 Linear spatial pyramid matching using sparse coding for image classiﬁcation. [sent-316, score-0.174]
</p><p>100 Efﬁcient highly over-complete sparse coding using a mixture model. [sent-319, score-0.147]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svm', 0.548), ('stacking', 0.278), ('codebook', 0.22), ('layer', 0.203), ('dcn', 0.188), ('svms', 0.174), ('stacked', 0.167), ('deep', 0.166), ('recursive', 0.146), ('rbf', 0.144), ('speech', 0.135), ('wl', 0.131), ('layers', 0.131), ('coding', 0.117), ('classi', 0.117), ('architecture', 0.109), ('rsvm', 0.094), ('projection', 0.08), ('ft', 0.079), ('er', 0.078), ('ers', 0.074), ('linear', 0.073), ('interspeech', 0.072), ('concatenation', 0.071), ('pipeline', 0.07), ('separability', 0.068), ('vision', 0.067), ('manifolds', 0.065), ('training', 0.063), ('xl', 0.062), ('deng', 0.061), ('feature', 0.059), ('ol', 0.057), ('separating', 0.057), ('hyperplane', 0.054), ('berkeley', 0.054), ('sigmoid', 0.05), ('yu', 0.049), ('phone', 0.049), ('modules', 0.048), ('multilayer', 0.047), ('predictions', 0.047), ('dimensionality', 0.047), ('vinyals', 0.047), ('ot', 0.046), ('acoustic', 0.046), ('architectures', 0.045), ('mohamed', 0.044), ('building', 0.042), ('kernel', 0.042), ('chief', 0.041), ('faced', 0.04), ('perceptron', 0.039), ('layered', 0.038), ('suffer', 0.038), ('recognition', 0.037), ('codes', 0.036), ('blocks', 0.036), ('uc', 0.036), ('timit', 0.036), ('output', 0.035), ('randomness', 0.035), ('original', 0.035), ('carried', 0.034), ('wy', 0.034), ('speakers', 0.034), ('cvpr', 0.034), ('generalization', 0.034), ('accuracy', 0.033), ('philosophy', 0.033), ('trend', 0.032), ('stacks', 0.031), ('maji', 0.031), ('visualizes', 0.031), ('projections', 0.031), ('units', 0.031), ('object', 0.03), ('controls', 0.03), ('representations', 0.03), ('lemma', 0.03), ('rd', 0.03), ('tuning', 0.03), ('especially', 0.03), ('features', 0.03), ('sparse', 0.03), ('adopted', 0.029), ('encoding', 0.029), ('recursively', 0.029), ('module', 0.029), ('networks', 0.029), ('testing', 0.029), ('manifold', 0.028), ('tuples', 0.028), ('prevents', 0.028), ('improvement', 0.027), ('input', 0.027), ('cation', 0.027), ('image', 0.027), ('hidden', 0.027), ('class', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="197-tfidf-1" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>2 0.19106048 <a title="197-tfidf-2" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>3 0.17694177 <a title="197-tfidf-3" href="./nips-2012-Multilabel_Classification_using_Bayesian_Compressed_Sensing.html">228 nips-2012-Multilabel Classification using Bayesian Compressed Sensing</a></p>
<p>Author: Ashish Kapoor, Raajay Viswanathan, Prateek Jain</p><p>Abstract: In this paper, we present a Bayesian framework for multilabel classiďŹ cation using compressed sensing. The key idea in compressed sensing for multilabel classiďŹ cation is to ďŹ rst project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections. Our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks. We then derive an efďŹ cient variational inference scheme that provides joint posterior distribution over all the unobserved labels. The two key beneďŹ ts of the model are that a) it can naturally handle datasets that have missing labels and b) it can also measure uncertainty in prediction. The uncertainty estimate provided by the model allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task. Our experiments show signiďŹ cant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case. Finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model. 1</p><p>4 0.16320576 <a title="197-tfidf-4" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>5 0.15714325 <a title="197-tfidf-5" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>6 0.15132993 <a title="197-tfidf-6" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>7 0.1504382 <a title="197-tfidf-7" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>8 0.15035118 <a title="197-tfidf-8" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>9 0.14833777 <a title="197-tfidf-9" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>10 0.14593209 <a title="197-tfidf-10" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>11 0.14588545 <a title="197-tfidf-11" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>12 0.13790412 <a title="197-tfidf-12" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>13 0.12223029 <a title="197-tfidf-13" href="./nips-2012-Volume_Regularization_for_Binary_Classification.html">361 nips-2012-Volume Regularization for Binary Classification</a></p>
<p>14 0.11253599 <a title="197-tfidf-14" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>15 0.11253163 <a title="197-tfidf-15" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>16 0.10872429 <a title="197-tfidf-16" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>17 0.10809851 <a title="197-tfidf-17" href="./nips-2012-The_Lov%C3%A1sz_%CF%91_function%2C_SVMs_and_finding_large_dense_subgraphs.html">337 nips-2012-The Lovász ϑ function, SVMs and finding large dense subgraphs</a></p>
<p>18 0.10792045 <a title="197-tfidf-18" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>19 0.10792045 <a title="197-tfidf-19" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>20 0.10188857 <a title="197-tfidf-20" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.24), (1, 0.071), (2, -0.211), (3, -0.033), (4, 0.19), (5, -0.026), (6, 0.012), (7, 0.08), (8, -0.046), (9, -0.106), (10, 0.056), (11, 0.124), (12, 0.079), (13, 0.161), (14, -0.026), (15, -0.169), (16, -0.066), (17, 0.029), (18, 0.041), (19, -0.063), (20, -0.046), (21, -0.056), (22, -0.037), (23, -0.071), (24, -0.054), (25, -0.012), (26, -0.089), (27, -0.063), (28, -0.04), (29, -0.084), (30, -0.098), (31, 0.082), (32, 0.012), (33, 0.033), (34, 0.007), (35, -0.075), (36, 0.085), (37, 0.118), (38, 0.123), (39, 0.016), (40, 0.037), (41, 0.049), (42, -0.011), (43, 0.009), (44, -0.032), (45, -0.039), (46, -0.116), (47, -0.088), (48, -0.007), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96796584 <a title="197-lsi-1" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>2 0.65992451 <a title="197-lsi-2" href="./nips-2012-Large_Scale_Distributed_Deep_Networks.html">170 nips-2012-Large Scale Distributed Deep Networks</a></p>
<p>Author: Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, Andrew Y. Ng</p><p>Abstract: Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm. 1</p><p>3 0.64878637 <a title="197-lsi-3" href="./nips-2012-Visual_Recognition_using_Embedded_Feature_Selection_for_Curvature_Self-Similarity.html">360 nips-2012-Visual Recognition using Embedded Feature Selection for Curvature Self-Similarity</a></p>
<p>Author: Angela Eigenstetter, Bjorn Ommer</p><p>Abstract: Category-level object detection has a crucial need for informative object representations. This demand has led to feature descriptors of ever increasing dimensionality like co-occurrence statistics and self-similarity. In this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular approximation of objects using straight lines. However, like all descriptors using second order statistics, ours also exhibits a high dimensionality. Although improving discriminability, the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality. Given only a limited amount of training data, even sophisticated learning algorithms such as the popular kernel methods are not able to suppress noisy or superﬂuous dimensions of such high-dimensional data. Consequently, there is a natural need for feature selection when using present-day informative features and, particularly, curvature self-similarity. We therefore suggest an embedded feature selection method for SVMs that reduces complexity and improves generalization capability of object models. By successfully integrating the proposed curvature self-similarity representation together with the embedded feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach. 1</p><p>4 0.63616514 <a title="197-lsi-4" href="./nips-2012-Convolutional-Recursive_Deep_Learning_for_3D_Object_Classification.html">87 nips-2012-Convolutional-Recursive Deep Learning for 3D Object Classification</a></p>
<p>Author: Richard Socher, Brody Huval, Bharath Bath, Christopher D. Manning, Andrew Y. Ng</p><p>Abstract: Recent advances in 3D sensing technologies make it possible to easily record color and depth images which together can improve object recognition. Most current methods rely on very well-designed features for this new 3D modality. We introduce a model based on a combination of convolutional and recursive neural networks (CNN and RNN) for learning features and classifying RGB-D images. The CNN layer learns low-level translationally invariant features which are then given as inputs to multiple, ﬁxed-tree RNNs in order to compose higher order features. RNNs can be seen as combining convolution and pooling into one efﬁcient, hierarchical operation. Our main result is that even RNNs with random weights compose powerful features. Our model obtains state of the art performance on a standard RGB-D object dataset while being more accurate and faster during training and testing than comparable architectures such as two-layer CNNs. 1</p><p>5 0.63206464 <a title="197-lsi-5" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>Author: Pietro D. Lena, Ken Nagata, Pierre F. Baldi</p><p>Abstract: Residue-residue contact prediction is a fundamental problem in protein structure prediction. Hower, despite considerable research efforts, contact prediction methods are still largely unreliable. Here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules. For contact prediction, the idea is implemented as a three-dimensional stack of Neural Networks NNk , where i and j index the spatial coordinates of the contact ij map and k indexes “time”. The temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process, but rather a progressive reﬁnement. Networks at level k in the stack can be trained in supervised fashion to reﬁne the predictions produced by the previous level, hence addressing the problem of vanishing gradients, typical of deep architectures. Increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction. The deep approach leads to an accuracy for difﬁcult long-range contacts of about 30%, roughly 10% above the state-of-the-art. Many variations in the architectures and the training algorithms are possible, leaving room for further improvements. Furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components. 1</p><p>6 0.62952363 <a title="197-lsi-6" href="./nips-2012-Deep_Representations_and_Codes_for_Image_Auto-Annotation.html">92 nips-2012-Deep Representations and Codes for Image Auto-Annotation</a></p>
<p>7 0.62294662 <a title="197-lsi-7" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>8 0.62024742 <a title="197-lsi-8" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>9 0.61824 <a title="197-lsi-9" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>10 0.60834676 <a title="197-lsi-10" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>11 0.6039775 <a title="197-lsi-11" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>12 0.59289247 <a title="197-lsi-12" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>13 0.58778876 <a title="197-lsi-13" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>14 0.58729386 <a title="197-lsi-14" href="./nips-2012-Cocktail_Party_Processing_via_Structured_Prediction.html">72 nips-2012-Cocktail Party Processing via Structured Prediction</a></p>
<p>15 0.58534181 <a title="197-lsi-15" href="./nips-2012-Volume_Regularization_for_Binary_Classification.html">361 nips-2012-Volume Regularization for Binary Classification</a></p>
<p>16 0.57497931 <a title="197-lsi-16" href="./nips-2012-Image_Denoising_and_Inpainting_with_Deep_Neural_Networks.html">159 nips-2012-Image Denoising and Inpainting with Deep Neural Networks</a></p>
<p>17 0.5689975 <a title="197-lsi-17" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>18 0.56110173 <a title="197-lsi-18" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>19 0.55157757 <a title="197-lsi-19" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>20 0.54883945 <a title="197-lsi-20" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.065), (17, 0.011), (21, 0.017), (38, 0.104), (39, 0.023), (42, 0.041), (44, 0.015), (54, 0.035), (55, 0.062), (74, 0.07), (76, 0.118), (80, 0.165), (92, 0.083), (96, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90558052 <a title="197-lda-1" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>Author: Oriol Vinyals, Yangqing Jia, Li Deng, Trevor Darrell</p><p>Abstract: Linear Support Vector Machines (SVMs) have become very popular in vision as part of state-of-the-art object recognition and other classiﬁcation tasks but require high dimensional feature spaces for good performance. Deep learning methods can ﬁnd more compact representations but current methods employ multilayer perceptrons that require solving a difﬁcult, non-convex optimization problem. We propose a deep non-linear classiﬁer whose layers are SVMs and which incorporates random projection as its core stacking element. Our method learns layers of linear SVMs recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer. Our method scales as linear SVMs, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based SVMs. This is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications. The use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous –often more complicated– methods on several vision and speech benchmarks. 1</p><p>2 0.89633209 <a title="197-lda-2" href="./nips-2012-Causal_discovery_with_scale-mixture_model_for_spatiotemporal_variance_dependencies.html">66 nips-2012-Causal discovery with scale-mixture model for spatiotemporal variance dependencies</a></p>
<p>Author: Zhitang Chen, Kun Zhang, Laiwan Chan</p><p>Abstract: In conventional causal discovery, structural equation models (SEM) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves. However, in many real world problems, there are signiﬁcant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies. In this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a speciﬁc type of generating mechanism of the observations. In particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a Structural Vector AutoRegressive model (SVAR). We prove the identiﬁability of this model under the non-Gaussian assumption on the innovation processes. We also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure. Experiments on synthetic and real world data are conducted to show the applicability of the proposed model and algorithms.</p><p>3 0.87181747 <a title="197-lda-3" href="./nips-2012-Multimodal_Learning_with_Deep_Boltzmann_Machines.html">229 nips-2012-Multimodal Learning with Deep Boltzmann Machines</a></p>
<p>Author: Nitish Srivastava, Ruslan Salakhutdinov</p><p>Abstract: A Deep Boltzmann Machine is described for learning a generative model of data that consists of multiple and diverse input modalities. The model can be used to extract a uniﬁed representation that fuses modalities together. We ﬁnd that this representation is useful for classiﬁcation and information retrieval tasks. The model works by learning a probability density over the space of multimodal inputs. It uses states of latent variables as representations of the input. The model can extract this representation even when some modalities are absent by sampling from the conditional distribution over them and ﬁlling them in. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBM can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries. We further demonstrate that this model signiﬁcantly outperforms SVMs and LDA on discriminative tasks. Finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves noticeable gains. 1</p><p>4 0.86894548 <a title="197-lda-4" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>Author: Firdaus Janoos, Weichang Li, Niranjan Subrahmanya, Istvan Morocz, William Wells</p><p>Abstract: Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) deďŹ ning a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series. In this paper, we present a network-aware feature-space to represent the states of a general network, that enables comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efďŹ cient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efďŹ ciency of the approximation, especially for large problems. 1</p><p>5 0.85896409 <a title="197-lda-5" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>Author: Deepak Venugopal, Vibhav Gogate</p><p>Abstract: First-order probabilistic models combine the power of ﬁrst-order logic, the de facto tool for handling relational structure, with probabilistic graphical models, the de facto tool for handling uncertainty. Lifted probabilistic inference algorithms for them have been the subject of much recent research. The main idea in these algorithms is to improve the accuracy and scalability of existing graphical models’ inference algorithms by exploiting symmetry in the ﬁrst-order representation. In this paper, we consider blocked Gibbs sampling, an advanced MCMC scheme, and lift it to the ﬁrst-order level. We propose to achieve this by partitioning the ﬁrst-order atoms in the model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster. We propose an approach for constructing the clusters and show how it can be used to trade accuracy with computational complexity in a principled manner. Our experimental evaluation shows that lifted Gibbs sampling is superior to the propositional algorithm in terms of accuracy, scalability and convergence.</p><p>6 0.85821879 <a title="197-lda-6" href="./nips-2012-Kernel_Latent_SVM_for_Visual_Recognition.html">168 nips-2012-Kernel Latent SVM for Visual Recognition</a></p>
<p>7 0.85810089 <a title="197-lda-7" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>8 0.85684621 <a title="197-lda-8" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>9 0.85627216 <a title="197-lda-9" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>10 0.85494834 <a title="197-lda-10" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>11 0.85462606 <a title="197-lda-11" href="./nips-2012-Complex_Inference_in_Neural_Circuits_with_Probabilistic_Population_Codes_and_Topic_Models.html">77 nips-2012-Complex Inference in Neural Circuits with Probabilistic Population Codes and Topic Models</a></p>
<p>12 0.8540265 <a title="197-lda-12" href="./nips-2012-Discriminative_Learning_of_Sum-Product_Networks.html">100 nips-2012-Discriminative Learning of Sum-Product Networks</a></p>
<p>13 0.85339606 <a title="197-lda-13" href="./nips-2012-Expectation_Propagation_in_Gaussian_Process_Dynamical_Systems.html">121 nips-2012-Expectation Propagation in Gaussian Process Dynamical Systems</a></p>
<p>14 0.8520304 <a title="197-lda-14" href="./nips-2012-Mixing_Properties_of_Conditional_Markov_Chains_with_Unbounded_Feature_Functions.html">218 nips-2012-Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions</a></p>
<p>15 0.84676892 <a title="197-lda-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.84483105 <a title="197-lda-16" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>17 0.84101403 <a title="197-lda-17" href="./nips-2012-Latent_Coincidence_Analysis%3A_A_Hidden_Variable_Model_for_Distance_Metric_Learning.html">171 nips-2012-Latent Coincidence Analysis: A Hidden Variable Model for Distance Metric Learning</a></p>
<p>18 0.84050965 <a title="197-lda-18" href="./nips-2012-Multiresolution_analysis_on_the_symmetric_group.html">234 nips-2012-Multiresolution analysis on the symmetric group</a></p>
<p>19 0.84036922 <a title="197-lda-19" href="./nips-2012-Provable_ICA_with_Unknown_Gaussian_Noise%2C_with_Implications_for_Gaussian_Mixtures_and_Autoencoders.html">281 nips-2012-Provable ICA with Unknown Gaussian Noise, with Implications for Gaussian Mixtures and Autoencoders</a></p>
<p>20 0.8398332 <a title="197-lda-20" href="./nips-2012-Priors_for_Diversity_in_Generative_Latent_Variable_Models.html">274 nips-2012-Priors for Diversity in Generative Latent Variable Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
