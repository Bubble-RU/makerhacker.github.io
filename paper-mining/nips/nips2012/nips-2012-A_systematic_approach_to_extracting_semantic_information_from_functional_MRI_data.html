<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-28" href="#">nips2012-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</h1>
<br/><p>Source: <a title="nips-2012-28-pdf" href="http://papers.nips.cc/paper/4608-a-systematic-approach-to-extracting-semantic-information-from-functional-mri-data.pdf">pdf</a></p><p>Author: Francisco Pereira, Matthew Botvinick</p><p>Abstract: This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure. 1</p><p>Reference: <a title="nips-2012-28-reference" href="../nips2012_reference/nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. [sent-4, score-0.131]
</p><p>2 The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. [sent-5, score-0.763]
</p><p>3 1  Introduction  Functional Magnetic Resonance Imaging (fMRI) is a technique used in psychological experiments to measure the blood oxygenation level throughout the brain, which is a proxy for neural activity; this measurement is called brain activation. [sent-7, score-0.217]
</p><p>4 In a typical experiment, brain activation is measured during a task of interest, e. [sent-9, score-0.382]
</p><p>5 reading nonsense words, with the goal of identifying brain locations where the two differ. [sent-13, score-0.272]
</p><p>6 The most common analysis technique for doing this – statistical parametric mapping [4] – tests each voxel individually by regressing its time series on a predicted time series determined by the task contrast of interest. [sent-14, score-0.334]
</p><p>7 This ﬁt is scored and thresholded at a given statistical signiﬁcance level to yield a brain image with clusters of voxels that respond very differently to the two tasks (colloquially, these are the images that show parts of the brain that “light up”). [sent-15, score-0.993]
</p><p>8 The output of this process for a given experiment is a set of 3D coordinates of all the voxel clusters that appear reliably across all the subjects in a study. [sent-17, score-0.486]
</p><p>9 This result is easy to interpret, since there is a lot of information about what processes each brain area may be involved in. [sent-18, score-0.217]
</p><p>10 In recent years, there has been increasing awareness of the fact that there is information in the entire pattern of brain activation and not just in saliently active locations. [sent-20, score-0.478]
</p><p>11 Classiﬁers have been the tool 1  of choice for capturing this information and used to make predictions ranging from what stimulus a subject is seeing, what kind of object they are thinking about or what decision they will make [12] [14] [8]. [sent-21, score-0.103]
</p><p>12 The most common situation is to have an example correspond to the average brain image during one or a few performances of the task of interest, and voxels as the features, and we will discuss various issues with this scenario in mind. [sent-22, score-0.68]
</p><p>13 If only two conditions are being contrasted this is relatively straightforward as information is, at its simplest, a difference in activation of a voxel in the two conditions. [sent-24, score-0.519]
</p><p>14 Often, this means the best accuracy is obtained using few voxels, from all across the brain, and that different voxels will be chosen in different cross-validation folds; this presents a problem for interpretability of the locations in question. [sent-26, score-0.529]
</p><p>15 One approach to this problem is to try and regularize classiﬁers so that they include as many informative voxels as possible [2], thus identifying localizable clusters of voxels that may overlap across folds. [sent-27, score-0.956]
</p><p>16 A different approach is to cross-validate classiﬁers over small sections of the grid covering the brain, known as searchlights [10]. [sent-28, score-0.309]
</p><p>17 This can be used to produce a map of the cross-validated accuracy in the searchlight around each voxel, taking advantage of the pattern of activation across all the voxels contained in it. [sent-29, score-1.376]
</p><p>18 Such a map can then be thresholded to leave only locations where accuracy is signiﬁcantly above chance. [sent-30, score-0.102]
</p><p>19 Knowing the location of a voxel does not sufﬁce to interpret what it is doing, as it could be very different from stimulus to stimulus (rather than just active or not, as in the two condition situation). [sent-32, score-0.388]
</p><p>20 This method is partially based on the notion of pattern feature introduced in an earlier paper by us [15], but has been developed much further so as to dispense with most parameters and allow the creation of spatial maps usable for group or exploratory analyses, as will be discussed later. [sent-36, score-0.157]
</p><p>21 1  Data and Methods Data  The grid covering the brain contains on the order of tens of thousands voxels, measured over time as tasks are performed, every 1-2 seconds, yielding hundreds to thousands of 3D images per experiment. [sent-38, score-0.441]
</p><p>22 During an experiment a given task is performed a certain number of times – trials – and often the images collected during one trial are collapsed or averaged together, giving us one 3D image that can be clearly labeled with what happened in that trial, e. [sent-39, score-0.087]
</p><p>23 Although the grid covers the entire head, only a fraction of its voxels contain cortex in a typical subject; hence we only consider these voxels as features. [sent-42, score-0.882]
</p><p>24 1 Interpretation is more complicated if nonlinear classiﬁers are being used [6], [17], but this is far less common  2  A searchlight is a small section of the 3D grid, in our case a 27 = 3 × 3 × 3 voxel cube. [sent-43, score-0.917]
</p><p>25 Analyses using searchlights generally entail computing a statistic [10] or cross-validating a classiﬁer over the dataset containing just those voxels [16], and do so for the searchlight around each voxel in the brain, covering it in its entirety. [sent-44, score-1.635]
</p><p>26 The intuition for this is that individual voxels are very noisy features, and an effect observed across a group of voxels is more trustworthy. [sent-45, score-0.892]
</p><p>27 In the experiment performed to obtain our dataset 2 [13], subjects observed a word and a line drawing of an item, displayed on a screen for 3 seconds and followed by 8 seconds of a blank screen. [sent-46, score-0.128]
</p><p>28 The items named/depicted belonged to one of 12 categories: animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen, man-made objects, tools, vegetables and vehicles. [sent-47, score-0.134]
</p><p>29 There were 5 different exemplars of each of the 12 categories and 6 experimental epochs. [sent-49, score-0.156]
</p><p>30 During an experiment the task repeated a total of 360 times, and a 3D image of the fMRI-measured brain activation acquired every second. [sent-51, score-0.444]
</p><p>31 Each example for classiﬁcation purposes is the average image during a 4 second span while the subject was thinking about the item shown a few seconds earlier (a period which contains the peak of the signal during the trial; the dataset thus contains 360 examples, as many as there were trials. [sent-52, score-0.133]
</p><p>32 The voxel size was 3 × 3 × 5 mm, with the number of voxels being between 20000 and 21000 depending on which of the 9 subjects was considered. [sent-53, score-0.811]
</p><p>33 The features in each example are voxels, and the example labels are the category of the item being shown in the trial each example came from. [sent-54, score-0.165]
</p><p>34 1  2  for each classiﬁcation task, cross-validate a classiﬁer in all of the searchlights  searchlight: - a 3x3x3 voxel cube - one centered around each voxel in cortex - overlapping  test the result at each searchlight, which yields a binary signiﬁcance image  e. [sent-55, score-0.985]
</p><p>35 90  3  image as a vector of voxels  result signiﬁcant  this is done for all 66 pairwise classiﬁcation tasks  and adjacent searchlights supporting similar pairwise distinctions are clustered together using modularity  5  animals vs insects  . [sent-64, score-1.054]
</p><p>36 vehicles vehicles  the binary vector of signiﬁcance for each searchlight is rearranged into a binary confusion matrix  animals insects tools buildings clothing body parts furniture  4  animals insects tools buildings clothing body parts furniture  . [sent-82, score-1.831]
</p><p>37 searchlight  Figure 1: Construction of data-driven searchlights. [sent-85, score-0.583]
</p><p>38 2  Method  The goal of the experiment our dataset comes from is to understand how a certain semantic category is represented throughout the brain (e. [sent-87, score-0.299]
</p><p>39 Intuitively, there is information in a given location if at least two categories can be distinguished looking at their respective patterns of activation there; otherwise, the pattern of activation is noise or common to all categories. [sent-91, score-0.664]
</p><p>40 the construction of data-driven searchlights, parcels of the 3D grid where the same discriminations between pairs of categories can be made (these are generally larger than the 3 × 3 × 3 basic searchlight) 2. [sent-94, score-0.198]
</p><p>41 the synthesis of pattern features from each data-driven searchlight, corresponding to the presence or absence of a certain pattern of activation across it 3. [sent-95, score-0.432]
</p><p>42 the training and use of a classiﬁer based on pattern features and the generation of an anatomical map of the impact of each voxel on classiﬁcation and these are described in detail in each of the following sections. [sent-96, score-0.641]
</p><p>43 1  Construction of data-driven searchlights  Create pairwise searchlight maps In order to identify informative locations we start by considering whether a given pair of categories can be distinguished in each of the thousands of 3 × 3 × 3 searchlights covering the brain: 1. [sent-99, score-1.447]
</p><p>44 For each searchlight cross-validate a classiﬁer using the voxels belonging to it, obtaining an accuracy value which will be assigned to the voxel at the center of the searchlight, as shown in part 1 of Figure 1. [sent-100, score-1.409]
</p><p>45 The classiﬁer used in this case was Linear Discriminant Analysis (LDA, [7]), with a shrinkage estimator for the covariance matrix [18], as this was shown to be effective at both modeling the joint activation of voxels in a searchlight and classiﬁcation [16]. [sent-101, score-1.172]
</p><p>46 Transform the resulting brain image with the accuracy of each voxel into a p-value brain image (of obtaining accuracy as high or higher under the null hypothesis that the classes are not distinguishable, see [11]), as shown in part 1 of Figure 1. [sent-103, score-0.92]
</p><p>47 Threshold the p-value brain image using False Discovery Rate [5] (q = 0. [sent-105, score-0.256]
</p><p>48 01) to correct multiple for multiple comparisons and get a binary brain image with candidate locations where this pair of categories can be distinguished, as shown in part 2 of Figure 1. [sent-106, score-0.471]
</p><p>49 The outcome for each pair of categories is a binary signiﬁcance image, where a voxel is 1 if the categories can be distinguished in the searchlight surrounding it or 0 if not; this is shown for all pairs of categories in part 3 of Figure 1. [sent-107, score-1.496]
</p><p>50 This can also be viewed per-searchlight, yielding a binary vector encoding which category pairs can be distinguished and which can be rearranged into a binary matrix, as shown in part 4 of Figure 1. [sent-108, score-0.3]
</p><p>51 That said, if the same categories are distinguishable in two adjacent searchlights – which overlap – then it is reasonable to assume that all their voxels put together would still be able to make the same distinctions. [sent-110, score-0.849]
</p><p>52 At the same time we would like to constrain data-driven searchlights to the boundaries of known, large, anatomically determined regions of interest (ROI), both for computational efﬁciency and for interpretability, as will be described later. [sent-112, score-0.237]
</p><p>53 At the start of the aggregation process, each searchlight is by itself and has an associated binary information vector with 66 entries corresponding to which pairs of classes can be distinguished in its surrounding searchlight (part 3 of Figure 1). [sent-113, score-1.322]
</p><p>54 For each searchlight we compute the similarity of its information vector with those of all its neighbours, which yields a 3D grid similarity graph. [sent-114, score-0.617]
</p><p>55 We then take the portion of the graph corresponding to each ROI in the AAL brain atlas [19], and use modularity [1] to divide it into a number of clusters of adjacent searchlights supporting similar distinctions, as shown in panel 5 of Figure 1. [sent-115, score-0.534]
</p><p>56 After this is done for all ROIs we obtain a partition of the brain into a few hundred clusters, the data-driven searchlights. [sent-116, score-0.217]
</p><p>57 Figure 2 depicts the granularity of a typical clustering across multiple brain slices of one of the participants. [sent-117, score-0.327]
</p><p>58 The centroid for each cluster encodes the pairs of categories that can be distinguished in that datadriven searchlight. [sent-122, score-0.308]
</p><p>59 The centroid is obtained by combining the binary information vectors for each of the searchlights in it using a soft-AND function, and is itself a binary information vector. [sent-123, score-0.302]
</p><p>60 A given entry is 1 – the respective pair of categories is distinguishable – if it is 1 in at least q% of the cluster members (where q is the false discovery rate used earlier to threshold the binary image for that pair of categories). [sent-124, score-0.282]
</p><p>61 2  Generation of pattern features from each data-driven searchlight voxels  examples  1  clusters (across class pairs)  clusters (across all examples)  training data  singular vectors  pattern features  SVD  cluster 1  . [sent-127, score-1.419]
</p><p>62 3  animals vs insects animals vs tools vegetables vs vehicles  2 cluster 2  . [sent-130, score-0.713]
</p><p>63 cluster 3  body parts vs buildings  animals vs insects  . [sent-133, score-0.556]
</p><p>64 body parts vs buildings vegetables vs vehicles  Figure 3: Construction of pattern detectors and pattern features from data-driven searchlights. [sent-139, score-0.706]
</p><p>65 Construct two-way classiﬁers from each data-driven searchlight Each data-driven searchlight has a set of pairs of categories that can be distinguished in it. [sent-140, score-1.414]
</p><p>66 This indicates that there are particular patterns of activation across the voxels in it which are characteristic of one or more categories, and absent in others. [sent-141, score-0.654]
</p><p>67 We can leverage this to convert the pattern of activation across the brain into a series of sub-patterns, one from each data-driven searchlight. [sent-142, score-0.522]
</p><p>68 5  Use two-way classiﬁers to generate pattern features The set of pattern-detectors learned from each data-driven searchlight can be applied to any example, not just the ones from the categories that were used to learn them. [sent-144, score-0.843]
</p><p>69 For each data-driven searchlight, we apply all of its detectors to all the examples in the training set, over the voxels belonging to the searchlight, as illustrated in part 2 of Figure 3. [sent-146, score-0.524]
</p><p>70 The output of each detector across all examples becomes a new, synthetic pattern feature. [sent-147, score-0.166]
</p><p>71 The number of these pattern features varies per searchlight, as does the number of searchlights per subject, but at the end we will typically have between 10K and 20K of them. [sent-148, score-0.364]
</p><p>72 ones that captured a pattern present in all animate object categories versus one present in all inanimate object ones); these will be highly correlated and redundant. [sent-151, score-0.229]
</p><p>73 We address this by using Singular Value Decomposition (SVD, [7]) to reduce the dimensionality of the matrix of pattern features to the same as the number of examples (180), keeping all singular vectors; this is shown in part 3 of Figure 3. [sent-152, score-0.206]
</p><p>74 Given the low-dimensional pattern feature dataset, we train a one-versus-rest classiﬁer (a linear SVM with λ = 1, [3]) for each category; these are then applied to each example in the test set, with the label prediction corresponding to the class with the highest class probability. [sent-157, score-0.096]
</p><p>75 The classiﬁers can also be used to determine the extent to which each data-driven searchlight was responsible for correctly predicting each class. [sent-158, score-0.583]
</p><p>76 A one-versus-rest category classiﬁer consists of a vector of 180 weights, which can be converted into an equivalent classiﬁer over pattern features by inverting the SVD, as shown in part 1 of Figure 4. [sent-159, score-0.21]
</p><p>77 The impact of each pattern feature in correctly predicting this category can be calculated by multiplying each weight by the values taken by the corresponding pattern feature over examples in the category, and averaging across all examples; this is shown in part 2 of Figure 4. [sent-160, score-0.474]
</p><p>78 These pattern-feature impact values can then be aggregated by the data-driven searchlight they came from, yielding a net impact value for that searchlight. [sent-161, score-0.894]
</p><p>79 This is the value that is propagated to each voxel in the data-driven searchlight (part 3 of Figure 4) in order to generate an impact map. [sent-162, score-1.046]
</p><p>80 1  Experiments and Discussion Classiﬁcation  Our goal in this experiment is to determine whether transforming the data from voxel features to pattern features preserves information, and how competitive the results are with a classiﬁer combined with voxel selection. [sent-164, score-0.849]
</p><p>81 If cross-validation inside a split-half training set is required, we use leave-one-epoch out cross-validation, Baseline We contrasted experimental results obtained with our method with a baseline of classiﬁcation using voxel selection. [sent-166, score-0.354]
</p><p>82 The scoring criterion used to rank each voxel was the accuracy of a LDA classiﬁer – same as described above – using the 3 × 3 × 3 searchlight around each voxel to do 12-category classiﬁcation. [sent-167, score-1.295]
</p><p>83 The number of voxels to use was selected by nested cross-validation inside the training set 3 . [sent-168, score-0.424]
</p><p>84 The classiﬁer used was a linear SVM (λ = 1, [3]), same as the whole brain classiﬁer in our method. [sent-169, score-0.217]
</p><p>85 Results The results are shown in the ﬁrst line of Table 1; across subjects, our method is better than voxel selection, with the p-value of a sign-test of this being < 0. [sent-170, score-0.378]
</p><p>86 It is substantially better than a classiﬁer using all the voxels in the brain directly. [sent-172, score-0.641]
</p><p>87 The ﬁrst is that some classes give rise to very similar patterns of activation (e. [sent-176, score-0.186]
</p><p>88 The second factor is that subjects vary in their ability to stay focused on the task and avoid stray thoughts or remembering other parts of the experiment, hence examples may not belong to the class corresponding to the label or even any class at all. [sent-179, score-0.122]
</p><p>89 2  Impact maps tool  building  Figure 5: Average example for categories “tool” and “building” in participant P1 (slices ordered from inferior to superior, red is activation above the image mean, blue below). [sent-210, score-0.497]
</p><p>90 3, an impact map can be produced for each category, showing the extent to which each data-driven searchlight helped classify that category correctly. [sent-213, score-0.791]
</p><p>91 Figure 5 shows the average example for the two categories; note how similar the two examples are across the slices, indicating that most activation is shared between the two categories. [sent-215, score-0.235]
</p><p>92 The impact maps for the same participant in Figure 6 show that much of the common activation is eliminated, and that the areas known to be informative are assigned high impact in their respective 3  Possible choices were 50, 100, 200, 400, 800, 1200, 1600, 2000, 4000, 8000, 16000 or all voxels. [sent-216, score-0.546]
</p><p>93 7  tool  building  Figure 6: Impact map for categories “tool” and “building” in participant P1. [sent-217, score-0.279]
</p><p>94 tool  building  Figure 7: Average impact map for categories “tool” and “building” across the nine participants. [sent-218, score-0.395]
</p><p>95 Impact is positive, regardless of whether activation in each voxel involved is above or below the mean of the image; the activation of each voxel inﬂuences the classiﬁer only in the context of its neighbours in each data-driven searchlight. [sent-220, score-1.022]
</p><p>96 Finally, consider that impact maps can be averaged across subjects, as shown in Figure 7, or undergo t-tests or a more complex second-level group analysis. [sent-222, score-0.207]
</p><p>97 Thresholding of statistical maps in functional neuroimaging using the false discovery rate. [sent-241, score-0.111]
</p><p>98 A neurosemantic theory of concrete noun representation based on the underlying brain codes. [sent-255, score-0.217]
</p><p>99 Predicting human brain activity associated with the meanings of nouns. [sent-290, score-0.239]
</p><p>100 Classiﬁcation of functional magnetic resonance imaging data using informative pattern features. [sent-304, score-0.226]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('searchlight', 0.583), ('voxels', 0.424), ('voxel', 0.334), ('searchlights', 0.237), ('brain', 0.217), ('activation', 0.165), ('categories', 0.133), ('impact', 0.129), ('insects', 0.103), ('buildings', 0.103), ('pattern', 0.096), ('animals', 0.085), ('er', 0.084), ('distinguished', 0.084), ('classi', 0.079), ('vs', 0.072), ('slices', 0.066), ('vegetables', 0.063), ('tools', 0.062), ('vehicles', 0.06), ('category', 0.059), ('participant', 0.057), ('neuroimage', 0.056), ('subjects', 0.053), ('clothing', 0.047), ('furniture', 0.047), ('distinctions', 0.046), ('ers', 0.045), ('across', 0.044), ('parts', 0.043), ('vj', 0.042), ('svd', 0.041), ('cluster', 0.039), ('body', 0.039), ('image', 0.039), ('covering', 0.038), ('tool', 0.037), ('functional', 0.037), ('mitchell', 0.036), ('locations', 0.036), ('grid', 0.034), ('maps', 0.034), ('thousands', 0.033), ('tens', 0.033), ('building', 0.032), ('clusters', 0.032), ('informative', 0.032), ('resonance', 0.032), ('fmri', 0.032), ('pereira', 0.032), ('yielding', 0.032), ('pairs', 0.031), ('vi', 0.031), ('features', 0.031), ('distinguishable', 0.031), ('anatomical', 0.031), ('detectors', 0.031), ('item', 0.029), ('magnetic', 0.029), ('singular', 0.029), ('marcel', 0.028), ('stimulus', 0.027), ('exploratory', 0.027), ('examples', 0.026), ('seconds', 0.026), ('mismatches', 0.026), ('rearranged', 0.026), ('roi', 0.026), ('cance', 0.025), ('trial', 0.025), ('princeton', 0.025), ('accuracy', 0.025), ('modularity', 0.024), ('neighbours', 0.024), ('xor', 0.024), ('adjacent', 0.024), ('part', 0.024), ('experiment', 0.023), ('exemplars', 0.023), ('neuroimaging', 0.022), ('activity', 0.022), ('binary', 0.022), ('thresholded', 0.021), ('came', 0.021), ('centroid', 0.021), ('invert', 0.021), ('patterns', 0.021), ('hundreds', 0.021), ('mri', 0.02), ('subject', 0.02), ('contrasted', 0.02), ('analyses', 0.02), ('map', 0.02), ('belonging', 0.019), ('around', 0.019), ('surrounding', 0.019), ('reading', 0.019), ('thinking', 0.019), ('discovery', 0.018), ('locate', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="28-tfidf-1" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>Author: Francisco Pereira, Matthew Botvinick</p><p>Abstract: This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure. 1</p><p>2 0.12800375 <a title="28-tfidf-2" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>Author: Alexander Lorbert, Peter J. Ramadge</p><p>Abstract: We offer a regularized, kernel extension of the multi-set, orthogonal Procrustes problem, or hyperalignment. Our new method, called Kernel Hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features. With direct application to fMRI data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large ROIs, including the entire cortex. We report experiments using real-world, multi-subject fMRI data. 1</p><p>3 0.12103223 <a title="28-tfidf-3" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>Author: Firdaus Janoos, Weichang Li, Niranjan Subrahmanya, Istvan Morocz, William Wells</p><p>Abstract: Identifying patterns from the neuroimaging recordings of brain activity related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem. The main challenges, however, for such an analysis of fMRI data are: a) deďŹ ning a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fMRI time-series. In this paper, we present a network-aware feature-space to represent the states of a general network, that enables comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efďŹ cient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts. This feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting â&euro;&oelig;massâ&euro;? over the network to transform one function into another. Through theoretical and empirical assessments, we demonstrate the accuracy and efďŹ ciency of the approximation, especially for large problems. 1</p><p>4 0.085656866 <a title="28-tfidf-4" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>Author: Joseph Wang, Venkatesh Saligrama</p><p>Abstract: We develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-speciﬁc classiﬁers. We formulate an empirical risk minimization problem that incorporates both partitioning and classiﬁcation in to a single global objective. We show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach. Nevertheless, we consider locally linear schemes by learning linear partitions and linear region classiﬁers. Locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-ﬁtting and generalization error. We train locally linear classiﬁers by using LDA, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions. We present experimental results demonstrating improved performance over state of the art classiﬁcation techniques on benchmark datasets. We also show improved robustness to label noise.</p><p>5 0.060272109 <a title="28-tfidf-5" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>Author: Chris Hinrichs, Vikas Singh, Jiming Peng, Sterling Johnson</p><p>Abstract: Multiple Kernel Learning (MKL) generalizes SVMs to the setting where one simultaneously trains a linear classiﬁer and chooses an optimal combination of given base kernels. Model complexity is typically controlled using various norm regularizations on the base kernel mixing coefﬁcients. Existing methods neither regularize nor exploit potentially useful information pertaining to how kernels in the input set ‘interact’; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?). We show that by substituting the norm penalty with an arbitrary quadratic function Q 0, one can impose a desired covariance structure on mixing weights, and use this as an inductive bias when learning the concept. This formulation signiﬁcantly generalizes the widely used 1- and 2-norm MKL objectives. We explore the model’s utility via experiments on a challenging Neuroimaging problem, where the goal is to predict a subject’s conversion to Alzheimer’s Disease (AD) by exploiting aggregate information from many distinct imaging modalities. Here, our new model outperforms the state of the art (p-values 10−3 ). We brieﬂy discuss ramiﬁcations in terms of learning bounds (Rademacher complexity). 1</p><p>6 0.058434103 <a title="28-tfidf-6" href="./nips-2012-Learning_about_Canonical_Views_from_Internet_Image_Collections.html">185 nips-2012-Learning about Canonical Views from Internet Image Collections</a></p>
<p>7 0.05711551 <a title="28-tfidf-7" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>8 0.053354345 <a title="28-tfidf-8" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>9 0.051142182 <a title="28-tfidf-9" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>10 0.049882758 <a title="28-tfidf-10" href="./nips-2012-Forward-Backward_Activation_Algorithm_for_Hierarchical_Hidden_Markov_Models.html">136 nips-2012-Forward-Backward Activation Algorithm for Hierarchical Hidden Markov Models</a></p>
<p>11 0.047445778 <a title="28-tfidf-11" href="./nips-2012-Learning_with_Recursive_Perceptual_Representations.html">197 nips-2012-Learning with Recursive Perceptual Representations</a></p>
<p>12 0.047220964 <a title="28-tfidf-12" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>13 0.046927068 <a title="28-tfidf-13" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>14 0.046521846 <a title="28-tfidf-14" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>15 0.04559882 <a title="28-tfidf-15" href="./nips-2012-FastEx%3A_Hash_Clustering_with_Exponential_Families.html">126 nips-2012-FastEx: Hash Clustering with Exponential Families</a></p>
<p>16 0.044623572 <a title="28-tfidf-16" href="./nips-2012-Burn-in%2C_bias%2C_and_the_rationality_of_anchoring.html">62 nips-2012-Burn-in, bias, and the rationality of anchoring</a></p>
<p>17 0.044623572 <a title="28-tfidf-17" href="./nips-2012-Emergence_of_Object-Selective_Features_in_Unsupervised_Feature_Learning.html">116 nips-2012-Emergence of Object-Selective Features in Unsupervised Feature Learning</a></p>
<p>18 0.044179391 <a title="28-tfidf-18" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>19 0.043065213 <a title="28-tfidf-19" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>20 0.042770423 <a title="28-tfidf-20" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.117), (1, 0.031), (2, -0.088), (3, -0.016), (4, 0.037), (5, -0.019), (6, -0.0), (7, 0.038), (8, -0.021), (9, 0.005), (10, -0.001), (11, -0.023), (12, 0.03), (13, -0.013), (14, 0.046), (15, -0.014), (16, -0.045), (17, 0.06), (18, -0.019), (19, -0.024), (20, -0.011), (21, 0.019), (22, -0.084), (23, -0.048), (24, 0.037), (25, -0.085), (26, -0.04), (27, 0.042), (28, 0.011), (29, 0.022), (30, -0.027), (31, 0.062), (32, 0.018), (33, -0.015), (34, 0.064), (35, -0.04), (36, -0.016), (37, 0.003), (38, -0.097), (39, -0.033), (40, -0.012), (41, -0.082), (42, 0.032), (43, 0.033), (44, 0.026), (45, -0.056), (46, -0.009), (47, 0.052), (48, 0.028), (49, -0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90076101 <a title="28-lsi-1" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>Author: Francisco Pereira, Matthew Botvinick</p><p>Abstract: This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure. 1</p><p>2 0.74444419 <a title="28-lsi-2" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>Author: Uri Maoz, Shengxuan Ye, Ian Ross, Adam Mamelak, Christof Koch</p><p>Abstract: The ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientiﬁc study of decision-making, agency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious, voluntary action as well as for brain-machine interfaces. Here, epilepsy patients, implanted with intracranial depth microelectrodes or subdural grid electrodes for clinical purposes, participated in a “matching-pennies” game against an opponent. In each trial, subjects were given a 5 s countdown, after which they had to raise their left or right hand immediately as the “go” signal appeared on a computer screen. They won a ﬁxed amount of money if they raised a different hand than their opponent and lost that amount otherwise. The question we here studied was the extent to which neural precursors of the subjects’ decisions can be detected in intracranial local ﬁeld potentials (LFP) prior to the onset of the action. We found that combined low-frequency (0.1–5 Hz) LFP signals from 10 electrodes were predictive of the intended left-/right-hand movements before the onset of the go signal. Our ORT system predicted which hand the patient would raise 0.5 s before the go signal with 68±3% accuracy in two patients. Based on these results, we constructed an ORT system that tracked up to 30 electrodes simultaneously, and tested it on retrospective data from 7 patients. On average, we could predict the correct hand choice in 83% of the trials, which rose to 92% if we let the system drop 3/10 of the trials on which it was less conﬁdent. Our system demonstrates— for the ﬁrst time—the feasibility of accurately predicting a binary action on single trials in real time for patients with intracranial recordings, well before the action occurs. 1 1</p><p>3 0.68890178 <a title="28-lsi-3" href="./nips-2012-Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button.html">50 nips-2012-Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button</a></p>
<p>Author: Joan Fruitet, Alexandra Carpentier, Maureen Clerc, Rémi Munos</p><p>Abstract: Brain-computer interfaces (BCI) allow users to “communicate” with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand, to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue. This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm, UCB-classif , based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefﬁcient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efﬁcient use of the BCI training session. Comparing the proposed method to the standard practice in task selection, for a ﬁxed time budget, UCB-classif leads to an improved classiﬁcation rate, and for a ﬁxed classiﬁcation rate, to a reduction of the time spent in training by 50%. 1</p><p>4 0.61740005 <a title="28-lsi-4" href="./nips-2012-A_P300_BCI_for_the_Masses%3A_Prior_Information_Enables_Instant_Unsupervised_Spelling.html">14 nips-2012-A P300 BCI for the Masses: Prior Information Enables Instant Unsupervised Spelling</a></p>
<p>Author: Pieter-jan Kindermans, Hannes Verschore, David Verstraeten, Benjamin Schrauwen</p><p>Abstract: The usability of Brain Computer Interfaces (BCI) based on the P300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus. In this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models). We show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session. 1</p><p>5 0.57735169 <a title="28-lsi-5" href="./nips-2012-Wavelet_based_multi-scale_shape_features_on_arbitrary_surfaces_for_cortical_thickness_discrimination.html">363 nips-2012-Wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</a></p>
<p>Author: Won H. Kim, Deepti Pachauri, Charles Hatt, Moo. K. Chung, Sterling Johnson, Vikas Singh</p><p>Abstract: Hypothesis testing on signals deﬁned on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in Neuroscience. The goal here is to identify regions that exhibit changes as a function of the clinical condition under study. As the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify. Indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive. In contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex. Our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-Euclidean settings (i.e., irregular weighted graphs). We provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results. Other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere. 1</p><p>6 0.56808096 <a title="28-lsi-6" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>7 0.5607776 <a title="28-lsi-7" href="./nips-2012-Identification_of_Recurrent_Patterns_in_the_Activation_of_Brain_Networks.html">157 nips-2012-Identification of Recurrent Patterns in the Activation of Brain Networks</a></p>
<p>8 0.53387392 <a title="28-lsi-8" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>9 0.53124768 <a title="28-lsi-9" href="./nips-2012-Assessing_Blinding_in_Clinical_Trials.html">46 nips-2012-Assessing Blinding in Clinical Trials</a></p>
<p>10 0.52200586 <a title="28-lsi-10" href="./nips-2012-On_the_connections_between_saliency_and_tracking.html">256 nips-2012-On the connections between saliency and tracking</a></p>
<p>11 0.49039647 <a title="28-lsi-11" href="./nips-2012-Graphical_Gaussian_Vector_for_Image_Categorization.html">146 nips-2012-Graphical Gaussian Vector for Image Categorization</a></p>
<p>12 0.48622921 <a title="28-lsi-12" href="./nips-2012-Recognizing_Activities_by_Attribute_Dynamics.html">289 nips-2012-Recognizing Activities by Attribute Dynamics</a></p>
<p>13 0.47769925 <a title="28-lsi-13" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>14 0.45733765 <a title="28-lsi-14" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>15 0.4461849 <a title="28-lsi-15" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>16 0.43660456 <a title="28-lsi-16" href="./nips-2012-From_Deformations_to_Parts%3A_Motion-based_Segmentation_of_3D_Objects.html">137 nips-2012-From Deformations to Parts: Motion-based Segmentation of 3D Objects</a></p>
<p>17 0.43596283 <a title="28-lsi-17" href="./nips-2012-High-Order_Multi-Task_Feature_Learning_to_Identify_Longitudinal_Phenotypic_Markers_for_Alzheimer%27s_Disease_Progression_Prediction.html">151 nips-2012-High-Order Multi-Task Feature Learning to Identify Longitudinal Phenotypic Markers for Alzheimer's Disease Progression Prediction</a></p>
<p>18 0.43412921 <a title="28-lsi-18" href="./nips-2012-Feature-aware_Label_Space_Dimension_Reduction_for_Multi-label_Classification.html">130 nips-2012-Feature-aware Label Space Dimension Reduction for Multi-label Classification</a></p>
<p>19 0.43350995 <a title="28-lsi-19" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>20 0.43339652 <a title="28-lsi-20" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (11, 0.01), (21, 0.021), (38, 0.073), (42, 0.022), (54, 0.017), (55, 0.035), (74, 0.036), (76, 0.573), (77, 0.014), (80, 0.049), (92, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99343228 <a title="28-lda-1" href="./nips-2012-Learning_High-Density_Regions_for_a_Generalized_Kolmogorov-Smirnov_Test_in_High-Dimensional_Data.html">175 nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</a></p>
<p>Author: Assaf Glazer, Michael Lindenbaum, Shaul Markovitch</p><p>Abstract: We propose an efﬁcient, generalized, nonparametric, statistical KolmogorovSmirnov test for detecting distributional change in high-dimensional data. To implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested. Our work is motivated by the need to detect changes in data streams, and the test is especially efﬁcient in this context. We provide the theoretical foundations of our test and show its superiority over existing methods. 1</p><p>2 0.9827475 <a title="28-lda-2" href="./nips-2012-Active_Learning_of_Model_Evidence_Using_Bayesian_Quadrature.html">33 nips-2012-Active Learning of Model Evidence Using Bayesian Quadrature</a></p>
<p>Author: Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K. Duvenaud, Stephen J. Roberts, Carl E. Rasmussen</p><p>Abstract: Numerical integration is a key component of many problems in scientiﬁc computing, statistical modelling, and machine learning. Bayesian Quadrature is a modelbased method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efﬁciency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model’s hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientiﬁc problem from astronomy. 1</p><p>3 0.98180956 <a title="28-lda-3" href="./nips-2012-Shifting_Weights%3A_Adapting_Object_Detectors_from_Image_to_Video.html">311 nips-2012-Shifting Weights: Adapting Object Detectors from Image to Video</a></p>
<p>Author: Kevin Tang, Vignesh Ramanathan, Li Fei-fei, Daphne Koller</p><p>Abstract: Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest ﬁrst. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features speciﬁc to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video [2] datasets that illustrate the beneﬁt of our approach to adapt object detectors to video. 1</p><p>same-paper 4 0.98163003 <a title="28-lda-4" href="./nips-2012-A_systematic_approach_to_extracting_semantic_information_from_functional_MRI_data.html">28 nips-2012-A systematic approach to extracting semantic information from functional MRI data</a></p>
<p>Author: Francisco Pereira, Matthew Botvinick</p><p>Abstract: This paper introduces a novel classiﬁcation method for functional magnetic resonance imaging datasets with tens of classes. The method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions. We provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure. 1</p><p>5 0.98049676 <a title="28-lda-5" href="./nips-2012-Random_Utility_Theory_for_Social_Choice.html">286 nips-2012-Random Utility Theory for Social Choice</a></p>
<p>Author: Hossein Azari, David Parks, Lirong Xia</p><p>Abstract: Random utility theory models an agent’s preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received signiﬁcant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce. 1</p><p>6 0.97428858 <a title="28-lda-6" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>7 0.9741469 <a title="28-lda-7" href="./nips-2012-Label_Ranking_with_Partial_Abstention_based_on_Thresholded_Probabilistic_Models.html">169 nips-2012-Label Ranking with Partial Abstention based on Thresholded Probabilistic Models</a></p>
<p>8 0.94630629 <a title="28-lda-8" href="./nips-2012-Iterative_Thresholding_Algorithm_for_Sparse_Inverse_Covariance_Estimation.html">164 nips-2012-Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation</a></p>
<p>9 0.941248 <a title="28-lda-9" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>10 0.94079894 <a title="28-lda-10" href="./nips-2012-Semi-Crowdsourced_Clustering%3A_Generalizing_Crowd_Labeling_by_Robust_Distance_Metric_Learning.html">307 nips-2012-Semi-Crowdsourced Clustering: Generalizing Crowd Labeling by Robust Distance Metric Learning</a></p>
<p>11 0.88596159 <a title="28-lda-11" href="./nips-2012-The_Perturbed_Variation.html">338 nips-2012-The Perturbed Variation</a></p>
<p>12 0.87380713 <a title="28-lda-12" href="./nips-2012-Sparse_Approximate_Manifolds_for_Differential_Geometric_MCMC.html">318 nips-2012-Sparse Approximate Manifolds for Differential Geometric MCMC</a></p>
<p>13 0.8713395 <a title="28-lda-13" href="./nips-2012-Generalization_Bounds_for_Domain_Adaptation.html">142 nips-2012-Generalization Bounds for Domain Adaptation</a></p>
<p>14 0.86447299 <a title="28-lda-14" href="./nips-2012-Dip-means%3A_an_incremental_clustering_method_for_estimating_the_number_of_clusters.html">99 nips-2012-Dip-means: an incremental clustering method for estimating the number of clusters</a></p>
<p>15 0.86428291 <a title="28-lda-15" href="./nips-2012-Structured_Learning_of_Gaussian_Graphical_Models.html">327 nips-2012-Structured Learning of Gaussian Graphical Models</a></p>
<p>16 0.8633064 <a title="28-lda-16" href="./nips-2012-Ancestor_Sampling_for_Particle_Gibbs.html">41 nips-2012-Ancestor Sampling for Particle Gibbs</a></p>
<p>17 0.86023271 <a title="28-lda-17" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>18 0.85660201 <a title="28-lda-18" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>19 0.85557318 <a title="28-lda-19" href="./nips-2012-Reducing_statistical_time-series_problems_to_binary_classification.html">291 nips-2012-Reducing statistical time-series problems to binary classification</a></p>
<p>20 0.85457921 <a title="28-lda-20" href="./nips-2012-Locating_Changes_in_Highly_Dependent_Data_with_Unknown_Number_of_Change_Points.html">203 nips-2012-Locating Changes in Highly Dependent Data with Unknown Number of Change Points</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
