<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>267 nips-2012-Perceptron Learning of SAT</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-267" href="#">nips2012-267</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>267 nips-2012-Perceptron Learning of SAT</h1>
<br/><p>Source: <a title="nips-2012-267-pdf" href="http://papers.nips.cc/paper/4533-perceptron-learning-of-sat.pdf">pdf</a></p><p>Author: Alex Flint, Matthew Blaschko</p><p>Abstract: Boolean satisﬁability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will ﬁnd an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware veriﬁcation task. 1</p><p>Reference: <a title="nips-2012-267-reference" href="../nips2012_reference/nips-2012-Perceptron_Learning_of_SAT_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. [sent-8, score-0.166]
</p><p>2 In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. [sent-11, score-0.371]
</p><p>3 Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. [sent-12, score-0.417]
</p><p>4 We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. [sent-14, score-0.172]
</p><p>5 In this work, we explore the application of a perceptron inspired learning algorithm applied to branching heuristics in the Davis-Putnam-Logemann-Loveland algorithm [8, 7]. [sent-21, score-0.252]
</p><p>6 The Davis-Putnam-Logemann-Loveland (DPLL) algorithm formulates SAT as a search problem, resulting in a valuation of variables that satisﬁes the sentence, or a tree resolution refutation proof indicating that the sentence is not satisﬁable. [sent-22, score-0.404]
</p><p>7 The branching rule in this depth-ﬁrst search procedure is a key determinant of the efﬁciency of the algorithm, and numerous heuristics have been proposed in the SAT literature [15, 16, 26, 18, 13]. [sent-23, score-0.278]
</p><p>8 Inspired by the recent framing of learning as search optimization [6], we explore here the application of a perceptron inspired learning rule to application speciﬁc samples of the SAT problem. [sent-24, score-0.172]
</p><p>9 Ruml applied reinforcement learning to ﬁnd valuations of satisﬁable sentences [25]. [sent-27, score-0.218]
</p><p>10 A similar approach to learning heuristics for search was explored in [12]. [sent-34, score-0.154]
</p><p>11 2  Theorem Proving as a Search Problem  The SAT problem [5] is to determine whether a sentence Ω in propositional logic is satisﬁable. [sent-35, score-0.304]
</p><p>12 A literal p is a proposition of the form q (a “positive literal”) or ¬q (a “negative literal”). [sent-38, score-0.337]
</p><p>13 A clause ωk is a disjunction of nk literals, p1 ∨ p2 ∨ · · · ∨ pnk . [sent-39, score-0.215]
</p><p>14 A sentence Ω in conjunctive normal form (CNF) [15] is a conjunction of m clauses, ω1 ∧ω2 ∧· · ·∧ωm . [sent-41, score-0.175]
</p><p>15 A sentence Ω is satisﬁable iff there exists a valuation under which Ω is true. [sent-44, score-0.3]
</p><p>16 All sentences in propositional logic can be transformed to CNF [15]. [sent-46, score-0.295]
</p><p>17 [7] proposed a simple procedure for recognising satisﬁabile CNF sentences on N variables. [sent-49, score-0.192]
</p><p>18 Their algorithm is essentially a depth ﬁrst search over all possible 2N valuations over the input sentence, with specialized criteria to prune the search and transformation rules to simplify the sentence. [sent-50, score-0.2]
</p><p>19 PickBranch applies a heuristic to choose a literal in Ω. [sent-53, score-0.37]
</p><p>20 Much recent work has focussed on choosing heuristics for the selection of branching literals since good heuristics have been empirically shown to reduce processing time by several orders of magnitude [28, 16, 13]. [sent-55, score-0.376]
</p><p>21 In this paper we learn heuristics by optimizing over a family of the form, argmaxp f (x, p) where x is a node in the search tree, p is a candidate literal, and f is a priority function mapping possible branches to real numbers. [sent-56, score-0.272]
</p><p>22 The state x will contain at least a CNF sentence and possibly pointers to ancestor nodes or statistics of the local search region. [sent-57, score-0.308]
</p><p>23 Given this relaxed notion of the search state, we are unaware of any branching heuristics in the literature that cannot be expressed in this form. [sent-58, score-0.275]
</p><p>24 3  Perceptron Learning of SAT  We propose to learn f from a sequence of sentences drawn from some distribution determined by a given application. [sent-60, score-0.166]
</p><p>25 We identify f with an element of a Hilbert space, H, the properties of which 2  are determined by a set of statistics polynomial time computable from a SAT instance, Ω. [sent-61, score-0.172]
</p><p>26 We use xj to denote a node that is visited in the application of the DPLL algorithm, and φi (xj ) to denote the feature map associated with instantiating literal pi . [sent-63, score-0.478]
</p><p>27 We deﬁne yij to be +1 if the instantiation of pi at xj leads to the shortest possible proof, and −1 otherwise. [sent-66, score-0.145]
</p><p>28 Our learning procedure therefore will ideally learn a setting of f that only instantiates literals for which yij is +1. [sent-67, score-0.209]
</p><p>29 We do note, however, that the DPLL algorithm is a depth–ﬁrst search over literal valuations. [sent-73, score-0.411]
</p><p>30 Furthermore, for satisﬁable sentences the length of the shortest proof is bounded by the number of variables. [sent-74, score-0.193]
</p><p>31 Consequently, in this case, all nodes visited on a branch of the search tree that resolved to unsatisﬁable have yij = −1 and the nodes on the branch leading to satisﬁable have yij = +1. [sent-75, score-0.606]
</p><p>32 We may run the DPLL algorithm with a current setting of f and if the sentence is satisﬁable, update f using the inferred yij . [sent-76, score-0.289]
</p><p>33 This learning framework is capable of computing in polynomial time valuations of satisﬁable sentences in the following sense. [sent-77, score-0.34]
</p><p>34 Theorem 1 ∃ a polynomial time computable φ with γ > 0 ⇐⇒ Ω belongs to a subset of satisﬁable sentences for which there exists a polynomial time algorithm to ﬁnd a valid valuation. [sent-78, score-0.46]
</p><p>35 Proof Necessity is shown by noting that the argmax in each step of the DPLL algorithm is computable in time polynomial in the sentence length by computing φ for all literals, and that there exists a setting of f such that there will be at most a number of steps equal to the number of variables. [sent-79, score-0.347]
</p><p>36 Sufﬁciency is shown by noting that we may run the polynomial algorithm to ﬁnd a valid valuation and use that valuation to construct a feature space with γ ≥ 0 in polynomial time. [sent-80, score-0.486]
</p><p>37 Concretely, choose a canonical ordering of literals indexed by i and let φi (xj ) be a scalar. [sent-81, score-0.142]
</p><p>38 Set φi (xj ) = +i if literal pi is instantiated in the solution found by the polynomial algorithm, −1 otherwise. [sent-82, score-0.459]
</p><p>39 2 Corollary 1 ∃ polynomial time computable feature space with γ > 0 for SAT ⇐⇒ P = N P Proof If P = N P there is a polynomial time solution to SAT, meaning that there is a polynomial time solution to ﬁnding valuations satisﬁable sentences. [sent-84, score-0.502]
</p><p>40 2 While Theorem 1 is positive for ﬁnding variable settings that satisfy sentences, unsatisﬁable sentences remain problematic when we are unsure that there exists γ > 0 or if we have an incorrect setting of f . [sent-87, score-0.191]
</p><p>41 We are unaware of an efﬁcient method to determine all yij for visited nodes in proofs of unsatisﬁable sentences. [sent-88, score-0.205]
</p><p>42 However, we expect that similar substructures will exist in satisﬁable and unsatisﬁable sentences resulting from the same application. [sent-89, score-0.166]
</p><p>43 Early iterations of our learning algorithm will mistakenly explore branches of the search tree for satisﬁable sentences and these branches will share important characteristics with inefﬁcient branches of proofs of unsatisﬁability. [sent-90, score-0.408]
</p><p>44 The positive nodes with a score less than T are averaged, as are negative nodes with a score greater than T . [sent-97, score-0.212]
</p><p>45 2  Davis-Putnam-Logemann-Loveland Stochastic Gradient  We use a modiﬁed perceptron style update based on the learning as search optimization framework proposed in [6]. [sent-102, score-0.169]
</p><p>46 We know that nodes on a path to a valuation that satisﬁes the sentence have positive labels, and those nodes that require backtracking have negative labels (Figure 1). [sent-105, score-0.451]
</p><p>47 If the sentence is satisﬁable, we may compute a DPLL stochastic gradient, DPLL , and update f . [sent-106, score-0.197]
</p><p>48 We deﬁne two sets of nodes, S+ and S− , such that all nodes in S+ have positive label and lower score than all nodes in S− (Figure 2). [sent-107, score-0.163]
</p><p>49 In this work, we have used the sufﬁcient condition of deﬁning these sets by setting a score threshold, T , such that fk (φi (xj )) < T ∀(i, j) ∈ S+ , fk (φi (xj )) > T ∀(i, j) ∈ S− , and |S+ | × |S− | is maximized. [sent-108, score-0.23]
</p><p>50 Considering the kth update, fk+1  2  = fk −  DPLL  2  = fk  2  − 2 fk ,  DPLL  +  DPLL  2  ≤ fk  2  + 0 + R2 . [sent-114, score-0.42]
</p><p>51 (4)  We note that it is the case that fk , DPLL ≥ 0 for any selection of training examples such that the average of the negative examples score higher than the average of the positive examples generated by running a DPLL search. [sent-115, score-0.207]
</p><p>52 It is possible that some negative examples with lower scores than the some positive nodes will be visited during the depth ﬁrst search of the DPLL algorithm, but we are guaranteed that at least one of them will have higher score. [sent-116, score-0.219]
</p><p>53 Features are computed as a function of a sentence Ω and a literal p. [sent-119, score-0.512]
</p><p>54 We next obtain a lower bound on u, fk+1 = u, fk − u, DPLL ≥ u, fk + γ. [sent-125, score-0.21]
</p><p>55 That − u, DPLL ≥ γ follows from the fact that the means of the positive and negative training examples lie in the convex hull of the positive and negative sets, respectively, and that u achieves a margin of γ. [sent-126, score-0.185]
</p><p>56 Recall that each node xj consists of a CNF sentence Ω together with a valuation for zero or more variables. [sent-135, score-0.354]
</p><p>57 Our feature function φ(x, p) maps a node x and a candidate branching literal p to a real vector φ. [sent-136, score-0.492]
</p><p>58 Many heuristics involve counting occurences of literals and variables. [sent-137, score-0.329]
</p><p>59 For notational convenience let C(p) be the number of occurences of p in Ω and let Ck (p) be the number of occurences of p among clauses of size k. [sent-138, score-0.539]
</p><p>60 1  Relationship to previous branching heuristics  Many branching heuristics have been proposed in the SAT literature [28, 13, 18, 26]. [sent-141, score-0.358]
</p><p>61 Silva [26] suggested two simple heuristics based directly on literal counts. [sent-144, score-0.417]
</p><p>62 The ﬁrst was to always branch on the literal that maximizes C(p) and the second was to maximize C(p) + C(¬p). [sent-145, score-0.424]
</p><p>63 Freeman [13] proposed a heuristic that identiﬁed the size of the smallest unsatisﬁed clause, m = min |ω|, ω ∈ Ω, and then identiﬁed the literal appearing most frequently amongst clauses of size m. [sent-148, score-0.645]
</p><p>64 Bohm [3] proposed a heuristic that selects the literal maximizing α max Ck (p, xj ), Ck (¬p, xj ) + β min Ck (p, xj ), Ck (¬p, xj ) , 5  (5)  with k = 2, or in the case of a tie, with k = 3 (and so on until all ties are broken). [sent-151, score-0.582]
</p><p>65 Jerosolow and Wang [18] proposed a voting scheme in which clauses vote for their components with weight 2−k , where k is the length of the clause. [sent-154, score-0.275]
</p><p>66 The total votes for a literal p is J(p) = 2−|ω| (6) where the sum is over clauses ω that contain p. [sent-155, score-0.612]
</p><p>67 Many modern SAT solvers use boolean constraint propagation (BCP) to speed up the search process [23]. [sent-160, score-0.163]
</p><p>68 One component of BCP generates new clauses as a result of conﬂicts encountered during the search. [sent-161, score-0.275]
</p><p>69 Several modern SAT solvers use the time since a variable was last added to a conﬂict clause to measure the “activity” of that variable . [sent-162, score-0.212]
</p><p>70 Empirically, resolving variables that have most recently appeared in conﬂict clauses results in an efﬁcient search[14]. [sent-163, score-0.275]
</p><p>71 After each decision we update the most–recent– activity table T (p) := t for each p added to a conﬂict clause during that iteration. [sent-166, score-0.281]
</p><p>72 1  Horn  A Horn clause [4] is a disjunction containing at most one positive literal, ¬q1 ∨ · · · ∨ ¬qk−1 ∨ qk . [sent-173, score-0.24]
</p><p>73 A sentence Ω is a Horn formula iff it is a conjunction of Horn clauses. [sent-174, score-0.196]
</p><p>74 If there are no unit clauses in Ω then Ω is trivially satisﬁable by setting all variables to false. [sent-177, score-0.33]
</p><p>75 Delete any clause from Ω that contains p and remove ¬p wherever it appears. [sent-179, score-0.191]
</p><p>76 Theorem 3 There is a margin for Horn clauses in our feature space. [sent-181, score-0.358]
</p><p>77 Proof We will show that there is a margin for Horn clauses in our feature space by showing that for a particular priority function f0 , our algorithm will emulate the unit propagation algorithm above. [sent-182, score-0.522]
</p><p>78 Consider a node x and let Ω be the input sentence Ω0 simpliﬁed according to the (perhaps partial) valuation at x. [sent-185, score-0.301]
</p><p>79 If Ω contains no unit clauses then clearly φ(x, p), f0 will be maximized for a negative literal p = ¬q. [sent-186, score-0.696]
</p><p>80 If Ω does contain unit clauses then for literals p which appear in unit clauses we have φ(x, p), f0 ≥ 1, while for all other literals we have φ(x, p), f0 < 1. [sent-187, score-0.894]
</p><p>81 Therefore H will select a unit literal if Ω contains one. [sent-188, score-0.392]
</p><p>82 First note that every sentence encountered contains at least one unit clause, since, if not, that sentence would be trivially satisﬁable by setting all variables to false and this would contradict the assumption that Ω is unsatisﬁable. [sent-191, score-0.405]
</p><p>83 So at each node x, the algorithm will ﬁrst branch on some unit clause p, then later will back–track to x and branch on ¬p. [sent-192, score-0.442]
</p><p>84 But since p appears in a unit clause at x this will immediately generate a contradiction and no further nodes will be expanded along that path. [sent-193, score-0.331]
</p><p>85 2 1  For concreteness let =  1 K+1  where K is the length of the input sentence Ω  6  (a) Performance for planar graph colouring  (b) Performance for hardware veriﬁcation  Figure 4: Results for our algorithm applied to (a) planar graph colouring; (b) hardware veriﬁcation. [sent-195, score-0.501]
</p><p>86 In ﬁgure (a) we report the mistake rate on the current training example since no training example is ever repeated, while in ﬁgure (b) it is computed on a seperate validation set (see ﬁgure 5). [sent-197, score-0.173]
</p><p>87 2  2–CNF  A 2–CNF sentence is a CNF sentence in which every clause contains exactly two literals. [sent-200, score-0.541]
</p><p>88 In this section we show that a function exists in our feature space for recognising satisﬁable 2–CNF sentences in polynomial time. [sent-201, score-0.348]
</p><p>89 If there are no unit clauses in Ω then pick any literal and add it to Ω. [sent-204, score-0.667]
</p><p>90 Otherwise, let {p} be a unit clause in Ω and apply unit propagation to p as described in the previous section. [sent-205, score-0.326]
</p><p>91 If a contradiction is generated then back–track to the last branch and negate the literal added there. [sent-206, score-0.45]
</p><p>92 Theorem 4 Under our feature space, H contains a priority function that recognizes 2–SAT sentences in polynomial time. [sent-210, score-0.37]
</p><p>93 When using this weight vector, our algorithm will branch on a unit literal whenever one is present. [sent-213, score-0.479]
</p><p>94 2  6  Empirical Results  Planar Graph Colouring: We applied our algorithm on the problem of planar graph colouring, for which polynomial time algorithms are known [1]. [sent-216, score-0.183]
</p><p>95 Due to the large size of these problems we extended an existing high–performance SAT solver, minisat [10], replacing its decision heuristic with our perceptron strategy. [sent-237, score-0.206]
</p><p>96 Planar graph colouring is a known polynomial time computable problem, but it is difﬁcult to characterize theoretically and an automated theorem prover was employed in the proof of polynomial solvability. [sent-244, score-0.458]
</p><p>97 In this work, we argued that learning on positive examples is sufﬁcient if the subset of SAT sentences generated by our application has a positive margin. [sent-248, score-0.216]
</p><p>98 Additionally, we may consider updates to f during a run of the DPLL algorithm when the algorithm backtracks from a branch of the search tree for which we can prove that all yij = −1. [sent-251, score-0.305]
</p><p>99 Linear-time algorithms for testing the satisﬁability of propositional horn formulae. [sent-317, score-0.149]
</p><p>100 The impact of branching heuristics in propositional satisﬁability algorithms. [sent-419, score-0.263]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sat', 0.492), ('dpll', 0.382), ('literal', 0.337), ('clauses', 0.275), ('unsatis', 0.206), ('clause', 0.191), ('sentence', 0.175), ('sentences', 0.166), ('occurences', 0.132), ('cnf', 0.131), ('polynomial', 0.122), ('literals', 0.117), ('fk', 0.105), ('valuation', 0.104), ('branching', 0.099), ('yij', 0.092), ('colouring', 0.088), ('branch', 0.087), ('propositional', 0.084), ('heuristics', 0.08), ('satis', 0.078), ('search', 0.074), ('jeroslow', 0.073), ('minisat', 0.073), ('perceptron', 0.073), ('horn', 0.065), ('mistake', 0.064), ('planar', 0.061), ('nodes', 0.059), ('hardware', 0.058), ('unit', 0.055), ('xj', 0.053), ('valuations', 0.052), ('computable', 0.05), ('margin', 0.049), ('branches', 0.048), ('priority', 0.048), ('able', 0.047), ('logic', 0.045), ('veri', 0.043), ('boolean', 0.043), ('activity', 0.041), ('emulate', 0.036), ('ict', 0.034), ('davis', 0.034), ('feature', 0.034), ('heuristic', 0.033), ('ck', 0.032), ('visited', 0.032), ('ci', 0.031), ('validation', 0.031), ('instances', 0.03), ('bcp', 0.029), ('hooker', 0.029), ('logemann', 0.029), ('pickbranch', 0.029), ('unitpropagate', 0.029), ('automated', 0.029), ('negative', 0.029), ('updates', 0.028), ('training', 0.028), ('checking', 0.028), ('proof', 0.027), ('decision', 0.027), ('contradiction', 0.026), ('neighbour', 0.026), ('existance', 0.026), ('emulates', 0.026), ('recognising', 0.026), ('hoos', 0.026), ('hutter', 0.026), ('canonical', 0.025), ('propagation', 0.025), ('rule', 0.025), ('positive', 0.025), ('cue', 0.024), ('colours', 0.024), ('negated', 0.024), ('symbolic', 0.024), ('disjunction', 0.024), ('jw', 0.024), ('tree', 0.024), ('xl', 0.023), ('unaware', 0.022), ('node', 0.022), ('ever', 0.022), ('competition', 0.022), ('gure', 0.022), ('update', 0.022), ('solver', 0.022), ('competitions', 0.021), ('ft', 0.021), ('iff', 0.021), ('solvers', 0.021), ('broken', 0.02), ('back', 0.02), ('score', 0.02), ('theorem', 0.02), ('ability', 0.02), ('tracks', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="267-tfidf-1" href="./nips-2012-Perceptron_Learning_of_SAT.html">267 nips-2012-Perceptron Learning of SAT</a></p>
<p>Author: Alex Flint, Matthew Blaschko</p><p>Abstract: Boolean satisﬁability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will ﬁnd an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware veriﬁcation task. 1</p><p>2 0.089338601 <a title="267-tfidf-2" href="./nips-2012-Spiking_and_saturating_dendrites_differentially_expand_single_neuron_computation_capacity.html">322 nips-2012-Spiking and saturating dendrites differentially expand single neuron computation capacity</a></p>
<p>Author: Romain Cazé, Mark Humphries, Boris S. Gutkin</p><p>Abstract: The integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input’s response taken separately. If this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating. Decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable Boolean functions (lnBFs). How can these lnBFs be implemented by dendritic architectures in practise? And can saturating dendrites equally expand computational capacity? To address these questions we use a binary neuron model and Boolean algebra. First, we conﬁrm that spiking dendrites enable a neuron to compute lnBFs using an architecture based on the disjunctive normal form (DNF). Second, we prove that saturating dendrites as well as spiking dendrites enable a neuron to compute lnBFs using an architecture based on the conjunctive normal form (CNF). Contrary to a DNF-based architecture, in a CNF-based architecture, dendritic unit tunings do not imply the neuron tuning, as has been observed experimentally. Third, we show that one cannot use a DNF-based architecture with saturating dendrites. Consequently, we show that an important family of lnBFs implemented with a CNF-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a DNF-architecture or a CNF-architecture always require a linear number of spiking dendritic units. This minimization could explain why a neuron spends energetic resources to make its dendrites spike. 1</p><p>3 0.073344052 <a title="267-tfidf-3" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>Author: Aharon Birnbaum, Shai S. Shwartz</p><p>Abstract: Given α, ϵ, we study the time complexity required to improperly learn a halfspace with misclassiﬁcation error rate of at most (1 + α) L∗ + ϵ, where L∗ is the γ γ optimal γ-margin error rate. For α = 1/γ, polynomial time and sample complexity is achievable using the hinge-loss. For α = 0, Shalev-Shwartz et al. [2011] showed that poly(1/γ) time is impossible, while learning is possible in ˜ time exp(O(1/γ)). An immediate question, which this paper tackles, is what is achievable if α ∈ (0, 1/γ). We derive positive results interpolating between the polynomial time for α = 1/γ and the exponential time for α = 0. In particular, we show that there are cases in which α = o(1/γ) but the problem is still solvable in polynomial time. Our results naturally extend to the adversarial online learning model and to the PAC learning with malicious noise model. 1</p><p>4 0.056271885 <a title="267-tfidf-4" href="./nips-2012-Tensor_Decomposition_for_Fast_Parsing_with_Latent-Variable_PCFGs.html">334 nips-2012-Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs</a></p>
<p>Author: Michael Collins, Shay B. Cohen</p><p>Abstract: We describe an approach to speed-up inference with latent-variable PCFGs, which have been shown to be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature. We also describe an error bound for this approximation, which gives guarantees showing that if the underlying tensors are well approximated, then the probability distribution over trees will also be well approximated. Empirical evaluation on real-world natural language parsing data demonstrates a signiﬁcant speed-up at minimal cost for parsing performance. 1</p><p>5 0.055814255 <a title="267-tfidf-5" href="./nips-2012-Identifiability_and_Unmixing_of_Latent_Parse_Trees.html">156 nips-2012-Identifiability and Unmixing of Latent Parse Trees</a></p>
<p>Author: Percy Liang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: This paper explores unsupervised learning of parsing models along two directions. First, which models are identiﬁable from inﬁnite data? We use a general technique for numerically checking identiﬁability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models. Second, for identiﬁable models, how do we estimate the parameters efﬁciently? EM suffers from local optima, while recent work using spectral methods [1] cannot be directly applied since the topology of the parse tree varies across sentences. We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models. 1</p><p>6 0.047895052 <a title="267-tfidf-6" href="./nips-2012-Nonparametric_Max-Margin_Matrix_Factorization_for_Collaborative_Prediction.html">246 nips-2012-Nonparametric Max-Margin Matrix Factorization for Collaborative Prediction</a></p>
<p>7 0.046917528 <a title="267-tfidf-7" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>8 0.046628203 <a title="267-tfidf-8" href="./nips-2012-Local_Supervised_Learning_through_Space_Partitioning.html">200 nips-2012-Local Supervised Learning through Space Partitioning</a></p>
<p>9 0.046083573 <a title="267-tfidf-9" href="./nips-2012-On_Lifting_the_Gibbs_Sampling_Algorithm.html">251 nips-2012-On Lifting the Gibbs Sampling Algorithm</a></p>
<p>10 0.045480147 <a title="267-tfidf-10" href="./nips-2012-A_Polylog_Pivot_Steps_Simplex_Algorithm_for_Classification.html">15 nips-2012-A Polylog Pivot Steps Simplex Algorithm for Classification</a></p>
<p>11 0.045180667 <a title="267-tfidf-11" href="./nips-2012-The_Time-Marginalized_Coalescent_Prior_for_Hierarchical_Clustering.html">339 nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</a></p>
<p>12 0.042382862 <a title="267-tfidf-12" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>13 0.04130549 <a title="267-tfidf-13" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<p>14 0.04043521 <a title="267-tfidf-14" href="./nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</a></p>
<p>15 0.036335886 <a title="267-tfidf-15" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>16 0.036163781 <a title="267-tfidf-16" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>17 0.034204893 <a title="267-tfidf-17" href="./nips-2012-A_new_metric_on_the_manifold_of_kernel_matrices_with_application_to_matrix_geometric_means.html">25 nips-2012-A new metric on the manifold of kernel matrices with application to matrix geometric means</a></p>
<p>18 0.034063477 <a title="267-tfidf-18" href="./nips-2012-Slice_Normalized_Dynamic_Markov_Logic_Networks.html">314 nips-2012-Slice Normalized Dynamic Markov Logic Networks</a></p>
<p>19 0.033085451 <a title="267-tfidf-19" href="./nips-2012-Dimensionality_Dependent_PAC-Bayes_Margin_Bound.html">98 nips-2012-Dimensionality Dependent PAC-Bayes Margin Bound</a></p>
<p>20 0.032974415 <a title="267-tfidf-20" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.108), (1, -0.008), (2, -0.003), (3, -0.026), (4, 0.009), (5, -0.007), (6, -0.014), (7, 0.011), (8, -0.036), (9, 0.028), (10, -0.002), (11, 0.006), (12, 0.045), (13, 0.017), (14, -0.017), (15, -0.023), (16, -0.04), (17, -0.0), (18, 0.017), (19, 0.018), (20, -0.023), (21, 0.022), (22, -0.002), (23, -0.012), (24, -0.055), (25, 0.025), (26, 0.023), (27, -0.059), (28, 0.008), (29, 0.059), (30, 0.035), (31, -0.005), (32, 0.014), (33, 0.043), (34, -0.046), (35, -0.009), (36, -0.09), (37, -0.057), (38, -0.031), (39, 0.004), (40, -0.082), (41, -0.034), (42, 0.036), (43, 0.003), (44, -0.047), (45, -0.082), (46, -0.006), (47, 0.019), (48, 0.045), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85277075 <a title="267-lsi-1" href="./nips-2012-Perceptron_Learning_of_SAT.html">267 nips-2012-Perceptron Learning of SAT</a></p>
<p>Author: Alex Flint, Matthew Blaschko</p><p>Abstract: Boolean satisﬁability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will ﬁnd an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware veriﬁcation task. 1</p><p>2 0.60113108 <a title="267-lsi-2" href="./nips-2012-Identifiability_and_Unmixing_of_Latent_Parse_Trees.html">156 nips-2012-Identifiability and Unmixing of Latent Parse Trees</a></p>
<p>Author: Percy Liang, Daniel J. Hsu, Sham M. Kakade</p><p>Abstract: This paper explores unsupervised learning of parsing models along two directions. First, which models are identiﬁable from inﬁnite data? We use a general technique for numerically checking identiﬁability based on the rank of a Jacobian matrix, and apply it to several standard constituency and dependency parsing models. Second, for identiﬁable models, how do we estimate the parameters efﬁciently? EM suffers from local optima, while recent work using spectral methods [1] cannot be directly applied since the topology of the parse tree varies across sentences. We develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models. 1</p><p>3 0.57842112 <a title="267-lsi-3" href="./nips-2012-Tensor_Decomposition_for_Fast_Parsing_with_Latent-Variable_PCFGs.html">334 nips-2012-Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs</a></p>
<p>Author: Michael Collins, Shay B. Cohen</p><p>Abstract: We describe an approach to speed-up inference with latent-variable PCFGs, which have been shown to be highly effective for natural language parsing. Our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable PCFGs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature. We also describe an error bound for this approximation, which gives guarantees showing that if the underlying tensors are well approximated, then the probability distribution over trees will also be well approximated. Empirical evaluation on real-world natural language parsing data demonstrates a signiﬁcant speed-up at minimal cost for parsing performance. 1</p><p>4 0.49326909 <a title="267-lsi-4" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>Author: Paul Vernaza, Drew Bagnell</p><p>Abstract: Maximum entropy (MaxEnt) modeling is a popular choice for sequence analysis in applications such as natural language processing, where the sequences are embedded in discrete, tractably-sized spaces. We consider the problem of applying MaxEnt to distributions over paths in continuous spaces of high dimensionality— a problem for which inference is generally intractable. Our main contribution is to show that this intractability can be avoided as long as the constrained features possess a certain kind of low dimensional structure. In this case, we show that the associated partition function is symmetric and that this symmetry can be exploited to compute the partition function efﬁciently in a compressed form. Empirical results are given showing an application of our method to learning models of high-dimensional human motion capture data. 1</p><p>5 0.47277683 <a title="267-lsi-5" href="./nips-2012-Learning_Partially_Observable_Models_Using_Temporally_Abstract_Decision_Trees.html">183 nips-2012-Learning Partially Observable Models Using Temporally Abstract Decision Trees</a></p>
<p>Author: Erik Talvitie</p><p>Abstract: This paper introduces timeline trees, which are partial models of partially observable environments. Timeline trees are given some speciﬁc predictions to make and learn a decision tree over history. The main idea of timeline trees is to use temporally abstract features to identify and split on features of key events, spread arbitrarily far apart in the past (whereas previous decision-tree-based methods have been limited to a ﬁnite sufﬁx of history). Experiments demonstrate that timeline trees can learn to make high quality predictions in complex, partially observable environments with high-dimensional observations (e.g. an arcade game). 1</p><p>6 0.47214699 <a title="267-lsi-6" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>7 0.46906614 <a title="267-lsi-7" href="./nips-2012-MAP_Inference_in_Chains_using_Column_Generation.html">204 nips-2012-MAP Inference in Chains using Column Generation</a></p>
<p>8 0.46552676 <a title="267-lsi-8" href="./nips-2012-Learning_with_Target_Prior.html">198 nips-2012-Learning with Target Prior</a></p>
<p>9 0.45281744 <a title="267-lsi-9" href="./nips-2012-Augmented-SVM%3A_Automatic_space_partitioning_for_combining_multiple_non-linear_dynamics.html">48 nips-2012-Augmented-SVM: Automatic space partitioning for combining multiple non-linear dynamics</a></p>
<p>10 0.45063207 <a title="267-lsi-10" href="./nips-2012-The_Time-Marginalized_Coalescent_Prior_for_Hierarchical_Clustering.html">339 nips-2012-The Time-Marginalized Coalescent Prior for Hierarchical Clustering</a></p>
<p>11 0.44813639 <a title="267-lsi-11" href="./nips-2012-Density_Propagation_and_Improved_Bounds_on_the_Partition_Function.html">96 nips-2012-Density Propagation and Improved Bounds on the Partition Function</a></p>
<p>12 0.44559681 <a title="267-lsi-12" href="./nips-2012-Spiking_and_saturating_dendrites_differentially_expand_single_neuron_computation_capacity.html">322 nips-2012-Spiking and saturating dendrites differentially expand single neuron computation capacity</a></p>
<p>13 0.43833831 <a title="267-lsi-13" href="./nips-2012-Majorization_for_CRFs_and_Latent_Likelihoods.html">206 nips-2012-Majorization for CRFs and Latent Likelihoods</a></p>
<p>14 0.43188488 <a title="267-lsi-14" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>15 0.42574909 <a title="267-lsi-15" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>16 0.42227697 <a title="267-lsi-16" href="./nips-2012-Scaling_MPE_Inference_for_Constrained_Continuous_Markov_Random_Fields_with_Consensus_Optimization.html">302 nips-2012-Scaling MPE Inference for Constrained Continuous Markov Random Fields with Consensus Optimization</a></p>
<p>17 0.41970709 <a title="267-lsi-17" href="./nips-2012-Multiplicative_Forests_for_Continuous-Time_Processes.html">232 nips-2012-Multiplicative Forests for Continuous-Time Processes</a></p>
<p>18 0.41316089 <a title="267-lsi-18" href="./nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</a></p>
<p>19 0.41094941 <a title="267-lsi-19" href="./nips-2012-Volume_Regularization_for_Binary_Classification.html">361 nips-2012-Volume Regularization for Binary Classification</a></p>
<p>20 0.40349177 <a title="267-lsi-20" href="./nips-2012-Globally_Convergent_Dual_MAP_LP_Relaxation_Solvers_using_Fenchel-Young_Margins.html">143 nips-2012-Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (17, 0.014), (21, 0.017), (38, 0.127), (39, 0.014), (42, 0.032), (54, 0.024), (55, 0.025), (74, 0.053), (76, 0.098), (80, 0.085), (92, 0.038), (98, 0.354)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80286682 <a title="267-lda-1" href="./nips-2012-How_They_Vote%3A_Issue-Adjusted_Models_of_Legislative_Behavior.html">154 nips-2012-How They Vote: Issue-Adjusted Models of Legislative Behavior</a></p>
<p>Author: Sean Gerrish, David M. Blei</p><p>Abstract: We develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers’ positions on speciﬁc political issues. Our model can be used to explore how a lawmaker’s voting patterns deviate from what is expected and how that deviation depends on what is being voted on. We derive approximate posterior inference algorithms based on variational methods. Across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model’s utility in interpreting an inherently multi-dimensional space. 1</p><p>same-paper 2 0.73079896 <a title="267-lda-2" href="./nips-2012-Perceptron_Learning_of_SAT.html">267 nips-2012-Perceptron Learning of SAT</a></p>
<p>Author: Alex Flint, Matthew Blaschko</p><p>Abstract: Boolean satisﬁability (SAT) as a canonical NP-complete decision problem is one of the most important problems in computer science. In practice, real-world SAT sentences are drawn from a distribution that may result in efﬁcient algorithms for their solution. Such SAT instances are likely to have shared characteristics and substructures. This work approaches the exploration of a family of SAT solvers as a learning problem. In particular, we relate polynomial time solvability of a SAT subset to a notion of margin between sentences mapped by a feature function into a Hilbert space. Provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that SAT subset based on the Davis-Putnam-Logemann-Loveland algorithm. Furthermore, we show that a simple perceptron-style learning rule will ﬁnd an optimal SAT solver with a bounded number of training updates. We derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of SAT. Empirical results show an order of magnitude improvement over a state-of-the-art SAT solver on a hardware veriﬁcation task. 1</p><p>3 0.66155642 <a title="267-lda-3" href="./nips-2012-Bayesian_Warped_Gaussian_Processes.html">55 nips-2012-Bayesian Warped Gaussian Processes</a></p>
<p>Author: Miguel Lázaro-gredilla</p><p>Abstract: Warped Gaussian processes (WGP) [1] model output observations in regression tasks as a parametric nonlinear transformation of a Gaussian process (GP). The use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets. In order to learn its parameters, maximum likelihood was used. In this work we show that it is possible to use a non-parametric nonlinear transformation in WGP and variationally integrate it out. The resulting Bayesian WGP is then able to work in scenarios in which the maximum likelihood WGP failed: Low data regime, data with censored values, classiﬁcation, etc. We demonstrate the superior performance of Bayesian warped GPs on several real data sets.</p><p>4 0.63228416 <a title="267-lda-4" href="./nips-2012-Unsupervised_Structure_Discovery_for_Semantic_Analysis_of_Audio.html">356 nips-2012-Unsupervised Structure Discovery for Semantic Analysis of Audio</a></p>
<p>Author: Sourish Chaudhuri, Bhiksha Raj</p><p>Abstract: Approaches to audio classiﬁcation and retrieval tasks largely rely on detectionbased discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the ﬁrst layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report signiﬁcant improvements over standard baselines. 1</p><p>5 0.60385805 <a title="267-lda-5" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>Author: Aharon Birnbaum, Shai S. Shwartz</p><p>Abstract: Given α, ϵ, we study the time complexity required to improperly learn a halfspace with misclassiﬁcation error rate of at most (1 + α) L∗ + ϵ, where L∗ is the γ γ optimal γ-margin error rate. For α = 1/γ, polynomial time and sample complexity is achievable using the hinge-loss. For α = 0, Shalev-Shwartz et al. [2011] showed that poly(1/γ) time is impossible, while learning is possible in ˜ time exp(O(1/γ)). An immediate question, which this paper tackles, is what is achievable if α ∈ (0, 1/γ). We derive positive results interpolating between the polynomial time for α = 1/γ and the exponential time for α = 0. In particular, we show that there are cases in which α = o(1/γ) but the problem is still solvable in polynomial time. Our results naturally extend to the adversarial online learning model and to the PAC learning with malicious noise model. 1</p><p>6 0.58527821 <a title="267-lda-6" href="./nips-2012-Learning_Mixtures_of_Tree_Graphical_Models.html">180 nips-2012-Learning Mixtures of Tree Graphical Models</a></p>
<p>7 0.48404521 <a title="267-lda-7" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>8 0.48146552 <a title="267-lda-8" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>9 0.47869077 <a title="267-lda-9" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>10 0.47841454 <a title="267-lda-10" href="./nips-2012-Learning_as_MAP_Inference_in_Discrete_Graphical_Models.html">186 nips-2012-Learning as MAP Inference in Discrete Graphical Models</a></p>
<p>11 0.47813493 <a title="267-lda-11" href="./nips-2012-Link_Prediction_in_Graphs_with_Autoregressive_Features.html">199 nips-2012-Link Prediction in Graphs with Autoregressive Features</a></p>
<p>12 0.477357 <a title="267-lda-12" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>13 0.47715592 <a title="267-lda-13" href="./nips-2012-Regularized_Off-Policy_TD-Learning.html">292 nips-2012-Regularized Off-Policy TD-Learning</a></p>
<p>14 0.47714788 <a title="267-lda-14" href="./nips-2012-Multiple_Choice_Learning%3A_Learning_to_Produce_Multiple_Structured_Outputs.html">230 nips-2012-Multiple Choice Learning: Learning to Produce Multiple Structured Outputs</a></p>
<p>15 0.47712931 <a title="267-lda-15" href="./nips-2012-Truncation-free_Online_Variational_Inference_for_Bayesian_Nonparametric_Models.html">355 nips-2012-Truncation-free Online Variational Inference for Bayesian Nonparametric Models</a></p>
<p>16 0.47692078 <a title="267-lda-16" href="./nips-2012-Mirror_Descent_Meets_Fixed_Share_%28and_feels_no_regret%29.html">216 nips-2012-Mirror Descent Meets Fixed Share (and feels no regret)</a></p>
<p>17 0.47676378 <a title="267-lda-17" href="./nips-2012-Cardinality_Restricted_Boltzmann_Machines.html">65 nips-2012-Cardinality Restricted Boltzmann Machines</a></p>
<p>18 0.47669342 <a title="267-lda-18" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>19 0.47665831 <a title="267-lda-19" href="./nips-2012-Near-Optimal_MAP_Inference_for_Determinantal_Point_Processes.html">236 nips-2012-Near-Optimal MAP Inference for Determinantal Point Processes</a></p>
<p>20 0.47652504 <a title="267-lda-20" href="./nips-2012-Latent_Graphical_Model_Selection%3A_Efficient_Methods_for_Locally_Tree-like_Graphs.html">172 nips-2012-Latent Graphical Model Selection: Efficient Methods for Locally Tree-like Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
