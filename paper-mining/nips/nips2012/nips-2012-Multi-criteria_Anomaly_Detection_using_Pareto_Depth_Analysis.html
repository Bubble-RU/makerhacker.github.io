<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-223" href="#">nips2012-223</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</h1>
<br/><p>Source: <a title="nips-2012-223-pdf" href="http://papers.nips.cc/paper/4612-multi-criteria-anomaly-detection-using-pareto-depth-analysis.pdf">pdf</a></p><p>Author: Ko-jen Hsiao, Kevin Xu, Jeff Calder, Alfred O. Hero</p><p>Abstract: We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be deﬁned, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria. 1</p><p>Reference: <a title="nips-2012-223-reference" href="../nips2012_reference/nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. [sent-4, score-0.504]
</p><p>2 In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. [sent-5, score-0.608]
</p><p>3 However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. [sent-6, score-0.228]
</p><p>4 In such a case, multiple criteria can be deﬁned, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. [sent-7, score-0.321]
</p><p>5 In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). [sent-9, score-0.533]
</p><p>6 PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. [sent-10, score-0.25]
</p><p>7 Many methods for anomaly detection have been developed using both parametric and non-parametric approaches. [sent-13, score-0.485]
</p><p>8 For complex high-dimensional data, multiple dissimilarity measures corresponding to different criteria may be required to detect certain types of anomalies. [sent-15, score-0.227]
</p><p>9 Multiple criteria, such as dissimilarity in object speeds or trajectory shapes, can be used to detect a greater range of anomalies than any single criterion. [sent-17, score-0.203]
</p><p>10 In order to perform anomaly detection using these multiple criteria, one could ﬁrst combine the dissimilarities using a linear combination. [sent-18, score-0.573]
</p><p>11 Furthermore, when the weights are changed, the anomaly detection algorithm needs to be re-executed using the new weights. [sent-21, score-0.514]
</p><p>12 In this paper we propose a novel non-parametric multi-criteria anomaly detection approach using Pareto depth analysis (PDA). [sent-22, score-0.533]
</p><p>13 Hence, PDA is able to detect anomalies under multiple combinations of the criteria without explicitly forming these combinations. [sent-27, score-0.215]
</p><p>14 Center: Dyads for the training samples (black dots) along with ﬁrst 20 Pareto fronts (green lines) under two criteria: |∆x| and |∆y|. [sent-29, score-0.311]
</p><p>15 The Pareto fronts induce a partial ordering on the set of dyads. [sent-30, score-0.265]
</p><p>16 Dyads associated with the test sample marked by the red circle concentrate around shallow fronts (near the lower left of the ﬁgure). [sent-31, score-0.331]
</p><p>17 The PDA approach involves creating dyads corresponding to dissimilarities between pairs of data samples under all of the dissimilarity measures. [sent-33, score-0.586]
</p><p>18 The ﬁrst Pareto front (depth one) is the set of non-dominated dyads. [sent-35, score-0.178]
</p><p>19 The second Pareto front (depth two) is obtained by removing these non-dominated dyads, i. [sent-36, score-0.178]
</p><p>20 peeling off the ﬁrst front, and recomputing the ﬁrst Pareto front of those remaining. [sent-38, score-0.178]
</p><p>21 In this way, each dyad is assigned to a Pareto front at some depth (see Fig. [sent-40, score-0.359]
</p><p>22 Nominal and anomalous samples are located near different Pareto front depths; thus computing the front depths of the dyads corresponding to a test sample can discriminate between nominal and anomalous samples. [sent-42, score-1.23]
</p><p>23 Under assumptions that the multi-criteria dyads can be modeled as a realizations from a smooth K-dimensional density we provide a mathematical analysis of the behavior of the ﬁrst Pareto front. [sent-44, score-0.408]
</p><p>24 Furthermore, this theoretical prediction is experimentally validated by comparing PDA to several state-of-the-art anomaly detection algorithms in two experiments involving both synthetic and real data sets. [sent-46, score-0.497]
</p><p>25 In Section 3 we provide an introduction to Pareto fronts and present a theoretical analysis of the properties of the ﬁrst Pareto front. [sent-49, score-0.265]
</p><p>26 Section 4 relates Pareto fronts to the multi-criteria anomaly detection problem, which leads to the PDA anomaly detection algorithm. [sent-50, score-1.235]
</p><p>27 These methods typically formulate machine learning problems as multi-objective optimization problems where ﬁnding even the ﬁrst Pareto front is quite difﬁcult. [sent-53, score-0.178]
</p><p>28 These methods differ from our use of Pareto optimality because we consider multiple Pareto fronts created from a ﬁnite set of items, so we do not need to employ sophisticated methods in order to ﬁnd these fronts. [sent-54, score-0.313]
</p><p>29 Hero and Fleury [4] introduced a method for gene ranking using Pareto fronts that is related to our approach. [sent-55, score-0.265]
</p><p>30 The method ranks genes, in order of interest to a biologist, by creating Pareto fronts of the data samples, i. [sent-56, score-0.265]
</p><p>31 In this paper, we consider Pareto fronts of dyads, which correspond to dissimilarities between pairs of data samples rather than the samples themselves, and use the distribution of dyads in Pareto fronts to perform multi-criteria anomaly detection rather than ranking. [sent-59, score-1.534]
</p><p>32 A similar area is that of multiple kernel learning [8], which is typically applied to supervised learning problems, unlike the unsupervised anomaly detection setting we consider. [sent-66, score-0.506]
</p><p>33 Finally, many other anomaly detection methods have previously been proposed. [sent-67, score-0.485]
</p><p>34 [2] both provide extensive surveys of different anomaly detection methods and applications. [sent-69, score-0.497]
</p><p>35 Byers and Raftery [9] proposed to use the distance between a sample and its kth-nearest neighbor as the anomaly score for the sample; similarly, Angiulli and Pizzuti [10] and Eskin et al. [sent-71, score-0.42]
</p><p>36 [12] used an anomaly score based on the local density of the k nearest neighbors of a sample. [sent-74, score-0.461]
</p><p>37 Hero [13] and Sricharan and Hero [14] introduced non-parametric adaptive anomaly detection methods using geometric entropy minimization, based on random k-point minimal spanning trees and bipartite k-nearest neighbor (k-NN) graphs, respectively. [sent-75, score-0.485]
</p><p>38 Zhao and Saligrama [15] proposed an anomaly detection algorithm k-LPE using local p-value estimation (LPE) based on a k-NN graph. [sent-76, score-0.485]
</p><p>39 These k-NN anomaly detection schemes only depend on the data through the pairs of data points (dyads) that deﬁne the edges in the k-NN graphs. [sent-77, score-0.511]
</p><p>40 All of the aforementioned methods are designed for single-criteria anomaly detection. [sent-78, score-0.371]
</p><p>41 In the multicriteria setting, the single-criteria algorithms must be executed multiple times with different weights, unlike the PDA anomaly detection algorithm that we propose in Section 4. [sent-79, score-0.526]
</p><p>42 Different choices of (nonnegative) weights in the linear combination could result in different minimizers; a set of items that are minimizers under some linear combination can then be created by using a grid search over the weights, for example. [sent-94, score-0.181]
</p><p>43 The second Pareto front can be constructed by ﬁnding items that are not strictly dominated by any of the remaining items, which are members of the set S \ F1 . [sent-101, score-0.251]
</p><p>44 More generally, deﬁne the ith Pareto front by   i−1  Fi = Pareto front of the set S \   Fj  . [sent-102, score-0.356]
</p><p>45 j=1  For convenience, we say that a Pareto front Fi is deeper than Fj if i > j. [sent-103, score-0.194]
</p><p>46 1  Mathematical properties of Pareto fronts  The distribution of the number of points on the ﬁrst Pareto front was ﬁrst studied by BarndorffNielsen and Sobel in their seminal work [17]. [sent-105, score-0.469]
</p><p>47 We will be concerned here with properties of the ﬁrst Pareto front that are relevant to the PDA anomaly detection algorithm and thus have not yet been considered in the literature. [sent-107, score-0.663]
</p><p>48 For a measurable set A ⊂ Rd , we denote by FA the points on the ﬁrst Pareto front of Y1 , . [sent-115, score-0.204]
</p><p>49 + + Although this is a common motivation for Pareto methods, there are, to the best of our knowledge, no results in the literature regarding how many points on the Pareto front are missed by scalarization. [sent-130, score-0.204]
</p><p>50 In the context of this paper, if some Pareto-optimal points are not identiﬁed, then the anomaly score (deﬁned in section 4. [sent-138, score-0.428]
</p><p>51 Hence the size of F \ L is a measure of how much the anomaly score is inﬂated and the degree to which Pareto methods will outperform linear scalarization. [sent-140, score-0.414]
</p><p>52 4  Figure 2: Left: Non-convexities in the Pareto front induced by the geometry of the domain Ω (Theorem 1). [sent-189, score-0.198]
</p><p>53 4  Multi-criteria anomaly detection  Assume that a training set XN = {X1 , . [sent-205, score-0.503]
</p><p>54 Given a test sample X, the objective of anomaly detection is to declare X to be an anomaly if X is signiﬁcantly different from samples in XN . [sent-209, score-0.934]
</p><p>55 Denote the dissimilarity between Xi and Xj computed using the measure corresponding to the lth criterion by dl (i, j). [sent-212, score-0.18]
</p><p>56 For convenience, denote the set of all dyads by D and the space of all 2 dyads RK by D. [sent-225, score-0.816]
</p><p>57 By the deﬁnition of strict dominance in Section 3, a dyad Dij strictly dominates + another dyad Di∗ j ∗ if dl (i, j) ≤ dl (i∗ , j ∗ ) for all l ∈ {1, . [sent-226, score-0.396]
</p><p>58 The ﬁrst Pareto front F1 corresponds to the set of dyads from D that are not strictly dominated by any other dyads from D. [sent-230, score-1.034]
</p><p>59 The second Pareto front F2 corresponds to the set of dyads from D \ F1 that are not strictly dominated by any other dyads from D \ F1 , and so on, as deﬁned in Section 3. [sent-231, score-1.034]
</p><p>60 Recall that we refer to Fi as a deeper front than Fj if i > j. [sent-232, score-0.194]
</p><p>61 1  Pareto fronts of dyads  For each sample Xn , there are N − 1 dyads corresponding to its connections with the other N − 1 samples. [sent-234, score-1.099]
</p><p>62 Deﬁne the set of N − 1 dyads associated with Xn by Dn . [sent-235, score-0.408]
</p><p>63 If most dyads in Dn are located at shallow Pareto fronts, then the dissimilarities between Xn and the other N − 1 samples are small under some combination of the criteria. [sent-236, score-0.549]
</p><p>64 This is the basic idea of the proposed multi-criteria anomaly detection method using PDA. [sent-238, score-0.485]
</p><p>65 , FM of the dyads from the training set, where the total number of fronts M is the required number of fronts such that each dyad is a member of a front. [sent-242, score-1.089]
</p><p>66 When a test sample X is obtained, we create new dyads corresponding to connections between X and training samples, as illustrated in Figure 1. [sent-243, score-0.478]
</p><p>67 Similar to many other anomaly detection methods, we connect each test sample to its k nearest neighbors. [sent-244, score-0.558]
</p><p>68 We create s = i=1 ki new dyads, which we denote by the set 5  Algorithm 1 PDA anomaly detection algorithm. [sent-246, score-0.528]
</p><p>69 Training phase: 1: for l = 1 → K do 2: Calculate pairwise dissimilarities dl (i, j) between all training samples Xi and Xj 3: Create dyads Dij = [d1 (i, j), . [sent-247, score-0.557]
</p><p>70 In other words, we create a dyad between X and Xj if Xj new is among the ki nearest neighbors1 of X in any criterion i. [sent-254, score-0.25]
</p><p>71 2  Anomaly detection using depths of dyads  In k-NN based anomaly detection algorithms such as those mentioned in Section 2, the anomaly score is a function of the k nearest neighbors to a test sample. [sent-262, score-1.519]
</p><p>72 With multiple criteria, one could deﬁne an anomaly score by scalarization. [sent-263, score-0.423]
</p><p>73 From the probabilistic properties of Pareto fronts discussed in Section 3. [sent-264, score-0.265]
</p><p>74 This motivates us to develop a multi-criteria anomaly score using Pareto fronts. [sent-266, score-0.402]
</p><p>75 We start with the observation from Figure 1 that dyads corresponding to a nominal test sample are typically located near shallower fronts than dyads corresponding to an anomalous test sample. [sent-267, score-1.371]
</p><p>76 Each test sample is new associated with s new dyads, where the ith dyad Di has depth ei . [sent-268, score-0.254]
</p><p>77 For each test sample X, we deﬁne the anomaly score v(X) to be the mean of the ei ’s, which corresponds to the average depth of the s dyads associated with X. [sent-269, score-0.931]
</p><p>78 Thus the anomaly score can be easily computed and compared to the decision threshold σ using the test v(X) =  1 s  s  ei i=1  H1  σ. [sent-270, score-0.457]
</p><p>79 H0  Pseudocode for the PDA anomaly detector is shown in Algorithm 1. [sent-271, score-0.371]
</p><p>80 Both the training time and the 1  If a training sample is one of the ki nearest neighbors in multiple criteria, then multiple copies of the dyad corresponding to the connection between the test sample and the training sample are created. [sent-273, score-0.387]
</p><p>81 dyads as well, and it is supported by experimental results presented in Section 2 of the supplementary material. [sent-282, score-0.408]
</p><p>82 To handle multiple criteria, other anomaly detection methods, such as the ones mentioned in Section 2, need to be re-executed multiple times using different (non-negative) linear combinations of the K criteria. [sent-316, score-0.568]
</p><p>83 5  Experiments  We compare the PDA method with four other nearest neighbor-based single-criterion anomaly detection algorithms mentioned in Section 2. [sent-320, score-0.522]
</p><p>84 For these methods, we use linear combinations of the criteria with different weights selected by grid search to compare performance with PDA. [sent-321, score-0.184]
</p><p>85 The anomalous samples are located just outside of this hypercube. [sent-325, score-0.184]
</p><p>86 Each class differs from the nominal distribution in one of the four dimensions; the distribution in the anomalous dimension is uniform on [1, 1. [sent-327, score-0.197]
</p><p>87 We draw 300 training samples from the nominal distribution followed by 100 test samples from a mixture of the nominal and anomalous distributions with a 0. [sent-329, score-0.353]
</p><p>88 If the criteria are combined using linear combinations, the combined dissimilarity measure reduces to weighted squared Euclidean distance. [sent-332, score-0.196]
</p><p>89 Right: A subset of the dyads for the training samples along with the ﬁrst 100 Pareto fronts. [sent-368, score-0.454]
</p><p>90 The fronts are highly non-convex, partially explaining the superior performance of PDA. [sent-369, score-0.265]
</p><p>91 We use two criteria for computing the dissimilarity between trajectories. [sent-370, score-0.184]
</p><p>92 For a more detailed comparison, the ROC curve for PDA and the attainable region for k-LPE (the region between the ROC curves corresponding to weights resulting in the best and worst AUCs) is shown in Figure 3 along with the ﬁrst 100 Pareto fronts for PDA. [sent-383, score-0.327]
</p><p>93 6  Conclusion  In this paper we proposed a new multi-criteria anomaly detection method. [sent-386, score-0.485]
</p><p>94 The proposed method uses Pareto depth analysis to compute the anomaly score of a test sample by examining the Pareto front depths of dyads corresponding to the test sample. [sent-387, score-1.123]
</p><p>95 Dyads corresponding to an anomalous sample tended to be located at deeper fronts compared to dyads corresponding to a nominal sample. [sent-388, score-0.927]
</p><p>96 Instead of choosing a speciﬁc weighting or performing a grid search on the weights for different dissimilarity measures, the proposed method can efﬁciently detect anomalies in a manner that scales linearly in the number of criteria. [sent-389, score-0.237]
</p><p>97 We also thank Daniel DeWoskin for suggesting a fast algorithm for computing Pareto fronts in two criteria. [sent-393, score-0.265]
</p><p>98 A geometric framework for unsupervised anomaly detection: Detecting intrusions in unlabeled data. [sent-458, score-0.371]
</p><p>99 Geometric entropy minimization (GEM) for anomaly detection and localization. [sent-474, score-0.485]
</p><p>100 Anomaly detection with score functions based on nearest neighbor graphs. [sent-485, score-0.182]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pareto', 0.578), ('dyads', 0.408), ('anomaly', 0.371), ('pda', 0.337), ('fronts', 0.265), ('front', 0.178), ('anomalous', 0.133), ('dyad', 0.133), ('detection', 0.114), ('dissimilarity', 0.095), ('criteria', 0.089), ('nominal', 0.064), ('dissimilarities', 0.055), ('anomalies', 0.054), ('scalarization', 0.051), ('hero', 0.05), ('dl', 0.048), ('depth', 0.048), ('auc', 0.046), ('lpe', 0.041), ('item', 0.038), ('criterion', 0.037), ('fi', 0.037), ('nearest', 0.037), ('ei', 0.037), ('di', 0.036), ('dij', 0.036), ('pedestrian', 0.036), ('depths', 0.033), ('items', 0.033), ('trajectory', 0.032), ('score', 0.031), ('lof', 0.031), ('combinations', 0.029), ('weights', 0.029), ('nb', 0.028), ('trajectories', 0.028), ('samples', 0.028), ('aucs', 0.027), ('optimality', 0.027), ('ki', 0.027), ('points', 0.026), ('grid', 0.025), ('fl', 0.024), ('located', 0.023), ('roc', 0.022), ('neighbors', 0.022), ('strictly', 0.022), ('detect', 0.022), ('multiple', 0.021), ('yn', 0.021), ('attainable', 0.021), ('angiulli', 0.02), ('baryshnikov', 0.02), ('breunig', 0.02), ('chandola', 0.02), ('fleury', 0.02), ('multicriteria', 0.02), ('nbl', 0.02), ('pizzuti', 0.02), ('sobel', 0.02), ('yukich', 0.02), ('geometry', 0.02), ('minimizers', 0.02), ('rd', 0.02), ('training', 0.018), ('dominated', 0.018), ('shallow', 0.018), ('test', 0.018), ('byers', 0.018), ('hodge', 0.018), ('sricharan', 0.018), ('saligrama', 0.018), ('eskin', 0.018), ('sample', 0.018), ('combination', 0.017), ('views', 0.016), ('xn', 0.016), ('create', 0.016), ('choices', 0.016), ('near', 0.016), ('deeper', 0.016), ('ated', 0.016), ('raftery', 0.015), ('postponed', 0.015), ('yl', 0.015), ('fj', 0.014), ('declare', 0.014), ('median', 0.014), ('walking', 0.014), ('outlier', 0.013), ('linear', 0.012), ('curve', 0.012), ('surveys', 0.012), ('fth', 0.012), ('dominates', 0.012), ('concentrate', 0.012), ('theorem', 0.012), ('scales', 0.012), ('validated', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="223-tfidf-1" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>Author: Ko-jen Hsiao, Kevin Xu, Jeff Calder, Alfred O. Hero</p><p>Abstract: We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be deﬁned, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria. 1</p><p>2 0.06913162 <a title="223-tfidf-2" href="./nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</a></p>
<p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><p>3 0.049840901 <a title="223-tfidf-3" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>Author: Du Tran, Junsong Yuan</p><p>Abstract: Structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured. Its extension to action localization in videos, however, is much more challenging, because we need to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video. The problem becomes intractable due to the exponentially large size of the structured video space where actions could occur. We propose a novel structured learning approach for spatio-temporal action localization. The mapping between a video and a spatio-temporal action trajectory is learned. The intractable inference and learning problems are addressed by leveraging an efﬁcient Max-Path search method, thus making it feasible to optimize the model over the whole structured space. Experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods. 1</p><p>4 0.04670272 <a title="223-tfidf-4" href="./nips-2012-Diffusion_Decision_Making_for_Adaptive_k-Nearest_Neighbor_Classification.html">97 nips-2012-Diffusion Decision Making for Adaptive k-Nearest Neighbor Classification</a></p>
<p>Author: Yung-kyun Noh, Frank Park, Daniel D. Lee</p><p>Abstract: This paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classiﬁcation. We show that conventional k-nearest neighbor classiﬁcation can be viewed as a special problem of the diffusion decision model in the asymptotic situation. By applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in knearest neighbor classiﬁcation. Making use of the sequential probability ratio test (SPRT) and Bayesian analysis, we propose ﬁve different criteria for adaptively acquiring nearest neighbors. Experiments with both synthetic and real datasets demonstrate the effectiveness of our classiﬁcation criteria. 1</p><p>5 0.04652613 <a title="223-tfidf-5" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>6 0.038580839 <a title="223-tfidf-6" href="./nips-2012-Finding_Exemplars_from_Pairwise_Dissimilarities_via_Simultaneous_Sparse_Recovery.html">133 nips-2012-Finding Exemplars from Pairwise Dissimilarities via Simultaneous Sparse Recovery</a></p>
<p>7 0.036192603 <a title="223-tfidf-7" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>8 0.033736113 <a title="223-tfidf-8" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>9 0.033701144 <a title="223-tfidf-9" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>10 0.033407483 <a title="223-tfidf-10" href="./nips-2012-Learning_Label_Trees_for_Probabilistic_Modelling_of_Implicit_Feedback.html">178 nips-2012-Learning Label Trees for Probabilistic Modelling of Implicit Feedback</a></p>
<p>11 0.032215454 <a title="223-tfidf-11" href="./nips-2012-Context-Sensitive_Decision_Forests_for_Object_Detection.html">81 nips-2012-Context-Sensitive Decision Forests for Object Detection</a></p>
<p>12 0.032097388 <a title="223-tfidf-12" href="./nips-2012-Practical_Bayesian_Optimization_of_Machine_Learning_Algorithms.html">272 nips-2012-Practical Bayesian Optimization of Machine Learning Algorithms</a></p>
<p>13 0.030787801 <a title="223-tfidf-13" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>14 0.029598033 <a title="223-tfidf-14" href="./nips-2012-Gradient_Weights_help_Nonparametric_Regressors.html">145 nips-2012-Gradient Weights help Nonparametric Regressors</a></p>
<p>15 0.029516205 <a title="223-tfidf-15" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>16 0.028953113 <a title="223-tfidf-16" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>17 0.02823597 <a title="223-tfidf-17" href="./nips-2012-Discriminatively_Trained_Sparse_Code_Gradients_for_Contour_Detection.html">101 nips-2012-Discriminatively Trained Sparse Code Gradients for Contour Detection</a></p>
<p>18 0.02816214 <a title="223-tfidf-18" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>19 0.027728904 <a title="223-tfidf-19" href="./nips-2012-Online_L1-Dictionary_Learning_with_Application_to_Novel_Document_Detection.html">258 nips-2012-Online L1-Dictionary Learning with Application to Novel Document Detection</a></p>
<p>20 0.027528541 <a title="223-tfidf-20" href="./nips-2012-Accuracy_at_the_Top.html">30 nips-2012-Accuracy at the Top</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.077), (1, -0.002), (2, -0.022), (3, -0.009), (4, 0.022), (5, -0.041), (6, 0.007), (7, 0.017), (8, -0.001), (9, 0.013), (10, -0.04), (11, -0.007), (12, 0.02), (13, -0.046), (14, 0.021), (15, 0.038), (16, 0.032), (17, -0.008), (18, -0.003), (19, -0.001), (20, 0.033), (21, -0.007), (22, 0.068), (23, -0.006), (24, -0.027), (25, 0.012), (26, 0.054), (27, -0.004), (28, 0.035), (29, -0.045), (30, 0.011), (31, 0.033), (32, 0.018), (33, 0.014), (34, -0.023), (35, 0.026), (36, 0.038), (37, 0.09), (38, -0.018), (39, 0.011), (40, -0.07), (41, 0.018), (42, -0.044), (43, -0.03), (44, 0.013), (45, -0.029), (46, -0.031), (47, 0.007), (48, 0.005), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85420758 <a title="223-lsi-1" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>Author: Ko-jen Hsiao, Kevin Xu, Jeff Calder, Alfred O. Hero</p><p>Abstract: We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be deﬁned, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria. 1</p><p>2 0.60179412 <a title="223-lsi-2" href="./nips-2012-Entropy_Estimations_Using_Correlated_Symmetric_Stable_Random_Projections.html">119 nips-2012-Entropy Estimations Using Correlated Symmetric Stable Random Projections</a></p>
<p>Author: Ping Li, Cun-hui Zhang</p><p>Abstract: Methods for efﬁciently estimating Shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the DDoS attacks). For nonnegative data streams, the method of Compressed Counting (CC) [11, 13] based on maximally-skewed stable random projections can provide accurate estimates of the Shannon entropy using small storage. However, CC is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams. In this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries. In our method, the Shannon entropy is approximated by the ﬁnite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables. Interestingly, the estimator for the moment we recommend for entropy estimation barely has bounded variance itself, whereas the common geometric mean estimator (which has bounded higher-order moments) is not sufﬁcient for entropy estimation. Our experiments conﬁrm that this method is able to well approximate the Shannon entropy using small storage.</p><p>3 0.53980684 <a title="223-lsi-3" href="./nips-2012-Learning_from_the_Wisdom_of_Crowds_by_Minimax_Entropy.html">189 nips-2012-Learning from the Wisdom of Crowds by Minimax Entropy</a></p>
<p>Author: Dengyong Zhou, Sumit Basu, Yi Mao, John C. Platt</p><p>Abstract: An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem. 1</p><p>4 0.4867616 <a title="223-lsi-4" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>Author: Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus D. Plessis, Song Liu, Ichiro Takeuchi</p><p>Abstract: We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of ﬁrst estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the ﬁrst step is performed without regard to the second step and thus a small estimation error incurred in the ﬁrst stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric ﬁnite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. We then show how the proposed density-difference estimator can be utilized in L2 -distance approximation. Finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.</p><p>5 0.48292762 <a title="223-lsi-5" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>Author: Kumar Sricharan, Alfred O. Hero</p><p>Abstract: The problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities. Kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy. However, for large feature dimension d, kernel plug-in estimators suﬀer from the curse of dimensionality: the MSE rate of convergence is glacially slow - of order O(T −γ/d ), where T is the number of samples, and γ > 0 is a rate parameter. In this paper, it is shown that for suﬃciently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric MSE rate of convergence of order O(T −1 ). Furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed oﬄine. This novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suﬀer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates. 1</p><p>6 0.47981003 <a title="223-lsi-6" href="./nips-2012-3D_Object_Detection_and_Viewpoint_Estimation_with_a_Deformable_3D_Cuboid_Model.html">1 nips-2012-3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model</a></p>
<p>7 0.47771674 <a title="223-lsi-7" href="./nips-2012-Localizing_3D_cuboids_in_single-view_images.html">201 nips-2012-Localizing 3D cuboids in single-view images</a></p>
<p>8 0.46427915 <a title="223-lsi-8" href="./nips-2012-Learning_High-Density_Regions_for_a_Generalized_Kolmogorov-Smirnov_Test_in_High-Dimensional_Data.html">175 nips-2012-Learning High-Density Regions for a Generalized Kolmogorov-Smirnov Test in High-Dimensional Data</a></p>
<p>9 0.46180913 <a title="223-lsi-9" href="./nips-2012-Analyzing_3D_Objects_in_Cluttered_Images.html">40 nips-2012-Analyzing 3D Objects in Cluttered Images</a></p>
<p>10 0.4592984 <a title="223-lsi-10" href="./nips-2012-Bayesian_estimation_of_discrete_entropy_with_mixtures_of_stick-breaking_priors.html">57 nips-2012-Bayesian estimation of discrete entropy with mixtures of stick-breaking priors</a></p>
<p>11 0.45241061 <a title="223-lsi-11" href="./nips-2012-Searching_for_objects_driven_by_context.html">303 nips-2012-Searching for objects driven by context</a></p>
<p>12 0.44561735 <a title="223-lsi-12" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>13 0.43077838 <a title="223-lsi-13" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>14 0.4238832 <a title="223-lsi-14" href="./nips-2012-Exponential_Concentration_for_Mutual_Information_Estimation_with_Application_to_Forests.html">123 nips-2012-Exponential Concentration for Mutual Information Estimation with Application to Forests</a></p>
<p>15 0.42366666 <a title="223-lsi-15" href="./nips-2012-Dynamical_And-Or_Graph_Learning_for_Object_Shape_Modeling_and_Detection.html">106 nips-2012-Dynamical And-Or Graph Learning for Object Shape Modeling and Detection</a></p>
<p>16 0.42157149 <a title="223-lsi-16" href="./nips-2012-Accuracy_at_the_Top.html">30 nips-2012-Accuracy at the Top</a></p>
<p>17 0.41888157 <a title="223-lsi-17" href="./nips-2012-Automatic_Feature_Induction_for_Stagewise_Collaborative_Filtering.html">49 nips-2012-Automatic Feature Induction for Stagewise Collaborative Filtering</a></p>
<p>18 0.41850927 <a title="223-lsi-18" href="./nips-2012-Unsupervised_Template_Learning_for_Fine-Grained_Object_Recognition.html">357 nips-2012-Unsupervised Template Learning for Fine-Grained Object Recognition</a></p>
<p>19 0.4183822 <a title="223-lsi-19" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>20 0.41623834 <a title="223-lsi-20" href="./nips-2012-Bayesian_nonparametric_models_for_bipartite_graphs.html">59 nips-2012-Bayesian nonparametric models for bipartite graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.036), (1, 0.373), (11, 0.01), (17, 0.012), (21, 0.021), (38, 0.105), (42, 0.033), (49, 0.013), (55, 0.016), (74, 0.066), (76, 0.105), (80, 0.048), (92, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68416953 <a title="223-lda-1" href="./nips-2012-Multi-criteria_Anomaly_Detection_using_Pareto_Depth_Analysis.html">223 nips-2012-Multi-criteria Anomaly Detection using Pareto Depth Analysis</a></p>
<p>Author: Ko-jen Hsiao, Kevin Xu, Jeff Calder, Alfred O. Hero</p><p>Abstract: We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be deﬁned, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria. 1</p><p>2 0.66040879 <a title="223-lda-2" href="./nips-2012-Patient_Risk_Stratification_for_Hospital-Associated_C._diff_as_a_Time-Series_Classification_Task.html">266 nips-2012-Patient Risk Stratification for Hospital-Associated C. diff as a Time-Series Classification Task</a></p>
<p>Author: Jenna Wiens, Eric Horvitz, John V. Guttag</p><p>Abstract: A patient’s risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient’s pathophysiology over time. Yet many investigators ignore this temporal aspect when modeling patient outcomes, considering only the patient’s current or aggregate state. In this paper, we represent patient risk as a time series. In doing so, patient risk stratiﬁcation becomes a time-series classiﬁcation task. The task differs from most applications of time-series analysis, like speech processing, since the time series itself must ﬁrst be extracted. Thus, we begin by deﬁning and extracting approximate risk processes, the evolving approximate daily risk of a patient. Once obtained, we use these signals to explore different approaches to time-series classiﬁcation with the goal of identifying high-risk patterns. We apply the classiﬁcation to the speciﬁc task of identifying patients at risk of testing positive for hospital acquired Clostridium difﬁcile. We achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients. Our two-stage approach to risk stratiﬁcation outperforms classiﬁers that consider only a patient’s current state (p<0.05). 1</p><p>3 0.56460297 <a title="223-lda-3" href="./nips-2012-Approximate_Message_Passing_with_Consistent_Parameter_Estimation_and_Applications_to_Sparse_Learning.html">43 nips-2012-Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning</a></p>
<p>Author: Ulugbek Kamilov, Sundeep Rangan, Michael Unser, Alyson K. Fletcher</p><p>Abstract: We consider the estimation of an i.i.d. vector x ∈ Rn from measurements y ∈ Rm obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. We present a method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector x. Our method can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identiﬁcation of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. This analysis shows that the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. The adaptive GAMP methodology thus provides a systematic, general and computationally efﬁcient method applicable to a large range of complex linear-nonlinear models with provable guarantees. 1</p><p>4 0.52976227 <a title="223-lda-4" href="./nips-2012-Best_Arm_Identification%3A_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence.html">61 nips-2012-Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence</a></p>
<p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Alessandro Lazaric</p><p>Abstract: We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: ﬁxed budget and ﬁxed conﬁdence. We propose a unifying approach that leads to a meta-algorithm called uniﬁed gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing ﬁxed budget and ﬁxed conﬁdence algorithms. 1</p><p>5 0.5231207 <a title="223-lda-5" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>6 0.50889558 <a title="223-lda-6" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>7 0.4773052 <a title="223-lda-7" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>8 0.44306201 <a title="223-lda-8" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>9 0.42640087 <a title="223-lda-9" href="./nips-2012-Predicting_Action_Content_On-Line_and_in_Real_Time_before_Action_Onset_%E2%80%93_an_Intracranial_Human_Study.html">273 nips-2012-Predicting Action Content On-Line and in Real Time before Action Onset – an Intracranial Human Study</a></p>
<p>10 0.42496136 <a title="223-lda-10" href="./nips-2012-Online_Sum-Product_Computation_Over_Trees.html">260 nips-2012-Online Sum-Product Computation Over Trees</a></p>
<p>11 0.42311996 <a title="223-lda-11" href="./nips-2012-Learning_Image_Descriptors_with_the_Boosting-Trick.html">176 nips-2012-Learning Image Descriptors with the Boosting-Trick</a></p>
<p>12 0.4212718 <a title="223-lda-12" href="./nips-2012-Natural_Images%2C_Gaussian_Mixtures_and_Dead_Leaves.html">235 nips-2012-Natural Images, Gaussian Mixtures and Dead Leaves</a></p>
<p>13 0.42112359 <a title="223-lda-13" href="./nips-2012-Small-Variance_Asymptotics_for_Exponential_Family_Dirichlet_Process_Mixture_Models.html">316 nips-2012-Small-Variance Asymptotics for Exponential Family Dirichlet Process Mixture Models</a></p>
<p>14 0.42098016 <a title="223-lda-14" href="./nips-2012-Clustering_Sparse_Graphs.html">69 nips-2012-Clustering Sparse Graphs</a></p>
<p>15 0.42063543 <a title="223-lda-15" href="./nips-2012-Hierarchical_Optimistic_Region_Selection_driven_by_Curiosity.html">149 nips-2012-Hierarchical Optimistic Region Selection driven by Curiosity</a></p>
<p>16 0.42004415 <a title="223-lda-16" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>17 0.41973948 <a title="223-lda-17" href="./nips-2012-Efficient_Spike-Coding_with_Multiplicative_Adaptation_in_a_Spike_Response_Model.html">112 nips-2012-Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model</a></p>
<p>18 0.41954792 <a title="223-lda-18" href="./nips-2012-Hamming_Distance_Metric_Learning.html">148 nips-2012-Hamming Distance Metric Learning</a></p>
<p>19 0.41899616 <a title="223-lda-19" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>20 0.41895813 <a title="223-lda-20" href="./nips-2012-Efficient_Sampling_for_Bipartite_Matching_Problems.html">111 nips-2012-Efficient Sampling for Bipartite Matching Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
