<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-350" href="#">nips2012-350</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</h1>
<br/><p>Source: <a title="nips-2012-350-pdf" href="http://papers.nips.cc/paper/4816-trajectory-based-short-sighted-probabilistic-planning.pdf">pdf</a></p><p>Author: Felipe Trevizan, Manuela Veloso</p><p>Abstract: Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artiﬁcial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP always ﬁnishes and is asymptotically optimal under sufﬁcient conditions on the structure of short-sighted SSPs. We empirically compare SSiPP using trajectorybased short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states. 1</p><p>Reference: <a title="nips-2012-350-reference" href="../nips2012_reference/nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. [sent-5, score-0.458]
</p><p>2 In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. [sent-6, score-0.584]
</p><p>3 We empirically compare SSiPP using trajectorybased short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. [sent-12, score-0.66]
</p><p>4 Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states. [sent-13, score-0.194]
</p><p>5 1  Introduction  The uncertainty of plan execution can be modeled by using probabilistic effects in actions, and therefore the probability of reaching different states from a given state and action. [sent-14, score-0.295]
</p><p>6 This search space, deﬁned by the probabilistic paths from the initial state to a goal state, challenges the scalability of planners. [sent-15, score-0.187]
</p><p>7 Planners manage the uncertainty by choosing a search strategy to explore the space. [sent-16, score-0.178]
</p><p>8 In this work, we present a novel approach to manage uncertainty for probabilistic planning problems that improves its scalability while still being optimal. [sent-17, score-0.347]
</p><p>9 One approach to manage uncertainty while searching for the solution of probabilistic planning problems is to consider the complete search space at once. [sent-18, score-0.379]
</p><p>10 Examples of such algorithms are value iteration and policy iteration [2]. [sent-19, score-0.119]
</p><p>11 , a universal mapping function from every state to the optimal action that leads to a goal state. [sent-22, score-0.16]
</p><p>12 Assuming the model correctly captures the cost and uncertainty of the actions in the environment, closed policies are extremely powerful as their execution never “fails,” and the planner does not need to be re-invoked ever. [sent-23, score-0.388]
</p><p>13 Value iteration based probabilistic planners can be improved by combining asynchronous updates and heuristic search [3–7]. [sent-25, score-0.291]
</p><p>14 Although these techniques allow planners to compute compact policies, in the worst case, these policies are still linear in the size of the state space, which itself can be exponential in the size of the state or goals. [sent-26, score-0.293]
</p><p>15 1  Another approach to manage uncertainty is basically to ignore uncertainty during planning, i. [sent-27, score-0.196]
</p><p>16 Besides the action space simpliﬁcation, uncertainty management can be performed by simplifying the problem horizon, i. [sent-34, score-0.099]
</p><p>17 The state space can also be simpliﬁed to manage uncertainty in probabilistic planning. [sent-38, score-0.25]
</p><p>18 EP computes an initial partial policy π and then prunes all the states not considered by π. [sent-40, score-0.201]
</p><p>19 In this paper, we introduced trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel model to manage uncertainty in probabilistic planning problems. [sent-44, score-0.329]
</p><p>20 Trajectory-based short-sighted SSPs manage uncertainty by pruning the state space based on the most likely trajectory between states and deﬁning artiﬁcial goal states that guide the solution towards the original goal. [sent-45, score-0.438]
</p><p>21 A policy π deﬁned only for the states reachable from s0 when following π is a closed policy w. [sent-57, score-0.371]
</p><p>22 For instance, in the SSP depicted in Figure 1(a), the policy π0 = {(s0 , a0 ), (s′ , a0 ), (s′ , a0 ), (s′ , a0 )} is a closed policy 1 2 3 w. [sent-61, score-0.26]
</p><p>23 1 2 3 Given a policy π, we deﬁne trajectory as a sequence Tπ = s(0) , . [sent-65, score-0.155]
</p><p>24 The probability of a trai<|T | jectory Tπ is deﬁned as P (Tπ ) = i=0 π P (s(i+1) |s(i) , π(s(i) )) and maximum probability of a trajectory between two states Pmax (s, s′ ) is deﬁned as maxπ P (Tπ = s, . [sent-69, score-0.1]
</p><p>25 An optimal policy π ∗ for an SSP is any policy that always reaches a goal state when followed from s0 and also minimizes the expected cost of Tπ∗ . [sent-73, score-0.418]
</p><p>26 , the mapping from states to the minimum expected cost to reach a goal state, is unique. [sent-76, score-0.125]
</p><p>27 The initial state is s0 , the goal state is sG , C(s, a, s ) = 1, ∀s ∈ S, a ∈ A and s′ ∈ S. [sent-131, score-0.171]
</p><p>28 Given an SSP S, if a goal state can be reached with positive probability from every state s ∈ S, then the reachability assumption (Deﬁnition 1) holds for S and 0 ≤ V ∗ (s) < ∞ [19]. [sent-137, score-0.265]
</p><p>29 Once V ∗ is known, any optimal policy π ∗ can be extracted from V ∗ by substituting the operator min by argmin in equation (1). [sent-138, score-0.137]
</p><p>30 Given an SSP S = S, s0 , G, A, P, C , a state s ∈ S, ρ ∈ [0, 1] and a heuristic H, the (s, ρ)-trajectory-based short-sighted SSP Ss,ρ = Ss,ρ , s, Gs,ρ , A, P, Cs,ρ associated with S is deﬁned as: • Ss,ρ = {s′ ∈ S|∃ˆ ∈ S and a ∈ A s. [sent-164, score-0.098]
</p><p>31 Our novel model, the trajectory-based short-sighted SSPs (Deﬁnition 4), addresses the issue of states with low trajectory probability by explicitly deﬁning its state space Ss,ρ based on maximum probability of a trajectory between s and the candidate states s′ (Pmax (s, s′ )). [sent-167, score-0.26]
</p><p>32 This example 1 2 3 shows how trajectory-based short-sighted SSP can manage uncertainty efﬁciently: for ρ = 0. [sent-172, score-0.146]
</p><p>33 Notice that the deﬁnition of Ss,ρ cannot be simpliﬁed to {ˆ ∈ S|Pmax (s, s) ≥ ρ} since not all s ˆ the resulting states of actions would be included in Ss,ρ . [sent-174, score-0.132]
</p><p>34 Due to the reduced size of the short-sighted problems, SSiPP solves each of them by computing a closed policy w. [sent-181, score-0.141]
</p><p>35 Alternatively, an anytime behavior is obtained if the execution of the computed closed policy for the short-sighted SSP is simulated (Algorithm 1 line 4) until an artiﬁcial goal sa is reached and this procedure is repeated, starting sa , until convergence or an interruption. [sent-186, score-0.267]
</p><p>36 In [1], we proved that SSiPP always terminates and is asymptotically optimal for depth-based shortsighted SSPs. [sent-187, score-0.132]
</p><p>37 O PTIMAL -SSP-S OLVER returns an optimal policy π ∗ w. [sent-200, score-0.137]
</p><p>38 Given an SSP S = S, s0 , G, A, P, C such that the reachability assumption holds, an admissible heuristic H and a short-sighted problem generator that respects Deﬁnition 5, then SSiPP always terminates. [sent-213, score-0.187]
</p><p>39 Since O PTIMAL -S OLVER always ﬁnishes and the short-sighted SSP is an SSP by deﬁnition, then a goal state sG of the short-sighted SSP is always reached, therefore the loop in line 3 of Algorithm 1 always ﬁnishes. [sent-215, score-0.15]
</p><p>40 , sG differs from the state s used as initial state for the short-sighted SSP generation. [sent-219, score-0.138]
</p><p>41 Suppose, for contradiction purpose, that every goal state reached during SSiPP execution is an artiﬁcial goal, i. [sent-221, score-0.186]
</p><p>42 Thus every execution of SSiPP reaches a goal state s′ ∈ G and therefore G terminates. [sent-228, score-0.186]
</p><p>43 Clearly, S(π ∗ , s0 ) ⊆ S∗ since a partial policy cannot be executed ad inﬁnitum without reaching a state in which it is not deﬁned. [sent-233, score-0.23]
</p><p>44 Since SSiPP performs Bellman updates in the original SSP space (Lemma 2) and every execution of SSiPP terminates (Theorem 3), then we can view the sequence of lower bounds V 0 , V 1 , · · · , V t generated by SSiPP as asynchronous value iteration. [sent-234, score-0.114]
</p><p>45 5  80  10  70  10  60  Number of States (log scale)  10  50  10  40  10  30  10  20  10  10  10  |S(π*,s )| 0  |S| 0  10  0  (a)  5  10  15  20  25 30 35 40 Triangle Tireworld Problem Size  45  50  55  60  (b)  Figure 2: (a) Map of the triangle tireworld for the sizes 1, 2 and 3. [sent-238, score-0.278]
</p><p>46 Circles (squares) represent locations in which there is one (no) spare tire. [sent-239, score-0.121]
</p><p>47 The shades of gray represent, for each location l, maxπ P (car reaches l and the tire is not ﬂat when following the policy π from s0 ). [sent-240, score-0.313]
</p><p>48 (b) Log-lin plot of the state space size (|S|) and the size of the states reachable from s0 when following the optimal policy π ∗ (|S(π ∗ , s0 )|) versus the number of the triangle tireworld problem. [sent-241, score-0.586]
</p><p>49 5  Experiments  We present two sets of experiments using the triangle tireworld problems [9, 11, 20], a series of probabilistic interesting problems [14] in which a car has to travel between locations in order to reach a goal location from its initial location. [sent-242, score-0.594]
</p><p>50 The roads are represented as directed graph in a shape of a triangle and, every time the car moves between locations, a ﬂat tire happens with probability 0. [sent-243, score-0.255]
</p><p>51 Some locations have a spare tire and in these locations the car can deterministically replace its ﬂat tire by new one. [sent-245, score-0.41]
</p><p>52 When the car has a ﬂat tire, it cannot change its location, therefore the car can get stuck in locations that do not have a spare tire (dead-ends). [sent-246, score-0.341]
</p><p>53 Figure 2(a) depicts the map of the triangle tireworld problems 1, 2 and 3 and Figure 2(b) shows the size of S and S(π ∗ , s0 ) for problems up to size 60. [sent-247, score-0.314]
</p><p>54 , 28 nodes in the corresponding graph on Figure 2(a), its state space has 19562 states and its optimal policy reaches 8190 states. [sent-250, score-0.33]
</p><p>55 Every triangle tireworld problem is a probabilistic interesting problem [14]: there is only one policy that reaches the goal with probability 1 and all the other policies have probability at most 0. [sent-251, score-0.553]
</p><p>56 This property is illustrated by the shades of gray in Figure 2(a) that represents, for each location l, maxπ P (car reaches l and the tire is not ﬂat when following the policy π from s0 ). [sent-255, score-0.313]
</p><p>57 For UCT, we disabled the random rollouts because the probability of any policy other than the optimal policy to reach a dead-end is at least 0. [sent-260, score-0.284]
</p><p>58 The solution computed during one round is simulated by MDPSIM in a client-server loop: MDPSIM sends a state s and requests an action from the planner, then the planner replies by sending the action a to be executed in s. [sent-267, score-0.381]
</p><p>59 The evaluation is done by the number of rounds simulated by MDPSIM that reached a goal state. [sent-268, score-0.169]
</p><p>60 The maximum number of actions allowed per round is 2000 and rounds that exceed this limit are stopped by MDPSIM and declared as failure, i. [sent-269, score-0.184]
</p><p>61 0  Table 1: Number of rounds solved out of 50 for experiment in Section 5. [sent-308, score-0.102]
</p><p>62 9  Table 2: Number of rounds solved out of 50 for experiment in Section 5. [sent-419, score-0.102]
</p><p>63 Formally, to decide what action to apply in a given state s, each planner is allowed to use at most B = |Ss,t | search nodes, i. [sent-430, score-0.317]
</p><p>64 We choose t equals to 8 since it obtains the best performance in the triangle tireworld problems [1]. [sent-433, score-0.296]
</p><p>65 Given the search nodes budget B, for UCT we sample the environment until the search tree contains B nodes; and for trajectory-based SSiPP we use ρ = argmaxρ {|Ss,ρ | s. [sent-434, score-0.105]
</p><p>66 The methodology for this experiment is as follows: for each problem, 10 runs of 50 rounds are performed for each planner using the search nodes budget B. [sent-437, score-0.354]
</p><p>67 We set as time and memory cut-off 8 hours and 8 Gb, respectively, and UCT for problems 35 to 60 was the only planner preempted by the time cut-off. [sent-439, score-0.194]
</p><p>68 Trace-based SSiPP outperforms both depth-based SSiPP and UCT, solving all the 50 rounds in all the 10 runs for all the problems. [sent-440, score-0.111]
</p><p>69 2  Fixed maximum planning time  In this experiment, we compare planners by limiting the maximum planning time. [sent-442, score-0.422]
</p><p>70 The methodology used in this experiment is similar to the one in IPPC’04 and IPPC’06: for each problem, planners need to solve 1 run of 50 rounds in 20 minutes. [sent-443, score-0.246]
</p><p>71 For this experiment, the planners are allowed to perform internal simulations, for instance, a planner can spend 15 minutes solving rounds using internal simulations and then use the computed policy to solve the required 50 rounds through MDPSIM in the remaining 5 minutes. [sent-444, score-0.611]
</p><p>72 The winners of IPPC’04, IPPC’06 and IPPC’08 are 7  omitted since their performance on the triangle tireworld problems are strictly dominated by depthbase SSiPP for t = 8. [sent-451, score-0.327]
</p><p>73 All the four parametrizations of trajectory-based SSiPP outperform the other planners for problems of size equal or greater than 45. [sent-453, score-0.193]
</p><p>74 , it reaches a goal state in all the 50 rounds in all the 10 runs for all the problems. [sent-457, score-0.254]
</p><p>75 125 reaches the 20 minutes time cut-off before solving 50 rounds, however all the solved rounds successfully reach the goal. [sent-461, score-0.164]
</p><p>76 This interesting behavior of trajectory-based SSiPP for the triangle tireworld can be explained by the following theorem: Theorem 5. [sent-462, score-0.278]
</p><p>77 For the triangle tireworld, trajectory-based SSiPP using an admissible heuristic never falls in a dead-end for ρ ∈ (0. [sent-463, score-0.183]
</p><p>78 The optimal policy for the triangle tireworld is to follow the longest path: move from the initial location l0 to the goal location lG passing through location lc , where l0 , lc and lG are the vertices of the triangle formed by the problem’s map. [sent-470, score-0.809]
</p><p>79 , there is only one applicable move-car action for all the locations in this path. [sent-473, score-0.099]
</p><p>80 Therefore all the decision making to ﬁnd the optimal policy happens between the locations l0 and lc . [sent-474, score-0.24]
</p><p>81 Each location l′ in the path from l0 to lc has either two or three applicable move-car actions and we refer to the set of locations l′ with three applicable move-car actions as N. [sent-475, score-0.312]
</p><p>82 Every location l′ ∈ N is reachable from l0 by applying an even number of move-car actions (Figure 2(a)) and the three applicable move-car actions in l′ are: (i) the optimal action ac , i. [sent-476, score-0.339]
</p><p>83 , move the car towards lc ; (ii) the action aG that moves the car towards lG ; and (iii) the action ap that moves the car parallel to the shortest-path from l0 to lG . [sent-478, score-0.384]
</p><p>84 The location reached by ap does not have a spare tire, therefore ap is never selected by a greedy choice over any admissible heuristic since it reaches a dead-end with probability 0. [sent-479, score-0.326]
</p><p>85 The locations reached by applying either ac or aG have a spare tire and the greedy choice between them depends on the admissible heuristic used, thus aG might be selected instead of ac . [sent-481, score-0.436]
</p><p>86 However, after applying aG , only one move-car action a is available and it reaches a location that does not have a spare tire. [sent-482, score-0.21]
</p><p>87 Therefore, the greedy choice between ac and aG considering two or more move-car actions is optimal under any admissible heuristic: every sequence of actions aG , a, . [sent-483, score-0.246]
</p><p>88 5 and at least one sequence of actions starting with ac has probability 0 to reach a dead-end, e. [sent-487, score-0.145]
</p><p>89 Given ρ, we denote as Ls,ρ the set of all locations corresponding to states in Ss,ρ and as ls the location corresponding to the state s. [sent-490, score-0.262]
</p><p>90 Thus, Ls,ρ contains all the locations reachable from ls using up to m = ⌊log0. [sent-491, score-0.145]
</p><p>91 If m is even and ls ∈ N, then every location in Ls,ρ ∩ N represents a state either in Gs,ρ or at least two move-car actions away from any state in Gs,ρ . [sent-493, score-0.276]
</p><p>92 Therefore the solution of the (s, ρ)-trajectory-based short-sighted SSP only chooses the action ac to move the car. [sent-494, score-0.113]
</p><p>93 Also, since m is even, every state s used by SSiPP for generating (s, ρ)-trajectory-based short-sighted SSPs has ls ∈ N. [sent-495, score-0.108]
</p><p>94 }, trajectory-based SSiPP always chooses the actions ac to move the car to lc , thus avoiding the all dead-ends. [sent-503, score-0.271]
</p><p>95 6  Conclusion  In this paper, we introduced trajectory-based short-sighted SSPs, a new model to manage uncertainty in probabilistic planning problems. [sent-504, score-0.329]
</p><p>96 This approach consists of pruning the state space based on the most likely trajectory between states and deﬁning artiﬁcial goal states that guide the solution towards the original goals. [sent-505, score-0.292]
</p><p>97 We empirically compared trajectory-based SSiPP with depth-based SSiPP and other state-of-the-art planners in the triangle tireworld. [sent-507, score-0.246]
</p><p>98 Trajectory-based SSiPP outperforms all the other planners and it is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states, under the IPPC evaluation methodology. [sent-508, score-0.338]
</p><p>99 The ﬁrst probabilistic track of the international planning competition. [sent-575, score-0.183]
</p><p>100 RFF: A robust, FF-based mdp planning algorithm for generating policies with low probability of failure. [sent-581, score-0.168]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ssipp', 0.635), ('ssp', 0.363), ('ssps', 0.306), ('sg', 0.189), ('planner', 0.176), ('tireworld', 0.176), ('planners', 0.144), ('planning', 0.139), ('uct', 0.135), ('policy', 0.119), ('pmax', 0.106), ('triangle', 0.102), ('ss', 0.098), ('manage', 0.096), ('ippc', 0.094), ('tire', 0.086), ('rounds', 0.086), ('mdpsim', 0.071), ('spare', 0.071), ('actions', 0.068), ('car', 0.067), ('states', 0.064), ('reachability', 0.062), ('state', 0.06), ('determinization', 0.059), ('icaps', 0.059), ('ptimal', 0.059), ('bellman', 0.055), ('lc', 0.053), ('olver', 0.052), ('uncertainty', 0.05), ('reached', 0.05), ('reaches', 0.05), ('locations', 0.05), ('action', 0.049), ('ac', 0.049), ('lg', 0.048), ('ls', 0.048), ('reachable', 0.047), ('probabilistic', 0.044), ('admissible', 0.043), ('execution', 0.043), ('location', 0.04), ('ag', 0.039), ('params', 0.038), ('shortest', 0.038), ('terminates', 0.038), ('heuristic', 0.038), ('trajectory', 0.036), ('lrtdp', 0.035), ('shortsighted', 0.035), ('gs', 0.035), ('scheduling', 0.034), ('reaching', 0.034), ('path', 0.033), ('goal', 0.033), ('search', 0.032), ('yoon', 0.031), ('parametrizations', 0.031), ('winners', 0.031), ('round', 0.03), ('competition', 0.029), ('policies', 0.029), ('arti', 0.028), ('reach', 0.028), ('mina', 0.027), ('nition', 0.026), ('cial', 0.025), ('runs', 0.025), ('generator', 0.025), ('bonet', 0.024), ('enerate', 0.024), ('ighted', 0.024), ('rtdp', 0.024), ('trajectorybased', 0.024), ('trevizan', 0.024), ('automated', 0.023), ('heuristically', 0.022), ('environment', 0.022), ('asymptotically', 0.022), ('closed', 0.022), ('nodes', 0.019), ('always', 0.019), ('pruning', 0.019), ('problems', 0.018), ('initial', 0.018), ('shades', 0.018), ('optimal', 0.018), ('admissibility', 0.017), ('ap', 0.017), ('executed', 0.017), ('dence', 0.017), ('updates', 0.017), ('asynchronous', 0.016), ('goals', 0.016), ('guide', 0.016), ('fern', 0.016), ('nishes', 0.016), ('experiment', 0.016), ('move', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="350-tfidf-1" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>Author: Felipe Trevizan, Manuela Veloso</p><p>Abstract: Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artiﬁcial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP always ﬁnishes and is asymptotically optimal under sufﬁcient conditions on the structure of short-sighted SSPs. We empirically compare SSiPP using trajectorybased short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states. 1</p><p>2 0.10719752 <a title="350-tfidf-2" href="./nips-2012-A_Stochastic_Gradient_Method_with_an_Exponential_Convergence__Rate_for_Finite_Training_Sets.html">20 nips-2012-A Stochastic Gradient Method with an Exponential Convergence  Rate for Finite Training Sets</a></p>
<p>Author: Nicolas L. Roux, Mark Schmidt, Francis R. Bach</p><p>Abstract: We propose a new stochastic gradient method for optimizing the sum of a ﬁnite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly. 1</p><p>3 0.10621543 <a title="350-tfidf-3" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>Author: Arthur Guez, David Silver, Peter Dayan</p><p>Abstract: Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, ﬁnding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayesoptimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a signiﬁcant margin on several well-known benchmark problems – because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an inﬁnite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration. 1</p><p>4 0.095932044 <a title="350-tfidf-4" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>Author: Abdeslam Boularias, Jan R. Peters, Oliver B. Kroemer</p><p>Abstract: We use a graphical model for representing policies in Markov Decision Processes. This new representation can easily incorporate domain knowledge in the form of a state similarity graph that loosely indicates which states are supposed to have similar optimal actions. A bias is then introduced into the policy search process by sampling policies from a distribution that assigns high probabilities to policies that agree with the provided state similarity graph, i.e. smoother policies. This distribution corresponds to a Markov Random Field. We also present forward and inverse reinforcement learning algorithms for learning such policy distributions. We illustrate the advantage of the proposed approach on two problems: cart-balancing with swing-up, and teaching a robot to grasp unknown objects. 1</p><p>5 0.089520775 <a title="350-tfidf-5" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>Author: Sergey Karayev, Tobias Baumgartner, Mario Fritz, Trevor Darrell</p><p>Abstract: In a large visual multi-class detection framework, the timeliness of results can be crucial. Our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time. Toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next. In contrast to previous work, our method signiﬁcantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values. We evaluate our method with a novel timeliness measure, computed as the area under an Average Precision vs. Time curve. Experiments are conducted on the PASCAL VOC object detection dataset. If execution is stopped when only half the detectors have been run, our method obtains 66% better AP than a random ordering, and 14% better performance than an intelligent baseline. On the timeliness measure, our method obtains at least 11% better performance. Our method is easily extensible, as it treats detectors and classiﬁers as black boxes and learns from execution traces using reinforcement learning. 1</p><p>6 0.088210776 <a title="350-tfidf-6" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>7 0.085683063 <a title="350-tfidf-7" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>8 0.084899686 <a title="350-tfidf-8" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>9 0.077732049 <a title="350-tfidf-9" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>10 0.072514065 <a title="350-tfidf-10" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>11 0.071544677 <a title="350-tfidf-11" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>12 0.071088567 <a title="350-tfidf-12" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>13 0.058677621 <a title="350-tfidf-13" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>14 0.056761645 <a title="350-tfidf-14" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>15 0.050321493 <a title="350-tfidf-15" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>16 0.049410179 <a title="350-tfidf-16" href="./nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<p>17 0.048329435 <a title="350-tfidf-17" href="./nips-2012-A_Unifying_Perspective_of_Parametric_Policy_Search_Methods_for_Markov_Decision_Processes.html">21 nips-2012-A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes</a></p>
<p>18 0.045648638 <a title="350-tfidf-18" href="./nips-2012-MCMC_for_continuous-time_discrete-state_systems.html">205 nips-2012-MCMC for continuous-time discrete-state systems</a></p>
<p>19 0.04280366 <a title="350-tfidf-19" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>20 0.041970007 <a title="350-tfidf-20" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.088), (1, -0.169), (2, -0.012), (3, -0.026), (4, -0.018), (5, -0.006), (6, -0.001), (7, -0.037), (8, -0.003), (9, -0.003), (10, -0.014), (11, -0.028), (12, 0.01), (13, 0.004), (14, -0.006), (15, -0.0), (16, -0.032), (17, -0.006), (18, -0.025), (19, 0.05), (20, -0.018), (21, 0.021), (22, -0.004), (23, -0.027), (24, 0.019), (25, 0.003), (26, -0.003), (27, 0.0), (28, 0.008), (29, 0.024), (30, 0.006), (31, -0.026), (32, -0.013), (33, 0.01), (34, -0.009), (35, 0.002), (36, 0.004), (37, 0.034), (38, -0.034), (39, 0.043), (40, 0.058), (41, -0.064), (42, 0.033), (43, -0.004), (44, -0.036), (45, -0.04), (46, -0.021), (47, 0.061), (48, 0.015), (49, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90440643 <a title="350-lsi-1" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>Author: Felipe Trevizan, Manuela Veloso</p><p>Abstract: Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artiﬁcial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP always ﬁnishes and is asymptotically optimal under sufﬁcient conditions on the structure of short-sighted SSPs. We empirically compare SSiPP using trajectorybased short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states. 1</p><p>2 0.70145422 <a title="350-lsi-2" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>Author: Katherine Chen, Michael Bowling</p><p>Abstract: Robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making. When faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance. One might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations. In this paper, we take a Bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty. Instead we focus on identifying optimization objectives for which solutions can be efﬁciently approximated. We introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable. We then introduce a broad subclass of this family for which robust policies can be approximated efﬁciently. Finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the MDP. 1</p><p>3 0.6983484 <a title="350-lsi-3" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>Author: Arthur Guez, David Silver, Peter Dayan</p><p>Abstract: Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, ﬁnding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayesoptimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a signiﬁcant margin on several well-known benchmark problems – because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an inﬁnite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration. 1</p><p>4 0.6809839 <a title="350-lsi-4" href="./nips-2012-On_the_Use_of_Non-Stationary_Policies_for_Stationary_Infinite-Horizon_Markov_Decision_Processes.html">255 nips-2012-On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes</a></p>
<p>Author: Bruno Scherrer, Boris Lesner</p><p>Abstract: We consider inﬁnite-horizon stationary γ-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error at each iteration, it is well-known that one 2γ can compute stationary policies that are (1−γ)2 -optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for com2γ puting non-stationary policies that can be up to 1−γ -optimal, which constitutes a signiﬁcant improvement in the usual situation when γ is close to 1. Surprisingly, this shows that the problem of “computing near-optimal non-stationary policies” is much simpler than that of “computing near-optimal stationary policies”. 1</p><p>5 0.67952263 <a title="350-lsi-5" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>Author: Jiarong Jiang, Adam Teichert, Jason Eisner, Hal Daume</p><p>Abstract: Users want inference to be both fast and accurate, but quality often comes at the cost of speed. The ﬁeld has experimented with approximate inference algorithms that make different speed-accuracy tradeoffs (for particular problems and datasets). We aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing [12]. Unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively. An attempt to counteract this by applying imitation learning algorithms also fails: the “teacher” follows a far better policy than anything in our learner’s policy space, free of the speed-accuracy tradeoff that arises when oracle information is unavailable, and thus largely insensitive to the known reward functﬁon. We propose a hybrid reinforcement/apprenticeship learning algorithm that learns to speed up an initial policy, trading off accuracy for speed according to various settings of a speed term in the loss function. 1</p><p>6 0.67253739 <a title="350-lsi-6" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>7 0.66277373 <a title="350-lsi-7" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>8 0.63039118 <a title="350-lsi-8" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<p>9 0.60663068 <a title="350-lsi-9" href="./nips-2012-A_Bayesian_Approach_for_Policy_Learning_from_Trajectory_Preference_Queries.html">3 nips-2012-A Bayesian Approach for Policy Learning from Trajectory Preference Queries</a></p>
<p>10 0.59858328 <a title="350-lsi-10" href="./nips-2012-Exploration_in_Model-based_Reinforcement_Learning_by_Empirically_Estimating_Learning_Progress.html">122 nips-2012-Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress</a></p>
<p>11 0.58497411 <a title="350-lsi-11" href="./nips-2012-Bayesian_Hierarchical_Reinforcement_Learning.html">51 nips-2012-Bayesian Hierarchical Reinforcement Learning</a></p>
<p>12 0.56536072 <a title="350-lsi-12" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>13 0.56497037 <a title="350-lsi-13" href="./nips-2012-Non-parametric_Approximate_Dynamic_Programming_via_the_Kernel_Method.html">243 nips-2012-Non-parametric Approximate Dynamic Programming via the Kernel Method</a></p>
<p>14 0.56473976 <a title="350-lsi-14" href="./nips-2012-Online_Regret_Bounds_for_Undiscounted_Continuous_Reinforcement_Learning.html">259 nips-2012-Online Regret Bounds for Undiscounted Continuous Reinforcement Learning</a></p>
<p>15 0.5588485 <a title="350-lsi-15" href="./nips-2012-Weighted_Likelihood_Policy_Search_with_Model_Selection.html">364 nips-2012-Weighted Likelihood Policy Search with Model Selection</a></p>
<p>16 0.55630672 <a title="350-lsi-16" href="./nips-2012-Risk_Aversion_in_Markov_Decision_Processes_via_Near_Optimal_Chernoff_Bounds.html">296 nips-2012-Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds</a></p>
<p>17 0.55028409 <a title="350-lsi-17" href="./nips-2012-On-line_Reinforcement_Learning_Using_Incremental_Kernel-Based_Stochastic_Factorization.html">250 nips-2012-On-line Reinforcement Learning Using Incremental Kernel-Based Stochastic Factorization</a></p>
<p>18 0.53450364 <a title="350-lsi-18" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>19 0.53447509 <a title="350-lsi-19" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>20 0.52949429 <a title="350-lsi-20" href="./nips-2012-Efficient_Monte_Carlo_Counterfactual_Regret_Minimization_in_Games_with_Many_Player_Actions.html">109 nips-2012-Efficient Monte Carlo Counterfactual Regret Minimization in Games with Many Player Actions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (17, 0.016), (20, 0.368), (21, 0.012), (38, 0.079), (42, 0.031), (54, 0.071), (55, 0.028), (74, 0.046), (76, 0.101), (80, 0.087), (92, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67705601 <a title="350-lda-1" href="./nips-2012-Trajectory-Based_Short-Sighted_Probabilistic_Planning.html">350 nips-2012-Trajectory-Based Short-Sighted Probabilistic Planning</a></p>
<p>Author: Felipe Trevizan, Manuela Veloso</p><p>Abstract: Probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action. In order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state. Several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling. In this paper, we introduce trajectory-based short-sighted Stochastic Shortest Path Problems (SSPs), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artiﬁcial goals that heuristically estimate their cost to reach a goal state. We also extend the theoretical results of Short-Sighted Probabilistic Planner (SSiPP) [1] by proving that SSiPP always ﬁnishes and is asymptotically optimal under sufﬁcient conditions on the structure of short-sighted SSPs. We empirically compare SSiPP using trajectorybased short-sighted SSPs with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems. Trajectory-based SSiPP outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately 1070 states. 1</p><p>2 0.54323977 <a title="350-lda-2" href="./nips-2012-Scaled_Gradients_on_Grassmann_Manifolds_for_Matrix_Completion.html">301 nips-2012-Scaled Gradients on Grassmann Manifolds for Matrix Completion</a></p>
<p>Author: Thanh Ngo, Yousef Saad</p><p>Abstract: This paper describes gradient methods based on a scaled metric on the Grassmann manifold for low-rank matrix completion. The proposed methods signiﬁcantly improve canonical gradient methods, especially on ill-conditioned matrices, while maintaining established global convegence and exact recovery guarantees. A connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established. The proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods. 1</p><p>3 0.51681513 <a title="350-lda-3" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>Author: Will Zou, Shenghuo Zhu, Kai Yu, Andrew Y. Ng</p><p>Abstract: We apply salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision. With tracked sequences as input, a hierarchical network of modules learns invariant features using a temporal slowness constraint. The network encodes invariance which are increasingly complex with hierarchy. Although learned from videos, our features are spatial instead of spatial-temporal, and well suited for extracting features from still images. We applied our features to four datasets (COIL-100, Caltech 101, STL-10, PubFig), and observe a consistent improvement of 4% to 5% in classiﬁcation accuracy. With this approach, we achieve state-of-the-art recognition accuracy 61% on STL-10 dataset. 1</p><p>4 0.46308556 <a title="350-lda-4" href="./nips-2012-Fused_sparsity_and_robust_estimation_for_linear_models_with_unknown_variance.html">139 nips-2012-Fused sparsity and robust estimation for linear models with unknown variance</a></p>
<p>Author: Arnak Dalalyan, Yin Chen</p><p>Abstract: In this paper, we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level. We propose an algorithm, termed Scaled Fused Dantzig Selector (SFDS), that accomplishes the aforementioned learning task by means of a second-order cone program. A special emphasize is put on the particular instance of fused sparsity corresponding to the learning in presence of outliers. We establish ﬁnite sample risk bounds and carry out an experimental evaluation on both synthetic and real data. 1</p><p>5 0.4382309 <a title="350-lda-5" href="./nips-2012-Random_function_priors_for_exchangeable_arrays_with_applications_to_graphs_and_relational_data.html">287 nips-2012-Random function priors for exchangeable arrays with applications to graphs and relational data</a></p>
<p>Author: James Lloyd, Peter Orbanz, Zoubin Ghahramani, Daniel M. Roy</p><p>Abstract: A fundamental problem in the analysis of structured relational data like graphs, networks, databases, and matrices is to extract a summary of the common structure underlying relations between individual entities. Relational data are typically encoded in the form of arrays; invariance to the ordering of rows and columns corresponds to exchangeable arrays. Results in probability theory due to Aldous, Hoover and Kallenberg show that exchangeable arrays can be represented in terms of a random measurable function which constitutes the natural model parameter in a Bayesian model. We obtain a ﬂexible yet simple Bayesian nonparametric model by placing a Gaussian process prior on the parameter function. Efﬁcient inference utilises elliptical slice sampling combined with a random sparse approximation to the Gaussian process. We demonstrate applications of the model to network data and clarify its relation to models in the literature, several of which emerge as special cases. 1</p><p>6 0.43402877 <a title="350-lda-6" href="./nips-2012-Learning_Invariant_Representations_of_Molecules_for_Atomization_Energy_Prediction.html">177 nips-2012-Learning Invariant Representations of Molecules for Atomization Energy Prediction</a></p>
<p>7 0.43240437 <a title="350-lda-7" href="./nips-2012-Efficient_high_dimensional_maximum_entropy_modeling_via_symmetric_partition_functions.html">115 nips-2012-Efficient high dimensional maximum entropy modeling via symmetric partition functions</a></p>
<p>8 0.43047744 <a title="350-lda-8" href="./nips-2012-Timely_Object_Recognition.html">344 nips-2012-Timely Object Recognition</a></p>
<p>9 0.42807978 <a title="350-lda-9" href="./nips-2012-Learned_Prioritization_for_Trading_Off_Accuracy_and_Speed.html">173 nips-2012-Learned Prioritization for Trading Off Accuracy and Speed</a></p>
<p>10 0.42267692 <a title="350-lda-10" href="./nips-2012-Inverse_Reinforcement_Learning_through_Structured_Classification.html">162 nips-2012-Inverse Reinforcement Learning through Structured Classification</a></p>
<p>11 0.42090115 <a title="350-lda-11" href="./nips-2012-Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-Based_Search.html">108 nips-2012-Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search</a></p>
<p>12 0.41923076 <a title="350-lda-12" href="./nips-2012-Projection_Retrieval_for_Classification.html">279 nips-2012-Projection Retrieval for Classification</a></p>
<p>13 0.41921735 <a title="350-lda-13" href="./nips-2012-Transferring_Expectations_in_Model-based_Reinforcement_Learning.html">353 nips-2012-Transferring Expectations in Model-based Reinforcement Learning</a></p>
<p>14 0.41826147 <a title="350-lda-14" href="./nips-2012-Algorithms_for_Learning_Markov_Field_Policies.html">38 nips-2012-Algorithms for Learning Markov Field Policies</a></p>
<p>15 0.41703418 <a title="350-lda-15" href="./nips-2012-Cost-Sensitive_Exploration_in_Bayesian_Reinforcement_Learning.html">88 nips-2012-Cost-Sensitive Exploration in Bayesian Reinforcement Learning</a></p>
<p>16 0.41572261 <a title="350-lda-16" href="./nips-2012-Max-Margin_Structured_Output_Regression_for_Spatio-Temporal_Action_Localization.html">209 nips-2012-Max-Margin Structured Output Regression for Spatio-Temporal Action Localization</a></p>
<p>17 0.41558 <a title="350-lda-17" href="./nips-2012-Tractable_Objectives_for_Robust_Policy_Optimization.html">348 nips-2012-Tractable Objectives for Robust Policy Optimization</a></p>
<p>18 0.41549093 <a title="350-lda-18" href="./nips-2012-Controlled_Recognition_Bounds_for_Visual_Learning_and_Exploration.html">83 nips-2012-Controlled Recognition Bounds for Visual Learning and Exploration</a></p>
<p>19 0.41544932 <a title="350-lda-19" href="./nips-2012-Clustering_by_Nonnegative_Matrix_Factorization_Using_Graph_Random_Walk.html">70 nips-2012-Clustering by Nonnegative Matrix Factorization Using Graph Random Walk</a></p>
<p>20 0.41522399 <a title="350-lda-20" href="./nips-2012-Imitation_Learning_by_Coaching.html">160 nips-2012-Imitation Learning by Coaching</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
