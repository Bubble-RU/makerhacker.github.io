<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2012" href="../home/nips2012_home.html">nips2012</a> <a title="nips-2012-340" href="#">nips2012-340</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</h1>
<br/><p>Source: <a title="nips-2012-340-pdf" href="http://papers.nips.cc/paper/4646-on-the-non-existence-of-convex-calibrated-surrogate-losses-for-ranking.pdf">pdf</a></p><p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>Reference: <a title="nips-2012-340-reference" href="../nips2012_reference/nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The representer theorem for Hilbert spaces: a necessary and sufﬁcient condition  Francesco Dinuzzo and Bernhard Sch¨ lkopf o Max Planck Institute for Intelligent Systems Spemannstrasse 38,72076 T¨ bingen u Germany [fdinuzzo@tuebingen. [sent-1, score-0.808]
</p><p>2 de]  Abstract The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. [sent-5, score-0.941]
</p><p>3 A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. [sent-6, score-1.739]
</p><p>4 A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. [sent-7, score-2.337]
</p><p>5 In this paper, we extend such result by weakening the assumptions on the regularization term. [sent-8, score-0.208]
</p><p>6 In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. [sent-9, score-1.793]
</p><p>7 In this paper, we focus on regularization problems deﬁned over a real Hilbert space H. [sent-11, score-0.231]
</p><p>8 A Hilbert space is a vector space endowed with a inner product and a norm that is complete1 . [sent-12, score-0.026]
</p><p>9 The focus of our study is the general problem of minimizing an extended real-valued regularization functional J : H → R ∪ {+∞} of the form J(w) = f (L1 w, . [sent-14, score-0.31]
</p><p>10 The functional J is the sum of an error term f , which typically depends on empirical data, and a regularization term Ω that enforces certain desirable properties on the solution. [sent-21, score-0.368]
</p><p>11 By allowing the error term f to take the value +∞, problems with hard constraints on the values Li w (for instance, interpolation problems) are included in the framework. [sent-22, score-0.077]
</p><p>12 Moreover, by allowing Ω to take the value +∞, regularization problems of the Ivanov type are also taken into account. [sent-23, score-0.284]
</p><p>13 1  It is easy to see that this setting is a particular case of (1), where the dependence on the data pairs (xi , yi ) can be absorbed into the deﬁnition of f , and Li are point-wise evaluation functionals, i. [sent-26, score-0.055]
</p><p>14 Several popular techniques can be cast in such regularization framework. [sent-29, score-0.208]
</p><p>15 Given binary labels yi = ±1, the SVM classiﬁer (without bias) can be interpreted as a regularization method corresponding to the choice c ((x1 , y1 , w(x1 )), · · · , (x , y , w(x )) = γ  max{0, 1 − yi w(xi )}, i=1  and Ω(w) = w 2 . [sent-34, score-0.318]
</p><p>16 Kernel PCA can be shown to be equivalent to a regularization problem where c ((x1 , y1 , w(x1 )), · · · , (x , y , w(x )) =  1 0, i=1 w(xi ) − +∞, otherwise  2  1 j=1  w(xj )  =1 ,  and Ω is any strictly monotonically increasing function of the norm w , see [4]. [sent-37, score-0.234]
</p><p>17 In this problem, there are no labels yi , but the feature extractor function w is constrained to produce vectors with unitary empirical variance. [sent-38, score-0.09]
</p><p>18 The possibility of choosing general continuous linear functionals Li in (1) allows to consider a much broader class of regularization problems. [sent-39, score-0.668]
</p><p>19 Given a “input signal” u : X → R, assume that the convolution u ∗ w is well-deﬁned for any w ∈ H, and the point-wise evaluated convolution functionals Li w = (u ∗ w)(xi ) =  u(s)w(xi − s)ds, X  are continuous. [sent-42, score-0.491]
</p><p>20 A possible way to recover w from noisy measurements yi of the “output signal” is to solve regularization problems such as min  w∈H  2  γ  (yi − (u ∗ w)(xi )) + w  2  ,  i=1  where the objective functional is of the form (1). [sent-43, score-0.388]
</p><p>21 X  Then, given output labels yi , one can learn a input-output relationship by solving regularization problems of the form min c ((y1 , EP1 (w)), · · · , (y , EP (w)) + w 2 . [sent-47, score-0.286]
</p><p>22 w∈H  If the expectations are bounded linear functionals, such regularization functional is of the form (1). [sent-48, score-0.372]
</p><p>23 By allowing the regularization term Ω to take the value +∞, we can also take into account the whole class of Ivanov-type regularization problems of the form min f (L1 w, . [sent-50, score-0.551]
</p><p>24 , L w), subject to φ(w) ≤ 1, w∈H  by reformulating them as the minimization of a functional of the type (1), where Ω(w) =  0, φ(w) ≤ 1 . [sent-53, score-0.13]
</p><p>25 1  The representer theorem  Let’s now go back to the general formulation (1). [sent-55, score-0.676]
</p><p>26 By the Riesz representation theorem [5, 6], J can be rewritten as J(w) = f ( w, w1 , . [sent-56, score-0.099]
</p><p>27 , w, w ) + Ω(w), where wi is the representer of the linear functional Li with respect to the inner product. [sent-59, score-0.737]
</p><p>28 A family F of regularization functionals of the form (1) is said to admit a linear representer theorem if, for any J ∈ F, and any choice of bounded linear functionals Li , there exists a minimizer w∗ that can be written as a linear combination of the representers: w∗ =  ci wi . [sent-62, score-2.122]
</p><p>29 i=1  If a linear representer theorem holds, the regularization problem under study can be reduced to a -dimensional optimization problem on the scalar coefﬁcients ci , independently of the dimension of H. [sent-63, score-0.911]
</p><p>30 Sufﬁcient conditions under which a family of functionals admits a representer theorem have been widely studied in the literature of statistics, inverse problems, and machine learning. [sent-65, score-1.255]
</p><p>31 The theorem also provides the foundations of learning techniques such as regularized kernel methods and support vector machines, see [7, 8, 9] and references therein. [sent-66, score-0.156]
</p><p>32 Representer theorems are of particular interest when H is a reproducing kernel Hilbert space (RKHS) [10]. [sent-67, score-0.207]
</p><p>33 Given a non-empty set X , a RKHS is a space of functions w : X → R such that point-wise evaluation functionals are bounded, namely, for any x ∈ X , there exists a non-negative real number Cx such that |w(x)| ≤ Cx w , ∀w ∈ H. [sent-68, score-0.433]
</p><p>34 It can be shown that a RKHS can be uniquely associated to a positive-semideﬁnite kernel function K : X × X → R (called reproducing kernel), such that the so-called reproducing property holds: w(x) = w, Kx ,  ∀ (x, w) ∈ X × H,  where the kernel sections Kx are deﬁned as Kx (y) = K(x, y),  ∀y ∈ X . [sent-69, score-0.397]
</p><p>35 The reproducing property states that the representers of point-wise evaluation functionals coincide with the kernel sections. [sent-70, score-0.683]
</p><p>36 Starting from the reproducing property, it is also easy to show that the representer of any bounded linear functional L is given by a function KL ∈ H such that KL (x) = LKx ,  ∀x ∈ X . [sent-71, score-0.853]
</p><p>37 Therefore, in a RKHS, the representer of any bounded linear functional can be obtained explicitly in terms of the reproducing kernel. [sent-72, score-0.853]
</p><p>38 If the regularization functional (1) admits minimizers, and the regularization term Ω is a nondecreasing function of the norm, i. [sent-73, score-0.837]
</p><p>39 Ω(w) = h( w ),  with h : R → R ∪ {+∞}, nondecreasing,  (2)  the linear representer theorem follows easily from the Pythagorean identity. [sent-75, score-0.703]
</p><p>40 A proof that the condition (2) is sufﬁcient appeared in [11] in the case where H is a RKHS and Li are point-wise evaluation functionals. [sent-76, score-0.046]
</p><p>41 Earlier instances of representer theorems can be found in [12, 13, 14]. [sent-77, score-0.615]
</p><p>42 More recently, the question of whether condition (2) is also necessary for the existence of linear representer theorems has been investigated [15]. [sent-78, score-0.786]
</p><p>43 In particular, [15] shows that, if Ω is differentiable (and certain technical existence conditions hold), then (2) is a necessary and sufﬁcient condition for certain classes of regularization functionals to admit a representer theorem. [sent-79, score-1.524]
</p><p>44 In the following, we indeed show that (2) is necessary and sufﬁcient for the family of regularization functionals of the form (1) to admit a linear representer theorem, by merely assuming that Ω is lower semicontinuous and satisﬁes basic conditions for the existence of minimizers. [sent-81, score-1.81]
</p><p>45 The proof is based on a characterization of radial nondecreasing functions deﬁned on a Hilbert space. [sent-82, score-0.357]
</p><p>46 3  2  A characterization of radial nondecreasing functions  In this section, we present a characterization of radial nondecreasing functions deﬁned over Hilbert spaces. [sent-83, score-0.714]
</p><p>47 The following Theorem provides a geometric characterization of radial nondecreasing functions deﬁned on a Hilbert space that generalizes the analogous result of [15] for differentiable functions. [sent-88, score-0.397]
</p><p>48 Let H denote a Hilbert space such that dim H ≥ 2, and Ω : H → R ∪ {+∞} a lower semicontinuous function. [sent-90, score-0.324]
</p><p>49 Then, (2) holds if and only if Ω(x + y) ≥ max{Ω(x), Ω(y)},  ∀x, y ∈ H : x, y = 0. [sent-91, score-0.025]
</p><p>50 Since dim H ≥ 2, by ﬁxing a generic vector x ∈ X \ {0} and a number λ ∈ [0, 1], there exists a vector y such that y = 1 and λ = 1 − cos2 θ, where  x, y . [sent-96, score-0.034]
</p><p>51 Since the last inequality trivially holds also when x = 0, we conclude that Ω(x) ≥ Ω(λx),  ∀x ∈ H,  ∀λ ∈ [0, 1],  (4)  so that Ω is nondecreasing along all the rays passing through the origin. [sent-98, score-0.272]
</p><p>52 Now, for any c ≥ Ω(0), consider the sublevel sets Sc = {x ∈ H : Ω(x) ≤ c} . [sent-100, score-0.024]
</p><p>53 In addition, since Ω is lower semicontinuous, Sc is also closed. [sent-102, score-0.028]
</p><p>54 We now show that Sc is either a closed ball centered at the origin, or the whole space. [sent-103, score-0.084]
</p><p>55 To this end, we show that, for any x ∈ Sc , the whole ball B = {y ∈ H : y ≤ x }, is contained in Sc . [sent-104, score-0.084]
</p><p>56 First, take any y ∈ int(B) \ span{x}, where int denotes the interior. [sent-105, score-0.077]
</p><p>57 Then, y has norm strictly less than x , that is 0< y < x , and is not aligned with x, i. [sent-106, score-0.064]
</p><p>58 See Figure 1 for a geometrical illustration of the sequence xk . [sent-111, score-0.232]
</p><p>59 By orthogonality, we have xk+1  2  = xk  2  + a2 = xk k  2  θ n  1 + tan2  = y  2  1 + tan2  θ n  k+1  . [sent-112, score-0.34]
</p><p>60 (5)  In addition, the angle between xk+1 and xk is given by ak xk  θk = arctan  =  θ , n  so that the total angle between y and xn is given by n−1  θk = θ. [sent-113, score-0.585]
</p><p>61 k=0  Since all the points xk belong to the subspace spanned by x and y, and the angle between x and xn is zero, we have that xn is positively aligned with x, that is xn = λx,  λ ≥ 0. [sent-114, score-0.47]
</p><p>62 Indeed, from (5) we have λ2 =  xn x  2  =  2  y x  1 + tan2  and it can be veriﬁed that  θ n  n  ,  n  θ = 1, n→+∞ n therefore λ ≤ 1 for a sufﬁciently large n. [sent-116, score-0.055]
</p><p>63 Now, write the difference vector in the form lim  1 + tan2  n−1  λx − y =  (xk+1 − xk ), k=0  and observe that xk+1 − xk , xk = 0. [sent-117, score-0.51]
</p><p>64 Since Sc is closed and the closure of int(B) \ span{x} is the whole ball B, every point y ∈ B is also included in Sc . [sent-119, score-0.084]
</p><p>65 This proves that Sc is either a closed ball centered at the origin, or the whole space H. [sent-120, score-0.084]
</p><p>66 5  y x  Figure 1: The sequence xk constructed in the proof of Theorem 1 is associated with a geometrical construction known as spiral of Theodorus. [sent-122, score-0.256]
</p><p>67 Starting from any y in the interior of the ball (excluding points aligned with x), a point of the type λx (with 0 ≤ λ ≤ 1) can be reached by using a ﬁnite number of right triangles. [sent-123, score-0.117]
</p><p>68 3  Representer theorem: a necessary and sufﬁcient condition  In this section, we prove that condition (2) is necessary and sufﬁcient for suitable families of regularization functionals of the type (1) to admit a linear representer theorem. [sent-124, score-1.573]
</p><p>69 Let F denote a family of functionals J : H → R ∪ {+∞} of the form (1) that admit minimizers, and assume that F contains a set of functionals of the form γ Jp (w) = γf ( w, p ) + Ω (w) , ∀p ∈ H, ∀γ ∈ R+ , (6) where f (z) is uniquely minimized at z = 1. [sent-127, score-1.154]
</p><p>70 Then, for any lower semicontinuous Ω, the family F admits a linear representer theorem if and only if (2) holds. [sent-128, score-1.139]
</p><p>71 The ﬁrst part of the theorem (sufﬁciency) follows from an orthogonality argument. [sent-130, score-0.127]
</p><p>72 Any minimizer w∗ of J can be uniquely decomposed as w∗ = u + v,  u ∈ R,  v ∈ R⊥ . [sent-136, score-0.107]
</p><p>73 Now, let’s prove the second part of the theorem (necessity). [sent-138, score-0.099]
</p><p>74 First of all, observe that the functional γ J0 (w) = γf (0) + Ω(w), γ obtained by setting p = 0 in (6), belongs to F. [sent-139, score-0.102]
</p><p>75 In addition, by the representer theorem, the only admissible minimizer of J0 is the origin, that is Ω(y) ≥ Ω(0), ∀y ∈ H. [sent-141, score-0.625]
</p><p>76 x 2  p=  γ By the representer theorem, the functional Jp of the form (6) admits a minimizer of the type  w = λ(γ)x. [sent-143, score-0.846]
</p><p>77 By using the fact that f (z) is minimized at z = 1, and the linear representer theorem, we have γ γ γf (1) + Ω (λ(γ)x) ≤ γf (λ(γ)) + Ω (λ(γ)x) = Jp (λ(γ)x) ≤ Jp (x + y) = γf (1) + Ω (x + y) . [sent-145, score-0.656]
</p><p>78 In the ﬁrst case, we trivially have Ω (x + y) ≥ Ω(x). [sent-148, score-0.024]
</p><p>79 (9)  Let γk denote a sequence such that limk→+∞ γk = +∞, and consider the sequence ak = γk (f (λ(γk )) − f (1)) . [sent-150, score-0.118]
</p><p>80 Since z = 1 is the only minimizer of f (z), the sequence ak can remain bounded only if lim λ(γk ) = 1. [sent-152, score-0.177]
</p><p>81 k→+∞  By taking the limit inferior in (8) for γ → +∞, and using the fact that Ω is lower semicontinuous, we obtain condition (3). [sent-153, score-0.074]
</p><p>82 The second part of Theorem 2 states that any lower semicontinuous regularization term Ω has to be of the form (2) in order for the family F to admit a linear representer theorem. [sent-155, score-1.308]
</p><p>83 Observe that Ω is not required to be differentiable or even continuous. [sent-156, score-0.04]
</p><p>84 Moreover, it needs not to have bounded lower level sets. [sent-157, score-0.063]
</p><p>85 For the necessary condition to hold, the family F has to be broad enough to contain at least a set of regularization functionals of the form (6). [sent-158, score-0.818]
</p><p>86 The following examples show how to apply the necessary condition of Theorem 2 to classes of regularization problems with standard loss functions. [sent-159, score-0.32]
</p><p>87 • Let L : R2 → R ∪ {+∞} denote any loss function of the type L(y, z) = L(y − z), such that L(t) is uniquely minimized at t = 0. [sent-160, score-0.139]
</p><p>88 Then, for any lower semicontinuous regularation term Ω, the family of regularization functionals of the form J(w) = γ  L (yi , w, wi ) + Ω(w), i=1  admits a linear representer theorem if and only if (2) holds. [sent-161, score-1.84]
</p><p>89 To see that the hypotheses of Theorem 2 are satisﬁed, it is sufﬁcient to consider the subset of functionals with = 1, y1 = 1, and w1 = p ∈ H. [sent-162, score-0.457]
</p><p>90 These functionals can be written in the form (6) with f (z) = L(1, z). [sent-163, score-0.433]
</p><p>91 • The class of regularization problems with the hinge (SVM) loss of the form J(w) = γ  max{0, 1 − yi w, wi } + Ω(w), i=1  with Ω lower semicontinuous, admits a linear representer theorem if and only if Ω satisfy (2). [sent-164, score-1.139]
</p><p>92 For instance, by choosing = 2, and (y1 , w1 ) = (1, p),  (y2 , w2 ) = (−1, p/2),  we obtain regularization functionals of the form (6) with f (z) = max{0, 1 − z} + max{0, 1 + z/2}, and it is easy to verify that f is uniquely minimized at z = 1. [sent-165, score-0.752]
</p><p>93 7  4  Conclusions  Sufﬁciently broad families of regularization functionals deﬁned over a Hilbert space with lower semicontinuous regularization term admit a linear representer theorem if and only if the regularization term is a radial nondecreasing function. [sent-166, score-2.564]
</p><p>94 As a concluding remark, it is important to observe that other types of regularization terms are possible if the representer theorem is only required to hold for a restricted subset of the data functionals. [sent-168, score-0.884]
</p><p>95 Exploring necessary conditions for the existence of representer theorems under different types of restrictions on the data functionals is an interesting future research direction. [sent-169, score-1.146]
</p><p>96 Nonlinear component analysis as a kernel eigeno u value problem. [sent-195, score-0.057]
</p><p>97 Sur une esp` ce de g´ om´ trie analytique des syst` mes de fonctions sommables. [sent-199, score-0.109]
</p><p>98 e e e e Comptes rendus de l’Acad´ mie des sciences Paris, 144:1409–1411, 1907. [sent-200, score-0.139]
</p><p>99 Sur les ensembles de fonctions et les op´ rations lin´ aires. [sent-203, score-0.09]
</p><p>100 Comptes rendus de e e e l’Acad´ mie des sciences Paris, 144:1414–1416, 1907. [sent-204, score-0.139]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('representer', 0.577), ('functionals', 0.433), ('semicontinuous', 0.262), ('regularization', 0.208), ('nondecreasing', 0.199), ('sc', 0.175), ('xk', 0.17), ('hilbert', 0.13), ('admit', 0.122), ('reproducing', 0.112), ('radial', 0.102), ('functional', 0.102), ('theorem', 0.099), ('rkhs', 0.092), ('admits', 0.091), ('representers', 0.081), ('jp', 0.072), ('ak', 0.07), ('kx', 0.066), ('uk', 0.064), ('uniquely', 0.059), ('li', 0.057), ('kernel', 0.057), ('characterization', 0.056), ('xn', 0.055), ('yi', 0.055), ('family', 0.055), ('existence', 0.055), ('ivanov', 0.054), ('span', 0.053), ('int', 0.052), ('minimizers', 0.052), ('minimized', 0.052), ('ball', 0.051), ('des', 0.051), ('minimizer', 0.048), ('angle', 0.048), ('condition', 0.046), ('comptes', 0.044), ('mie', 0.044), ('rendus', 0.044), ('necessary', 0.043), ('lkopf', 0.043), ('differentiable', 0.04), ('theorems', 0.038), ('aligned', 0.038), ('geometrical', 0.038), ('tikhonov', 0.036), ('fonctions', 0.036), ('sur', 0.036), ('origin', 0.036), ('bounded', 0.035), ('sch', 0.035), ('unitary', 0.035), ('spline', 0.035), ('acad', 0.034), ('dim', 0.034), ('suf', 0.033), ('broad', 0.033), ('whole', 0.033), ('cx', 0.032), ('wi', 0.031), ('svm', 0.031), ('term', 0.029), ('convolution', 0.029), ('argyriou', 0.029), ('type', 0.028), ('lower', 0.028), ('orthogonality', 0.028), ('les', 0.027), ('linear', 0.027), ('subspace', 0.026), ('norm', 0.026), ('xi', 0.025), ('take', 0.025), ('holds', 0.025), ('paris', 0.025), ('sequence', 0.024), ('trivially', 0.024), ('rays', 0.024), ('syst', 0.024), ('om', 0.024), ('arctan', 0.024), ('sublevel', 0.024), ('dpi', 0.024), ('fdinuzzo', 0.024), ('spiral', 0.024), ('winston', 0.024), ('riesz', 0.024), ('esp', 0.024), ('hypotheses', 0.024), ('spanned', 0.023), ('problems', 0.023), ('orthogonal', 0.023), ('machines', 0.022), ('ceedings', 0.022), ('une', 0.022), ('epi', 0.022), ('deconvolution', 0.022), ('dinuzzo', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="340-tfidf-1" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>2 0.15180667 <a title="340-tfidf-2" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>3 0.12184626 <a title="340-tfidf-3" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>Author: Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a ﬂexible SVM (FlexSVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. 1</p><p>4 0.12104792 <a title="340-tfidf-4" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>5 0.10059207 <a title="340-tfidf-5" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><p>6 0.096456505 <a title="340-tfidf-6" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>7 0.091748193 <a title="340-tfidf-7" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>8 0.09158282 <a title="340-tfidf-8" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>9 0.089856915 <a title="340-tfidf-9" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>10 0.076277807 <a title="340-tfidf-10" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>11 0.073837101 <a title="340-tfidf-11" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>12 0.07075423 <a title="340-tfidf-12" href="./nips-2012-Multiclass_Learning_with_Simplex_Coding.html">227 nips-2012-Multiclass Learning with Simplex Coding</a></p>
<p>13 0.06747853 <a title="340-tfidf-13" href="./nips-2012-Ensemble_weighted_kernel_estimators_for_multivariate_entropy_estimation.html">117 nips-2012-Ensemble weighted kernel estimators for multivariate entropy estimation</a></p>
<p>14 0.060304966 <a title="340-tfidf-14" href="./nips-2012-Synchronization_can_Control_Regularization_in_Neural_Systems_via_Correlated_Noise_Processes.html">333 nips-2012-Synchronization can Control Regularization in Neural Systems via Correlated Noise Processes</a></p>
<p>15 0.058341879 <a title="340-tfidf-15" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>16 0.054501504 <a title="340-tfidf-16" href="./nips-2012-Nonparametric_Reduced_Rank_Regression.html">247 nips-2012-Nonparametric Reduced Rank Regression</a></p>
<p>17 0.053994555 <a title="340-tfidf-17" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>18 0.053351123 <a title="340-tfidf-18" href="./nips-2012-Volume_Regularization_for_Binary_Classification.html">361 nips-2012-Volume Regularization for Binary Classification</a></p>
<p>19 0.049608536 <a title="340-tfidf-19" href="./nips-2012-A_Nonparametric_Conjugate_Prior_Distribution_for_the_Maximizing_Argument_of_a_Noisy_Function.html">13 nips-2012-A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function</a></p>
<p>20 0.047471397 <a title="340-tfidf-20" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, 0.032), (2, 0.089), (3, -0.062), (4, 0.099), (5, 0.066), (6, 0.015), (7, 0.016), (8, 0.1), (9, -0.016), (10, -0.052), (11, 0.059), (12, 0.094), (13, -0.059), (14, -0.022), (15, -0.137), (16, 0.022), (17, 0.036), (18, 0.014), (19, -0.007), (20, -0.074), (21, -0.021), (22, 0.009), (23, -0.068), (24, 0.037), (25, 0.074), (26, -0.013), (27, -0.044), (28, -0.022), (29, 0.006), (30, -0.021), (31, -0.047), (32, -0.031), (33, 0.122), (34, 0.064), (35, 0.018), (36, -0.003), (37, -0.079), (38, 0.01), (39, -0.017), (40, 0.025), (41, -0.017), (42, 0.054), (43, -0.006), (44, -0.023), (45, -0.009), (46, 0.057), (47, -0.003), (48, -0.019), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94265139 <a title="340-lsi-1" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>2 0.68793678 <a title="340-lsi-2" href="./nips-2012-Scalable_nonconvex_inexact_proximal_splitting.html">300 nips-2012-Scalable nonconvex inexact proximal splitting</a></p>
<p>Author: Suvrit Sra</p><p>Abstract: We study a class of large-scale, nonsmooth, and nonconvex optimization problems. In particular, we focus on nonconvex problems with composite objectives. This class includes the extensively studied class of convex composite objective problems as a subclass. To solve composite nonconvex problems we introduce a powerful new framework based on asymptotically nonvanishing errors, avoiding the common stronger assumption of vanishing errors. Within our new framework we derive both batch and incremental proximal splitting algorithms. To our knowledge, our work is ﬁrst to develop and analyze incremental nonconvex proximalsplitting algorithms, even if we were to disregard the ability to handle nonvanishing errors. We illustrate one instance of our general framework by showing an application to large-scale nonsmooth matrix factorization. 1</p><p>3 0.63572985 <a title="340-lsi-3" href="./nips-2012-Exact_and_Stable_Recovery_of_Sequences_of_Signals_with_Sparse_Increments_via_Differential__1-Minimization.html">120 nips-2012-Exact and Stable Recovery of Sequences of Signals with Sparse Increments via Differential  1-Minimization</a></p>
<p>Author: Demba Ba, Behtash Babadi, Patrick Purdon, Emery Brown</p><p>Abstract: We consider the problem of recovering a sequence of vectors, (xk )K , for which k=0 the increments xk − xk−1 are Sk -sparse (with Sk typically smaller than S1 ), based on linear measurements (yk = Ak xk + ek )K , where Ak and ek denote the meak=1 surement matrix and noise, respectively. Assuming each Ak obeys the restricted isometry property (RIP) of a certain order—depending only on Sk —we show that in the absence of noise a convex program, which minimizes the weighted sum of the ℓ1 -norm of successive differences subject to the linear measurement constraints, recovers the sequence (xk )K exactly. This is an interesting result bek=1 cause this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the RIP requirements in the standard sense, and yet we can achieve exact recovery. In the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence. We supplement our theoretical analysis with simulations and an application to real video data. These further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</p><p>4 0.62840384 <a title="340-lsi-4" href="./nips-2012-A_quasi-Newton_proximal_splitting_method.html">27 nips-2012-A quasi-Newton proximal splitting method</a></p>
<p>Author: Stephen Becker, Jalal Fadili</p><p>Abstract: A new result in convex analysis on the calculation of proximity operators in certain scaled norms is derived. We describe efﬁcient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem. The second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-Newton method. The optimization method compares favorably against state-of-the-art alternatives. The algorithm has extensive applications including signal processing, sparse recovery and machine learning and classiﬁcation. 1</p><p>5 0.62358016 <a title="340-lsi-5" href="./nips-2012-Proximal_Newton-type_methods_for_convex_optimization.html">282 nips-2012-Proximal Newton-type methods for convex optimization</a></p>
<p>Author: Jason Lee, Yuekai Sun, Michael Saunders</p><p>Abstract: We seek to solve convex optimization problems in composite form: minimize f (x) := g(x) + h(x), n x∈R where g is convex and continuously differentiable and h : Rn → R is a convex but not necessarily differentiable function whose proximal mapping can be evaluated efﬁciently. We derive a generalization of Newton-type methods to handle such convex but nonsmooth objective functions. We prove such methods are globally convergent and achieve superlinear rates of convergence in the vicinity of an optimal solution. We also demonstrate the performance of these methods using problems of relevance in machine learning and statistics. 1</p><p>6 0.6002298 <a title="340-lsi-6" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>7 0.59856272 <a title="340-lsi-7" href="./nips-2012-Optimal_kernel_choice_for_large-scale_two-sample_tests.html">264 nips-2012-Optimal kernel choice for large-scale two-sample tests</a></p>
<p>8 0.59610683 <a title="340-lsi-8" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>9 0.58680081 <a title="340-lsi-9" href="./nips-2012-Q-MKL%3A_Matrix-induced_Regularization_in_Multi-Kernel_Learning_with_Applications_to_Neuroimaging.html">284 nips-2012-Q-MKL: Matrix-induced Regularization in Multi-Kernel Learning with Applications to Neuroimaging</a></p>
<p>10 0.55193347 <a title="340-lsi-10" href="./nips-2012-Kernel_Hyperalignment.html">167 nips-2012-Kernel Hyperalignment</a></p>
<p>11 0.53195447 <a title="340-lsi-11" href="./nips-2012-The_Bethe_Partition_Function_of_Log-supermodular_Graphical_Models.html">335 nips-2012-The Bethe Partition Function of Log-supermodular Graphical Models</a></p>
<p>12 0.52850503 <a title="340-lsi-12" href="./nips-2012-Gradient-based_kernel_method_for_feature_extraction_and_variable_selection.html">144 nips-2012-Gradient-based kernel method for feature extraction and variable selection</a></p>
<p>13 0.50484347 <a title="340-lsi-13" href="./nips-2012-Learning_Halfspaces_with_the_Zero-One_Loss%3A_Time-Accuracy_Tradeoffs.html">174 nips-2012-Learning Halfspaces with the Zero-One Loss: Time-Accuracy Tradeoffs</a></p>
<p>14 0.50110316 <a title="340-lsi-14" href="./nips-2012-Nystr%C3%B6m_Method_vs_Random_Fourier_Features%3A_A_Theoretical_and_Empirical_Comparison.html">249 nips-2012-Nyström Method vs Random Fourier Features: A Theoretical and Empirical Comparison</a></p>
<p>15 0.47373965 <a title="340-lsi-15" href="./nips-2012-A_Polynomial-time_Form_of_Robust_Regression.html">16 nips-2012-A Polynomial-time Form of Robust Regression</a></p>
<p>16 0.46670589 <a title="340-lsi-16" href="./nips-2012-Active_Learning_of_Multi-Index_Function_Models.html">34 nips-2012-Active Learning of Multi-Index Function Models</a></p>
<p>17 0.45409444 <a title="340-lsi-17" href="./nips-2012-Recovery_of_Sparse_Probability_Measures_via_Convex_Programming.html">290 nips-2012-Recovery of Sparse Probability Measures via Convex Programming</a></p>
<p>18 0.4381403 <a title="340-lsi-18" href="./nips-2012-Persistent_Homology_for_Learning_Densities_with_Bounded_Support.html">269 nips-2012-Persistent Homology for Learning Densities with Bounded Support</a></p>
<p>19 0.42420232 <a title="340-lsi-19" href="./nips-2012-A_new_metric_on_the_manifold_of_kernel_matrices_with_application_to_matrix_geometric_means.html">25 nips-2012-A new metric on the manifold of kernel matrices with application to matrix geometric means</a></p>
<p>20 0.41974187 <a title="340-lsi-20" href="./nips-2012-Accelerated_Training_for_Matrix-norm_Regularization%3A_A_Boosting_Approach.html">29 nips-2012-Accelerated Training for Matrix-norm Regularization: A Boosting Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.016), (21, 0.034), (38, 0.107), (39, 0.011), (42, 0.014), (54, 0.017), (55, 0.478), (74, 0.023), (76, 0.121), (80, 0.059), (92, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81276405 <a title="340-lda-1" href="./nips-2012-The_representer_theorem_for_Hilbert_spaces%3A_a_necessary_and_sufficient_condition.html">340 nips-2012-The representer theorem for Hilbert spaces: a necessary and sufficient condition</a></p>
<p>Author: Francesco Dinuzzo, Bernhard Schölkopf</p><p>Abstract: The representer theorem is a property that lies at the foundation of regularization theory and kernel methods. A class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the ﬁnite dimensional subspace spanned by the representers of the data. A recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function. In this paper, we extend such result by weakening the assumptions on the regularization term. In particular, the main result of this paper implies that, for a sufﬁciently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data. 1</p><p>2 0.8110348 <a title="340-lda-2" href="./nips-2012-ImageNet_Classification_with_Deep_Convolutional_Neural_Networks.html">158 nips-2012-ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p>Author: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. 1</p><p>3 0.80844557 <a title="340-lda-3" href="./nips-2012-Bayesian_Nonparametric_Modeling_of_Suicide_Attempts.html">52 nips-2012-Bayesian Nonparametric Modeling of Suicide Attempts</a></p>
<p>Author: Francisco Ruiz, Isabel Valera, Carlos Blanco, Fernando Pérez-Cruz</p><p>Abstract: The National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database contains a large amount of information, regarding the way of life, medical conditions, etc., of a representative sample of the U.S. population. In this paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the nature of the data, we need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efﬁcient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. Finally, the experiments over the NESARC database show that our model properly captures some of the hidden causes that model suicide attempts. 1</p><p>4 0.80056852 <a title="340-lda-4" href="./nips-2012-Human_memory_search_as_a_random_walk_in_a_semantic_network.html">155 nips-2012-Human memory search as a random walk in a semantic network</a></p>
<p>Author: Joseph L. Austerweil, Joshua T. Abbott, Thomas L. Griffiths</p><p>Abstract: The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These ﬁndings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more uniﬁed account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters. 1</p><p>5 0.73224741 <a title="340-lda-5" href="./nips-2012-Meta-Gaussian_Information_Bottleneck.html">211 nips-2012-Meta-Gaussian Information Bottleneck</a></p>
<p>Author: Melanie Rey, Volker Roth</p><p>Abstract: We present a reformulation of the information bottleneck (IB) problem in terms of copula, using the equivalence between mutual information and negative copula entropy. Focusing on the Gaussian copula we extend the analytical IB solution available for the multivariate Gaussian case to distributions with a Gaussian dependence structure but arbitrary marginal densities, also called meta-Gaussian distributions. This opens new possibles applications of IB to continuous data and provides a solution more robust to outliers. 1</p><p>6 0.64793456 <a title="340-lda-6" href="./nips-2012-Minimizing_Uncertainty_in_Pipelines.html">215 nips-2012-Minimizing Uncertainty in Pipelines</a></p>
<p>7 0.63935602 <a title="340-lda-7" href="./nips-2012-Density-Difference_Estimation.html">95 nips-2012-Density-Difference Estimation</a></p>
<p>8 0.50521481 <a title="340-lda-8" href="./nips-2012-Semantic_Kernel_Forests_from_Multiple_Taxonomies.html">306 nips-2012-Semantic Kernel Forests from Multiple Taxonomies</a></p>
<p>9 0.49904141 <a title="340-lda-9" href="./nips-2012-Deep_Neural_Networks_Segment_Neuronal_Membranes_in_Electron_Microscopy_Images.html">91 nips-2012-Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a></p>
<p>10 0.49693769 <a title="340-lda-10" href="./nips-2012-Multiple_Operator-valued_Kernel_Learning.html">231 nips-2012-Multiple Operator-valued Kernel Learning</a></p>
<p>11 0.49480566 <a title="340-lda-11" href="./nips-2012-Neurally_Plausible_Reinforcement_Learning_of_Working_Memory_Tasks.html">238 nips-2012-Neurally Plausible Reinforcement Learning of Working Memory Tasks</a></p>
<p>12 0.48730129 <a title="340-lda-12" href="./nips-2012-Deep_Learning_of_Invariant_Features_via_Simulated_Fixations_in_Video.html">90 nips-2012-Deep Learning of Invariant Features via Simulated Fixations in Video</a></p>
<p>13 0.48329461 <a title="340-lda-13" href="./nips-2012-Learning_to_Align_from_Scratch.html">193 nips-2012-Learning to Align from Scratch</a></p>
<p>14 0.47411859 <a title="340-lda-14" href="./nips-2012-Scalable_Inference_of_Overlapping_Communities.html">298 nips-2012-Scalable Inference of Overlapping Communities</a></p>
<p>15 0.4625192 <a title="340-lda-15" href="./nips-2012-Learning_from_Distributions_via_Support_Measure_Machines.html">188 nips-2012-Learning from Distributions via Support Measure Machines</a></p>
<p>16 0.46177021 <a title="340-lda-16" href="./nips-2012-A_Better_Way_to_Pretrain_Deep_Boltzmann_Machines.html">4 nips-2012-A Better Way to Pretrain Deep Boltzmann Machines</a></p>
<p>17 0.45788154 <a title="340-lda-17" href="./nips-2012-Memorability_of_Image_Regions.html">210 nips-2012-Memorability of Image Regions</a></p>
<p>18 0.45699769 <a title="340-lda-18" href="./nips-2012-Deep_Spatio-Temporal_Architectures_and_Learning_for_Protein_Structure_Prediction.html">93 nips-2012-Deep Spatio-Temporal Architectures and Learning for Protein Structure Prediction</a></p>
<p>19 0.45042834 <a title="340-lda-19" href="./nips-2012-Topic-Partitioned_Multinetwork_Embeddings.html">345 nips-2012-Topic-Partitioned Multinetwork Embeddings</a></p>
<p>20 0.44599819 <a title="340-lda-20" href="./nips-2012-Sparse_Prediction_with_the_%24k%24-Support_Norm.html">319 nips-2012-Sparse Prediction with the $k$-Support Norm</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
