<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-214" href="#">acl2012-214</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</h1>
<br/><p>Source: <a title="acl-2012-214-pdf" href="http://aclweb.org/anthology//P/P12/P12-1028.pdf">pdf</a></p><p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>Reference: <a title="acl-2012-214-reference" href="../acl2012_reference/acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 it l@ Abstract In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. [sent-5, score-0.17]
</p><p>2 First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. [sent-6, score-0.422]
</p><p>3 Then, we design advanced similarity functions between such structures, i. [sent-7, score-0.16]
</p><p>4 , semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. [sent-9, score-0.373]
</p><p>5 The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean-  ingful syntactic/semantic structures, which allows for improving the state-of-the-art. [sent-10, score-0.169]
</p><p>6 1 Introduction Verb classification is a fundamental topic of computational linguistics research given its importance for understanding the role of verbs in conveying semantics ofnatural language (NL). [sent-11, score-0.185]
</p><p>7 Additionally, generalization based on verb classification is central to many NL applications, ranging from shallow semantic parsing to semantic search or information extraction. [sent-12, score-0.395]
</p><p>8 Currently, a lot of interest has been paid to two verb categorization schemes: VerbNet (Schuler, 2005) and FrameNet (Baker et al. [sent-13, score-0.216]
</p><p>9 Such work has shown that syntax is necessary for helping to predict the roles of verb arguments and consequently their verb sense (Gildea and Jurasfky, 2002; Pradhan et al. [sent-15, score-0.474]
</p><p>10 However, the definition of models for optimally combining lexical and syntactic constraints is 263 Alessandro Moschitti  University of Trento 38123 Povo (TN), Italy mos chitt i dis i @ . [sent-17, score-0.15]
</p><p>11 In particular, the exhaustive design and experimentation of lexical and syntactic features for learning verb classification appears to be computationally problematic. [sent-21, score-0.427]
</p><p>12 For example, the verb order can belongs to the two VerbNet classes: The class 60. [sent-22, score-0.336]
</p><p>13 , order someone to do something as shown in: The Illinois Supreme Court ordered the commission to audit Commonwealth Edison ’s construction expenses and refund any unreasonable expenses . [sent-25, score-0.241]
</p><p>14 Clearly, the syntactic realization can be used to dis–  –  cern the cases above but it would not be enough to correctly classify the following verb occurrence: . [sent-32, score-0.292]
</p><p>15 These have also been shown to be useful for semantic role classification (Zapirain et al. [sent-40, score-0.168]
</p><p>16 Note that their coding in learning algorithms is rather complex: we need to take into account syntactic structures, which may require an exponential number of syntactic features (i. [sent-42, score-0.152]
</p><p>17 In this paper, we tackle the problem above by studying innovative representations for automatic verb classification according to VerbNet and FrameNet. [sent-46, score-0.31]
</p><p>18 We define syntactic and semantic structures capturing essential lexical and syntactic properties of verbs. [sent-47, score-0.346]
</p><p>19 , kernel functions, which can also exploit distributional lexical semantics, to train automatic classifiers. [sent-52, score-0.265]
</p><p>20 The basic idea of such functions  is to compute the similarity between two verbs in terms of all the possible substructures of their syntactic frames. [sent-53, score-0.357]
</p><p>21 Then, we apply kernel functions that jointly model structural and lexical similarity so that syntactic properties are combined with generalized lexemes. [sent-55, score-0.525]
</p><p>22 The nice property of kernel functions is that they can be used in place of the scalar product of feature vectors to train algorithms such as Support Vector Machines (SVMs). [sent-56, score-0.172]
</p><p>23 This way SVMs can learn the association between syntactic (sub-) structures whose lexical arguments are generalized and target verb classes, i. [sent-57, score-0.562]
</p><p>24 We carried out extensive experiments on verb class and frame detection which showed that our models greatly improve on the state-of-the-art (up to about 13% of relative error reduction). [sent-60, score-0.385]
</p><p>25 Such results are nicely assessed by manually inspecting the most important substructures used by the classifiers as they largely correlate with syntactic frames defined in VerbNet. [sent-61, score-0.155]
</p><p>26 2 Related work Our target task is verb classification but at the same time our models exploit distributional models as well as structural kernels. [sent-69, score-0.49]
</p><p>27 The introductory verb classification example has intuitively shown the complexity of defining a comprehensive feature representation. [sent-72, score-0.277]
</p><p>28 It has been often observed that verb senses tend to show different selectional constraints in a specific argument position and the above verb order is a clear example. [sent-74, score-0.535]
</p><p>29 This is not true for the direct object dependency of the alternative sense 13. [sent-78, score-0.166]
</p><p>30 When properly generalized, the direct object information has thus been shown highly predictive about verb sense distinctions. [sent-81, score-0.346]
</p><p>31 , the set of verbs that are typically collocated with a direct object, are shown to be more helpful than lexical information (e. [sent-85, score-0.196]
</p><p>32 The set of typical verbs taking a noun n as a direct object is in fact a strong characterization for semantic similarity, as all the nouns m similar to n tend to collocate with the same verbs. [sent-88, score-0.223]
</p><p>33 This is true also for other syntactic dependencies, among which the direct object dependency is possibly the strongest cue (as shown for example in (Dligach and Palmer, 2008)). [sent-89, score-0.2]
</p><p>34 Their ability to capture lexical similarity is well established in  WSD tasks (e. [sent-91, score-0.188]
</p><p>35 1 of the verb order, DMs can be used to suggest that the role PATIENT can be inherited by all these words, as suitable Organisations. [sent-105, score-0.264]
</p><p>36 Tree and sequence kernels have been successfully used in many NLP applications, e. [sent-119, score-0.207]
</p><p>37 Recently, DMs have been also proposed in integrated syntactic-semantic structures that feed advanced learning functions, such as the semantic tree kernels discussed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Mehdad et al. [sent-135, score-0.45]
</p><p>38 3 Structural Similarity Functions In this paper we model verb classifiers by exploiting previous technology for kernel methods. [sent-138, score-0.376]
</p><p>39 In particular, we design new models for verb classification by adopting algorithms for structural similarity, known as Smoothed Partial Tree Kernels (SPTKs) (Croce et al. [sent-139, score-0.328]
</p><p>40 We define new innovative structures and similarity functions based on LSA. [sent-141, score-0.254]
</p><p>41 The main idea of SPTK is rather simple: (i) measuring the similarity between two trees in terms of the number of shared subtrees; and (ii) such number also includes similar fragments whose lexical nodes 265 are just related (so they can be different). [sent-142, score-0.296]
</p><p>42 The contribution of (ii) is proportional to the lexical similar-  ity of the tree lexical nodes, where the latter can be evaluated according to distributional models or also lexical resources, e. [sent-143, score-0.41]
</p><p>43 1 LSA as lexical similarity model Robust representations can be obtained through intelligent dimensionality reduction methods. [sent-148, score-0.188]
</p><p>44 Given two words w1 and w2,  UkSk1/2,  the term similarity function σ is estimated as the cosine similarity between the corresponding projections w~ 1 , w~ 2 in the LSA space, i. [sent-155, score-0.228]
</p><p>45 The ∆ function determines the richness of the kernel space and thus induces different tree kernels, for example, the syntactic tree kernel (STK) (Collins and Duffy, 2002) or the partial tree kernel (PTK) (Moschitti, 2006). [sent-167, score-0.823]
</p><p>46 In the next section we show how we exploit the class of SPTKs, for verb classification. [sent-181, score-0.336]
</p><p>47 1To have a similarity score between 0 and 1, a normalization in the kernel space, i. [sent-182, score-0.24]
</p><p>48 266  4 Verb Classification Models The design of SPTK-based algorithms for our verb classification requires the modeling of two different aspects: (i) a tree representation for the verbs; and (ii) the lexical similarity suitable for the task. [sent-185, score-0.588]
</p><p>49 We also modified SPTK to apply different similarity functions to different nodes to introduce flexibility. [sent-186, score-0.216]
</p><p>50 1 Verb Structural Representation The implicit feature space generated by structural kernels and the corresponding notion of similarity  between verbs obviously depends on the input structures. [sent-188, score-0.448]
</p><p>51 For example, for representing the first example of the introduction, we can use the constituency tree (CT) in Figure 1, where the target verb node is enriched with the TARGET label. [sent-191, score-0.47]
</p><p>52 Here, we apply tree pruning to reduce the computational complexity of tree kernels as it is proportional to the number of nodes in the input trees. [sent-192, score-0.509]
</p><p>53 To further improve generalization, we lemmatize lexical nodes and add generalized POS-Tags, i. [sent-195, score-0.168]
</p><p>54 , noun (n::), verb (v::), adjective (::a), determiner (::d) and so on, to them. [sent-197, score-0.216]
</p><p>55 This is useful for constraining similarity to be only contributed by lexical pairs of the same grammatical category. [sent-198, score-0.188]
</p><p>56 , grammatical functions (GR), and POS-Tags, again as tree nodes. [sent-201, score-0.169]
</p><p>57 We designed two different tree types: (i) in the first type, GR are central nodes from which dependencies are drawn and all the other features of the central node, i. [sent-202, score-0.179]
</p><p>58 (ii) The second type of tree uses lexicals as central nodes on which both GR and POS-Tag are added as the rightmost children. [sent-206, score-0.212]
</p><p>59 For both trees, the pruning strategy only preserves the verb node, its direct ancestors (father and siblings) and its descendants up to two levels (i. [sent-208, score-0.262]
</p><p>60 Note that, our dependency tree can capture the semantic head of the verbal argument along with the main syntactic  construct, e. [sent-211, score-0.343]
</p><p>61 2 Generalized node similarity for SPTK We have defined the new similarity στ to be used in Eq. [sent-215, score-0.228]
</p><p>62 στ takes two nodes n1 and n2 and applies a different similarity for each node type. [sent-218, score-0.17]
</p><p>63 In our experiment, we assign 0/1 similarity for SYNT and POS nodes according to string matching. [sent-226, score-0.17]
</p><p>64 For LEX type, we apply a lexical similarity learned with LSA to only pairs of lexicals associated with the same POSTag. [sent-227, score-0.221]
</p><p>65 It should be noted that the type-based similarity allows for potentially applying a different similarity for each node. [sent-228, score-0.228]
</p><p>66 For the former, we used the same verb classification setting of (Brown et al. [sent-234, score-0.277]
</p><p>67 It contains 113K verb instances, 97K of which are verbs represented in at least one VerbNet  class. [sent-238, score-0.292]
</p><p>68 Semlink includes 495 verbs, whose instances are labeled with more than one class (including one single VerbNet class or none). [sent-239, score-0.24]
</p><p>69 We used all instances of the corpus for a total of 45,584 instances for 180 verb classes. [sent-240, score-0.216]
</p><p>70 When instances labeled with the none class are not included, the number of examples becomes 23,719. [sent-241, score-0.175]
</p><p>71 Our verb (multi) classifier is designed with the one-vs-all (Rifkin and Klautau, 2004) multiclassification schema. [sent-253, score-0.216]
</p><p>72 This uses a set of binary SVM classifiers, one for each verb class (frame) i. [sent-254, score-0.336]
</p><p>73 The sentences whose verb is labeled with the class iare positive examples for the classifier i. [sent-255, score-0.336]
</p><p>74 The sen-  tences whose verbs are compatible with the class i but evoking a different class or labeled with none (no current verb class applies) are added as negative examples. [sent-256, score-0.707]
</p><p>75 In the classification phase the binary classifiers are applied by (i) only considering classes that are compatible with the target verbs; and (ii) selecting the class associated with the maximum positive SVM margin. [sent-257, score-0.35]
</p><p>76 Regarding SPTK setting, we used the lexical similarity σ defined in Sec. [sent-263, score-0.188]
</p><p>77 The entries of M are the point-wise mutual 3(Structural kernels in SVMLight (Joachims, 2000)) available at http://disi. [sent-272, score-0.207]
</p><p>78 08% Table 1: VerbNet accuracy with the none class STKPTKSPTK lw Acc. [sent-294, score-0.335]
</p><p>79 84% Table 2: FrameNet accuracy without the none class information between them. [sent-307, score-0.175]
</p><p>80 For generating the CT, GRCT and LCT structures, we used the constituency trees generated by the Charniak parser (Charniak, 2000) and the dependency structures generated by the LTH syntactic parser (described in (Johansson and Nugues, 2008)). [sent-309, score-0.207]
</p><p>81 For it, we represent an instance of a verb in a sentence using all words of the sentence (by creating a special feature for the predicate word). [sent-316, score-0.259]
</p><p>82 86% Table 3: VerbNet accuracy without the none class generally in contrast with standard text categorization tasks, for which n-gram models show accuracy comparable to the simpler BOW. [sent-346, score-0.175]
</p><p>83 On the other hand, it simply confirms that verb classification requires  the dependency information between words (i. [sent-347, score-0.313]
</p><p>84 CT does not produce the same improvement since it does not allow PTK to directly compare the lexical structure (lexemes are all leaf nodes in CT and to connect some pairs of them very large trees are needed). [sent-363, score-0.2]
</p><p>85 This suggests that word order around the predicate is more important for deriving the VerbNet class than the FrameNet frame. [sent-376, score-0.163]
</p><p>86 Additionally, LCT or GRCT seems to be invariant for both PTK and SPTK whereas the lexical similarity still produces a relevant improvement on PTK, i. [sent-377, score-0.188]
</p><p>87 This may suggest that SPTK can help in complex settings, where verb class characterization is more difficult. [sent-390, score-0.336]
</p><p>88 Many mistakes are related to false positives and negatives of the none class (about 72% of the errors). [sent-396, score-0.175]
</p><p>89 Most errors are also due to lack of lexical information available to the SPTK kernel: (i) in 30% of the errors, the argument heads were proper nouns for which the lexical generalization provided by the DMs was not 5http://danielepighin. [sent-398, score-0.197]
</p><p>90 Additionally, ambiguity characterizes errors where the system is linguistically consistent but the learned selectional preferences are not sufficient to separate verb senses. [sent-402, score-0.27]
</p><p>91 Table 4 and 5 report the tree structures characterizing the most informative training examples of the two senses of the verb order, i. [sent-407, score-0.4]
</p><p>92 All fragments of the sense 60 emphasize instead the sentential complement of the verb that in fact expresses the standard PROPOSITION role in VerbNet. [sent-417, score-0.358]
</p><p>93 the capability of tree kernels to implicitly trigger useful linguistic inductions for complex semantic tasks. [sent-425, score-0.389]
</p><p>94 When SPTK are adopted, verb arguments can be lexically generalized into word classes, i. [sent-426, score-0.254]
</p><p>95 7 Conclusion We have proposed new approaches to characterize verb classes in learning algorithms. [sent-434, score-0.254]
</p><p>96 The key idea is the use of structural representation of verbs based on syntactic dependencies and the use of structural kernels to measure similarity between such representations. [sent-435, score-0.575]
</p><p>97 Very interestingly, we can encode distributional lexical similarity in the similarity function acting over syntactic  structures and this allows for generalizing selection restrictions through a sort of (supervised) syntactic and semantic co-clustering. [sent-439, score-0.639]
</p><p>98 The verb classification results show a large improvement over the state-of-the-art for both VerbNet and FrameNet, with a relative error reduction of about 13. [sent-440, score-0.277]
</p><p>99 Combined syntactic and semantic kernels for text clas-  sification. [sent-458, score-0.342]
</p><p>100 Efficient convolution kernels for dependency and constituent syntactic trees. [sent-600, score-0.319]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sptk', 0.334), ('verbnet', 0.313), ('verb', 0.216), ('kernels', 0.207), ('nmod', 0.201), ('lct', 0.167), ('oprd', 0.167), ('obj', 0.164), ('lw', 0.16), ('grct', 0.151), ('ptk', 0.131), ('vb', 0.129), ('moschitti', 0.129), ('kernel', 0.126), ('tree', 0.123), ('class', 0.12), ('similarity', 0.114), ('nnp', 0.112), ('framenet', 0.112), ('croce', 0.107), ('dms', 0.1), ('target', 0.097), ('alessandro', 0.093), ('vp', 0.091), ('commission', 0.087), ('sk', 0.086), ('bloehdorn', 0.084), ('vbd', 0.082), ('sbj', 0.082), ('pighin', 0.082), ('verbs', 0.076), ('syntactic', 0.076), ('lexical', 0.074), ('vbg', 0.073), ('leaf', 0.07), ('pmod', 0.067), ('distributional', 0.065), ('lsa', 0.064), ('vc', 0.064), ('bow', 0.062), ('im', 0.062), ('ct', 0.062), ('gr', 0.062), ('classification', 0.061), ('structures', 0.061), ('semantic', 0.059), ('nodes', 0.056), ('none', 0.055), ('selectional', 0.054), ('stk', 0.053), ('supreme', 0.053), ('fragments', 0.052), ('structural', 0.051), ('audit', 0.05), ('sptks', 0.05), ('stkptksptk', 0.05), ('court', 0.05), ('vbn', 0.05), ('frame', 0.049), ('argument', 0.049), ('centered', 0.048), ('role', 0.048), ('vbz', 0.047), ('functions', 0.046), ('direct', 0.046), ('kudo', 0.045), ('substructures', 0.045), ('dligach', 0.044), ('br', 0.044), ('dt', 0.043), ('predicate', 0.043), ('lex', 0.043), ('martha', 0.043), ('object', 0.042), ('sense', 0.042), ('illinois', 0.041), ('synt', 0.04), ('something', 0.038), ('generalized', 0.038), ('classes', 0.038), ('dependency', 0.036), ('danilo', 0.035), ('np', 0.035), ('classifiers', 0.034), ('constituency', 0.034), ('generalizations', 0.033), ('landauer', 0.033), ('lexemes', 0.033), ('innovative', 0.033), ('ddn', 0.033), ('delegation', 0.033), ('expenses', 0.033), ('firth', 0.033), ('gianni', 0.033), ('giuglea', 0.033), ('golub', 0.033), ('lexicals', 0.033), ('semlink', 0.033), ('vbp', 0.033), ('vergata', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="214-tfidf-1" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>2 0.29193372 <a title="214-tfidf-2" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>3 0.27344203 <a title="214-tfidf-3" href="./acl-2012-State-of-the-Art_Kernels_for_Natural_Language_Processing.html">183 acl-2012-State-of-the-Art Kernels for Natural Language Processing</a></p>
<p>Author: Alessandro Moschitti</p><p>Abstract: unkown-abstract</p><p>4 0.21802711 <a title="214-tfidf-4" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>Author: Zhaopeng Tu ; Yifan He ; Jennifer Foster ; Josef van Genabith ; Qun Liu ; Shouxun Lin</p><p>Abstract: Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. 1</p><p>5 0.21372321 <a title="214-tfidf-5" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<p>Author: Ingrid Falk ; Claire Gardent ; Jean-Charles Lamirel</p><p>Abstract: We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70.</p><p>6 0.11996128 <a title="214-tfidf-6" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<p>7 0.11822063 <a title="214-tfidf-7" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>8 0.11224917 <a title="214-tfidf-8" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>9 0.10323995 <a title="214-tfidf-9" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>10 0.10254104 <a title="214-tfidf-10" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>11 0.093449131 <a title="214-tfidf-11" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>12 0.090770684 <a title="214-tfidf-12" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>13 0.089097574 <a title="214-tfidf-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.086441599 <a title="214-tfidf-14" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>15 0.086149052 <a title="214-tfidf-15" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>16 0.085793473 <a title="214-tfidf-16" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>17 0.080680296 <a title="214-tfidf-17" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>18 0.07730227 <a title="214-tfidf-18" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>19 0.076651104 <a title="214-tfidf-19" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>20 0.074741848 <a title="214-tfidf-20" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.238), (1, 0.069), (2, -0.127), (3, -0.127), (4, -0.045), (5, -0.02), (6, -0.062), (7, 0.25), (8, -0.172), (9, -0.082), (10, -0.013), (11, -0.21), (12, 0.323), (13, 0.156), (14, -0.078), (15, 0.121), (16, -0.135), (17, 0.08), (18, 0.079), (19, -0.057), (20, -0.088), (21, -0.032), (22, 0.073), (23, 0.026), (24, 0.012), (25, -0.14), (26, 0.006), (27, -0.029), (28, 0.083), (29, -0.001), (30, -0.086), (31, 0.015), (32, -0.083), (33, 0.071), (34, 0.042), (35, 0.005), (36, 0.034), (37, 0.004), (38, -0.009), (39, 0.013), (40, -0.038), (41, -0.021), (42, 0.024), (43, 0.011), (44, 0.033), (45, -0.015), (46, 0.012), (47, 0.022), (48, -0.041), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94207966 <a title="214-lsi-1" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>2 0.79223752 <a title="214-lsi-2" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>3 0.75949532 <a title="214-lsi-3" href="./acl-2012-State-of-the-Art_Kernels_for_Natural_Language_Processing.html">183 acl-2012-State-of-the-Art Kernels for Natural Language Processing</a></p>
<p>Author: Alessandro Moschitti</p><p>Abstract: unkown-abstract</p><p>4 0.64644957 <a title="214-lsi-4" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>Author: Zhaopeng Tu ; Yifan He ; Jennifer Foster ; Josef van Genabith ; Qun Liu ; Shouxun Lin</p><p>Abstract: Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. 1</p><p>5 0.56658459 <a title="214-lsi-5" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<p>Author: Ingrid Falk ; Claire Gardent ; Jean-Charles Lamirel</p><p>Abstract: We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70.</p><p>6 0.53936881 <a title="214-lsi-6" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>7 0.53158391 <a title="214-lsi-7" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<p>8 0.37492892 <a title="214-lsi-8" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>9 0.37358704 <a title="214-lsi-9" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>10 0.36186302 <a title="214-lsi-10" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>11 0.34132272 <a title="214-lsi-11" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>12 0.34060061 <a title="214-lsi-12" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>13 0.32872674 <a title="214-lsi-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.3051922 <a title="214-lsi-14" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>15 0.30138204 <a title="214-lsi-15" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>16 0.29842871 <a title="214-lsi-16" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>17 0.29818928 <a title="214-lsi-17" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>18 0.2936171 <a title="214-lsi-18" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>19 0.28746545 <a title="214-lsi-19" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>20 0.28604051 <a title="214-lsi-20" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (22, 0.032), (25, 0.039), (26, 0.05), (28, 0.039), (30, 0.03), (37, 0.076), (39, 0.046), (48, 0.031), (57, 0.01), (74, 0.029), (82, 0.02), (84, 0.026), (85, 0.052), (90, 0.088), (92, 0.07), (94, 0.025), (98, 0.206), (99, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90808427 <a title="214-lda-1" href="./acl-2012-A_Web-based_Evaluation_Framework_for_Spatial_Instruction-Giving_Systems.html">24 acl-2012-A Web-based Evaluation Framework for Spatial Instruction-Giving Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We demonstrate a web-based environment for development and testing of different pedestrian route instruction-giving systems. The environment contains a City Model, a TTS interface, a game-world, and a user GUI including a simulated street-view. We describe the environment and components, the metrics that can be used for the evaluation of pedestrian route instruction-giving systems, and the shared challenge which is being organised using this environment.</p><p>2 0.9010815 <a title="214-lda-2" href="./acl-2012-INPROwidth.3emiSS%3A_A_Component_for_Just-In-Time_Incremental_Speech_Synthesis.html">113 acl-2012-INPROwidth.3emiSS: A Component for Just-In-Time Incremental Speech Synthesis</a></p>
<p>Author: Timo Baumann ; David Schlangen</p><p>Abstract: We present a component for incremental speech synthesis (iSS) and a set of applications that demonstrate its capabilities. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities.</p><p>same-paper 3 0.77852327 <a title="214-lda-3" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>4 0.70562088 <a title="214-lda-4" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>Author: Xiaohua Liu ; Furu Wei ; Ming Zhou ; QuickView Team Microsoft</p><p>Abstract: Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.</p><p>5 0.59294683 <a title="214-lda-5" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>6 0.57968229 <a title="214-lda-6" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>7 0.57858372 <a title="214-lda-7" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>8 0.5762136 <a title="214-lda-8" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>9 0.57098222 <a title="214-lda-9" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>10 0.56845105 <a title="214-lda-10" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>11 0.56591731 <a title="214-lda-11" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>12 0.56481409 <a title="214-lda-12" href="./acl-2012-IRIS%3A_a_Chat-oriented_Dialogue_System_based_on_the_Vector_Space_Model.html">114 acl-2012-IRIS: a Chat-oriented Dialogue System based on the Vector Space Model</a></p>
<p>13 0.56265002 <a title="214-lda-13" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>14 0.56193471 <a title="214-lda-14" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>15 0.56169194 <a title="214-lda-15" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>16 0.56051093 <a title="214-lda-16" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>17 0.55977827 <a title="214-lda-17" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>18 0.55920595 <a title="214-lda-18" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>19 0.55916101 <a title="214-lda-19" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>20 0.55840945 <a title="214-lda-20" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
