<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-95" href="#">acl2012-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</h1>
<br/><p>Source: <a title="acl-2012-95-pdf" href="http://aclweb.org/anthology//P/P12/P12-1019.pdf">pdf</a></p><p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>Reference: <a title="acl-2012-95-reference" href="../acl2012_reference/acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. [sent-3, score-0.38]
</p><p>2 In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. [sent-4, score-0.598]
</p><p>3 We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. [sent-5, score-1.04]
</p><p>4 When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction. [sent-6, score-0.81]
</p><p>5 However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N-best lists (Khudanpur and Wu, 2000; Collins et al. [sent-17, score-0.381]
</p><p>6 Moreover, the non-local features used in rescoring are usually extracted via auxiliary tools which in the case of syntactic features include part of speech taggers and parsers from a set of ASR system hypotheses. [sent-21, score-0.578]
</p><p>7 Separately applying auxiliary tools to each N-best list hypothesis leads to major inefficiencies as many hypotheses differ only slightly. [sent-22, score-0.424]
</p><p>8 Recent work on hill climbing algorithms for ASR lattice rescoring iteratively searches for a higherscoring hypothesis in a local neighborhood of the current-best hypothesis, leading to a much more efficient algorithm in terms of the number, N, of hypotheses evaluated (Rastrow et al. [sent-23, score-1.224]
</p><p>9 , 2011b); the idea also leads to a discriminative hill climbing training algorithm (Rastrow et al. [sent-24, score-0.722]
</p><p>10 Even so, the reliance on auxiliary tools slow LM application to the point of being impractical for real time systems. [sent-26, score-0.227]
</p><p>11 The key idea is to share substructure states in transition based structured prediction algorithms, i. [sent-31, score-0.591]
</p><p>12 We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al. [sent-34, score-0.218]
</p><p>13 , 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. [sent-35, score-0.377]
</p><p>14 , 2010), yielding auxiliary tools that are both fast and accurate. [sent-37, score-0.215]
</p><p>15 The result is significant speed improvements and a reduction in word error rate (WER) for both N-best list and the already fast hill climbing rescoring. [sent-38, score-0.72]
</p><p>16 2 Syntactic Language Models There have been several approaches to include syntactic information in both generative and discriminative language models. [sent-40, score-0.21]
</p><p>17 Structured language modeling incorporates syntactic parse trees to identify the head words in a hypothesis for modeling dependencies beyond n-grams. [sent-42, score-0.309]
</p><p>18 Khudanpur and Wu (2000) exploit such syntactic head word dependencies as features in a maximum entropy framework. [sent-44, score-0.203]
</p><p>19 (2009) integrate syntactic features into a neural network LM for Arabic speech recognition. [sent-46, score-0.226]
</p><p>20 Additionally, discriminative models are directly trained to resolve the acoustic confusion in the  decoded hypotheses of an ASR system. [sent-48, score-0.279]
</p><p>21 (2005) uses the Perceptron algorithm to train a global linear discriminative model 176 which incorporates long-span features, such as headto-head dependencies and part of speech tags. [sent-51, score-0.253]
</p><p>22 The LM score S(w, a) for each hypothesis w of a speech Mutte srcaonrece S w(with, aac)o fuosrt eica sequence a siiss s b wase odf on the baseline ASR system score b(w, a) (initial ngram LM score and the acoustic score) and α0, the weight assigned to the baseline score. [sent-57, score-0.428]
</p><p>23 ,sm) α0 ·  Xi= X1  where F is the discriminative LM’s score for the hypothesis w, and s1, . [sent-64, score-0.28]
</p><p>24 , sm)i Our nLoMta uses f eSa(twur,eas )fr =om h tαh,eΦ dependency tree an)di part of speech (POS) tag sequence. [sent-76, score-0.24]
</p><p>25 (2009) to identify the two previous exposed head words, h−2, h−1, at each position iin the input hypothesis and include the following syntactic based features into our LM:  sm  1. [sent-78, score-0.464]
</p><p>26 1 Hill Climbing Rescoring We adopt the so called hill climbing framework of Rastrow et al. [sent-90, score-0.592]
</p><p>27 Given a speech utterance’s lattice L from a first pass vAeSnR a decoder, tthteer neighborhood N(w, i) ofifr a hypothesis w = w1 w2 . [sent-95, score-0.461]
</p><p>28 Given a position iin a word sequence w, all hypotheses in N(w, i) are rescored using the longspan tmheosdeesl iann dN t(hwe hypothesis wˆ 0(i) swinithg tthhee highest score becomes the new w. [sent-100, score-0.419]
</p><p>29 Incorporating this into training yields a discriminative hill climbing algorithm (Ras–  –  trow et al. [sent-107, score-0.693]
</p><p>30 3  Incorporating Syntactic Structures  Long-span models generative or discriminative, N-best or hill climbing rely on auxiliary tools, such as a POS tagger or a parser, for extracting features for each hypothesis during rescoring, and during training for discriminative models. [sent-109, score-1.104]
</p><p>31 For example, can be a part of speech tag or a syntactic dependency. [sent-117, score-0.229]
</p><p>32 A major complexity factor is due to processing 100s or 1000s of hypotheses for each speech utterance, even during hill climbing, each of which must be POS tagged and parsed. [sent-154, score-0.505]
</p><p>33 However, the candidate hypotheses of an utterance share equivalent substructures, especially in hill climbing methods due to the locality present in the neighborhood generation. [sent-155, score-0.904]
</p><p>34 Figure 1 demonstrates such repetition in an N-best list (N=10) and a hill climbing neighborhood hypothesis set for a speech utterance from broadcast news. [sent-156, score-1.005]
</p><p>35 For example, the word “ENDORSE” occurs within the same local context in all hypotheses and should receive the same part of speech tag in each case. [sent-157, score-0.258]
</p><p>36 We propose a general algorithmic approach to reduce the complexity of processing a hypothesis set by sharing common substructures among the hypotheses. [sent-159, score-0.342]
</p><p>37 We first present our approach and then demonstrate its generality by applying it to a dependency parser and part of speech tagger. [sent-161, score-0.381]
</p><p>38 All states along the way are chosen from the possible states Π. [sent-164, score-0.304]
</p><p>39 These actions are applied |toπ transition to new states πi+1 . [sent-179, score-0.288]
</p><p>40 The N-best listHill climbing neighborhood  score of the new state is then p(πi+1) = pg (ωi |πi) · p(πi) (1) Classification decisions require a feature representation of πi, which is provided by feature functions f : Π → Y, that map states to features. [sent-181, score-0.801]
</p><p>41 Equivalent states are defined as two states π and π0 with an identical feature representation: π ≡ π0 iff f(π) = f(π0)  ×  If two states are equivalent, then g imposes the same distribution over actions. [sent-184, score-0.456]
</p><p>42 We can benefit from this substructure redundancy, both within and between hypotheses, by saving these distributions in memory, sharing a distribution computed just once across equivalent states. [sent-185, score-0.54]
</p><p>43 A similar idea of equivalent states is used by Huang and Sagae (2010), except they use equivalence to facilitate dynamic programming for shift-reduce parsing, whereas we generalize it for improving the processing time of similar hypotheses in general models. [sent-186, score-0.36]
</p><p>44 (2)  Equivalent distributions are stored in a hash table H : → Ω R; the hash keys are the states and the Hval :ue Πs are Ωdi×strRib;u thtioen hsa2s over asct airoen ths:e {ω, pg (ω|π)}. [sent-188, score-0.462]
</p><p>45 178 H caches equivalent states in a hypothesis set and re-  sets for each new utterance. [sent-190, score-0.365]
</p><p>46 For each state, we first check H for equivalent states before computing the action distribution; each cache hit reduces decoding time. [sent-191, score-0.347]
</p><p>47 Distributing hypotheses wi across different CPU threads is another way to obtain speedups, and we can still benefit from substructure sharing by storing H in shared memory. [sent-192, score-0.741]
</p><p>48 To apply substructure sharing to a transition based model, we need only define the set of states Π (including π0 and πf), actions Ω and kernel feature functions f˜. [sent-198, score-0.823]
</p><p>49 The resulting speedup depends on the amount of substructure duplication among the hypotheses, which we will show is significant for ASR lattice rescoring. [sent-199, score-0.491]
</p><p>50 2 Dependency Parsing We use the best-first probabilistic shift-reduce dependency parser of Sagae and Tsujii (2007), a transition-based parser (K¨ ubler et al. [sent-203, score-0.453]
</p><p>51 Substructure sharing reduces  ×  these steps for equivalent states, which are persistent throughout a candidate set. [sent-235, score-0.228]
</p><p>52 We summarize substructure sharing for dependency parsing in Algorithm 1. [sent-237, score-0.622]
</p><p>53 We extend the definition of states to be {S, Q, p} where p denotes the score no fo ft sheta tsetsat teo: bthee { probability eorfe th pe d aecnotiotens sequence that resulted in the current state. [sent-238, score-0.246]
</p><p>54 find (πcurrent ) ActList ← H[πcurrent ] [retrieve action list from the hash table] elseA [need to construct action list] for all ω ∈ Ω [for all actions] pω ←ω pg (ω |πcurrent ) [action score] Act←List p . [sent-242, score-0.438]
</p><p>55 uinrsreertn(t{ , ωA, pctLi})st) [Store the action list into hash table] end if for all {ω , pω } ∈ ActList [compute new states] πnew ←, p πcurrent ×L ω Heap. [sent-245, score-0.216]
</p><p>56 pu←sh(π πnew ) [push to the heap] end while  =  ×  lowing Sagae and Tsujii (2007) a heap is used to maintain states prioritized by their scores, for applying the best-first strategy. [sent-246, score-0.296]
</p><p>57 For each step, a state from the top of the heap is considered and all actions (and scores) are either retrieved from H or computed using g. [sent-247, score-0.302]
</p><p>58 3 Part of Speech Tagging We use the part of speech (POS) tagger of Tsuruoka et al. [sent-250, score-0.218]
</p><p>59 Whi)le t =he tagger extracts prefix and suffix features, it suffices to look at wi for determining state equiv-  f˜(πi)  oΩ w  alence. [sent-262, score-0.357]
</p><p>60 Search space pruning is achieved by filtering heap states for probability greater than b1 the probability of the most likely state in the heap with the same number of actions. [sent-265, score-0.526]
</p><p>61 4We note that while we have demonstrated substructure sharing for dependency parsing, the same improvements can be made to a shift-reduce constituent parser (Sagae and Lavie, 2006). [sent-267, score-0.852]
</p><p>62 1wti|T12|wti21+T |1 wi+2 3  Figure 2: POS tagger with lookahead search of d=1. [sent-270, score-0.226]
</p><p>63 At wi the search considers the current state and next state. [sent-271, score-0.316]
</p><p>64 first search lookahead procedure to select the best  action at each step, which considers future decisions up to depth d5. [sent-272, score-0.213]
</p><p>65 Using d = 1for the lookahead search strategy, we modify the kernel features since the decision for wi is affected by the state πi+1. [sent-274, score-0.467]
</p><p>66 The kernel features in position ishould be f˜(πi) ∪ ˜f(πi+1): f˜(πi) = {ti−2, ti−1 , wi−2,  wi−1  , wi, wi+1 , wi+2, wi+3}  4 Up-Training While we have fast decoding algorithms for the parsing and tagging, the simpler underlying models can lead to worse performance. [sent-275, score-0.36]
</p><p>67 (2010) used up-training as a domain adaptation technique: a constituent parser which is more robust to domain changes was used to label a new domain, and a fast dependency parser –  –  5 Tsuruoka et al. [sent-284, score-0.605]
</p><p>68 We apply up-training to improve the accuracy of both our fast POS tagger and dependency parser. [sent-290, score-0.271]
</p><p>69 We parse a large corpus of text with a very accurate but very slow constituent parser and use the resulting  data to up-train our tools. [sent-291, score-0.343]
</p><p>70 5 Related Work The idea of efficiently processing a hypothesis set is similar to “lattice-parsing”, in which a parser consider an entire lattice at once (Hall, 2005; Cheppalier et al. [sent-293, score-0.466]
</p><p>71 In other words, they search in the joint space of word sequences present in the lattice and their syntactic analyses; they are not guaranteed to produce a syntactic analysis for all hypotheses. [sent-296, score-0.311]
</p><p>72 In contrast, substructure sharing is a general purpose method that we have applied to two different algorithms. [sent-297, score-0.47]
</p><p>73 Hall (Hall, 2005) uses a lattice parsing strategy which aims to compute the marginal probabilities of all word sequences in the lattice by summing over syntactic analyses of each word sequence. [sent-299, score-0.367]
</p><p>74 The parser sums over multiple parses of a word sequence implicitly. [sent-300, score-0.209]
</p><p>75 The lattice parser there-  fore, is itself a language model. [sent-301, score-0.294]
</p><p>76 These differences make substructure sharing a more attractive option for efficient algorithms. [sent-304, score-0.47]
</p><p>77 While Huang and Sagae (2010) use the notion of “equivalent states”, they do so for dynamic programming in a shift-reduce parser to broaden the search space. [sent-305, score-0.219]
</p><p>78 Additionally, we extend the definition of equivalent states to general transition based structured prediction models, and demonstrate applications beyond parsing as well as the novel setting of hypothesis set parsing. [sent-307, score-0.524]
</p><p>79 , 2006) with state of the art discriminative acoustic models. [sent-309, score-0.227]
</p><p>80 Word-lattices for discriminative training and rescoring come from this baseline ASR system. [sent-312, score-0.272]
</p><p>81 6 The longspan discriminative LM’s baseline feature weight (α0) is tuned on dev data and hill climbing (Rastrow et al. [sent-313, score-0.734]
</p><p>82 The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. [sent-315, score-0.643]
</p><p>83 Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem. [sent-318, score-0.433]
</p><p>84 Figure 3 shows improvements to parser accuracy through up-training for different amount of  (randomly selected) data, where the last column indicates constituent parser score (91. [sent-322, score-0.508]
</p><p>85 While there is a large difference between the constituent and dependency parser without up-training (91. [sent-325, score-0.353]
</p><p>86 ) The first  column is the dependency parser with supervised training only and the last column is the constituent parser (after converting to dependency trees. [sent-330, score-0.625]
</p><p>87 )8 The dependency parser remains much smaller and faster; the up-trained dependency model is 700MB with 6m features compared with 32GB for constituency model. [sent-335, score-0.4]
</p><p>88 We train the syntactic discriminative LM, with head-word and POS tag features, using the faster parser and tagger and then rescore the ASR hypotheses. [sent-339, score-0.586]
</p><p>89 Detailed speedups on substructure sharing are shown in Table 4; the POS tagger achieves a 5. [sent-342, score-0.703]
</p><p>90 )  The above results are for the already fast hill climbing decoding, but substructure sharing can also be used for N-best list rescoring. [sent-346, score-1.161]
</p><p>91 Figure 4 (logarithmic scale) illustrates the time for the parser and tagger to process N-best lists ofvarying size, with more substantial speedups for larger lists. [sent-347, score-0.414]
</p><p>92 For example, for N=100 (a typical setting) the parsing time re8Better performance is due to the exact CKY-style – compared with best-first and beam– search and that the constituent parser uses the product of huge self-trained grammars. [sent-348, score-0.361]
</p><p>93 Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N-best lists with and without substructure sharing. [sent-349, score-0.373]
</p><p>94 Time spent is dominated by the parser, so the faster parser accounts for much of the overall speedup. [sent-356, score-0.225]
</p><p>95 7 Conclusion The computational complexity of accurate syntactic processing can make structured language models impractical for applications such as ASR that require scoring hundreds of hypotheses per input. [sent-360, score-0.332]
</p><p>96 presented substructure sharing, a general framework that greatly improves the speed of syntactic tools that process candidate hypotheses. [sent-367, score-0.472]
</p><p>97 The result is a large speedup in rescoring time, even on top of the already fast hill climbing framework, and reductions in WER from up-training. [sent-369, score-0.931]
</p><p>98 Acknowledgments Thanks to Kenji Sagae for sharing his shift-reduce dependency parser and the anonymous reviewers for helpful comments. [sent-371, score-0.43]
</p><p>99 Hill climbing on speech lattices : A new rescoring framework. [sent-467, score-0.622]
</p><p>100 Dependency parsing and domain adaptation with LR models and parser ensembles. [sent-485, score-0.271]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('substructure', 0.312), ('climbing', 0.305), ('hill', 0.287), ('asr', 0.246), ('parser', 0.181), ('rescoring', 0.171), ('rastrow', 0.165), ('wi', 0.162), ('sharing', 0.158), ('states', 0.152), ('heap', 0.144), ('hypothesis', 0.143), ('sagae', 0.142), ('pg', 0.126), ('speedups', 0.124), ('kuo', 0.115), ('lattice', 0.113), ('speech', 0.109), ('tagger', 0.109), ('hypotheses', 0.109), ('lm', 0.108), ('discriminative', 0.101), ('neighborhood', 0.096), ('wer', 0.096), ('action', 0.096), ('hash', 0.092), ('dependency', 0.091), ('lms', 0.089), ('state', 0.086), ('ariya', 0.082), ('uptraining', 0.082), ('constituent', 0.081), ('tools', 0.08), ('syntactic', 0.08), ('lookahead', 0.079), ('khudanpur', 0.073), ('actions', 0.072), ('pos', 0.071), ('fast', 0.071), ('equivalent', 0.07), ('speedup', 0.066), ('kernel', 0.065), ('auxiliary', 0.064), ('transition', 0.064), ('parsing', 0.061), ('tsuruoka', 0.059), ('ti', 0.057), ('tsujii', 0.056), ('perceptron', 0.055), ('sm', 0.053), ('exposed', 0.046), ('chelba', 0.046), ('faster', 0.044), ('dependencies', 0.043), ('head', 0.043), ('slow', 0.042), ('oo', 0.041), ('sanjeev', 0.041), ('impractical', 0.041), ('substructures', 0.041), ('actlist', 0.041), ('cheppalier', 0.041), ('filimonov', 0.041), ('longspan', 0.041), ('tag', 0.04), ('uas', 0.04), ('acoustic', 0.04), ('roark', 0.04), ('accurate', 0.039), ('collins', 0.039), ('search', 0.038), ('utterance', 0.037), ('features', 0.037), ('lattices', 0.037), ('simpler', 0.037), ('mangu', 0.036), ('whi', 0.036), ('score', 0.036), ('huang', 0.035), ('structured', 0.034), ('sim', 0.033), ('dredze', 0.033), ('position', 0.031), ('iin', 0.031), ('reductions', 0.031), ('rescore', 0.031), ('current', 0.03), ('models', 0.029), ('idea', 0.029), ('decoding', 0.029), ('generative', 0.029), ('improvements', 0.029), ('sequence', 0.028), ('list', 0.028), ('beam', 0.028), ('structures', 0.028), ('terminate', 0.028), ('transcription', 0.028), ('goldberg', 0.028), ('deterministic', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="95-tfidf-1" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>2 0.17876443 <a title="95-tfidf-2" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</p><p>3 0.15773006 <a title="95-tfidf-3" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>4 0.14576611 <a title="95-tfidf-4" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>5 0.14052556 <a title="95-tfidf-5" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>6 0.1343527 <a title="95-tfidf-6" href="./acl-2012-Automated_Essay_Scoring_Based_on_Finite_State_Transducer%3A_towards_ASR_Transcription_of_Oral_English_Speech.html">32 acl-2012-Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech</a></p>
<p>7 0.12880106 <a title="95-tfidf-7" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>8 0.12598087 <a title="95-tfidf-8" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>9 0.12147058 <a title="95-tfidf-9" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>10 0.12014608 <a title="95-tfidf-10" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>11 0.11655039 <a title="95-tfidf-11" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>12 0.10476479 <a title="95-tfidf-12" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>13 0.10173013 <a title="95-tfidf-13" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>14 0.099384949 <a title="95-tfidf-14" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>15 0.099180572 <a title="95-tfidf-15" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>16 0.09907493 <a title="95-tfidf-16" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>17 0.098909408 <a title="95-tfidf-17" href="./acl-2012-Efficient_Search_for_Transformation-based_Inference.html">78 acl-2012-Efficient Search for Transformation-based Inference</a></p>
<p>18 0.096931614 <a title="95-tfidf-18" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>19 0.093730882 <a title="95-tfidf-19" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>20 0.089134455 <a title="95-tfidf-20" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.24), (1, -0.032), (2, -0.191), (3, -0.151), (4, -0.079), (5, 0.011), (6, 0.057), (7, -0.049), (8, 0.054), (9, -0.022), (10, -0.001), (11, 0.079), (12, 0.002), (13, -0.011), (14, 0.056), (15, 0.063), (16, 0.0), (17, 0.15), (18, 0.033), (19, 0.002), (20, 0.069), (21, -0.114), (22, 0.016), (23, -0.039), (24, -0.055), (25, -0.064), (26, 0.162), (27, 0.16), (28, -0.109), (29, 0.028), (30, 0.026), (31, -0.066), (32, -0.065), (33, 0.011), (34, -0.045), (35, 0.044), (36, -0.109), (37, 0.157), (38, 0.139), (39, 0.149), (40, -0.046), (41, -0.044), (42, -0.032), (43, -0.082), (44, 0.089), (45, 0.042), (46, 0.029), (47, -0.063), (48, -0.027), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93848187 <a title="95-lsi-1" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>2 0.70735836 <a title="95-lsi-2" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>Author: Khe Chai Sim</p><p>Abstract: This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition (HVR), a multimodal input method designed to provide efficient text entry on modern mobile devices. HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. Experimental results show that both the word error rate and runtime factor can be re- duced by a factor of two using HVR.</p><p>3 0.67136681 <a title="95-lsi-3" href="./acl-2012-Automated_Essay_Scoring_Based_on_Finite_State_Transducer%3A_towards_ASR_Transcription_of_Oral_English_Speech.html">32 acl-2012-Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech</a></p>
<p>Author: Xingyuan Peng ; Dengfeng Ke ; Bo Xu</p><p>Abstract: Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters.</p><p>4 0.60728323 <a title="95-lsi-4" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>5 0.5674513 <a title="95-lsi-5" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</p><p>6 0.56012326 <a title="95-lsi-6" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>7 0.55262774 <a title="95-lsi-7" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>8 0.54582226 <a title="95-lsi-8" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>9 0.54146761 <a title="95-lsi-9" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>10 0.51239252 <a title="95-lsi-10" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>11 0.49567482 <a title="95-lsi-11" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>12 0.47020343 <a title="95-lsi-12" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>13 0.4678576 <a title="95-lsi-13" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>14 0.46720383 <a title="95-lsi-14" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>15 0.46597096 <a title="95-lsi-15" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>16 0.45974094 <a title="95-lsi-16" href="./acl-2012-The_OpenGrm_open-source_finite-state_grammar_software_libraries.html">196 acl-2012-The OpenGrm open-source finite-state grammar software libraries</a></p>
<p>17 0.42698339 <a title="95-lsi-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.42373654 <a title="95-lsi-18" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>19 0.41847029 <a title="95-lsi-19" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>20 0.40339515 <a title="95-lsi-20" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.014), (26, 0.029), (28, 0.026), (30, 0.019), (37, 0.051), (39, 0.03), (71, 0.014), (74, 0.415), (82, 0.02), (84, 0.022), (85, 0.034), (90, 0.12), (92, 0.064), (94, 0.015), (99, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95743495 <a title="95-lda-1" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>Author: Toshikazu Tajiri ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate con- tribution to tense/aspect error correction.</p><p>2 0.92087597 <a title="95-lda-2" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>Author: Yunyao Li ; Laura Chiticariu ; Huahai Yang ; Frederick Reiss ; Arnaldo Carreno-fuentes</p><p>Abstract: Information extraction (IE) is becoming a critical building block in many enterprise applications. In order to satisfy the increasing text analytics demands of enterprise applications, it is crucial to enable developers with general computer science background to develop high quality IE extractors. In this demonstration, we present WizIE, an IE development environment intended to reduce the development life cycle and enable developers with little or no linguistic background to write high quality IE rules. WizI E provides an integrated wizard-like environment that guides IE developers step-by-step throughout the entire development process, based on best practices synthesized from the experience of expert developers. In addition, WizIE reduces the manual effort involved in performing key IE development tasks by offering automatic result explanation and rule discovery functionality. Preliminary results indicate that WizI E is a step forward towards enabling extractor development for novice IE developers.</p><p>same-paper 3 0.83788222 <a title="95-lda-3" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>4 0.80331302 <a title="95-lda-4" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>5 0.64683872 <a title="95-lda-5" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>6 0.59077543 <a title="95-lda-6" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>7 0.5307613 <a title="95-lda-7" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>8 0.52980942 <a title="95-lda-8" href="./acl-2012-Grammar_Error_Correction_Using_Pseudo-Error_Sentences_and_Domain_Adaptation.html">103 acl-2012-Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation</a></p>
<p>9 0.52290654 <a title="95-lda-9" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>10 0.51705414 <a title="95-lda-10" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>11 0.51548493 <a title="95-lda-11" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>12 0.5152505 <a title="95-lda-12" href="./acl-2012-FLOW%3A_A_First-Language-Oriented_Writing_Assistant_System.html">92 acl-2012-FLOW: A First-Language-Oriented Writing Assistant System</a></p>
<p>13 0.51249242 <a title="95-lda-13" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>14 0.50846428 <a title="95-lda-14" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.50023228 <a title="95-lda-15" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>16 0.4936102 <a title="95-lda-16" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>17 0.49231932 <a title="95-lda-17" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>18 0.48702869 <a title="95-lda-18" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>19 0.48671138 <a title="95-lda-19" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>20 0.48108113 <a title="95-lda-20" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
