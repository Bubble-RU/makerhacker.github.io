<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-122" href="#">acl2012-122</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</h1>
<br/><p>Source: <a title="acl-2012-122-pdf" href="http://aclweb.org/anthology//P/P12/P12-2002.pdf">pdf</a></p><p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>Reference: <a title="acl-2012-122-reference" href="../acl2012_reference/acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 se  Abstract We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. [sent-8, score-0.638]
</p><p>2 The protocol uses distance-based metrics defined for the space of trees over lattices. [sent-9, score-0.247]
</p><p>3 Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). [sent-10, score-0.817]
</p><p>4 Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios. [sent-11, score-0.391]
</p><p>5 1 Introduction A parser takes a sentence in natural language as input and returns a syntactic parse tree representing the sentence’s human-perceived interpretation. [sent-12, score-0.268]
</p><p>6 Current state-of-the-art parsers assume that the space-  delimited words in the input are the basic units of syntactic analysis. [sent-13, score-0.148]
</p><p>7 , 1991 ; Buchholz and Marsi, 2006) accordingly assume that the yield of the parse tree is known in advance. [sent-15, score-0.108]
</p><p>8 This assumption breaks down when parsing morphologically rich languages (Tsarfaty et al. [sent-16, score-0.317]
</p><p>9 , 2010), where every space-delimited word may be effectively composed of multiple morphemes, each of which having a distinct role in the syntactic parse tree. [sent-17, score-0.114]
</p><p>10 In order to parse such input the text needs to undergo morphological segmentation, that is, identifying the morphological segments of each word and assigning the corresponding part-ofspeech (PoS) tags to them. [sent-18, score-0.671]
</p><p>11 The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence. [sent-20, score-0.618]
</p><p>12 One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let  the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). [sent-21, score-0.38]
</p><p>13 If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics break down. [sent-22, score-0.655]
</p><p>14 Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upperbound on parser performance. [sent-23, score-0.321]
</p><p>15 This paper presents a full treatment of evaluation in different parsing scenarios, using distance-based measures defined for trees over a shared common denominator defined in terms of a lattice structure. [sent-24, score-0.365]
</p><p>16 We demonstrate the informativeness of our metrics by evaluating joint segmentation and parsing performance for the Semitic language Modern Hebrew, using the best performing systems, both constituencybased and dependency-based (Tsarfaty, 2010; Goldberg, 2011a). [sent-25, score-0.409]
</p><p>17 Our experiments demonstrate that, for all parsers, significant performance gaps between realistic and non-realistic scenarios crucially depend on the kind of information initially provided to the parser. [sent-26, score-0.15]
</p><p>18 The tool and metrics that we provide are  completely general and can straightforwardly apply to other languages, treebanks and different tasks. [sent-27, score-0.09]
</p><p>19 Erroneous nodes in the parse hypothesis are marked in italics. [sent-31, score-0.122]
</p><p>20 Missing nodes from the hypothesis are marked in bold. [sent-32, score-0.055]
</p><p>21 2  The Challenge: Evaluation for MRLs  In morphologically rich languages (MRLs) substantial information about the grammatical relations between entities is expressed at word level using inflectional affixes. [sent-33, score-0.162]
</p><p>22 In particular, in MRLs such as Hebrew, Arabic, Turkish or Maltese, elements such as determiners, definite articles and conjunction markers appear as affixes that are appended to an openclass word. [sent-34, score-0.055]
</p><p>23 Note that morphological segmentation is not the inverse of concatenation. [sent-37, score-0.397]
</p><p>24 For instance, the overt definite article H and the possessor FL show up only in the analysis. [sent-38, score-0.097]
</p><p>25 The correct parse for the Hebrew phrase “BCLM HNEIM” is shown in Figure 1 (tree1), and it presupposes that these segments can be identified and assigned the correct PoS tags. [sent-39, score-0.109]
</p><p>26 However, morphological segmentation is non-trivial due to massive wordlevel ambiguity. [sent-40, score-0.397]
</p><p>27 2 The multitude of morphological analyses may be encoded in a lattice structure, as illustrated in Figure 2. [sent-42, score-0.42]
</p><p>28 2The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). [sent-45, score-0.05]
</p><p>29 7  Figure 2: The morphological segmentation possibilities of BCLM HNEIM. [sent-47, score-0.397]
</p><p>30 In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice. [sent-49, score-0.242]
</p><p>31 , 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010). [sent-51, score-0.118]
</p><p>32 Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals. [sent-52, score-0.786]
</p><p>33 To understand why, consider the trees in Figure 1. [sent-54, score-0.119]
</p><p>34 , 1991) calculate the harmonic means of precision and recall  on labeled spans hi, label, ji where i,j are termionanl l baboeulneddar siepsa. [sent-56, score-0.063]
</p><p>35 n Now, tbheel ,NjiP dominating r“esh taedrmowiof them” has been identified and labeled correctly in tree2, but in tree1 it spans h2, NP, 5i and in tree2 iitn spans h1, NP, 4i. [sent-57, score-0.161]
</p><p>36 1T ihti ssp anonsde h 2w,Nil P t,h5ein a n bed c ionu trnetee2d as an error NfoPr tree2, along dwei twh iltls t hdeonm binea ctoeud natendd dominating structure, and PARSEVAL will score 0. [sent-58, score-0.035]
</p><p>37 A generalized version of PARSEVAL which considers i,j character-based indices instead of terminal boundaries (Tsarfaty, 2006) will fail here too, since the missing overt definite article H will cause similar misalignments. [sent-59, score-0.139]
</p><p>38 Metrics for dependencybased evaluation such as ATTACHMENT SCORES (Buchholz and Marsi, 2006) suffer from similar problems, since they assume that both trees have the same nodes an assumption that breaks down in the case of incorrect morphological segmentation. [sent-60, score-0.492]
</p><p>39 Although great advances have been made in parsing MRLs in recent years, this evaluation challenge remained unsolved. [sent-61, score-0.152]
</p><p>40 —  3  The Proposal: Distance-Based Metrics  Input and Output Spaces We view the joint task as a structured prediction function h : X → Y from input space eXd pornetdoi output space Y. [sent-64, score-0.043]
</p><p>41 , wn hof e spacedxel ∈imi Xted i sw oard sesq furoenmc a xset =W . [sent-68, score-0.039]
</p><p>42 w We assume a lexicon LEX, ditiesdtin wcot fdros mfro W, containing pairs mofe segments draw,n d firstoimnc a set mT W Wof, t ceormntianianilns gan pda iPrsoS o categories ddrraawwnn f frroomm a set NT ooff tneormntienramlsin aanlds. [sent-69, score-0.042]
</p><p>43 LEX = {hs, pi |s  ∈  T ,p  ∈  N}  Each word wi in the input may admit multiple morphological analyses, constrained by a languagespecific morphological analyzer MA. [sent-70, score-0.59]
</p><p>44 The morphological analysis of an input word MA(wi) can be represented as a lattice Li in which every arc corresponds to a lexicon entry hs, pi. [sent-71, score-0.413]
</p><p>45 The morphologircesalp analysis ao lfe an input tsreynt hesn,pcei x ish eth meno a hloatltoicgeL obtained through the concatenation of the lattices L1, . [sent-72, score-0.077]
</p><p>46 , wn be a sentence with a morphological analysis lattice MA(x) = L. [sent-82, score-0.409]
</p><p>47 We define the output space YMA(x)=L for h (abbreviated YL), as hthee o suettp uoft linearly-ordered labeled trees such tYhat the yield of LEX entries hs1,p1i,. [sent-83, score-0.119]
</p><p>48 ,hsk, pki in  each tree (where si ∈ Tnt rainesd pi ∈ Ni,,. [sent-86, score-0.104]
</p><p>49 Cohen and Smith (2007) aimed to fix this, but in their implementation syntactic nodes internal to word boundaries may be lost without scoring. [sent-93, score-0.102]
</p><p>50 Tih ∈e operations in A are properly constrained by the lattice, athtiaotn is, we can only aedrldy a cnodn dsetrlaeitnee dlex beym tehse t lhaat-t belong to LEX, and we can only add and delete them where they can occur in the lattice. [sent-95, score-0.039]
</p><p>51 , ami as tahned sum noef tthhee ccoosstts o off a a slle operathiaons in the sequence C(ha1 , . [sent-99, score-0.05]
</p><p>52 , aPmi is a sequence of operations that) )tu =rns h y1 into y2. [sent-106, score-0.039]
</p><p>53 iTh ise treeedit distance is the minimum cost of any edit script  that turns  y1  into  y2  (Bille, 2005). [sent-107, score-0.069]
</p><p>54 We would need to delete all lexemes and nodes in p and add all the lexemes and nodes of g, except for roots. [sent-109, score-0.186]
</p><p>55 An Example Both trees in Figure 1 are contained in YL for the lattice L in Figure 2. [sent-110, score-0.247]
</p><p>56 If we replace terminal boundaries with lattice indices from Figure 2, we need 6 edit operations to turn tree2 into tree1 (deleting the nodes in italic, adding the nodes in bold) and the evaluation score will be TEDEVAL(tree2,tree1) = 1−14+160−2 = 0. [sent-111, score-0.388]
</p><p>57 h 843e6542Berk-  ley Parser trained on bare-bone trees (PS) and relationalrealizational trees (RR). [sent-122, score-0.295]
</p><p>58 027 5  Table 2: Dependency parsing results by MaltParser (MP) and EasyFirst (EF), trained on the treebank converted into unlabeled dependencies, and parsing the entire dev-set. [sent-133, score-0.236]
</p><p>59 For constituency-based parsing we use two models trained by the Berkeley parser (Petrov et al. [sent-134, score-0.188]
</p><p>60 , 2006) one on phrase-structure (PS) trees and one on relational-realizational (RR) trees (Tsarfaty and Sima’an, 2008). [sent-135, score-0.238]
</p><p>61 In the raw scenario we let a latticebased parser choose its own segmentation and tags (Goldberg, 2011b). [sent-136, score-0.31]
</p><p>62 For dependency parsing we use MaltParser (Nivre et al. [sent-137, score-0.157]
</p><p>63 , 2007b) optimized for Hebrew by Ballesteros and Nivre (2012), and the EasyFirst parser ofGoldberg and Elhadad (2010) with the features therein. [sent-138, score-0.07]
</p><p>64 Since these parsers cannot choose their own tags, automatically predicted segments  and tags are provided by Adler and Elhadad (2006). [sent-139, score-0.224]
</p><p>65 We use PARSEVAL for evaluating phrase-structure trees, ATTACHSCORES for evaluating dependency trees, and TEDEVAL for evaluating all trees in all scenarios. [sent-142, score-0.296]
</p><p>66 We implement SEGEVAL for evaluating segmentation based on our TEDEVAL implementation, replacing the tree distance and size with string terms. [sent-143, score-0.242]
</p><p>67 9  Table 1 shows the constituency-based parsing results for all scenarios. [sent-144, score-0.118]
</p><p>68 All of our results confirm that gold information leads to much higher scores. [sent-145, score-0.112]
</p><p>69 TEDEVAL allows us to precisely quantify the drop in accuracy from gold to predicted (as in PARSEVAL) and than from predicted to raw on a single scale. [sent-146, score-0.425]
</p><p>70 Unlabeled TEDEVAL shows a greater drop when moving from predicted to raw than from gold to predicted, and for labeled TEDEVAL it is the other way round. [sent-148, score-0.345]
</p><p>71 This demonstrates the great importance of gold tags  which provide morphologically disambiguated information for identifying phrase content. [sent-149, score-0.304]
</p><p>72 Table 2 shows that dependency parsing results confirm the same trends, but we see a much smaller drop when moving from gold to predicted. [sent-150, score-0.363]
</p><p>73 This is due to the fact that we train the parsers for predicted on a treebank containing predicted tags. [sent-151, score-0.236]
</p><p>74 There is however a great drop when moving from predicted to raw, which confirms that evaluation benchmarks on gold input as in Nivre et al. [sent-152, score-0.372]
</p><p>75 (2007a) do not provide a realistic indication of parser performance. [sent-153, score-0.124]
</p><p>76 Cross-framework evaluation may be conducted by combining this metric with the cross-framework protocol of Tsarfaty et al. [sent-158, score-0.038]
</p><p>77 5 Conclusion We presented distance-based metrics defined for trees over lattices and applied them to evaluating parsers on joint morphological and syntactic disambiguation. [sent-160, score-0.636]
</p><p>78 Our contribution is both technical, providing an evaluation tool that can be straight-  forwardly applied for parsing scenarios involving trees over and methodological, suggesting to evaluate parsers in all possible scenarios in order to get a realistic indication of parser performance. [sent-161, score-0.611]
</p><p>79 A procedure for quantitatively comparing the syntactic coverage of English grammars. [sent-192, score-0.047]
</p><p>80 A single framework for joint morphological segmentation and syntactic parsing. [sent-210, score-0.444]
</p><p>81 Joint morphological segmentation and syntactic parsing using a PCFGLA lattice parser. [sent-220, score-0.69]
</p><p>82 Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. [sent-229, score-0.278]
</p><p>83 Morphological disambiguation of Hebrew: A case study in classifier combination. [sent-250, score-0.036]
</p><p>84 Statistical parsing for morphologically rich language (SPMRL): What, how and whither. [sent-262, score-0.28]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tsarfaty', 0.387), ('tedeval', 0.286), ('hebrew', 0.276), ('morphological', 0.242), ('reut', 0.199), ('goldberg', 0.191), ('segmentation', 0.155), ('lattice', 0.128), ('sima', 0.127), ('morphologically', 0.123), ('trees', 0.119), ('parsing', 0.118), ('bclm', 0.114), ('mrls', 0.114), ('gold', 0.112), ('yoav', 0.102), ('scenarios', 0.096), ('nivre', 0.096), ('lex', 0.091), ('parseval', 0.091), ('metrics', 0.09), ('joakim', 0.09), ('predicted', 0.089), ('modern', 0.087), ('evelina', 0.086), ('shadow', 0.086), ('green', 0.077), ('hs', 0.077), ('maltparser', 0.076), ('parser', 0.07), ('edit', 0.069), ('khalil', 0.068), ('parse', 0.067), ('rr', 0.065), ('adler', 0.064), ('spans', 0.063), ('pi', 0.063), ('ps', 0.063), ('buchholz', 0.06), ('np', 0.059), ('parsers', 0.058), ('elhadad', 0.057), ('easyfirst', 0.057), ('hneim', 0.057), ('predigcrtaoelwd', 0.057), ('relationalrealizational', 0.057), ('shacham', 0.057), ('sparseval', 0.057), ('spmrl', 0.057), ('tpopp', 0.057), ('yoad', 0.057), ('arabic', 0.055), ('nodes', 0.055), ('definite', 0.055), ('realistic', 0.054), ('drop', 0.051), ('cohen', 0.051), ('raw', 0.05), ('ballesteros', 0.05), ('ami', 0.05), ('mp', 0.05), ('analyses', 0.05), ('sandra', 0.049), ('syntactic', 0.047), ('evaluating', 0.046), ('assuming', 0.046), ('moving', 0.043), ('input', 0.043), ('overt', 0.042), ('spence', 0.042), ('erwin', 0.042), ('segments', 0.042), ('terminal', 0.042), ('tree', 0.041), ('marsi', 0.04), ('yl', 0.04), ('operations', 0.039), ('incorrect', 0.039), ('dependency', 0.039), ('rich', 0.039), ('wn', 0.039), ('protocol', 0.038), ('lexemes', 0.038), ('shay', 0.037), ('breaks', 0.037), ('roark', 0.037), ('es', 0.036), ('disambiguation', 0.036), ('ma', 0.036), ('black', 0.035), ('habash', 0.035), ('alon', 0.035), ('fl', 0.035), ('dominating', 0.035), ('tags', 0.035), ('great', 0.034), ('johan', 0.034), ('lattices', 0.034), ('jens', 0.034), ('quantify', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000026 <a title="122-tfidf-1" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>2 0.1327873 <a title="122-tfidf-2" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>3 0.12963466 <a title="122-tfidf-3" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>4 0.12842974 <a title="122-tfidf-4" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>5 0.11174625 <a title="122-tfidf-5" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>6 0.10706897 <a title="122-tfidf-6" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>7 0.1005843 <a title="122-tfidf-7" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>8 0.099711768 <a title="122-tfidf-8" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>9 0.09740027 <a title="122-tfidf-9" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>10 0.097106881 <a title="122-tfidf-10" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>11 0.086777061 <a title="122-tfidf-11" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>12 0.084620997 <a title="122-tfidf-12" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>13 0.080353804 <a title="122-tfidf-13" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>14 0.079866186 <a title="122-tfidf-14" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>15 0.079765484 <a title="122-tfidf-15" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>16 0.077973753 <a title="122-tfidf-16" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>17 0.075234339 <a title="122-tfidf-17" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>18 0.072801001 <a title="122-tfidf-18" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>19 0.072301038 <a title="122-tfidf-19" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>20 0.071550593 <a title="122-tfidf-20" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.193), (1, -0.036), (2, -0.165), (3, -0.111), (4, -0.032), (5, 0.036), (6, 0.045), (7, -0.123), (8, 0.032), (9, 0.011), (10, -0.033), (11, 0.022), (12, 0.05), (13, -0.072), (14, 0.046), (15, -0.072), (16, -0.133), (17, -0.039), (18, -0.105), (19, 0.061), (20, 0.037), (21, -0.031), (22, 0.017), (23, -0.058), (24, -0.032), (25, -0.002), (26, -0.016), (27, 0.041), (28, 0.039), (29, -0.024), (30, -0.009), (31, 0.003), (32, -0.059), (33, -0.049), (34, -0.039), (35, -0.003), (36, -0.002), (37, 0.061), (38, -0.065), (39, -0.046), (40, -0.016), (41, 0.081), (42, -0.029), (43, -0.069), (44, -0.034), (45, -0.06), (46, -0.067), (47, 0.055), (48, 0.005), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95308852 <a title="122-lsi-1" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>2 0.58393914 <a title="122-lsi-2" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>Author: David Stallard ; Jacob Devlin ; Michael Kayser ; Yoong Keok Lee ; Regina Barzilay</p><p>Abstract: If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter.</p><p>3 0.58102989 <a title="122-lsi-3" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>4 0.57873613 <a title="122-lsi-4" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>5 0.57446873 <a title="122-lsi-5" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>Author: Seyed Abolghasem Mirroshandel ; Alexis Nasr ; Joseph Le Roux</p><p>Abstract: Treebanks are not large enough to reliably model precise lexical phenomena. This deficiency provokes attachment errors in the parsers trained on such data. We propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser. Experiments on the French Treebank showed a relative decrease ofthe error rate of 7. 1% Labeled Accuracy Score yielding the best parsing results on this treebank.</p><p>6 0.56892228 <a title="122-lsi-6" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>7 0.55808866 <a title="122-lsi-7" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>8 0.55468112 <a title="122-lsi-8" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>9 0.54481626 <a title="122-lsi-9" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>10 0.53737205 <a title="122-lsi-10" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>11 0.53533828 <a title="122-lsi-11" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>12 0.52756768 <a title="122-lsi-12" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>13 0.49920803 <a title="122-lsi-13" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>14 0.49590141 <a title="122-lsi-14" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>15 0.49416226 <a title="122-lsi-15" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>16 0.48483145 <a title="122-lsi-16" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>17 0.48234704 <a title="122-lsi-17" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>18 0.48225391 <a title="122-lsi-18" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>19 0.47918117 <a title="122-lsi-19" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>20 0.47721684 <a title="122-lsi-20" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.014), (26, 0.02), (28, 0.027), (30, 0.03), (37, 0.025), (39, 0.02), (57, 0.017), (71, 0.043), (74, 0.036), (82, 0.016), (84, 0.481), (85, 0.029), (90, 0.071), (92, 0.038), (94, 0.016), (99, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9073863 <a title="122-lda-1" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>2 0.85425842 <a title="122-lda-2" href="./acl-2012-Decoding_Running_Key_Ciphers.html">68 acl-2012-Decoding Running Key Ciphers</a></p>
<p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. We consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using Gibbs sampling with a word language model. We evaluate our method on synthetic ciphertexts of different lengths, and find that it outperforms previous work that employs Viterbi decoding with character-based models.</p><p>3 0.84552431 <a title="122-lda-3" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>Author: Shomir Wilson</p><p>Abstract: Metalanguage is an essential linguistic mechanism which allows us to communicate explicit information about language itself. However, it has been underexamined in research in language technologies, to the detriment of the performance of systems that could exploit it. This paper describes the creation of the first tagged and delineated corpus of English metalanguage, accompanied by an explicit definition and a rubric for identifying the phenomenon in text. This resource will provide a basis for further studies of metalanguage and enable its utilization in language technologies.</p><p>4 0.82988936 <a title="122-lda-4" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>Author: Preethi Raghavan ; Albert Lai ; Eric Fosler-Lussier</p><p>Abstract: We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen’s temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the Timebank corpus. This finding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range ofdomains to fully understand the nature of temporal ordering.</p><p>5 0.76580834 <a title="122-lda-5" href="./acl-2012-Fast_Online_Lexicon_Learning_for_Grounded_Language_Acquisition.html">93 acl-2012-Fast Online Lexicon Learning for Grounded Language Acquisition</a></p>
<p>Author: David Chen</p><p>Abstract: Learning a semantic lexicon is often an important first step in building a system that learns to interpret the meaning of natural language. It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context. Recent work by Chen and Mooney (201 1) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs. While the algorithm produced good lexicons for the task of learning to interpret navigation instructions, it only works in batch settings and does not scale well to large datasets. In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the stateof-the-art results. We show that by changing the grammar of the formal meaning represen- . tation language and training on additional data collected from Amazon’s Mechanical Turk we can further improve the results. We also include experimental results on a Chinese translation of the training data to demonstrate the generality of our approach.</p><p>6 0.40084437 <a title="122-lda-6" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>7 0.37800461 <a title="122-lda-7" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>8 0.37548691 <a title="122-lda-8" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>9 0.37271592 <a title="122-lda-9" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<p>10 0.37117141 <a title="122-lda-10" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>11 0.37028712 <a title="122-lda-11" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>12 0.36777115 <a title="122-lda-12" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>13 0.35599259 <a title="122-lda-13" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>14 0.35253662 <a title="122-lda-14" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>15 0.3517971 <a title="122-lda-15" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>16 0.34753034 <a title="122-lda-16" href="./acl-2012-Graph-based_Semi-Supervised_Learning_Algorithms_for_NLP.html">104 acl-2012-Graph-based Semi-Supervised Learning Algorithms for NLP</a></p>
<p>17 0.34559941 <a title="122-lda-17" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>18 0.34394905 <a title="122-lda-18" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>19 0.33775339 <a title="122-lda-19" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>20 0.33387467 <a title="122-lda-20" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
