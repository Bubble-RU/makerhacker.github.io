<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-126" href="#">acl2012-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</h1>
<br/><p>Source: <a title="acl-2012-126-pdf" href="http://aclweb.org/anthology//P/P12/P12-1011.pdf">pdf</a></p><p>Author: Nathanael Chambers</p><p>Abstract: Temporal reasoners for document understanding typically assume that a document’s creation date is known. Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. Unfortunately, the timestamp is not always known, particularly on the Web. This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. The first is a discriminative classifier with new features extracted from the text’s time expressions (e.g., ‘since 1999’). This model alone improves on previous generative models by 77%. The second model learns probabilistic constraints between time expressions and the unknown document time. Imposing these learned constraints on the discriminative model further improves its accuracy. Finally, we present a new experiment design that facil- itates easier comparison by future work.</p><p>Reference: <a title="acl-2012-126-reference" href="../acl2012_reference/acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Temporal reasoners for document understanding typically assume that a document’s creation date is known. [sent-2, score-0.335]
</p><p>2 Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. [sent-3, score-0.701]
</p><p>3 Unfortunately, the timestamp is not always known, particularly on the Web. [sent-4, score-0.42]
</p><p>4 This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. [sent-5, score-0.28]
</p><p>5 The first is a discriminative classifier with new features extracted from the text’s time expressions (e. [sent-6, score-0.363]
</p><p>6 The second model learns probabilistic constraints between time expressions and the unknown document time. [sent-10, score-0.546]
</p><p>7 1 Introduction This paper addresses a relatively new task in the NLP community: automatic document dating. [sent-13, score-0.249]
</p><p>8 Given a document with unknown origins, what characteristics of its text indicate the year in which the document was written? [sent-14, score-1.016]
</p><p>9 This paper proposes a learning approach that builds constraints from a document’s use of time expressions, and combines them with a new discriminative classifier that greatly improves previous work. [sent-15, score-0.318]
</p><p>10 The temporal reasoning community has long depended on document timestamps to ground rela98 tive time expressions and events (Mani and Wilson, 2000; Llid o´ et al. [sent-16, score-0.905]
</p><p>11 , 2003): And while there was no profit this year from discontinued operations, last year they contributed 34 million, before tax. [sent-19, score-0.984]
</p><p>12 Reconstructing the timeline of events from this doc-  ument requires extensive temporal knowledge, most notably, the document’s creation date to ground its relative expressions (e. [sent-20, score-0.429]
</p><p>13 , 2009) include tasks to link events to the (known) document creation time, but state-of-the-art event-event ordering algorithms also rely on these timestamps (Chambers and Jurafsky, 2008; Yoshikawa et al. [sent-25, score-0.445]
</p><p>14 Several IR applications depend on knowledge ofwhen documents were posted, such as computing document relevance (Li and Croft, 2003; Dakka et al. [sent-29, score-0.313]
</p><p>15 , 2008) and labeling search queries with temporal profiles (Diaz and Jones, 2004; Zhang et al. [sent-30, score-0.206]
</p><p>16 The first part of this paper describes a novel learning approach to document dating, presenting a discriminative model and rich linguistic features that have not been applied to document dating. [sent-39, score-0.581]
</p><p>17 The second half of this paper describes a novel learning algorithm that orders time expressions against the unknown timestamp. [sent-42, score-0.235]
</p><p>18 These labels impose constraints on the possible timestamp and narrow down its range of valid dates. [sent-44, score-0.482]
</p><p>19 We combine these constraints with our discriminative learner and see another relative improvement in accuracy by 9%. [sent-45, score-0.178]
</p><p>20 2  Previous Work  Most work on dating documents has come from the IR and knowledge management communities interested in dating documents with unknown origins. [sent-46, score-0.686]
</p><p>21 They learned unigram language models (LMs) for specific time periods and scored articles with log-likelihood ratio scores. [sent-49, score-0.225]
</p><p>22 As above, they learned unigram LMs, but instead measured the KLdivergence between a document and a time period’s LM. [sent-55, score-0.395]
</p><p>23 Our proposed models differ from this work by applying rich linguistic features, discriminative models, and by focusing on how time expressions improve accuracy. [sent-56, score-0.261]
</p><p>24 They computed probability distributions over different  time periods (e. [sent-59, score-0.151]
</p><p>25 99  They focused on finding words that show periodic spikes (defined by the word’s standard deviation in its distribution over time), weighted with inverse document frequency scores. [sent-63, score-0.249]
</p><p>26 who focus on fiction) all train on news articles from a particular time period, and test on articles in the same time period. [sent-66, score-0.238]
</p><p>27 In fact, one of the systems in Kanhabua and Norvag (2008) simply searches for one training document that best matches a test document, and assigns its timestamp. [sent-68, score-0.249]
</p><p>28 The majority of articles in our dataset contain time expressions (e. [sent-71, score-0.209]
</p><p>29 , the year 1998), yet these have not been incorporated into the models despite their obvious connection to the article’s timestamp. [sent-73, score-0.492]
</p><p>30 This paper first describes how to include time expressions as traditional features, and then describes a more sophisticated temporal reasoning component that naturally fits into our classifier. [sent-74, score-0.427]
</p><p>31 3  Timestamp Classifiers  Labeling documents with timestamps is similar to topic classification, but instead of choosing from topics, we choose the most likely year (or other granularity) in which it was written. [sent-75, score-0.638]
</p><p>32 The subsequent sections then introduce our novel classifiers and temporal reasoners to compare against this model. [sent-77, score-0.273]
</p><p>33 It weights tokens by the ratio of their probability in a specific year to their probability over the entire corpus. [sent-81, score-0.522]
</p><p>34 The model thus requires an LM for each year and an LM for the entire corpus:  NLLR(D,Y ) =wX∈DP(w|D) ∗ log(PP((ww||CY) ))  (1)  where D is the target document, Y is the time span (e. [sent-82, score-0.594]
</p><p>35 A document is labeled with the year that satisfies argmaxYNLLR(D, Y ). [sent-85, score-0.772]
</p><p>36 Named entities are important to document dating due to the nature of people and places coming in and out of the news at precise moments in time. [sent-100, score-0.549]
</p><p>37 However, document dating is not just a simple topic classification application, but rather relates to temporal phenomena that is often explicitly described in the text itself. [sent-111, score-0.691]
</p><p>38 Language contains words and phrases that discuss the very time periods we aim to recover. [sent-112, score-0.151]
</p><p>39 However, time expressions are sometimes included, and the last sentence in the original text contains a helpful relative clause: Their tickets will entitle them to a preview of. [sent-122, score-0.437]
</p><p>40 This one clause is more valuable than the rest of the document, allowing us to infer that the document’s timestamp is before February, 2000. [sent-126, score-0.425]
</p><p>41 An educated guess might surmise the article appeared in  the year prior, 1999, which is the correct year. [sent-127, score-0.528]
</p><p>42 Previous work on document dating does not integrate this information except to include the unigram ‘2000’ in the model. [sent-129, score-0.559]
</p><p>43 2 Time Features To our knowledge, the following time features have not been used in a document dating setting. [sent-136, score-0.648]
</p><p>44 Typed Dependency: The most basic time feature is including governors of year mentions and the relation between them. [sent-139, score-0.804]
</p><p>45 For example, consider the following context for the mention 1997: Torre, who watched the Kansas City Royals beat the Yankees, 13-6, on Friday for the first time since 1997. [sent-141, score-0.173]
</p><p>46 This generalizes the features to capture time expressions with prepositions, as noun modifiers, or other constructs. [sent-145, score-0.24]
</p><p>47 Verb Tense: An important syntactic feature for temporal positioning is the tense of the verb that dominates the time expression. [sent-146, score-0.388]
</p><p>48 Verb Path: The verb path feature is the dependency  path from the nearest verb to the year expression. [sent-150, score-0.638]
</p><p>49 tincbevfhiotryelawthesrRlimatubolefnaotre o2u0s  Figure 1: Three year mentions and their relation to the document creation year. [sent-158, score-0.993]
</p><p>50 Relations can be correctly identified for training using known document timestamps. [sent-159, score-0.249]
</p><p>51 People and places are often discussed during specific time periods, particularly in the news genre. [sent-162, score-0.168]
</p><p>52 Collecting named entity mentions will differentiate between an article discussing a bill and one discussing the US President, Bill Clinton. [sent-163, score-0.212]
</p><p>53 We extract NER features as sequences of uninterrupted tokens labeled with the same NER tag, ignoring unigrams (since unigrams are already included in the base model). [sent-164, score-0.198]
</p><p>54 4  Learning Time Constraints  This section departs from the above document classifiers and instead classifies individual emphyear mentions. [sent-166, score-0.302]
</p><p>55 The goal is to automatically learn temporal constraints on the document’s timestamp. [sent-167, score-0.238]
</p><p>56 Instead of predicting a single year for a document, a temporal constraint predicts a range of years. [sent-168, score-0.738]
</p><p>57 Each time mention, such as ‘not since 2009’, is a constraint representing its relation to the document’s timestamp. [sent-169, score-0.206]
</p><p>58 For example, the mentioned year ‘2009’ must occur before the year of document creation. [sent-170, score-1.233]
</p><p>59 This section builds a classifier to label time mentions with their relations (e. [sent-171, score-0.431]
</p><p>60 , before, after, or simultane-  ous with the document’s timestamp), enabling these mentions to constrain the document classifiers described above. [sent-173, score-0.478]
</p><p>61 Figure 1 gives an example of time mentions and the desired labels we wish to learn. [sent-174, score-0.278]
</p><p>62 510 9 519 619 719 8Yea1r9 C9las20 20 120 420 5 Figure 2: Distribution over years for a single document as output by a MaxEnt classifier. [sent-178, score-0.337]
</p><p>63 Figure 2 illustrate a typical distribution output by a document classifier for a training document. [sent-179, score-0.32]
</p><p>64 Two of the years appear likely (1999 and 2001), however, the document contains a time expression that seems to impose a strict constraint that should eliminate 2001 from consideration:  Their tickets will entitle them to a preview of. [sent-180, score-0.742]
</p><p>65 The clause until February 2000 in a present tense context may not definitively identify the document’s timestamp (1999 is a good guess), but as discussed earlier, it should remove all future years beyond 2000 from consideration. [sent-184, score-0.578]
</p><p>66 We thus want to impose a constraint based on this phrase that says, loosely, ‘this document was likely written before 2000’ . [sent-185, score-0.351]
</p><p>67 The document classifiers described in previous sections cannot capture such ordering information. [sent-186, score-0.329]
</p><p>68 2 add richer time information (such as until pobj 2000 and open prep until pobj 2000), but they compete with many other features that can mislead the final classification. [sent-189, score-0.365]
</p><p>69 An independent constraint learner may push the document classifier in the right direction. [sent-190, score-0.427]
</p><p>70 1 Constraint Types We learn several types of constraints between each year mention and the document’s timestamp. [sent-192, score-0.625]
</p><p>71 Year mentions are defined as tokens with exactly four digits, numerically between 1900 and 2100. [sent-193, score-0.176]
</p><p>72 Let T  be the document timestamp’s year, and M the year mention. [sent-194, score-0.741]
</p><p>73 The learning process is a typical training environment where year mentions are treated as labeled training examples. [sent-207, score-0.699]
</p><p>74 Labels for year mentions are automatically computed by comparing the actual timestamp of the training document (all documents in Gigaword have dates) with the integer value of  the year token. [sent-208, score-1.861]
</p><p>75 For example, a document written in 1997 might contain the phrase, “in the year 2000”. [sent-209, score-0.741]
</p><p>76 The year token (2000) is thus three+ years after the timestamp (1997). [sent-210, score-0.968]
</p><p>77 We use this relation for the year mention as a labeled training example. [sent-211, score-0.628]
</p><p>78 Ultimately, we want to use similar syntactic constructs in training so that “in the year 2000” and “in the year 2003” mutually inform each other. [sent-212, score-1.031]
</p><p>79 We thus compute the label for each time expression, and replace the integer year with the generic YEAR token to generalize mentions. [sent-213, score-0.594]
</p><p>80 The text for this example becomes “in the year YEAR” (labeled as three+ years after). [sent-214, score-0.58]
</p><p>81 We train a MaxEnt model on each year mention, to be described next. [sent-215, score-0.492]
</p><p>82 The vast majority of year mentions are references to the future (e. [sent-217, score-0.668]
</p><p>83 2 Constraint Learner The features we use to classify year mentions are given in Table 1. [sent-221, score-0.699]
</p><p>84 The same time features in the document classifier of Section 3. [sent-222, score-0.453]
</p><p>85 We use a MaxEnt classifier trained on the individ-  ual year mentions. [sent-225, score-0.563]
</p><p>86 Documents often contain multiple (and different) year mentions; all are included in training and testing. [sent-226, score-0.518]
</p><p>87 This classifier labels mentions with relations, but in order to influence the document classifier, we need to map the relations to individual Time Constraint Features  TnVBD-eaygprcbaeodmPTfeaDWtnhesop. [sent-227, score-0.547]
</p><p>88 ConstraintCount BAfetfeorr Te iTmimesetastmampp11,26083,1,08150 Same as Timestamp141,201 Table 2: Training size of year mentions (and their relation to the document timestamp) in Gigaword’s NYT section. [sent-231, score-0.951]
</p><p>89 We represent a MaxEnt classifier by PY (R|t) for a time mention t ∈ Td and possible relati(oRns| tR) . [sent-234, score-0.244]
</p><p>90 Table 6 shows that performance increased most on the documents that contain at least one year mention (60% of the corpus). [sent-237, score-0.627]
</p><p>91 Finally, Table 5 shows the results of the temporal constraint classifiers on year mentions. [sent-238, score-0.791]
</p><p>92 7% Table 6: Accuracy on all documents and documents with at least one year mention (about 60% of the corpus). [sent-252, score-0.691]
</p><p>93 Our richer syntax-based features only apply to year mentions, but this small textual phenomena leads to a surprising 13% relative improvement in accuracy. [sent-256, score-0.576]
</p><p>94 Table 6 shows that a significant chunk of this improvement comes from docu-  ments containing year mentions, as expected. [sent-257, score-0.492]
</p><p>95 Although most of its features are in the document classifier, by learning constraints it captures a different picture of time that a traditional document classifier does not address. [sent-259, score-0.764]
</p><p>96 Combining this picture with the document classifier leads to another 3. [sent-260, score-0.32]
</p><p>97 Although we focused on year mentions here, there are several avenues for future study, including explorations of how other types of time expressions might inform the task. [sent-262, score-0.924]
</p><p>98 We hope our explicit train/test environment encourages future comparison and progress  on document dating. [sent-269, score-0.249]
</p><p>99 Using temporal  profiles of queries for precision prediction. [sent-293, score-0.206]
</p><p>100 Improving temporal language models for determining time of non-timestamped documents. [sent-301, score-0.278]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('year', 0.492), ('timestamp', 0.388), ('dating', 0.266), ('document', 0.249), ('kanhabua', 0.193), ('norvag', 0.177), ('temporal', 0.176), ('mentions', 0.176), ('jong', 0.155), ('expressions', 0.107), ('maxent', 0.103), ('time', 0.102), ('nllr', 0.089), ('years', 0.088), ('pobj', 0.088), ('timestamps', 0.082), ('mention', 0.071), ('classifier', 0.071), ('constraint', 0.07), ('entitle', 0.067), ('hayden', 0.067), ('preview', 0.067), ('tickets', 0.067), ('tense', 0.065), ('documents', 0.064), ('constraints', 0.062), ('february', 0.062), ('tempeval', 0.059), ('ner', 0.057), ('verhagen', 0.057), ('unigrams', 0.055), ('classifiers', 0.053), ('discriminative', 0.052), ('relations', 0.051), ('kumar', 0.051), ('ir', 0.049), ('periods', 0.049), ('core', 0.047), ('inform', 0.047), ('events', 0.045), ('verb', 0.045), ('dakka', 0.044), ('dalli', 0.044), ('jintao', 0.044), ('kjetil', 0.044), ('llid', 0.044), ('planetarium', 0.044), ('pyear', 0.044), ('reasoners', 0.044), ('unigram', 0.044), ('creation', 0.042), ('reasoning', 0.042), ('collocations', 0.041), ('gaizauskas', 0.039), ('historical', 0.039), ('nattiya', 0.039), ('depended', 0.039), ('diaz', 0.039), ('officially', 0.039), ('reproducing', 0.039), ('chambers', 0.038), ('learner', 0.037), ('clause', 0.037), ('article', 0.036), ('heritage', 0.035), ('rel', 0.035), ('news', 0.034), ('relation', 0.034), ('museum', 0.033), ('timebank', 0.033), ('impose', 0.032), ('ground', 0.032), ('particularly', 0.032), ('gigaword', 0.032), ('features', 0.031), ('labeled', 0.031), ('lowercased', 0.031), ('hepple', 0.031), ('mani', 0.031), ('yoshikawa', 0.031), ('history', 0.031), ('builds', 0.031), ('community', 0.031), ('de', 0.03), ('ratio', 0.03), ('prep', 0.03), ('nathanael', 0.03), ('schilder', 0.03), ('profiles', 0.03), ('typed', 0.029), ('path', 0.028), ('lms', 0.027), ('td', 0.027), ('relative', 0.027), ('ordering', 0.027), ('unknown', 0.026), ('richer', 0.026), ('dates', 0.026), ('pustejovsky', 0.026), ('included', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="126-tfidf-1" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Temporal reasoners for document understanding typically assume that a document’s creation date is known. Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. Unfortunately, the timestamp is not always known, particularly on the Web. This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. The first is a discriminative classifier with new features extracted from the text’s time expressions (e.g., ‘since 1999’). This model alone improves on previous generative models by 77%. The second model learns probabilistic constraints between time expressions and the unknown document time. Imposing these learned constraints on the discriminative model further improves its accuracy. Finally, we present a new experiment design that facil- itates easier comparison by future work.</p><p>2 0.22817378 <a title="126-tfidf-2" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>Author: Guillermo Garrido ; Anselmo Penas ; Bernardo Cabaleiro ; Alvaro Rodrigo</p><p>Abstract: Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported.</p><p>3 0.16422851 <a title="126-tfidf-3" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions.</p><p>4 0.14386429 <a title="126-tfidf-4" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>Author: Remy Kessler ; Xavier Tannier ; Caroline Hagege ; Veronique Moriceau ; Andre Bittar</p><p>Abstract: We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related.</p><p>5 0.12646142 <a title="126-tfidf-5" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>Author: Yafang Wang ; Maximilian Dylla ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: The Web and digitized text sources contain a wealth of information about named entities such as politicians, actors, companies, or cultural landmarks. Extracting this information has enabled the automated construction oflarge knowledge bases, containing hundred millions of binary relationships or attribute values about these named entities. However, in reality most knowledge is transient, i.e. changes over time, requiring a temporal dimension in fact extraction. In this paper we develop a methodology that combines label propagation with constraint reasoning for temporal fact extraction. Label propagation aggressively gathers fact candidates, and an Integer Linear Program is used to clean out false hypotheses that violate temporal constraints. Our method is able to improve on recall while keeping up with precision, which we demonstrate by experiments with biography-style Wikipedia pages and a large corpus of news articles.</p><p>6 0.11460845 <a title="126-tfidf-6" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>7 0.10177377 <a title="126-tfidf-7" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>8 0.098156318 <a title="126-tfidf-8" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>9 0.09804225 <a title="126-tfidf-9" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>10 0.095718391 <a title="126-tfidf-10" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>11 0.092876822 <a title="126-tfidf-11" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>12 0.085150845 <a title="126-tfidf-12" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>13 0.083314329 <a title="126-tfidf-13" href="./acl-2012-A_Novel_Burst-based_Text_Representation_Model_for_Scalable_Event_Detection.html">17 acl-2012-A Novel Burst-based Text Representation Model for Scalable Event Detection</a></p>
<p>14 0.081490584 <a title="126-tfidf-14" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>15 0.079950005 <a title="126-tfidf-15" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>16 0.079779796 <a title="126-tfidf-16" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>17 0.06837707 <a title="126-tfidf-17" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>18 0.058457132 <a title="126-tfidf-18" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>19 0.057820372 <a title="126-tfidf-19" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>20 0.05699243 <a title="126-tfidf-20" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.188), (1, 0.157), (2, -0.074), (3, 0.184), (4, 0.002), (5, -0.1), (6, -0.011), (7, -0.06), (8, 0.0), (9, -0.184), (10, -0.072), (11, -0.033), (12, -0.027), (13, 0.007), (14, 0.006), (15, -0.001), (16, 0.028), (17, 0.007), (18, -0.038), (19, -0.068), (20, -0.003), (21, -0.0), (22, -0.039), (23, 0.015), (24, 0.095), (25, 0.024), (26, 0.055), (27, -0.043), (28, 0.017), (29, -0.06), (30, -0.1), (31, 0.092), (32, 0.027), (33, 0.026), (34, 0.111), (35, 0.028), (36, -0.031), (37, 0.058), (38, 0.061), (39, 0.063), (40, 0.062), (41, 0.008), (42, -0.118), (43, -0.019), (44, -0.091), (45, -0.085), (46, 0.087), (47, -0.051), (48, -0.035), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95306057 <a title="126-lsi-1" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Temporal reasoners for document understanding typically assume that a document’s creation date is known. Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. Unfortunately, the timestamp is not always known, particularly on the Web. This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. The first is a discriminative classifier with new features extracted from the text’s time expressions (e.g., ‘since 1999’). This model alone improves on previous generative models by 77%. The second model learns probabilistic constraints between time expressions and the unknown document time. Imposing these learned constraints on the discriminative model further improves its accuracy. Finally, we present a new experiment design that facil- itates easier comparison by future work.</p><p>2 0.7658456 <a title="126-lsi-2" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>Author: Preethi Raghavan ; Albert Lai ; Eric Fosler-Lussier</p><p>Abstract: We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen’s temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the Timebank corpus. This finding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range ofdomains to fully understand the nature of temporal ordering.</p><p>3 0.7482425 <a title="126-lsi-3" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>Author: Guillermo Garrido ; Anselmo Penas ; Bernardo Cabaleiro ; Alvaro Rodrigo</p><p>Abstract: Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported.</p><p>4 0.71989197 <a title="126-lsi-4" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>Author: Yafang Wang ; Maximilian Dylla ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: The Web and digitized text sources contain a wealth of information about named entities such as politicians, actors, companies, or cultural landmarks. Extracting this information has enabled the automated construction oflarge knowledge bases, containing hundred millions of binary relationships or attribute values about these named entities. However, in reality most knowledge is transient, i.e. changes over time, requiring a temporal dimension in fact extraction. In this paper we develop a methodology that combines label propagation with constraint reasoning for temporal fact extraction. Label propagation aggressively gathers fact candidates, and an Integer Linear Program is used to clean out false hypotheses that violate temporal constraints. Our method is able to improve on recall while keeping up with precision, which we demonstrate by experiments with biography-style Wikipedia pages and a large corpus of news articles.</p><p>5 0.71331245 <a title="126-lsi-5" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>Author: Remy Kessler ; Xavier Tannier ; Caroline Hagege ; Veronique Moriceau ; Andre Bittar</p><p>Abstract: We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related.</p><p>6 0.5516665 <a title="126-lsi-6" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>7 0.53478396 <a title="126-lsi-7" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>8 0.48192465 <a title="126-lsi-8" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>9 0.46360067 <a title="126-lsi-9" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>10 0.42464343 <a title="126-lsi-10" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>11 0.42200699 <a title="126-lsi-11" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>12 0.42034298 <a title="126-lsi-12" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>13 0.41968808 <a title="126-lsi-13" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>14 0.41265711 <a title="126-lsi-14" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>15 0.41103122 <a title="126-lsi-15" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>16 0.40102735 <a title="126-lsi-16" href="./acl-2012-Toward_Automatically_Assembling_Hittite-Language_Cuneiform_Tablet_Fragments_into_Larger_Texts.html">200 acl-2012-Toward Automatically Assembling Hittite-Language Cuneiform Tablet Fragments into Larger Texts</a></p>
<p>17 0.38724595 <a title="126-lsi-17" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>18 0.36305684 <a title="126-lsi-18" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>19 0.35599238 <a title="126-lsi-19" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>20 0.33675551 <a title="126-lsi-20" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.015), (26, 0.062), (28, 0.033), (30, 0.029), (37, 0.031), (39, 0.055), (52, 0.271), (59, 0.012), (64, 0.011), (74, 0.039), (82, 0.058), (84, 0.03), (85, 0.027), (90, 0.138), (92, 0.06), (94, 0.016), (99, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77464098 <a title="126-lda-1" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>Author: Nathanael Chambers</p><p>Abstract: Temporal reasoners for document understanding typically assume that a document’s creation date is known. Algorithms to ground relative time expressions and order events often rely on this timestamp to assist the learner. Unfortunately, the timestamp is not always known, particularly on the Web. This paper addresses the task of automatic document timestamping, presenting two new models that incorporate rich linguistic features about time. The first is a discriminative classifier with new features extracted from the text’s time expressions (e.g., ‘since 1999’). This model alone improves on previous generative models by 77%. The second model learns probabilistic constraints between time expressions and the unknown document time. Imposing these learned constraints on the discriminative model further improves its accuracy. Finally, we present a new experiment design that facil- itates easier comparison by future work.</p><p>2 0.74933904 <a title="126-lda-2" href="./acl-2012-Automatically_Mining_Question_Reformulation_Patterns_from_Search_Log_Data.html">35 acl-2012-Automatically Mining Question Reformulation Patterns from Search Log Data</a></p>
<p>Author: Xiaobing Xue ; Yu Tao ; Daxin Jiang ; Hang Li</p><p>Abstract: Natural language questions have become popular in web search. However, various questions can be formulated to convey the same information need, which poses a great challenge to search systems. In this paper, we automatically mined 5w1h question reformulation patterns from large scale search log data. The question reformulations generated from these patterns are further incorporated into the retrieval model. Experiments show that using question reformulation patterns can significantly improve the search performance of natural language questions.</p><p>3 0.72108847 <a title="126-lda-3" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>Author: Xu Sun ; Houfeng Wang ; Wenjie Li</p><p>Abstract: We present a joint model for Chinese word segmentation and new word detection. We present high dimensional new features, including word-based features and enriched edge (label-transition) features, for the joint modeling. As we know, training a word segmentation system on large-scale datasets is already costly. In our case, adding high dimensional new features will further slow down the training speed. To solve this problem, we propose a new training method, adaptive online gradient descent based on feature frequency information, for very fast online training of the parameters, even given large-scale datasets with high dimensional features. Compared with existing training methods, our training method is an order magnitude faster in terms of training time, and can achieve equal or even higher accuracies. The proposed fast training method is a general purpose optimization method, and it is not limited in the specific task discussed in this paper.</p><p>4 0.72055012 <a title="126-lda-4" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>Author: Junhui Li ; Zhaopeng Tu ; Guodong Zhou ; Josef van Genabith</p><p>Abstract: This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1</p><p>5 0.58500874 <a title="126-lda-5" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Pradeep Dasigi ; Mona Diab ; Dragomir Radev</p><p>Abstract: The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.</p><p>6 0.57151818 <a title="126-lda-6" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>7 0.55971831 <a title="126-lda-7" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>8 0.55841845 <a title="126-lda-8" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>9 0.55641639 <a title="126-lda-9" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>10 0.55628616 <a title="126-lda-10" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>11 0.55489033 <a title="126-lda-11" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>12 0.55442512 <a title="126-lda-12" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>13 0.55396479 <a title="126-lda-13" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>14 0.55384177 <a title="126-lda-14" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>15 0.55383676 <a title="126-lda-15" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>16 0.55237091 <a title="126-lda-16" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>17 0.55230165 <a title="126-lda-17" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>18 0.55208731 <a title="126-lda-18" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>19 0.5509268 <a title="126-lda-19" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>20 0.55083847 <a title="126-lda-20" href="./acl-2012-A_Graph-based_Cross-lingual_Projection_Approach_for_Weakly_Supervised_Relation_Extraction.html">12 acl-2012-A Graph-based Cross-lingual Projection Approach for Weakly Supervised Relation Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
