<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-27" href="#">acl2012-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</h1>
<br/><p>Source: <a title="acl-2012-27-pdf" href="http://aclweb.org/anthology//P/P12/P12-2043.pdf">pdf</a></p><p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>Reference: <a title="acl-2012-27-reference" href="../acl2012_reference/acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Ali Qatar Computing Research Institute Qatar Foundation, Doha, Qatar kdarwi sh@ qf . [sent-2, score-0.039]
</p><p>2 qa @ kdar  Abstract Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. [sent-6, score-0.47]
</p><p>3 However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. [sent-7, score-0.498]
</p><p>4 In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. [sent-8, score-0.413]
</p><p>5 The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. [sent-9, score-0.554]
</p><p>6 The technique can potentially be applied to other languages. [sent-10, score-0.048]
</p><p>7 Introduction Arabic exhibits rich morphological phenomena that complicate retrieval. [sent-12, score-0.147]
</p><p>8 Arabic nouns and verbs  are typically derived from a set of 10,000 roots that are cast into stems using templates that may add infixes, double letters, or remove letters. [sent-13, score-0.455]
</p><p>9 Stems can accept the attachment of clitics, in the form of prefixes or suffixes, such as prepositions, determiners, pronouns, etc. [sent-14, score-0.084]
</p><p>10 Orthographic rules can cause the addition, deletion, or substitution of letters during suffix and prefix attachment. [sent-15, score-0.104]
</p><p>11 Further, stems can be inflected to obtain plural forms via the addition of suffixes or through using a different stem form altogether producing socalled broken1 (aka irregular) plurals. [sent-16, score-0.659]
</p><p>12 For retrieval, we would ideally like to match “related” stem forms regardless of inflected form or attached clitic. [sent-17, score-0.13]
</p><p>13 Tolerating some form of derivational morphology where nouns are transformed into adjectives via the attachment of  1 “Broken” is a direct translation of the Arabic word “takseer”, which refers to this kind of plural. [sent-18, score-0.175]
</p><p>14 ‫( ﻣﺼﺮ‬  as they  Matching all stems that are cast from the same root would introduce undesired ambiguity, because a single root can produce up to 1,000 stems. [sent-23, score-0.451]
</p><p>15 The first approach involves stemming, which removes clitics, plural and gender markers, and suffixes such as ‫ ﻱي‬y). [sent-25, score-0.139]
</p><p>16 Statistical stemming was reported to be the most effective for Arabic retrieval (Darwish et al. [sent-26, score-0.473]
</p><p>17 Stemming does not handle infixes and hence cannot conflate singular and broken plural word forms. [sent-29, score-0.225]
</p><p>18 For example, the plural of the Arabic word for book “‫ ”ﻛﺘﺎﺏب‬ktAb) is “‫ ”ﻛﺘﺐ‬ktb). [sent-30, score-0.063]
</p><p>19 Stemming of some named entities, which are important for retrieval, and their inflected forms may produce different stems as word endings may change with the attachment of suffixes. [sent-32, score-0.521]
</p><p>20 Consider the Arabic words for America ‫ﺃأﻣﺮﻳﯾﻜﺎ‬ (>mrykA) and American ‫> ﺃأﻣﺮﻳﯾﻜﻲ‬mryky), where the final letter is transformed from “A” to “y”. [sent-33, score-0.082]
</p><p>21 The second approach involves using character 3or 4-grams (as opposed to words) (Mayfield et al. [sent-34, score-0.101]
</p><p>22 This approach though it has been shown to improve retrieval effectiveness, it has the following drawbacks: 1. [sent-37, score-0.147]
</p><p>23 I t cannot handle broken plurals, though it would handle words where stemming would produce different stems for different inflected forms. [sent-38, score-0.986]
</p><p>24 For example, using a 6 letter word would produce 4 trigram chunks, which would have 12 letters. [sent-41, score-0.083]
</p><p>25 Longer words would yield more character ngram chunks compared to shorter ones leading to skewed weights for query words. [sent-43, score-0.233]
</p><p>26 2 We use Buckwalter transliteration in the paper Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-44, score-0.049]
</p><p>27 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi2c 1s8–2 2, To address this problem, we propose the use of a character level transformation model that can generate tokens that are morphologically related to query tokens. [sent-46, score-0.371]
</p><p>28 We train the model using morphological related stems that are extracted from hypertext/page title pairs from Wikipedia. [sent-47, score-0.692]
</p><p>29 Such pairs are good for the task at hand, because they show different ways to refer to the same concept. [sent-48, score-0.03]
</p><p>30 We show that expanding stems in a query  with related stems using our model outperforms the use of state-of-the-art statistical Arabic stemming. [sent-49, score-0.916]
</p><p>31 Further, the expansion can be applied to words directly to perform at par with statistical stemming. [sent-50, score-0.092]
</p><p>32 Laterally, the model can help produce spelling variants of transliterated names. [sent-51, score-0.128]
</p><p>33 The contribution of this paper is as follows: • We proposed an automatic method for learning character-level morphological transformations from Wikipedia hypertext/page title pairs. [sent-52, score-0.318]
</p><p>34 • When applied to stems, we show that the method overcomes some morphological problems that are associated with stemming, statistically significantly outperforming Arabic retrieval using statistical stemming and character n-grams. [sent-53, score-0.841]
</p><p>35 • When applied to words, we show that the method yields retrieval effectiveness at par with statistical stemming. [sent-54, score-0.21]
</p><p>36 Related Work Most studies are based on a single large collection from the TREC-2001/2002 cross-language retrieval track (Gey and Oard, 2001 ; Oard and Gey, 2002). [sent-56, score-0.179]
</p><p>37 , stems and roots (Darwish and Oard, 2002), light stemming (Aljlayl et al. [sent-60, score-0.803]
</p><p>38 , 2002), and character n-grams of various lengths (Darwish and Oard, 2002; Mayfield et al. [sent-62, score-0.101]
</p><p>39 The effects of normalizing alternative characters, removal of diacritics and stop-word removal have also been explored (Xu et al. [sent-64, score-0.062]
</p><p>40 These studies suggest that light stemming, character n-grams, and statistical stemming are the better index terms. [sent-66, score-0.509]
</p><p>41 Morphological approaches assume an Arabic word is constituted from prefixes-stem-suffixes and aim to remove prefixes and suffixes. [sent-67, score-0.048]
</p><p>42 Since Arabic morphology is ambiguous, statistical stemming attempts to find the most likely segmentation of 219 words. [sent-68, score-0.494]
</p><p>43 (2003) used a trigram language model with a minimal set of manually crafted rules to achieve a stemming accuracy of 97. [sent-71, score-0.326]
</p><p>44 (2005) to lead to statistical improvements over using light stemming. [sent-74, score-0.082]
</p><p>45 Although consistency is more important for IR applications than linguistic correctness, perhaps improved correctness would naturally yield great consistency. [sent-79, score-0.063]
</p><p>46 In this paper, we used a reimplementation of the system proposed by Diab (2009) with the same training set as a baseline. [sent-80, score-0.042]
</p><p>47 Concerning the automatic induction of morphologically related word-forms, Hammarström (2009) surveyed fairly comprehensively many unsupervised morphology learning approaches. [sent-81, score-0.33]
</p><p>48 MDL based approach was improved by: Goldsmith (2001) who applied the EM algorithm to improve the precision of pairing stems prior to suffix induction; and Schone and Jurafsky (2001) who applied latent semantic analysis to determine if two words are semantically related. [sent-84, score-0.454]
</p><p>49 Baroni (2002) extended his work by incorporating semantic similarity features, via  mutual information, and orthographic features, via edit distance. [sent-88, score-0.071]
</p><p>50 Chen and Gey (2002) utilized a bilingual dictionary to find Arabic words with a common stem that map to the same English stem. [sent-89, score-0.124]
</p><p>51 Also in the cross-language spirit, Snyder and Barzilay (2008) used cross-language mappings to learn morpheme patterns and consequently automatically segment words. [sent-90, score-0.123]
</p><p>52 Creutz and Lagus (2007) proposed a probabilistic model for automatic word segment discovery. [sent-92, score-0.035]
</p><p>53 Most of these approaches can discover suffixes and prefixes without human intervention. [sent-93, score-0.124]
</p><p>54 However, they may not be able to handle infixation and spelling variations. [sent-94, score-0.076]
</p><p>55 (2006) used approximate string matching to automatically map morphologically similar words in noisy dictionary data. [sent-96, score-0.222]
</p><p>56 They used the mappings to learn affixation, including infixiation, from noisy data. [sent-97, score-0.054]
</p><p>57 In this paper, we propose a new technique for finding morphologically related word-forms based on learning character-level mappings. [sent-98, score-0.211]
</p><p>58 1 Training Data In our experiments, we extracted Wikipedia hypertext to page title pairs as in Figure 1. [sent-102, score-0.296]
</p><p>59 From them, we attempted to find word pairs that were morphologically related. [sent-106, score-0.193]
</p><p>60 From the example in Figure 1, given the hypertext ‫ ( ﺑﺎﻟﺒﺮﺗﻐﺎﻟﻴﯿﺔ‬bAlbrtgAlyp – in Portuguese) and the page title that it points to ‫ ( ﻟﻐﺔ ﺑﺮﺗﻐﺎﻟﻴﯿﺔ‬lgp brtgAlyp Portuguese language) we needed to extract the  pairs ‫ ( ﺑﺎﻟﺒﺮﺗﻐﺎﻟﻴﯿﺔ‬bAlbrtgAlyp) and ‫ ( ﺑﺮﺗﻐﺎﻟﻴﯿﺔ‬brtgAlyp). [sent-107, score-0.296]
</p><p>61 We assumed that a word in the hypertext and another in Wikipedia title were morphologically related using the following criteria: • The words share the first 2 letters or the last 2 letters. [sent-108, score-0.492]
</p><p>62 • The edit distance between the two words must be <= 3. [sent-110, score-0.061]
</p><p>63 The choice of 3 was motivated by the fact that Arabic prefixes and suffixes are typically 1, 2, or 3 letters long. [sent-111, score-0.193]
</p><p>64 • The edit distance was less than 50% of the length of the shorter of the two words. [sent-112, score-0.032]
</p><p>65 This was important to insure that short words that share common letters but are in fact different are filtered out. [sent-113, score-0.098]
</p><p>66 All words in the word pairs were stemmed using a reimplementation of the stemmer of Diab (2009). [sent-115, score-0.143]
</p><p>67 In the first, we aligned the stems of the word pairs at character level. [sent-118, score-0.521]
</p><p>68 In the second, we aligned the  words of the word pairs at character level without stemming. [sent-119, score-0.16]
</p><p>69 To apply a machine translation analogy, we treated words as sentences and the letters from which were constructed as tokens. [sent-122, score-0.098]
</p><p>70 Source character sequence lengths were restricted to 3 letters. [sent-124, score-0.101]
</p><p>71 Generating related stems/words: We treated the problem of generating morphologically related stems (or words) like a transliteration mining problem akin to that in Udupa et al. [sent-125, score-0.602]
</p><p>72 Briefly, the miner used character segment mappings to generate all possible transformations while constraining generation to the existing tokens (either stems or words) in a list of unique tokens in the retrieval test collection. [sent-127, score-0.847]
</p><p>73 Basically, given a query token, all possible segmentations, where each segment has a maximum length of 3 characters, were produced along with their associated mappings. [sent-128, score-0.105]
</p><p>74 Given all mapping combinations, combinations producing valid target tokens were retained and sorted according to the product of their mapping probabilities. [sent-129, score-0.037]
</p><p>75 To illustrate how this works, consider  the following example: Given a query word “min”, target words in the word list {moon, men, man, min} , and the possible mappings for the segments and their probabilities: m = {(m, 0. [sent-130, score-0.153]
</p><p>76 1  Experimental Setup  We used extrinsic IR evaluation to determine the quality of the related stems that were generated. [sent-156, score-0.39]
</p><p>77 We performed experiments on the TREC 2001/2002 cross language track collection, which contains 383,872 Arabic newswire articles and 75 topics with their relevance judgments (Oard and Gey, 2002). [sent-157, score-0.032]
</p><p>78 This is presently the best available large Arabic information retrieval test collection. [sent-158, score-0.147]
</p><p>79 We used Mean Average Precision (MAP) as the measure of goodness for this retrieval task. [sent-159, score-0.147]
</p><p>80 All experiments were performed using the Indri retrieval toolkit, which uses a retrieval model that combines inference networks and language modeling and implements advanced query operators (Metzler and Croft, 2004). [sent-162, score-0.364]
</p><p>81 05 to determine if a set of retrieval results was better than another. [sent-164, score-0.147]
</p><p>82 We replaced each query tokens with all the related stems that were generated using a weighted synonym operator (Wang and Oard, 2006), where the weights correspond to the product of the mapping probabilities for each related word. [sent-165, score-0.555]
</p><p>83 With the weighted synonym operator, we did not need to  threshold the generated related stems as ones with low probabilities were demoted. [sent-166, score-0.419]
</p><p>84 Probabilities were normalized by the score of the original query word. [sent-167, score-0.07]
</p><p>85 For example, given the stem ‫ ( ﺻﻨﺎﻉع‬SnAE) it was replaced with: #wsyn(1 . [sent-168, score-0.065]
</p><p>86 2 A76249P654 w oSrdtas i/ tecmals/yhbaert4-grhamns  221 namely: using raw words, using statistical stemming (Diab, 2009), and character 4-grams. [sent-176, score-0.456]
</p><p>87 For all runs, we performed letter normalization, where we conflated: variants of “alef”, “ta marbouta” and “ha”, “alef maqsoura” and “ya”, and the different forms of “hamza”. [sent-177, score-0.053]
</p><p>88 2  Experimental Results  Table 1 reports retrieval results. [sent-179, score-0.147]
</p><p>89 Expanding stems  using morphologically related stems yielded statistically significant improvements over using words, stems, and character 4-grams. [sent-180, score-1.096]
</p><p>90 Expanding words yielded results that were statistically significantly better than using words, and statistically indistinguishable from using 4-grams and stems. [sent-181, score-0.172]
</p><p>91 As the results show, the proposed technique improves upon statistical stemming by overcoming the shortfalls of stemming. [sent-182, score-0.44]
</p><p>92 Another phenomenon that was addressed implicitly by the proposed technique had to do with detecting variant spellings of transliterated names. [sent-183, score-0.153]
</p><p>93 This draws from the fact that differences in spelling variations and the construction of broken plurals are typically due to the insertion or deletion of long vowels. [sent-184, score-0.174]
</p><p>94 Conclusion In this paper, we presented a method for generating morphologically related tokens from Wikipedia hypertext to page title pairs. [sent-187, score-0.466]
</p><p>95 We showed that the method overcomes some of the problems of statistical stemming to yield statistically significant  improvements in Arabic retrieval over using statistical stemming. [sent-188, score-0.655]
</p><p>96 The technique can also be applied on words to yield results that statistically indistinguishable from statistical stemming. [sent-189, score-0.23]
</p><p>97 The technique had the added advantage of detecting variable spellings of transliterated named entities. [sent-190, score-0.153]
</p><p>98 For future work, we would like to try the proposed technique on other languages, because it would likely be effective in automatically learning character-level morphological transformations as well as overcoming some of the problems associated with stemming. [sent-191, score-0.278]
</p><p>99 It is worthwhile to devise models that concurrently generate morphological and phonologically related tokens. [sent-192, score-0.147]
</p><p>100 Unsupervised discovery of morphologically related words based on orthographic and semantic similarity. [sent-216, score-0.231]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('arabic', 0.425), ('stems', 0.39), ('stemming', 0.326), ('oard', 0.231), ('darwish', 0.211), ('gey', 0.186), ('morphologically', 0.163), ('retrieval', 0.147), ('morphological', 0.147), ('morphology', 0.139), ('trec', 0.139), ('title', 0.125), ('hypertext', 0.106), ('character', 0.101), ('larkey', 0.08), ('diab', 0.076), ('suffixes', 0.076), ('broken', 0.072), ('query', 0.07), ('mayfield', 0.069), ('letters', 0.069), ('stem', 0.065), ('inflected', 0.065), ('plurals', 0.063), ('plural', 0.063), ('wikipedia', 0.061), ('transliterated', 0.059), ('gaithersburg', 0.056), ('qatar', 0.056), ('mappings', 0.054), ('alef', 0.053), ('aljlayl', 0.053), ('balbrtgalyp', 0.053), ('brtgalyp', 0.053), ('goldsmith', 0.053), ('hammarstr', 0.053), ('infixes', 0.053), ('jacquemin', 0.053), ('snae', 0.053), ('udupa', 0.053), ('light', 0.053), ('letter', 0.053), ('statistically', 0.052), ('min', 0.051), ('transliteration', 0.049), ('prefixes', 0.048), ('technique', 0.048), ('cairo', 0.046), ('clitics', 0.046), ('emam', 0.046), ('schone', 0.046), ('spellings', 0.046), ('transformations', 0.046), ('creutz', 0.042), ('lagus', 0.042), ('mdl', 0.042), ('portuguese', 0.042), ('reimplementation', 0.042), ('stemmer', 0.042), ('metzler', 0.039), ('indistinguishable', 0.039), ('overcomes', 0.039), ('baroni', 0.039), ('qf', 0.039), ('orthographic', 0.039), ('spelling', 0.039), ('tokens', 0.037), ('men', 0.037), ('overcoming', 0.037), ('brent', 0.037), ('expanding', 0.037), ('handle', 0.037), ('attachment', 0.036), ('semitic', 0.035), ('suffix', 0.035), ('segment', 0.035), ('page', 0.035), ('morpheme', 0.034), ('roots', 0.034), ('par', 0.034), ('drawbacks', 0.034), ('croft', 0.034), ('ahmed', 0.033), ('snyder', 0.033), ('yield', 0.033), ('edit', 0.032), ('track', 0.032), ('cast', 0.031), ('removal', 0.031), ('pairs', 0.03), ('correctness', 0.03), ('map', 0.03), ('produce', 0.03), ('operator', 0.029), ('statistical', 0.029), ('mi', 0.029), ('words', 0.029), ('synonym', 0.029), ('qa', 0.029), ('unsupervised', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="27-tfidf-1" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>2 0.25300935 <a title="27-tfidf-2" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>Author: Emad Mohamed ; Behrang Mohit ; Kemal Oflazer</p><p>Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.</p><p>3 0.24667746 <a title="27-tfidf-3" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>4 0.1561216 <a title="27-tfidf-4" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>Author: David Stallard ; Jacob Devlin ; Michael Kayser ; Yoong Keok Lee ; Regina Barzilay</p><p>Abstract: If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter.</p><p>5 0.11174625 <a title="27-tfidf-5" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>6 0.1013151 <a title="27-tfidf-6" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>7 0.099074125 <a title="27-tfidf-7" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>8 0.094120294 <a title="27-tfidf-8" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>9 0.091587268 <a title="27-tfidf-9" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>10 0.081631318 <a title="27-tfidf-10" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>11 0.077066682 <a title="27-tfidf-11" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>12 0.076580696 <a title="27-tfidf-12" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>13 0.075802132 <a title="27-tfidf-13" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>14 0.06846381 <a title="27-tfidf-14" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>15 0.063111857 <a title="27-tfidf-15" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>16 0.062766381 <a title="27-tfidf-16" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>17 0.061783034 <a title="27-tfidf-17" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>18 0.057655435 <a title="27-tfidf-18" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>19 0.057387196 <a title="27-tfidf-19" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>20 0.055254392 <a title="27-tfidf-20" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.165), (1, -0.052), (2, -0.017), (3, 0.015), (4, 0.094), (5, 0.201), (6, 0.081), (7, -0.146), (8, -0.015), (9, -0.027), (10, -0.136), (11, -0.072), (12, 0.16), (13, -0.119), (14, 0.039), (15, -0.211), (16, -0.248), (17, -0.118), (18, -0.123), (19, 0.004), (20, 0.068), (21, 0.007), (22, 0.091), (23, -0.077), (24, -0.084), (25, 0.013), (26, 0.018), (27, -0.003), (28, -0.059), (29, 0.034), (30, -0.088), (31, -0.039), (32, -0.07), (33, -0.018), (34, -0.038), (35, 0.065), (36, 0.059), (37, -0.021), (38, -0.019), (39, 0.003), (40, 0.073), (41, -0.082), (42, -0.032), (43, -0.027), (44, -0.038), (45, -0.016), (46, -0.001), (47, -0.034), (48, 0.014), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95030391 <a title="27-lsi-1" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>2 0.82494837 <a title="27-lsi-2" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>Author: Emad Mohamed ; Behrang Mohit ; Kemal Oflazer</p><p>Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.</p><p>3 0.76567829 <a title="27-lsi-3" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>Author: David Stallard ; Jacob Devlin ; Michael Kayser ; Yoong Keok Lee ; Regina Barzilay</p><p>Abstract: If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter.</p><p>4 0.54572427 <a title="27-lsi-4" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>5 0.4446972 <a title="27-lsi-5" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>6 0.434598 <a title="27-lsi-6" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>7 0.38695779 <a title="27-lsi-7" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>8 0.37599164 <a title="27-lsi-8" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>9 0.30498773 <a title="27-lsi-9" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>10 0.30251333 <a title="27-lsi-10" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>11 0.29782104 <a title="27-lsi-11" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>12 0.29645994 <a title="27-lsi-12" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<p>13 0.27963689 <a title="27-lsi-13" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>14 0.27825361 <a title="27-lsi-14" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>15 0.27510703 <a title="27-lsi-15" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>16 0.24938378 <a title="27-lsi-16" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>17 0.24870783 <a title="27-lsi-17" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>18 0.24773058 <a title="27-lsi-18" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>19 0.2387428 <a title="27-lsi-19" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>20 0.23565556 <a title="27-lsi-20" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.023), (26, 0.048), (28, 0.041), (30, 0.016), (37, 0.013), (39, 0.056), (57, 0.434), (74, 0.014), (82, 0.02), (84, 0.037), (85, 0.028), (90, 0.102), (92, 0.049), (94, 0.021), (99, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78680217 <a title="27-lda-1" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>2 0.65559298 <a title="27-lda-2" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>Author: William Yang Wang ; Elijah Mayfield ; Suresh Naidu ; Jeremiah Dittmar</p><p>Abstract: We propose a latent variable model to enhance historical analysis of large corpora. This work extends prior work in topic modelling by incorporating metadata, and the interactions between the components in metadata, in a general way. To test this, we collect a corpus of slavery-related United States property law judgements sampled from the years 1730 to 1866. We study the language use in these legal cases, with a special focus on shifts in opinions on controversial topics across different regions. Because this is a longitudinal data set, we are also interested in understanding how these opinions change over the course of decades. We show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identification tasks. Experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting, and that these improvements are robust across different parameter settings.</p><p>3 0.65191472 <a title="27-lda-3" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>4 0.59369946 <a title="27-lda-4" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>5 0.49708742 <a title="27-lda-5" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>6 0.45271826 <a title="27-lda-6" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>7 0.42601705 <a title="27-lda-7" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>8 0.40061897 <a title="27-lda-8" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>9 0.38803807 <a title="27-lda-9" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>10 0.3711952 <a title="27-lda-10" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>11 0.37026924 <a title="27-lda-11" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>12 0.36390975 <a title="27-lda-12" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>13 0.36229125 <a title="27-lda-13" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>14 0.36151838 <a title="27-lda-14" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>15 0.35990858 <a title="27-lda-15" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>16 0.35987192 <a title="27-lda-16" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>17 0.35772491 <a title="27-lda-17" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>18 0.3548578 <a title="27-lda-18" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>19 0.35359469 <a title="27-lda-19" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>20 0.35198373 <a title="27-lda-20" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
