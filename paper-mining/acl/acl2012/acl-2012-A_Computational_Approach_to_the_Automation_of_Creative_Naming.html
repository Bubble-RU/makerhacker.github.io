<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 acl-2012-A Computational Approach to the Automation of Creative Naming</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-7" href="#">acl2012-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 acl-2012-A Computational Approach to the Automation of Creative Naming</h1>
<br/><p>Source: <a title="acl-2012-7-pdf" href="http://aclweb.org/anthology//P/P12/P12-1074.pdf">pdf</a></p><p>Author: Gozde Ozbal ; Carlo Strapparava</p><p>Abstract: In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation ofthe naming process.</p><p>Reference: <a title="acl-2012-7-reference" href="../acl2012_reference/acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. [sent-2, score-0.758]
</p><p>2 The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation ofthe naming process. [sent-5, score-0.48]
</p><p>3 1 Introduction A catchy, memorable and creative name is an important key to a successful business since the name provides the first image and defines the identity of  the service to be promoted. [sent-6, score-0.528]
</p><p>4 Besides, this task requires a good understanding of the service to be promoted, creativity and high linguistic skills to be able to play with words. [sent-9, score-0.197]
</p><p>5 Furthermore, since many new products and companies emerge every year, the naming style is continuously changing and creativity standards need to be adapted to rapidly changing requirements. [sent-10, score-0.627]
</p><p>6 eu and effective names do not come out of the blue. [sent-13, score-0.137]
</p><p>7 Although it might not be easy to perceive all the effort behind the naming process just based on the final  output, both a training phase and a long process consisting of many iterations are certainly required for coming up with a good name. [sent-14, score-0.522]
</p><p>8 From a practical point of view, naming agencies and branding firms, together with automatic name generators, can be considered as two alternative services that facilitate the naming process. [sent-15, score-1.108]
</p><p>9 naming agencies and na¨ ıve generators) that can be used for obtaining name suggestions, we propose a system which combines several linguistic resources and natural language processing (NLP) techniques to generate creative names, more specifically neologisms based on homophonic puns and metaphors. [sent-20, score-1.154]
</p><p>10 In this system, similarly to the previously mentioned generators, users are able to determine the category of the service to be promoted together with the features to be emphasized. [sent-21, score-0.152]
</p><p>11 Our improvement lies in the fact that instead of random  generation, we take semantic, phonetic, lexical and morphological knowledge into consideration to automatize the naming process. [sent-22, score-0.436]
</p><p>12 Although various resources provide distinct tips for inventing creative names, no attempt has been made to combine all means of creativity that can be used during the naming process. [sent-23, score-0.748]
</p><p>13 Furthermore, in addition to the devices stated by copywriters, there Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-24, score-0.141]
</p><p>14 Therefore, we consider the task of discovering and accumulating all crucial features of creativity to be essential before attempting to automatize the naming process. [sent-27, score-0.59]
</p><p>15 Accordingly, we create a gold standard of creative names and the corresponding creative devices that we collect from various sources. [sent-28, score-0.594]
</p><p>16 This resource is the starting point of our research in linguistic creativity for naming. [sent-29, score-0.196]
</p><p>17 First, we review the state-of-the-art relevant to the naming task. [sent-31, score-0.436]
</p><p>18 Later on, we describe the model that we have designed for the automatization of the naming process. [sent-33, score-0.473]
</p><p>19 2  Related Work  In this section, we will analyze the state of the art concerning the naming task from three different aspects: i) linguistic ii) computational iii) commercial. [sent-36, score-0.478]
</p><p>20 1 Linguistic Little research has been carried out to investigate the linguistic aspects of the naming mechanism. [sent-38, score-0.436]
</p><p>21 Bergh (1987) built a four-fold linguistic topology consisting of phonetic, orthographic, morphological and semantic categories to evaluate the frequency of linguistic devices in brand names. [sent-41, score-0.385]
</p><p>22 (2008) investigated the effects of relevance, connotation, and pronunciation of brand names on preferences of consumers. [sent-43, score-0.47]
</p><p>23 (2005) analyzed consumer evaluations of meaningful and nonmeaningful brand names and the results suggested that non-meaningful brand names are evaluated less favorably than meaningful ones even after repeated exposure. [sent-48, score-0.762]
</p><p>24 Lastly, cog (201 1) focused on the semantics of branding and based on the analysis of several 704 international brand names, it was shown that cognitive operations such as domain reduction/expansion, mitigation, and strengthening might be used unconsciously while creating a new brand name. [sent-49, score-0.598]
</p><p>25 2 Computational To the best of our knowledge, there is only one computational study in the literature that can be applied to the automatization of name generation. [sent-51, score-0.156]
</p><p>26 As more na¨ ıve solutions, automatic name generators can be used as a source of inspiration in the brainstorming phase to get ideas for good names. [sent-55, score-0.214]
</p><p>27 com randomly combines abbreviations, syllables and generic short words from different domains to obtain creative combinations. [sent-58, score-0.24]
</p><p>28 com randomly generates name ideas and available domains based on alliterations, compound words and custom word lists. [sent-61, score-0.164]
</p><p>29 Users can determine the prefix and suffix of the names to be generated. [sent-62, score-0.137]
</p><p>30 A shortcoming of these kinds of automatic generators is that random generation can output so many bad suggestions and users have to be patient to find the name that they are looking for. [sent-69, score-0.266]
</p><p>31 3 Commercial Many naming agencies and branding firms1 provide professional service to aid with the naming of new 1e. [sent-72, score-1.032]
</p><p>32 com  www  com, www  products, domains, companies and brands. [sent-80, score-0.147]
</p><p>33 Although the resulting names can be successful and satisfactory, these services are very expensive and the processing time is rather long. [sent-83, score-0.226]
</p><p>34 These names were compiled from a book dedicated to brand naming strategies (Botton and Cegarra, 1990) and various web resources related to creative naming such as adslogans . [sent-86, score-1.411]
</p><p>35 Our list contains names which were invented via various creativity methods. [sent-90, score-0.291]
</p><p>36 While the creativity in some of these names is independent of the context and the names themselves are sufficient to realize the methods used (e. [sent-91, score-0.428]
</p><p>37 For instance, Thanks a Latte is a coffee bar name where the phonetic similarity between “lot” and “latte” (a coffee type meaning “milk” in Italian) is exploited. [sent-94, score-0.246]
</p><p>38 The first branch required the annotators to fill in the domain descrip-  tion of the names in question together with their etymologies if required, while the second asked them to determine the devices of creativity used in each name. [sent-98, score-0.556]
</p><p>39 In order to obtain the list of creativity devices, we collected a total of 3 1 attributes used in the naming process from various resources including academic papers, naming agents, branding and advertisement experts. [sent-99, score-1.215]
</p><p>40 The phonetic category includes attributes such as rhyme (i. [sent-105, score-0.315]
</p><p>41 Teenie Weenie), while the orthographic category consists ofdevices such as acronyms (e. [sent-113, score-0.153]
</p><p>42 Finally, the semantic category includes attributes such as metaphors (i. [sent-129, score-0.282]
</p><p>43 Accordingly, we have made a systematic attempt to replicate these processes, and implemented a system which combines methods and resources used in various areas of Natural Language Processing (NLP) to create neologisms based on homophonic puns and metaphors. [sent-139, score-0.397]
</p><p>44 While the variety of creativity devices is actually much bigger, our work can be consid-  ered as a starting point to investigate which kinds of technologies can successfully be exploited in which way to support the naming process. [sent-140, score-0.731]
</p><p>45 1 Specifying the category and properties Our design allows users to determine the category of the product/brand/company to be advertised (e. [sent-144, score-0.296]
</p><p>46 In the current implementation, categories are required to be nouns while properties are required to be adjectives. [sent-149, score-0.162]
</p><p>47 These inputs that are specified by users constitute the main  ingredients of the naming process. [sent-150, score-0.675]
</p><p>48 After the determination of these ingredients, several techniques and resources are utilized to enlarge the ingredient list, and thereby to increase the variety of new and creative names. [sent-151, score-0.273]
</p><p>49 For instance, if the category has been determined as “shampoo”, we need to learn that “it is used for washing hair” or “it can be found in the bathroom”, so that all this extra information can be included in the naming process. [sent-154, score-0.545]
</p><p>50 We add all the words appearing in relations with the category word to our ingredient list. [sent-174, score-0.266]
</p><p>51 3  Adding semantically related words  To further increase the size of the ingredient list,  we utilize another resource called WordNet (Miller, 1995), which is a large lexical database for English. [sent-182, score-0.157]
</p><p>52 In addition to the direct hypernyms of the category word, we increase the size of the ingredient list by adding synonyms of the category word, the new words coming from the relations and the properties determined by the user. [sent-191, score-0.497]
</p><p>53 4 Retrieving metaphors A metaphor is a figure of speech in which an implied comparison is made to indicate how two things that are not alike in most ways are similar in one important way. [sent-196, score-0.189]
</p><p>54 Metaphors are common devices for evocation, which has been found to be a very important technique used in naming according to the analysis of our dataset. [sent-197, score-0.577]
</p><p>55 In this work, to metaphorically ascribe a property to a term, stereotypes for which the property is culturally salient are intersected with stereotypes to which the term is pragmatically comparable. [sent-199, score-0.256]
</p><p>56 The stereotypes for a property are found by querying on the web with the simile pattern “as hpropertyi as *”. [sent-200, score-0.165]
</p><p>57 Unlike the proposed approach, hwper dpoe rntoyit aaspp *ly” any intersection with comparable  stereotypes since the naming task should favor further terms to the category word in order to exaggerate, to evoke and thereby to be more effective. [sent-201, score-0.618]
</p><p>58 Although the list that we obtain in the end has many potentially valuable metaphors (e. [sent-210, score-0.168]
</p><p>59 To illustrate, for the same example property bright, the metaphors obtained at the end of the process are sun, light and day. [sent-224, score-0.186]
</p><p>60 5 Generating neologisms After the ingredient list is complete, the phonetic  module analyzes all ingredient pairs to generate neologisms with possibly homophonic puns based on phonetic similarity. [sent-226, score-1.118]
</p><p>61 and the current phoneme set contains 39 phonemes based on the ARPAbet symbol set, which has been developed for speech recognition uses. [sent-230, score-0.141]
</p><p>62 We conducted a mapping from the ARPAbet phonemes to the international phonetic alphabet (IPA) phonemes and we grouped the IPA phonemes based on the  phoneme classification documented in IPA. [sent-231, score-0.464]
</p><p>63 After having the pronunciation of each word in the ingredient list, shorter pronunciation strings are compared against the substrings of longer ones. [sent-233, score-0.293]
</p><p>64 Among the different possible distance metrics that can be applied for calculating the phonetic similarity between two pronunciation strings, we have chosen the Levenshtein distance (Levenshtein, 1966). [sent-234, score-0.216]
</p><p>65 tFhor ‘ it’h)e, sditisttinan →ce csiatltcinulgat (iionnse, rwtieo nem o-f ploy relaxation by giving a smaller penalty for the  708 phonemes appearing in the same phoneme groups mentioned previously. [sent-238, score-0.141]
</p><p>66 Since there is no one-to-one relationship between letters and phonemes and no information about which phoneme is related to which letter(s) is available, it is not straightforward to combine two words after determining the pairs via Levenshtein distance calculation. [sent-241, score-0.141]
</p><p>67 To adapt this tool according to our needs, we split all the words in our dictionary into letters and their mapped pronunciation to their phonemes, so that the aligner could learn a mapping from phonemes to characters. [sent-244, score-0.187]
</p><p>68 As an example, if the first ingredient is bright and the second ingredient is light, the name blight can be obtained at the end of 2http://code. [sent-247, score-0.413]
</p><p>69 5  Evaluation  We evaluated the performance of our system with a manual annotation in which 5 annotators judged a set of neologisms along 4 dimensions: 1) appropriateness, i. [sent-256, score-0.319]
</p><p>70 the number of ingredients (0, 1 or 2) used to generate the neologism which are appropriate for the input; 2) pleasantness, i. [sent-258, score-0.349]
</p><p>71 a binary decision concerning the conformance of the neologism to the sound patterns of English; 3) humor/wittiness, i. [sent-260, score-0.217]
</p><p>72 an assessment of the fitness of the neologism as a name for the target category/properties (unsuccessful, neutral, successful). [sent-264, score-0.229]
</p><p>73 To determine the properties to be underlined, we asked two annotators to state the properties that they would expect to have in a product or company belonging to each category in our category list. [sent-266, score-0.494]
</p><p>74 Then, we merged the answers coming from the two annotators to create the final set of properties for each category. [sent-267, score-0.204]
</p><p>75 Therefore, we implemented a ranking mechanism  which used a hybrid scoring method by giving equal weights to the language model and the normalized phonetic similarity. [sent-269, score-0.178]
</p><p>76 Among the ranked neologisms for each input, we only selected the top 20 to build the dataset. [sent-270, score-0.237]
</p><p>77 With 5 annotators, a majority class greater than or equal to 3 means that the absolute majority of the annotators agreed on the same decision. [sent-291, score-0.235]
</p><p>78 For pleasantness (PLE) and humor (HUM), the absolute majority of the annotators (i. [sent-293, score-0.231]
</p><p>79 Nevertheless, in almost 73% of the cases the absolute majority of the annotators agreed on the annotation of this dimension. [sent-297, score-0.177]
</p><p>80 Table 4 shows the micro and macro-average of the percentage of cases in which at least 3 annotators have labeled the ingredients as appropriate (APP), and the neologisms as pleasant (PLE), humorous (HUM) or successful (SUX). [sent-298, score-0.742]
</p><p>81 The system selects appropriate ingredients in approximately 60% of the cases, and outputs pleasant, English-sounding names in ∼87% of the cases. [sent-299, score-0.376]
</p><p>82 Even though  we do not explicitly try to inject humor in the neologisms, more than 15% of the generated names turn out to be witty or amusing. [sent-302, score-0.191]
</p><p>83 The system managed to generate at least one successful name for all 50 input categories and at least one witty name for 42. [sent-303, score-0.327]
</p><p>84 This finding confirms our intuition that amusing names have the potential to be very appealing to the customers. [sent-318, score-0.174]
</p><p>85 In more than 76% of the cases, a humorous name is the product of the combination of appropriate ingredients. [sent-319, score-0.177]
</p><p>86 In Table 2, we show a selection of successful  and unsuccessful outputs generated for the category and the set of properties listed under the block of columns labeled as Input according to the majority of annotators (i. [sent-320, score-0.496]
</p><p>87 The model correctly selects the ingredients eat (a restaurant is UsedFor eating), pizza and pasta (which are found AtLocation restaurant) to generate an appropriate name. [sent-324, score-0.419]
</p><p>88 The three “palatable” neologisms generated are eatalian (from the combination of eat and Italian), pastarant (pasta + restaurant) andpeatza (pizza + eat). [sent-325, score-0.312]
</p><p>89 As a matter of fact, it turns out that the name Eatalian is actually used by at least one real Italian restaurant located in Los Angeles, CA3. [sent-327, score-0.206]
</p><p>90 For the same set of stimuli, the model also selects some ingredients which are not really related to the use-case, e. [sent-328, score-0.239]
</p><p>91 Another class of negative results includes neologisms generated from ingredients that the model cannot combine in a good English-sounding neologism (e. [sent-340, score-0.586]
</p><p>92 6  Conclusion  In this paper, we have focused on the task of automa-  tizing the naming process and described a computational approach to generate neologisms with homophonic puns based on phonetic similarity. [sent-343, score-0.96]
</p><p>93 This study is our first step towards the systematic emulation of the various creative devices involved in the naming process by means of computational methods. [sent-344, score-0.735]
</p><p>94 Due to the complexity of the problem, a unified model to handle all the creative devices at the same time seems outside the reach of the current state-ofthe-art NLP techniques. [sent-345, score-0.299]
</p><p>95 We also want to extend the model to include multiword ingredients and to generate not only words but also short phrases. [sent-349, score-0.239]
</p><p>96 Then, we would like to focus on other classes of creative devices, such as affixation or rhyming. [sent-350, score-0.158]
</p><p>97 Creating new brand names: Effects of relevance, connotation, and pronunciation. [sent-365, score-0.244]
</p><p>98 Strategic brand management: building, measuring and managing brand equity. [sent-380, score-0.488]
</p><p>99 Creating brand names with meaning: The use of sound symbolism. [sent-385, score-0.446]
</p><p>100 Creating brand identity: a study of evaluation of new brand names. [sent-389, score-0.488]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('naming', 0.436), ('brand', 0.244), ('ingredients', 0.239), ('neologisms', 0.237), ('shampoo', 0.164), ('conceptnet', 0.159), ('creative', 0.158), ('creativity', 0.154), ('devices', 0.141), ('names', 0.137), ('metaphors', 0.131), ('phonetic', 0.127), ('name', 0.119), ('ingredient', 0.115), ('neologism', 0.11), ('category', 0.109), ('phonemes', 0.098), ('generators', 0.095), ('atlocation', 0.091), ('successful', 0.089), ('pronunciation', 0.089), ('restaurant', 0.087), ('wordnet', 0.086), ('annotators', 0.082), ('homophonic', 0.08), ('milk', 0.08), ('puns', 0.08), ('unsuccessful', 0.08), ('properties', 0.078), ('branding', 0.073), ('hum', 0.073), ('madeof', 0.073), ('stereotypes', 0.073), ('sound', 0.065), ('bright', 0.064), ('app', 0.058), ('humorous', 0.058), ('metaphor', 0.058), ('majority', 0.058), ('property', 0.055), ('appropriateness', 0.055), ('bergh', 0.055), ('blending', 0.055), ('pasta', 0.055), ('sux', 0.055), ('usedfor', 0.055), ('humor', 0.054), ('suggestions', 0.052), ('ple', 0.051), ('mechanism', 0.051), ('www', 0.051), ('connotation', 0.048), ('zbal', 0.048), ('com', 0.045), ('coming', 0.044), ('automation', 0.044), ('acronyms', 0.044), ('agencies', 0.044), ('service', 0.043), ('levenshtein', 0.043), ('phoneme', 0.043), ('attributes', 0.042), ('relations', 0.042), ('concerning', 0.042), ('required', 0.042), ('resource', 0.042), ('accordingly', 0.041), ('concepts', 0.041), ('eat', 0.038), ('company', 0.038), ('products', 0.037), ('obtain', 0.037), ('agreed', 0.037), ('edits', 0.037), ('strapparava', 0.037), ('advertisement', 0.037), ('amusing', 0.037), ('automatization', 0.037), ('bao', 0.037), ('bathroom', 0.037), ('botton', 0.037), ('cegarra', 0.037), ('eatalian', 0.037), ('hasproperty', 0.037), ('hpropertyi', 0.037), ('ipa', 0.037), ('kitten', 0.037), ('kohli', 0.037), ('latte', 0.037), ('leanne', 0.037), ('mysterious', 0.037), ('mystic', 0.037), ('perfume', 0.037), ('pleasant', 0.037), ('pleasantness', 0.037), ('provocative', 0.037), ('rhyme', 0.037), ('spocleang', 0.037), ('sunglasses', 0.037), ('unconsciously', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="7-tfidf-1" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>Author: Gozde Ozbal ; Carlo Strapparava</p><p>Abstract: In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation ofthe naming process.</p><p>2 0.11639985 <a title="7-tfidf-2" href="./acl-2012-Humor_as_Circuits_in_Semantic_Networks.html">112 acl-2012-Humor as Circuits in Semantic Networks</a></p>
<p>Author: Igor Labutov ; Hod Lipson</p><p>Abstract: This work presents a first step to a general implementation of the Semantic-Script Theory of Humor (SSTH). Of the scarce amount of research in computational humor, no research had focused on humor generation beyond simple puns and punning riddles. We propose an algorithm for mining simple humorous scripts from a semantic network (ConceptNet) by specifically searching for dual scripts that jointly maximize overlap and incongruity metrics in line with Raskin’s Semantic-Script Theory of Humor. Initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles. We evaluate the said metrics through a user-assessed quality of the generated two-liners.</p><p>3 0.095656589 <a title="7-tfidf-3" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>Author: Karin Mauge ; Khash Rohanimanesh ; Jean-David Ruvini</p><p>Abstract: Large e-commerce enterprises feature millions of items entered daily by a large variety of sellers. While some sellers provide rich, structured descriptions of their items, a vast majority of them provide unstructured natural language descriptions. In the paper we present a 2 steps method for structuring items into descriptive properties. The first step consists in unsupervised property discovery and extraction. The second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm. We evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall.</p><p>4 0.087674186 <a title="7-tfidf-4" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Jacob Eisenstein</p><p>Abstract: ILCC, School of Informatics School of Interactive Computing University of Edinburgh Georgia Institute of Technology Edinburgh, EH8 9AB, UK Atlanta, GA, 30308, USA (a) intended: /ju want w2n/ /want e kUki/ (b) surface: [j@ w a?P w2n] [wan @ kUki] During early language acquisition, infants must learn both a lexicon and a model of phonetics that explains how lexical items can vary in pronunciation—for instance “the” might be realized as [Di] or [D@]. Previous models of acquisition have generally tackled these problems in isolation, yet behavioral evidence suggests infants acquire lexical and phonetic knowledge simultaneously. We present a Bayesian model that clusters together phonetic variants of the same lexical item while learning both a language model over lexical items and a log-linear model of pronunciation variability based on articulatory features. The model is trained on transcribed surface pronunciations, and learns by bootstrapping, without access to the true lexicon. We test the model using a corpus of child-directed speech with realistic phonetic variation and either gold standard or automatically induced word boundaries. In both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item has a unique pronunciation.</p><p>5 0.087479562 <a title="7-tfidf-5" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>Author: Hao Tang ; Joseph Keshet ; Karen Livescu</p><p>Abstract: We address the problem of learning the mapping between words and their possible pronunciations in terms of sub-word units. Most previous approaches have involved generative modeling of the distribution of pronunciations, usually trained to maximize likelihood. We propose a discriminative, feature-rich approach using large-margin learning. This approach allows us to optimize an objective closely related to a discriminative task, to incorporate a large number of complex features, and still do inference efficiently. We test the approach on the task of lexical access; that is, the prediction of a word given a phonetic transcription. In experiments on a subset of the Switchboard conversational speech corpus, our models thus far improve classification error rates from a previously published result of 29.1% to about 15%. We find that large-margin approaches outperform conditional random field learning, and that the Passive-Aggressive algorithm for largemargin learning is faster to converge than the Pegasos algorithm.</p><p>6 0.083530948 <a title="7-tfidf-6" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>7 0.075611189 <a title="7-tfidf-7" href="./acl-2012-Applications_of_GPC_Rules_and_Character_Structures_in_Games_for_Learning_Chinese_Characters.html">26 acl-2012-Applications of GPC Rules and Character Structures in Games for Learning Chinese Characters</a></p>
<p>8 0.073235169 <a title="7-tfidf-8" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>9 0.063904479 <a title="7-tfidf-9" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>10 0.060577616 <a title="7-tfidf-10" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>11 0.049861051 <a title="7-tfidf-11" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>12 0.048174996 <a title="7-tfidf-12" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>13 0.047917075 <a title="7-tfidf-13" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>14 0.047619201 <a title="7-tfidf-14" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>15 0.047275666 <a title="7-tfidf-15" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>16 0.047110725 <a title="7-tfidf-16" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>17 0.046675093 <a title="7-tfidf-17" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>18 0.046572767 <a title="7-tfidf-18" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>19 0.046287611 <a title="7-tfidf-19" href="./acl-2012-PDTB-style_Discourse_Annotation_of_Chinese_Text.html">157 acl-2012-PDTB-style Discourse Annotation of Chinese Text</a></p>
<p>20 0.045955893 <a title="7-tfidf-20" href="./acl-2012-CSNIPER_-_Annotation-by-query_for_Non-canonical_Constructions_in_Large_Corpora.html">44 acl-2012-CSNIPER - Annotation-by-query for Non-canonical Constructions in Large Corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.152), (1, 0.068), (2, -0.019), (3, 0.038), (4, 0.049), (5, 0.13), (6, 0.027), (7, 0.014), (8, -0.028), (9, 0.024), (10, -0.005), (11, 0.023), (12, -0.005), (13, 0.075), (14, -0.041), (15, -0.087), (16, 0.01), (17, 0.059), (18, -0.048), (19, 0.017), (20, -0.016), (21, -0.036), (22, -0.018), (23, -0.04), (24, -0.124), (25, 0.048), (26, 0.014), (27, -0.071), (28, 0.054), (29, -0.057), (30, 0.109), (31, 0.008), (32, 0.059), (33, 0.089), (34, 0.045), (35, -0.002), (36, 0.052), (37, 0.066), (38, 0.143), (39, -0.086), (40, 0.047), (41, -0.062), (42, 0.092), (43, -0.004), (44, -0.178), (45, 0.221), (46, -0.103), (47, 0.085), (48, -0.141), (49, -0.173)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.926072 <a title="7-lsi-1" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>Author: Gozde Ozbal ; Carlo Strapparava</p><p>Abstract: In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation ofthe naming process.</p><p>2 0.81047171 <a title="7-lsi-2" href="./acl-2012-Humor_as_Circuits_in_Semantic_Networks.html">112 acl-2012-Humor as Circuits in Semantic Networks</a></p>
<p>Author: Igor Labutov ; Hod Lipson</p><p>Abstract: This work presents a first step to a general implementation of the Semantic-Script Theory of Humor (SSTH). Of the scarce amount of research in computational humor, no research had focused on humor generation beyond simple puns and punning riddles. We propose an algorithm for mining simple humorous scripts from a semantic network (ConceptNet) by specifically searching for dual scripts that jointly maximize overlap and incongruity metrics in line with Raskin’s Semantic-Script Theory of Humor. Initial results show that a more relaxed constraint of this form is capable of generating humor of deeper semantic content than wordplay riddles. We evaluate the said metrics through a user-assessed quality of the generated two-liners.</p><p>3 0.65984702 <a title="7-lsi-3" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>Author: Karin Mauge ; Khash Rohanimanesh ; Jean-David Ruvini</p><p>Abstract: Large e-commerce enterprises feature millions of items entered daily by a large variety of sellers. While some sellers provide rich, structured descriptions of their items, a vast majority of them provide unstructured natural language descriptions. In the paper we present a 2 steps method for structuring items into descriptive properties. The first step consists in unsupervised property discovery and extraction. The second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm. We evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall.</p><p>4 0.61198115 <a title="7-lsi-4" href="./acl-2012-You_Had_Me_at_Hello%3A_How_Phrasing_Affects_Memorability.html">218 acl-2012-You Had Me at Hello: How Phrasing Affects Memorability</a></p>
<p>Author: Cristian Danescu-Niculescu-Mizil ; Justin Cheng ; Jon Kleinberg ; Lillian Lee</p><p>Abstract: Understanding the ways in which information achieves widespread public awareness is a research question of significant interest. We consider whether, and how, the way in which the information is phrased the choice of words and sentence structure — can affect this process. To this end, we develop an analysis framework and build a corpus of movie quotes, annotated with memorability information, in which we are able to control for both the speaker and the setting of the quotes. We find that there are significant differences between memorable and non-memorable quotes in several key dimensions, even after controlling for situational and contextual factors. One is lexical distinctiveness: in aggregate, memorable quotes use less common word choices, but at the same time are built upon a scaffolding of common syntactic patterns. Another is that memorable quotes tend to be more general in ways that make them easy to apply in new contexts — that is, more portable. — We also show how the concept of “memorable language” can be extended across domains. 1 Hello. My name is Inigo Montoya. Understanding what items will be retained in the public consciousness, and why, is a question of fundamental interest in many domains, including marketing, politics, entertainment, and social media; as we all know, many items barely register, whereas others catch on and take hold in many people’s minds. An active line of recent computational work has employed a variety of perspectives on this question. 892 Building on a foundation in the sociology of diffusion [27, 31], researchers have explored the ways in which network structure affects the way information spreads, with domains of interest including blogs [1, 11], email [37], on-line commerce [22], and social media [2, 28, 33, 38]. There has also been recent research addressing temporal aspects of how different media sources convey information [23, 30, 39] and ways in which people react differently to infor- mation on different topics [28, 36]. Beyond all these factors, however, one’s everyday experience with these domains suggests that the way in which a piece of information is expressed the choice of words, the way it is phrased might also have a fundamental effect on the extent to which it takes hold in people’s minds. Concepts that attain wide reach are often carried in messages such as political slogans, marketing phrases, or aphorisms whose language seems intuitively to be memorable, “catchy,” or otherwise compelling. Our first challenge in exploring this hypothesis is to develop a notion of “successful” language that is precise enough to allow for quantitative evaluation. We also face the challenge of devising an evaluation setting that separates the phrasing of a message from the conditions in which it was delivered highlycited quotes tend to have been delivered under compelling circumstances or fit an existing cultural, political, or social narrative, and potentially what appeals to us about the quote is really just its invocation of these extra-linguistic contexts. Is the form of the language adding an effect beyond or independent of these (obviously very crucial) factors? To — — — investigate the question, one needs a way of controlProce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A.s ?c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi8c 9s2–901, ling as much as possible for the role that the surrounding context of the language plays. — — The present work (i): Evaluating language-based memorability Defining what makes an utterance memorable is subtle, and scholars in several domains have written about this question. There is a rough consensus that an appropriate definition involves elements of both recognition people should be able to retain the quote and recognize it when they hear it invoked and production people should be motivated to refer to it in relevant situations [15]. One suggested reason for why some memes succeed is their ability to provoke emotions [16]. Alternatively, memorable quotes can be good for expressing the feelings, mood, or situation of an individual, a group, or a culture (the zeitgeist): “Certain quotes exquisitely capture the mood or feeling we wish to communicate to someone. We hear them ... and store them away for future use” [10]. None of these observations, however, serve as definitions, and indeed, we believe it desirable to — — — not pre-commit to an abstract definition, but rather to adopt an operational formulation based on external human judgments. In designing our study, we focus on a domain in which (i) there is rich use of language, some of which has achieved deep cultural penetration; (ii) there already exist a large number of external human judgments perhaps implicit, but in a form we can extract; and (iii) we can control for the setting in which the text was used. Specifically, we use the complete scripts of roughly 1000 movies, representing diverse genres, eras, and levels of popularity, and consider which lines are the most “memorable”. To acquire memorability labels, for each sentence in each script, we determine whether it has been listed as a “memorable quote” by users of the widely-known IMDb (the Internet Movie Database), and also estimate the number oftimes it appears on the Web. Both ofthese serve as memorability metrics for our purposes. When we evaluate properties of memorable quotes, we comparethemwithquotes thatarenotassessed as memorable, but were spoken by the same character, at approximately the same point in the same movie. This enables us to control in a fairly — fine-grained way for the confounding effects of context discussed above: we can observe differences 893 that persist even after taking into account both the speaker and the setting. In a pilot validation study, we find that human subjects are effective at recognizing the more IMDbmemorable of two quotes, even for movies they have not seen. This motivates a search for features intrinsic to the text of quotes that signal memorability. In fact, comments provided by the human subjects as part of the task suggested two basic forms that such textual signals could take: subjects felt that (i) memorable quotes often involve a distinctive turn of phrase; and (ii) memorable quotes tend to invoke general themes that aren’t tied to the specific setting they came from, and hence can be more easily invoked for future (out of context) uses. We test both of these principles in our analysis of the data. The present work (ii): What distinguishes memorable quotes Under the controlled-comparison setting sketched above, we find that memorable quotes exhibit significant differences from nonmemorable quotes in several fundamental respects, and these differences in the data reinforce the two main principles from the human pilot study. First, we show a concrete sense in which memorable quotes are indeed distinctive: with respect to lexical language models trained on the newswire portions of the Brown corpus [21], memorable quotes have significantly lower likelihood than their nonmemorable counterparts. Interestingly, this distinctiveness takes place at the level of words, but not at the level of other syntactic features: the part-ofspeech composition of memorable quotes is in fact more likely with respect to newswire. Thus, we can think of memorable quotes as consisting, in an aggregate sense, of unusual word choices built on a scaffolding of common part-of-speech patterns. We also identify a number of ways in which memorable quotes convey greater generality. In their patterns of verb tenses, personal pronouns, and determiners, memorable quotes are structured so as to be more “free-standing,” containing fewer markers that indicate references to nearby text. Memorable quotes differ in other interesting as- pects as well, such as sound distributions. Our analysis ofmemorable movie quotes suggests a framework by which the memorability of text in a range of different domains could be investigated. We provide evidence that such cross-domain properties may hold, guided by one of our motivating applications in marketing. In particular, we analyze a corpus of advertising slogans, and we show that these slogans have significantly greater likelihood at both the word level and the part-of-speech level with respect to a language model trained on memorable movie quotes, compared to a corresponding language model trained on non-memorable movie quotes. This suggests that some of the principles underlying memorable text have the potential to apply across different areas. Roadmap §2 lays the empirical foundations of our work: the design yasntdh ecerematpioirnic aofl our movie-quotes dataset, which we make publicly available (§2. 1), a pilot study cwhit hw ehu mmakaen subjects validating §I2M.1D),b abased memorability labels (§2.2), and further study bofa incorporating search-engine c2)o,u anntds (§2.3). §3 uddeytoafi lisn our analysis aenardc prediction experiments, using both movie-quotes data and, as an exploration of cross-domain applicability, slogans data. §4 surveys rcerloastse-dd owmoarkin across a variety goafn fsie dladtsa.. §5 briefly sruelmatmedar wizoesrk ka andcr ionsdsic aat veasr some ffuft uierled sd.ire §c5tio bnrsie. 2 I’m ready for my close-up. 2.1 Data To study the properties of memorable movie quotes, we need a source of movie lines and a designation of memorability. Following [8], we constructed a corpus consisting of all lines from roughly 1000 movies, varying in genre, era, and popularity; for each movie, we then extracted the list of quotes from IMDb’s Memorable Quotes page corresponding to the movie.1 A memorable quote in IMDb can appear either as an individual sentence spoken by one character, or as a multi-sentence line, or as a block of dialogue involving multiple characters. In the latter two cases, it can be hard to determine which particular portion is viewed as memorable (some involve a build-up to a punch line; others involve the follow-through after a well-phrased opening sentence), and so we focus in our comparisons on those memorable quotes that 1This extraction involved some edit-distance-based alignment, since the exact form of the line in the script can exhibit minor differences from the version typed into IMDb. rmotuqsfebmaNerolbm543281760 0 1234D5ecil678910 894 Figure 1: Location of memorable quotes in each decile of movie scripts (the first 10th, the second 10th, etc.), summed over all movies. The same qualitative results hold if we discard each movie’s very first and last line, which might have privileged status. appear as a single sentence rather than a multi-line block.2 We now formulate a task that we can use to evaluate the features of memorable quotes. Recall that our goal is to identify effects based in the language of the quotes themselves, beyond any factors arising from the speaker or context. Thus, for each (singlesentence) memorable quote M, we identify a nonmemorable quote that is as similar as possible to M in all characteristics but the choice of words. This means we want it to be spoken by the same character in the same movie. It also means that we want it to have the same length: controlling for length is important because we expect that on average, shorter quotes will be easier to remember than long quotes, and that wouldn’t be an interesting textual effect to report. Moreover, we also want to control for the fact that a quote’s position in a movie can affect memorability: certain scenes produce more memorable dialogue, and as Figure 1 demonstrates, in aggregate memorable quotes also occur disproportionately near the beginnings and especially the ends of movies. In summary, then, for each M, we pick a contrasting (single-sentence) quote N from the same movie that is as close in the script as possible to M (either before or after it), subject to the conditions that (i) M and N are uttered by the same speaker, (ii) M and N have the same number of words, and (iii) N does not occur in the IMDb list of memorable 2We also ran experiments relaxing the single-sentence assumption, which allows for stricter scene control and a larger dataset but complicates comparisons involving syntax. The non-syntax results were in line with those reported here. TaJSOMbtrclodekviTn1ra:eBTykhoPrwNenpmlxeasipFIHAeaithrclsfnitkaQeomuifltw’sdaveoitycmsnedoqatbuliocrkeytsl f.woEeimlanchguwspakyirdfsebavot;ilmsdfcoenti’dus.erx-citaINmSnrkeioamct:ohenwmardleytQ.howfeu t’yvrecp,o’gsmrtpuaosnmtyef o rtgnhqieuvrobt.pehasirtdeosfpykuern close together in the movie by the same while the other is not. (Contractions character, have the same length, and one is labeled memorable by the IMDb such as “it’s” count as two words.) quotes for the movie (either as a single line or as part of a larger block). Given such pairs, we formulate a pairwise comparison task: given M and N, determine which is the memorable quote. Psychological research on subjective evaluation [35], as well as initial experiments using ourselves as subjects, indicated that this pairwise set-up easier to work with than simply presenting a single sentence and asking whether it is memorable or not; the latter requires agreement on an “absolute” criterion for memorability that is very hard to impose consistently, whereas the former simply requires a judgment that one quote is more memorable than another. Our main dataset, available at http://www.cs. cornell.edu/∼cristian/memorability.html,3 thus consists of approximately 2200 such (M, N) pairs, separated by a median of 5 same-character lines in the script. The reader can get a sense for the nature of the data from the three examples in Table 1. We now discuss two further aspects to the formulation of the experiment: a preliminary pilot study involving human subjects, and the incorporation of search engine counts into the data. 2.2 Pilot study: Human performance As a preliminary consideration, we did a small pilot study to see if humans can distinguish memorable from non-memorable quotes, assuming our IMDBinduced labels as gold standard. Six subjects, all native speakers of English and none an author of this paper, were presented with 11 or 12 pairs of memorable vs. non-memorable quotes; again, we controlled for extra-textual effects by ensuring that in each pair the two quotes come from the same movie, are by the same character, have the same length, and 3Also available there: other examples and factoids. 895 Table 2: Human pilot study: number of matches to IMDb-induced annotation, ordered by decreasing match percentage. For the null hypothesis of random guessing, these results are statistically significant, p < 2−6 ≈ .016. appear as nearly as possible in the same scene.4 The order of quotes within pairs was randomized. Importantly, because we wanted to understand whether the language of the quotes by itself contains signals about memorability, we chose quotes from movies that the subjects said they had not seen. (This means that each subject saw a different set of quotes.) Moreover, the subjects were requested not to consult any external sources of information.5 The reader is welcome to try a demo version of the task at http: //www.cs.cornell.edu/∼cristian/memorability.html. Table 2 shows that all the subjects performed (sometimes much) better than chance, and against the null hypothesis that all subjects are guessing randomly, the results are statistically significant, p < 2−6 ≈ .016. These preliminary findings provide evidenc≈e f.0or1 t6h.e T validity eolifm our traysk fi:n despite trohev apparent difficulty of the job, even humans who haven’t seen the movie in question can recover our IMDb4In this pilot study, we allowed multi-sentence quotes. 5We did not use crowd-sourcing because we saw no way to ensure that this condition would be obeyed by arbitrary subjects. We do note, though, that after our research was completed and as of Apr. 26, 2012, ≈ 11,300 people completed the online test: average accuracy: 27,2 ≈%, 1 1m,3o0d0e npueompbleer c coomrrpelcett:e d9 t/1he2. induced labels with some reliability.6 2.3 Incorporating search engine counts Thus far we have discussed a dataset in which memorability is determined through an explicit labeling drawn from the IMDb. Given the “production” aspect of memorability discussed in § 1, we stihoonu”ld a saplesoc expect tmhaotr mabeimlityora dbislce quotes nw §il1l ,te wnde to appear more extensively on Web pages than nonmemorable quotes; note that incorporating this insight makes it possible to use the (implicit) judgments of a much larger number of people than are represented by the IMDb database. It therefore makes sense to try using search-engine result counts as a second indication of memorability. We experimented with several ways of constructing memorability information from search-engine counts, but this proved challenging. Searching for a quote as a stand-alone phrase runs into the problem that a number of quotes are also sentences that people use without the movie in mind, and so high counts for such quotes do not testify to the phrase’s status as a memorable quote from the movie. On the other hand, searching for the quote in a Boolean conjunction with the movie’s title discards most of these uses, but also eliminates a large fraction of the appearances on the Web that we want to find: precisely because memorable quotes tend to have widespread cultural usage, people generally don’t feel the need to include the movie’s title when invoking them. Finally, since we are dealing with roughly 1000 movies, the result counts vary over an enormous range, from recent blockbusters to movies with relatively small fan bases. In the end, we found that it was more effective to use the result counts in conjunction with the IMDb labels, so that the counts played the role of an additional filter rather than a free-standing numerical value. Thus, for each pair (M, N) produced using the IMDb methodology above, we searched for each of M and N as quoted expressions in a Boolean conjunction with the title of the movie. We then kept only those pairs for which M (i) produced more than five results in our (quoted, conjoined) search, and (ii) produced at least twice as many results as the cor6The average accuracy being below 100% reinforces that context is very important, too. 896 responding search for N. We created a version of this filtered dataset using each of Google and Bing, and all the main findings were consistent with the results on the IMDb-only dataset. Thus, in what follows, we will focus on the main IMDb-only dataset, discussing the relationship to the dataset filtered by search engine counts where relevant (in which case we will refer to the +Google dataset). 3 Never send a human to do a machine’s job. We now discuss experiments that investigate the hypotheses discussed in §1. In particular, we devise pmoetthheosdess t dhiastc can assess 1th.e Idnis ptianrcttiicvuelnaer,ss w aend d generality hypotheses and test whether there exists a notion of “memorable language” that operates across domains. In addition, we evaluate and compare the predictive power of these hypotheses. 3.1 Distinctiveness One of the hypotheses we examine is whether the use of language in memorable quotes is to some extent unusual. In order to quantify the level of distinctiveness of a quote, we take a language-model approach: we model “common language” using the newswire sections of the Brown corpus [21]7, and evaluate how distinctive a quote is by evaluating its likelihood with respect to this model the lower the likelihood, the more distinctive. In order to assess different levels of lexical and syntactic distinctiveness, we employ a total of six Laplacesmoothed8 language models: 1-gram, 2-gram, and — 3-gram word LMs and 1-gram, 2-gram and 3-gram LMs. We find strong evidence that from a lexical perspective, memorable quotes are more distinctive than their non-memorable counterparts. As indicated in Table 3, for each of our lexical “common language” models, in about 60% of the quote pairs, the memorable quote is more distinctive. Interestingly, the reverse is true when it comes to part-of-speech9 7Results were qualitatively similar if we used the fiction portions. The age of the Brown corpus makes it less likely to contain modern movie quotes. 8We employ Laplace (additive) smoothing with a smoothing parameter of 0.2. The language models’ vocabulary was that of the entire training corpus. 9Throughout we obtain part-of-speech tags by using the NLTK maximum entropy tagger with default parameters. in which the the memorable quote is more distinctive than the non-memorable one according to the respective “common language” model. Significance according to a two-tailed sign test is indicated using *-notation (∗∗∗=“p<.001”). syntax: memorable quotes appear to follow the syntactic patterns of “common language” as closely as or more closely than non-memorable quotes. Together, these results suggest that memorable quotes consist of unusual word sequences built on common syntactic scaffolding. 3.2 Generality Another of our hypotheses is that memorable quotes are easier to use outside the specific context in which they were uttered that is, more “portable” and therefore exhibit fewer terms that refer to those settings. We use the following syntactic properties as proxies for the generality of a quote: • Fewer 3rd-person pronouns, since these commonly r 3efer to a person or object that was introduced earlier in the discourse. Utterances that employ fewer such pronouns are easier to adapt to new contexts, and so will be considered more — — general. • More indefinite articles like a and an, since they are more likely ttioc lreesfer li ktoe general concepts than definite articles. Quotes with more indefinite articles will be considered more general. Fewer past tense verbs and more present tFeenwsee verbs, tseinncsee t vheer bfosrm aenrd are more likely to refer to specific previous events. Therefore utterances that employ fewer past tense verbs (and more present tense verbs) will be considered more general. Table 4 gives the results for each of these four metrics in each case, we show the percentage of • — 897 TalfmGebowsnre4pa:in3srGldet sypfne.msrate.lripnctysoe: purncsetaI56gM47e.326D9o710bf% -qo∗u n∗l tyepa+56iG892rs.o7i364ng% wl∗ eh∗i ch the memorable quote is more general than the non- memorable ones according to the respective metric. Pairs where the metric does not distinguish between the quotes are not considered. quote pairs for which the memorable quote scores better on the generality metric. Note that because the issue of generality is a complex one for which there is no straightforward single metric, our approach here is based on several proxies for generality, considered independently; yet, as the results show, all of these point in a consistent direction. It is an interesting open question to develop richer ways of assessing whether a quote has greater generality, in the sense that people intuitively attribute to memorable quotes. 3.3 “Memorable” language beyond movies One of the motivating questions in our analysis is whether there are general principles underlying “memorable language.” The results thus far suggest potential families of such principles. A further question in this direction is whether the notion of memorability can be extended across different domains, and for this we collected (and distribute on our website) 431 phrases that were explicitly designed to be memorable: advertising slogans (e.g., “Quality never goes out of style.”). The focus on slogans is also in keeping with one of the initial motivations in studying memorability, namely, marketing applications in other words, assessing whether a proposed slogan has features that are consistent with memorable text. The fact that it’s not clear how to construct a collection of “non-memorable” counterparts to slogans appears to pose a technical challenge. However, we can still use a language-modeling approach to assess whether the textual properties of the slogans are closer to the memorable movie quotes (as one would conjecture) or to the non-memorable movie quotes. Specifically, we train one language model on memorable quotes and another on non-memorable quotes — guage: percentage of slogans that have higher likelihood under the memorable language model than under the nonmemorable one (for each of the six language models considered). Rightmost column: for reference, the percentage of newswire sentences that have higher likelihood under the memorable language model than under the nonmemorable one. TaG% ble3nipared6stpa:lfeitrnSsyilto.megpareotnsicluaerns mo1s42lto.61g048ae% nseral2w1m.h16e3mn% .comn2p-63ma.0r46e19dm% .to memorable and non-memorable quotes. (%s of 3rd pers. pronouns and indefinite articles are relative to all tokens, %s of past tense are relative to all past and present verbs.) and compare how likely each slogan is to be produced according to these two models. As shown in the middle column of Table 5, we find that slogans are better predicted both lexically and syntactically by the former model. This result thus offers evidence for a concept of “memorable language” that can be applied beyond a single domain. We also note that the higher likelihood of slogans under a “memorable language” model is not simply occurring for the trivial reason that this model predicts all other large bodies of text better. In particular, the newswire section of the Brown corpus is predicted better at the lexical level by the language model trained on non-memorable quotes. Finally, Table 6 shows that slogans employ general language, in the sense that for each of our generality metrics, we see a slogans/memorablequotes/non-memorable quotes spectrum. 3.4 Prediction task We now show how the principles discussed above can provide features for a basic prediction task, corresponding to the task in our human pilot study: 898 given a pair of quotes, identify the memorable one. Our first formulation of the prediction task uses a standard bag-of-words model10. If there were no information in the textual content of a quote to determine whether it were memorable, then an SVM employing bag-of-words features should perform no better than chance. Instead, though, it obtains 59.67% (10-fold cross-validation) accuracy, as shown in Table 7. We then develop models using features based on the measures formulated earlier in this section: generality measures (the four listed in Table 4); distinctiveness measures (likelihood according to 1, 2, and 3-gram “common language” models at the lexical and part-of-speech level for each quote in the pair, their differences, and pairwise comparisons between them); and similarityto-slogans measures (likelihood according to 1, 2, and 3-gram slogan-language models at the lexical and part-of-speech level for each quote in the pair, their differences, and pairwise comparisons between them). Even a relatively small number of distinctiveness features, on their own, improve significantly over the much larger bag-of-words model. When we include additional features based on generality and language-model features measuring similarity to slogans, the performance improves further (last line of Table 7). Thus, the main conclusion from these prediction tasks is that abstracting notions such as distinctiveness and generality can produce relatively streamlined models that outperform much heavier-weight bag-of-words models, and can suggest steps toward approaching the performance of human judges who very much unlike our system have the full cultural context in which movies occur at their disposal. — — 3.5 Other characteristics We also made some auxiliary observations that may be ofinterest. Specifically, we find differences in letter and sound distribution (e.g., memorable quotes after curse-word removal use significantly more “front sounds” (labials or front vowels such as represented by the letter i) and significantly fewer “back sounds” such as the one represented by u),11 — — 10We discarded terms appearing fewer than 10 times. 11These findings may relate to marketing research on sound symbolism [7, 19, 40]. TablesdgF7lieao:sngtPiehnorauefc dtliswevctymeo irnp.des:StoVgeMh10r-fo#ldec9ra265ot42sv5aA6l8942ic.d36720atu57%ri aocn∗yresult using the respective feature sets. Random baseline accuracy is 50%. Accuracies statistically significantly greater than bag-of-words according to a two-tailed t-test are indicated with *(p<.05) and **(p<.01). word complexity (e.g., memorable quotes use words with significantly more syllables) and phrase complexity (e.g., memorable quotes use fewer coordinating conjunctions). The latter two are in line with our distinctiveness hypothesis. 4 A long time ago, in a galaxy far, far away How an item’s linguistic form affects the reaction it generates has been studied in several contexts, including evaluations of product reviews [9], political speeches [12], on-line posts [13], scientific papers [14], and retweeting of Twitter posts [36]. We use a different set of features, abstracting the notions of distinctiveness and generality, in order to focus on these higher-level aspects of phrasing rather than on particular lower-level features. Related to our interest in distinctiveness, work in advertising research has studied the effect of syntactic complexity on recognition and recall of slogans [5, 6, 24]. There may also be connections to Von Restorff’s isolation effect Hunt [17], which asserts that when all but one item in a list are similar in some way, memory for the different item is enhanced. Related to our interest in generality, Knapp et al. [20] surveyed subjects regarding memorable messages or pieces of advice they had received, finding that the ability to be applied to multiple concrete situations was an important factor. Memorability, although distinct from “memorizability”, relates to short- and long-term recall. Thorn and Page [34] survey sub-lexical, lexical, and semantic attributes affecting short-term memorability of lexical items. Studies of verbatim recall have also considered the task of distinguishing an exact quote from close paraphrases [3]. Investigations of longterm recall have included studies ofculturally signif- 899 icant passages of text [29] and findings regarding the effect of rhetorical devices of alliterative [4], “rhythmic, poetic, and thematic constraints” [18, 26]. Finally, there are complex connections between humor and memory [32], which may lead to interactions with computational humor recognition [25]. 5 I think this is the beginning of a beautiful friendship. Motivated by the broad question of what kinds of information achieve widespread public awareness, we studied the the effect of phrasing on a quote’s memorability. A challenge is that quotes differ not only in how they are worded, but also in who said them and under what circumstances; to deal with this difficulty, we constructed a controlled corpus of movie quotes in which lines deemed memorable are paired with non-memorable lines spoken by the same character at approximately the same point in the same movie. After controlling for context and situation, memorable quotes were still found to exhibit, on av- erage (there will always be individual exceptions), significant differences from non-memorable quotes in several important respects, including measures capturing distinctiveness and generality. Our experiments with slogans show how the principles we identify can extend to a different domain. Future work may lead to applications in marketing, advertising and education [4]. Moreover, the subtle nature of memorability, and its connection to research in psychology, suggests a range of further research directions. We believe that the framework developed here can serve as the basis for further computational studies of the process by which information takes hold in the public consciousness, and the role that language effects play in this process. My mother thanks you. My father thanks you. My sister thanks you. And Ithank you: Rebecca Hwa, Evie Kleinberg, Diana Minculescu, Alex Niculescu-Mizil, Jennifer Smith, Benjamin Zimmer, and the anonymous reviewers for helpful discussions and comments; our annotators Steven An, Lars Backstrom, Eric Baumer, Jeff Chadwick, Evie Kleinberg, and Myle Ott; and the makers of Cepacol, Robitussin, and Sudafed, whose products got us through the submission deadline. This paper is based upon work supported in part by NSF grants IIS-0910664, IIS-1016099, Google, and Yahoo! References [1] [2] [3] [4] [5] Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M. Lukose. Implicit structure and the dynamics of blogspace. In Workshop on the Weblogging Ecosystem, 2004. Lars Backstrom, Dan Huttenlocher, Jon Kleinberg, and Xiangyang Lan. Group formation in large social networks: Membership, growth, and evolution. In Proceedings of KDD, 2006. Elizabeth Bates, Walter Kintsch, Charles R. Fletcher, and Vittoria Giuliani. The role of pronominalization and ellipsis in texts: Some memory experiments. Journal of Experimental Psychology: Human Learning and Memory, 6 (6):676–691, 1980. Frank Boers and Seth Lindstromberg. Finding ways to make phrase-learning feasible: The mnemonic effect of alliteration. System, 33(2): 225–238, 2005. Samuel D. Bradley and Robert Meeds. Surface-structure transformations and advertising slogans: The case for moderate syntactic complexity. Psychology and Marketing, 19: 595–619, 2002. [6] Robert Chamblee, Robert Gilmore, Gloria Thomas, and Gary Soldow. When copy complexity can help ad readership. Journal of Advertising Research, 33(3):23–23, 1993. [7] John Colapinto. Famous names. The New Yorker, pages 38–43, 2011. [8] Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, 2011. [9] Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinberg, and Lillian Lee. How opinions are received by online communities: A case study on Amazon.com helpfulness votes. In Proceedings of WWW, pages 141–150, 2009. [10] Stuart Fischoff, Esmeralda Cardenas, Angela Hernandez, Korey Wyatt, Jared Young, and 900 [11] [12] [13] [14] [15] Rachel Gordon. Popular movie quotes: Reflections of a people and a culture. In Annual Convention of the American Psychological Association, 2000. Daniel Gruhl, R. Guha, David Liben-Nowell, and Andrew Tomkins. Information diffusion through blogspace. Proceedings of WWW, pages 491–501, 2004. Marco Guerini, Carlo Strapparava, and Oliviero Stock. Trusting politicians’ words (for persuasive NLP). In Proceedings of CICLing, pages 263–274, 2008. Marco Guerini, Carlo Strapparava, and G o¨zde O¨zbal. Exploring text virality in social networks. In Proceedings of ICWSM (poster), 2011. Marco Guerini, Alberto Pepe, and Bruno Lepri. Do linguistic style and readability of scientific abstracts affect their virality? In Proceedings of ICWSM, 2012. Richard Jackson Harris, Abigail J. Werth, Kyle E. Bures, and Chelsea M. Bartel. Social movie quoting: What, why, and how? Ciencias Psicologicas, 2(1):35–45, 2008. [16] Chip Heath, Chris Bell, and Emily Steinberg. Emotional selection in memes: The case of urban legends. Journal of Personality, 81(6): 1028–1041, 2001. [17] R. Reed Hunt. The subtlety of distinctiveness: What von Restorff really did. Psychonomic Bulletin & Review, 2(1): 105–1 12, 1995. [18] Ira E. Hyman Jr. and David C. Rubin. Memorabeatlia: A naturalistic study of long-term memory. Memory & Cognition, 18(2):205– 214, 1990. [19] Richard R. Klink. Creating brand names with meaning: The use of sound symbolism. Marketing Letters, 11(1):5–20, 2000. [20] Mark L. Knapp, Cynthia Stohl, and Kathleen K. Reardon. “Memorable” messages. Journal of Communication, 3 1(4):27– 41, 1981. [21] Henry Kuˇ cera and W. Nelson Francis. Computational analysis of present-day American English. Dartmouth Publishing Group, 1967. [22] Jure Leskovec, Lada Adamic, and Bernardo Huberman. The dynamics of viral marketing. ACM Transactions on the Web, 1(1), May [23] [24] [25] [26] [27] [28] [29] 2007. Jure Leskovec, Lars Backstrom, and Jon Kleinberg. Meme-tracking and the dynamics of the news cycle. In Proceedings of KDD, pages 497–506, 2009. Tina M. Lowrey. The relation between script complexity and commercial memorability. Journal of Advertising, 35(3):7–15, 2006. Rada Mihalcea and Carlo Strapparava. Learning to laugh (automatically): Computational models for humor recognition. Computational Intelligence, 22(2): 126–142, 2006. Milman Parry and Adam Parry. The making of Homeric verse: The collected papers of Milman Parry. Clarendon Press, Oxford, 1971. Everett Rogers. Diffusion of Innovations. Free Press, fourth edition, 1995. Daniel M. Romero, Brendan Meeder, and Jon Kleinberg. Differences in the mechanics of information diffusion across topics: Idioms, political hashtags, and complex contagion on Twitter. Proceedings of WWW, pages 695–704, 2011. David C. Rubin. Very long-term memory for [30] [3 1] [32] [33] prose and verse. Journal of Verbal Learning and Verbal Behavior, 16(5):61 1–621, 1977. Nathan Schneider, Rebecca Hwa, Philip Gianfortoni, Dipanjan Das, Michael Heilman, Alan W. Black, Frederick L. Crabbe, and Noah A. Smith. Visualizing topical quotations over time to understand news discourse. Technical Report CMU-LTI-01-103, CMU, 2010. David Strang and Sarah Soule. Diffusion in organizations and social movements: From hybrid corn to poison pills. Annual Review of Sociology, 24:265–290, 1998. Hannah Summerfelt, Louis Lippman, and Ira E. Hyman Jr. The effect of humor on memory: Constrained by the pun. The Journal of General Psychology, 137(4), 2010. Eric Sun, Itamar Rosenn, Cameron Marlow, and Thomas M. Lento. Gesundheit! Model- 901 ing contagion through Facebook News Feed. In Proceedings of ICWSM, 2009. [34] Annabel Thorn and Mike Page. Interactions Between Short-Term and Long-Term Memory [35] [36] [37] [38] [39] [40] in the Verbal Domain. Psychology Press, 2009. Louis L. Thurstone. A law of comparative judgment. Psychological Review, 34(4):273– 286, 1927. Oren Tsur and Ari Rappoport. What’s in a Hashtag? Content based prediction of the spread of ideas in microblogging communities. In Proceedings of WSDM, 2012. Fang Wu, Bernardo A. Huberman, Lada A. Adamic, and Joshua R. Tyler. Information flow in social groups. Physica A: Statistical and Theoretical Physics, 337(1-2):327–335, 2004. Shaomei Wu, Jake M. Hofman, Winter A. Mason, and Duncan J. Watts. Who says what to whom on Twitter. In Proceedings of WWW, 2011. Jaewon Yang and Jure Leskovec. Patterns of temporal variation in online media. In Proceedings of WSDM, 2011. Eric Yorkston and Geeta Menon. A sound idea: Phonetic effects of brand names on consumer judgments. Journal of Consumer Research, 3 1 (1):43–51, 2004.</p><p>5 0.53595406 <a title="7-lsi-5" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>Author: Shomir Wilson</p><p>Abstract: Metalanguage is an essential linguistic mechanism which allows us to communicate explicit information about language itself. However, it has been underexamined in research in language technologies, to the detriment of the performance of systems that could exploit it. This paper describes the creation of the first tagged and delineated corpus of English metalanguage, accompanied by an explicit definition and a rubric for identifying the phenomenon in text. This resource will provide a basis for further studies of metalanguage and enable its utilization in language technologies.</p><p>6 0.43406004 <a title="7-lsi-6" href="./acl-2012-Tokenization%3A_Returning_to_a_Long_Solved_Problem__A_Survey%2C_Contrastive_Experiment%2C_Recommendations%2C_and_Toolkit_.html">197 acl-2012-Tokenization: Returning to a Long Solved Problem  A Survey, Contrastive Experiment, Recommendations, and Toolkit </a></p>
<p>7 0.4298732 <a title="7-lsi-7" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>8 0.421875 <a title="7-lsi-8" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>9 0.41166148 <a title="7-lsi-9" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>10 0.40667769 <a title="7-lsi-10" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>11 0.40565231 <a title="7-lsi-11" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>12 0.40122345 <a title="7-lsi-12" href="./acl-2012-Applications_of_GPC_Rules_and_Character_Structures_in_Games_for_Learning_Chinese_Characters.html">26 acl-2012-Applications of GPC Rules and Character Structures in Games for Learning Chinese Characters</a></p>
<p>13 0.34818321 <a title="7-lsi-13" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>14 0.33812049 <a title="7-lsi-14" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>15 0.29546034 <a title="7-lsi-15" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<p>16 0.29295066 <a title="7-lsi-16" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>17 0.29181385 <a title="7-lsi-17" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>18 0.28318444 <a title="7-lsi-18" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>19 0.26462945 <a title="7-lsi-19" href="./acl-2012-Multilingual_WSD_with_Just_a_Few_Lines_of_Code%3A_the_BabelNet_API.html">152 acl-2012-Multilingual WSD with Just a Few Lines of Code: the BabelNet API</a></p>
<p>20 0.26224887 <a title="7-lsi-20" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.024), (26, 0.032), (28, 0.027), (30, 0.015), (34, 0.016), (37, 0.018), (39, 0.521), (74, 0.015), (82, 0.014), (84, 0.019), (85, 0.032), (90, 0.081), (92, 0.043), (99, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96510965 <a title="7-lda-1" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>Author: Wen-Tai Hsieh ; Chen-Ming Wu ; Tsun Ku ; Seng-cho T. Chou</p><p>Abstract: Social Event Radar is a new social networking-based service platform, that aim to alert as well as monitor any merchandise flaws, food-safety related issues, unexpected eruption of diseases or campaign issues towards to the Government, enterprises of any kind or election parties, through keyword expansion detection module, using bilingual sentiment opinion analysis tool kit to conclude the specific event social dashboard and deliver the outcome helping authorities to plan “risk control” strategy. With the rapid development of social network, people can now easily publish their opinions on the Internet. On the other hand, people can also obtain various opinions from others in a few seconds even though they do not know each other. A typical approach to obtain required information is to use a search engine with some relevant keywords. We thus take the social media and forum as our major data source and aim at collecting specific issues efficiently and effectively in this work. 163 Chen-Ming Wu Institute for Information Industry cmwu@ i i i .org .tw Seng-cho T. Chou Department of IM, National Taiwan University chou @ im .ntu .edu .tw 1</p><p>2 0.91681659 <a title="7-lda-2" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>Author: Karin Mauge ; Khash Rohanimanesh ; Jean-David Ruvini</p><p>Abstract: Large e-commerce enterprises feature millions of items entered daily by a large variety of sellers. While some sellers provide rich, structured descriptions of their items, a vast majority of them provide unstructured natural language descriptions. In the paper we present a 2 steps method for structuring items into descriptive properties. The first step consists in unsupervised property discovery and extraction. The second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm. We evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall.</p><p>same-paper 3 0.9088583 <a title="7-lda-3" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>Author: Gozde Ozbal ; Carlo Strapparava</p><p>Abstract: In this paper, we propose a computational approach to generate neologisms consisting of homophonic puns and metaphors based on the category of the service to be named and the properties to be underlined. We describe all the linguistic resources and natural language processing techniques that we have exploited for this task. Then, we analyze the performance of the system that we have developed. The empirical results show that our approach is generally effective and it constitutes a solid starting point for the automation ofthe naming process.</p><p>4 0.88364762 <a title="7-lda-4" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>Author: Sindhu Raghavan ; Raymond Mooney ; Hyeonseo Ku</p><p>Abstract: Most information extraction (IE) systems identify facts that are explicitly stated in text. However, in natural language, some facts are implicit, and identifying them requires “reading between the lines”. Human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts. We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining firstorder logic and Bayesian networks, to infer additional implicit information from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach.</p><p>5 0.88104963 <a title="7-lda-5" href="./acl-2012-Efficient_Tree-Based_Topic_Modeling.html">79 acl-2012-Efficient Tree-Based Topic Modeling</a></p>
<p>Author: Yuening Hu ; Jordan Boyd-Graber</p><p>Abstract: Topic modeling with a tree-based prior has been used for a variety of applications because it can encode correlations between words that traditional topic modeling cannot. However, its expressive power comes at the cost of more complicated inference. We extend the SPARSELDA (Yao et al., 2009) inference scheme for latent Dirichlet allocation (LDA) to tree-based topic models. This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments. We further improve performance by iteratively refining the sampling distribution only when needed. Experiments show that the proposed techniques dramatically improve the computation time.</p><p>6 0.58141887 <a title="7-lda-6" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>7 0.54369611 <a title="7-lda-7" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>8 0.51381814 <a title="7-lda-8" href="./acl-2012-Multilingual_Subjectivity_and_Sentiment_Analysis.html">151 acl-2012-Multilingual Subjectivity and Sentiment Analysis</a></p>
<p>9 0.51231807 <a title="7-lda-9" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>10 0.50593561 <a title="7-lda-10" href="./acl-2012-Topic_Models%2C_Latent_Space_Models%2C_Sparse_Coding%2C_and_All_That%3A_A_Systematic_Understanding_of_Probabilistic_Semantic_Extraction_in_Large_Corpus.html">198 acl-2012-Topic Models, Latent Space Models, Sparse Coding, and All That: A Systematic Understanding of Probabilistic Semantic Extraction in Large Corpus</a></p>
<p>11 0.50356513 <a title="7-lda-11" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>12 0.50104666 <a title="7-lda-12" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>13 0.49015775 <a title="7-lda-13" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>14 0.48322275 <a title="7-lda-14" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>15 0.48042411 <a title="7-lda-15" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>16 0.47690228 <a title="7-lda-16" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>17 0.47583735 <a title="7-lda-17" href="./acl-2012-Demonstration_of_IlluMe%3A_Creating_Ambient_According_to_Instant_Message_Logs.html">70 acl-2012-Demonstration of IlluMe: Creating Ambient According to Instant Message Logs</a></p>
<p>18 0.47583458 <a title="7-lda-18" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>19 0.46840161 <a title="7-lda-19" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>20 0.46587878 <a title="7-lda-20" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
