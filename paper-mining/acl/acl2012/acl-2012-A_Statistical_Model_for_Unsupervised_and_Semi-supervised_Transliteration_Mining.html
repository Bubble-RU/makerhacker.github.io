<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-20" href="#">acl2012-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</h1>
<br/><p>Source: <a title="acl-2012-20-pdf" href="http://aclweb.org/anthology//P/P12/P12-1049.pdf">pdf</a></p><p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>Reference: <a title="acl-2012-20-reference" href="../acl2012_reference/acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  ,  Abstract We propose a novel model to automatically extract transliteration pairs from parallel corpora. [sent-3, score-0.926]
</p><p>2 Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. [sent-4, score-1.073]
</p><p>3 We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. [sent-5, score-1.805]
</p><p>4 1 Introduction Transliteration mining is the extraction of transliteration pairs from unlabelled data. [sent-7, score-1.178]
</p><p>5 Most transliteration mining systems are built using labelled training data or using heuristics to extract transliteration pairs. [sent-8, score-2.039]
</p><p>6 Our sys-  tem extracts transliteration pairs in an unsupervised fashion. [sent-10, score-1.049]
</p><p>7 We present a novel model of transliteration mining defined as a mixture of a transliteration model and a non-transliteration model. [sent-12, score-1.775]
</p><p>8 The transliteration model is a joint source channel model (Li et al. [sent-13, score-0.836]
</p><p>9 At test time, we label word 469 pairs as transliterations if they have a higher probability assigned by the transliteration sub-model than by the non-transliteration sub-model. [sent-17, score-1.085]
</p><p>10 The S-step takes the probability estimates from unlabelled data (computed in the Mstep) and uses them as a backoff distribution to  smooth probabilities which were estimated from labelled data. [sent-19, score-0.468]
</p><p>11 We evaluate our unsupervised and semisupervised transliteration mining system on the datasets available from the NEWS 2010 shared task on transliteration mining (Kumaran et al. [sent-22, score-2.19]
</p><p>12 Compared with a baseline unsupervised system our unsupervised system achieves up to 5% better F-measure. [sent-25, score-0.416]
</p><p>13 Additional experiments on parallel corpora show that we are able to effectively mine transliteration pairs from very noisy data. [sent-31, score-0.955]
</p><p>14 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi4c 6s9–47 , 2  Previous Work  We first discuss the literature on semi-supervised and supervised techniques for transliteration mining and then describe a previously defined unsupervised system. [sent-39, score-1.139]
</p><p>15 Supervised and semi-supervised systems use a manually labelled set of training data to learn character mappings between source and target strings. [sent-40, score-0.358]
</p><p>16 The labelled training data either consists of a few hundred transliteration pairs or of just a few carefully selected transliteration pairs. [sent-41, score-1.922]
</p><p>17 The NEWS 2010 shared task on transliteration mining (NEWS10) (Kumaran et al. [sent-42, score-0.987]
</p><p>18 Our transliteration mining model can mine transliterations without using any labelled  data. [sent-48, score-1.375]
</p><p>19 The transliteration mining systems evaluated on the NEWS10 dataset generally used heuristic methods, discriminative models or generative models for transliteration mining (Kumaran et al. [sent-50, score-1.961]
</p><p>20 They presented two discriminative methods an SVM-based classifier and alignment-based string similarity for transliteration mining. [sent-54, score-0.81]
</p><p>21 We propose a flexible generative model for transliteration mining usable for both unsupervised and semi-supervised learning. [sent-56, score-1.107]
</p><p>22 Our method is different from theirs as our generative story explains the unlabelled data using a combination of a transliteration –  and a non-transliteration sub-model. [sent-60, score-0.957]
</p><p>23 The transliteration model jointly generates source and target 470 strings, whereas the non-transliteration system generates them independently of each other. [sent-61, score-0.921]
</p><p>24 (201 1) proposed a heuristic-based unsupervised transliteration mining system. [sent-63, score-1.107]
</p><p>25 It is the only unsupervised mining system that was evaluated on the NEWS10 dataset up until now, as far as we know. [sent-65, score-0.383]
</p><p>26 In this paper, we propose a novel model-based approach to transliteration mining. [sent-68, score-0.81]
</p><p>27 Unlike the previous unsupervised system, and unlike the supervised and semi-supervised systems we mentioned, our model can be used for both unsupervised and semi-supervised mining in a consistent way. [sent-70, score-0.471]
</p><p>28 The joint transliteration probability p1(e, f) of a word pair is the sum of the probabilities of all alignment sequences:  p1(e,f)  =  X p(a) a∈AXlign(e,f)  (1)  Transliteration systems are trained on a list of transliteration pairs. [sent-74, score-1.862]
</p><p>29 The alignment between the transliteration pairs is learned with Expectation Maximization (EM). [sent-75, score-0.919]
</p><p>30 We use a simple unigram model, so an alignment sequence from function Align(e, f) is a combination of 0–1, 1–1, and 1– 0 character alignments between a source word e and its transliteration f. [sent-76, score-1.016]
</p><p>31 ,q|a|) =Y|a|p(qj)  (2)  Yj=1  While transliteration systems are trained on a clean list of transliteration pairs, our transliteration mining system has to learn from data containing both transliterations and non-transliterations. [sent-82, score-2.868]
</p><p>32 The transliteration model p1(e, f) handles only the transliteration pairs. [sent-83, score-1.62]
</p><p>33 Interpolation with the non-transliteration model allows the transliteration model to concentrate on modelling transliterations during EM training. [sent-85, score-0.955]
</p><p>34 After EM training, transliteration word pairs are assigned a high probability by the transliteration submodel and a low probability by the non-transliteration submodel, and vice versa for non-transliteration pairs. [sent-86, score-1.818]
</p><p>35 p2(e, f)  The traQnsliteration mining modelQ Qis an interpolation of the transliteration model p1(e, f) and the non-transliteration model p2 (e, f) : p(e, f) = (1 λ)p1 (e, f) + λp2 (e, f) (4) −  λ is the prior probability of non-transliteration. [sent-92, score-1.036]
</p><p>36 1 Model Estimation In this section, we discuss the estimation of the parameters ofthe transliteration model p1(e, f) and the non-transliteration model p2 (e, f). [sent-94, score-0.81]
</p><p>37 For the transliteration model, we implement a simplified form of the grapheme-to-phoneme converter, g2p (Bisani and Ney, 2008). [sent-97, score-0.81]
</p><p>38 In the E-step the EM algorithm computes expected counts for the multigrams and in the M-step the multigram probabilities are reestimated from these counts. [sent-103, score-0.33]
</p><p>39 The expected count of a multigram q (E-step) is computed by multiplying the posterior probability of each alignment a with the frequency of q in a and summing these weighted frequencies over all alignments of all word pairs. [sent-107, score-0.349]
</p><p>40 Consider a node r which is connected with a node s via an arc labelled with the multigram q. [sent-113, score-0.401]
</p><p>41 We multiply the expected count of a transition by the posterior probability of transliteration (1 pntr(e, f)) werhiiocrh p pirnodbicabaitelist yho owf likely ttherea string pair is to be a transliteration. [sent-116, score-0.926]
</p><p>42 The counts γrs are then summed for all multigram types q over all training pairs to obtain the frequencies c(q) which are used to reestimate the multigram probabilities according to Equation 5. [sent-117, score-0.465]
</p><p>43 −  4  Semi-supervised Transliteration Mining Model  Our unsupervised transliteration mining system can be applied to language pairs for which no labelled data is available. [sent-118, score-1.464]
</p><p>44 However, the unsupervised system is focused on high recall and also mines close transliterations (see Section 5 for details). [sent-119, score-0.414]
</p><p>45 In a task dependent scenario, it is difficult for the unsupervised system to mine transliteration pairs according to the details of a particular definition of what is considered a transliteration (which may vary somewhat  with the task). [sent-120, score-1.912]
</p><p>46 In this section, we propose an extension of our unsupervised model which overcomes this shortcoming by using labelled data. [sent-121, score-0.378]
</p><p>47 The idea is to rely on probabilities from labelled data where they can be estimated reliably and to use probabilities from unlabelled data where the labelled data is sparse. [sent-122, score-0.707]
</p><p>48 This is achieved by smoothing the labelled data probabilities using the unlabelled data probabilities as a backoff. [sent-123, score-0.471]
</p><p>49 We obtain this effect by smoothing the probability distribution of unlabelled and labelled data using a technique similar to Witten-Bell smoothing (Witten and Bell,  1991), as we describe below. [sent-127, score-0.424]
</p><p>50 The first step creates a reasonable alignment of the labelled data from which multigram counts can be obtained. [sent-130, score-0.469]
</p><p>51 The labelled data is a small list of transliteration  pairs. [sent-131, score-1.111]
</p><p>52 Therefore we use the unlabelled data to help correctly align it and train our unsupervised mining system on the combined labelled and unlabelled training data. [sent-132, score-0.882]
</p><p>53 We start the second step with the probability estimates from the first step and run the E-step separately on labelled and unlabelled data. [sent-135, score-0.424]
</p><p>54 The E-step on the labelled data is done using Equation 8, which forces the posterior probability ofnon-transliteration to zero, while the E-step on the unlabelled data uses Equation 4. [sent-136, score-0.45]
</p><p>55 After the two E-steps, we estimate a probability distribution from the counts obtained from the unlabelled data (M-step) and use it as a backoff distribution in computing smoothed probabilities from the labelled data counts (S-step). [sent-137, score-0.557]
</p><p>56 5 Evaluation We evaluate our unsupervised system and semisupervised system on two tasks, NEWS10 and parallel corpora. [sent-139, score-0.343]
</p><p>57 NEWS 10 is a standard task on transliteration mining from WIL. [sent-140, score-0.965]
</p><p>58 On NEWS10, we compare our results with the unsupervised mining system of Sajjad et al. [sent-141, score-0.352]
</p><p>59 The seed data is a list of 1000 transliteration pairs provided to semi-supervised systems for initial training. [sent-150, score-0.999]
</p><p>60 We compared the word-aligned list with the NEWS10 reference data and found that the word-aligned list is missing some transliteration pairs because ofword-alignment errors. [sent-159, score-1.006]
</p><p>61 We built another list by adding a word pair for every source word that cooccurs with a target word in a parallel phrase/sentence and call it the cross-product list  later on. [sent-160, score-0.383]
</p><p>62 The cross-product list is noisier but contains almost all transliteration pairs in the corpus. [sent-161, score-0.968]
</p><p>63 2 The word-aligned list calculated from the NEWS10 dataset is used to compare our unsupervised system with the unsupervised system of Sajjad et al. [sent-170, score-0.509]
</p><p>64 2 Unsupervised Transliteration Mining We run our unsupervised transliteration mining system on the word-aligned list and the crossproduct list. [sent-177, score-1.254]
</p><p>65 The word pairs with a posterior probability of transliteration 1 − pntr(e, f) = 1 λp2 (ei, fi)/p(ei, fi) greater t−ha np 0. [sent-178, score-0.966]
</p><p>66 We compare our unsupervised system with the unsupervised system of Sajjad1 1. [sent-180, score-0.394]
</p><p>67 On the same machine, our transliteration mining system only takes  1. [sent-194, score-1.02]
</p><p>68 95 Table 2: F-measure results on NEWS10 datasets where SJD is the unsupervised system of Sajjad1 1, OU is our unsupervised system built on the cross-product list, OS is our semi-supervised system, SBest is the best NEWS10 system, GR is the supervised system of Kahki et al. [sent-205, score-0.509]
</p><p>69 (201 1) and DBN is the semi-supervised system of Nabende (201 1) Our unsupervised mining system built on the cross-product list consistently outperforms the one built on the word-aligned list. [sent-206, score-0.528]
</p><p>70 Table 2 shows the results of our unsupervised sys-  tem OU in comparison with the unsupervised system of Sajjad1 1(SJD), the best semi-supervised systems presented at NEWS 10 (SBEST) and the best semi-supervised results reported on the NEWS10 dataset (GR, DBN). [sent-208, score-0.37]
</p><p>71 Cognates are close transliterations which differ by only one or two characters from an exact transliteration pair. [sent-220, score-1.016]
</p><p>72 The unsupervised system learns to delete the additional one or two characters with a high probability and incorrectly  mines such word pairs as transliterations. [sent-221, score-0.435]
</p><p>73 137 Table 3: Precision(P), Recall(R) and F-measure(F) of our unsupervised and semi-supervised transliteration mining systems on NEWS 10 datasets 5. [sent-231, score-1.107]
</p><p>74 This shows that the unlabelled training data is  already providing most of the transliteration information. [sent-235, score-0.957]
</p><p>75 The seed data is used to help the transliteration mining system to learn the right definition of transliteration. [sent-236, score-1.096]
</p><p>76 The increase in precision shows that the seed data is helping the system in disambiguating transliteration pairs from cognates. [sent-241, score-0.989]
</p><p>77 Our transliteration mining system wrongly extracts such pairs as transliterations. [sent-249, score-1.154]
</p><p>78 Ta-  Table 4: Word pairs with pronunciation differences  Table 5: Examples of word pairs which are wrongly annotated as transliterations in the gold standard ble 4 shows a few examples of such word pairs. [sent-253, score-0.392]
</p><p>79 Inconsistencies in the gold standard: There are several inconsistencies in the gold standard where our transliteration system correctly identifies a word pair as a transliteration but it is marked as a nontransliteration or vice versa. [sent-254, score-1.788]
</p><p>80 Our semi-supervised system learns this as a non-transliteration but it is wrongly annotated as a transliteration in the gold standard. [sent-256, score-0.956]
</p><p>81 Our mining system classifies such cases as non-transliterations, but 24 of them are incorrectly annotated as transliterations in the gold standard. [sent-259, score-0.406]
</p><p>82 Often the Russian word differs only by the last character from a correct transliteration of the English word. [sent-267, score-0.881]
</p><p>83 Due to the large amount of such word pairs in the English/Russian data, our mining system learns to delete the final case marking characters from the Russian words. [sent-268, score-0.359]
</p><p>84 It assigns a high transliteration prob475  Table 6: A few examples of English/Russian cognates ability to these word pairs and extracts them as transliterations. [sent-269, score-0.985]
</p><p>85 2  Transliteration Mining using Parallel Corpora The percentage of transliteration pairs in the NEWS10 datasets is high. [sent-279, score-0.876]
</p><p>86 We further check the effectiveness of our unsupervised and semi-supervised mining systems by evaluating them on parallel corpora with as few as 2% transliteration pairs. [sent-280, score-1.157]
</p><p>87 The English/Hindi and English/Arabic transliteration gold standards were provided by Sajjad et al. [sent-285, score-0.842]
</p><p>88 We first train and test our unsupervised mining system on the word-aligned list and compare our results with Sajjad et al. [sent-292, score-0.417]
</p><p>89 35 Table 7: Transliteration mining results of our unsupervised system and Sajjad1 1 system trained and tested on the word-aligned list of English/Hindi and English/Arabic parallel corpus  TPFNTNFPPRF  EEHHSU 3 9653 1479 1 2 234709 16289 8745. [sent-307, score-0.522]
</p><p>90 59 Table 8: Transliteration mining results of our unsuper-  vised and semi-supervised systems trained on the wordaligned list and tested on the cross-product list of English/Hindi and English/Arabic parallel corpus aligned list but has almost 100% recall of transliteration pairs. [sent-313, score-1.259]
</p><p>91 The English-Hindi cross-product list has almost 55% more transliteration pairs (412 types) than the word-aligned list (180 types). [sent-314, score-1.006]
</p><p>92 transliteration pairs of our unsupervised tains 65 and 111 close transliterations glish/Hindi and English/Arabic task  The mined system confor the Enrespectively. [sent-323, score-1.241]
</p><p>93 We define their probability as the inverse of the number of multigram tokens in the Viterbi alignment of the labelled and unlabelled data together. [sent-325, score-0.632]
</p><p>94 We think these pairs provide transliteration information to the systems and help them to avoid problems with data sparseness. [sent-327, score-0.876]
</p><p>95 6 Conclusion and Future Work We presented a novel model to automatically mine transliteration pairs. [sent-332, score-0.839]
</p><p>96 Both the unsupervised and semisupervised systems achieve higher accuracy than the only unsupervised transliteration mining system we are aware of and are competitive with the stateof-the-art supervised and semi-supervised systems. [sent-334, score-1.377]
</p><p>97 These language pairs require oneto-many character mappings to learn transliteration units, while our current system only learns unigram character alignments. [sent-337, score-1.105]
</p><p>98 Whitepaper of NEWS 2010 shared task on transliteration mining. [sent-382, score-0.832]
</p><p>99 Language independent transliteration mining system using finite state automata framework. [sent-402, score-1.02]
</p><p>100 An algorithm for unsupervised transliteration mining with an application to word alignment. [sent-411, score-1.13]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transliteration', 0.81), ('labelled', 0.236), ('multigram', 0.165), ('sajjad', 0.165), ('mining', 0.155), ('unlabelled', 0.147), ('transliterations', 0.145), ('unsupervised', 0.142), ('kumaran', 0.088), ('kahki', 0.082), ('multigrams', 0.069), ('pairs', 0.066), ('list', 0.065), ('seed', 0.058), ('cognates', 0.055), ('system', 0.055), ('parallel', 0.05), ('em', 0.049), ('character', 0.048), ('probabilities', 0.044), ('jiampojamarn', 0.044), ('alignment', 0.043), ('bisani', 0.041), ('semisupervised', 0.041), ('probability', 0.041), ('arabic', 0.04), ('news', 0.039), ('characters', 0.038), ('unigram', 0.038), ('wrongly', 0.037), ('alphabetic', 0.036), ('fraser', 0.034), ('pronounced', 0.034), ('vowels', 0.033), ('uppsala', 0.032), ('gold', 0.032), ('supervised', 0.032), ('dataset', 0.031), ('helmut', 0.031), ('russian', 0.031), ('extracts', 0.031), ('target', 0.03), ('interpolation', 0.03), ('mines', 0.029), ('wordaligned', 0.029), ('mine', 0.029), ('built', 0.028), ('alignments', 0.028), ('axlign', 0.027), ('crossproduct', 0.027), ('deligne', 0.027), ('deutsche', 0.027), ('eisele', 0.027), ('forschungsgemeinschaft', 0.027), ('noeman', 0.027), ('noisier', 0.027), ('pntr', 0.027), ('quran', 0.027), ('reestimated', 0.027), ('sjd', 0.027), ('submodel', 0.027), ('tpfntnfpprf', 0.027), ('expectation', 0.027), ('hassan', 0.026), ('posterior', 0.026), ('pair', 0.026), ('source', 0.026), ('gr', 0.025), ('participated', 0.025), ('counts', 0.025), ('dbn', 0.024), ('khapra', 0.024), ('mitesh', 0.024), ('nabende', 0.024), ('sbest', 0.024), ('wil', 0.024), ('later', 0.024), ('word', 0.023), ('count', 0.023), ('equation', 0.023), ('maximization', 0.023), ('wikipedia', 0.023), ('close', 0.023), ('learns', 0.022), ('darwish', 0.022), ('kareem', 0.022), ('stuttgart', 0.022), ('achieves', 0.022), ('shared', 0.022), ('haizhou', 0.022), ('smoothed', 0.021), ('witten', 0.02), ('fi', 0.02), ('recall', 0.02), ('schmid', 0.019), ('incorrectly', 0.019), ('ei', 0.019), ('calculated', 0.019), ('estimate', 0.018), ('learn', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="20-tfidf-1" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>2 0.27933666 <a title="20-tfidf-2" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>Author: Preslav Nakov ; Jorg Tiedemann</p><p>Abstract: We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.</p><p>3 0.23276511 <a title="20-tfidf-3" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>Author: Joseph Z. Chang ; Jason S. Chang ; Roger Jyh-Shing Jang</p><p>Abstract: Jason S. Chang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j s chang@ c s .nthu . edu .tw Jyh-Shing Roger Jang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j ang@ c s .nthu .edu .tw identifying such translation counterparts Web, we can cope with the OOV problem. In this paper, we present a new method on the for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 1</p><p>4 0.12722953 <a title="20-tfidf-4" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>Author: Sungchul Kim ; Kristina Toutanova ; Hwanjo Yu</p><p>Abstract: In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.</p><p>5 0.11694294 <a title="20-tfidf-5" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>6 0.094120294 <a title="20-tfidf-6" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>7 0.080341443 <a title="20-tfidf-7" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<p>8 0.062525384 <a title="20-tfidf-8" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>9 0.061490238 <a title="20-tfidf-9" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>10 0.050451092 <a title="20-tfidf-10" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>11 0.050140366 <a title="20-tfidf-11" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>12 0.046986267 <a title="20-tfidf-12" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>13 0.0449747 <a title="20-tfidf-13" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>14 0.041586321 <a title="20-tfidf-14" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>15 0.040578619 <a title="20-tfidf-15" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>16 0.040431853 <a title="20-tfidf-16" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>17 0.040268708 <a title="20-tfidf-17" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>18 0.039835982 <a title="20-tfidf-18" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>19 0.038865097 <a title="20-tfidf-19" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>20 0.037977651 <a title="20-tfidf-20" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.143), (1, -0.049), (2, 0.037), (3, 0.045), (4, 0.098), (5, 0.102), (6, 0.016), (7, -0.059), (8, -0.0), (9, -0.064), (10, 0.018), (11, -0.077), (12, -0.025), (13, -0.01), (14, 0.026), (15, -0.063), (16, -0.042), (17, -0.008), (18, 0.044), (19, 0.194), (20, -0.183), (21, 0.051), (22, 0.194), (23, -0.085), (24, -0.138), (25, 0.129), (26, -0.092), (27, -0.052), (28, -0.093), (29, 0.181), (30, -0.184), (31, -0.07), (32, -0.184), (33, -0.099), (34, 0.152), (35, -0.116), (36, 0.116), (37, 0.076), (38, -0.018), (39, 0.004), (40, -0.014), (41, -0.033), (42, 0.207), (43, -0.234), (44, -0.04), (45, 0.038), (46, 0.018), (47, -0.157), (48, 0.114), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95024568 <a title="20-lsi-1" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>2 0.62268716 <a title="20-lsi-2" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>Author: Preslav Nakov ; Jorg Tiedemann</p><p>Abstract: We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.</p><p>3 0.48858213 <a title="20-lsi-3" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>Author: Joseph Z. Chang ; Jason S. Chang ; Roger Jyh-Shing Jang</p><p>Abstract: Jason S. Chang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j s chang@ c s .nthu . edu .tw Jyh-Shing Roger Jang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j ang@ c s .nthu .edu .tw identifying such translation counterparts Web, we can cope with the OOV problem. In this paper, we present a new method on the for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 1</p><p>4 0.36473942 <a title="20-lsi-4" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>5 0.34735599 <a title="20-lsi-5" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>Author: Sungchul Kim ; Kristina Toutanova ; Hwanjo Yu</p><p>Abstract: In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.</p><p>6 0.30714861 <a title="20-lsi-6" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>7 0.29086307 <a title="20-lsi-7" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<p>8 0.28426355 <a title="20-lsi-8" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>9 0.24408323 <a title="20-lsi-9" href="./acl-2012-Applications_of_GPC_Rules_and_Character_Structures_in_Games_for_Learning_Chinese_Characters.html">26 acl-2012-Applications of GPC Rules and Character Structures in Games for Learning Chinese Characters</a></p>
<p>10 0.23844182 <a title="20-lsi-10" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>11 0.20927413 <a title="20-lsi-11" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>12 0.20392783 <a title="20-lsi-12" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>13 0.20050012 <a title="20-lsi-13" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>14 0.19482729 <a title="20-lsi-14" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>15 0.19314829 <a title="20-lsi-15" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>16 0.19028705 <a title="20-lsi-16" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>17 0.18583833 <a title="20-lsi-17" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>18 0.1779619 <a title="20-lsi-18" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>19 0.17338656 <a title="20-lsi-19" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>20 0.17146353 <a title="20-lsi-20" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.024), (26, 0.063), (28, 0.061), (30, 0.018), (37, 0.035), (39, 0.058), (57, 0.019), (74, 0.029), (82, 0.023), (84, 0.019), (85, 0.025), (89, 0.28), (90, 0.145), (92, 0.037), (94, 0.023), (99, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.74967068 <a title="20-lda-1" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>Author: Xiaohua Liu ; Ming Zhou ; Xiangyang Zhou ; Zhongyang Fu ; Furu Wei</p><p>Abstract: Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively.</p><p>same-paper 2 0.70311201 <a title="20-lda-2" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>3 0.69170886 <a title="20-lda-3" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>Author: Andrea Gesmundo ; Tanja Samardzic</p><p>Abstract: We present a novel approach to the task of word lemmatisation. We formalise lemmatisation as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a specific language. In this way, a lemmatisation system can be trained and tested using any supervised tagging model. In contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information. We test our approach on eight languages reaching a new state-of-the-art level for the lemmatisation task.</p><p>4 0.55410588 <a title="20-lda-4" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>5 0.54929352 <a title="20-lda-5" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>6 0.54817206 <a title="20-lda-6" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>7 0.54810119 <a title="20-lda-7" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>8 0.54768437 <a title="20-lda-8" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>9 0.54713905 <a title="20-lda-9" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>10 0.5459429 <a title="20-lda-10" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>11 0.54497659 <a title="20-lda-11" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>12 0.5435012 <a title="20-lda-12" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>13 0.54306453 <a title="20-lda-13" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>14 0.54191154 <a title="20-lda-14" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>15 0.54187763 <a title="20-lda-15" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>16 0.54186416 <a title="20-lda-16" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>17 0.54174542 <a title="20-lda-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.54117799 <a title="20-lda-18" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>19 0.53949106 <a title="20-lda-19" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>20 0.53949028 <a title="20-lda-20" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
