<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-10" href="#">acl2012-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</h1>
<br/><p>Source: <a title="acl-2012-10-pdf" href="http://aclweb.org/anthology//P/P12/P12-1040.pdf">pdf</a></p><p>Author: Michael Wick ; Sameer Singh ; Andrew McCallum</p><p>Abstract: Sameer Singh Andrew McCallum University of Massachusetts University of Massachusetts 140 Governor’s Drive 140 Governor’s Drive Amherst, MA Amherst, MA s ameer@ cs .umas s .edu mccal lum@ c s .umas s .edu Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap- proach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</p><p>Reference: <a title="acl-2012-10-reference" href="../acl2012_reference/acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap-  proach to coreference. [sent-8, score-0.711]
</p><p>2 However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. [sent-9, score-0.339]
</p><p>3 As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. [sent-10, score-0.273]
</p><p>4 These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. [sent-12, score-1.03]
</p><p>5 We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU. [sent-13, score-1.219]
</p><p>6 1 Introduction Coreference resolution, the task of clustering mentions into partitions representing their underlying real-world entities, is fundamental for high-level information extraction and data integration, including semantic search, question answering, and knowl-  edge base construction. [sent-14, score-0.367]
</p><p>7 For example, coreference is vital for determining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. [sent-15, score-0.656]
</p><p>8 Over the years, various machine learning techniques have been applied to different variations of the coreference problem. [sent-20, score-0.587]
</p><p>9 A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al. [sent-21, score-0.93]
</p><p>10 That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B? [sent-23, score-1.074]
</p><p>11 The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. [sent-26, score-0.273]
</p><p>12 Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al. [sent-27, score-0.915]
</p><p>13 , 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). [sent-30, score-0.587]
</p><p>14 A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-31, score-1.231]
</p><p>15 graph  for coreference:  Latent entity nodes (white boxes)  Pairwise factors (black squares) measure compatibilities  nodes, avoiding quadratic blow-up. [sent-34, score-0.288]
</p><p>16 Corresponding  between child and parent  decision variables (open circles) indicate whether one  node is the child of another. [sent-35, score-0.326]
</p><p>17 number of mentions in each entity cluster is also large. [sent-38, score-0.51]
</p><p>18 , 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al. [sent-41, score-0.398]
</p><p>19 However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. [sent-46, score-1.051]
</p><p>20 This scaling with the number of mentions per entity seems particularly wasteful because although it is common for an entity to be referenced by a large number of mentions, many of these coreferent mentions are highly similar to each other. [sent-47, score-0.981]
</p><p>21 For example, in author  coreference the two most common strings that refer to Richard Hamming might have the form “R. [sent-48, score-0.656]
</p><p>22 ” In newswire coreference, a prominent entity like Barack Obama may have millions of “Obama” mentions (many occurring in similar semantic contexts). [sent-50, score-0.467]
</p><p>23 Deciding whether 380 a mention belongs to this entity need not involve comparisons to all contextually similar “Obama” mentions; rather we prefer a more compact representation in order to efficiently reason about them. [sent-51, score-0.33]
</p><p>24 In this paper we propose a novel hierarchical discriminative factor graph for coreference resolution that recursively structures each entity as a tree of latent sub-entities with mentions at the leaves. [sent-52, score-1.386]
</p><p>25 Thus, a small handful of upper-level nodes may summarize millions of mentions (for example, a single node may summarize all contextually similar “R. [sent-55, score-0.494]
</p><p>26 Moreover, each step of inference is computationally efficient because evaluating the cost of attaching (or detaching) sub-trees requires computing just a single compatibility function (as seen in Figure 1). [sent-59, score-0.313]
</p><p>27 , entities with more mentions may require a deeper hierarchy to compress). [sent-63, score-0.411]
</p><p>28 Third, the trees can provide split points for finer-grained entities by placing contextually similar mentions under the same subtree. [sent-65, score-0.481]
</p><p>29 Finally, if memory is limited, redundant mentions can be pruned by replacing subtrees with their roots. [sent-66, score-0.339]
</p><p>30 Empirically, we demonstrate that our model is several orders of magnitude faster than a pairwise model, allowing us to perform efficient coreference on nearly six million author mentions in under four hours using a single CPU. [sent-67, score-1.385]
</p><p>31 2  Background: Pairwise Coreference  Coreference is the problem of clustering mentions such that mentions in the same set refer to the same real-world entity; it is also known as entity disambiguation, record linkage, and de-duplication. [sent-68, score-0.872]
</p><p>32 For example, in author coreference, each mention might be represented as a record extracted from the author field of a textual citation or BibTeX record. [sent-69, score-0.378]
</p><p>33 The mention record may contain attributes for the first, middle, and last name of the author, as well as contextual information occurring in the citation string,  ×  co-authors, titles, topics, and institutions. [sent-70, score-0.277]
</p><p>34 The goal is to cluster these mention records into sets, each containing all the mentions of the author to which they refer; we use this task as a running pedagogical example. [sent-71, score-0.64]
</p><p>35 For example, in author coreference, t φhe( mfeature functions φ might test whether the name fields for two author mentions are string identical, or compute cosine similarity between the two mentions’ bags-of-words, each representing a mention’s context. [sent-75, score-0.525]
</p><p>36 The corresponding real-valued weights θ determine the impact of these features on the overall pairwise score. [sent-76, score-0.273]
</p><p>37 Coreference can be solved by introducing a set of binary coreference decision variables for each mention pair and predicting a setting to their values that maximizes the sum of pairwise compatibility functions. [sent-77, score-1.37]
</p><p>38 While it is possible to independently make pairwise decisions and enforce transitivity post hoc, this can lead to poor accuracy because the decisions are tightly coupled. [sent-78, score-0.527]
</p><p>39 For higher accuracy, a graphical model such as a conditional random field (CRF) is constructed from the compatibility functions to jointly reason about the pairwise decisions (McCallum and Wellner, 2004). [sent-79, score-0.588]
</p><p>40 We now describe the pairwise CRF for coreference as a factor graph. [sent-80, score-0.907]
</p><p>41 1 Pairwise Conditional Random Field Each mention mi ∈ M is an observed variable, and for each mention pair (mi, mj) we dh vaaveri a binary coreference decision variable yij whose value determines whether mi and mj refer to the same entity (i. [sent-82, score-1.402]
</p><p>42 The pairwise compatibility functions become the factors in the graphical model. [sent-85, score-0.622]
</p><p>43 Each factor examines the properties of its mention pair as well as the setting to the coreference decision  ×  variable and outputs a score indicating how likely the setting of that coreference variable is. [sent-86, score-1.536]
</p><p>44 ,  ψ : M M {0, 1}  →  <  Figure 2: Pairwise model on six mentions: Open circles are the binary coreference decision variables, shaded circles are the observed mentions, and the black boxes are the factors of the graphical model that encode the pairwise compatibility functions. [sent-89, score-1.342]
</p><p>45 constraint that the setting to the coreference variables obey transitivity;2 this is the maximum probability estimate (MPE) setting. [sent-90, score-0.644]
</p><p>46 2 Approximate Inference An approximate inference framework that has successfully been used for coreference models is Metropolis-Hastings (MH) (Milch et al. [sent-93, score-0.644]
</p><p>47 A proposal function q takes the current coreference hypothesis and proposes a new hypothesis by modifying a subset of the decision variables. [sent-96, score-0.715]
</p><p>48 2We say that a full y  (2)  assignment to the coreference variables obeys transitivity if ∀ ijk yij = 1∧ yjk = 1 =⇒ yik = 1  382 When using MH for MPE inference, the second term q(y|y0)/q(y0|y) is optional, and usually omitted. [sent-99, score-0.754]
</p><p>49 The primary advantages of MH for coreference are (1) only the compatibility functions of the changed decision variablesJ nmeieed,C taol a bne, evaluateJd. [sent-102, score-0.928]
</p><p>50 , Ctoa laacnc,epv ,t a move, and (2) the proposal function can enforce the transitivity constraint by exploring only variable settings that result in valid coreference partitionings. [sent-103, score-0.781]
</p><p>51 Typically, MH is employed by first initializing to a singleton configuration (all entities have one mention), and then executing the MH for a certain number of steps (or until the predicted coreference hypothesis stops changing). [sent-107, score-0.736]
</p><p>52 This proposal distribution always moves a single mention m from some entity ei to another entity ej and thus the configuration y and y0 only differ by the setting of decision variables governing to which entity m refers. [sent-108, score-0.878]
</p><p>53 In order to guarantee transitivity and a valid coreference equivalence relation, we must properly remove m from ei by untethering m from each mention in ei (this requires computing |ei | − 1pairwise factors). [sent-109, score-0.897]
</p><p>54 d Seirm tiol complete tnh,e f move into ej we must coref m to each mention in ej (this requires computing |ej | pairwise factors). [sent-111, score-0.555]
</p><p>55 Thus, evaluating each proposal for the pairwise model scales linearly with the number of mentions assigned to the entities, requiring the evaluation of 2( |ei | + |ej | − 1) compatibility functions (factors). [sent-114, score-0.963]
</p><p>56 3  Hierarchical Coreference  Instead of only capturing a single coreference clustering between mention pairs, we can imagine mul-  tiple levels of coreference decisions over different granularities. [sent-115, score-1.417]
</p><p>57 For example, mentions of an author may be further partitioned into semantically similar sets, such that mentions from each set have topically similar papers. [sent-116, score-0.747]
</p><p>58 In this section, we describe a model that captures arbitrarily deep hierarchies over such layers of coreference decisions, enabling efficient inference and rich entity representations. [sent-120, score-0.802]
</p><p>59 1 Discriminative Hierarchical Model In contrast to the pairwise model, where each entity is a flat cluster of mentions, our proposed model structures each entity recursively as a tree. [sent-122, score-0.621]
</p><p>60 The leaves of the tree are the observed mentions with a set of attribute values. [sent-123, score-0.423]
</p><p>61 Each internal node of the tree is latent and contains a set of unobserved attributes; recursively, these node records summarize the attributes of their child nodes (see Figure 1), for example, they may aggregate the bags of context words of the children. [sent-124, score-0.288]
</p><p>62 Formally, the coreference decision vari-  ables in the hierarchical model no longer represent pairwise decisions directly. [sent-126, score-1.103]
</p><p>63 Instead, a decision variable yri,rj = 1indicates that node-record rj is the parent of node-record ri. [sent-127, score-0.331]
</p><p>64 In order to represent our recursive model of coreference, we include two types of factors: pairwise factors ψpw that measure compatibility between a child node-record and its parent, and unit-wise factors ψrw that measure compatibilities of the noderecords themselves. [sent-131, score-0.767]
</p><p>65 The unit-wise factors can examine compatibility of settings to the attribute variables for a particular node (for example, the set of topics may be too diverse to represent just a single entity), as well as enforce priors over the tree’s breadth and depth. [sent-133, score-0.483]
</p><p>66 2 MCMC Inference for Hierarchical models The state space of our hierarchical model is substantially larger (theoretically infinite) than the pairwise model due to the arbitrarily deep (and wide) latent structure of the cluster trees. [sent-135, score-0.494]
</p><p>67 Inference must simultaneously determine the structure of the tree, the latent node-record values, as well as the coreference decisions themselves. [sent-136, score-0.683]
</p><p>68 Indeed, despite the enlarged state space, inference in the hierarchical model is substantially faster than a pairwise model with a smaller state space. [sent-138, score-0.486]
</p><p>69 Further, evaluating each proposal during inference in the hierarchical model is substantially faster than in the pairwise model. [sent-141, score-0.577]
</p><p>70 More specificaly, for each MH step, we  first randomly select two subtrees headed by noderecords ri and rj from the current coreference hypothesis. [sent-147, score-0.888]
</p><p>71 • Merge Up - merges node ri with node rj by creating a new parent nesod neo-rdeecro rd variable with ri and rj as the children. [sent-149, score-0.777]
</p><p>72 Computing the model ratio for all of coreference proposals requires only a constant number of compatibility functions. [sent-153, score-0.884]
</p><p>73 On the other hand, evaluating  rp  rp  proposals in the pairwise model requires evaluating a number of compatibility functions equal to the number of mentions in the clusters being modified. [sent-154, score-1.129]
</p><p>74 Note that changes to the attribute values of the node-record and collapsing still require evaluating a linear number of factors, but this is only linear in the number of child nodes, not linear in the number of mentions referring to the entity. [sent-155, score-0.467]
</p><p>75 Finally, we incrementally update bags during coreference to reflect the aggregates of their children. [sent-157, score-0.587]
</p><p>76 4  Experiments: Author Coreference  Author coreference is a tremendously important task, enabling improved search and mining of scientific papers by researchers, funding agencies, and governments. [sent-158, score-0.587]
</p><p>77 In addition we include 2,833 mentions from the REXA dataset4 labeled for coreference, in order to assess accuracy. [sent-163, score-0.339]
</p><p>78 The pairwise compatibility functions punish a string difference in first, middle, and last name, (−8); reward a match (+2); and reward matching ,in (i−tia8l)s; (+1). [sent-167, score-0.533]
</p><p>79 These compatibility functions define the scores of the factors in the pairwise model and the parent-child factors in the hierarchical model. [sent-169, score-0.818]
</p><p>80 We augment these samplers using canopies constructed by concatenating the first initial and last name: that is, mentions are only selected from within the same canopy (or block) to reduce the search space (Bilenko et al. [sent-177, score-0.339]
</p><p>81 During the course of MCMC inference, we record the pairwise F1 scores of the labeled subset. [sent-179, score-0.311]
</p><p>82 5"6#& %3(*  Figure 3: Example coreference proposals for the case where  ri  and rj  are  initially in different clusters. [sent-202, score-0.941]
</p><p>83 The pairwise model struggles with the large cluster sizes while the hierarchical model is hardly affected. [sent-211, score-0.423]
</p><p>84 Even though the hierarchical model is evaluating up to four proposals for each sample, it is still able to sample much faster than the pairwise model; this is expected because the cost of evaluating a proposal requires evaluating fewer factors. [sent-212, score-0.693]
</p><p>85 Next, we plot coreference F1 accuracy over time and show in Figure 5a that the prolific sampling rate ofthe hierarchical model results in faster coreference. [sent-213, score-0.777]
</p><p>86 For example, on the 145k mention dataset, at a 60% accuracy level the hierarchical model is 19 times faster and at 90% accuracy it is 3 1 times faster. [sent-215, score-0.384]
</p><p>87 These performance improvements are even more profound on larger datasets: the hierarchical model achieves a 60% level of accuracy 72 times faster than the pairwise model on the 1. [sent-216, score-0.463]
</p><p>88 3 Large Scale Experiments In order to demonstrate the scalability of the hierarchical model, we run it on nearly 5 million author mentions from DBLP. [sent-220, score-0.553]
</p><p>89 Finally, we combine DBLP with BibTeX data to produce a dataset with almost 6 million mentions (5,803,81 1). [sent-222, score-0.377]
</p><p>90 (201 1) introduce a hierarchical model for coreference that treats entities as a two-tiered structure, by introducing the concept of sub-entities and super-entities. [sent-225, score-0.766]
</p><p>91 Subentities provide a tighter granularity of coreference and can be used to perform larger block moves during MCMC. [sent-227, score-0.625]
</p><p>92 Even more importantly, their model has pairwise factors and suffers from the quadratic curse, which they address by distributing inference. [sent-230, score-0.401]
</p><p>93 Our hierarchical model provides the advantages of recently proposed entity-based coreference sys-  tems that are known to provide higher accuracy (Haghighi and Klein, 2007; Culotta et al. [sent-236, score-0.728]
</p><p>94 samples (convergence accuracy as dashes) Figure 4: Sampling Performance Plots for 145k mentions  Accuracy versus Time  Running time (s) (a) Accuracy vs. [sent-242, score-0.407]
</p><p>95 6  Conclusion  In this paper we present a new hierarchical model for large scale coreference and demonstrate it on  the problem of author disambiguation. [sent-247, score-0.795]
</p><p>96 Indeed, inference in the hierarchy is orders of magnitude faster than a pairwise CRF, allowing us to infer accurate coreference on 386 six million mentions on one CPU in just 4 hours. [sent-249, score-1.343]
</p><p>97 Machine learning for coreference resolution: From local classification to global ranking. [sent-334, score-0.587]
</p><p>98 Large-scale crossdocument coreference using distributed inference and hierarchical models. [sent-367, score-0.783]
</p><p>99 A machine learning approach to coreference resolution of noun phrases. [sent-380, score-0.635]
</p><p>100 An entity-mention model for coreference resolution with inductive logic programming. [sent-401, score-0.635]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.587), ('mentions', 0.339), ('pairwise', 0.273), ('compatibility', 0.212), ('mention', 0.16), ('rj', 0.154), ('mh', 0.134), ('entity', 0.128), ('ri', 0.115), ('mj', 0.111), ('hierarchical', 0.107), ('factors', 0.089), ('proposals', 0.085), ('culotta', 0.083), ('wick', 0.083), ('decision', 0.081), ('transitivity', 0.078), ('entities', 0.072), ('hamming', 0.069), ('author', 0.069), ('singh', 0.068), ('compress', 0.059), ('parent', 0.059), ('inference', 0.057), ('variables', 0.057), ('mcmc', 0.056), ('singla', 0.055), ('decisions', 0.055), ('mi', 0.053), ('node', 0.049), ('recursively', 0.049), ('faster', 0.049), ('merge', 0.049), ('resolution', 0.048), ('aron', 0.048), ('bibtex', 0.048), ('bilenko', 0.048), ('milch', 0.048), ('mpe', 0.048), ('swendsen', 0.048), ('functions', 0.048), ('proposal', 0.047), ('factor', 0.047), ('coreferent', 0.047), ('mccallum', 0.046), ('merges', 0.045), ('pedro', 0.045), ('singleton', 0.045), ('attribute', 0.044), ('evaluating', 0.044), ('cluster', 0.043), ('haghighi', 0.043), ('ej', 0.043), ('rp', 0.042), ('citation', 0.042), ('contextually', 0.042), ('latent', 0.041), ('domingos', 0.041), ('tree', 0.04), ('child', 0.04), ('quadratic', 0.039), ('million', 0.038), ('record', 0.038), ('moves', 0.038), ('sameer', 0.038), ('variable', 0.037), ('attributes', 0.037), ('ei', 0.036), ('move', 0.036), ('andrew', 0.036), ('amherst', 0.035), ('circles', 0.035), ('collapse', 0.035), ('titles', 0.034), ('accuracy', 0.034), ('samples', 0.034), ('rao', 0.033), ('enforce', 0.032), ('summarize', 0.032), ('configuration', 0.032), ('scale', 0.032), ('governor', 0.032), ('bengston', 0.032), ('compatibilities', 0.032), ('crossdocument', 0.032), ('dblp', 0.032), ('detaching', 0.032), ('factorie', 0.032), ('lifted', 0.032), ('metropolis', 0.032), ('noderecords', 0.032), ('ravin', 0.032), ('rexa', 0.032), ('yij', 0.032), ('arbitrarily', 0.03), ('boxes', 0.03), ('hours', 0.03), ('running', 0.029), ('clustering', 0.028), ('placing', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="10-tfidf-1" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>Author: Michael Wick ; Sameer Singh ; Andrew McCallum</p><p>Abstract: Sameer Singh Andrew McCallum University of Massachusetts University of Massachusetts 140 Governor’s Drive 140 Governor’s Drive Amherst, MA Amherst, MA s ameer@ cs .umas s .edu mccal lum@ c s .umas s .edu Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap- proach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</p><p>2 0.29272789 <a title="10-tfidf-2" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.</p><p>3 0.26304424 <a title="10-tfidf-3" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>4 0.2047213 <a title="10-tfidf-4" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>Author: Katja Markert ; Yufang Hou ; Michael Strube</p><p>Abstract: Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classification and focuses on conversational dialogue. We here introduce the task of classifying finegrained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status ofall mentions. Our approach strongly outperforms reimplementations of previous work.</p><p>5 0.15702167 <a title="10-tfidf-5" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>Author: Einat Minkov ; Luke Zettlemoyer</p><p>Abstract: This paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention de- tection and template filling tasks.</p><p>6 0.13005283 <a title="10-tfidf-6" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>7 0.12563653 <a title="10-tfidf-7" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>8 0.10283373 <a title="10-tfidf-8" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>9 0.099857926 <a title="10-tfidf-9" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>10 0.095504664 <a title="10-tfidf-10" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>11 0.092876822 <a title="10-tfidf-11" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>12 0.084936477 <a title="10-tfidf-12" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>13 0.08461725 <a title="10-tfidf-13" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>14 0.080917649 <a title="10-tfidf-14" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>15 0.075186893 <a title="10-tfidf-15" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>16 0.073438518 <a title="10-tfidf-16" href="./acl-2012-Efficient_Search_for_Transformation-based_Inference.html">78 acl-2012-Efficient Search for Transformation-based Inference</a></p>
<p>17 0.0718477 <a title="10-tfidf-17" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>18 0.068430953 <a title="10-tfidf-18" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>19 0.060611475 <a title="10-tfidf-19" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>20 0.058514476 <a title="10-tfidf-20" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.196), (1, 0.12), (2, -0.062), (3, 0.108), (4, 0.004), (5, 0.088), (6, -0.053), (7, 0.138), (8, 0.147), (9, -0.055), (10, 0.04), (11, -0.224), (12, -0.179), (13, -0.154), (14, 0.06), (15, 0.115), (16, 0.096), (17, 0.141), (18, -0.348), (19, -0.152), (20, -0.034), (21, 0.065), (22, 0.008), (23, 0.009), (24, 0.088), (25, 0.0), (26, -0.027), (27, 0.073), (28, -0.049), (29, -0.082), (30, -0.138), (31, 0.034), (32, -0.032), (33, -0.072), (34, 0.035), (35, 0.018), (36, 0.004), (37, -0.039), (38, 0.051), (39, -0.008), (40, -0.045), (41, -0.047), (42, 0.078), (43, 0.101), (44, 0.054), (45, -0.087), (46, -0.037), (47, -0.035), (48, -0.045), (49, -0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97217447 <a title="10-lsi-1" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>Author: Michael Wick ; Sameer Singh ; Andrew McCallum</p><p>Abstract: Sameer Singh Andrew McCallum University of Massachusetts University of Massachusetts 140 Governor’s Drive 140 Governor’s Drive Amherst, MA Amherst, MA s ameer@ cs .umas s .edu mccal lum@ c s .umas s .edu Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap- proach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</p><p>2 0.86101681 <a title="10-lsi-2" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.</p><p>3 0.80485183 <a title="10-lsi-3" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>Author: Katja Markert ; Yufang Hou ; Michael Strube</p><p>Abstract: Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classification and focuses on conversational dialogue. We here introduce the task of classifying finegrained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status ofall mentions. Our approach strongly outperforms reimplementations of previous work.</p><p>4 0.7924751 <a title="10-lsi-4" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>5 0.50848687 <a title="10-lsi-5" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>Author: Einat Minkov ; Luke Zettlemoyer</p><p>Abstract: This paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention de- tection and template filling tasks.</p><p>6 0.39356717 <a title="10-lsi-6" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>7 0.34761292 <a title="10-lsi-7" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>8 0.34454733 <a title="10-lsi-8" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>9 0.32546923 <a title="10-lsi-9" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>10 0.30668768 <a title="10-lsi-10" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>11 0.2575174 <a title="10-lsi-11" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>12 0.25494832 <a title="10-lsi-12" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>13 0.25417671 <a title="10-lsi-13" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>14 0.25140622 <a title="10-lsi-14" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>15 0.25017315 <a title="10-lsi-15" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>16 0.24144168 <a title="10-lsi-16" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>17 0.24111803 <a title="10-lsi-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.23658098 <a title="10-lsi-18" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>19 0.23099597 <a title="10-lsi-19" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>20 0.22745727 <a title="10-lsi-20" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.011), (26, 0.039), (28, 0.038), (30, 0.033), (37, 0.046), (39, 0.067), (49, 0.01), (59, 0.023), (73, 0.215), (74, 0.028), (82, 0.034), (84, 0.016), (85, 0.024), (90, 0.13), (92, 0.099), (94, 0.044), (99, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83968472 <a title="10-lda-1" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>Author: Michael Wick ; Sameer Singh ; Andrew McCallum</p><p>Abstract: Sameer Singh Andrew McCallum University of Massachusetts University of Massachusetts 140 Governor’s Drive 140 Governor’s Drive Amherst, MA Amherst, MA s ameer@ cs .umas s .edu mccal lum@ c s .umas s .edu Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap- proach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</p><p>2 0.69567817 <a title="10-lda-2" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>Author: Boxing Chen ; Roland Kuhn ; Samuel Larkin</p><p>Abstract: Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT , a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves 1 consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties). 1</p><p>3 0.66071445 <a title="10-lda-3" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart Shieber</p><p>Abstract: We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data.</p><p>4 0.6598087 <a title="10-lda-4" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>Author: Yanir Seroussi ; Fabian Bohnert ; Ingrid Zukerman</p><p>Abstract: Authorship attribution deals with identifying the authors of anonymous texts. Building on our earlier finding that the Latent Dirichlet Allocation (LDA) topic model can be used to improve authorship attribution accuracy, we show that employing a previously-suggested Author-Topic (AT) model outperforms LDA when applied to scenarios with many authors. In addition, we define a model that combines LDA and AT by representing authors and documents over two disjoint topic sets, and show that our model outperforms LDA, AT and support vector machines on datasets with many authors.</p><p>5 0.65821809 <a title="10-lda-5" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>Author: Hiroyuki Shindo ; Yusuke Miyao ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers.</p><p>6 0.65722436 <a title="10-lda-6" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>7 0.6567806 <a title="10-lda-7" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>8 0.65409935 <a title="10-lda-8" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>9 0.65178174 <a title="10-lda-9" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>10 0.650226 <a title="10-lda-10" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>11 0.64803612 <a title="10-lda-11" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>12 0.64749658 <a title="10-lda-12" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>13 0.64333135 <a title="10-lda-13" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>14 0.6425876 <a title="10-lda-14" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>15 0.64202249 <a title="10-lda-15" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>16 0.64162439 <a title="10-lda-16" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>17 0.64106399 <a title="10-lda-17" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>18 0.64004499 <a title="10-lda-18" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>19 0.63968849 <a title="10-lda-19" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>20 0.63527226 <a title="10-lda-20" href="./acl-2012-Topic_Models%2C_Latent_Space_Models%2C_Sparse_Coding%2C_and_All_That%3A_A_Systematic_Understanding_of_Probabilistic_Semantic_Extraction_in_Large_Corpus.html">198 acl-2012-Topic Models, Latent Space Models, Sparse Coding, and All That: A Systematic Understanding of Probabilistic Semantic Extraction in Large Corpus</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
