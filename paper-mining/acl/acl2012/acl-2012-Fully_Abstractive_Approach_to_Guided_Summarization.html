<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 acl-2012-Fully Abstractive Approach to Guided Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-101" href="#">acl2012-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 acl-2012-Fully Abstractive Approach to Guided Summarization</h1>
<br/><p>Source: <a title="acl-2012-101-pdf" href="http://aclweb.org/anthology//P/P12/P12-2069.pdf">pdf</a></p><p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>Reference: <a title="acl-2012-101-reference" href="../acl2012_reference/acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca ro  Abstract This paper shows that full abstraction can be accomplished in the context of guided summarization. [sent-6, score-0.485]
</p><p>2 We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. [sent-7, score-0.186]
</p><p>3 1 Introduction In the last decade, automatic text summarization has been dominated by extractive approaches that rely purely on shallow statistics. [sent-9, score-0.341]
</p><p>4 The field is also getting saturated near what appears to be a ceiling in performance. [sent-11, score-0.087]
</p><p>5 , 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. [sent-14, score-0.183]
</p><p>6 Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al. [sent-15, score-0.458]
</p><p>7 , 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). [sent-16, score-0.037]
</p><p>8 gov/tac 354 rewriting techniques based on syntactical analysis, offering little improvement over extractive methods in the content selection process. [sent-19, score-0.398]
</p><p>9 We believe that a fully abstractive approach with a separate process for the analysis of the text, the content selection, and the generation of the summary has the most potential for generating summaries at a  level comparable to human. [sent-20, score-1.031]
</p><p>10 For the foreseeable future, we think that such a process for full abstraction is impossible in the general case, since it is almost equivalent to perfect text understanding. [sent-21, score-0.29]
</p><p>11 In specific domains, however, an approximation of full abstraction is possible. [sent-22, score-0.29]
</p><p>12 This paper shows that full abstraction can be accomplished in the context of guided summarization. [sent-23, score-0.485]
</p><p>13 We propose a methodology that relies on Information Extraction and Natural Language Generation, and discuss our early results. [sent-24, score-0.037]
</p><p>14 2  Guided Summarization  The stated goal of the guided summarization task at TAC is to motivate a move towards abstractive approaches. [sent-25, score-0.875]
</p><p>15 It is an oriented multidocument summarization task in which a category is attributed to a cluster of 10 source documents to be summarized in 100 words or less. [sent-26, score-0.446]
</p><p>16 Each category is associated with a list of aspects to address in the summary. [sent-28, score-0.196]
</p><p>17 Figure 1  shows the aspects for the Attacks category. [sent-29, score-0.06]
</p><p>18 We use this specification ofcategories and aspects to accomplish domain-specific summarization. [sent-30, score-0.06]
</p><p>19 4 PERPETRATORS: individuals or groups responsible for the attack 2. [sent-37, score-0.252]
</p><p>20 6 WHO AFFECTED: casualties (death, injury), or individuals otherwise negatively affected 2. [sent-39, score-0.095]
</p><p>21 The idea to use an IE system for summarization can be traced back to the FRUMP system (DeJong, 1982), which generates brief summaries about various kinds of stories; (White et al. [sent-42, score-0.51]
</p><p>22 , 2001) also wrote abstractive summaries using the output of an IE system applied to events such as natural disasters. [sent-43, score-0.691]
</p><p>23 In both cases, the end result is a generated summary from the information available. [sent-44, score-0.084]
</p><p>24 What is common to all these approaches is that the IE system is designed for a specific purpose, separate from summarization. [sent-47, score-0.04]
</p><p>25 However, to properly address each aspect requires a system designed specifically for that task. [sent-48, score-0.121]
</p><p>26 To our knowledge, tailoring IE to the needs of abstractive summarization has not been done before. [sent-49, score-0.685]
</p><p>27 Our methodology uses a rule-based, custom-designed IE module, integrated with Content Selection and Generation in order to write short, well-written abstractive summaries. [sent-50, score-0.47]
</p><p>28 Before tackling these, we perform some preprocessing on the cluster of documents. [sent-51, score-0.056]
</p><p>29 It includes: cleaning up and normalization ofthe input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al. [sent-52, score-0.037]
</p><p>30 , 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al. [sent-53, score-0.123]
</p><p>31 We have also developed a date resolution engine that focuses on days of the week and relative terms. [sent-56, score-0.06]
</p><p>32 An abstraction scheme consists of IE rules, content selection heuristics and one or more generation patterns, all created by hand. [sent-59, score-0.717]
</p><p>33 Each abstraction scheme is designed to address a theme or subcategory. [sent-60, score-0.476]
</p><p>34 Thus, rules that extract information for the same aspect within the same scheme will share a similar meaning. [sent-61, score-0.204]
</p><p>35 An abstraction scheme aims to answer one or more aspects of its category, and more than one scheme can be linked to the same aspect. [sent-62, score-0.574]
</p><p>36 Figure 2 shows two of the schemes that we have created. [sent-63, score-0.11]
</p><p>37 For the scheme killing, the IE rules would match X as the perpetrator and Y as a victim for all of the following phrases: X kil led Y, Y was as s as s inated by X, and the murder o f X by Y. [sent-64, score-0.244]
</p><p>38 Other schemes have similar structure and purpose, such as wounding, abducting, damaging and destroying. [sent-65, score-0.11]
</p><p>39 To create extraction rules for a scheme, we must find several verbs and nouns sharing a similar meaning and identify the syntactical position of the roles we are interested in. [sent-66, score-0.296]
</p><p>40 Three resources have helped us in designing extraction rules: a thesaurus to find semantically related nouns and  verbs; VerbNet (Kipper et al. [sent-67, score-0.09]
</p><p>41 , 2006), which provides amongst other things the semantic roles of the syntactical dependents of verbs; and a hand-crafted list of aspect-relevant word stems provided by the team that made CLASSY (Conroy et al. [sent-68, score-0.153]
</p><p>42 Schemes and their extraction rules can also be quite different from this first example, as shown with the scheme event. [sent-70, score-0.216]
</p><p>43 This scheme gathers the basic information about the attack event: WHAT category of attack, WHEN and WHERE it occurred. [sent-71, score-0.383]
</p><p>44 A list of key words is used to identify words that imply an attack event, while a list of EVENT NOUNs is used to identify specifically words that refer to a type of attack. [sent-72, score-0.302]
</p><p>45 The information extraction rules translate preprocessing annotations into candidate answers for a specific aspect. [sent-74, score-0.104]
</p><p>46 Content selection determines which candidate will be included in the generated sentence for each aspect. [sent-75, score-0.122]
</p><p>47 Note that the predicate DEP matches any syntactical dependency and that key words refer to a premade list of category-relevant verbs and nouns. [sent-78, score-0.234]
</p><p>48 2  Content Selection  A large number of candidates are found by the IE rules for each aspect. [sent-80, score-0.045]
</p><p>49 The content selection module selects the best ones and sends them to the generation module. [sent-81, score-0.319]
</p><p>50 More than one candidate may be selected for the aspect WHO AFFECTED, the victims of the attack. [sent-83, score-0.047]
</p><p>51 Several heuristics are used to avoid redundancies and uninformative answers. [sent-84, score-0.037]
</p><p>52 News articles may contain references to more than one event of a given category, but our summaries describe only one. [sent-85, score-0.336]
</p><p>53 To avoid mixing candidates from two different event instances that might appear in the same cluster of documents, we rely on dates. [sent-86, score-0.134]
</p><p>54 The ancestors of a date in the dependency tree are associated with that date, and excluded from the summary if the main event occurs on a different date. [sent-87, score-0.222]
</p><p>55 3  Generation  The text of a summary must be fluid and feel natural, while being straightforward and concise. [sent-89, score-0.084]
</p><p>56 Thus, we have designed straightforward generation patterns for each scheme. [sent-91, score-0.132]
</p><p>57 They are implemented using the SimpleNLG realizer (Gatt and Reiter, 2009), which takes a sentence structure and words in their root form as input and gives a sentence with resolved agreements and sentence markers as output. [sent-92, score-0.148]
</p><p>58 The content selection module selects a lemma that should serve as noun phrase head, and its number, modifiers and specifier must be determined during generation. [sent-94, score-0.311]
</p><p>59 Frequencies and heuristics are again used to identify appropriate modifiers, this time from all those used with that head within the source documents. [sent-95, score-0.037]
</p><p>60 We apply the constraint that the On April 20, 1999, a massacre occurred at Columbine High School. [sent-96, score-0.137]
</p><p>61 On November 2, 2004, a brutal murder occurred in Amsterdam. [sent-98, score-0.158]
</p><p>62 On February 14,  2005,  a suicide car bombing occurred in Beirut. [sent-101, score-0.071]
</p><p>63 Figure 3: Brief fully abstractive  summaries  on clusters D1001A-A, D1039G-A  and D1043H-A, respectively  Columbine massacre, the murder of Theo van Gogh and the assassination of Rafik Hariri. [sent-103, score-0.841]
</p><p>64 on the  combination of number and modifiers chosen must appear at least once as an IE rule match. [sent-104, score-0.055]
</p><p>65 As for any generated text, a good summary also requires a text plan (Hovy, 1988) (McKeown, 1985). [sent-105, score-0.084]
</p><p>66 For example, an Attack summary begins with the scheme event. [sent-107, score-0.196]
</p><p>67 This ordering also determines which scheme to favor in the case of redundancy, e. [sent-108, score-0.112]
</p><p>68 given that a building was both damaged and destroyed, only the fact that is was destroyed will be mentioned. [sent-110, score-0.058]
</p><p>69 4  Results and Discussion  We have implemented this fully abstractive summarization methodology. [sent-111, score-0.748]
</p><p>70 The abstraction schemes and text plan for the Attack category are written in an XML document, designed to easily allow the addition of more schemes and the design of new categories. [sent-112, score-0.622]
</p><p>71 Our system, which is meant as a proof of concept, can generate useful summaries for the Attack category, as can be seen in Figure 3. [sent-114, score-0.258]
</p><p>72 The key elements of information are present in each case, stated in a  way that is easy to understand. [sent-115, score-0.077]
</p><p>73 These short summaries have a high density of information, in terms of how much content from the source documents they cover for a given number of words. [sent-116, score-0.42]
</p><p>74 For example, using the most widely used content metric, Pyramid (Nenkova et al. [sent-117, score-0.101]
</p><p>75 , 2007), the two sentences generated for the cluster D1001AA contain 8 Semantic Content Units (SCU) for a weighted total of 30 out of a maximum of 56, for a raw Pyramid score of 0. [sent-118, score-0.056]
</p><p>76 Only 3 of the 43 automatic summaries beat this score on this cluster that year (the average was 0. [sent-120, score-0.314]
</p><p>77 Note that the summaries that we compare against contain up to 100 357 words, whereas ours is only 21 words long. [sent-122, score-0.258]
</p><p>78 We conclude that our method has the potential for creating summaries with much greater information density than the current state of the art. [sent-123, score-0.319]
</p><p>79 5  Conclusion and Future Work  We have developed and implemented a fully abstractive summarization methodology in the context of guided summarization. [sent-125, score-0.941]
</p><p>80 The higher density of information in our short summaries is one key to address the performance ceiling of extractive summarization methods. [sent-126, score-0.824]
</p><p>81 Although fully abstractive summarization is a daunting challenge, our work shows the feasibility and usefulness of this new direction for summarization research. [sent-127, score-1.0]
</p><p>82 We are now expanding the variety and complexity of the abstraction schemes and generation patterns to deal with more aspects and other categories. [sent-128, score-0.552]
</p><p>83 Kathy McKeown, of Columbia University, for fruitful discussions on abstractive summarization, and Dr. [sent-132, score-0.433]
</p><p>84 John  Conroy, both of the IDA / Center for Computing Sciences, for providing us with their hand-crafted list of category- and aspect-relevant keywords. [sent-134, score-0.03]
</p><p>85 Incorporating non-local information into information extraction systems by Gibbs sampling. [sent-178, score-0.059]
</p><p>86 The pyramid method: Incorporating human content selection variation in summarization evaluation. [sent-227, score-0.508]
</p><p>87 Overview of the TAC 2011 summarization track: Guided task and aesop task. [sent-234, score-0.252]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('abstractive', 0.433), ('abstraction', 0.29), ('summaries', 0.258), ('summarization', 0.252), ('attack', 0.199), ('genest', 0.166), ('guided', 0.156), ('ie', 0.137), ('syntactical', 0.123), ('scheme', 0.112), ('schemes', 0.11), ('tac', 0.102), ('content', 0.101), ('lapalme', 0.099), ('generation', 0.092), ('extractive', 0.089), ('ceiling', 0.087), ('killing', 0.087), ('murder', 0.087), ('selection', 0.085), ('summary', 0.084), ('conroy', 0.079), ('attacks', 0.079), ('event', 0.078), ('guy', 0.074), ('category', 0.072), ('occurred', 0.071), ('gaithersburg', 0.07), ('pyramid', 0.07), ('multidocument', 0.066), ('columbine', 0.066), ('countermeasures', 0.066), ('damages', 0.066), ('dejong', 0.066), ('frump', 0.066), ('gatt', 0.066), ('massacre', 0.066), ('rafik', 0.066), ('simplenlg', 0.066), ('fully', 0.063), ('density', 0.061), ('date', 0.06), ('aspects', 0.06), ('maryland', 0.059), ('barzilay', 0.059), ('extraction', 0.059), ('schlesinger', 0.058), ('tanaka', 0.058), ('montr', 0.058), ('destroyed', 0.058), ('cluster', 0.056), ('modifiers', 0.055), ('standards', 0.055), ('classy', 0.053), ('cunningham', 0.053), ('gate', 0.053), ('individuals', 0.053), ('theo', 0.053), ('mckeown', 0.051), ('kathleen', 0.049), ('judith', 0.049), ('owczarzak', 0.049), ('kipper', 0.049), ('aspect', 0.047), ('nenkova', 0.047), ('fusion', 0.047), ('eal', 0.047), ('rules', 0.045), ('revision', 0.044), ('verbnet', 0.044), ('prep', 0.044), ('key', 0.043), ('affected', 0.042), ('module', 0.041), ('dep', 0.041), ('compression', 0.041), ('designed', 0.04), ('accomplished', 0.039), ('verbs', 0.038), ('heuristics', 0.037), ('sentence', 0.037), ('morristown', 0.037), ('markers', 0.037), ('marneffe', 0.037), ('methodology', 0.037), ('white', 0.036), ('cohn', 0.035), ('address', 0.034), ('stroudsburg', 0.034), ('stated', 0.034), ('pa', 0.033), ('finkel', 0.031), ('nouns', 0.031), ('list', 0.03), ('nj', 0.03), ('eduard', 0.03), ('ji', 0.03), ('stanford', 0.03), ('lemma', 0.029), ('tni', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="101-tfidf-1" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>2 0.22318566 <a title="101-tfidf-2" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>3 0.1913901 <a title="101-tfidf-3" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>Author: Karolina Owczarzak ; Peter A. Rankel ; Hoa Trang Dang ; John M. Conroy</p><p>Abstract: We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.</p><p>4 0.10347703 <a title="101-tfidf-4" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>Author: Joel Nothman ; Matthew Honnibal ; Ben Hachey ; James R. Curran</p><p>Abstract: Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement.</p><p>5 0.097343996 <a title="101-tfidf-5" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>Author: Dong Wang ; Xian Qian ; Yang Liu</p><p>Abstract: This paper presents a two-step approach to compress spontaneous spoken utterances. In the first step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences. In the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them. For evaluation, we compare our system output with multiple human references. Our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references.</p><p>6 0.078927495 <a title="101-tfidf-6" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>7 0.070810154 <a title="101-tfidf-7" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>8 0.068309516 <a title="101-tfidf-8" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>9 0.067020588 <a title="101-tfidf-9" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>10 0.061628483 <a title="101-tfidf-10" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>11 0.060916144 <a title="101-tfidf-11" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<p>12 0.059494957 <a title="101-tfidf-12" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>13 0.059108276 <a title="101-tfidf-13" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>14 0.057100993 <a title="101-tfidf-14" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>15 0.056915171 <a title="101-tfidf-15" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>16 0.05526717 <a title="101-tfidf-16" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>17 0.054284967 <a title="101-tfidf-17" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<p>18 0.054240264 <a title="101-tfidf-18" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>19 0.052161366 <a title="101-tfidf-19" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>20 0.05046175 <a title="101-tfidf-20" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.166), (1, 0.08), (2, -0.047), (3, 0.064), (4, 0.018), (5, -0.035), (6, -0.044), (7, 0.015), (8, -0.079), (9, 0.064), (10, -0.086), (11, 0.008), (12, 0.002), (13, -0.003), (14, -0.084), (15, 0.054), (16, 0.102), (17, -0.036), (18, -0.055), (19, -0.002), (20, 0.2), (21, -0.02), (22, 0.307), (23, 0.031), (24, 0.04), (25, 0.142), (26, -0.061), (27, -0.054), (28, 0.147), (29, 0.022), (30, 0.14), (31, 0.124), (32, -0.024), (33, -0.027), (34, -0.182), (35, 0.057), (36, 0.063), (37, 0.096), (38, -0.107), (39, 0.073), (40, -0.091), (41, -0.061), (42, -0.026), (43, -0.084), (44, -0.081), (45, 0.026), (46, 0.056), (47, 0.029), (48, 0.051), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94472933 <a title="101-lsi-1" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>2 0.86275673 <a title="101-lsi-2" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>Author: Karolina Owczarzak ; Peter A. Rankel ; Hoa Trang Dang ; John M. Conroy</p><p>Abstract: We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.</p><p>3 0.74400938 <a title="101-lsi-3" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>4 0.3835279 <a title="101-lsi-4" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>Author: Yunyao Li ; Laura Chiticariu ; Huahai Yang ; Frederick Reiss ; Arnaldo Carreno-fuentes</p><p>Abstract: Information extraction (IE) is becoming a critical building block in many enterprise applications. In order to satisfy the increasing text analytics demands of enterprise applications, it is crucial to enable developers with general computer science background to develop high quality IE extractors. In this demonstration, we present WizIE, an IE development environment intended to reduce the development life cycle and enable developers with little or no linguistic background to write high quality IE rules. WizI E provides an integrated wizard-like environment that guides IE developers step-by-step throughout the entire development process, based on best practices synthesized from the experience of expert developers. In addition, WizIE reduces the manual effort involved in performing key IE development tasks by offering automatic result explanation and rule discovery functionality. Preliminary results indicate that WizI E is a step forward towards enabling extractor development for novice IE developers.</p><p>5 0.34325597 <a title="101-lsi-5" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>Author: Remy Kessler ; Xavier Tannier ; Caroline Hagege ; Veronique Moriceau ; Andre Bittar</p><p>Abstract: We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related.</p><p>6 0.33156362 <a title="101-lsi-6" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>7 0.29059222 <a title="101-lsi-7" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>8 0.27786562 <a title="101-lsi-8" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>9 0.26626909 <a title="101-lsi-9" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>10 0.26327962 <a title="101-lsi-10" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>11 0.26140571 <a title="101-lsi-11" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>12 0.26081896 <a title="101-lsi-12" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>13 0.25961301 <a title="101-lsi-13" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>14 0.25696179 <a title="101-lsi-14" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>15 0.25016308 <a title="101-lsi-15" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>16 0.24968505 <a title="101-lsi-16" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>17 0.23534849 <a title="101-lsi-17" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>18 0.2334573 <a title="101-lsi-18" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>19 0.22738795 <a title="101-lsi-19" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>20 0.22547962 <a title="101-lsi-20" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.022), (26, 0.04), (28, 0.024), (30, 0.011), (37, 0.021), (39, 0.038), (59, 0.016), (74, 0.032), (82, 0.013), (84, 0.021), (85, 0.027), (90, 0.077), (92, 0.032), (94, 0.015), (99, 0.546)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97966731 <a title="101-lda-1" href="./acl-2012-Reducing_Wrong_Labels_in_Distant_Supervision_for_Relation_Extraction.html">169 acl-2012-Reducing Wrong Labels in Distant Supervision for Relation Extraction</a></p>
<p>Author: Shingo Takamatsu ; Issei Sato ; Hiroshi Nakagawa</p><p>Abstract: In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision. When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. However, this heuristic can fail with the result that some sentences are labeled wrongly. This noisy labeled data causes poor extraction performance. In this paper, we propose a method to reduce the number of wrong labels. We present a novel generative model that directly models the heuristic labeling process of distant supervision. The model predicts whether assigned labels are correct or wrong via its hidden variables. Our experimental results show that this model detected wrong labels with higher performance than baseline methods. In the ex- periment, we also found that our wrong label reduction boosted the performance of relation extraction.</p><p>2 0.97463065 <a title="101-lda-2" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>Author: Alexandre Davis ; Adriano Veloso ; Altigran Soares ; Alberto Laender ; Wagner Meira Jr.</p><p>Abstract: The named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique realworld entity. This task can be modeled as a classification problem, provided that positive and negative examples are available for learning binary classifiers. High-quality senseannotated data, however, are hard to be obtained in streaming environments, since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream. On the other hand, few positive examples plus large amounts of unlabeled data may be easily acquired. Producing binary classifiers directly from this data, however, leads to poor disambiguation performance. Thus, we propose to enhance the quality of the classifiers using finer-grained variations of the well-known ExpectationMaximization (EM) algorithm. We conducted a systematic evaluation using Twitter streaming data and the results show that our classifiers are extremely effective, providing improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster.</p><p>3 0.94696057 <a title="101-lda-3" href="./acl-2012-Movie-DiC%3A_a_Movie_Dialogue_Corpus_for_Research_and_Development.html">149 acl-2012-Movie-DiC: a Movie Dialogue Corpus for Research and Development</a></p>
<p>Author: Rafael E. Banchs</p><p>Abstract: This paper describes Movie-DiC a Movie Dialogue Corpus recently collected for research and development purposes. The collected dataset comprises 132,229 dialogues containing a total of 764,146 turns that have been extracted from 753 movies. Details on how the data collection has been created and how it is structured are provided along with its main statistics and characteristics. 1</p><p>4 0.94462317 <a title="101-lda-4" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>Author: Elena Cabrio ; Serena Villata</p><p>Abstract: Blogs and forums are widely adopted by online communities to debate about various issues. However, a user that wants to cut in on a debate may experience some difficulties in extracting the current accepted positions, and can be discouraged from interacting through these applications. In our paper, we combine textual entailment with argumentation theory to automatically extract the arguments from debates and to evaluate their acceptability.</p><p>same-paper 5 0.91261578 <a title="101-lda-5" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>6 0.89047045 <a title="101-lda-6" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>7 0.61477822 <a title="101-lda-7" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>8 0.60318846 <a title="101-lda-8" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>9 0.59911311 <a title="101-lda-9" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>10 0.55982286 <a title="101-lda-10" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>11 0.5506258 <a title="101-lda-11" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>12 0.54969227 <a title="101-lda-12" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>13 0.54239398 <a title="101-lda-13" href="./acl-2012-Graph-based_Semi-Supervised_Learning_Algorithms_for_NLP.html">104 acl-2012-Graph-based Semi-Supervised Learning Algorithms for NLP</a></p>
<p>14 0.53035605 <a title="101-lda-14" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>15 0.52465224 <a title="101-lda-15" href="./acl-2012-PDTB-style_Discourse_Annotation_of_Chinese_Text.html">157 acl-2012-PDTB-style Discourse Annotation of Chinese Text</a></p>
<p>16 0.52307308 <a title="101-lda-16" href="./acl-2012-Multilingual_Subjectivity_and_Sentiment_Analysis.html">151 acl-2012-Multilingual Subjectivity and Sentiment Analysis</a></p>
<p>17 0.50793874 <a title="101-lda-17" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>18 0.5078038 <a title="101-lda-18" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>19 0.50757802 <a title="101-lda-19" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>20 0.5053345 <a title="101-lda-20" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
