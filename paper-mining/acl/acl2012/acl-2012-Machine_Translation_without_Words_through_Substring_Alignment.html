<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 acl-2012-Machine Translation without Words through Substring Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-140" href="#">acl2012-140</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>140 acl-2012-Machine Translation without Words through Substring Alignment</h1>
<br/><p>Source: <a title="acl-2012-140-pdf" href="http://aclweb.org/anthology//P/P12/P12-1018.pdf">pdf</a></p><p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>Reference: <a title="acl-2012-140-reference" href="../acl2012_reference/acl-2012-Machine_Translation_without_Words_through_Substring_Alignment_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transl', 0.341), ('align', 0.312), ('itg', 0.302), ('neubig', 0.226), ('morpholog', 0.176), ('pcooc', 0.17), ('saer', 0.141), ('phrasal', 0.138), ('subst', 0.128), ('ak', 0.127), ('bipars', 0.113), ('fuv', 0.113), ('beam', 0.113), ('invert', 0.113), ('prob', 0.111), ('kyoto', 0.104), ('vil', 0.104), ('compound', 0.104), ('span', 0.094), ('characterbas', 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="140-tfidf-1" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>2 0.30931488 <a title="140-tfidf-2" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>3 0.28707188 <a title="140-tfidf-3" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>4 0.26387179 <a title="140-tfidf-4" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>5 0.246509 <a title="140-tfidf-5" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>6 0.24144568 <a title="140-tfidf-6" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>7 0.23485562 <a title="140-tfidf-7" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>8 0.23481949 <a title="140-tfidf-8" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>9 0.22592854 <a title="140-tfidf-9" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>10 0.21513084 <a title="140-tfidf-10" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>11 0.1962928 <a title="140-tfidf-11" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>12 0.18969266 <a title="140-tfidf-12" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>13 0.18800667 <a title="140-tfidf-13" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>14 0.18496469 <a title="140-tfidf-14" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>15 0.18432283 <a title="140-tfidf-15" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>16 0.18362065 <a title="140-tfidf-16" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>17 0.16634429 <a title="140-tfidf-17" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>18 0.16325176 <a title="140-tfidf-18" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>19 0.1617821 <a title="140-tfidf-19" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>20 0.15933786 <a title="140-tfidf-20" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.398), (1, -0.334), (2, 0.135), (3, 0.023), (4, 0.0), (5, 0.098), (6, -0.056), (7, -0.014), (8, -0.002), (9, 0.011), (10, -0.038), (11, 0.014), (12, -0.036), (13, 0.036), (14, -0.087), (15, -0.14), (16, -0.064), (17, -0.117), (18, 0.033), (19, 0.006), (20, 0.067), (21, -0.014), (22, 0.042), (23, -0.077), (24, -0.012), (25, -0.041), (26, 0.024), (27, 0.021), (28, 0.045), (29, 0.048), (30, 0.033), (31, 0.093), (32, 0.004), (33, 0.017), (34, 0.161), (35, 0.017), (36, -0.008), (37, -0.049), (38, 0.023), (39, 0.029), (40, 0.046), (41, 0.059), (42, -0.062), (43, 0.014), (44, -0.005), (45, 0.078), (46, 0.041), (47, -0.009), (48, 0.053), (49, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96657693 <a title="140-lsi-1" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>2 0.89542216 <a title="140-lsi-2" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>3 0.89032596 <a title="140-lsi-3" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>Author: Ning Xi ; Guangchao Tang ; Xinyu Dai ; Shujian Huang ; Jiajun Chen</p><p>Abstract: The dominant practice of statistical machine translation (SMT) uses the same Chinese word segmentation specification in both alignment and translation rule induction steps in building Chinese-English SMT system, which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses two different segmentation specifications for alignment and translation respectively: we use Chinese character as the basic unit for alignment, and then convert this alignment to conventional word alignment for translation rule induction. Experimentally, our approach outperformed two baselines: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance. 1Introduction Chinese Word segmentation is a necessary step in Chinese-English statistical machine translation (SMT) because Chinese sentences do not delimit words by spaces. The key characteristic of a Chinese word segmenter is the segmentation specification1. As depicted in Figure 1(a), the dominant practice of SMT uses the same word segmentation for both word alignment and translation rule induction. For brevity, we will refer to the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. 285 (a) WSA=WSR (b) WSA≠WSR Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word alignment for translation rule induction. In the 2 Interestingly, word is also a basic token in syntax-based rules. Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A.s ?c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi2c 8s5–290, experiment, our approach consistently outperformed two baselines with three different word segmenters: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance. The remainder of this paper is structured as follows: Section 2 analyzes the influences of WSA and WSR on SMT respectively; Section 3 discusses how to convert character alignment to word alignment; Section 4 presents experimental results, followed by conclusions and future work in section 5. 2 Understanding WSA and WSR We propose a solution to tackle the suboptimal problem: using Chinese character for alignment while using Chinese word for translation. Character alignment differs from conventional word alignment in the basic tokens of the Chinese side of the training corpus3. Table 1 compares the token distributions of character-based corpus (CCorpus) and word-based corpus (WCorpus). We see that the WCorpus has a longer-tailed distribution than the CCorpus. More than 70% of the unique tokens appear less than 5 times in WCorpus. However, over half of the tokens appear more than or equal to 5 times in the CCorpus. This indicates that modeling word alignment could suffer more from data sparsity than modeling character alignment. Table 2 shows the numbers of the unique tokens (#UT) and unique bilingual token pairs (#UTP) of the two corpora. Consider two extensively features, fertility and translation features, which are extensively used by many state-of-the-art word aligners. The number of parameters w.r.t. fertility features grows linearly with #UT while the number of parameters w.r.t. translation features grows linearly with #UTP. We compare #UT and #UTP of both corpora in Table 2. As can be seen, CCorpus has less UT and UTP than WCorpus, i.e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in statistical modeling (Collins, 1999). Another advantage of character alignment is the reduction in alignment errors caused by word seg3 Several works have proposed to use character (letter) on both sides of the parallel corpus for SMT between similar (European) languages (Vilar et al., 2007; Tiedemann, 2009), however, Chinese is not similar to English. 286 Frequency Characters (%) Words (%) 1 27.22 45.39 2 11.13 14.61 3 6.18 6.47 4 4.26 4.32 5(+) 50.21 29.21 Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, “切尼 (Cheney)” and “愿 (will)” are wrongly merged into one word 切 尼 by the word segmenter, and 切 尼 wrongly aligns to a comma in English sentence in the word alignment; However, both 切 and 尼 align to “Cheney” correctly in the character alignment. However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word (Duan et al., 2010). As character could preserve more meanings than word in Chinese, it seems that a character can be wrongly aligned to many English words by the aligner. However, we found this can be avoided to a great extent by the basic features (co-occurrence and distortion) used by many alignment models. For example, we observed that the four characters of the non-compositional word “阿拉法特 (Arafat)” align to Arafat correctly, although these characters preserve different meanings from that of Arafat. This can be attributed to the frequent co-occurrence (192 愿 愿 times) of these characters and Arafat in CCorpus. Moreover, 法 usually means France in Chinese, thus it may co-occur very often with France in CCorpus. If both France and Arafat appear in the English sentence, 法 may wrongly align to France. However, if 阿 aligns to Arafat, 法 will probably align to Arafat, because aligning 法 to Arafat could result in a lower distortion cost than aligning it to France. Different from alignment, translation is a pattern matching procedure (Lopez, 2008). WSR determines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word 阿拉法特, i.e. translating literally, we may get a wrong translation. That’s because the linguistic knowledge that the four characters convey a specific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sentence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conventional word, as WSR is quite necessary. 3 Converting Character Alignment to Word Alignment In order to use word as WSR, we employ the same method as Elming and Habash (2007)4 to convert the character alignment (CA) to its word-based version (CA ’) for translation rule induction. The conversion is very intuitive: for every English-Chinese word pair ??, ?? in the sentence pair, we align ? to ? as a link in CA ’, if and only if there is at least one Chinese character of ? aligns to ? in CA. Given two different segmentations A and B of the same sentence, it is easy to prove that if every word in A is finer-grained than the word of B at the corresponding position, the conversion is unambiguity (we omit the proof due to space limitation). As character is a finer-grained than its original word, character alignment can always be converted to alignment based on any word segmentation. Therefore, our approach can be naturally scaled to syntax-based system by converting character alignment to word alignment where the word seg- mentation is consistent with the parsers. We compare CA with the conventional word alignment (WA) as follows: We hand-align some sentence pairs as the evaluation set based on characters (ESChar), and converted it to the evaluation set based on word (ESWord) using the above conversion method. It is worth noting that comparing CA and WA by evaluating CA on ESChar and evaluating WA on ESWord is meaningless, because the basic tokens in CA and WA are different. However, based on the conversion method, comparing CA with WA can be accomplished by evaluating both CA ’ and WA on ESWord. 4 They used this conversion for word alignment combination only, no translation results were reported. 287 4 Experiments 4.1 Setup FBIS corpus (LDC2003E14) (210K sentence pairs) was used for small-scale task. A large bilingual corpus of our lab (1.9M sentence pairs) was used for large-scale task. The NIST’06 and NIST’08 test sets were used as the development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing WSA and WSR. We then evaluated the translation performance. The baselines are fully word-based MT systems (WordSys), i.e. using word as both WSA and WSR, and fully character-based systems (CharSys). Table 5 http://www.ictclas.org/ 6 http://nlp.stanford.edu/software/segmenter.shtml TLSablCIPeKT3BUAlig87 n609P5mW.0162eonrdt8avl52R01ai.g l6489numatieo78n29F t. 46590PrecC87 i1hP28s.oa3027rn(ctPe89)r6R05,.ar7162e3licganm8 (15F62eR.n983)t, TableSL4TWwrcahonSraAdslatioWw no SerdRvalu2Ct31iT.o405Bn1724ofW2P 301Ko.895rU61d Sy2sI03Ca.29nT035d4 proand F-score (F) with ? ? 0.5 (Fraser and Marcu, 2007) posed system using BLEU-SBP (Chiang et al., 2008) 4 compares WordSys to our proposed system. Significant testing was carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level. We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010). Comparing Table 4 and 5, we see that all CharSys underperformed WordSys. This observation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys underperformed our proposed systems, that’s because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa. We finally compared our approaches to Ma et al. (2007) and Ma and Way (2009), which proposed “packed word (PW)” and “bilingual motivated word (BS)” respectively. Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in small- 288 Context Size 7 9 11 13 BLEU 20.90 21.19 20.89 21.09 Table 5 Translation evaluation of CharSys. CWPhrSoayps+TdoPtSaBebWmySdsle6wWPcCBhoWSa rAmdpawWrPBisoWS rRdnwiC2t1hT.2504oB6the2r1P0w9K.2o178U496rk s2I10C.9T547 scale task, we see that both PW and BS underperformed our approach. This may be attributed to the low recall of the learned BS or PW in their approaches. BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques (Dyer et al., 2008) to tackle the low recall of BS, which was removed from our experiments for fair comparison. Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point). This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation performance. 5 Conclusions and Future Work We proposed a SMT framework that uses character for alignment and word for translation, which improved both alignment quality and translation performance. We believe that in this framework, using other finer-grained segmentation, with fewer ambiguities than character, would better parameterize the alignment models, while using other coarser-grained segmentation as WSR can help capture more linguistic knowledge than word to get better translation. We also believe that our approach, if integrated with combination techniques (Dyer et al., 2008; Xi et al., 2011), can yield better results. Acknowledgments We thank ACL reviewers. This work is supported by the National Natural Science Foundation of China (No. 61003 112), the National Fundamental Research Program of China (2010CB327903). References Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Peitra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), pages 263-3 11. Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of third workshop on SMT, pages 224-232. David Chiang, Steve DeNeefe, Yee Seng Chan and Hwee Tou Ng. 2008. Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 610-619. Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 718-726. Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania. Xiangyu Duan, Min Zhang, and Haizhou Li. 2010. Pseudo-word for phrase-based machine translation. In Proceedings of the Association for Computational Linguistics, pages 148-156. Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of the Association for Computational Linguistics, pages 1012-1020. Jakob Elming and Nizar Habash. 2007. Combination of statistical word alignments based on multiple preprocessing schemes. In Proceedings of the Association for Computational Linguistics, pages 25-28. Alexander Fraser and Daniel Marcu. 2007. Squibs and Discussions: Measuring Word Alignment Quality for Statistical Machine Translation. In Computational Linguistics, 33(3), pages 293-303. Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of the Association for Computational Linguistics, pages 923-93 1. Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan,W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Association for Computational Linguistics, pages 177-1 80. 289 Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 388-395. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit. Adam David Lopez. 2008. Machine translation by pattern matching. Ph.D. thesis, University of Maryland. Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the Association for Computational Linguistics, pages 304-3 11. Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of the Conference of the European Chapter of the ACL, pages 549-557. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Association for Computational Linguistics, pages 440-447. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), pages 19-5 1. Michael Paul, Andrew Finch and Eiichiro Sumita. 2010. Integration of multiple bilingually-learned segmentation schemes into statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 400-408. Jörg Tiedemann. 2009. Character-based PSMT for closely related languages. In Proceedings of the Annual Conference of the European Association for machine Translation, pages 12-19. David Vilar, Jan-T. Peter and Hermann Ney. 2007. Can we translate letters? In Proceedings of the Second Workshop on Statistical Machine Translation, pages 33-39. Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu and Shouxun Lin. 2010. Joint tokenization and translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1200-1208. Ning Xi, Guangchao Tang, Boyuan Li, and Yinggong Zhao. 2011. Word alignment combination over multiple word segmentation. In Proceedings of the ACL 2011 Student Session, pages 1-5. Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. of the Third Workshop on Statistical Machine Translation, pages 216-223. 290 In Proceedings</p><p>4 0.86168575 <a title="140-lsi-4" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>Author: Darcey Riley ; Daniel Gildea</p><p>Abstract: Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score.</p><p>5 0.85100782 <a title="140-lsi-5" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>6 0.79418778 <a title="140-lsi-6" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>7 0.72903478 <a title="140-lsi-7" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>8 0.69545341 <a title="140-lsi-8" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>9 0.69255483 <a title="140-lsi-9" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>10 0.68145555 <a title="140-lsi-10" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>11 0.67575496 <a title="140-lsi-11" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>12 0.67118591 <a title="140-lsi-12" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>13 0.64048755 <a title="140-lsi-13" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>14 0.63834524 <a title="140-lsi-14" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>15 0.62941879 <a title="140-lsi-15" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>16 0.62344593 <a title="140-lsi-16" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>17 0.61711526 <a title="140-lsi-17" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>18 0.61489385 <a title="140-lsi-18" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>19 0.60755724 <a title="140-lsi-19" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>20 0.59592456 <a title="140-lsi-20" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.036), (8, 0.041), (18, 0.012), (22, 0.053), (31, 0.043), (38, 0.029), (47, 0.042), (49, 0.017), (58, 0.047), (61, 0.063), (77, 0.046), (91, 0.381), (92, 0.052), (96, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97882164 <a title="140-lda-1" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>2 0.97545701 <a title="140-lda-2" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>Author: Pei Yang ; Wei Gao ; Qi Tan ; Kam-Fai Wong</p><p>Abstract: We use multiple views for cross-domain document classification. The main idea is to strengthen the views’ consistency for target data with source training data by identifying the correlations of domain-specific features from different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM) based on a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated domain-specific features from both sides and iteratively boost the consistency between document clusterings based on word and link views. Experiments show that IMAM significantly outperforms state-of-the-art baselines.</p><p>same-paper 3 0.97529691 <a title="140-lda-3" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>4 0.96715069 <a title="140-lda-4" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>5 0.96565998 <a title="140-lda-5" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>Author: Wen Chan ; Xiangdong Zhou ; Wei Wang ; Tat-Seng Chua</p><p>Abstract: We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of “incomplete answer”, i.e., the “best answer” of a complex multi-sentence question misses valuable information that is contained in other answers. In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary method with group L1 regularization. Various textual and non-textual QA features are explored. Specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. To further unleash the potential of the abundant cQA features, we introduce the group L1 regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task.</p><p>6 0.95564115 <a title="140-lda-6" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>7 0.95043164 <a title="140-lda-7" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>8 0.92755842 <a title="140-lda-8" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>9 0.92549604 <a title="140-lda-9" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>10 0.8998577 <a title="140-lda-10" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>11 0.89350379 <a title="140-lda-11" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>12 0.88616025 <a title="140-lda-12" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>13 0.88293493 <a title="140-lda-13" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>14 0.88180685 <a title="140-lda-14" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>15 0.87529039 <a title="140-lda-15" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>16 0.87449837 <a title="140-lda-16" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>17 0.87440592 <a title="140-lda-17" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>18 0.87329572 <a title="140-lda-18" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>19 0.87135375 <a title="140-lda-19" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>20 0.8658464 <a title="140-lda-20" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
