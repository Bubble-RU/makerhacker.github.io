<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>108 acl-2012-Hierarchical Chunk-to-String Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-108" href="#">acl2012-108</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>108 acl-2012-Hierarchical Chunk-to-String Translation</h1>
<br/><p>Source: <a title="acl-2012-108-pdf" href="http://aclweb.org/anthology//P/P12/P12-1100.pdf">pdf</a></p><p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>Reference: <a title="acl-2012-108-reference" href="../acl2012_reference/acl-2012-Hierarchical_Chunk-to-String_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hierarchical Chunk-to-String Translation∗ Yang Feng† Dongdong Zhang‡ Mu Li‡ Ming Zhou‡ Qun Liu⋆ † Department of Computer Science ‡ Microsoft Research Asia University of Sheffield do zhang@mi cro s o ft . [sent-1, score-0.169]
</p><p>2 cn ct  Abstract We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. [sent-9, score-1.018]
</p><p>3 With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. [sent-10, score-0.592]
</p><p>4 Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. [sent-11, score-0.703]
</p><p>5 Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. [sent-12, score-0.548]
</p><p>6 ∗This work was done when the first author visited Microsoft Research Asia as an intern. [sent-15, score-0.045]
</p><p>7 950 However, it is often desirable to consider syntactic constituents of subphrases, e. [sent-16, score-0.134]
</p><p>8 the hierarchical phrase X  →  hX1 for X2, X2 de X1i  can be applied to both of the following strings in Figure 1  “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. [sent-18, score-0.5]
</p><p>9 In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. [sent-19, score-0.242]
</p><p>10 If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. [sent-20, score-0.112]
</p><p>11 , 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. [sent-23, score-0.224]
</p><p>12 Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. [sent-24, score-0.177]
</p><p>13 Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. [sent-25, score-0.426]
</p><p>14 Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. [sent-26, score-0.304]
</p><p>15 Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001 ; Eisner, 2003;  Graehl and Knight, 2004; Quirk et al. [sent-27, score-0.354]
</p><p>16 This will lead to data sparseness and being vulnerable to parse errors. [sent-33, score-0.193]
</p><p>17 In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two models. [sent-34, score-0.602]
</p><p>18 Instead of parse trees, our model introduces linguistic information in the form of chunks, so it does not need to care the internal structures and the roles in the main sentence of chunks. [sent-35, score-0.311]
</p><p>19 Based on shallow parsing results, it learns rules consisting of either words (terminals) or chunks (nonterminals), where adjacent chunks are packed into one nonterminal. [sent-36, score-0.842]
</p><p>20 It searches for the best derivation through the SCFG-motivated space defined by these rules and get target translation simultaneously. [sent-37, score-0.515]
</p><p>21 In some sense, our model can be seen as a compromise between the hierarchical phrase-based model and the tree-to-  string model, specifically •  •  •  Compared with the hierarchical phrase-based model, eitd integrates linguistic syntax saen-db sseatdisfies syntactic cohesion. [sent-38, score-0.913]
</p><p>22 Compared with the tree-to-string model, it only nCeoemdsp to perform seh tarleleo-wto parsing wodhieclh, i itn otnrolyduces less parsing errors. [sent-39, score-0.16]
</p><p>23 Besides, our model allows a nonterminal in a rule to cover several chunks, which can alleviate data sparseness and the influence of parsing errors. [sent-40, score-0.494]
</p><p>24 we refine our hierarchical chunk-to-string mwoede rle ifnintoe two rmo hdieerlsa:r a liocaolse model (Section 2. [sent-41, score-0.438]
</p><p>25 1) which is more similar to the hierarchical phrase-based model and a tight model (Section 2. [sent-42, score-0.588]
</p><p>26 The experiments show that on the 2008 NIST English-Chinese MT translation test set, both the loose model and the tight model outperform the hierarchical phrase-based model and the tree-to-string model, where the loose model has a better perfor-  mance. [sent-44, score-1.251]
</p><p>27 While in terms of speed, the tight model runs faster and its speed ranking is between the treeto-string model and the hierarchical phrase-based model. [sent-45, score-0.627]
</p><p>28 951 NP  IN  NP  IN  NP  VBD  VP  Agouremqauiestgfuofrenapduerchaseshenoqfingsharesbeiwasdimjia doe Tghaei bankyinhanghasyijnfgiledshenfqoirng banpokcruhapntcy 购买  NP  该  的  股份  VBZ  银行  申请  被  递交  (a)  VBN  已经 (b)  IN  NP  申请  破产  Figure 1: A running example of two sentences. [sent-46, score-0.048]
</p><p>29 For each sentence, the first row gives the chunk sequence. [sent-47, score-0.346]
</p><p>30 S NP VP  DT NN VBZ VP The bank has VBN PP filed IN NP for NN  bankruptcy  (a) A parse B-NP The  I-NP bank  B-VBZ has  (b) A chunk  tree  B-VBN filed  sequence got  B-IN for  B-NP bankruptcy  from the parse  tree  Figure 2: An example of shallow parsing. [sent-48, score-1.689]
</p><p>31 2  Modeling  Shallow parsing (also chunking) is an analysis of a sentence which identifies the constituents (noun groups, verbs, verb groups, etc), but neither specifies their internal structures, nor their roles in the main sentence. [sent-49, score-0.28]
</p><p>32 In Figure 1, we give the chunk sequence in the first row for each sentence. [sent-50, score-0.388]
</p><p>33 We treat shallow parsing as a sequence label task, and a sentence f can have many possible different chunk la-  bel sequences. [sent-51, score-0.604]
</p><p>34 A SCFG produces a derivation by starting with a pair of start symbols and recursively rewrites every two coindexed nonterminals with the corresponding components of a matched rule. [sent-54, score-0.391]
</p><p>35 A derivation yields a pair of strings on the right-hand side which are translation of each other. [sent-55, score-0.472]
</p><p>36 In a weighted SCFG, each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation. [sent-56, score-0.394]
</p><p>37 A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. [sent-57, score-0.341]
</p><p>38 We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. [sent-59, score-1.221]
</p><p>39 The two models differ in the form of rules and the way of estimating rule probabilities. [sent-60, score-0.171]
</p><p>40 While for decoding, we employ the same decoding  algorithm for the two models: given a test sentence, the decoders first perform shallow parsing to get the best chunk sequence, then apply a CYK parsing algorithm with beam search. [sent-61, score-0.688]
</p><p>41 1 A Loose Model In our model, we employ rules containing nonterminals to handle long-distance reordering where boundary words play an important role. [sent-63, score-0.363]
</p><p>42 So for the subphrases which cover more than one chunk, we just maintain boundary chunks: we bundle adjacent chunks into one nonterminal and denote it as the first chunk tag immediately followed by “-” and next followed by the last chunk tag. [sent-64, score-1.404]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('chunk', 0.3), ('hierarchical', 0.261), ('filed', 0.241), ('subphrases', 0.241), ('chunks', 0.221), ('np', 0.188), ('tight', 0.179), ('translation', 0.177), ('loose', 0.169), ('derivation', 0.164), ('bankruptcy', 0.158), ('scfg', 0.154), ('nonterminals', 0.143), ('shallow', 0.129), ('cohesion', 0.127), ('sheffield', 0.121), ('shenqing', 0.121), ('nonterminal', 0.116), ('vp', 0.109), ('rule', 0.1), ('cro', 0.095), ('merits', 0.09), ('compromise', 0.09), ('vbn', 0.09), ('vbz', 0.085), ('constituents', 0.084), ('terminals', 0.08), ('feng', 0.08), ('parsing', 0.08), ('parse', 0.077), ('model', 0.074), ('ft', 0.074), ('request', 0.074), ('roles', 0.072), ('rules', 0.071), ('denoting', 0.069), ('mi', 0.067), ('side', 0.066), ('phrasebased', 0.065), ('strings', 0.065), ('sparseness', 0.063), ('got', 0.062), ('cover', 0.061), ('bank', 0.059), ('production', 0.059), ('syntax', 0.058), ('searches', 0.056), ('refine', 0.055), ('synchronous', 0.055), ('asia', 0.054), ('vulnerable', 0.053), ('iuqun', 0.053), ('bel', 0.053), ('khk', 0.053), ('observes', 0.053), ('purchase', 0.053), ('reordering', 0.052), ('employ', 0.052), ('syntactic', 0.05), ('rle', 0.048), ('aln', 0.048), ('doe', 0.048), ('microsoft', 0.048), ('get', 0.047), ('row', 0.046), ('specify', 0.046), ('boundary', 0.045), ('longdistance', 0.045), ('conveniently', 0.045), ('hk', 0.045), ('kd', 0.045), ('mul', 0.045), ('sin', 0.045), ('visited', 0.045), ('string', 0.045), ('knight', 0.044), ('introduces', 0.044), ('internal', 0.044), ('matched', 0.044), ('tree', 0.043), ('besides', 0.043), ('nn', 0.043), ('adjacent', 0.042), ('vbd', 0.042), ('acts', 0.042), ('dongdong', 0.042), ('xc', 0.042), ('mingzhou', 0.042), ('reorderings', 0.042), ('grammars', 0.042), ('sequence', 0.042), ('rewrites', 0.04), ('followed', 0.039), ('learns', 0.039), ('speed', 0.039), ('groups', 0.039), ('occurrences', 0.039), ('fox', 0.039), ('etc', 0.039), ('packed', 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="108-tfidf-1" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>2 0.17243892 <a title="108-tfidf-2" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>3 0.16813083 <a title="108-tfidf-3" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>4 0.15788887 <a title="108-tfidf-4" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>5 0.1569604 <a title="108-tfidf-5" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>6 0.15017906 <a title="108-tfidf-6" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>7 0.15006971 <a title="108-tfidf-7" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>8 0.14467408 <a title="108-tfidf-8" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>9 0.11961356 <a title="108-tfidf-9" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>10 0.11008604 <a title="108-tfidf-10" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>11 0.10485014 <a title="108-tfidf-11" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>12 0.10172882 <a title="108-tfidf-12" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>13 0.099013604 <a title="108-tfidf-13" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>14 0.0956508 <a title="108-tfidf-14" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>15 0.095023386 <a title="108-tfidf-15" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>16 0.093959391 <a title="108-tfidf-16" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>17 0.093865916 <a title="108-tfidf-17" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>18 0.093481414 <a title="108-tfidf-18" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>19 0.090766959 <a title="108-tfidf-19" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>20 0.0873513 <a title="108-tfidf-20" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.238), (1, -0.184), (2, 0.001), (3, -0.031), (4, -0.039), (5, -0.114), (6, -0.028), (7, 0.101), (8, 0.051), (9, 0.028), (10, 0.011), (11, -0.09), (12, -0.027), (13, 0.003), (14, 0.009), (15, -0.082), (16, -0.02), (17, -0.019), (18, 0.009), (19, -0.13), (20, 0.028), (21, 0.108), (22, -0.118), (23, 0.091), (24, -0.057), (25, 0.033), (26, -0.017), (27, -0.154), (28, 0.044), (29, -0.044), (30, 0.028), (31, 0.056), (32, -0.142), (33, 0.042), (34, -0.123), (35, -0.007), (36, -0.06), (37, 0.017), (38, -0.02), (39, -0.069), (40, 0.078), (41, 0.03), (42, 0.003), (43, -0.057), (44, 0.067), (45, -0.064), (46, -0.032), (47, -0.016), (48, 0.043), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94693518 <a title="108-lsi-1" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>2 0.778781 <a title="108-lsi-2" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>Author: Junhui Li ; Zhaopeng Tu ; Guodong Zhou ; Josef van Genabith</p><p>Abstract: This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1</p><p>3 0.65528876 <a title="108-lsi-3" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>4 0.65397966 <a title="108-lsi-4" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>5 0.6421721 <a title="108-lsi-5" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>6 0.62005728 <a title="108-lsi-6" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>7 0.58182496 <a title="108-lsi-7" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>8 0.55320913 <a title="108-lsi-8" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>9 0.53543466 <a title="108-lsi-9" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>10 0.52152658 <a title="108-lsi-10" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>11 0.51572448 <a title="108-lsi-11" href="./acl-2012-Strong_Lexicalization_of_Tree_Adjoining_Grammars.html">185 acl-2012-Strong Lexicalization of Tree Adjoining Grammars</a></p>
<p>12 0.509359 <a title="108-lsi-12" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>13 0.50446159 <a title="108-lsi-13" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>14 0.49671653 <a title="108-lsi-14" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>15 0.48930132 <a title="108-lsi-15" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>16 0.48768416 <a title="108-lsi-16" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>17 0.4873665 <a title="108-lsi-17" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>18 0.4541373 <a title="108-lsi-18" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>19 0.45345506 <a title="108-lsi-19" href="./acl-2012-Heuristic_Cube_Pruning_in_Linear_Time.html">107 acl-2012-Heuristic Cube Pruning in Linear Time</a></p>
<p>20 0.44366848 <a title="108-lsi-20" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(26, 0.051), (28, 0.066), (30, 0.054), (37, 0.025), (39, 0.041), (53, 0.297), (57, 0.015), (74, 0.024), (82, 0.033), (84, 0.013), (85, 0.026), (90, 0.1), (92, 0.056), (94, 0.062), (99, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80324006 <a title="108-lda-1" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<p>Author: Polina Kuznetsova ; Vicente Ordonez ; Alexander Berg ; Tamara Berg ; Yejin Choi</p><p>Abstract: We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.</p><p>same-paper 2 0.71576399 <a title="108-lda-2" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>3 0.47304162 <a title="108-lda-3" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>4 0.47076729 <a title="108-lda-4" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>Author: Jonathan Berant ; Ido Dagan ; Meni Adler ; Jacob Goldberger</p><p>Abstract: Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm.</p><p>5 0.46972954 <a title="108-lda-5" href="./acl-2012-Distributional_Semantics_in_Technicolor.html">76 acl-2012-Distributional Semantics in Technicolor</a></p>
<p>Author: Elia Bruni ; Gemma Boleda ; Marco Baroni ; Nam Khanh Tran</p><p>Abstract: Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.</p><p>6 0.46871755 <a title="108-lda-6" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>7 0.46748823 <a title="108-lda-7" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>8 0.46447083 <a title="108-lda-8" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>9 0.46365497 <a title="108-lda-9" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>10 0.4635461 <a title="108-lda-10" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>11 0.46265081 <a title="108-lda-11" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>12 0.46167693 <a title="108-lda-12" href="./acl-2012-You_Had_Me_at_Hello%3A_How_Phrasing_Affects_Memorability.html">218 acl-2012-You Had Me at Hello: How Phrasing Affects Memorability</a></p>
<p>13 0.46095955 <a title="108-lda-13" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>14 0.46090326 <a title="108-lda-14" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>15 0.46077579 <a title="108-lda-15" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>16 0.46015409 <a title="108-lda-16" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>17 0.46006352 <a title="108-lda-17" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>18 0.45945662 <a title="108-lda-18" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>19 0.45943293 <a title="108-lda-19" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>20 0.4591969 <a title="108-lda-20" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
