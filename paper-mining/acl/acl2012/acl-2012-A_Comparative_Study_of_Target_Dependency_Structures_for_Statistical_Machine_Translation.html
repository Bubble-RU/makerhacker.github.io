<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-4" href="#">acl2012-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2012-4-pdf" href="http://aclweb.org/anthology//P/P12/P12-2020.pdf">pdf</a></p><p>Author: Xianchao Wu ; Katsuhito Sudoh ; Kevin Duh ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies. 1</p><p>Reference: <a title="acl-2012-4-reference" href="../acl2012_reference/acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp {t sukada s Abstract This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. [sent-8, score-0.531]
</p><p>2 Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. [sent-9, score-0.436]
</p><p>3 Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. [sent-10, score-1.146]
</p><p>4 The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies. [sent-11, score-0.494]
</p><p>5 1  Introduction  Target language side dependency structures have been successfully used in statistical machine translation (SMT) by Shen et al. [sent-12, score-0.523]
</p><p>6 (2008) and achieved state-of-the-art results as reported in the NIST 2008 Open MT Evaluation workshop and the NTCIR-9 Chinese-to-English patent translation task (Goto et al. [sent-13, score-0.137]
</p><p>7 It is known that dependency-style structures can be transformed from a number of linguistic struc∗Now at Baidu Inc. [sent-16, score-0.162]
</p><p>8 For example, using the constituent-todependency conversion approach proposed by Jo-  hansson and Nugues (2007), we can easily yield dependency trees from PCFG style trees. [sent-23, score-0.472]
</p><p>9 A semantic dependency representation of a whole sentence, predicate-argument structures (PASs), are also included in the output trees of (1) a state-of-the-art head-driven phrase structure grammar (HPSG) (Pollard and Sag, 1994; Sag et al. [sent-24, score-0.545]
</p><p>10 The motivation of this paper is to investigate the impact of these non-isomorphic dependency structures to be used for SMT. [sent-26, score-0.436]
</p><p>11 That is, we would like to provide a comparative evaluation of these dependencies in a string-to-dependency decoder (Shen et al. [sent-27, score-0.11]
</p><p>12 1 Dependency tree We follow the definition of dependency graph and dependency tree as given in (McDonald and Nivre, 2011). [sent-30, score-0.887]
</p><p>13 A dependency graph G for sentence s is called a dependency tree when it satisfies, (1) the nodes cover all the words in s besides the ROOT; (2) one node can have one and only one head (word) with a determined syntactic role; and (3) the ROOT of the graph is reachable from all other nodes. [sent-31, score-1.127]
</p><p>14 For extracting string-to-dependency transfer rules, we use well-formed dependency structures, either fixed or floating, as defined in (Shen et al. [sent-32, score-0.354]
</p><p>15 both d uri ng rule e xtra cting and target dependency language model (LM) training. [sent-50, score-0.401]
</p><p>16 2 Dependency parsing Graph-based and transition-based are two predominant paradigms for data-driven dependency parsing. [sent-52, score-0.372]
</p><p>17 , 2005) and the Malt parser (Nivre, 2003) stand for two typical parsers, respectively. [sent-54, score-0.086]
</p><p>18 Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (201 1). [sent-55, score-0.32]
</p><p>19 Here, we compare them on the SMT tasks through parsing the real-world SMT data. [sent-56, score-0.052]
</p><p>20 3 PCFG parsing For PCFG parsing, we select the Berkeley parser (Petrov and Klein, 2007). [sent-58, score-0.138]
</p><p>21 In order to generate wordlevel dependency trees from the PCFG tree, we use the LTH constituent-to-dependency conversion tool3 written by Johansson and Nugues (2007). [sent-59, score-0.472]
</p><p>22 The head finding rules4 are according to Magerman (1995) and Collins (1997). [sent-60, score-0.172]
</p><p>23 4 HPSG parsing In the Enju English HPSG grammar (Miyao et al. [sent-64, score-0.052]
</p><p>24 In an HPSG tree, each leaf node generally introduces a predicate, which is represented by the pair made up of the lexical entry feature and predicate type feature. [sent-72, score-0.203]
</p><p>25 The arguments of a predicate are designated by the arrows from the argument features in a leaf node to non-terminal nodes (e. [sent-73, score-0.267]
</p><p>26 in the HPSG tree (Figure 1), this prevents their direct usage in a string-to-dependency decoder. [sent-79, score-0.07]
</p><p>27 We thus need an algorithm to transform these phrasal predicateargument dependencies into a word-to-word depende ncy tree. [sent-80, score-0.097]
</p><p>28 Our algorithm (refer to Figure 1 for an example) for changing PASs into word-based dependency trees is as follows:  1. [sent-81, score-0.429]
</p><p>29 , find the syntactic/semantic head word of each argument node through a bottomup traversal of the tree; 2. [sent-84, score-0.281]
</p><p>30 , determine the arc directions (among a predicate word and the syntactic/semantic head words ofthe argument nodes) for each predicate type according to Table 1. [sent-87, score-0.537]
</p><p>31 , post modifying the dependency graph according to the definition ofdependency tree (Section 2. [sent-91, score-0.528]
</p><p>32 Table 1lists the mapping from HPSG’s PAS types to word-level dependency arcs. [sent-93, score-0.368]
</p><p>33 Since a non-terminal node in an HPSG tree has two kinds of heads, syntactic or semantic, we will generate two dependency graphs after mapping. [sent-94, score-0.508]
</p><p>34 We use “PAS+syn” to represent the dependency trees generated from the HPSG PASs guided by the syntactic heads. [sent-95, score-0.506]
</p><p>35 Its arg1 = c16 (with syntactic head t10), arg2  = c3 (with syntactic head t6), and PAS type = conj arg12. [sent-98, score-0.432]
</p><p>36 In Table 1, this PAS type corresponds to arg2→pred→arg1, then the result word-level dependency pirse t6(is)→t0(when)→t10(is). [sent-99, score-0.32]
</p><p>37 Wndee nnceyed is to post modify etnh)e→ dependency graph after applying the mapping, since it is not guaranteed to be a dependency tree. [sent-100, score-0.747]
</p><p>38 Referring to the definition of dependency tree (Section 2. [sent-101, score-0.39]
</p><p>39 1), we need the strategy for (1) selecting only one head from multiple PAS TypeDependency Relation  Table 1: Mapping from HPSG’s PAS types to dependency relations. [sent-102, score-0.492]
</p><p>40 heads and (2) appending dependency relations for those words/punctuation that do not have any head. [sent-104, score-0.392]
</p><p>41 The selection strategy is that, if this arc was deleted, it will cause the biggest number of words that can not reach to the root word anymore. [sent-106, score-0.186]
</p><p>42 In case  of a tie, we greedily pack the arc that connect two words wi and wj where |i −j | is the biggest. [sent-107, score-0.13]
</p><p>43 For all the words and punctuation thja|t i sd toh en boti ghgaevset a head, we greedily take the root word of the sentence as their heads. [sent-108, score-0.198]
</p><p>44 In order to fully use the training data, if there are directed cycles in the result dependency graph, we still use the graph in our experiments, where only partial dependency arcs, i. [sent-109, score-0.797]
</p><p>45 , those target flat/hierarchical phrases attached with well-formed dependency structures, can be used during translation rule extraction. [sent-111, score-0.457]
</p><p>46 5 CCG parsing We also use the predicate-argument dependencies generated by the CCG parser developed by Clark and Curran (2007). [sent-113, score-0.236]
</p><p>47 The algorithm for generating word-level dependency tree is easier than processing the PASs included in the HPSG trees, since the word level predicate-argument relations have already been included in the output of CCG parser. [sent-114, score-0.39]
</p><p>48 The mapping from predicate types to the gold-standard grammatical relations can be found in Table 13 in (Clark and 102 Curran, 2007). [sent-115, score-0.169]
</p><p>49 The post-processing is like that de-  scribed for HPSG parsing, except we greedily use the MST’s sentence root when we can not determine it based on the CCG parser’s PASs. [sent-116, score-0.198]
</p><p>50 Dependency structures from non-isomorphic syntactic/semantic parsers are separately used to train the transfer rules as well as target dependency LMs. [sent-120, score-0.616]
</p><p>51 0b35 (Pauls and Klein, 2011), was employed to  train (1) a five-gram LM on the Xinhua portion of LDC English Gigaword corpus v3 (LDC2007T07) and (2) a tri-gram dependency LM on the English dependency structures of the training data. [sent-131, score-0.756]
</p><p>52 We report the translation quality using the case-insensitive BLEU-4 metric (Papineni et al. [sent-132, score-0.087]
</p><p>53 2 Statistics of dependencies We compare the similarity of the dependencies with each other, as shown in Table 2. [sent-135, score-0.13]
</p><p>54 Basically, we investigate (1) if two dependency graphs of one sentence share the same root word and (2) if the head of one word in one sentence are identical in two dependency graphs. [sent-136, score-1.061]
</p><p>55 In terms of root word comparison, we observe that MST and CCG share 87. [sent-137, score-0.168]
</p><p>56 3% of identical root words, caused by borrowing roots from MST to CCG. [sent-138, score-0.21]
</p><p>57 Then, it is interesting that Berkeley and PAS+syn share 74. [sent-139, score-0.041]
</p><p>58 Note that the Berkeley parser is trained on the Penn treebank (Marcus et al. [sent-141, score-0.116]
</p><p>59 , 1994) yet the HPSG parser is trained on the HPSG treebank (Miyao and Tsujii, 5http://code. [sent-142, score-0.155]
</p><p>60 com/p/berkeleylm/ DependencyPrecisionRecallBLEU-DevBLEU-Test# phrases# hier rules# illegal dep trees# directed cycles  PBCM AeSaorGsTlkt+eas-ybe21mln3:C0o- m. [sent-144, score-0.172]
</p><p>61 Without () = % of similar root words; with () = % of similar head words. [sent-158, score-0.299]
</p><p>62 In terms of head word comparison, PAS+syn and PAS+sem share 79. [sent-160, score-0.213]
</p><p>63 This is basically due to that we used the similar PASs ofthe HPSG trees. [sent-162, score-0.038]
</p><p>64 This reflects the significant difference between syntactic and semantic heads. [sent-165, score-0.044]
</p><p>65 We also manually created the golden dependency trees for the first 200 English sentences in the training data. [sent-166, score-0.429]
</p><p>66 We observe that (1) the translation accuracies approximately follow the P/R scores yet are not that sensitive to their large variances, and (2) it is still tough for domain-adapting from the treebanktrained parsers to parse the real-world SMT data. [sent-168, score-0.246]
</p><p>67 PAS+syn performed the best by avoiding the errors of missing of arguments for a predicate, wrongly identified head words for a linguistic phrase, and inconsistency dependencies inside relatively long coordinate structures. [sent-169, score-0.237]
</p><p>68 These errors significantly influence the number of extractable translation rules and the final translation accuracies. [sent-170, score-0.21]
</p><p>69 Note that, these P/R scores on the first 200 sentences (all from less than 20 newswire documents) shall only be taken as an approximation of the total 103 training data and not necessarily exactly follow the tendency of the final BLEU scores. [sent-171, score-0.029]
</p><p>70 For example, CCG is worse than Malt in terms of P/R yet with a higher BLEU score. [sent-172, score-0.039]
</p><p>71 We argue this is mainly due to  that the number of illegal dependency trees generated by Malt is the highest. [sent-173, score-0.587]
</p><p>72 Consequently, the number of flat/hierarchical rules generated by using Malt trees is the lowest. [sent-174, score-0.178]
</p><p>73 Also, PAS+sem has a lower P/R than Berkeley, yet their final BLEU scores are not statistically different. [sent-175, score-0.039]
</p><p>74 3 Results Table 3 also shows the BLEU scores, the number of flat phrases and hierarchical rules (both integrated with target dependency structures), and the number of illegal dependency trees generated by each parser. [sent-177, score-0.961]
</p><p>75 From the table, we have the following observations: (1) all the dependency structures (except Malt) achieved a significant better BLEU score than the phrasal Moses; (2) PAS+syn performed the best in the test set (0. [sent-178, score-0.468]
</p><p>76 These results lead us to argue that the robustness of deep syntactic parsers can be advantageous in SMT compared with traditional dependency parsers. [sent-185, score-0.456]
</p><p>77 4  Conclusion  We have constructed a string-to-dependency translation platform for comparing non-isomorphic target dependency structures. [sent-186, score-0.457]
</p><p>78 Specially, we proposed an algorithm for generating word-based dependency trees from PASs which are generated by a state-ofthe-art HPSG parser. [sent-187, score-0.462]
</p><p>79 We found that dependency trees transformed from these HPSG PASs achieved the best dependency/translation accuracies. [sent-188, score-0.475]
</p><p>80 Acknowledgments We thank the anonymous reviewers for their constructive comments and suggestions. [sent-189, score-0.031]
</p><p>81 In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. [sent-193, score-0.039]
</p><p>82 Widecoverage efficient statistical parsing with ccg and loglinear models. [sent-198, score-0.283]
</p><p>83 In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain, July. [sent-203, score-0.039]
</p><p>84 Overview of the patent machine translation task at the ntcir-9 workshop. [sent-208, score-0.137]
</p><p>85 In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180. [sent-217, score-0.039]
</p><p>86 Bbn’s systems for the chinese-english sub-task of the ntcir-9 patentmt evaluation. [sent-220, score-0.031]
</p><p>87 In In Proceedings of of the 33rd Annual Meeting of the Association for Computational Linguis-  tics, pages 276–283. [sent-225, score-0.039]
</p><p>88 In Proceedings of the Workshop on HLT, pages 114–1 19, Plainsboro. [sent-229, score-0.039]
</p><p>89 In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan, June. [sent-237, score-0.039]
</p><p>90 In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 285– 291, Borovets. [sent-246, score-0.039]
</p><p>91 In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT, pages 149–160. [sent-250, score-0.039]
</p><p>92 In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 258–267, Portland, Oregon, USA, June. [sent-262, score-0.039]
</p><p>93 In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–41 1, Rochester, New York, April. [sent-267, score-0.039]
</p><p>94 A new string-to-dependency machine translation algorithm with a target dependency language model. [sent-283, score-0.457]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hpsg', 0.414), ('pas', 0.327), ('dependency', 0.32), ('ccg', 0.231), ('malt', 0.213), ('syn', 0.186), ('head', 0.172), ('mst', 0.142), ('pass', 0.142), ('root', 0.127), ('predicate', 0.121), ('structures', 0.116), ('trees', 0.109), ('miyao', 0.096), ('ntt', 0.095), ('illegal', 0.093), ('translation', 0.087), ('parser', 0.086), ('pcfg', 0.086), ('mcdonald', 0.086), ('sag', 0.085), ('shen', 0.08), ('graph', 0.078), ('berkeley', 0.077), ('sem', 0.075), ('heads', 0.072), ('greedily', 0.071), ('tree', 0.07), ('smt', 0.069), ('bleu', 0.068), ('curran', 0.065), ('dependencies', 0.065), ('argument', 0.064), ('csli', 0.062), ('parsers', 0.06), ('nivre', 0.059), ('arc', 0.059), ('moses', 0.057), ('pollard', 0.057), ('goto', 0.053), ('clark', 0.052), ('parsing', 0.052), ('identical', 0.052), ('target', 0.05), ('buchholz', 0.05), ('patent', 0.05), ('mapping', 0.048), ('sudoh', 0.047), ('transformed', 0.046), ('node', 0.045), ('cycles', 0.045), ('nugues', 0.045), ('pauls', 0.045), ('comparative', 0.045), ('syntactic', 0.044), ('conversion', 0.043), ('nist', 0.041), ('share', 0.041), ('lm', 0.04), ('yet', 0.039), ('ivan', 0.039), ('pages', 0.039), ('basically', 0.038), ('johansson', 0.038), ('tsujii', 0.038), ('yusuke', 0.038), ('joakim', 0.037), ('leaf', 0.037), ('rules', 0.036), ('ann', 0.036), ('directed', 0.034), ('transfer', 0.034), ('alexandra', 0.034), ('generated', 0.033), ('ichi', 0.033), ('argue', 0.032), ('phrasal', 0.032), ('aaki', 0.031), ('borrowing', 0.031), ('constructive', 0.031), ('kat', 0.031), ('kevinduh', 0.031), ('lingustics', 0.031), ('mas', 0.031), ('nso', 0.031), ('ofdependency', 0.031), ('patentmt', 0.031), ('tartu', 0.031), ('tough', 0.031), ('uri', 0.031), ('variances', 0.031), ('wasow', 0.031), ('association', 0.03), ('koehn', 0.03), ('treebank', 0.03), ('follow', 0.029), ('papineni', 0.029), ('graphs', 0.029), ('post', 0.029), ('ryan', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="4-tfidf-1" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>Author: Xianchao Wu ; Katsuhito Sudoh ; Kevin Duh ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies. 1</p><p>2 0.23733954 <a title="4-tfidf-2" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein ; James R. Curran</p><p>Abstract: We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (5 1.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.</p><p>3 0.23216327 <a title="4-tfidf-3" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</p><p>4 0.19704446 <a title="4-tfidf-4" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>5 0.18913926 <a title="4-tfidf-5" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>6 0.17576471 <a title="4-tfidf-6" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>7 0.1723763 <a title="4-tfidf-7" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>8 0.16952914 <a title="4-tfidf-8" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>9 0.1527127 <a title="4-tfidf-9" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>10 0.14049426 <a title="4-tfidf-10" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>11 0.12930177 <a title="4-tfidf-11" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>12 0.12464432 <a title="4-tfidf-12" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>13 0.12214352 <a title="4-tfidf-13" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>14 0.1218068 <a title="4-tfidf-14" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>15 0.11954965 <a title="4-tfidf-15" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>16 0.11854956 <a title="4-tfidf-16" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>17 0.11435001 <a title="4-tfidf-17" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>18 0.10610685 <a title="4-tfidf-18" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>19 0.10534809 <a title="4-tfidf-19" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>20 0.10240566 <a title="4-tfidf-20" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.279), (1, -0.128), (2, -0.196), (3, -0.153), (4, -0.063), (5, -0.2), (6, -0.036), (7, 0.009), (8, 0.103), (9, 0.001), (10, 0.154), (11, 0.144), (12, 0.041), (13, -0.014), (14, -0.105), (15, -0.004), (16, -0.068), (17, -0.06), (18, -0.09), (19, 0.001), (20, 0.009), (21, -0.065), (22, 0.106), (23, -0.07), (24, -0.047), (25, -0.059), (26, 0.042), (27, -0.019), (28, -0.115), (29, -0.004), (30, 0.009), (31, -0.027), (32, 0.069), (33, -0.068), (34, 0.117), (35, -0.023), (36, -0.015), (37, -0.042), (38, -0.023), (39, -0.032), (40, -0.139), (41, -0.006), (42, 0.054), (43, 0.141), (44, -0.071), (45, -0.008), (46, -0.082), (47, 0.047), (48, 0.011), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94813138 <a title="4-lsi-1" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>Author: Xianchao Wu ; Katsuhito Sudoh ; Kevin Duh ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies. 1</p><p>2 0.73230398 <a title="4-lsi-2" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>Author: Dominick Ng ; James R. Curran</p><p>Abstract: Optimising for one grammatical representation, but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing. We find that this mismatch causes many n-best CCG parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best F-score by 0.7% and reranking accuracy by 0.4%. We also present a comprehensive analysis of errors made by the C&C; CCG parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy.</p><p>3 0.7275508 <a title="4-lsi-3" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</p><p>4 0.66197509 <a title="4-lsi-4" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>Author: Emily Pitler</p><p>Abstract: Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.</p><p>5 0.6614024 <a title="4-lsi-5" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein ; James R. Curran</p><p>Abstract: We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (5 1.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.</p><p>6 0.64769733 <a title="4-lsi-6" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>7 0.59634507 <a title="4-lsi-7" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>8 0.58849645 <a title="4-lsi-8" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>9 0.55121189 <a title="4-lsi-9" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>10 0.53272647 <a title="4-lsi-10" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>11 0.53152972 <a title="4-lsi-11" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>12 0.52206898 <a title="4-lsi-12" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>13 0.51891637 <a title="4-lsi-13" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>14 0.49168697 <a title="4-lsi-14" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>15 0.48765531 <a title="4-lsi-15" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>16 0.46909255 <a title="4-lsi-16" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>17 0.45787036 <a title="4-lsi-17" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>18 0.45349479 <a title="4-lsi-18" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>19 0.4477748 <a title="4-lsi-19" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>20 0.43992394 <a title="4-lsi-20" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.013), (26, 0.025), (28, 0.034), (30, 0.025), (37, 0.063), (39, 0.03), (57, 0.043), (59, 0.014), (71, 0.297), (74, 0.029), (82, 0.034), (84, 0.014), (85, 0.065), (90, 0.118), (92, 0.038), (94, 0.027), (99, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76310652 <a title="4-lda-1" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>Author: Xianchao Wu ; Katsuhito Sudoh ; Kevin Duh ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these nonisomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser’s PASs achieved the best dependency and translation accuracies. 1</p><p>2 0.76207602 <a title="4-lda-2" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>3 0.5433057 <a title="4-lda-3" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>4 0.5364334 <a title="4-lda-4" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>Author: Emily Pitler</p><p>Abstract: Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.</p><p>5 0.52358413 <a title="4-lda-5" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</p><p>6 0.52041197 <a title="4-lda-6" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>7 0.4935571 <a title="4-lda-7" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>8 0.49331537 <a title="4-lda-8" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>9 0.48937958 <a title="4-lda-9" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>10 0.48695049 <a title="4-lda-10" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>11 0.48623824 <a title="4-lda-11" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>12 0.4853434 <a title="4-lda-12" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>13 0.48497388 <a title="4-lda-13" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>14 0.48443022 <a title="4-lda-14" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>15 0.48234314 <a title="4-lda-15" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>16 0.48205844 <a title="4-lda-16" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>17 0.48169941 <a title="4-lda-17" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>18 0.48018485 <a title="4-lda-18" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>19 0.47935683 <a title="4-lda-19" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>20 0.47828692 <a title="4-lda-20" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
