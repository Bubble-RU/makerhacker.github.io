<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-202" href="#">acl2012-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</h1>
<br/><p>Source: <a title="acl-2012-202-pdf" href="http://aclweb.org/anthology//P/P12/P12-2035.pdf">pdf</a></p><p>Author: Emad Mohamed ; Behrang Mohit ; Kemal Oflazer</p><p>Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.</p><p>Reference: <a title="acl-2012-202-reference" href="../acl2012_reference/acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). [sent-6, score-0.056]
</p><p>2 84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28. [sent-9, score-0.048]
</p><p>3 The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e. [sent-12, score-0.185]
</p><p>4 , this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. [sent-14, score-0.382]
</p><p>5 It can also considerably speed up the annotation of Arabic dialects. [sent-15, score-0.024]
</p><p>6 Dialectal varieties have not received much attention due to the lack of dialectal tools and annotated texts (Duh and Kirchoff, 2005). [sent-18, score-0.185]
</p><p>7 The transformation process relies on the observation that dialectal varieties of Arabic differ mainly in the use of affixes and function words while the word stem mostly remains unchanged. [sent-20, score-0.401]
</p><p>8 For example, given the Buckwalter-encoded MSA sentence “AlAxwAn Almslmwn lm yfwzwA fy AlAntxbAt” the rules produce “AlAxwAn Almslmyn mfAzw$ f AlAntxAbAt” (‫ ,االخىات( المسلميت( ه مفازوت( ش ت( االوتخابات‬The Muslim Brotherhood did not win the elections). [sent-21, score-0.106]
</p><p>9 The availability of segment-based part-of-speech tags is essential since many of the affixes in MSA are ambiguous. [sent-22, score-0.119]
</p><p>10 For example, lm could be either a negative particle or a question work, and the word AlAxwAn could be either made of two segments (Al+ xw+An, the two brothers). [sent-23, score-0.242]
</p><p>11 We first introduce the transformation rules, and show that in many cases it is feasible to transform MSA to CEA, although there are cases that require much more than POS tags. [sent-24, score-0.053]
</p><p>12 We then provide a typical case in which we utilize the transformed text of the Arabic Treebank (Bies and Maamouri, 2003)  to build a part-of-speech tagger for CEA. [sent-25, score-0.047]
</p><p>13 The tagger improves the accuracy of POS tagging on authentic Egyptian Arabic by 13% absolute (from 73. [sent-26, score-0.158]
</p><p>14 84%) and reduces the percentage of out-of-vocabulary words from 28. [sent-28, score-0.048]
</p><p>15 Both can be translated into: “We did not write it for them. [sent-33, score-0.034]
</p><p>16 ” MSA has three words while CEA is more synthetic as the preposition and the negative particle turn into clitics. [sent-34, score-0.206]
</p><p>17 Table 1 illustrates the end product of one of the Imperfect transformation rules, namely the case where the Imperfect Verb is preceded by the negative particle lm. [sent-35, score-0.187]
</p><p>18 The rules also cover certain lexical items as 400 words in MSA have been converted to their comProce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-37, score-0.087]
</p><p>19 Examples of lexical conversions include ZlAm and Dlmp (darkness), rjl and rAjl (man), rjAl and rjAlp (men), and kvyr and ktyr (many), where the first word is the MSA version and the second is the CEA version. [sent-40, score-0.102]
</p><p>20 For example, the word rjl can either mean man or leg. [sent-42, score-0.119]
</p><p>21 When it means man, the CEA form is rAjl, but the word for leg is the same in both MSA and CEA. [sent-43, score-0.022]
</p><p>22 While they have different vowel patterns (rajul and rijol respectively), the vowel information is harder to get correctly than POS tags. [sent-44, score-0.064]
</p><p>23 The problem may arise especially when dealing with raw data for which we need to provide POS tags (and vowels) so we may be able to convert it to the colloquial form. [sent-45, score-0.305]
</p><p>24 Below, we provide two sample rules: The imperfect verb is used, inter alia, to express the negated past, for which CEA uses the perfect verb. [sent-46, score-0.113]
</p><p>25 What makes things more complicated is that  CEA treats negative particles and prepositional phrases as clitics. [sent-47, score-0.082]
</p><p>26 An example of this is the word mktbthlhm$ (I did not write it for them) in Table 1 above. [sent-48, score-0.056]
</p><p>27 It is made of the negative particle m, the stem ktb (to write), the object pronoun h, the preposition l, the pronoun hm (them) and the negative particle $. [sent-49, score-0.502]
</p><p>28 Figure 1, and the following steps show the conversions of lm nktbhA lhm to mktbnhAlhm$: 1. [sent-50, score-0.095]
</p><p>29 Replace the negative word lm with one of the prefixes m, mA or the word mA. [sent-51, score-0.147]
</p><p>30 For example, the IV first person singular subject prefix > turns into t in the PV. [sent-54, score-0.023]
</p><p>31 If the verb is followed by a prepositional phrase headed by the preposition lthat contains a pronominal object, convert the preposition to a prepositional clitic. [sent-56, score-0.25]
</p><p>32 Transform the dual to plural and the plural feminine to plural masculine. [sent-58, score-0.434]
</p><p>33 Add the negative suffix $ (or the variant $y, which is less probable) As alluded to in 1) above, given that colloquial orthography is not standardized, many affixes and clitics can be written in different ways. [sent-60, score-0.371]
</p><p>34 For exam-  ple, the word mktbnhlhm$, can be written in 24 ways. [sent-61, score-0.022]
</p><p>35 177  Figure1:OnenegatedIVforminMSAcangen rate24 (3x2x2x2) possible forms in CEA  MSA possessive pronouns inflect for gender, number (singular, dual, and plural), and person. [sent-64, score-0.038]
</p><p>36 In CEA, there is no distinction between the dual and the plural, and a single pronoun is used for the plural feminine and masculine. [sent-65, score-0.25]
</p><p>37 The three MSA forms ktAbhm, ktAbhmA and ktAbhn (their book for the masculine plural, the dual, and the feminine plural respectively) all collapse to ktAbhm. [sent-66, score-0.165]
</p><p>38 Table 2 has examples of some other rules we have applied. [sent-67, score-0.044]
</p><p>39 We note that the stem, in bold, hardly changes, and that the changes mainly affect function segments. [sent-68, score-0.036]
</p><p>40 The last example is a lexical rule in which the stem has to change. [sent-69, score-0.058]
</p><p>41 POS Tagging Egyptian Arabic We use the conversion above to build a POS tagger for Egyptian Arabic. [sent-71, score-0.104]
</p><p>42 We follow Mohamed and Kuebler (2010) in using whole word tagging, i. [sent-72, score-0.022]
</p><p>43 For example, the word wHnktblhm (and we will write to them, ‫)وحىكتبلهم‬ receives the tag PRT+PRT+VRB+PRT+NOM. [sent-76, score-0.056]
</p><p>44 This results in 58 composite tags, 9 of which occur 5 times or less in the converted ECA training set. [sent-77, score-0.076]
</p><p>45 We converted two sections of the Arabic Treebank (ATB): p2v3 and p3v2. [sent-78, score-0.043]
</p><p>46 For all the POS tagging experiments, we use the memory-based POS tagger (MBT) (Daelemans et al. [sent-79, score-0.117]
</p><p>47 , 1996) The best results, tuned on a dev set, were obtained, in nonexhaustive search, with the Modified Value Difference Metric as a distance metric and with k (the number of nearest neighbors) = 25. [sent-80, score-0.055]
</p><p>48 For known words, we use the IGTree algorithm and 2 words to the left, their POS tags, the focus word and its list of possible tags, 1 right context word and its list of possible tags as features. [sent-81, score-0.092]
</p><p>49 For unknown words, we use the IB 1 algorithm and the word itself, its first 5 and last 3 characters, 1 left context word and its POS tag, and 1 right context word and its list of possible tags as features. [sent-82, score-0.163]
</p><p>50 Development and Test Data As a development set, we use 100 user-contributed  comments (2757 words) from the website masrawy. [sent-85, score-0.054]
</p><p>51 The test set contains 192 comments (7092 words) from the same website with the same criterion. [sent-87, score-0.054]
</p><p>52 The development and test sets were handannotated with composite tags as illustrated above by two native Arabic-speaking students. [sent-88, score-0.104]
</p><p>53 The test and development sets contained spelling errors (mostly run-on words). [sent-89, score-0.023]
</p><p>54 The most common of these is the vocative particle yA, which is usually attached to following word (e. [sent-90, score-0.139]
</p><p>55 The same holds true for the variation between the letters * and z, (‫ ذ‬and ‫ ز‬in Arabic) which are pronounced exactly the same way in CEA to the extent that the substitution may not be considered a spelling error. [sent-94, score-0.023]
</p><p>56 Experiments and Results We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard, where we train the tagger on the ATB MSA data, (b) 3-gram LM, where for each MSA sentence we generate all transformed sentences (see Section 2. [sent-97, score-0.104]
</p><p>57 1 This corpus is  1Available  probable sentence model built from user contributed highly dialectal  from http://www. [sent-100, score-0.168]
</p><p>58 Hybridization is necessary since most Arabic data in blogs and comments are a mix of MSA and CEA, and (e) Hybrid + dev, where we enrich the Hybrid training set with the dev data. [sent-105, score-0.083]
</p><p>59 We use the following metrics for evaluation: KWA: Known Word Accuracy (%), UWA: Unknown Word Accuracy (%), TA: Total Accuracy (%), and UW: unknown words (%) in the  respective set in the respective experiment. [sent-106, score-0.049]
</p><p>60 W9208 4 We notice that randomly selecting a sentence from the correct generated sentences yields better results than choosing the most probable sentence according to a language model. [sent-112, score-0.036]
</p><p>61 This drop in the percentage of unknown words may indicate that generating all possible variations of CEA may be more useful than using a language model in general. [sent-118, score-0.097]
</p><p>62 Even in a CEA corpus of 35 million words, one third of the words generated by the rules are not in the corpus, while many of these are in both the test set and the development set. [sent-119, score-0.044]
</p><p>63 We also notice that the conversion alone improves tagging accuracy from 75. [sent-120, score-0.155]
</p><p>64 Combining the original MSA and the best scoring converted data (Random) raises the accuracies to 84. [sent-125, score-0.043]
</p><p>65 The fact that the percentage of unknown words drops further to 16. [sent-131, score-0.12]
</p><p>66 66% in the Hybrid+dev experiment points out the authentic colloquial data contains elements that have not been captured using conversion alone. [sent-132, score-0.325]
</p><p>67 Related Work To the best of our knowledge, ours is the first work that generates CEA automatically from morphologically disambiguated MSA, but Habash et al. [sent-134, score-0.056]
</p><p>68 (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. [sent-135, score-0.215]
</p><p>69 MAGED incorporates the morphology, phonology, and orthography of several Arabic dialects. [sent-136, score-0.032]
</p><p>70 (2010) worked on the annotation of dialectal Arabic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval. [sent-138, score-0.215]
</p><p>71 Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morphological analyzer and information gleaned from the intersection of several Arabic dialects. [sent-139, score-0.153]
</p><p>72 This is different from our approach for which POS tagging is  only an application. [sent-140, score-0.07]
</p><p>73 Our focus is to use any existing MSA data to generate colloquial Arabic resources that can be used in virtually any NLP task. [sent-141, score-0.253]
</p><p>74 Conclusions and Future Work We have a presented a method to convert Modern Standard Arabic to Egyptian Colloquial Arabic with an example application to the POS tagging task. [sent-145, score-0.1]
</p><p>75 This approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation, for example. [sent-146, score-0.382]
</p><p>76 While the rules of conversion were mainly morphological in nature, they have proved useful in handling colloquial data. [sent-147, score-0.443]
</p><p>77 However, morphology alone is not enough for handling key points of difference between CEA and MSA. [sent-148, score-0.053]
</p><p>78 While CEA is mainly an SVO language, MSA is mainly VSO, and while demonstratives are pre-nominal in MSA,  they are post-nominal in CEA. [sent-149, score-0.072]
</p><p>79 These phenomena can be handled only through syntactic conversion. [sent-150, score-0.025]
</p><p>80 When no gold standard segment-based POS tags are available, tools that produce segment-based annotation can be used, e. [sent-152, score-0.048]
</p><p>81 segment-based POS tagging (Mohamed and Kuebler, 2010) or MADA (Habash et al, 2009), although these are not expected to yield the same results as gold standard part-of-speech tags. [sent-154, score-0.07]
</p><p>82 We thank the two native speaker annotators and the anonymous reviewers for their instructive and enriching feedback. [sent-157, score-0.023]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('msa', 0.494), ('cea', 0.441), ('arabic', 0.438), ('colloquial', 0.227), ('egyptian', 0.145), ('dialectal', 0.132), ('particle', 0.117), ('plural', 0.112), ('habash', 0.102), ('mohamed', 0.1), ('pos', 0.082), ('morphological', 0.079), ('prt', 0.076), ('alaxwan', 0.071), ('affixes', 0.071), ('imperfect', 0.071), ('tagging', 0.07), ('qatar', 0.067), ('lm', 0.062), ('kuebler', 0.062), ('nizar', 0.061), ('stem', 0.058), ('conversion', 0.057), ('daelemans', 0.057), ('dialects', 0.057), ('dev', 0.055), ('gender', 0.054), ('feminine', 0.053), ('varieties', 0.053), ('man', 0.05), ('unknown', 0.049), ('percentage', 0.048), ('preposition', 0.048), ('tags', 0.048), ('colaba', 0.047), ('nprp', 0.047), ('rajl', 0.047), ('rjl', 0.047), ('vrb', 0.047), ('tagger', 0.047), ('hybrid', 0.047), ('dual', 0.045), ('rules', 0.044), ('converted', 0.043), ('duh', 0.042), ('verb', 0.042), ('maamouri', 0.041), ('authentic', 0.041), ('behrang', 0.041), ('kundu', 0.041), ('maged', 0.041), ('mbt', 0.041), ('mada', 0.041), ('modern', 0.041), ('negative', 0.041), ('prepositional', 0.041), ('rambow', 0.04), ('pronoun', 0.04), ('roth', 0.039), ('pronouns', 0.038), ('bies', 0.038), ('june', 0.037), ('probable', 0.036), ('mainly', 0.036), ('atb', 0.035), ('write', 0.034), ('composite', 0.033), ('kirchhoff', 0.033), ('conversions', 0.033), ('treebank', 0.033), ('vowel', 0.032), ('semitic', 0.032), ('orthography', 0.032), ('convert', 0.03), ('walter', 0.03), ('transformation', 0.029), ('morphologically', 0.029), ('comments', 0.028), ('alone', 0.028), ('diab', 0.027), ('analyzer', 0.027), ('disambiguated', 0.027), ('owen', 0.026), ('resources', 0.026), ('website', 0.026), ('morphology', 0.025), ('arbor', 0.025), ('phenomena', 0.025), ('srilm', 0.024), ('stolcke', 0.024), ('cheap', 0.024), ('considerably', 0.024), ('ann', 0.024), ('feasible', 0.024), ('al', 0.023), ('spelling', 0.023), ('drops', 0.023), ('singular', 0.023), ('native', 0.023), ('word', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="202-tfidf-1" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>Author: Emad Mohamed ; Behrang Mohit ; Kemal Oflazer</p><p>Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.</p><p>2 0.38247341 <a title="202-tfidf-2" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>Author: David Stallard ; Jacob Devlin ; Michael Kayser ; Yoong Keok Lee ; Regina Barzilay</p><p>Abstract: If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter.</p><p>3 0.29516953 <a title="202-tfidf-3" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>4 0.25300935 <a title="202-tfidf-4" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>5 0.11640296 <a title="202-tfidf-5" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>Author: Nathan Schneider ; Behrang Mohit ; Kemal Oflazer ; Noah A. Smith</p><p>Abstract: “Lightweight” semantic annotation of text calls for a simple representation, ideally without requiring a semantic lexicon to achieve good coverage in the language and domain. In this paper, we repurpose WordNet’s supersense tags for annotation, developing specific guidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains. The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement.</p><p>6 0.085451961 <a title="202-tfidf-6" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.079866186 <a title="202-tfidf-7" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>8 0.071723491 <a title="202-tfidf-8" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<p>9 0.070208564 <a title="202-tfidf-9" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>10 0.064125903 <a title="202-tfidf-10" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>11 0.06291303 <a title="202-tfidf-11" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>12 0.056872383 <a title="202-tfidf-12" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>13 0.056731258 <a title="202-tfidf-13" href="./acl-2012-Grammar_Error_Correction_Using_Pseudo-Error_Sentences_and_Domain_Adaptation.html">103 acl-2012-Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation</a></p>
<p>14 0.053765055 <a title="202-tfidf-14" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>15 0.052265007 <a title="202-tfidf-15" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>16 0.049701463 <a title="202-tfidf-16" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>17 0.048811007 <a title="202-tfidf-17" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>18 0.046441905 <a title="202-tfidf-18" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>19 0.045592159 <a title="202-tfidf-19" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>20 0.042813949 <a title="202-tfidf-20" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.141), (1, -0.046), (2, -0.05), (3, -0.044), (4, 0.071), (5, 0.194), (6, 0.107), (7, -0.223), (8, -0.013), (9, -0.021), (10, -0.224), (11, -0.076), (12, 0.195), (13, -0.197), (14, 0.036), (15, -0.164), (16, -0.328), (17, -0.19), (18, -0.203), (19, -0.106), (20, 0.151), (21, -0.009), (22, -0.012), (23, -0.05), (24, 0.053), (25, -0.07), (26, 0.039), (27, -0.014), (28, -0.12), (29, 0.069), (30, -0.001), (31, -0.022), (32, 0.001), (33, 0.066), (34, -0.118), (35, 0.001), (36, 0.021), (37, -0.077), (38, -0.033), (39, 0.062), (40, 0.076), (41, -0.056), (42, 0.014), (43, 0.048), (44, 0.013), (45, 0.029), (46, -0.048), (47, -0.019), (48, 0.015), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95173401 <a title="202-lsi-1" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>Author: Emad Mohamed ; Behrang Mohit ; Kemal Oflazer</p><p>Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.</p><p>2 0.82051015 <a title="202-lsi-2" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>Author: David Stallard ; Jacob Devlin ; Michael Kayser ; Yoong Keok Lee ; Regina Barzilay</p><p>Abstract: If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter.</p><p>3 0.77464789 <a title="202-lsi-3" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>4 0.54955435 <a title="202-lsi-4" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>5 0.39741889 <a title="202-lsi-5" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>Author: Nathan Schneider ; Behrang Mohit ; Kemal Oflazer ; Noah A. Smith</p><p>Abstract: “Lightweight” semantic annotation of text calls for a simple representation, ideally without requiring a semantic lexicon to achieve good coverage in the language and domain. In this paper, we repurpose WordNet’s supersense tags for annotation, developing specific guidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains. The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement.</p><p>6 0.38917834 <a title="202-lsi-6" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>7 0.32275176 <a title="202-lsi-7" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>8 0.22718723 <a title="202-lsi-8" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<p>9 0.21369164 <a title="202-lsi-9" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>10 0.20382957 <a title="202-lsi-10" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>11 0.17649022 <a title="202-lsi-11" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>12 0.17528576 <a title="202-lsi-12" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<p>13 0.174492 <a title="202-lsi-13" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>14 0.17297684 <a title="202-lsi-14" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>15 0.17242646 <a title="202-lsi-15" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>16 0.17208484 <a title="202-lsi-16" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>17 0.1709352 <a title="202-lsi-17" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>18 0.16823165 <a title="202-lsi-18" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>19 0.15964922 <a title="202-lsi-19" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>20 0.15008116 <a title="202-lsi-20" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.272), (25, 0.044), (26, 0.029), (28, 0.034), (30, 0.022), (37, 0.025), (39, 0.03), (57, 0.09), (74, 0.038), (82, 0.018), (83, 0.014), (84, 0.04), (85, 0.046), (90, 0.092), (92, 0.043), (94, 0.017), (99, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7197848 <a title="202-lda-1" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>Author: Emad Mohamed ; Behrang Mohit ; Kemal Oflazer</p><p>Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-ofvocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.</p><p>2 0.70299941 <a title="202-lda-2" href="./acl-2012-Chinese_Comma_Disambiguation_for_Discourse_Analysis.html">47 acl-2012-Chinese Comma Disambiguation for Discourse Analysis</a></p>
<p>Author: Yaqin Yang ; Nianwen Xue</p><p>Abstract: The Chinese comma signals the boundary of discourse units and also anchors discourse relations between adjacent text spans. In this work, we propose a discourse structureoriented classification of the comma that can be automatically extracted from the Chinese Treebank based on syntactic patterns. We then experimented with two supervised learning methods that automatically disambiguate the Chinese comma based on this classification. The first method integrates comma classification into parsing, and the second method adopts a “post-processing” approach that extracts features from automatic parses to train a classifier. The experimental results show that the second approach compares favorably against the first approach.</p><p>3 0.49838883 <a title="202-lda-3" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>Author: William Yang Wang ; Elijah Mayfield ; Suresh Naidu ; Jeremiah Dittmar</p><p>Abstract: We propose a latent variable model to enhance historical analysis of large corpora. This work extends prior work in topic modelling by incorporating metadata, and the interactions between the components in metadata, in a general way. To test this, we collect a corpus of slavery-related United States property law judgements sampled from the years 1730 to 1866. We study the language use in these legal cases, with a special focus on shifts in opinions on controversial topics across different regions. Because this is a longitudinal data set, we are also interested in understanding how these opinions change over the course of decades. We show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identification tasks. Experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting, and that these improvements are robust across different parameter settings.</p><p>4 0.4981035 <a title="202-lda-4" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>5 0.4913829 <a title="202-lda-5" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>6 0.48727003 <a title="202-lda-6" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>7 0.46211314 <a title="202-lda-7" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>8 0.4473418 <a title="202-lda-8" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>9 0.44600889 <a title="202-lda-9" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>10 0.44543314 <a title="202-lda-10" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>11 0.44361457 <a title="202-lda-11" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>12 0.44113129 <a title="202-lda-12" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>13 0.43819866 <a title="202-lda-13" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>14 0.4364374 <a title="202-lda-14" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>15 0.43398672 <a title="202-lda-15" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>16 0.43391258 <a title="202-lda-16" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>17 0.43348429 <a title="202-lda-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.43267348 <a title="202-lda-18" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>19 0.43251249 <a title="202-lda-19" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>20 0.43223634 <a title="202-lda-20" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
