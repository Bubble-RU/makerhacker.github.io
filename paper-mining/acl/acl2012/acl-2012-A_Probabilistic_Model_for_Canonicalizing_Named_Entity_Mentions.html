<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-18" href="#">acl2012-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</h1>
<br/><p>Source: <a title="acl-2012-18-pdf" href="http://aclweb.org/anthology//P/P12/P12-1072.pdf">pdf</a></p><p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>Reference: <a title="acl-2012-18-reference" href="../acl2012_reference/acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  Abstract We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). [sent-4, score-1.152]
</p><p>2 The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. [sent-5, score-0.222]
</p><p>3 Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. [sent-6, score-0.383]
</p><p>4 We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work. [sent-7, score-0.6]
</p><p>5 1 Introduction Proper handling of mentions in text of real-world entities—identifying and resolving them—is a central part of many NLP applications. [sent-8, score-0.287]
</p><p>6 We seek an algorithm that infers a set of real-world entities from  mentions in a text, mapping each entity mention token to an entity, and discovers general categories of words used in names (e. [sent-9, score-0.908]
</p><p>7 Here, we use a probabilistic model to infer a structured representation of canonical forms of entity attributes through transductive learning from named entity mentions with a small number of seeds (see Table 1). [sent-12, score-0.809]
</p><p>8 The input is a collection of mentions found by a named entity recognizer, along with their contexts, and, following Eisenstein et al. [sent-13, score-0.491]
</p><p>9 (201 1), the output is a table in which entities are rows (the number of which is not pre-specified) and attribute words are organized into columns. [sent-14, score-0.35]
</p><p>10 In a political blogs dataset, the mentions refer to political figures in the United States (e. [sent-19, score-0.489]
</p><p>11 , Michelle, Obamai— ewrshile p simultaneously performing coreference resolution for named entity mentions. [sent-24, score-0.314]
</p><p>12 In the sports news dataset, the model is provided with named entity mentions of heterogenous types, and success here consists of identifying the correct team for every player (e. [sent-25, score-0.891]
</p><p>13 In this scenario, given a few seed examples, the model begins to identify simple relations among named entities (in addition to discovering attribute structures), since attributes are expressed as named entities across multiple mentions. [sent-28, score-0.616]
</p><p>14 leeyrins Table 1: Seeds for politics (above) and sports (below). [sent-33, score-0.472]
</p><p>15 Top, the generation of the table: C is the number of attributes/columns, the number of rows is infinite, α is a vector of concentration parameters, φ is a multinomial distribution over strings, and x is a word in a table cell. [sent-37, score-0.216]
</p><p>16 Lower left, for choosing entities to be mentioned: τ determines the stick lengths and η is the distribution over entities to be selected for mention. [sent-38, score-0.328]
</p><p>17 erating mentions: M is the number of mentions in the data, w is a word token set from an entity/row r and attribute/column c. [sent-40, score-0.327]
</p><p>18 2  Model  We begin by assuming as input a set of mention tokens, each one or more words. [sent-44, score-0.285]
</p><p>19 In our experiments these are obtained by running a named entity recognizer. [sent-45, score-0.204]
</p><p>20 The output is a table in which rows are understood to correspond to entities (types, not mention tokens) and columns are fields, each associated with an attribute or a part of it. [sent-46, score-0.784]
</p><p>21 The generative story, depicted in Figure 1, is: • For each column j ∈ {1, . [sent-50, score-0.201]
</p><p>22 For each row i(ofwhich Gtheenree are infinitely many), adcrhawro an entry xi,j for cell i,j from φj. [sent-59, score-0.331]
</p><p>23 A few of these entries (the seeds) are observed; we denote those ◦ Draw weights βj that associate shape and posDirtiaowna wl feeigathutrse sβ with columns from a 0-mean, µ-variance Laplace distribution. [sent-60, score-0.205]
</p><p>24 For each row r: ◦ eDnerarwate a monutlextitn odmisitarilb over sth. [sent-63, score-0.262]
</p><p>25 For each mention token m: ◦ Dr eraawch an entity/row r ∼ η. [sent-65, score-0.368]
</p><p>26 ion w, given some of iFtso rf eeaatcuhre ws f (assumed observed): Choose a column c ∼ Z1 This uses a log-linear d cist ∼ribution with partition function Z. [sent-67, score-0.201]
</p><p>27 first-order dependencies among the columns are enabled; these introduce a dynamic character to the graphical model that is not shown in Figure 1. [sent-71, score-0.223]
</p><p>28 In the E-step, we perform collapsed Gibbs sampling to obtain distributions over row and column indices for every mention, given the current value of the hyperparamaters. [sent-98, score-0.605]
</p><p>29 1 E-step For the mth mention, we sample row index r, then for each word wm‘, we sample column index c. [sent-101, score-0.727]
</p><p>30 (201 1), when we sample the row for a mention, we use Bayes’ rule and marginalize the columns. [sent-105, score-0.313]
</p><p>31 ) ∝ p(rm = r | r−m, η) (Q‘ Pc p(wm‘ | x, rm = r, cm‘ = c)) (QQt pP(smt | rm = r)) 687  We consider each quantity in turn. [sent-110, score-0.238]
</p><p>32 The probability of drawing a row index follows a stick breaking distribution. [sent-112, score-0.38]
</p><p>33 This allows us to have an unbounded number of rows and let the model infer the optimal value from data. [sent-113, score-0.21]
</p><p>34 A standard marginalization of η gives us:  p(rm= r | r−m,τ) =( N N+τ−r+mτ ioft Nher−rwmise>, 0 where N is the number of mentions, Nr is the number of mentions assigned to row r, and Nr−m is the number of mentions assigned to row r, excluding m. [sent-114, score-1.136]
</p><p>35 In order to compute the likelihood of observing mentions in the dataset, we have to consider a few cases. [sent-116, score-0.321]
</p><p>36 With base distribution G0,1 and marginalizing φ, we have:  p(wm‘ | x, rm = r, cm‘ = c, αc) =  (1)     G0(wNmc−N1‘mc)−w? [sent-123, score-0.216]
</p><p>37 ‘c−αmcα‘+αci f x xr c = 6∈={w∅ wm a n‘ d ,∅ N }cw =>0 where Nc−m‘ is the number of cells in column c that are not empty and Nc−wm‘ is the number of cells in column c that are set to the word wm‘; both counts excluding the current word under consideration. [sent-125, score-0.55]
</p><p>38 It is important to be able to use context information to determine which row a mention should go into. [sent-127, score-0.598]
</p><p>39 As a novel extension, our model also uses surrounding words of a mention as its “context”—similar context words can encourage two mentions that do not share any words to be merged. [sent-128, score-0.658]
</p><p>40 For every row in the table, we have a multinomial distribution over context vocabulary θr from a Dirichlet prior λ. [sent-130, score-0.432]
</p><p>41 2 Sampling Columns Our column sampling procedure is novel to this work and substantially differs from that of Eisenstein et al. [sent-135, score-0.297]
</p><p>42 First, we note that when we sample column indices for each word in a mention, the row index for the mention r has already been sampled. [sent-137, score-0.926]
</p><p>43 Also, our model has interdependencies among column indices of a mention. [sent-138, score-0.282]
</p><p>44 For faster mixing, we experiment with first-order dependencies between columns when sampling column indices. [sent-140, score-0.485]
</p><p>45 We sample the column index c1 for the first word  in the mention, marginalizing out probabilities of other words in the mention. [sent-144, score-0.389]
</p><p>46 After we sample the column index for the first word, we sample the column index c2 for the second word, fixing the previous word to be in column c1, and marginalizing out probabilities of c3, . [sent-145, score-0.995]
</p><p>47 (Pwm‘ | x, rm = r, cm‘ = c, αc) , |  x,  2As shown in Figure 1, column indices in a mention form “v-structures” with the row index r. [sent-160, score-0.994]
</p><p>48 Our model can use arbitrary features to choose a column index. [sent-166, score-0.236]
</p><p>49 Our transition probabilities are as follows:  p(cm‘= c | cm‘−= c−) =PcN0c−−Nm,cc−0m,c, where Nc−m,c is the number of times we observe transitions from column c− to c, excluding mention m. [sent-172, score-0.557]
</p><p>50 Mention likelihood p(wm‘ | x, rm = r, cm‘ = c, αc) is identical to when we sample the row index (Eq. [sent-175, score-0.513]
</p><p>51 For each word in a mention w, we introduced 12 binary features f for our featurized log-linear distribution (§3. [sent-181, score-0.363]
</p><p>52 ncased all words in mentions for the purpose of defining the table and the mention words w. [sent-187, score-0.572]
</p><p>53 Ten context words (5 each to the left and right) define s for each mention token. [sent-188, score-0.336]
</p><p>54 1  Datasets  We collected named entity mentions from two corpora: political blogs and sports news. [sent-197, score-0.906]
</p><p>55 The political blogs corpus is a collection of blog posts about politics in the United States (Eisenstein and Xing, 2010), andthe sports news corpus contains news summaries of major league sports games (National Basketball 3On our moderate-sized datasets (see §4. [sent-198, score-0.992]
</p><p>56 , 2005), which tagged named entities mentions as person, location, and organization. [sent-217, score-0.501]
</p><p>57 We used named entity of type person from the political blogs corpus, while we are interested in person and organization entities for the sports news corpus. [sent-218, score-0.897]
</p><p>58 Table 2 summarizes statistics for both datasets of named entity mentions. [sent-220, score-0.245]
</p><p>59 ’s manually built 125-entity (282 vocabulary items) reference table for the politics dataset. [sent-223, score-0.3]
</p><p>60 For the sports data, we obtained a roster of all NBA, NFL, and MLB players in 2009. [sent-225, score-0.366]
</p><p>61 We built our sports reference table using the players’ names, teams and locations, to get 3,642 players and 15,932 vocabulary items. [sent-226, score-0.512]
</p><p>62 The gold standard table for the politics dataset is incomplete, whereas it is complete for the sports dataset. [sent-227, score-0.552]
</p><p>63 2 Evaluation Scores We propose both a row evaluation to determine how well a model disambiguates entities and merges mentions of the same entity and a column evaluation to measure how well the model relates words used in different mentions. [sent-231, score-1.06]
</p><p>64 The first step in evaluation is to find a maximum score bipartite matching between rows in the response and reference table. [sent-233, score-0.337]
</p><p>65 5 Given the response and 5Treating each row as a set of words, we can optimize the matching using the Jonker and Volgenant (1987) algorithm. [sent-234, score-0.347]
</p><p>66 The column evaluation is identical, except that sets of words that are matched are defined by columns. [sent-235, score-0.248]
</p><p>67 3  Baselines  Our simple baseline is an agglomerative clustering algorithm that clusters mentions into entities using the single-linkage criterion. [sent-241, score-0.521]
</p><p>68 Initially, each unique mention forms its own cluster that we incrementally merge together to form rows in the table. [sent-242, score-0.46]
</p><p>69 (201 1) and use the string edit distance between mention strings in each cluster to define the score. [sent-245, score-0.285]
</p><p>70 For the sports dataset, since mentions contain person and organization named entity types, our score for clustering uses the Jaccard distance between context words of the mentions. [sent-246, score-0.991]
</p><p>71 Therefore, we first  match words in mentions of type person against a regular expression to recognize entity attributes from a fixed set of titles and suffixes, and the first, middle and last names. [sent-248, score-0.555]
</p><p>72 We treat words in mentions of type organization as a single As we merge clusters together, we arrange words such that  attribute. [sent-249, score-0.344]
</p><p>73 Precision prefers a more compact response row (or column), which unfairly penalizes situations like those of our sports dataset, where rows are heterogeneous (e. [sent-252, score-0.808]
</p><p>74 perfect precision by matching tsihme fiilarsrtit row ntcot tiohen , r wef-e erence row for Kobe Bryant and the latter row to any Lakers player. [sent-261, score-0.786]
</p><p>75 690 all words within a column belong to the same attribute, creating columns as necessary  to accomo-  date multiple similar attributes as a result of merging. [sent-267, score-0.42]
</p><p>76 4 Results For both the politics and sports dataset, we run the procedure in §3. [sent-272, score-0.472]
</p><p>77 We also analyze how much each of our four main extensions (shape features, context information, noise, and first-order column dependencies) to EEA contributes to overall performance by ablating each in turn (also shown in Fig. [sent-278, score-0.252]
</p><p>78 Indeed, the best model did not have column dependencies. [sent-282, score-0.236]
</p><p>79 In row evaluation, the baseline system can achieve a high reference score by creating one entity row per unique string, but as it merges strings, the clusters encompass more word tokens, improving response score at the expense of reference score. [sent-284, score-0.878]
</p><p>80 With fewer clusters, there are fewer entities in the response table for matching and the response score suffers. [sent-285, score-0.295]
</p><p>81 In column evaluation, our method without column dependencies was best. [sent-288, score-0.441]
</p><p>82 The results for the sports dataset are shown in Figure 3. [sent-290, score-0.366]
</p><p>83 Our best-performing complete model’s response table contains 599 rows, of which 561 were matched to the reference table. [sent-291, score-0.209]
</p><p>84 3 5-depc-nbfocdaemo-sanet pEuoltcErei nxsetA set reference score  reference score  Figure 2: Row (left) and column (right) scores for the politics dataset. [sent-303, score-0.541]
</p><p>85 5-depc-nbfocdaemo-san EetpuolcEire nAxset set reference score  reference score  Figure 3: Row (left) and column (right) scores for the sports dataset. [sent-320, score-0.641]
</p><p>86 The reference score is low since the reference set includes all entities in the NBA, NFL, and MLB, but most of them were  not  mentioned in our dataset. [sent-322, score-0.279]
</p><p>87 uation, our model lies above the baseline responsereference score curve, demonstrating its ability to  correctly identify and combine player mentions with their team names. [sent-323, score-0.401]
</p><p>88 Similar to the previous dataset, our model is also substantially better in column evaluation, indicating that it mapped mention words into a coherent set of five columns. [sent-324, score-0.521]
</p><p>89 In the politics dataset, most of the mentions contain information about people. [sent-328, score-0.473]
</p><p>90 Therefore, besides canonicalizing named entities, the model also resolves within-document and cross-document coreference, since it assigned a row index for every mention. [sent-329, score-0.574]
</p><p>91 For example, our model learned that most mentions of John McCain, Sen. [sent-330, score-0.322]
</p><p>92 Hus ein  Table 3: A small segment of the output table for the politics dataset, showing a few noteworthy correct (blue) and incorrect (red) examples. [sent-337, score-0.254]
</p><p>93 In the sports dataset, persons and organizations are mentioned. [sent-353, score-0.286]
</p><p>94 Our model can do better, since it makes use of context information and features, and it can put a person and an organization in one row even though they do not share common words. [sent-356, score-0.502]
</p><p>95 Surprisingly, the model failed to infer that Derek  Jeter plays for New York Yankees, although mentions of the organization New York Yankees can be found in our dataset. [sent-358, score-0.379]
</p><p>96 The next two rows show cases where our model successfully matched players with their teams and put each word token to its respective column. [sent-361, score-0.458]
</p><p>97 The sixth row is interesting because Dave Toub is indeed affiliated with the Chicago Bears. [sent-365, score-0.262]
</p><p>98 However, when the model saw a mention token The Bears, it did not have any other columns to put the word token The, and decided to put it in the fourth column although it is not a location. [sent-366, score-0.848]
</p><p>99 Our model is focused on the problem of canonicalizing mention strings into their parts, though its r variables (which map mentions to rows) could be interpreted as (within-document and cross-document) coreference resolution, which has been tackled using a range of probabilistic models (Li et al. [sent-374, score-0.787]
</p><p>100 6  Conclusions  We presented an improved probabilistic model for canonicalizing named entities into a table. [sent-378, score-0.356]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mentions', 0.287), ('sports', 0.286), ('mention', 0.285), ('row', 0.262), ('eisenstein', 0.243), ('cm', 0.222), ('column', 0.201), ('politics', 0.186), ('rows', 0.175), ('columns', 0.149), ('entities', 0.125), ('rm', 0.119), ('entity', 0.115), ('wm', 0.11), ('canonicalizing', 0.107), ('seeds', 0.098), ('sampling', 0.096), ('mccain', 0.093), ('named', 0.089), ('eea', 0.085), ('response', 0.085), ('index', 0.081), ('dataset', 0.08), ('players', 0.08), ('reference', 0.077), ('bayesian', 0.077), ('coreference', 0.073), ('political', 0.073), ('fixing', 0.072), ('attributes', 0.07), ('cell', 0.069), ('noteworthy', 0.068), ('league', 0.064), ('xref', 0.064), ('backward', 0.058), ('sim', 0.058), ('clustering', 0.058), ('organization', 0.057), ('shape', 0.056), ('blogs', 0.056), ('marginalizing', 0.056), ('names', 0.056), ('agglomerative', 0.051), ('bryant', 0.051), ('laplace', 0.051), ('sample', 0.051), ('context', 0.051), ('gibbs', 0.05), ('attribute', 0.05), ('nr', 0.049), ('put', 0.049), ('person', 0.048), ('matched', 0.047), ('indices', 0.046), ('player', 0.045), ('zero', 0.044), ('eraawch', 0.043), ('hkobe', 0.043), ('jonker', 0.043), ('kobe', 0.043), ('lakersi', 0.043), ('mlb', 0.043), ('nfl', 0.043), ('sref', 0.043), ('initialized', 0.043), ('pc', 0.041), ('dr', 0.041), ('prior', 0.041), ('datasets', 0.041), ('distribution', 0.041), ('token', 0.04), ('dependencies', 0.039), ('haghighi', 0.039), ('excluding', 0.038), ('featurized', 0.037), ('jensen', 0.037), ('lakers', 0.037), ('nba', 0.037), ('palin', 0.037), ('sres', 0.037), ('stick', 0.037), ('yankees', 0.037), ('yano', 0.037), ('resolution', 0.037), ('vocabulary', 0.037), ('obama', 0.036), ('model', 0.035), ('titles', 0.035), ('monte', 0.035), ('elsner', 0.034), ('michelle', 0.034), ('mochihashi', 0.034), ('pb', 0.034), ('observing', 0.034), ('team', 0.034), ('transition', 0.033), ('nonparametric', 0.033), ('notion', 0.033), ('seed', 0.033), ('teams', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="18-tfidf-1" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>2 0.26304424 <a title="18-tfidf-2" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>Author: Michael Wick ; Sameer Singh ; Andrew McCallum</p><p>Abstract: Sameer Singh Andrew McCallum University of Massachusetts University of Massachusetts 140 Governor’s Drive 140 Governor’s Drive Amherst, MA Amherst, MA s ameer@ cs .umas s .edu mccal lum@ c s .umas s .edu Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap- proach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</p><p>3 0.1795374 <a title="18-tfidf-3" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>4 0.17586917 <a title="18-tfidf-4" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>Author: Limin Yao ; Sebastian Riedel ; Andrew McCallum</p><p>Abstract: To discover relation types from text, most methods cluster shallow or syntactic patterns of relation mentions, but consider only one possible sense per pattern. In practice this assumption is often violated. In this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns. In particular, we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features. We merge these sense clusters into semantic relations using hierarchical agglomerative clustering. We compare against several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial.</p><p>5 0.17123471 <a title="18-tfidf-5" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>Author: Einat Minkov ; Luke Zettlemoyer</p><p>Abstract: This paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention de- tection and template filling tasks.</p><p>6 0.15126997 <a title="18-tfidf-6" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>7 0.14500125 <a title="18-tfidf-7" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>8 0.11099153 <a title="18-tfidf-8" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>9 0.10801408 <a title="18-tfidf-9" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>10 0.10736392 <a title="18-tfidf-10" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>11 0.09804225 <a title="18-tfidf-11" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>12 0.095324039 <a title="18-tfidf-12" href="./acl-2012-SITS%3A_A_Hierarchical_Nonparametric_Model_using_Speaker_Identity_for_Topic_Segmentation_in_Multiparty_Conversations.html">171 acl-2012-SITS: A Hierarchical Nonparametric Model using Speaker Identity for Topic Segmentation in Multiparty Conversations</a></p>
<p>13 0.091649301 <a title="18-tfidf-13" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>14 0.087460823 <a title="18-tfidf-14" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>15 0.085370369 <a title="18-tfidf-15" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>16 0.078293733 <a title="18-tfidf-16" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>17 0.078232966 <a title="18-tfidf-17" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>18 0.075244531 <a title="18-tfidf-18" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>19 0.072374165 <a title="18-tfidf-19" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>20 0.068133742 <a title="18-tfidf-20" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.223), (1, 0.132), (2, -0.015), (3, 0.125), (4, 0.002), (5, 0.147), (6, -0.031), (7, 0.063), (8, 0.132), (9, -0.057), (10, 0.071), (11, -0.24), (12, -0.168), (13, -0.12), (14, 0.029), (15, 0.098), (16, 0.076), (17, 0.092), (18, -0.293), (19, -0.05), (20, -0.036), (21, 0.029), (22, -0.009), (23, -0.016), (24, 0.052), (25, -0.0), (26, -0.031), (27, 0.059), (28, -0.032), (29, -0.027), (30, -0.109), (31, 0.049), (32, 0.004), (33, -0.055), (34, 0.038), (35, -0.028), (36, -0.074), (37, 0.082), (38, 0.018), (39, 0.064), (40, -0.021), (41, -0.019), (42, -0.001), (43, -0.052), (44, -0.022), (45, -0.013), (46, -0.058), (47, 0.11), (48, -0.026), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96050853 <a title="18-lsi-1" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>2 0.88313401 <a title="18-lsi-2" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>Author: Michael Wick ; Sameer Singh ; Andrew McCallum</p><p>Abstract: Sameer Singh Andrew McCallum University of Massachusetts University of Massachusetts 140 Governor’s Drive 140 Governor’s Drive Amherst, MA Amherst, MA s ameer@ cs .umas s .edu mccal lum@ c s .umas s .edu Hamming” who authored “The unreasonable effectiveness of mathematics.” Features of the mentions Methods that measure compatibility between mention pairs are currently the dominant ap- proach to coreference. However, they suffer from a number of drawbacks including difficulties scaling to large numbers of mentions and limited representational power. As these drawbacks become increasingly restrictive, the need to replace the pairwise approaches with a more expressive, highly scalable alternative is becoming urgent. In this paper we propose a novel discriminative hierarchical model that recursively partitions entities into trees of latent sub-entities. These trees succinctly summarize the mentions providing a highly compact, information-rich structure for reasoning about entities and coreference uncertainty at massive scales. We demonstrate that the hierarchical model is several orders of magnitude faster than pairwise, allowing us to perform coreference on six million author mentions in under four hours on a single CPU.</p><p>3 0.74636245 <a title="18-lsi-3" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: To address semantic ambiguities in coreference resolution, we use Web n-gram features that capture a range of world knowledge in a diffuse but robust way. Specifically, we exploit short-distance cues to hypernymy, semantic compatibility, and semantic context, as well as general lexical co-occurrence. When added to a state-of-the-art coreference baseline, our Web features give significant gains on multiple datasets (ACE 2004 and ACE 2005) and metrics (MUC and B3), resulting in the best results reported to date for the end-to-end task of coreference resolution.</p><p>4 0.69888812 <a title="18-lsi-4" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>Author: Katja Markert ; Yufang Hou ; Michael Strube</p><p>Abstract: Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classification and focuses on conversational dialogue. We here introduce the task of classifying finegrained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status ofall mentions. Our approach strongly outperforms reimplementations of previous work.</p><p>5 0.661264 <a title="18-lsi-5" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>Author: Einat Minkov ; Luke Zettlemoyer</p><p>Abstract: This paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention de- tection and template filling tasks.</p><p>6 0.51892817 <a title="18-lsi-6" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>7 0.50696564 <a title="18-lsi-7" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>8 0.50085181 <a title="18-lsi-8" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>9 0.47699735 <a title="18-lsi-9" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>10 0.40847728 <a title="18-lsi-10" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>11 0.36427116 <a title="18-lsi-11" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>12 0.35415605 <a title="18-lsi-12" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>13 0.35395616 <a title="18-lsi-13" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>14 0.33646283 <a title="18-lsi-14" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>15 0.33125758 <a title="18-lsi-15" href="./acl-2012-A_Joint_Model_for_Discovery_of_Aspects_in_Utterances.html">14 acl-2012-A Joint Model for Discovery of Aspects in Utterances</a></p>
<p>16 0.3276659 <a title="18-lsi-16" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>17 0.3176626 <a title="18-lsi-17" href="./acl-2012-A_Nonparametric_Bayesian_Approach_to_Acoustic_Model_Discovery.html">16 acl-2012-A Nonparametric Bayesian Approach to Acoustic Model Discovery</a></p>
<p>18 0.30359858 <a title="18-lsi-18" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>19 0.30226609 <a title="18-lsi-19" href="./acl-2012-Efficient_Tree-Based_Topic_Modeling.html">79 acl-2012-Efficient Tree-Based Topic Modeling</a></p>
<p>20 0.29970378 <a title="18-lsi-20" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.24), (25, 0.014), (26, 0.04), (28, 0.048), (30, 0.02), (37, 0.041), (39, 0.075), (57, 0.013), (59, 0.011), (74, 0.032), (82, 0.027), (84, 0.022), (85, 0.023), (90, 0.168), (92, 0.084), (94, 0.026), (99, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87877595 <a title="18-lda-1" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>same-paper 2 0.81167209 <a title="18-lda-2" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>3 0.66654277 <a title="18-lda-3" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>Author: Dave Golland ; John DeNero ; Jakob Uszkoreit</p><p>Abstract: We present LLCCM, a log-linear variant ofthe constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</p><p>4 0.66573209 <a title="18-lda-4" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>5 0.66493291 <a title="18-lda-5" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>Author: Xiaohua Liu ; Furu Wei ; Ming Zhou ; QuickView Team Microsoft</p><p>Abstract: Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.</p><p>6 0.66173244 <a title="18-lda-6" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>7 0.66146314 <a title="18-lda-7" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>8 0.66122001 <a title="18-lda-8" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>9 0.66038245 <a title="18-lda-9" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>10 0.65931535 <a title="18-lda-10" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>11 0.65786451 <a title="18-lda-11" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>12 0.65780622 <a title="18-lda-12" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>13 0.65591389 <a title="18-lda-13" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>14 0.65500581 <a title="18-lda-14" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>15 0.65491456 <a title="18-lda-15" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>16 0.65475053 <a title="18-lda-16" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>17 0.65379769 <a title="18-lda-17" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>18 0.65330386 <a title="18-lda-18" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>19 0.6531834 <a title="18-lda-19" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<p>20 0.65252542 <a title="18-lda-20" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
