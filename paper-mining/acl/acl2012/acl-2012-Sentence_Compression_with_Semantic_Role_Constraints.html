<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 acl-2012-Sentence Compression with Semantic Role Constraints</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-176" href="#">acl2012-176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>176 acl-2012-Sentence Compression with Semantic Role Constraints</h1>
<br/><p>Source: <a title="acl-2012-176-pdf" href="http://aclweb.org/anthology//P/P12/P12-2068.pdf">pdf</a></p><p>Author: Katsumasa Yoshikawa ; Ryu Iida ; Tsutomu Hirao ; Manabu Okumura</p><p>Abstract: For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.</p><p>Reference: <a title="acl-2012-176-reference" href="../acl2012_reference/acl-2012-Sentence_Compression_with_Semantic_Role_Constraints_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Tsutomu Hirao NTT Communication Science Laboratories, NTT Corporation, Japan hirao  . [sent-3, score-0.036]
</p><p>2 jp Abstract  For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. [sent-7, score-0.441]
</p><p>3 These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. [sent-8, score-0.457]
</p><p>4 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. [sent-10, score-0.096]
</p><p>5 Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). [sent-11, score-0.471]
</p><p>6 Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). [sent-12, score-0.519]
</p><p>7 In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. [sent-13, score-0.146]
</p><p>8 Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. [sent-14, score-0.071]
</p><p>9 Figure 1 shows an example with a coordination structure. [sent-15, score-0.025]
</p><p>10 jp Manabu Okumura Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan oku@lr pi titech ac jp  . [sent-22, score-0.054]
</p><p>11 DependencyRelation  In this example, a SR labeler annotated that Harari is an A0 argument of left and an A1 argument of became. [sent-27, score-0.178]
</p><p>12 However, Harari is not dependent on became and we are hence unable to utilize a dependency relation between Harari and became di–  rectly. [sent-29, score-0.123]
</p><p>13 SRs allow us to model the relations between a predicate and its arguments in a direct fashion. [sent-30, score-0.227]
</p><p>14 SR constraints are also advantageous in that we can compress sentences with semantic information. [sent-31, score-0.263]
</p><p>15 In Figure 1, became has three arguments, Harari as A1, businessman as A2, and shortly afterward as AM-TMP. [sent-32, score-0.079]
</p><p>16 As shown in this example, shortly afterword can be omitted (shaded boxes). [sent-33, score-0.038]
</p><p>17 In general, modifier arguments like AM-TMP or AM-LOC are more likely to be reduced than complement cases like A0-A4. [sent-34, score-0.204]
</p><p>18 Liu and Gildea (2010) suggests that SR features contribute to generating more readable sentence in machine translation. [sent-36, score-0.041]
</p><p>19 We expect that SR features also help our system to improve readability in sentence compression and summarization. [sent-37, score-0.485]
</p><p>20 Before describing our system, we show the statistics in terms of predicates, arguments and their relaProce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-39, score-0.108]
</p><p>21 There are 3 137 verbal predicates and 7852 unique arguments. [sent-46, score-0.091]
</p><p>22 We performed SR labeling by LTH (Johansson and Nugues, 2008), an SR labeler for CoNLL2008 shared task. [sent-47, score-0.067]
</p><p>23 Based on the SR labels annotated by LTH, we investigated that, for all predicates in compression, how many their arguments were also in. [sent-48, score-0.203]
</p><p>24 Table 1 shows the survival ratio of main arguments in compression. [sent-49, score-0.179]
</p><p>25 Labels A0, A1, and A2 are complement case roles and over 85% of them survive  with their predicates. [sent-50, score-0.125]
</p><p>26 On the other hand, for modifier arguments (AM-X), survival ratios are down to lower than 65%. [sent-51, score-0.303]
</p><p>27 Our SR constraints implement the difference of survival ratios by SR labels. [sent-52, score-0.318]
</p><p>28 Note that dependency labels SBJ and OBJ generally correspond to SR labels A0 and A1, respectively. [sent-53, score-0.099]
</p><p>29 Thus, SR labels can connect much more arguments to their predicates. [sent-55, score-0.112]
</p><p>30 3 Approach This section describes our new approach to sentence compression. [sent-56, score-0.041]
</p><p>31 In order to introduce rich syntactic and semantic constraints to a sentence compression model, we employ Markov Logic (Richardson and Domingos, 2006). [sent-57, score-0.684]
</p><p>32 Since Markov Logic supports both soft and hard constraints, we can implement our SR constraints in simple and direct fashion. [sent-58, score-0.218]
</p><p>33 com/p/thebeast/  350 on building a set of formulae called Markov Logic Network (MLN). [sent-65, score-0.175]
</p><p>34 We have only one hidden MLN predicate, inComp(i) which models the decision we need to make: whether a token i is in compression or not. [sent-70, score-0.444]
</p><p>35 The other MLN predicates are called observed which provide features. [sent-71, score-0.113]
</p><p>36 With our MLN predicates defined, we can now go on to incorporate our intuition about the task using weighted first-order logic formulae. [sent-72, score-0.172]
</p><p>37 We define SR constraints and the other formulae in Sections 3. [sent-73, score-0.311]
</p><p>38 1 Semantic Role Constraints Semantic role labeling generally includes the three subtasks: predicate identification; argument role labeling; sense disambiguation. [sent-80, score-0.413]
</p><p>39 Our model exploits the results of predicate identification and argument role labeling. [sent-81, score-0.299]
</p><p>40 4 pred(i) and role(i, j,r) indicate the results of predicate identification and role labeling, respectively. [sent-82, score-0.229]
</p><p>41 First, the formula describing a local property of a predicate is pred(i) ⇒ inComp(i) (1) which denotes that, if token iis a predicate then iis in compression. [sent-83, score-0.694]
</p><p>42 A formula with exact one hidden predicate is called local formula. [sent-84, score-0.348]
</p><p>43 The formula reducing some predicates is pred(i) ∧ height(i, +n) ⇒ ¬inComp(i) (2) which implies that a predicate iis not in compression with n height in a dependency tree. [sent-86, score-1.011]
</p><p>44 As mentioned earlier, our SR constraints model the difference of the survival rate of role labels in compression. [sent-88, score-0.39]
</p><p>45 These formulae are called global formulae because they have more than two hidden MLN predicates. [sent-91, score-0.361]
</p><p>46 With global formulae, our model makes two decisions at a time. [sent-92, score-0.033]
</p><p>47 As a result, our system gives “1-Harari” more chance to survive in compression. [sent-95, score-0.048]
</p><p>48 We also add some extensions of Formula (3) combined with dep(i, j,+d) and path(i, j,+l) which enhance SR constraints. [sent-96, score-0.044]
</p><p>49 Note, all our SR constraints are “predicate-driven” (only ⇒  not ⇔ as oinn sFtroarimntusla a (13)). [sent-97, score-0.179]
</p><p>50 dBieccaatues-der an argument ⇒is usually rsel iante Fdo to multiple predicates, aitn i sa rdgiuffmiceunltt to model “argument-driven” formula. [sent-98, score-0.07]
</p><p>51 2 Lexical and Syntactic Features For lexical and syntactic features, we mainly refer to the previous work (McDonald, 2006; Clarke and Lapata, 2008). [sent-101, score-0.037]
</p><p>52 The first two formulae in this section capture the relation of the tokens with their lexical and syntactic properties. [sent-102, score-0.19]
</p><p>53 The formula describing such a local property of a word form is word(i, +w) ⇒ inComp(i) (7) which implies that a token iis in compression with a weight that depends on the word form. [sent-103, score-0.749]
</p><p>54 (10) is a combination of POS features and a height in a 351  dependency tree. [sent-107, score-0.109]
</p><p>55 The next formula combines POS bigram features with dependency relations. [sent-108, score-0.223]
</p><p>56 (11) Moreover, our model includes the following global formulae, dep(i, j,+d) ∧ inComp(i) ⇒ inComp(j) (12) dep(i, j,+d) ∧ inComp(i) ⇔ inComp(j) (13) which enforce the consistencies between head and modifier tokens. [sent-110, score-0.158]
</p><p>57 Formula (12) represents that if we include a head token in compression then its modifier must also be included. [sent-111, score-0.569]
</p><p>58 Formula (13) ensures that head and modifier words must be simultaneously kept in compression or dropped. [sent-112, score-0.539]
</p><p>59 Though Clarke and Lapata (2008) implemented these dependency constraints by ILP, we implement them by soft constraints of MLN. [sent-113, score-0.417]
</p><p>60 Note that Formula (12) expresses the same properties as Formula (3) replacing dep(i, j,+d) by role(i, j,+r). [sent-114, score-0.027]
</p><p>61 5 and dependency parsing by MST-parser (McDonald et al. [sent-119, score-0.041]
</p><p>62 In addition, LTH 6 was exploited to perform both dependency parsing and SR labeling. [sent-121, score-0.041]
</p><p>63 The first evaluation is dependency based evaluation same as Riezler et al. [sent-124, score-0.041]
</p><p>64 We performed dependency parsing on gold data and system outputs by RASP. [sent-126, score-0.069]
</p><p>65 In order to demonstrate how well our SR constraints keep correct predicate-argument structures in compression, we propose SRL based evaluation. [sent-128, score-0.158]
</p><p>66 We performed SR labeling on gold data 5http ://nlp . [sent-129, score-0.057]
</p><p>67 uk/research/ groups/nlp/rasp/ Original[A0They] [predsay] [A1the refugees will enhance productivity and economic growth]. [sent-141, score-0.152]
</p><p>68 MLN with SRL Gold Standard  [A0 They] [pred say] [A1 the refugees will enhance growth]. [sent-142, score-0.116]
</p><p>69 6-mile-long artificial lake to be known as the Roadford Reservoir]. [sent-146, score-0.087]
</p><p>70 MLN with SRL  [A0 A dam] will [pred hold] back [A1 a artificial lake to be known as the Roadford Reservoir]. [sent-147, score-0.115]
</p><p>71 2 Results Table 3 shows the results of our compression models by compression rate (CompR), dependencybased F1 (F1-Dep), and SRL-based F1 (F1-SRL). [sent-160, score-0.85]
</p><p>72 Therefore, we think the compression rate of the better system should get closer to that of human compression. [sent-166, score-0.436]
</p><p>73 Because MLN models have global constraints and can generate syntactically correct sentences. [sent-168, score-0.213]
</p><p>74 Our concern is how a model with SR constraints is superior to a model without them. [sent-169, score-0.18]
</p><p>75 The compression rate of MLN with SRL goes up to 73. [sent-172, score-0.436]
</p><p>76 SRL-based evaluation also shows that SR constraints actually help extract correct predicate-argument structures. [sent-176, score-0.158]
</p><p>77 It is difficult to directly compare our results with  those of state-of-the-art systems (Cohn and Lapata, 2009; Clarke and Lapata, 2010; Galanis and Androutsopoulos, 2010) since they have different testing sets and the results with different compression rates. [sent-178, score-0.414]
</p><p>78 However, though our MLN model with SR constraints utilizes no large-scale data, it is the only model which achieves close on 60% in F1-Dep. [sent-179, score-0.158]
</p><p>79 3 Error Analysis Table 4 indicates two critical examples which our SR constraints failed to compress correctly. [sent-181, score-0.229]
</p><p>80 For the first example, our model leaves an argument with its predicate because our SR constraints are “predicatedriven”. [sent-182, score-0.372]
</p><p>81 In addition, “say” is the main verb in this sentence and hard to be deleted due to the syntactic significance. [sent-183, score-0.078]
</p><p>82 The second example in Table 4 requires to identify a coreference relation between artificial lake and Roadford Reservour. [sent-184, score-0.11]
</p><p>83 We consider that discourse constraints (Clarke and Lapata, 2010) help our model handle these cases. [sent-185, score-0.193]
</p><p>84 Discourse and coreference information enable our model to select important arguments and their predicates. [sent-186, score-0.106]
</p><p>85 5 Conclusion In this paper, we proposed new semantic con-  straints for sentence compression. [sent-187, score-0.096]
</p><p>86 Our model with global constraints of semantic roles selected correct predicate-argument structures and successfully improved performance of sentence compression. [sent-188, score-0.314]
</p><p>87 We will also investigate the correlation between readability and SRLbased score by manual evaluations. [sent-190, score-0.03]
</p><p>88 Furthermore, we would like to combine discourse constraints with SR constraints. [sent-191, score-0.193]
</p><p>89 References  using ambiguity packing and stochastic disambigua-  James Clarke and Mirella Lapata. [sent-192, score-0.021]
</p><p>90 Global infer-  ence for sentence compression: An integer linear programming approach. [sent-194, score-0.041]
</p><p>91 Summarization beyond sentence extraction: A probabilistic approach to sentence compression. [sent-221, score-0.082]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('incomp', 0.481), ('compression', 0.414), ('mln', 0.383), ('sr', 0.187), ('formula', 0.182), ('constraints', 0.158), ('clarke', 0.154), ('formulae', 0.153), ('predicate', 0.144), ('srl', 0.129), ('harari', 0.12), ('lapata', 0.103), ('pred', 0.096), ('roadford', 0.096), ('survival', 0.096), ('wnc', 0.096), ('modifier', 0.092), ('predicates', 0.091), ('role', 0.085), ('arguments', 0.083), ('logic', 0.081), ('mcdonald', 0.078), ('dep', 0.074), ('dam', 0.072), ('galanis', 0.072), ('refugees', 0.072), ('reservoir', 0.072), ('compress', 0.071), ('iis', 0.071), ('argument', 0.07), ('height', 0.068), ('lake', 0.063), ('japan', 0.053), ('sbj', 0.051), ('cohn', 0.05), ('markov', 0.049), ('pos', 0.048), ('survive', 0.048), ('roles', 0.048), ('lth', 0.044), ('enhance', 0.044), ('richardson', 0.042), ('compr', 0.042), ('tsutomu', 0.042), ('srs', 0.042), ('mirella', 0.042), ('became', 0.041), ('sentence', 0.041), ('dependency', 0.041), ('androutsopoulos', 0.038), ('labeler', 0.038), ('shortly', 0.038), ('growth', 0.037), ('tokyo', 0.037), ('syntactic', 0.037), ('productivity', 0.036), ('hirao', 0.036), ('discourse', 0.035), ('semantic', 0.034), ('obj', 0.034), ('head', 0.033), ('global', 0.033), ('ntt', 0.032), ('extractive', 0.032), ('ratios', 0.032), ('ilp', 0.032), ('implement', 0.032), ('token', 0.03), ('riezler', 0.03), ('readability', 0.03), ('labeling', 0.029), ('labels', 0.029), ('complement', 0.029), ('soft', 0.028), ('gold', 0.028), ('back', 0.028), ('properties', 0.027), ('property', 0.027), ('jp', 0.027), ('johansson', 0.026), ('coordination', 0.025), ('summarization', 0.025), ('describing', 0.025), ('trevor', 0.025), ('artificial', 0.024), ('coreference', 0.023), ('called', 0.022), ('syntactically', 0.022), ('superior', 0.022), ('rate', 0.022), ('dimitrios', 0.021), ('crouch', 0.021), ('tracy', 0.021), ('gurobi', 0.021), ('packing', 0.021), ('kiril', 0.021), ('straints', 0.021), ('oinn', 0.021), ('iida', 0.021), ('mg', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="176-tfidf-1" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>Author: Katsumasa Yoshikawa ; Ryu Iida ; Tsutomu Hirao ; Manabu Okumura</p><p>Abstract: For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.</p><p>2 0.35826874 <a title="176-tfidf-2" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>Author: Dong Wang ; Xian Qian ; Yang Liu</p><p>Abstract: This paper presents a two-step approach to compress spontaneous spoken utterances. In the first step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences. In the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them. For evaluation, we compare our system output with multiple human references. Our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references.</p><p>3 0.17424405 <a title="176-tfidf-3" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>4 0.15838985 <a title="176-tfidf-4" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English transla- , tion tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</p><p>5 0.14432791 <a title="176-tfidf-5" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>Author: Nikhil Garg ; James Henserdon</p><p>Abstract: We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering.</p><p>6 0.078998767 <a title="176-tfidf-6" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>7 0.078877702 <a title="176-tfidf-7" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>8 0.076080061 <a title="176-tfidf-8" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>9 0.070018895 <a title="176-tfidf-9" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>10 0.057720322 <a title="176-tfidf-10" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>11 0.056915171 <a title="176-tfidf-11" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>12 0.056508038 <a title="176-tfidf-12" href="./acl-2012-Text-level_Discourse_Parsing_with_Rich_Linguistic_Features.html">193 acl-2012-Text-level Discourse Parsing with Rich Linguistic Features</a></p>
<p>13 0.054129612 <a title="176-tfidf-13" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>14 0.05361652 <a title="176-tfidf-14" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>15 0.053365406 <a title="176-tfidf-15" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<p>16 0.052239444 <a title="176-tfidf-16" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>17 0.051247839 <a title="176-tfidf-17" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>18 0.049689882 <a title="176-tfidf-18" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>19 0.048138402 <a title="176-tfidf-19" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>20 0.046736985 <a title="176-tfidf-20" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.143), (1, 0.019), (2, -0.1), (3, 0.001), (4, 0.002), (5, -0.036), (6, -0.03), (7, 0.059), (8, 0.002), (9, 0.139), (10, 0.005), (11, -0.052), (12, 0.061), (13, -0.094), (14, -0.308), (15, 0.109), (16, 0.116), (17, -0.124), (18, 0.077), (19, 0.122), (20, 0.111), (21, 0.087), (22, 0.096), (23, -0.075), (24, 0.096), (25, 0.129), (26, 0.102), (27, 0.036), (28, -0.299), (29, -0.242), (30, 0.05), (31, 0.11), (32, -0.177), (33, 0.03), (34, 0.041), (35, 0.206), (36, 0.137), (37, -0.08), (38, 0.087), (39, 0.115), (40, 0.098), (41, 0.188), (42, 0.139), (43, 0.003), (44, 0.039), (45, -0.014), (46, 0.031), (47, 0.064), (48, -0.011), (49, -0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95164406 <a title="176-lsi-1" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>Author: Katsumasa Yoshikawa ; Ryu Iida ; Tsutomu Hirao ; Manabu Okumura</p><p>Abstract: For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.</p><p>2 0.78825164 <a title="176-lsi-2" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>Author: Dong Wang ; Xian Qian ; Yang Liu</p><p>Abstract: This paper presents a two-step approach to compress spontaneous spoken utterances. In the first step, we use a sequence labeling method to determine if a word in the utterance can be removed, and generate n-best compressed sentences. In the second step, we use a discriminative training approach to capture sentence level global information from the candidates and rerank them. For evaluation, we compare our system output with multiple human references. Our results show that the new features we introduced in the first compression step improve performance upon the previous work on the same data set, and reranking is able to yield additional gain, especially when training is performed to take into account multiple references.</p><p>3 0.41144997 <a title="176-lsi-3" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>Author: Nikhil Garg ; James Henserdon</p><p>Abstract: We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering.</p><p>4 0.35215229 <a title="176-lsi-4" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>5 0.33652884 <a title="176-lsi-5" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>Author: Sindhu Raghavan ; Raymond Mooney ; Hyeonseo Ku</p><p>Abstract: Most information extraction (IE) systems identify facts that are explicitly stated in text. However, in natural language, some facts are implicit, and identifying them requires “reading between the lines”. Human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts. We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining firstorder logic and Bayesian networks, to infer additional implicit information from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach.</p><p>6 0.29779834 <a title="176-lsi-6" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>7 0.23707777 <a title="176-lsi-7" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>8 0.23120362 <a title="176-lsi-8" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>9 0.22250588 <a title="176-lsi-9" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>10 0.20267874 <a title="176-lsi-10" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>11 0.20073934 <a title="176-lsi-11" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>12 0.1930338 <a title="176-lsi-12" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>13 0.16972603 <a title="176-lsi-13" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>14 0.16929747 <a title="176-lsi-14" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>15 0.16602664 <a title="176-lsi-15" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>16 0.16255601 <a title="176-lsi-16" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>17 0.16035262 <a title="176-lsi-17" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>18 0.1592513 <a title="176-lsi-18" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>19 0.15920931 <a title="176-lsi-19" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>20 0.15801379 <a title="176-lsi-20" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(26, 0.046), (28, 0.026), (30, 0.012), (37, 0.069), (39, 0.041), (71, 0.022), (74, 0.013), (82, 0.025), (85, 0.022), (90, 0.082), (92, 0.039), (94, 0.427), (99, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94568557 <a title="176-lda-1" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>same-paper 2 0.83381742 <a title="176-lda-2" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>Author: Katsumasa Yoshikawa ; Ryu Iida ; Tsutomu Hirao ; Manabu Okumura</p><p>Abstract: For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.</p><p>3 0.81688607 <a title="176-lda-3" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>Author: Apoorv Agarwal ; Adinoyi Omuya ; Aaron Harnly ; Owen Rambow</p><p>Abstract: Many researchers have attempted to predict the Enron corporate hierarchy from the data. This work, however, has been hampered by a lack of data. We present a new, large, and freely available gold-standard hierarchy. Using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems.</p><p>4 0.815736 <a title="176-lda-4" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>5 0.78956419 <a title="176-lda-5" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>Author: JinYeong Bak ; Suin Kim ; Alice Oh</p><p>Abstract: In social psychology, it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship. We present a computational framework for automatically analyzing such self-disclosure behavior in Twitter conversations. Our framework uses text mining techniques to discover topics, emotions, sentiments, lexical patterns, as well as personally identifiable information (PII) and personally embarrassing information (PEI). Our preliminary results illustrate that in relationships with high relationship strength, Twitter users show significantly more frequent behaviors of self-disclosure.</p><p>6 0.50766551 <a title="176-lda-6" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>7 0.43870163 <a title="176-lda-7" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>8 0.43094411 <a title="176-lda-8" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>9 0.41337919 <a title="176-lda-9" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>10 0.40900964 <a title="176-lda-10" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>11 0.39792958 <a title="176-lda-11" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>12 0.39480108 <a title="176-lda-12" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>13 0.39467621 <a title="176-lda-13" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>14 0.38866007 <a title="176-lda-14" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>15 0.38768834 <a title="176-lda-15" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>16 0.37624329 <a title="176-lda-16" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>17 0.3757228 <a title="176-lda-17" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<p>18 0.37227255 <a title="176-lda-18" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>19 0.36915171 <a title="176-lda-19" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>20 0.36827019 <a title="176-lda-20" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
