<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-192" href="#">acl2012-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</h1>
<br/><p>Source: <a title="acl-2012-192-pdf" href="http://aclweb.org/anthology//P/P12/P12-2039.pdf">pdf</a></p><p>Author: Toshikazu Tajiri ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate con- tribution to tense/aspect error correction.</p><p>Reference: <a title="acl-2012-192-reference" href="../acl2012_reference/acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Tense and Aspect Error Correction for ESL Learners Using Global Context Toshikazu Tajiri Mamoru Komachi Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan { t o shika zu-t , komachi , mat su } @ i . [sent-1, score-0.155]
</p><p>2 j p s  Abstract As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. [sent-3, score-0.973]
</p><p>3 However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. [sent-4, score-0.232]
</p><p>4 One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. [sent-5, score-0.895]
</p><p>5 Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. [sent-6, score-0.665]
</p><p>6 In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. [sent-7, score-0.871]
</p><p>7 Our experiments show that the global context makes a moderate con-  tribution to tense/aspect error correction. [sent-8, score-0.546]
</p><p>8 1 Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. [sent-9, score-0.708]
</p><p>9 It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). [sent-10, score-0.658]
</p><p>10 However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. [sent-11, score-0.098]
</p><p>11 Thus, a variety of automatic methods for helping English learning and education have been proposed. [sent-12, score-0.04]
</p><p>12 The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al. [sent-13, score-0.726]
</p><p>13 , 2007; Rozovskaya and 198 Roth, 2011), that commonly occur in essays by ESL learners. [sent-15, score-0.25]
</p><p>14 On the other hand, tense and aspect errors have been little studied, even though they are also commonly found in learners’ essays (Lee and Seneff, 2006; Bitchener et al. [sent-16, score-0.644]
</p><p>15 For instance, Lee (2008) corrects English verb inflection errors, but  they do not deal with tense/aspect errors because the choice of tense and aspect highly depends on global context, which makes correction difficult. [sent-18, score-1.473]
</p><p>16 Consider the following sentences taken from a corpus of a Japanese learner of English. [sent-19, score-0.059]
</p><p>17 In this example, go in the second sentence should be written as went. [sent-22, score-0.088]
</p><p>18 It is difficult to correct this type of error because there are two choices for correction, namely went and will go. [sent-23, score-0.239]
</p><p>19 In this case, we can exploit global context to determine which correction is appropriate: the first sentence describes a past event, and the second sentence refers the first sentence. [sent-24, score-0.827]
</p><p>20 This deduction is easy for humans, but is difficult for machines. [sent-26, score-0.099]
</p><p>21 One way to incorporate such global context into tense/aspect error correction is to use a machine learning-based sequence labeling approach. [sent-27, score-0.971]
</p><p>22 Therefore, we regard the task as sequence labeling: each verb phrase in the document is labeled with  tense/aspect depending on surrounding labels. [sent-28, score-0.68]
</p><p>23 Our experiments show that global context makes a moderate contribution to tense/aspect correction. [sent-30, score-0.333]
</p><p>24 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi1c 9s8–202, 2  Tense/Aspect Error Corpus  Developing a high-quality tense and aspect error correction system requires a large corpus annotated with tense/aspect errors. [sent-34, score-0.875]
</p><p>25 Therefore, we constructed a large-scale tense/aspect corpus from Lang-8,3 a social networking service for learners of foreign languages. [sent-36, score-0.34]
</p><p>26 ESL learners post their writing to be collaboratively corrected by native speakers. [sent-37, score-0.477]
</p><p>27 We leverage these corrections in creating our tense/aspect annotation. [sent-38, score-0.043]
</p><p>28 Lang-8 has 300,000 users from 180 countries worldwide, with more than 580,000 entries, approximately 170,000 of them in English. [sent-39, score-0.095]
</p><p>29 4 After cleaning the data, the corpus consists of approximately 120,000 English entries  containing 2,000,000 verb phrases with 750,000 verb phrases having corrections. [sent-40, score-0.8]
</p><p>30 5 The annotated tense/aspect labels include 12 combinations of tense (past, present, future) and aspect (nothing, perfect, progressive, perfect progressive). [sent-41, score-0.348]
</p><p>31 3  Error Correction Using Global Context  As we described in Section 1, using only local information about the target verb phrase may lead to inaccurate correction of tense/aspect errors. [sent-42, score-0.994]
</p><p>32 Thus, we take into account global context: the relation between target and preceding/following verb phrases. [sent-43, score-0.521]
</p><p>33 In this paper, we formulate the task as sequence labeling, and use Conditional Random Fields (Lafferty, 2001), which provides state-of-the-art performance in sequence labeling while allowing flexible feature design for combining local and global feature sets. [sent-44, score-0.609]
</p><p>34 1 Local Features Table 1 shows the local features used to train the error correction model. [sent-46, score-0.661]
</p><p>35 2Konan-JIEM Learner Corpus Second Edition (http : / / gs k . [sent-47, score-0.05]
</p><p>36 5Note that not all the 750,000 verb phrases were corrected due to the misuse of tense/aspect. [sent-58, score-0.468]
</p><p>37 199 Table 1: Local features for a verb phrase  namedescription  We use dependency relations such as nsubj, dobj, aux, pobj, and advmod for syntactic features. [sent-59, score-0.446]
</p><p>38 If a sentence including a target verb phrase is a complex sentence, we use the conj feature and add either the main-clause or the sub-clause feature depending on whether the target verb is in the main clause or in a subordinate clause. [sent-60, score-1.157]
</p><p>39 In both sentences, we use the feature main-clause for the verb phrase pours, and sub-clause for the verb phrase rains along with the feature conj:when for both verb phrases. [sent-64, score-1.357]
</p><p>40 Regarding p-tmod, we extract a noun phrase including a word labeled tmod (temporal adverb). [sent-65, score-0.195]
</p><p>41 For instance, consider the following sentence containing a temporal adverb: (4) I had a good time last night. [sent-66, score-0.244]
</p><p>42 In (4), the word night is the head of the noun phrase last night and is a temporal noun,6 so we add the feature p-tmod:last night for the verb phrase had. [sent-67, score-1.257]
</p><p>43 Table 2 shows the value of the feature norm-p-tmod and the corresponding temporal keywords. [sent-69, score-0.246]
</p><p>44 We use norm-p-tmod when p-tmod 6We made our own temporal noun list. [sent-70, score-0.225]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('correction', 0.426), ('learners', 0.295), ('verb', 0.286), ('essays', 0.198), ('global', 0.191), ('esl', 0.181), ('tense', 0.172), ('temporal', 0.171), ('error', 0.151), ('night', 0.142), ('pours', 0.141), ('rains', 0.141), ('aspect', 0.126), ('progressive', 0.123), ('phrase', 0.104), ('nara', 0.099), ('komachi', 0.099), ('errors', 0.096), ('conj', 0.094), ('adverb', 0.087), ('labeling', 0.086), ('local', 0.084), ('cat', 0.084), ('corrected', 0.078), ('regard', 0.076), ('feature', 0.075), ('moderate', 0.074), ('past', 0.07), ('context', 0.068), ('depending', 0.068), ('tutor', 0.062), ('certificate', 0.062), ('ikoma', 0.062), ('misuse', 0.062), ('og', 0.062), ('nakatani', 0.062), ('tribution', 0.062), ('growing', 0.062), ('surrounding', 0.06), ('learner', 0.059), ('templates', 0.058), ('english', 0.057), ('chodorow', 0.056), ('mizumoto', 0.056), ('namedescription', 0.056), ('exams', 0.056), ('rozovskaya', 0.056), ('worldwide', 0.056), ('mat', 0.056), ('demand', 0.056), ('pobj', 0.056), ('aux', 0.056), ('dobj', 0.056), ('deduction', 0.056), ('feedback', 0.055), ('noun', 0.054), ('writing', 0.054), ('seneff', 0.053), ('mainstream', 0.053), ('summer', 0.053), ('cleaning', 0.053), ('brockett', 0.053), ('commonly', 0.052), ('go', 0.052), ('perfect', 0.05), ('collaboratively', 0.05), ('gs', 0.05), ('inaccurate', 0.05), ('sequence', 0.049), ('highly', 0.048), ('approximately', 0.048), ('corrects', 0.047), ('countries', 0.047), ('constantly', 0.047), ('al', 0.047), ('nothing', 0.045), ('inflection', 0.045), ('subordinate', 0.045), ('mamoru', 0.045), ('went', 0.045), ('trh', 0.045), ('pointwise', 0.045), ('networking', 0.045), ('target', 0.044), ('corrections', 0.043), ('january', 0.043), ('grammatical', 0.043), ('entries', 0.043), ('difficult', 0.043), ('nagata', 0.042), ('phrases', 0.042), ('helping', 0.04), ('concerning', 0.04), ('ol', 0.04), ('edition', 0.038), ('labeled', 0.037), ('last', 0.037), ('sentence', 0.036), ('lee', 0.036), ('choice', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="192-tfidf-1" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>Author: Toshikazu Tajiri ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate con- tribution to tense/aspect error correction.</p><p>2 0.26556242 <a title="192-tfidf-2" href="./acl-2012-Grammar_Error_Correction_Using_Pseudo-Error_Sentences_and_Domain_Adaptation.html">103 acl-2012-Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation</a></p>
<p>Author: Kenji Imamura ; Kuniko Saito ; Kugatsu Sadamitsu ; Hitoshi Nishikawa</p><p>Abstract: This paper presents grammar error correction for Japanese particles that uses discriminative sequence conversion, which corrects erroneous particles by substitution, insertion, and deletion. The error correction task is hindered by the difficulty of collecting large error corpora. We tackle this problem by using pseudoerror sentences generated automatically. Furthermore, we apply domain adaptation, the pseudo-error sentences are from the source domain, and the real-error sentences are from the target domain. Experiments show that stable improvement is achieved by using domain adaptation.</p><p>3 0.18399541 <a title="192-tfidf-3" href="./acl-2012-A_Meta_Learning_Approach_to_Grammatical_Error_Correction.html">15 acl-2012-A Meta Learning Approach to Grammatical Error Correction</a></p>
<p>Author: Hongsuck Seo ; Jonghoon Lee ; Seokhwan Kim ; Kyusong Lee ; Sechun Kang ; Gary Geunbae Lee</p><p>Abstract: We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1.</p><p>4 0.16885982 <a title="192-tfidf-4" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>Author: John Lee ; Jonathan Webster</p><p>Abstract: This paper describes the creation of the first large-scale corpus containing drafts and final versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI).</p><p>5 0.14766827 <a title="192-tfidf-5" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>Author: Yukino Baba ; Hisami Suzuki</p><p>Abstract: This paper presents a comparative study of spelling errors that are corrected as you type, vs. those that remain uncorrected. First, we generate naturally occurring online error correction data by logging users’ keystrokes, and by automatically deriving pre- and postcorrection strings from them. We then perform an analysis of this data against the errors that remain in the final text as well as across languages. Our analysis shows a clear distinction between the types of errors that are generated and those that remain uncorrected, as well as across languages.</p><p>6 0.14490519 <a title="192-tfidf-6" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>7 0.11725818 <a title="192-tfidf-7" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>8 0.099030457 <a title="192-tfidf-8" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>9 0.098156318 <a title="192-tfidf-9" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>10 0.09116479 <a title="192-tfidf-10" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>11 0.09091758 <a title="192-tfidf-11" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>12 0.090770684 <a title="192-tfidf-12" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>13 0.086157441 <a title="192-tfidf-13" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<p>14 0.086069681 <a title="192-tfidf-14" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>15 0.085387617 <a title="192-tfidf-15" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>16 0.084508248 <a title="192-tfidf-16" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>17 0.082999505 <a title="192-tfidf-17" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>18 0.070545524 <a title="192-tfidf-18" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>19 0.070495531 <a title="192-tfidf-19" href="./acl-2012-FLOW%3A_A_First-Language-Oriented_Writing_Assistant_System.html">92 acl-2012-FLOW: A First-Language-Oriented Writing Assistant System</a></p>
<p>20 0.068365991 <a title="192-tfidf-20" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.177), (1, 0.086), (2, -0.075), (3, 0.076), (4, 0.04), (5, -0.051), (6, 0.057), (7, -0.059), (8, -0.106), (9, -0.183), (10, -0.117), (11, 0.01), (12, -0.008), (13, 0.154), (14, -0.112), (15, 0.032), (16, 0.105), (17, -0.308), (18, 0.07), (19, -0.262), (20, -0.242), (21, -0.167), (22, -0.058), (23, -0.033), (24, 0.082), (25, -0.067), (26, -0.0), (27, 0.039), (28, -0.068), (29, -0.032), (30, -0.079), (31, 0.015), (32, -0.037), (33, -0.037), (34, -0.041), (35, 0.062), (36, 0.026), (37, 0.07), (38, -0.073), (39, 0.074), (40, 0.067), (41, -0.039), (42, 0.02), (43, 0.025), (44, 0.021), (45, 0.006), (46, -0.008), (47, 0.008), (48, -0.017), (49, 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97795689 <a title="192-lsi-1" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>Author: Toshikazu Tajiri ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate con- tribution to tense/aspect error correction.</p><p>2 0.7768541 <a title="192-lsi-2" href="./acl-2012-A_Meta_Learning_Approach_to_Grammatical_Error_Correction.html">15 acl-2012-A Meta Learning Approach to Grammatical Error Correction</a></p>
<p>Author: Hongsuck Seo ; Jonghoon Lee ; Seokhwan Kim ; Kyusong Lee ; Sechun Kang ; Gary Geunbae Lee</p><p>Abstract: We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1.</p><p>3 0.74715388 <a title="192-lsi-3" href="./acl-2012-Grammar_Error_Correction_Using_Pseudo-Error_Sentences_and_Domain_Adaptation.html">103 acl-2012-Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation</a></p>
<p>Author: Kenji Imamura ; Kuniko Saito ; Kugatsu Sadamitsu ; Hitoshi Nishikawa</p><p>Abstract: This paper presents grammar error correction for Japanese particles that uses discriminative sequence conversion, which corrects erroneous particles by substitution, insertion, and deletion. The error correction task is hindered by the difficulty of collecting large error corpora. We tackle this problem by using pseudoerror sentences generated automatically. Furthermore, we apply domain adaptation, the pseudo-error sentences are from the source domain, and the real-error sentences are from the target domain. Experiments show that stable improvement is achieved by using domain adaptation.</p><p>4 0.67921925 <a title="192-lsi-4" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>Author: Yukino Baba ; Hisami Suzuki</p><p>Abstract: This paper presents a comparative study of spelling errors that are corrected as you type, vs. those that remain uncorrected. First, we generate naturally occurring online error correction data by logging users’ keystrokes, and by automatically deriving pre- and postcorrection strings from them. We then perform an analysis of this data against the errors that remain in the final text as well as across languages. Our analysis shows a clear distinction between the types of errors that are generated and those that remain uncorrected, as well as across languages.</p><p>5 0.64668012 <a title="192-lsi-5" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>Author: John Lee ; Jonathan Webster</p><p>Abstract: This paper describes the creation of the first large-scale corpus containing drafts and final versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI).</p><p>6 0.41443819 <a title="192-lsi-6" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>7 0.384992 <a title="192-lsi-7" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>8 0.34678563 <a title="192-lsi-8" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>9 0.34193256 <a title="192-lsi-9" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>10 0.34075028 <a title="192-lsi-10" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>11 0.33574697 <a title="192-lsi-11" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>12 0.31951115 <a title="192-lsi-12" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<p>13 0.31926957 <a title="192-lsi-13" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>14 0.31605634 <a title="192-lsi-14" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>15 0.31365857 <a title="192-lsi-15" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>16 0.30611917 <a title="192-lsi-16" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>17 0.30307561 <a title="192-lsi-17" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>18 0.29996365 <a title="192-lsi-18" href="./acl-2012-FLOW%3A_A_First-Language-Oriented_Writing_Assistant_System.html">92 acl-2012-FLOW: A First-Language-Oriented Writing Assistant System</a></p>
<p>19 0.28869164 <a title="192-lsi-19" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>20 0.28306961 <a title="192-lsi-20" href="./acl-2012-Automated_Essay_Scoring_Based_on_Finite_State_Transducer%3A_towards_ASR_Transcription_of_Oral_English_Speech.html">32 acl-2012-Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(26, 0.014), (28, 0.035), (39, 0.029), (74, 0.638), (82, 0.014), (85, 0.019), (90, 0.102), (92, 0.025), (94, 0.011), (99, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92514861 <a title="192-lda-1" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>Author: Toshikazu Tajiri ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: As the number of learners of English is constantly growing, automatic error correction of ESL learners’ writing is an increasingly active area of research. However, most research has mainly focused on errors concerning articles and prepositions even though tense/aspect errors are also important. One of the main reasons why tense/aspect error correction is difficult is that the choice of tense/aspect is highly dependent on global context. Previous research on grammatical error correction typically uses pointwise prediction that performs classification on each word independently, and thus fails to capture the information of neighboring labels. In order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate con- tribution to tense/aspect error correction.</p><p>2 0.85973102 <a title="192-lda-2" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>Author: Yunyao Li ; Laura Chiticariu ; Huahai Yang ; Frederick Reiss ; Arnaldo Carreno-fuentes</p><p>Abstract: Information extraction (IE) is becoming a critical building block in many enterprise applications. In order to satisfy the increasing text analytics demands of enterprise applications, it is crucial to enable developers with general computer science background to develop high quality IE extractors. In this demonstration, we present WizIE, an IE development environment intended to reduce the development life cycle and enable developers with little or no linguistic background to write high quality IE rules. WizI E provides an integrated wizard-like environment that guides IE developers step-by-step throughout the entire development process, based on best practices synthesized from the experience of expert developers. In addition, WizIE reduces the manual effort involved in performing key IE development tasks by offering automatic result explanation and rule discovery functionality. Preliminary results indicate that WizI E is a step forward towards enabling extractor development for novice IE developers.</p><p>3 0.73283237 <a title="192-lda-3" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>4 0.71876359 <a title="192-lda-4" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>5 0.51828372 <a title="192-lda-5" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>6 0.47229218 <a title="192-lda-6" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>7 0.39781147 <a title="192-lda-7" href="./acl-2012-Grammar_Error_Correction_Using_Pseudo-Error_Sentences_and_Domain_Adaptation.html">103 acl-2012-Grammar Error Correction Using Pseudo-Error Sentences and Domain Adaptation</a></p>
<p>8 0.39000586 <a title="192-lda-8" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>9 0.38891399 <a title="192-lda-9" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>10 0.38553411 <a title="192-lda-10" href="./acl-2012-FLOW%3A_A_First-Language-Oriented_Writing_Assistant_System.html">92 acl-2012-FLOW: A First-Language-Oriented Writing Assistant System</a></p>
<p>11 0.35967061 <a title="192-lda-11" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>12 0.35802662 <a title="192-lda-12" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>13 0.35753366 <a title="192-lda-13" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>14 0.35741258 <a title="192-lda-14" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>15 0.35264057 <a title="192-lda-15" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>16 0.34969392 <a title="192-lda-16" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>17 0.34872973 <a title="192-lda-17" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>18 0.34642744 <a title="192-lda-18" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>19 0.34081963 <a title="192-lda-19" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>20 0.33188528 <a title="192-lda-20" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
