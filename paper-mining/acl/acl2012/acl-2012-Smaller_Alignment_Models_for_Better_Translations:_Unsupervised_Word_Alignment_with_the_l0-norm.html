<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-179" href="#">acl2012-179</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</h1>
<br/><p>Source: <a title="acl-2012-179-pdf" href="http://aclweb.org/anthology//P/P12/P12-1033.pdf">pdf</a></p><p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>Reference: <a title="acl-2012-179-reference" href="../acl2012_reference/acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. [sent-2, score-0.78]
</p><p>2 Although many models have surpassed them in accuracy, none have supplanted them in practice. [sent-3, score-0.189]
</p><p>3 In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. [sent-4, score-0.373]
</p><p>4 We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6. [sent-5, score-0.501]
</p><p>5 1 Introduction  Automatic word alignment is a vital component of nearly all current statistical translation pipelines. [sent-8, score-0.536]
</p><p>6 Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. [sent-9, score-0.714]
</p><p>7 The dominant approach to word alignment has been the IBM models (Brown et al. [sent-10, score-0.394]
</p><p>8 These models are unsupervised, making them applicable to any language pair for which parallel text is available. [sent-13, score-0.1]
</p><p>9 311 In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. [sent-16, score-0.236]
</p><p>10 Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al. [sent-17, score-0.145]
</p><p>11 Other models are unsupervised like the IBM models (Liang et al. [sent-20, score-0.158]
</p><p>12 In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. [sent-24, score-0.699]
</p><p>13 , 1998), to encourage sparsity in the word-to-word translation model (Section 2. [sent-26, score-0.244]
</p><p>14 This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al. [sent-28, score-0.113]
</p><p>15 , 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2. [sent-29, score-0.38]
</p><p>16 Experiments on Czech-, Arabic-, Chinese- and UrduEnglish translation (Section 3) demonstrate consis-  tent significant improvements over IBM Model 4 in both word alignment (up to +6. [sent-31, score-0.583]
</p><p>17 Our implementation has been released as a simple modification to the GIZA++ toolkit that can be used as a drop-in replacement for GIZA++ in any existing MT pipeline. [sent-34, score-0.096]
</p><p>18 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi3c 1s1–319, 2  Method  We start with a brief review of the IBM and HMM word alignment models, then describe how to extend them with a smoothed ‘0 prior and how to efficiently train them. [sent-37, score-0.54]
</p><p>19 1 IBM Models and HMM Given a French string f = f1 · · · fj · · · fm and an English string e = e1 · · · ei · · · e‘, ·th fes·e· ·m fodels describe the process by ·w·heic·h· ethe French string is generated by the English string via the alignment a = a1, . [sent-39, score-0.364]
</p><p>20 Each aj is a hidden variables, indicating which English word eaj the French word fj is aligned to. [sent-46, score-0.388]
</p><p>21 In IBM Model 1–2 and the HMM model, the joint probability of the French sentence and alignment  given the English sentence is  P(f,a | e) =Yjm=1d(aj| aj−1, j)t(fj| eaj). [sent-47, score-0.287]
</p><p>22 (1)  The parameters of these models are the distortion probabilities d(aj | aj−1 ,j) and the translation probabilities t(fj | eaj). [sent-48, score-0.252]
</p><p>23 The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data: ˆθ = argθmin? [sent-55, score-0.103]
</p><p>24 In word alignment, one well-known manifestation of overfitting is that rare words can act as “garbage collectors” 312 (Moore, 2004), aligning to many unrelated words. [sent-61, score-0.126]
</p><p>25 We have previously proposed another simple remedy to overfitting in the context of unsupervised part-of-speech tagging (Vaswani et al. [sent-65, score-0.122]
</p><p>26 , 2010), which is to minimize the size of the model using a smoothed ‘0 prior. [sent-66, score-0.202]
</p><p>27 Applying this prior to an HMM improves tagging accuracy for both Italian and English. [sent-67, score-0.064]
</p><p>28 Here, our goal is to apply a similar prior in a word-alignment model to the word-to-word translation probabilities t(f | e). [sent-68, score-0.261]
</p><p>29 (6)  P(θ) ∝ and  is a smoothed approximation of the ‘0-norm. [sent-75, score-0.137]
</p><p>30 Substituting back into (4) and dropping constant terms, we get the following optimization problem: minimize  −logP(f | e,θ) − αXe,fexp−t(fβ | e)  (7)  subject to the constraints  Xft(f | e) = 1  for all e. [sent-77, score-0.178]
</p><p>31 (8)  We can carry out the optimization in (7) with the MAP-EM algorithm (Bishop, 2006). [sent-78, score-0.074]
</p><p>32 EM and MAPEM share the same E-step; the difference lies in the  Figure 1: The ‘0-norm (top curve) and smoothed approximations (below) for β = 0. [sent-79, score-0.137]
</p><p>33 For vanilla EM, the M-step is:  ˆθ = argθmin−Xe,fE[C(e, f)]logt(f | e)  (9)  again subject to the constraints (8). [sent-84, score-0.114]
</p><p>34 | e) −  (10)  This optimization problem is non-convex, and we do not know of a closed-form solution. [sent-87, score-0.074]
</p><p>35 , 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. [sent-89, score-0.466]
</p><p>36 3 Projected gradient descent Following Schoenemann (201 1b), we use projected gradient descent (PGD) to solve the M-step (but with the ‘0-norm instead of the ‘1-norm). [sent-92, score-0.544]
</p><p>37 Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). [sent-93, score-0.192]
</p><p>38 Let F(θ) be the objective function in 313 (10); we seek to minimize this function. [sent-94, score-0.111]
</p><p>39 Each iteration has two steps, a projection step and a line search. [sent-105, score-0.079]
</p><p>40 The gradient ∇F(θk) is  ∂t(∂fF | e)= −Et[(Cf( |f e,e))]+βαexp−t(βf | e)  (12)  In contrast to Schoenemann (201 1b), we use an O(n log n) algorithm for the projection step due to ODu(nchloi et. [sent-107, score-0.183]
</p><p>41 min then  Fmin = F(θk + δm) θmin = θk + δm  end if  ifFb r(θeka+k δm) ≤ F(θk) + σ? [sent-130, score-0.097]
</p><p>42 } W aen run tdhe n projection step and line search alternately for at most K iterations, terminating early if there is no change in θk from one iteration to the next. [sent-145, score-0.079]
</p><p>43 We measured the accuracy of word alignments generated by GIZA++ with and without the ‘0-norm, 314 and also translation accuracy of systems trained using the word alignments. [sent-150, score-0.514]
</p><p>44 Across all tests, we found strong improvements from adding the ‘0-norm. [sent-151, score-0.085]
</p><p>45 1 Training We have implemented our algorithm as an opensource extension to GIZA++. [sent-153, score-0.065]
</p><p>46 1 Usage of the extension is identical to standard GIZA++, except that the user can switch the ‘0 prior on or off, and adjust the  hyperparameters α and β. [sent-154, score-0.274]
</p><p>47 For vanilla EM, we ran five iterations of Model 1, five iterations of HMM, and ten iterations of Model 4. [sent-155, score-0.425]
</p><p>48 For our approach, we first ran one iteration of Model 1, followed by four iterations of Model 1 with smoothed ‘0, followed by five iterations of HMM with smoothed ‘0. [sent-156, score-0.571]
</p><p>49 We set probcutoff to 0 because we would like the optimization to learn the parameter values. [sent-171, score-0.074]
</p><p>50 For a fair comparison, we applied the same setting to our vanilla EM training as well. [sent-172, score-0.075]
</p><p>51 To test, we ran GIZA++ with the default setting on the smaller of our two Arabic-English datasets with the same number of iterations and found no change in F-score. [sent-173, score-0.164]
</p><p>52 ng  u u u u  w `aiji ¯ao xu ´ehu `ı hu `ızh ˇang  u li ´u u u sh ¯uq ¯ıng u hu `ıji `an sh ´ı uu u u z `aizu `o  . [sent-176, score-0.067]
</p><p>53 ru´g ˇo  u y `ao u u l `ul `u zhu ˇan uu q `u dehu` a  ne  ,  u u h ˇen u h ˇen u h ˇen u h ˇen u m ´afan de u (c)  over40 0guest fromhomeandabroadatentdhe dopenicnegrem. [sent-178, score-0.067]
</p><p>54 ony  u u u u u u  ,  zh ¯ongw `ai l´ aib ¯ın s` ıqi ¯an du ¯o  r  ´en  u  ch u¯x ´ı  u uu  le k ¯aim `ush `ı  . [sent-179, score-0.142]
</p><p>55 zh`eg u  ch `ul ˇı  w´ an u  y ˇıh `ou  ne u  ,  h´ ai u zh `a u le u s `ıge u di ¯aob ˇao u  . [sent-181, score-0.075]
</p><p>56 In particular, the baseline system demonstrates typical “garbage-collection” phenomena in proper name “shuqing” in both languages in (a), number “4000” and word “l a´ib ı¯n” (lit. [sent-184, score-0.091]
</p><p>57 Most interestingly, in (c), our smoothed-‘0 system correctly aligns “extremely” to “hˇ en hˇ en hˇ en h eˇn” (lit. [sent-189, score-0.234]
</p><p>58 315  Table 1: Adding the ‘0-norm to the IBM models improves both alignment and translation accuracy across four different  language pairs. [sent-191, score-0.579]
</p><p>59 The word trans column also shows that the number of distinct word translations lexical weighting table) is reduced. [sent-192, score-0.155]
</p><p>60 , the size of the  column shows the average fertility of once-seen source words. [sent-196, score-0.126]
</p><p>61 4  We set the hyperparameters α and β by tuning on gold-standard word alignments (to maximize F1) when possible. [sent-200, score-0.448]
</p><p>62 We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al. [sent-206, score-0.504]
</p><p>63 For models with the smoothed ‘0 prior, we tuned α and β separately in each direction. [sent-208, score-0.192]
</p><p>64 2 Alignment First, we evaluated alignment accuracy directly by  comparing against gold-standard word alignments. [sent-210, score-0.339]
</p><p>65 316 The results are shown in the alignment F1 column of Table 1. [sent-213, score-0.338]
</p><p>66 We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). [sent-214, score-0.287]
</p><p>67 Our alignments show smaller fertility for once-seen words, suggesting that they suffer from “garbage collection” effects less than the baseline alignments do. [sent-218, score-0.54]
</p><p>68 The fact that we had to use hand-aligned data to tune the hyperparameters α and β means that our method is no longer completely unsupervised. [sent-219, score-0.21]
</p><p>69 However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2. [sent-220, score-0.331]
</p><p>70 As we will see below, we still obtained strong improvements in translation quality when hand-aligned data was unavailable. [sent-221, score-0.244]
</p><p>71 We also tried generating 50 word classes using the tool provided in GIZA++. [sent-222, score-0.09]
</p><p>72 We found that adding  word classes improved alignment quality a little, but more so for the baseline system (see Table 3). [sent-223, score-0.454]
</p><p>73 We used the alignments generated by training with word classes for our translation experiments. [sent-224, score-0.5]
</p><p>74 Table  2: Almost  all hyperparameter settings achieve higher F-scores than the baseline IBM Model  4 and HMM  model  for Arabic-English alignment (α = 0). [sent-225, score-0.386]
</p><p>75 8  Table 3: Adding word classes improves the F-score in both directions for Arabic-English alignment by a little,  for the baseline system more so than ours. [sent-229, score-0.416]
</p><p>76 Figure 2 shows four examples of ChineseEnglish alignment, comparing the baseline with our smoothed-‘0 method. [sent-230, score-0.079]
</p><p>77 In all four cases, the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed-‘0 results are correct. [sent-231, score-0.489]
</p><p>78 In particular, the baseline system demonstrates typical “garbage collection” behavior (Moore, 2004) in all four examples. [sent-232, score-0.079]
</p><p>79 3  Translation  We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero (Chiang, 2007). [sent-234, score-0.659]
</p><p>80 21879 Table 4: Optimizing hyperparameters on alignment F1 score does not necessarily lead to optimal B . [sent-239, score-0.432]
</p><p>81 The first two columns indicate whether we used the first- or second-best alignments in each direction (according to F1); the third column shows the F1 of the symmetrized alignments, whose corresponding B scores are shown in the last two columns. [sent-240, score-0.405]
</p><p>82 For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. [sent-255, score-0.278]
</p><p>83 Ideally, one would want to tune α and β to maximize B . [sent-257, score-0.103]
</p><p>84 However, this is prohibitively expensive, especially if we must tune them separately  in each alignment direction before symmetrization. [sent-258, score-0.399]
</p><p>85 We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality. [sent-259, score-0.328]
</p><p>86 For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. [sent-260, score-0.347]
</p><p>87 Table 4 shows B scores for translation models learned from these alignments. [sent-261, score-0.252]
</p><p>88 Unfortunately, we find that optimizing F1 is not optimal for B —using the second-best alignments yields a further improvement of 0. [sent-262, score-0.213]
</p><p>89 In later work, Schoenemann (201 1b) used projected gradient descent for the ‘1norm. [sent-268, score-0.328]
</p><p>90 Here, we have adopted his use of projected gradient descent, but using a smoothed ‘0-norm. [sent-269, score-0.353]
</p><p>91 (2010) explore modifications to the HMM model that encourage bijectivity and symmetry. [sent-273, score-0.087]
</p><p>92 The modifications 318 take the form of constraints on the posterior distribution over alignments that is computed during the E-step. [sent-274, score-0.292]
</p><p>93 5  Conclusion  We have extended the IBM models and HMM model by the addition of an ‘0 prior to the word-to-word translation model, which compacts the word-toword translation table, reducing overfitting, and, in particular, the “garbage collection” effect. [sent-277, score-0.513]
</p><p>94 We have shown how to perform MAP-EM with this prior efficiently, even for large datasets. [sent-278, score-0.064]
</p><p>95 The method is implemented as a modification to the open-source  toolkit GIZA++, and we have shown that it significantly improves translation quality across four different language pairs. [sent-279, score-0.333]
</p><p>96 Even though we have used a small set of gold-standard alignments to tune our hyperparameters, we found that performance was fairly robust to variation in the hyperparameters, and translation performance was good even when goldstandard alignments were unavailable. [sent-280, score-0.732]
</p><p>97 We hope that our method, due to its simplicity, generality, and effectiveness, will find wide application for training better statistical translation systems. [sent-281, score-0.197]
</p><p>98 We thank Jason Riesa for providing the Arabic-English and Chinese-English hand-aligned data and the alignment visualization tool, and Chris Dyer for the Czech-English handaligned data. [sent-283, score-0.287]
</p><p>99 Online large-margin training of syntactic and structural translation features. [sent-318, score-0.197]
</p><p>100 Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging. [sent-400, score-0.168]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ibm', 0.292), ('alignment', 0.287), ('alignments', 0.213), ('giza', 0.201), ('translation', 0.197), ('vaswani', 0.175), ('schoenemann', 0.168), ('hmm', 0.154), ('hyperparameters', 0.145), ('smoothed', 0.137), ('aj', 0.119), ('projected', 0.112), ('descent', 0.112), ('gra', 0.107), ('gradient', 0.104), ('armijo', 0.101), ('pgd', 0.101), ('min', 0.097), ('symmetrized', 0.094), ('garbage', 0.094), ('iterations', 0.093), ('nist', 0.091), ('riesa', 0.088), ('eaj', 0.088), ('projection', 0.079), ('arg', 0.078), ('en', 0.078), ('fj', 0.077), ('vanilla', 0.075), ('zh', 0.075), ('fertility', 0.075), ('overfitting', 0.074), ('optimization', 0.074), ('ran', 0.071), ('isi', 0.071), ('ao', 0.068), ('uu', 0.067), ('barron', 0.067), ('bodrumlu', 0.067), ('fmin', 0.067), ('prokopov', 0.067), ('supplanted', 0.067), ('surpassed', 0.067), ('moore', 0.067), ('em', 0.065), ('extension', 0.065), ('tune', 0.065), ('vogel', 0.065), ('minimize', 0.065), ('prior', 0.064), ('ul', 0.062), ('logp', 0.062), ('hyperparameter', 0.06), ('urdu', 0.058), ('models', 0.055), ('taskar', 0.054), ('simplex', 0.053), ('uq', 0.053), ('nonlinear', 0.053), ('invention', 0.053), ('french', 0.053), ('dyer', 0.053), ('word', 0.052), ('satisfies', 0.051), ('chiang', 0.051), ('column', 0.051), ('modification', 0.05), ('circles', 0.05), ('pseudocode', 0.05), ('mermer', 0.05), ('unsupervised', 0.048), ('direction', 0.047), ('decades', 0.047), ('sara', 0.047), ('chineseenglish', 0.047), ('ashish', 0.047), ('encourage', 0.047), ('improvements', 0.047), ('toolkit', 0.046), ('objective', 0.046), ('liang', 0.046), ('exp', 0.046), ('parallel', 0.045), ('ond', 0.045), ('dempster', 0.045), ('fairly', 0.044), ('ben', 0.042), ('fraser', 0.041), ('ca', 0.041), ('modifications', 0.04), ('four', 0.04), ('constraints', 0.039), ('baseline', 0.039), ('gale', 0.038), ('brown', 0.038), ('koehn', 0.038), ('adding', 0.038), ('classes', 0.038), ('maximize', 0.038), ('della', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="179-tfidf-1" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>2 0.29007378 <a title="179-tfidf-2" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>Author: Darcey Riley ; Daniel Gildea</p><p>Abstract: Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score.</p><p>3 0.26864415 <a title="179-tfidf-3" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>4 0.21586876 <a title="179-tfidf-4" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>5 0.18036108 <a title="179-tfidf-5" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>6 0.17545265 <a title="179-tfidf-6" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>7 0.15766585 <a title="179-tfidf-7" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>8 0.14974907 <a title="179-tfidf-8" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>9 0.14902709 <a title="179-tfidf-9" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>10 0.14020024 <a title="179-tfidf-10" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>11 0.12809171 <a title="179-tfidf-11" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>12 0.12064199 <a title="179-tfidf-12" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>13 0.12060875 <a title="179-tfidf-13" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>14 0.11721986 <a title="179-tfidf-14" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>15 0.11706816 <a title="179-tfidf-15" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>16 0.11112321 <a title="179-tfidf-16" href="./acl-2012-A_Graph-based_Cross-lingual_Projection_Approach_for_Weakly_Supervised_Relation_Extraction.html">12 acl-2012-A Graph-based Cross-lingual Projection Approach for Weakly Supervised Relation Extraction</a></p>
<p>17 0.10811664 <a title="179-tfidf-17" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>18 0.1077686 <a title="179-tfidf-18" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>19 0.1072842 <a title="179-tfidf-19" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>20 0.10426952 <a title="179-tfidf-20" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.301), (1, -0.238), (2, 0.107), (3, 0.067), (4, 0.072), (5, 0.019), (6, 0.006), (7, -0.021), (8, -0.008), (9, -0.03), (10, -0.015), (11, -0.105), (12, -0.08), (13, -0.012), (14, 0.007), (15, -0.015), (16, -0.01), (17, 0.096), (18, 0.079), (19, 0.141), (20, -0.091), (21, -0.034), (22, 0.039), (23, 0.007), (24, 0.045), (25, -0.091), (26, -0.041), (27, -0.144), (28, 0.139), (29, -0.004), (30, -0.069), (31, -0.053), (32, 0.057), (33, -0.039), (34, -0.041), (35, 0.211), (36, -0.115), (37, -0.149), (38, -0.067), (39, 0.231), (40, -0.093), (41, 0.061), (42, 0.076), (43, 0.046), (44, -0.066), (45, 0.032), (46, 0.077), (47, -0.003), (48, -0.043), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95908314 <a title="179-lsi-1" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>2 0.94146252 <a title="179-lsi-2" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>Author: Darcey Riley ; Daniel Gildea</p><p>Abstract: Bayesian approaches have been shown to reduce the amount of overfitting that occurs when running the EM algorithm, by placing prior probabilities on the model parameters. We apply one such Bayesian technique, variational Bayes, to the IBM models of word alignment for statistical machine translation. We show that using variational Bayes improves the performance of the widely used GIZA++ software, as well as improving the overall performance of the Moses machine translation system in terms of BLEU score.</p><p>3 0.78816456 <a title="179-lsi-3" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>4 0.73908198 <a title="179-lsi-4" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>5 0.72961468 <a title="179-lsi-5" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>Author: Ning Xi ; Guangchao Tang ; Xinyu Dai ; Shujian Huang ; Jiajun Chen</p><p>Abstract: The dominant practice of statistical machine translation (SMT) uses the same Chinese word segmentation specification in both alignment and translation rule induction steps in building Chinese-English SMT system, which may suffer from a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses two different segmentation specifications for alignment and translation respectively: we use Chinese character as the basic unit for alignment, and then convert this alignment to conventional word alignment for translation rule induction. Experimentally, our approach outperformed two baselines: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance. 1Introduction Chinese Word segmentation is a necessary step in Chinese-English statistical machine translation (SMT) because Chinese sentences do not delimit words by spaces. The key characteristic of a Chinese word segmenter is the segmentation specification1. As depicted in Figure 1(a), the dominant practice of SMT uses the same word segmentation for both word alignment and translation rule induction. For brevity, we will refer to the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 We hereafter use “word segmentation” for short. 285 (a) WSA=WSR (b) WSA≠WSR Figure 1. WSA and WSR in SMT pipeline rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word alignment for translation rule induction. In the 2 Interestingly, word is also a basic token in syntax-based rules. Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A.s ?c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi2c 8s5–290, experiment, our approach consistently outperformed two baselines with three different word segmenters: fully word-based system (using word for both alignment and translation) and fully character-based system, in terms of alignment quality and translation performance. The remainder of this paper is structured as follows: Section 2 analyzes the influences of WSA and WSR on SMT respectively; Section 3 discusses how to convert character alignment to word alignment; Section 4 presents experimental results, followed by conclusions and future work in section 5. 2 Understanding WSA and WSR We propose a solution to tackle the suboptimal problem: using Chinese character for alignment while using Chinese word for translation. Character alignment differs from conventional word alignment in the basic tokens of the Chinese side of the training corpus3. Table 1 compares the token distributions of character-based corpus (CCorpus) and word-based corpus (WCorpus). We see that the WCorpus has a longer-tailed distribution than the CCorpus. More than 70% of the unique tokens appear less than 5 times in WCorpus. However, over half of the tokens appear more than or equal to 5 times in the CCorpus. This indicates that modeling word alignment could suffer more from data sparsity than modeling character alignment. Table 2 shows the numbers of the unique tokens (#UT) and unique bilingual token pairs (#UTP) of the two corpora. Consider two extensively features, fertility and translation features, which are extensively used by many state-of-the-art word aligners. The number of parameters w.r.t. fertility features grows linearly with #UT while the number of parameters w.r.t. translation features grows linearly with #UTP. We compare #UT and #UTP of both corpora in Table 2. As can be seen, CCorpus has less UT and UTP than WCorpus, i.e. character alignment model has a compact parameterization than word alignment model, where the compactness of parameterization is shown very important in statistical modeling (Collins, 1999). Another advantage of character alignment is the reduction in alignment errors caused by word seg3 Several works have proposed to use character (letter) on both sides of the parallel corpus for SMT between similar (European) languages (Vilar et al., 2007; Tiedemann, 2009), however, Chinese is not similar to English. 286 Frequency Characters (%) Words (%) 1 27.22 45.39 2 11.13 14.61 3 6.18 6.47 4 4.26 4.32 5(+) 50.21 29.21 Table 1 Token distribution of CCorpus and WCorpus Stats. Characters Words #UT 9.7K 88.1K #UTP 15.8M 24.2M Table 2 #UT and #UTP in CCorpus and WCorpus mentation errors. For example, “切尼 (Cheney)” and “愿 (will)” are wrongly merged into one word 切 尼 by the word segmenter, and 切 尼 wrongly aligns to a comma in English sentence in the word alignment; However, both 切 and 尼 align to “Cheney” correctly in the character alignment. However, this kind of errors cannot be fixed by methods which learn new words by packing already segmented words, such as word packing (Ma et al., 2007) and Pseudo-word (Duan et al., 2010). As character could preserve more meanings than word in Chinese, it seems that a character can be wrongly aligned to many English words by the aligner. However, we found this can be avoided to a great extent by the basic features (co-occurrence and distortion) used by many alignment models. For example, we observed that the four characters of the non-compositional word “阿拉法特 (Arafat)” align to Arafat correctly, although these characters preserve different meanings from that of Arafat. This can be attributed to the frequent co-occurrence (192 愿 愿 times) of these characters and Arafat in CCorpus. Moreover, 法 usually means France in Chinese, thus it may co-occur very often with France in CCorpus. If both France and Arafat appear in the English sentence, 法 may wrongly align to France. However, if 阿 aligns to Arafat, 法 will probably align to Arafat, because aligning 法 to Arafat could result in a lower distortion cost than aligning it to France. Different from alignment, translation is a pattern matching procedure (Lopez, 2008). WSR determines how the translation rules would be matched by the source sentences. For example, if we use translation rules with character as WSR to translate name entities such as the non-compositional word 阿拉法特, i.e. translating literally, we may get a wrong translation. That’s because the linguistic knowledge that the four characters convey a specific meaning different from the characters has been lost, which cannot always be totally recovered even by using phrase in phrase-based SMT systems (see Chang et al. (2008) for detail). Duan et al. (2010) and Paul et al., (2010) further pointed out that coarser-grained segmentation of the source sentence do help capture more contexts in translation. Therefore, rather than using character, using coarser-grained, at least as coarser as the conventional word, as WSR is quite necessary. 3 Converting Character Alignment to Word Alignment In order to use word as WSR, we employ the same method as Elming and Habash (2007)4 to convert the character alignment (CA) to its word-based version (CA ’) for translation rule induction. The conversion is very intuitive: for every English-Chinese word pair ??, ?? in the sentence pair, we align ? to ? as a link in CA ’, if and only if there is at least one Chinese character of ? aligns to ? in CA. Given two different segmentations A and B of the same sentence, it is easy to prove that if every word in A is finer-grained than the word of B at the corresponding position, the conversion is unambiguity (we omit the proof due to space limitation). As character is a finer-grained than its original word, character alignment can always be converted to alignment based on any word segmentation. Therefore, our approach can be naturally scaled to syntax-based system by converting character alignment to word alignment where the word seg- mentation is consistent with the parsers. We compare CA with the conventional word alignment (WA) as follows: We hand-align some sentence pairs as the evaluation set based on characters (ESChar), and converted it to the evaluation set based on word (ESWord) using the above conversion method. It is worth noting that comparing CA and WA by evaluating CA on ESChar and evaluating WA on ESWord is meaningless, because the basic tokens in CA and WA are different. However, based on the conversion method, comparing CA with WA can be accomplished by evaluating both CA ’ and WA on ESWord. 4 They used this conversion for word alignment combination only, no translation results were reported. 287 4 Experiments 4.1 Setup FBIS corpus (LDC2003E14) (210K sentence pairs) was used for small-scale task. A large bilingual corpus of our lab (1.9M sentence pairs) was used for large-scale task. The NIST’06 and NIST’08 test sets were used as the development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing WSA and WSR. We then evaluated the translation performance. The baselines are fully word-based MT systems (WordSys), i.e. using word as both WSA and WSR, and fully character-based systems (CharSys). Table 5 http://www.ictclas.org/ 6 http://nlp.stanford.edu/software/segmenter.shtml TLSablCIPeKT3BUAlig87 n609P5mW.0162eonrdt8avl52R01ai.g l6489numatieo78n29F t. 46590PrecC87 i1hP28s.oa3027rn(ctPe89)r6R05,.ar7162e3licganm8 (15F62eR.n983)t, TableSL4TWwrcahonSraAdslatioWw no SerdRvalu2Ct31iT.o405Bn1724ofW2P 301Ko.895rU61d Sy2sI03Ca.29nT035d4 proand F-score (F) with ? ? 0.5 (Fraser and Marcu, 2007) posed system using BLEU-SBP (Chiang et al., 2008) 4 compares WordSys to our proposed system. Significant testing was carried out using bootstrap re-sampling method proposed by Koehn (2004) with a 95% confidence level. We see that our proposed systems outperformed WordSys in all segmentation specifications settings. Table 5 lists the results of CharSys in small-scale task. In this setting, we gradually set the phrase length and the distortion limits of the phrase-based decoder (context size) to 7, 9, 11 and 13, in order to remove the disadvantage of shorter context size of using character as WSR for fair comparison with WordSys as suggested by Duan et al. (2010). Comparing Table 4 and 5, we see that all CharSys underperformed WordSys. This observation is consistent with Chang et al. (2008) which claimed that using characters, even with large phrase length (up to 13 in our experiment) cannot always capture everything a Chinese word segmenter can do, and using word for translation is quite necessary. We also see that CharSys underperformed our proposed systems, that’s because the harm of using character as WSR outweighed the benefit of using character as WSA, which indicated that word segmentation better for alignment is not necessarily better for translation, and vice versa. We finally compared our approaches to Ma et al. (2007) and Ma and Way (2009), which proposed “packed word (PW)” and “bilingual motivated word (BS)” respectively. Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in small- 288 Context Size 7 9 11 13 BLEU 20.90 21.19 20.89 21.09 Table 5 Translation evaluation of CharSys. CWPhrSoayps+TdoPtSaBebWmySdsle6wWPcCBhoWSa rAmdpawWrPBisoWS rRdnwiC2t1hT.2504oB6the2r1P0w9K.2o178U496rk s2I10C.9T547 scale task, we see that both PW and BS underperformed our approach. This may be attributed to the low recall of the learned BS or PW in their approaches. BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques (Dyer et al., 2008) to tackle the low recall of BS, which was removed from our experiments for fair comparison. Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point). This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation performance. 5 Conclusions and Future Work We proposed a SMT framework that uses character for alignment and word for translation, which improved both alignment quality and translation performance. We believe that in this framework, using other finer-grained segmentation, with fewer ambiguities than character, would better parameterize the alignment models, while using other coarser-grained segmentation as WSR can help capture more linguistic knowledge than word to get better translation. We also believe that our approach, if integrated with combination techniques (Dyer et al., 2008; Xi et al., 2011), can yield better results. Acknowledgments We thank ACL reviewers. This work is supported by the National Natural Science Foundation of China (No. 61003 112), the National Fundamental Research Program of China (2010CB327903). References Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Peitra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), pages 263-3 11. Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of third workshop on SMT, pages 224-232. David Chiang, Steve DeNeefe, Yee Seng Chan and Hwee Tou Ng. 2008. Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 610-619. Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 718-726. Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania. Xiangyu Duan, Min Zhang, and Haizhou Li. 2010. Pseudo-word for phrase-based machine translation. In Proceedings of the Association for Computational Linguistics, pages 148-156. Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of the Association for Computational Linguistics, pages 1012-1020. Jakob Elming and Nizar Habash. 2007. Combination of statistical word alignments based on multiple preprocessing schemes. In Proceedings of the Association for Computational Linguistics, pages 25-28. Alexander Fraser and Daniel Marcu. 2007. Squibs and Discussions: Measuring Word Alignment Quality for Statistical Machine Translation. In Computational Linguistics, 33(3), pages 293-303. Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of the Association for Computational Linguistics, pages 923-93 1. Phillip Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan,W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Association for Computational Linguistics, pages 177-1 80. 289 Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 388-395. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit. Adam David Lopez. 2008. Machine translation by pattern matching. Ph.D. thesis, University of Maryland. Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the Association for Computational Linguistics, pages 304-3 11. Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of the Conference of the European Chapter of the ACL, pages 549-557. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Association for Computational Linguistics, pages 440-447. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), pages 19-5 1. Michael Paul, Andrew Finch and Eiichiro Sumita. 2010. Integration of multiple bilingually-learned segmentation schemes into statistical machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 400-408. Jörg Tiedemann. 2009. Character-based PSMT for closely related languages. In Proceedings of the Annual Conference of the European Association for machine Translation, pages 12-19. David Vilar, Jan-T. Peter and Hermann Ney. 2007. Can we translate letters? In Proceedings of the Second Workshop on Statistical Machine Translation, pages 33-39. Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu and Shouxun Lin. 2010. Joint tokenization and translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1200-1208. Ning Xi, Guangchao Tang, Boyuan Li, and Yinggong Zhao. 2011. Word alignment combination over multiple word segmentation. In Proceedings of the ACL 2011 Student Session, pages 1-5. Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. of the Third Workshop on Statistical Machine Translation, pages 216-223. 290 In Proceedings</p><p>6 0.61071694 <a title="179-lsi-6" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>7 0.54655915 <a title="179-lsi-7" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>8 0.5449245 <a title="179-lsi-8" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>9 0.5278089 <a title="179-lsi-9" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>10 0.51170498 <a title="179-lsi-10" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>11 0.49195758 <a title="179-lsi-11" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>12 0.48927709 <a title="179-lsi-12" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>13 0.48537764 <a title="179-lsi-13" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>14 0.48148772 <a title="179-lsi-14" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>15 0.47777051 <a title="179-lsi-15" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>16 0.47661161 <a title="179-lsi-16" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>17 0.46979931 <a title="179-lsi-17" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>18 0.46042943 <a title="179-lsi-18" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>19 0.45775691 <a title="179-lsi-19" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>20 0.44928285 <a title="179-lsi-20" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(26, 0.033), (28, 0.061), (30, 0.019), (37, 0.027), (39, 0.04), (57, 0.012), (59, 0.014), (74, 0.026), (82, 0.019), (85, 0.026), (90, 0.102), (92, 0.036), (94, 0.48), (99, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96443963 <a title="179-lda-1" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>same-paper 2 0.83206969 <a title="179-lda-2" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>3 0.82358581 <a title="179-lda-3" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>Author: Katsumasa Yoshikawa ; Ryu Iida ; Tsutomu Hirao ; Manabu Okumura</p><p>Abstract: For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.</p><p>4 0.81121576 <a title="179-lda-4" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>Author: Apoorv Agarwal ; Adinoyi Omuya ; Aaron Harnly ; Owen Rambow</p><p>Abstract: Many researchers have attempted to predict the Enron corporate hierarchy from the data. This work, however, has been hampered by a lack of data. We present a new, large, and freely available gold-standard hierarchy. Using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems.</p><p>5 0.7917068 <a title="179-lda-5" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>Author: JinYeong Bak ; Suin Kim ; Alice Oh</p><p>Abstract: In social psychology, it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship. We present a computational framework for automatically analyzing such self-disclosure behavior in Twitter conversations. Our framework uses text mining techniques to discover topics, emotions, sentiments, lexical patterns, as well as personally identifiable information (PII) and personally embarrassing information (PEI). Our preliminary results illustrate that in relationships with high relationship strength, Twitter users show significantly more frequent behaviors of self-disclosure.</p><p>6 0.5271129 <a title="179-lda-6" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>7 0.44731107 <a title="179-lda-7" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>8 0.4211064 <a title="179-lda-8" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>9 0.41938278 <a title="179-lda-9" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>10 0.41459334 <a title="179-lda-10" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>11 0.41436592 <a title="179-lda-11" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>12 0.40416309 <a title="179-lda-12" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>13 0.40321285 <a title="179-lda-13" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>14 0.38547176 <a title="179-lda-14" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>15 0.38455263 <a title="179-lda-15" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>16 0.38349515 <a title="179-lda-16" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>17 0.38202706 <a title="179-lda-17" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>18 0.37591836 <a title="179-lda-18" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>19 0.37528646 <a title="179-lda-19" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>20 0.37447757 <a title="179-lda-20" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
