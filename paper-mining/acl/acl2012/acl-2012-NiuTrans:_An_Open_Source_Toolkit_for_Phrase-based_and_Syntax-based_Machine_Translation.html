<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-155" href="#">acl2012-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</h1>
<br/><p>Source: <a title="acl-2012-155-pdf" href="http://aclweb.org/anthology//P/P12/P12-3004.pdf">pdf</a></p><p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>Reference: <a title="acl-2012-155-reference" href="../acl2012_reference/acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ‡  Abstract We present a new open source toolkit for phrase-based and syntax-based machine translation. [sent-5, score-0.423]
</p><p>2 The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. [sent-6, score-0.608]
</p><p>3 The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. [sent-7, score-1.183]
</p><p>4 Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate  training for weight tuning. [sent-8, score-0.47]
</p><p>5 1  Introduction  We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. [sent-9, score-0.607]
</p><p>6 The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al. [sent-10, score-1.063]
</p><p>7 , 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al. [sent-11, score-0.117]
</p><p>8 In particular, 19 a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. [sent-14, score-0.501]
</p><p>9 Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. [sent-15, score-0.493]
</p><p>10 In addition, the toolkit provides easy-to-use APIs for  the development of new features. [sent-16, score-0.288]
</p><p>11 The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the NTCIR-9 Chinese-to-English PatentMT task (Goto et al. [sent-17, score-0.478]
</p><p>12 We implemented the toolkit in C++ language, with special consideration of extensibility and efficiency. [sent-19, score-0.338]
</p><p>13 C++ enables us to develop efficient translation engines which have high running speed for both training and decoding stages. [sent-20, score-0.629]
</p><p>14 This property is especially important when the programs are used for large scale translation. [sent-21, score-0.067]
</p><p>15 While the development of C++ program is slower than that of the similar programs written in other popular languages such as Java, the modern compliers generally result in C++ programs being consistently faster than the Java-based counterparts. [sent-22, score-0.332]
</p><p>16 The toolkit is available under the GNU general public license . [sent-23, score-0.288]
</p><p>17 1  2  Motivation  As in current approaches to statistical machine translation, NiuTrans is based on a log-linear  1 http://www. [sent-28, score-0.072]
</p><p>18 c s 2o0c1ia2ti Aosns fo cria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsteiscs 19–24, model where a number of features are defined to model the translation process. [sent-34, score-0.19]
</p><p>19 While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows:  2  ? [sent-42, score-0.288]
</p><p>20 Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). [sent-45, score-0.174]
</p><p>21 3  several useful decoding options, including: standard phrase-based decoding, decoding as parsing, decoding as tree-parsing, and forest-based decoding. [sent-48, score-1.065]
</p><p>22 In addition to the special attention to usability, the running speed of the system is also improved in several ways. [sent-52, score-0.084]
</p><p>23 For example, we used several pruning and multithreading techniques to speed-up the system. [sent-53, score-0.332]
</p><p>24 Toolkit  The toolkit serves as an end-to-end platform for training and evaluating statistical machine translation models. [sent-54, score-0.55]
</p><p>25 To build new translation systems, all you need is a collection of wordaligned sentences , and a set of additional sentences with one or more reference translations for weight tuning and test. [sent-55, score-0.365]
</p><p>26 Given a number of sentence-pairs and the word alignments between them, the toolkit first extracts a phrase table and two reordering models for the phrase-based system, or a Synchronous Context-free/Tree-substitution Grammar (SCFG/STSG) for the hierarchical phrase-based and syntax-based systems. [sent-63, score-0.716]
</p><p>27 Finally, the resulting models are incorporated into the decoder which can automatically tune feature weights on the development set using minimum error rate training (Och, 2003) and translate new sentences with the optimized weights. [sent-65, score-0.223]
</p><p>28 1 Phrase Extraction and Reordering Model We use a standard way to implement the phrase extraction module for the phrase-based model. [sent-68, score-0.081]
</p><p>29 They are two phrase translation probabilities, two lexical weights, and a feature of phrase penalty. [sent-71, score-0.278]
</p><p>30 Unlike previous systems that adopt only one reordering model, our toolkit supports two different reordering models which are trained independently but jointly used during decoding. [sent-74, score-0.848]
</p><p>31 The first of these is a discriminative reordering model. [sent-77, score-0.229]
</p><p>32 Thus the reordering problem is modeled as a classification problem, and the reordering probability can be efficiently computed using a (log-)linear combination of features. [sent-79, score-0.458]
</p><p>33 The second model is the MSD reordering model4 which has been successfully used in the Moses system. [sent-82, score-0.229]
</p><p>34 Unlike Moses, our toolkit supports both the word-based and  phrase-based  methods  for estimating  the  4 Term MSD refers to the three orientations (reordering types), including Monotone (M), Swap (S), and Discontinuous (D). [sent-83, score-0.446]
</p><p>35 2  Translation Rule Extraction  For the hierarchical phrase-based model, we follow the general framework of SCFG where a grammar rule has three parts – a source-side, a target-side and alignments between source and target nonterminals. [sent-86, score-0.212]
</p><p>36 To learn SCFG rules from word-aligned sentences, we choose the algorithm proposed in (Chiang, 2007) and estimate the associated feature values as in the phrase-based system. [sent-87, score-0.095]
</p><p>37 For the syntax-based models, all non-terminals in translation rules are annotated with syntactic labels. [sent-88, score-0.285]
</p><p>38 We use the GHKM algorithm to extract (minimal) translation rules from bilingual sentences with parse trees on source-language side and/or target-language side5 . [sent-89, score-0.441]
</p><p>39 Also, two or more minimal rules can be composed together to obtain larger rules and involve more contextual information. [sent-90, score-0.26]
</p><p>40 3  N-gram Language Modeling  The toolkit includes a simple but effective n-gram language model (LM). [sent-94, score-0.288]
</p><p>41 The LM builder is basically a “sorted” trie structure (Pauls and Klein, 201 1), where a map is developed to implement an array of key/value pairs, guaranteeing that the keys can be accessed in sorted order. [sent-95, score-0.179]
</p><p>42 Moreover, an n-gram cache is implemented to speed up n-gram probability requests for decoding. [sent-97, score-0.134]
</p><p>43 4  Weight Tuning  We implement the weight tuning component according to the minimum error rate training (MERT) method (Och, 2003). [sent-99, score-0.237]
</p><p>44 This procedure  5 For tree-to-tree models, we use a natural extension of the GHKM algorithm which defines admissible nodes on treepairs and obtains tree-to-tree rules on all pairs of source and target tree-fragments. [sent-102, score-0.152]
</p><p>45 In this way, our program can introduce some randomness into weight training. [sent-106, score-0.094]
</p><p>46 Given a source sentence, the decoder generates 1-best or k-best translations in a bottom-up fashion using a CKY-style parsing algorithm. [sent-110, score-0.298]
</p><p>47 The basic data structure used in the decoder is a chart, where an array of cells is organized in topological order. [sent-111, score-0.269]
</p><p>48 The decoding process starts with the minimal cells, and proceeds by repeatedly applying translation rules or composing items in adjunct cells to obtain new items. [sent-113, score-1.0]
</p><p>49 The decoder can work with all (hierarchical) phrase-based and syntax-based models. [sent-119, score-0.132]
</p><p>50 In particular, our toolkit provides the following decoding modes. [sent-120, score-0.643]
</p><p>51 To fit the phrasebased model into the CKY paring framework, we restrict the phrase-based decoding with the ITG constraint (Wu, 1996). [sent-124, score-0.395]
</p><p>52 In this way, each pair of items in adjunct cells can be composed in either monotone order or inverted order. [sent-125, score-0.254]
</p><p>53 Hence the decoding can be trivially implemented by a three-loop structure as in standard CKY parsing. [sent-126, score-0.405]
</p><p>54 This mode is designed for decoding with SCFGs/STSGs which are used in the hierarchical phrase-based and  syntax-based systems. [sent-129, score-0.472]
</p><p>55 In the general framework of synchronous grammars and tree transducers, decoding can be regarded as a parsing problem. [sent-130, score-0.43]
</p><p>56 Therefore, the above chart-based decoder is directly applicable to the hierarchical phrase-based and syntaxbased models. [sent-131, score-0.339]
</p><p>57 For efficient integration of ngram language model into decoding, rules containing more than two variables are binarized into binary rules. [sent-132, score-0.179]
</p><p>58 In addition to the rules learned from bilingual data, glue rules are employed to glue the translations of a sequence of chunks. [sent-133, score-0.395]
</p><p>59 If the parse tree of source sentence is provided, decoding (for tree-tostring and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). [sent-136, score-0.517]
</p><p>60 In tree-parsing, translation rules are first mapped onto the nodes of input parse tree. [sent-137, score-0.349]
</p><p>61 This results in a translation tree/forest (or a hypergraph) where each edge represents a rule application. [sent-138, score-0.19]
</p><p>62 Then decoding can proceed on the hypergraph as usual. [sent-139, score-0.43]
</p><p>63 That is, we visit in bottom-up order each node in the parse tree, and calculate  the model score for each edge rooting at the node. [sent-140, score-0.064]
</p><p>64 The final output is the 1-best/k-best translations maintained by the root node of the parse tree. [sent-141, score-0.13]
</p><p>65 Since tree-parsing restricts its search space to the derivations that exactly match with the input parse tree, it in general has a much higher decoding speed than a normal parsing procedure. [sent-142, score-0.503]
</p><p>66 But it in turn results in lower translation quality due to more search errors. [sent-143, score-0.19]
</p><p>67 In principle, forest is a data structure that can encode exponential number of trees efficiently. [sent-148, score-0.077]
</p><p>68 Since our internal representation is already in a hypergraph structure, it is easy to extend the decoder to handle the input forest, with little modification of the code. [sent-150, score-0.207]
</p><p>69 1 Multithreading The decoder supports multithreading to make full advantage of the modern computers where more than one CPUs (or cores) are provided. [sent-153, score-0.471]
</p><p>70 In general, the decoding speed can be improved when multiple threads are involved. [sent-154, score-0.511]
</p><p>71 However, modern MT decoders do not run faster when too many threads are used (Cer et al. [sent-155, score-0.156]
</p><p>72 2 Pruning To make decoding computational feasible, beam pruning is used to aggressively prune the search space. [sent-158, score-0.567]
</p><p>73 In our implementation, we maintain a beam for each cell. [sent-159, score-0.068]
</p><p>74 Once all the items of the cell are proved, only the top-k best items according to model score are kept and the rest are discarded. [sent-160, score-0.189]
</p><p>75 Also, we re-implemented the cube pruning method described in (Chiang, 2007) to further speed-up the system. [sent-161, score-0.194]
</p><p>76 The MT outputs are finally generated by composing the translations of those segments. [sent-165, score-0.15]
</p><p>77 3 APIs for Feature Engineering To ease the implementation and test of new features, the toolkit offers APIs for experimenting with the features developed by users. [sent-167, score-0.5]
</p><p>78 The parse  trees on both the Chinese and English sides were  and s2t represent the tree-to-string, tree-to-tree, and string-to-tree systems, respectively. [sent-177, score-0.105]
</p><p>79 generated using the Berkeley Parser, which were then binarized in a head-out fashion A 5-gram language model was trained on the Xinhua portion of the Gigaword corpus in addition to the English part of the LDC bilingual training data. [sent-178, score-0.178]
</p><p>80 The translation quality was evaluated with the case-insensitive IBM-version BLEU4. [sent-181, score-0.19]
</p><p>81 For the hierarchical phrase-based system, all SCFG rules have at most two variables. [sent-183, score-0.212]
</p><p>82 For the syntaxbased systems, minimal rules were extracted from the binarized trees on both (either) languageside(s). [sent-184, score-0.38]
</p><p>83 Larger rules were then generated by composing two or three minimal rules. [sent-185, score-0.249]
</p><p>84 By default, all these systems used a beam of size 30 for decoding. [sent-186, score-0.068]
</p><p>85 We see, first of all, that our phrase-based and hierarchical phrase-based systems achieve competitive performance, even outperforms the Moses system over 0. [sent-190, score-0.117]
</p><p>86 Also, the syntax-based systems obtain very 6 The  parse  trees follow the nested bracketing format,  as  defined in the Penn Treebank. [sent-192, score-0.145]
</p><p>87 For example, the string-to-tree system significantly outperforms the phrase-based and hierarchical phrase-based counterparts. [sent-199, score-0.117]
</p><p>88 In addition, Table 1 gives a test of different decoding methods (for syntax-based systems). [sent-200, score-0.355]
</p><p>89 For example, it is 5-8 times slower than the tree-parsing-based  method in our experiments. [sent-203, score-0.063]
</p><p>90 The forest-based decoding further improves the BLEU scores on top of tree-parsing. [sent-204, score-0.355]
</p><p>91 3  System Speed-up  We also study the effectiveness of pruning and multithreading techniques. [sent-208, score-0.332]
</p><p>92 Table 2 shows that all the pruning methods implemented in the toolkit is helpful in speeding up the (phrase-based) system, while does not result in significant decrease in BLEU score. [sent-209, score-0.482]
</p><p>93 On top of a straightforward baseline (only beam pruning is used), cube pruning and pruning with punctuations give a speed improvement of 25 times together7. [sent-210, score-0.634]
</p><p>94 Moreover, the decoding process can be further accelerated by using multithreading technique. [sent-211, score-0.543]
</p><p>95 However, more than 8 threads do not help in our experiments. [sent-212, score-0.072]
</p><p>96 6  Conclusion and Future Work  We have presented a new open-source toolkit for phrase-based and syntax-based machine translation. [sent-213, score-0.324]
</p><p>97 Moreover, it supports several state-of-the-art models ranging  from phrase-based models to syntax-based models, 7 The translation speed is tested on Intel Core Due 2 E8500 processors running at 3. [sent-215, score-0.376]
</p><p>98 The experimental results on NIST MT tasks show that the MT systems built with our toolkit achieve state-of-the-art translation performance. [sent-218, score-0.512]
</p><p>99 The next version of NiuTrans will support ARPA-format LMs, MIRA for weight tuning and a beam-stack decoder which removes the ITG constraint for phrase decoding. [sent-219, score-0.285]
</p><p>100 Scalable inferences and training of context-rich syntax translation models. [sent-249, score-0.19]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('decoding', 0.355), ('niutrans', 0.294), ('toolkit', 0.288), ('reordering', 0.229), ('translation', 0.19), ('multithreading', 0.188), ('pruning', 0.144), ('decoder', 0.132), ('hierarchical', 0.117), ('mert', 0.115), ('galley', 0.114), ('supports', 0.102), ('mt', 0.102), ('rules', 0.095), ('moses', 0.091), ('syntaxbased', 0.09), ('cells', 0.089), ('speed', 0.084), ('bleu', 0.084), ('binarized', 0.084), ('composing', 0.084), ('hypergraph', 0.075), ('msd', 0.075), ('cer', 0.072), ('threads', 0.072), ('scfg', 0.072), ('minimal', 0.07), ('apis', 0.069), ('beam', 0.068), ('programs', 0.067), ('translations', 0.066), ('tuning', 0.066), ('jane', 0.065), ('ghkm', 0.065), ('michel', 0.065), ('items', 0.064), ('parse', 0.064), ('slower', 0.063), ('cell', 0.061), ('utilities', 0.06), ('cdec', 0.06), ('dyer', 0.059), ('chris', 0.059), ('nist', 0.058), ('item', 0.057), ('source', 0.057), ('developed', 0.056), ('ease', 0.056), ('goto', 0.056), ('orientations', 0.056), ('koehn', 0.054), ('offers', 0.053), ('vilar', 0.053), ('adjunct', 0.053), ('zollmann', 0.053), ('weese', 0.053), ('pages', 0.053), ('qun', 0.052), ('program', 0.051), ('bilingual', 0.051), ('cky', 0.05), ('itg', 0.05), ('cube', 0.05), ('implemented', 0.05), ('modern', 0.049), ('minimum', 0.049), ('monotone', 0.048), ('pauls', 0.048), ('array', 0.048), ('implementation', 0.047), ('xiong', 0.044), ('glue', 0.044), ('phrase', 0.044), ('daniel', 0.043), ('weight', 0.043), ('decode', 0.043), ('fashion', 0.043), ('joshua', 0.043), ('jonathan', 0.043), ('moreover', 0.043), ('rate', 0.042), ('open', 0.042), ('trees', 0.041), ('tree', 0.041), ('phrasebased', 0.04), ('bracketing', 0.04), ('shouxun', 0.04), ('chart', 0.04), ('sorted', 0.038), ('alignments', 0.038), ('implement', 0.037), ('smt', 0.037), ('forest', 0.036), ('machine', 0.036), ('statistical', 0.036), ('alexandra', 0.036), ('education', 0.035), ('faster', 0.035), ('built', 0.034), ('synchronous', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="155-tfidf-1" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>2 0.24085668 <a title="155-tfidf-2" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>3 0.23688602 <a title="155-tfidf-3" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>4 0.2307497 <a title="155-tfidf-4" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; George Foster ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</p><p>5 0.22007738 <a title="155-tfidf-5" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>6 0.21295215 <a title="155-tfidf-6" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>7 0.20877191 <a title="155-tfidf-7" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>8 0.20695721 <a title="155-tfidf-8" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>9 0.20660779 <a title="155-tfidf-9" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>10 0.20531382 <a title="155-tfidf-10" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>11 0.20068274 <a title="155-tfidf-11" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>12 0.19792259 <a title="155-tfidf-12" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>13 0.1892347 <a title="155-tfidf-13" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>14 0.17170602 <a title="155-tfidf-14" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>15 0.16813083 <a title="155-tfidf-15" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>16 0.16247655 <a title="155-tfidf-16" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>17 0.15664998 <a title="155-tfidf-17" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>18 0.15482 <a title="155-tfidf-18" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>19 0.14974907 <a title="155-tfidf-19" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>20 0.14258136 <a title="155-tfidf-20" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.353), (1, -0.35), (2, 0.093), (3, 0.012), (4, 0.047), (5, -0.147), (6, -0.009), (7, 0.043), (8, 0.072), (9, 0.013), (10, -0.003), (11, -0.022), (12, -0.021), (13, -0.06), (14, -0.003), (15, -0.006), (16, -0.04), (17, 0.099), (18, 0.066), (19, -0.217), (20, 0.102), (21, -0.007), (22, -0.11), (23, 0.072), (24, -0.038), (25, 0.047), (26, -0.008), (27, 0.035), (28, 0.005), (29, -0.034), (30, 0.041), (31, -0.063), (32, 0.022), (33, 0.035), (34, -0.065), (35, -0.055), (36, 0.021), (37, 0.127), (38, -0.017), (39, -0.036), (40, -0.041), (41, 0.045), (42, -0.001), (43, -0.049), (44, 0.025), (45, 0.056), (46, 0.064), (47, -0.047), (48, -0.043), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96398336 <a title="155-lsi-1" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>2 0.85853457 <a title="155-lsi-2" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>Author: Junhui Li ; Zhaopeng Tu ; Guodong Zhou ; Josef van Genabith</p><p>Abstract: This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1</p><p>3 0.78981328 <a title="155-lsi-3" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Mu Li ; Ming Zhou</p><p>Abstract: In this paper, we address the issue for learning better translation consensus in machine translation (MT) research, and explore the search of translation consensus from similar, rather than the same, source sentences or their spans. Unlike previous work on this topic, we formulate the problem as structured labeling over a much smaller graph, and we propose a novel structured label propagation for the task. We convert such graph-based translation consensus from similar source strings into useful features both for n-best output reranking and for decoding algorithm. Experimental results show that, our method can significantly improve machine translation performance on both IWSLT and NIST data, compared with a state-ofthe-art baseline. 1</p><p>4 0.75610673 <a title="155-lsi-4" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>5 0.73611528 <a title="155-lsi-5" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>6 0.72358751 <a title="155-lsi-6" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>7 0.70503855 <a title="155-lsi-7" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>8 0.6998806 <a title="155-lsi-8" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>9 0.69036239 <a title="155-lsi-9" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>10 0.68758571 <a title="155-lsi-10" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>11 0.68239897 <a title="155-lsi-11" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>12 0.67614245 <a title="155-lsi-12" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>13 0.65933871 <a title="155-lsi-13" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>14 0.64024609 <a title="155-lsi-14" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>15 0.6274032 <a title="155-lsi-15" href="./acl-2012-Heuristic_Cube_Pruning_in_Linear_Time.html">107 acl-2012-Heuristic Cube Pruning in Linear Time</a></p>
<p>16 0.61004049 <a title="155-lsi-16" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>17 0.59776562 <a title="155-lsi-17" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>18 0.59634757 <a title="155-lsi-18" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>19 0.57681954 <a title="155-lsi-19" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>20 0.57475561 <a title="155-lsi-20" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.014), (26, 0.036), (28, 0.067), (30, 0.037), (37, 0.017), (39, 0.052), (57, 0.297), (59, 0.01), (74, 0.047), (82, 0.033), (84, 0.018), (85, 0.057), (90, 0.129), (92, 0.042), (94, 0.046), (99, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8738935 <a title="155-lda-1" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>Author: Kareem Darwish ; Ahmed Ali</p><p>Abstract: Due to Arabic’s morphological complexity, Arabic retrieval benefits greatly from morphological analysis – particularly stemming. However, the best known stemming does not handle linguistic phenomena such as broken plurals and malformed stems. In this paper we propose a model of character-level morphological transformation that is trained using Wikipedia hypertext to page title links. The use of our model yields statistically significant improvements in Arabic retrieval over the use of the best statistical stemming technique. The technique can potentially be applied to other languages.</p><p>same-paper 2 0.79688984 <a title="155-lda-2" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>3 0.77970397 <a title="155-lda-3" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>Author: William Yang Wang ; Elijah Mayfield ; Suresh Naidu ; Jeremiah Dittmar</p><p>Abstract: We propose a latent variable model to enhance historical analysis of large corpora. This work extends prior work in topic modelling by incorporating metadata, and the interactions between the components in metadata, in a general way. To test this, we collect a corpus of slavery-related United States property law judgements sampled from the years 1730 to 1866. We study the language use in these legal cases, with a special focus on shifts in opinions on controversial topics across different regions. Because this is a longitudinal data set, we are also interested in understanding how these opinions change over the course of decades. We show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identification tasks. Experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting, and that these improvements are robust across different parameter settings.</p><p>4 0.7463178 <a title="155-lda-4" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>5 0.64826155 <a title="155-lda-5" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>6 0.61051548 <a title="155-lda-6" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<p>7 0.60970962 <a title="155-lda-7" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>8 0.58416605 <a title="155-lda-8" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>9 0.57867151 <a title="155-lda-9" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>10 0.55520535 <a title="155-lda-10" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>11 0.55502528 <a title="155-lda-11" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>12 0.55347884 <a title="155-lda-12" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>13 0.5521943 <a title="155-lda-13" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>14 0.54834306 <a title="155-lda-14" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>15 0.5464952 <a title="155-lda-15" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>16 0.5450545 <a title="155-lda-16" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>17 0.54470009 <a title="155-lda-17" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>18 0.54352069 <a title="155-lda-18" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>19 0.54041797 <a title="155-lda-19" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>20 0.53458214 <a title="155-lda-20" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
