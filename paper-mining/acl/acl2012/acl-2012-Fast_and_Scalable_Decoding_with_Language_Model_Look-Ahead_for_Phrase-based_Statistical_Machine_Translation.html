<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-97" href="#">acl2012-97</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2012-97-pdf" href="http://aclweb.org/anthology//P/P12/P12-2006.pdf">pdf</a></p><p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>Reference: <a title="acl-2012-97-reference" href="../acl2012_reference/acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. [sent-3, score-0.685]
</p><p>2 Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. [sent-4, score-0.284]
</p><p>3 Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. [sent-5, score-0.651]
</p><p>4 We compare our ap-  proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. [sent-6, score-0.163]
</p><p>5 At a speed of roughly 70 words per second, Moses reaches 17. [sent-7, score-0.248]
</p><p>6 1 Introduction Research efforts to increase search efficiency for phrase-based MT (Koehn et al. [sent-10, score-0.138]
</p><p>7 , 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al. [sent-11, score-0.106]
</p><p>8 , 2006) to additional early pruning techniques (Delaney et al. [sent-12, score-0.185]
</p><p>9 This work extends the approach by (Zens and Ney, 2008) with two techniques to increase translation speed and scalability. [sent-14, score-0.325]
</p><p>10 We show that taking a heuristic LM score estimate for pre-sorting the 28 Richard Zens* Google, Inc. [sent-15, score-0.103]
</p><p>11 com phrase translation candidates has a positive effect on both translation quality and speed. [sent-17, score-0.451]
</p><p>12 The idea of LM look-ahead is to incorporate the LM probabilities into the pruning process of the beam search as early as possible. [sent-19, score-0.388]
</p><p>13 First-word LM look-ahead exploits the search structure to use the LM costs of the first word of a new phrase as a lower bound for the full LM costs of the phrase. [sent-23, score-0.311]
</p><p>14 Phrase-only LM look-ahead makes use of a pre-computed estimate of the full LM costs for each phrase. [sent-24, score-0.103]
</p><p>15 We detail the implementation of these methods and analyze their effect with respect to the number of LM computations and hypothesis expansions as well as on translation speed and quality. [sent-25, score-0.607]
</p><p>16 We also run comparisons with the Moses decoder (Koehn et al. [sent-26, score-0.081]
</p><p>17 , 2007), which yields the same performance in BLEU, but is outperformed significantly in terms of scalability for faster translation. [sent-27, score-0.124]
</p><p>18 Our implementation is available under  a non-commercial open source licence†. [sent-28, score-0.033]
</p><p>19 2  Search Algorithm Extensions  We apply the decoding algorithm described in (Zens and Ney, 2008). [sent-29, score-0.072]
</p><p>20 A beam search strategy is used to find the best hypothesis. [sent-31, score-0.203]
</p><p>21 During search we perform pruning controlled by the parameters coverage histogram size‡ Nc and lexical ∗Richard Zens’s contribution was during his time at RWTH. [sent-32, score-0.399]
</p><p>22 de/ jane ‡number of hypothesized coverage vectors per cardinality Proce Jedijung, sR oefpu thbeli c50 othf K Aonrneua,a8l -M14e Jtiunlgy o 2f0 t1h2e. [sent-36, score-0.093]
</p><p>23 c s 2o0c1ia2ti Aosns fo cria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsteiscs 28–32, histogram size§ Nl. [sent-38, score-0.128]
</p><p>24 1 Phrase candidate pre-sorting In addition to the source sentence f1J, the beam search algorithm takes a matrix E(·, ·) as input, sweharecrhe faolrg reiatchmh contiguous phrase ·f,˜· = fj . [sent-40, score-0.336]
</p><p>25 fj0 within the source sentence, E(j, j0) contains a list of all candidate translations for f˜. [sent-43, score-0.033]
</p><p>26 The candidate lists  are sorted according to their model score, which was observed to speed up translation by Delaney et al. [sent-44, score-0.301]
</p><p>27 In addition to sorting according to the purely phrase-internal scores, which is common practice, we compute an estimate qLME( e˜) for the LM score of each target phrase e˜ . [sent-46, score-0.228]
</p><p>28 qLME( e˜) is the weighted LM score we receive by assuming e˜ to be a complete sentence without using sentence start and end markers. [sent-47, score-0.072]
</p><p>29 We limit the number of translation options per source phrase to the No top scoring candidates (observation histogram pruning). [sent-48, score-0.522]
</p><p>30 The pre-sorting during phrase matching has two effects on the search algorithm. [sent-49, score-0.162]
</p><p>31 Firstly, it defines the order in which the hypothesis expansions take place. [sent-50, score-0.216]
</p><p>32 As higher scoring phrases are considered first, it is less likely that already created partial hypotheses will have to be replaced, thus effectively reducing the expected number of hypothesis expansions. [sent-51, score-0.091]
</p><p>33 Secondly, due to the observation pruning the sorting affects the considered phrase candidates and consequently the search space. [sent-52, score-0.491]
</p><p>34 A better pre-selection can be expected to improve translation quality. [sent-53, score-0.163]
</p><p>35 2  Language Model Look-Ahead  LM score computations are among the most expensive in decoding. [sent-55, score-0.168]
</p><p>36 (2006) report significant improvements in runtime by removing unnecessary LM lookups via early pruning. [sent-57, score-0.137]
</p><p>37 The innermost loop of the search algorithm iterates over all translation options for a single source phrase to consider them for expanding the current hypothesis. [sent-59, score-0.516]
</p><p>38 We introduce an LM look-ahead score qLMLA( e˜| e˜0), which is computed for each of the transla(t ieo˜| ne˜ options. [sent-60, score-0.072]
</p><p>39 This score is added to the overall hypothesis score, and if the pruning threshold is §number of lexical hypotheses  per coverage  vector  29 exceeded, we discard the expansion without computing the full LM score. [sent-61, score-0.417]
</p><p>40 First-word LM look-ahead pruning defines the LM look-ahead score qLMLA( e˜| e˜0) = qLM( e˜1 | e˜0) to be the LM score of the first w(o e˜r| de˜ of target phrase e˜ given history e˜ 0. [sent-62, score-0.475]
</p><p>41 As qLM( e˜1 | e˜0) is an upper bound for the full LM score, the technique does not introduce  additional seach errors. [sent-63, score-0.059]
</p><p>42 The score can be reused, if the LM score of the full phrase e˜ needs to be computed afterwards. [sent-64, score-0.247]
</p><p>43 We can exploit the structure of the search to speed up the LM lookups for the first word. [sent-65, score-0.309]
</p><p>44 Usually, each LM lookup consists of first traversing the trie to find the node corresponding to the current LM history and then retrieving the probability for the next word. [sent-67, score-0.191]
</p><p>45 However, the LM history for the first words of all phrases within the innermost loop of the search algorithm is identical. [sent-69, score-0.279]
</p><p>46 Just before the loop we can therefore traverse the trie once for the current history and each of its lower order ngrams and store the pointers to the resulting nodes. [sent-70, score-0.287]
</p><p>47 To retrieve the LM look-ahead scores, we can then directly access the nodes without the need to traverse the trie again. [sent-71, score-0.168]
</p><p>48 This implementational detail was confirmed to increase translation speed by roughly 20% in a short experiment. [sent-72, score-0.353]
</p><p>49 Phrase-only LM look-ahead pruning defines the  look-ahead score qLMLA( e˜| e˜0) = qLME( e˜) to be the LM score of phrase e˜ , assuming e˜ to be the full sentence. [sent-73, score-0.436]
</p><p>50 It was already used for sorting the phrases, is therefore pre-computed and does not require additional LM lookups. [sent-74, score-0.049]
</p><p>51 As it is not a lower bound for the real LM score, this pruning technique can introduce additional search errors. [sent-75, score-0.277]
</p><p>52 org/wmt 11  Table 1: Comparison of the number of hypothesis expansions per source word (#HYP) and LM computations per source word (#LM) with respect to LM pre-sorting, firstword LM look-ahead and phrase-only LM look-ahead on newste st 2 0 0 9. [sent-81, score-0.567]
</p><p>53 Results are given with (No = 100) and without (No = ∞) observation pruning. [sent-83, score-0.044]
</p><p>54 The English language model is a 4-gram LM created with the SRILM toolkit (Stolcke, 2002) on all bilingual and parts of the provided monolingual data. [sent-84, score-0.024]
</p><p>55 newste st 2 0 0 8 is used for parameter optimization, newste st 2 0 0 9 as a blind test set. [sent-85, score-0.222]
</p><p>56 We use identical phrase tables and scaling factors for Moses and our decoder. [sent-90, score-0.101]
</p><p>57 The phrase table is pruned to a maximum of 400 target candidates per source phrase before decoding. [sent-91, score-0.276]
</p><p>58 The phrase table and LM are loaded into memory before translating and loading time is eliminated for speed measurements. [sent-92, score-0.239]
</p><p>59 2  Methodological analysis  To observe the effect of the proposed search algorithm extensions, we ran experiments with fixed pruning parameters, keeping track of the number of hypothesis expansions and LM computations. [sent-94, score-0.478]
</p><p>60 The LM score pre-sorting affects both the set of phrase candidates due to observation histogram pruning and the order in which they are considered. [sent-95, score-0.556]
</p><p>61 To separate these effects, experiments were run both with histogram pruning (No = 100) and without. [sent-96, score-0.287]
</p><p>62 From Table 1 we can see that in terms of efficiency both cases show similar improvements over the baseline, †http : / / iws lt 2 0 11. [sent-97, score-0.054]
</p><p>63 org  30  words/sec Figure 1: Translation performance in BLEU [%] on the newstest 2 0 0 9 set vs. [sent-98, score-0.057]
</p><p>64 We compare Moses with our approach without LM lookahead and LM score pre-sorting (baseline), with added LM pre-sorting and with either first-word or phrase-only LM look-ahead on top of +pre-sort. [sent-100, score-0.072]
</p><p>65 Observation histogram size is fixed to No = 100 for both decoders. [sent-101, score-0.151]
</p><p>66 which performs pre-sorting with respect to the trans-  lation model scores only. [sent-102, score-0.024]
</p><p>67 The number of hypothesis expansions is reduced by ∼20% and the number of eLxMpa lookups by d∼uc50e%d. [sent-103, score-0.271]
</p><p>68 b yW ∼h2e0n% %ob asnedrv tahteio nnu pruning iLsM applied, we additionally ohebnser ovbes a vsmatiaolnl i pnrcurenainseg by 0. [sent-104, score-0.159]
</p><p>69 Application of first-word LM look-ahead further reduces the number of LM lookups by 23%, resulting in doubled translation speed, part of which derives from fewer trie node searches. [sent-106, score-0.398]
</p><p>70 The heuristic phrase-only LM look-ahead method introduces additional search errors, resulting in a BLEU drop by 0. [sent-107, score-0.086]
</p><p>71 3%, but yields another 85% reduction in LM computations and increases throughput by a factor of2. [sent-108, score-0.217]
</p><p>72 3  Performance evaluation  In this section we evaluate the proposed extensions to the original beam search algorithm in terms of scalability and their usefulness for different application constraints. [sent-111, score-0.358]
</p><p>73 We compare Moses and four different setups of our decoder: LM score pre-sorting switched on or off without LM look-ahead and both LM look-ahead methods with LM score pre-sorting. [sent-112, score-0.144]
</p><p>74 We translated the test set with the beam sizes set to  Nc = Nl = {1,2, 4, 8, 16, 24, 32, 48, 64}. [sent-113, score-0.117]
</p><p>75 923 fastest Mhios we sork: pfihr sat-sweo-ordnly( 1 1, 1 ) 214. [sent-131, score-0.17]
</p><p>76 We consider both the best and the fastest possible translation, as well as the fastest settings resulting in no more than 1% and 2% BLEU loss on the development set. [sent-139, score-0.385]
</p><p>77 tion performance in BLEU is plotted against speed in Figure 1. [sent-141, score-0.138]
</p><p>78 Without the proposed extensions, Moses slightly outperforms our decoder in terms of BLEU. [sent-142, score-0.081]
</p><p>79 With LM score pre-sorting, the best BLEU value is similar to Moses while further accelerating translation, yielding identical performance at 16 words/sec as Moses at 1. [sent-144, score-0.122]
</p><p>80 Application of first-word LM look-ahead shifts the graph to the right, now reaching the same performance at 31 words/sec. [sent-146, score-0.056]
</p><p>81 At a fixed translation speed of roughly 70 words/sec, our approach yields 20. [sent-147, score-0.396]
</p><p>82 For phrase-only LM look-ahead the graph is somewhat flatter. [sent-150, score-0.024]
</p><p>83 It yields nearly the same top performance with an even better trade-off between translation quality and speed. [sent-151, score-0.207]
</p><p>84 However, if we allow for a small degradation in translation performance, our approaches clearly outperform Moses 31 in terms of translation speed. [sent-155, score-0.326]
</p><p>85 With phrase-only LM look-ahead, our decoder is faster by a factor of 6 for no more than 1% BLEU loss, a factor of 11 for 2% BLEU loss and a factor of 22 in the fastest setting. [sent-156, score-0.562]
</p><p>86 Here, the speed difference reaches a factor of 19 in the fastest setting. [sent-158, score-0.425]
</p><p>87 4  Conclusions  This work introduces two extensions to the wellknown beam search algorithm for phrase-based machine translation. [sent-159, score-0.336]
</p><p>88 Both pre-sorting the phrase translation candidates with an LM score estimate and LM look-ahead during search are shown to have a positive effect on translation speed. [sent-160, score-0.64]
</p><p>89 We compare our decoder to Moses, reaching a similar highest BLEU score, but clearly outperforming it in terms of scalability with respect to the trade-off ratio between translation quality and speed. [sent-161, score-0.345]
</p><p>90 In our experiments, the fastest settings of our decoder and Moses differ in translation speed by a factor of 22 on the WMT data and a factor of 19 on the IWSLT data. [sent-162, score-0.706]
</p><p>91 Our software is part of the open source toolkit Jane. [sent-163, score-0.057]
</p><p>92 An efficient graph search decoder for phrase-based statistical machine translation. [sent-168, score-0.191]
</p><p>93 Faster beam-search decoding for phrasal  statistical machine translation. [sent-191, score-0.048]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lm', 0.699), ('moses', 0.196), ('fastest', 0.17), ('translation', 0.163), ('pruning', 0.159), ('delaney', 0.143), ('speed', 0.138), ('histogram', 0.128), ('expansions', 0.125), ('trie', 0.125), ('bleu', 0.119), ('beam', 0.117), ('computations', 0.096), ('zens', 0.089), ('search', 0.086), ('newste', 0.086), ('ortiz', 0.086), ('qlme', 0.086), ('qlmla', 0.086), ('steinbiss', 0.086), ('extensions', 0.086), ('lookups', 0.085), ('decoder', 0.081), ('factor', 0.077), ('phrase', 0.076), ('ortmanns', 0.075), ('score', 0.072), ('history', 0.066), ('wmt', 0.062), ('hypothesis', 0.061), ('mhios', 0.057), ('newstest', 0.057), ('qlm', 0.057), ('rndly', 0.057), ('tmhios', 0.057), ('spoken', 0.053), ('loop', 0.053), ('iwslt', 0.051), ('snover', 0.051), ('innermost', 0.05), ('sorting', 0.049), ('candidates', 0.049), ('koehn', 0.049), ('decoding', 0.048), ('loss', 0.045), ('scalability', 0.045), ('costs', 0.045), ('yields', 0.044), ('observation', 0.044), ('moore', 0.043), ('traverse', 0.043), ('per', 0.042), ('reaches', 0.04), ('ney', 0.038), ('papineni', 0.035), ('faster', 0.035), ('richard', 0.035), ('stack', 0.034), ('source', 0.033), ('wade', 0.032), ('reaching', 0.032), ('bound', 0.032), ('options', 0.031), ('estimate', 0.031), ('hermann', 0.03), ('hypotheses', 0.03), ('defines', 0.03), ('srilm', 0.029), ('efficiency', 0.028), ('affects', 0.028), ('roughly', 0.028), ('full', 0.027), ('alexandra', 0.027), ('chris', 0.027), ('improvements', 0.026), ('coverage', 0.026), ('early', 0.026), ('identical', 0.025), ('licence', 0.025), ('mountain', 0.025), ('reused', 0.025), ('oseo', 0.025), ('quaero', 0.025), ('isca', 0.025), ('tran', 0.025), ('accelerating', 0.025), ('doubled', 0.025), ('eng', 0.025), ('hyp', 0.025), ('jane', 0.025), ('loading', 0.025), ('st', 0.025), ('respect', 0.024), ('nc', 0.024), ('increase', 0.024), ('algorithm', 0.024), ('toolkit', 0.024), ('graph', 0.024), ('fixed', 0.023), ('wellknown', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="97-tfidf-1" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>2 0.22437969 <a title="97-tfidf-2" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>Author: Malte Nuhn ; Arne Mauser ; Hermann Ney</p><p>Abstract: In this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages. We present a modification of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words. On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with an n-gram language model. The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the VERBMOBIL corpus. We also report results using data from the monolingual French and English GIGAWORD corpora.</p><p>3 0.20783882 <a title="97-tfidf-3" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>4 0.20660779 <a title="97-tfidf-4" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>5 0.14898519 <a title="97-tfidf-5" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>6 0.13567248 <a title="97-tfidf-6" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>7 0.12880106 <a title="97-tfidf-7" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>8 0.11704715 <a title="97-tfidf-8" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>9 0.11375494 <a title="97-tfidf-9" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>10 0.10534266 <a title="97-tfidf-10" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>11 0.098665543 <a title="97-tfidf-11" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>12 0.097840354 <a title="97-tfidf-12" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>13 0.097569637 <a title="97-tfidf-13" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>14 0.096577585 <a title="97-tfidf-14" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>15 0.093504235 <a title="97-tfidf-15" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>16 0.092903234 <a title="97-tfidf-16" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>17 0.08757443 <a title="97-tfidf-17" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>18 0.085619405 <a title="97-tfidf-18" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>19 0.080818474 <a title="97-tfidf-19" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>20 0.080652826 <a title="97-tfidf-20" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.214), (1, -0.218), (2, 0.077), (3, 0.042), (4, 0.057), (5, -0.034), (6, 0.025), (7, -0.015), (8, 0.026), (9, -0.024), (10, -0.032), (11, 0.023), (12, -0.013), (13, -0.04), (14, 0.038), (15, -0.036), (16, -0.018), (17, 0.168), (18, 0.049), (19, -0.097), (20, 0.088), (21, -0.143), (22, 0.041), (23, -0.013), (24, 0.112), (25, -0.062), (26, 0.144), (27, 0.133), (28, -0.109), (29, 0.023), (30, 0.068), (31, -0.122), (32, -0.048), (33, 0.059), (34, 0.136), (35, -0.054), (36, -0.017), (37, 0.153), (38, -0.001), (39, 0.015), (40, -0.098), (41, -0.078), (42, -0.04), (43, 0.126), (44, 0.049), (45, 0.085), (46, 0.101), (47, 0.072), (48, 0.042), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96444964 <a title="97-lsi-1" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>2 0.6950047 <a title="97-lsi-2" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>Author: Malte Nuhn ; Arne Mauser ; Hermann Ney</p><p>Abstract: In this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages. We present a modification of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words. On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with an n-gram language model. The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the VERBMOBIL corpus. We also report results using data from the monolingual French and English GIGAWORD corpora.</p><p>3 0.65244377 <a title="97-lsi-3" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>4 0.57792222 <a title="97-lsi-4" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>5 0.55447704 <a title="97-lsi-5" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Mu Li ; Ming Zhou</p><p>Abstract: In this paper, we address the issue for learning better translation consensus in machine translation (MT) research, and explore the search of translation consensus from similar, rather than the same, source sentences or their spans. Unlike previous work on this topic, we formulate the problem as structured labeling over a much smaller graph, and we propose a novel structured label propagation for the task. We convert such graph-based translation consensus from similar source strings into useful features both for n-best output reranking and for decoding algorithm. Experimental results show that, our method can significantly improve machine translation performance on both IWSLT and NIST data, compared with a state-ofthe-art baseline. 1</p><p>6 0.55426544 <a title="97-lsi-6" href="./acl-2012-Heuristic_Cube_Pruning_in_Linear_Time.html">107 acl-2012-Heuristic Cube Pruning in Linear Time</a></p>
<p>7 0.52021664 <a title="97-lsi-7" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>8 0.51988697 <a title="97-lsi-8" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>9 0.49660119 <a title="97-lsi-9" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>10 0.49604854 <a title="97-lsi-10" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>11 0.49425519 <a title="97-lsi-11" href="./acl-2012-Decoding_Running_Key_Ciphers.html">68 acl-2012-Decoding Running Key Ciphers</a></p>
<p>12 0.47664812 <a title="97-lsi-12" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>13 0.45866635 <a title="97-lsi-13" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>14 0.45345429 <a title="97-lsi-14" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>15 0.45230755 <a title="97-lsi-15" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>16 0.45100954 <a title="97-lsi-16" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>17 0.42067698 <a title="97-lsi-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.40452805 <a title="97-lsi-18" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>19 0.40381128 <a title="97-lsi-19" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>20 0.40100521 <a title="97-lsi-20" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.227), (25, 0.022), (26, 0.036), (28, 0.045), (30, 0.016), (37, 0.029), (39, 0.035), (49, 0.018), (57, 0.071), (59, 0.028), (74, 0.095), (84, 0.023), (85, 0.046), (90, 0.113), (92, 0.047), (94, 0.044), (99, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75905687 <a title="97-lda-1" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>2 0.59280592 <a title="97-lda-2" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>3 0.58250344 <a title="97-lda-3" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>4 0.576388 <a title="97-lda-4" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>5 0.56995267 <a title="97-lda-5" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>Author: William Yang Wang ; Elijah Mayfield ; Suresh Naidu ; Jeremiah Dittmar</p><p>Abstract: We propose a latent variable model to enhance historical analysis of large corpora. This work extends prior work in topic modelling by incorporating metadata, and the interactions between the components in metadata, in a general way. To test this, we collect a corpus of slavery-related United States property law judgements sampled from the years 1730 to 1866. We study the language use in these legal cases, with a special focus on shifts in opinions on controversial topics across different regions. Because this is a longitudinal data set, we are also interested in understanding how these opinions change over the course of decades. We show that the joint learning scheme of our sparse mixed-effects model improves on other state-of-the-art generative and discriminative models on the region and time period identification tasks. Experiments show that our sparse mixed-effects model is more accurate quantitatively and qualitatively interesting, and that these improvements are robust across different parameter settings.</p><p>6 0.56814921 <a title="97-lda-6" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>7 0.5612933 <a title="97-lda-7" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>8 0.55977875 <a title="97-lda-8" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>9 0.55877221 <a title="97-lda-9" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>10 0.558061 <a title="97-lda-10" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>11 0.55355024 <a title="97-lda-11" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>12 0.55327743 <a title="97-lda-12" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>13 0.54641867 <a title="97-lda-13" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>14 0.54524887 <a title="97-lda-14" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>15 0.54387063 <a title="97-lda-15" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>16 0.54138356 <a title="97-lda-16" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>17 0.54023468 <a title="97-lda-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.53837097 <a title="97-lda-18" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>19 0.53834528 <a title="97-lda-19" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>20 0.53791726 <a title="97-lda-20" href="./acl-2012-Transforming_Standard_Arabic_to_Colloquial_Arabic.html">202 acl-2012-Transforming Standard Arabic to Colloquial Arabic</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
