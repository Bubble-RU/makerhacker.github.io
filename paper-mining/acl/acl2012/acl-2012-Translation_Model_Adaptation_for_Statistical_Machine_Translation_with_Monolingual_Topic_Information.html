<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-203" href="#">acl2012-203</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</h1>
<br/><p>Source: <a title="acl-2012-203-pdf" href="http://aclweb.org/anthology//P/P12/P12-1048.pdf">pdf</a></p><p>Author: Jinsong Su ; Hua Wu ; Haifeng Wang ; Yidong Chen ; Xiaodong Shi ; Huailin Dong ; Qun Liu</p><p>Abstract: To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.</p><p>Reference: <a title="acl-2012-203-reference" href="../acl2012_reference/acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  ,  ,  ,  ,  Abstract To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. [sent-8, score-0.737]
</p><p>2 In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. [sent-9, score-1.723]
</p><p>3 Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. [sent-10, score-1.191]
</p><p>4 Experimental result on the NIST Chinese-English translation task shows  that our approach significantly outperforms the baseline system. [sent-11, score-0.288]
</p><p>5 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al. [sent-12, score-0.361]
</p><p>6 However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. [sent-16, score-0.203]
</p><p>7 459 approximate the empirical distributions of the training data, which typically consist of bilingual sentences and monolingual target language sentences. [sent-18, score-0.392]
</p><p>8 When the translated texts and the training data come from the same domain, SMT systems can achieve good performance, otherwise the translation quality  degrades dramatically. [sent-19, score-0.316]
</p><p>9 Therefore, it is of significant importance to develop translation systems which can be effectively transferred from one domain to another, for example, from newswire to weblog. [sent-20, score-0.321]
</p><p>10 According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. [sent-21, score-0.831]
</p><p>11 Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. [sent-22, score-0.729]
</p><p>12 However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improve-  ment. [sent-28, score-0.72]
</p><p>13 In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-29, score-0.785]
</p><p>14 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi4c 5s9–468, monolingual corpora. [sent-31, score-0.25]
</p><p>15 , 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. [sent-33, score-0.797]
</p><p>16 For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ e `an”. [sent-34, score-0.625]
</p><p>17 Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. [sent-35, score-0.318]
</p><p>18 In a monolingual corpus, if “bank” occurs more often in the sentences related to the economy topic than the ones related to the geography topic, it is more likely that “bank” is translated to “y´inh´ ang” than to “h´ e `an”. [sent-36, score-0.578]
</p><p>19 With the  out-of-domain bilingual corpus, we first incorporate the topic information into translation probability estimation, aiming to quantify the effect of the topical context information on translation selection. [sent-37, score-1.176]
</p><p>20 Then, we rescore all phrase pairs according to the phrasetopic and the word-topic posterior distributions of the additional in-domain monolingual corpora. [sent-38, score-0.488]
</p><p>21 As compared to the previous works, our method takes advantage of both the in-domain monolingual corpora and the out-of-domain bilingual corpus to incorporate the topic information into our translation model, thus breaking down the corpus barrier for translation quality improvement. [sent-39, score-1.328]
</p><p>22 The reminder of this paper is organized as follows: Section 2 provides a brief description of translation probability estimation. [sent-41, score-0.363]
</p><p>23 Section 3 introduces the adaptation method which incorporates the topic information into the translation model; Section 4 describes and discusses the experimental results; Section 5 briefly summarizes the recent related work about translation model adaptation. [sent-42, score-1.043]
</p><p>24 2  Background  The statistical translation model, which contains phrase pairs with bi-directional phrase probabilities and bi-directional lexical probabilities, has a great 460 effect on the performance of SMT system. [sent-44, score-0.622]
</p><p>25 Phrase probability measures the co-occurrence frequency of a phrase pair, and lexical probability is used to validate the quality of the phrase pair by checking how well its words are translated to each other. [sent-45, score-0.436]
</p><p>26 , I, the phrase pair ⊆(f˜, ( e) )is : sja =id 1t,o. [sent-65, score-0.129]
</p><p>27 (Och haned p Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. [sent-69, score-0.419]
</p><p>28 Thus, the statistical model estimated from the training data is not suitable for text translation in different domains, resulting in a significant drop in translation quality. [sent-72, score-0.619]
</p><p>29 3 Translation Model Adaptation via Monolingual Topic Information In this section, we first briefly review the principle of Hidden Topic Markov Model(HTMM) which is the basis of our method, then describe our approach to translation model adaptation in detail. [sent-73, score-0.458]
</p><p>30 1 Hidden Topic Markov Model During the last couple of years, topic models such as Probabilistic Latent Semantic Analysis (Hofmann, 1999) and Latent Dirichlet Allocation model (Blei, 2003), have drawn more and more attention and been applied successfully in NLP community. [sent-75, score-0.269]
</p><p>31 Based on the assumption that all words in the same sentence have the same topic and the successive sentences are more likely to have the same topic, HTMM incorporates the local dependency between words by Hidden Markov Model for better topic estimation. [sent-81, score-0.567]
</p><p>32 That is, HTMM can estimate the probability distribution of a topic over words, i. [sent-83, score-0.448]
</p><p>33 Besides, HTMM Pde(rwivoesr i|nthoepriec)nt topics itrna sentences rather than in documents, so we can easily obtain the sentencetopic distribution P(topic|sentence) in training corpus. [sent-86, score-0.144]
</p><p>34 2 Adapted Phrase Probability Estimation We utilize the additional in-domain monolingual corpora to adapt the out-of-domain translation model for domain-specific translation task. [sent-89, score-0.922]
</p><p>35 In detail, we build an adapted translation model in the following steps: •  •  •  Build a topic-specific translation model to quantify th toep eicf-fescpte coiffi tche t topic tiniofnorm maotidoenl on the translation probability estimation. [sent-90, score-1.259]
</p><p>36 Estimate the topic posterior distributions of phrases ein hthee itno-pdicom paoinst monolingual corpora. [sent-91, score-0.593]
</p><p>37 Score the phrase pairs according to the predefSicnoerde topic-specific itrrsa ancsclaotirodnin mg toode thl ea pndre tdheetopic posterior distribution of phrases. [sent-92, score-0.251]
</p><p>38 To compute φ(˜ e|f˜), we first apply HTMM to respectively ptruatien φ tw( ˜eo| monolingual topic models with the following corpora: one is the source part of the out-of-domain bilingual corpus Cf out, the other is the in-domain monolingual corpus Cf in in the source language. [sent-94, score-0.945]
</p><p>39 To aφv(˜ eo|id confusion, we further refine φ( ˜e|f˜,tf) and P(tf |f˜) nwfiuthsi φ(˜ e|f˜, tf out) arn rde P(tf in |f˜), respectively. [sent-96, score-0.59]
</p><p>40 Here, tf out is the topic clustere|d from the corpus Cf out, and tf in represents the topic derived from the corpus Cf in. [sent-97, score-1.78]
</p><p>41 Besides, their topic dimensions are not assured to be the same. [sent-100, score-0.269]
</p><p>42 To solve this problem, we introduce the topic mapping probability P(tf out |tf in) to map the in-domain phrase-topic distribution| into the one in the out-domain topic space. [sent-101, score-0.641]
</p><p>43 1  Topic-Specific Phrase Translation Probability φ(˜ e|f˜, tf out) We follow the common practice (Koehn et al. [sent-105, score-0.59]
</p><p>44 , 2003) to calculate the topic-specific phrase translation probability, and the only difference is that our method takes the topical context information into account when collecting the fractional counts of phrase pairs. [sent-106, score-0.655]
</p><p>45 2 Topic Mapping Probability P(tf out |tf in) Based on the two monolingual topic models respectively trained from Cf in and Cf out, we compute the topic mapping probability by using source word f as the pivot variable. [sent-110, score-0.891]
</p><p>46 Noticing that there 462 are some words occurring in one corpus only, we use the words belonging to both corpora during the mapping procedure. [sent-111, score-0.116]
</p><p>47 |f˜)  Pword(tf in|f˜) = 1− Pword(t¯f in|f˜) ≈ 1−Y P(¯tf in|fj) fYj∈f˜ =  1  −Y(1 fYj∈f˜  − P(tf in|fj))  (12)  |f˜)  where Pword(¯tf in represents the probability that tf in is not the topic of the phrase f˜. [sent-121, score-1.063]
</p><p>48 Similarly, P(¯tf in |fj) indicates the probability that tf in is not the topic of the word fj. [sent-122, score-0.934]
</p><p>49 With the assumption that tf in is the topic of if at least one of the words in f˜ belongs to this topic, we derive Pword(tf in as follows:  f˜  |f˜) Pword(tf in|f˜) ≈ X P(tf in|fj)/|f˜|  (13)  fXj∈f˜ where  |f˜| denotes  the number of words in phrase  f˜. [sent-124, score-1.017]
</p><p>50 3 Adapted Lexical Probability Estimation Now we briefly describe how to estimate the adapted lexical weight for phrase pairs, which can be adjusted in a similar way to the phrase probability. [sent-126, score-0.367]
</p><p>51 With the adjusted lexical translation probability, we resort to formula (4) to update the lexical weight for the phrase pair (f˜, e). [sent-128, score-0.513]
</p><p>52 463 4  Experiment  We evaluate our method on the Chinese-to-English translation task for the weblog text. [sent-129, score-0.33]
</p><p>53 After a brief description of the experimental setup, we investigate the effects of various factors on the translation system performance. [sent-130, score-0.316]
</p><p>54 , 2006) as the in-domain monolingual corpora in the source language and target language, respectively. [sent-136, score-0.307]
</p><p>55 To obtain more accurate topic information by HTMM, we firstly filter the noisy blog documents and the ones consisting of short sentences. [sent-137, score-0.385]
</p><p>56 Then, we sample equal numbers of documents from the in-domain monolingual corpora in the source language and the target language to respectively train two in-domain topic models. [sent-141, score-0.61]
</p><p>57 To obtain various topic distributions for the outof-domain training corpus and the in-domain monolingual corpora in the source language and the target language respectively, we use HTMM tool developed by Gruber et al. [sent-143, score-0.667]
</p><p>58 GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. [sent-161, score-0.114]
</p><p>59 The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al. [sent-164, score-0.288]
</p><p>60 We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. [sent-171, score-0.309]
</p><p>61 For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5K and 40K. [sent-172, score-0.25]
</p><p>62 Table 1reports the BLEU scores of the translation system under various conditions. [sent-184, score-0.288]
</p><p>63 Using the out-ofdomain phrase table, the baseline system achieves a BLEU score of 20. [sent-185, score-0.129]
</p><p>64 In the experiments with the small-scale in-domain monolingual corpora, the 464 BLEU scores acquired by two methods are 20. [sent-187, score-0.25]
</p><p>65 In the experiments with the large-scale monolingual in-domain corpora, similar results are obtained, with absolute improvements of 0. [sent-192, score-0.25]
</p><p>66 This is because the “Noisy-OR” method involves the multiplication of the word-topic distribution (shown in formula (12)), which leads to much sharper phrase-topic distribution than “Averaging” method, and is more likely to introduce bias to the translation probability estimation. [sent-197, score-0.581]
</p><p>67 2 Effect of Combining Two Phrase Tables In the above experiments, we replace the out-ofdomain phrase table with the adapted phrase table. [sent-201, score-0.309]
</p><p>68 Here we combine these two phrase tables in a loglinear framework to see if we could obtain further improvement. [sent-202, score-0.161]
</p><p>69 To offer a clear description, we represent the out-of-domain phrase table and the adapted phrase table with “OutBP” and “AdapBP”, respectively. [sent-203, score-0.309]
</p><p>70 Table 2 shows the results of experiments using different phrase tables. [sent-217, score-0.129]
</p><p>71 Applying our adaptation approach, both “AdapBP” and “OutBP + AdapBP” consistently outperform the baseline, and the lat-  Figure 1: Effect of in-domain monolingual corpus size on translation quality. [sent-218, score-0.739]
</p><p>72 The underlying reason is that the probability distribution of each in-domain sentence often converges on some topics in the “AdapBP” method and some translation probabilities are overestimated, which leads to negative effects on the translation quality. [sent-227, score-0.763]
</p><p>73 By using two tables together, our approach reduces the bias introduced by “AdapBP”, therefore further improving the translation quality. [sent-228, score-0.288]
</p><p>74 3  Effect of In-domain Monolingual Corpus Size Finally, we investigate the effect of in-domain monolingual corpus size on translation quality. [sent-231, score-0.602]
</p><p>75 In the experiment, we try different sizes of in-domain documents to train different monolingual topic mod-  els: from 5K to 80K with an increment of 5K each time. [sent-232, score-0.553]
</p><p>76 Figure 1 shows the BLEU scores of the translation system on the test set. [sent-234, score-0.288]
</p><p>77 It can be seen that the more data, the better translation quality when the corpus size is less than 30K. [sent-235, score-0.319]
</p><p>78 For this experimental result, we speculate that with the increment of in-domain monolingual data, the corresponding topic models provide more accurate topic informa-  tion to improve the translation system. [sent-244, score-1.104]
</p><p>79 However, this effect weakens when the monolingual corpora continue to increase. [sent-245, score-0.34]
</p><p>80 5  Related work  Most previous researches about translation model adaptation focused on parallel data collection. [sent-246, score-0.497]
</p><p>81 (2005) employed information retrieval technology to gather the bilingual sentences, which are similar to the test set, from available in-domain and out-of-domain training data to build an adaptive translation model. [sent-248, score-0.402]
</p><p>82 With the same motivation, Munteanu and Marcu (2005) extracted in-domain bilingual sentence pairs from comparable corpora. [sent-249, score-0.114]
</p><p>83 Since large-scale monolingual corpus is easier to obtain than parallel corpus, there have been some studies on how to generate parallel sentences with monolingual sentences. [sent-250, score-0.641]
</p><p>84 (2008) used an in-domain translation dictionary and monolingual corpora to adapt an out-of-domain translation model for the indomain text. [sent-253, score-0.922]
</p><p>85 Under this framework, the training corpus is first divided into different parts, each of which is used to train a sub translation model, then these sub models are used together with different weights during decoding. [sent-256, score-0.319]
</p><p>86 , 2009) or the phrase pairs of phrase table (Foster et al. [sent-258, score-0.258]
</p><p>87 Our method deals with translation model adaptation by making use of the topical context, so let us take a look at the recent research development on the application of topic models in SMT. [sent-261, score-0.806]
</p><p>88 (2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. [sent-264, score-1.025]
</p><p>89 Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. [sent-265, score-0.472]
</p><p>90 Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. [sent-266, score-0.667]
</p><p>91 The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the  latter built the classifiers which combine rich context information to better select translation during decoding. [sent-275, score-0.348]
</p><p>92 With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. [sent-276, score-0.318]
</p><p>93 •  •  We focus on how to adapt a translation modeWl efo fro domain-specific dtraapnstl aat itorann stalaskti ownit mh othdehelp of additional in-domain monolingual corpora, which are far from full exploitation in the parallel data collection and mixture modeling framework. [sent-278, score-0.69]
</p><p>94 In addition to the utilization  of in-domain  466 monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al. [sent-279, score-0.25]
</p><p>95 (3) Instead of rescoring phrase pairs online, our approach calculate the translation probabilities offline, which brings no additional burden to translation systems and is suitable to translate the texts without the topic distribution information. [sent-281, score-1.05]
</p><p>96 6  Conclusion and future work  This paper presents a novel method for SMT system adaptation by making use of the monolingual corpora in new domains. [sent-283, score-0.477]
</p><p>97 Our approach first estimates the translation probabilities from the out-ofdomain bilingual corpus given the topic information, and then rescores the phrase pairs via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. [sent-284, score-1.57]
</p><p>98 Experimental results show that our method achieves better performance than the baseline system, without increasing the burden of the translation system. [sent-285, score-0.288]
</p><p>99 Furthermore, since the in-domain phrase-topic distribution is currently estimated with simple smoothing interpolations, we expect that the translation system could benefit from other sophisticated smoothing methods. [sent-287, score-0.424]
</p><p>100 Finally, the reasonable estimation of topic number for better translation model adaptation will also become our study emphasis. [sent-288, score-0.768]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tf', 0.59), ('translation', 0.288), ('topic', 0.269), ('monolingual', 0.25), ('htmm', 0.193), ('adaptation', 0.17), ('adapbp', 0.158), ('pword', 0.158), ('cf', 0.151), ('phrase', 0.129), ('bilingual', 0.114), ('outbp', 0.088), ('topical', 0.079), ('distribution', 0.076), ('probability', 0.075), ('fj', 0.07), ('countf', 0.07), ('formula', 0.066), ('pages', 0.06), ('corpora', 0.057), ('averaging', 0.057), ('bleu', 0.056), ('gong', 0.056), ('tam', 0.056), ('gruber', 0.056), ('hasan', 0.056), ('zhao', 0.053), ('cpf', 0.053), ('pmle', 0.053), ('txf', 0.053), ('ney', 0.052), ('smt', 0.051), ('foster', 0.051), ('adapted', 0.051), ('blog', 0.05), ('xing', 0.05), ('posterior', 0.046), ('civera', 0.046), ('eck', 0.046), ('hildebrand', 0.046), ('ueffing', 0.046), ('sa', 0.043), ('statistical', 0.043), ('weblog', 0.042), ('och', 0.042), ('qun', 0.041), ('estimation', 0.041), ('parallel', 0.039), ('matthias', 0.039), ('zens', 0.039), ('adapt', 0.039), ('mixture', 0.037), ('hermann', 0.037), ('exploitation', 0.037), ('mle', 0.037), ('lv', 0.037), ('topics', 0.036), ('adapb', 0.035), ('cout', 0.035), ('devw', 0.035), ('fyj', 0.035), ('inh', 0.035), ('phrasetopic', 0.035), ('tfx', 0.035), ('twste', 0.035), ('xtf', 0.035), ('koehn', 0.035), ('stephan', 0.034), ('matsoukas', 0.034), ('mauser', 0.034), ('documents', 0.034), ('domain', 0.033), ('effect', 0.033), ('obtain', 0.032), ('ei', 0.032), ('alignment', 0.032), ('roland', 0.031), ('corpus', 0.031), ('xiamen', 0.031), ('ruiz', 0.031), ('almut', 0.031), ('bacchiani', 0.031), ('silja', 0.031), ('geography', 0.031), ('michiel', 0.031), ('adjusted', 0.03), ('context', 0.03), ('bing', 0.03), ('smoothing', 0.03), ('machine', 0.03), ('assumption', 0.029), ('latent', 0.029), ('lexicon', 0.028), ('mapping', 0.028), ('interpolation', 0.028), ('shouxun', 0.028), ('experimental', 0.028), ('translated', 0.028), ('estimate', 0.028), ('distributions', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000018 <a title="203-tfidf-1" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>Author: Jinsong Su ; Hua Wu ; Haifeng Wang ; Yidong Chen ; Xiaodong Shi ; Huailin Dong ; Qun Liu</p><p>Abstract: To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.</p><p>2 0.37120053 <a title="203-tfidf-2" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong ; Min Zhang ; Qun Liu ; Shouxun Lin</p><p>Abstract: Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</p><p>3 0.28203824 <a title="203-tfidf-3" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>Author: Vladimir Eidelman ; Jordan Boyd-Graber ; Philip Resnik</p><p>Abstract: We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation without any human annotation. We use these topic distributions to compute topic-dependent lex- ical weighting probabilities and directly incorporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topicrelevant output, resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline.</p><p>4 0.23327364 <a title="203-tfidf-4" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>5 0.22461452 <a title="203-tfidf-5" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; George Foster ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</p><p>6 0.17906803 <a title="203-tfidf-6" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>7 0.167649 <a title="203-tfidf-7" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>8 0.1481183 <a title="203-tfidf-8" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>9 0.14230041 <a title="203-tfidf-9" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>10 0.14103518 <a title="203-tfidf-10" href="./acl-2012-SITS%3A_A_Hierarchical_Nonparametric_Model_using_Speaker_Identity_for_Topic_Segmentation_in_Multiparty_Conversations.html">171 acl-2012-SITS: A Hierarchical Nonparametric Model using Speaker Identity for Topic Segmentation in Multiparty Conversations</a></p>
<p>11 0.13701837 <a title="203-tfidf-11" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>12 0.13467696 <a title="203-tfidf-12" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>13 0.13214226 <a title="203-tfidf-13" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>14 0.1250037 <a title="203-tfidf-14" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>15 0.1242551 <a title="203-tfidf-15" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>16 0.12138501 <a title="203-tfidf-16" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>17 0.12064199 <a title="203-tfidf-17" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>18 0.11858319 <a title="203-tfidf-18" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>19 0.11529802 <a title="203-tfidf-19" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>20 0.11277722 <a title="203-tfidf-20" href="./acl-2012-Finding_Bursty_Topics_from_Microblogs.html">98 acl-2012-Finding Bursty Topics from Microblogs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.311), (1, -0.194), (2, 0.32), (3, 0.093), (4, -0.188), (5, -0.059), (6, -0.042), (7, -0.062), (8, 0.008), (9, -0.015), (10, 0.059), (11, 0.031), (12, 0.059), (13, 0.015), (14, 0.013), (15, 0.027), (16, -0.008), (17, -0.001), (18, 0.016), (19, 0.021), (20, -0.103), (21, -0.02), (22, 0.058), (23, -0.026), (24, 0.118), (25, -0.051), (26, 0.035), (27, 0.006), (28, -0.09), (29, -0.01), (30, 0.005), (31, 0.089), (32, -0.009), (33, -0.01), (34, -0.059), (35, -0.028), (36, 0.018), (37, -0.05), (38, 0.095), (39, -0.107), (40, -0.002), (41, -0.043), (42, -0.073), (43, -0.016), (44, -0.075), (45, -0.027), (46, -0.004), (47, 0.004), (48, 0.061), (49, -0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97401834 <a title="203-lsi-1" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>Author: Jinsong Su ; Hua Wu ; Haifeng Wang ; Yidong Chen ; Xiaodong Shi ; Huailin Dong ; Qun Liu</p><p>Abstract: To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.</p><p>2 0.91800761 <a title="203-lsi-2" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>Author: Vladimir Eidelman ; Jordan Boyd-Graber ; Philip Resnik</p><p>Abstract: We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts, where topics are induced in an unsupervised way using topic models; this can be thought of as inducing subcorpora for adaptation without any human annotation. We use these topic distributions to compute topic-dependent lex- ical weighting probabilities and directly incorporate them into our translation model as features. Conditioning lexical probabilities on the topic biases translations toward topicrelevant output, resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline.</p><p>3 0.85190147 <a title="203-lsi-3" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong ; Min Zhang ; Qun Liu ; Shouxun Lin</p><p>Abstract: Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</p><p>4 0.7234385 <a title="203-lsi-4" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; George Foster ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</p><p>5 0.70244318 <a title="203-lsi-5" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>6 0.68415034 <a title="203-lsi-6" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>7 0.66747338 <a title="203-lsi-7" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>8 0.59674293 <a title="203-lsi-8" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>9 0.59667408 <a title="203-lsi-9" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>10 0.58937526 <a title="203-lsi-10" href="./acl-2012-SITS%3A_A_Hierarchical_Nonparametric_Model_using_Speaker_Identity_for_Topic_Segmentation_in_Multiparty_Conversations.html">171 acl-2012-SITS: A Hierarchical Nonparametric Model using Speaker Identity for Topic Segmentation in Multiparty Conversations</a></p>
<p>11 0.58912116 <a title="203-lsi-11" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>12 0.57926446 <a title="203-lsi-12" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>13 0.5791406 <a title="203-lsi-13" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>14 0.57605857 <a title="203-lsi-14" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>15 0.57529926 <a title="203-lsi-15" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>16 0.56912881 <a title="203-lsi-16" href="./acl-2012-Efficient_Tree-Based_Topic_Modeling.html">79 acl-2012-Efficient Tree-Based Topic Modeling</a></p>
<p>17 0.56212896 <a title="203-lsi-17" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>18 0.54735678 <a title="203-lsi-18" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>19 0.54404593 <a title="203-lsi-19" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>20 0.53845549 <a title="203-lsi-20" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.021), (26, 0.031), (28, 0.102), (30, 0.032), (37, 0.05), (39, 0.035), (57, 0.028), (59, 0.015), (74, 0.051), (82, 0.024), (84, 0.017), (85, 0.038), (90, 0.18), (92, 0.061), (94, 0.034), (95, 0.186), (99, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85300887 <a title="203-lda-1" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>same-paper 2 0.84449768 <a title="203-lda-2" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>Author: Jinsong Su ; Hua Wu ; Haifeng Wang ; Yidong Chen ; Xiaodong Shi ; Huailin Dong ; Qun Liu</p><p>Abstract: To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.</p><p>3 0.75744843 <a title="203-lda-3" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong ; Min Zhang ; Qun Liu ; Shouxun Lin</p><p>Abstract: Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</p><p>4 0.75042105 <a title="203-lda-4" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>5 0.74865681 <a title="203-lda-5" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>6 0.74802697 <a title="203-lda-6" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>7 0.74611562 <a title="203-lda-7" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>8 0.74253684 <a title="203-lda-8" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>9 0.74013174 <a title="203-lda-9" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>10 0.73762506 <a title="203-lda-10" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>11 0.73587662 <a title="203-lda-11" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>12 0.73486704 <a title="203-lda-12" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>13 0.73473489 <a title="203-lda-13" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>14 0.73425078 <a title="203-lda-14" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>15 0.73324955 <a title="203-lda-15" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>16 0.73295975 <a title="203-lda-16" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>17 0.73254442 <a title="203-lda-17" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>18 0.72930181 <a title="203-lda-18" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>19 0.7288577 <a title="203-lda-19" href="./acl-2012-You_Had_Me_at_Hello%3A_How_Phrasing_Affects_Memorability.html">218 acl-2012-You Had Me at Hello: How Phrasing Affects Memorability</a></p>
<p>20 0.72826713 <a title="203-lda-20" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
