<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-66" href="#">acl2012-66</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</h1>
<br/><p>Source: <a title="acl-2012-66-pdf" href="http://aclweb.org/anthology//P/P12/P12-3010.pdf">pdf</a></p><p>Author: Ming-Hong Bai ; Yu-Ming Hsieh ; Keh-Jiann Chen ; Jason S. Chang</p><p>Abstract: In this paper, we propose a web-based bilingual concordancer, DOMCAT , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient 1 for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively. 1</p><p>Reference: <a title="acl-2012-66-reference" href="../acl2012_reference/acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Chang2 1Institute of Information Science, Academia Sinica, Taiwan 2 Department of Computer Science, National Tsing-Hua University, Taiwan mhbai @ s ini ca . [sent-2, score-0.057]
</p><p>2 kchen@ i s s ini ca edu tw, i Abstract In this paper, we propose a web-based bilingual concordancer, DOMCAT , for domain-specific computer assisted translation. [sent-10, score-0.288]
</p><p>3 To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. [sent-12, score-1.803]
</p><p>4 To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient  1  for assessing the correlation between the query and the translation equivalents. [sent-13, score-1.242]
</p><p>5 The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively. [sent-14, score-0.777]
</p><p>6 1  Introduction  A bilingual concordancer is a tool that can retrieve aligned sentence pairs in a parallel corpus whose source sentences contain the query and the translation equivalents of the query are identified in the target sentences. [sent-15, score-1.48]
</p><p>7 It helps not only on finding translation equivalents of the query but also presenting various contexts of occurrence. [sent-16, score-0.676]
</p><p>8 As a result, it is extremely useful for bilingual 1 http://ckip. [sent-17, score-0.13]
</p><p>9 Identifying the translation equivalents, translation spotting, is the most challenging part of a bilingual concordancer. [sent-25, score-0.67]
</p><p>10 Recently, most of the existing bilingual concordancers spot translation equivalents in terms of word alignment-based method. [sent-26, score-0.874]
</p><p>11 First, aligning a rare (low frequency) term may encounter the garbage collection effect (Moore, 2004; Liang et al. [sent-32, score-0.151]
</p><p>12 , 2006) that cause the term to align to many unrelated words. [sent-33, score-0.081]
</p><p>13 Second, the statistical word alignment model is not good at many-to-many alignment due to the fact that translation equivalents are not always correlated in lexical level. [sent-34, score-0.564]
</p><p>14 Unfortunately, the above effects will be intensified in a domain-specific concordancer because the queries are usually domain-specific terms, which are mostly multi-word low-frequency terms and semantically non-compositional terms. [sent-35, score-0.291]
</p><p>15 (2003) employed a statistical association criterion to spot translation equivalents in their bilingual concordancer. [sent-37, score-0.917]
</p><p>16 The associationbased criterion can avoid the above mentioned effects. [sent-38, score-0.164]
</p><p>17 However, it has other drawbacks in translation spotting task. [sent-39, score-0.691]
</p><p>18 First, it will encounter the  contextual effect that causes the system incorrectly spot the translations of the strongly collocated context. [sent-40, score-0.425]
</p><p>19 Second, the association-based translation spotting tends to spot the common subsequence of a set of similar translations instead of the full translations. [sent-41, score-1.004]
</p><p>20 c s 2o0c1ia2ti Aosns fo cria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsteiscs 5 –60, painting painted by ‘Fan K'uan/范 寬 ’ since the painter’s name is strongly collocated with the name of the painting. [sent-44, score-0.09]
</p><p>21 ‘Fan K'uan’ may be incorrectly spotted as part of the translation of ‘谿 山 行旅 圖 ’, if pure association method is applied. [sent-46, score-0.575]
</p><p>22 Figure 2 illustrates an example of common subsequence effect, in which ‘ 清明 上河圖 ’ (the  River During the Qingming Festival/ Up the River During Qingming) has two similar translations as quoted, but the Dice coefficient tends to spot the common subsequences of the translations. [sent-47, score-0.533]
</p><p>23 (2009) proposed a normalized frequency criterion to extract translation equivalents form sentence aligned parallel corpus. [sent-52, score-0.969]
</p><p>24 This criterion takes lexical-level contexture effect into account, so it can effectively resolve the above mentioned effect. [sent-53, score-0.148]
</p><p>25 But the goal of their method is to find most common translations instead of spotting translations, so the normalized frequency criterion tends to ignore rare translations. [sent-54, score-0.844]
</p><p>26 In this paper, we propose a bilingual concordancer, DOMCAT, for computer assisted domain-specific term translation. [sent-55, score-0.287]
</p><p>27 To remedy the  above mentioned effects, we extended the normalized frequency of Bai et al. [sent-56, score-0.32]
</p><p>28 (2009) to a normalized correlation criterion to spot translation equivalents. [sent-57, score-0.939]
</p><p>29 The normalized correlation inherits the characteristics of normalized frequency and is adjusted for spotting rare translations. [sent-58, score-1.052]
</p><p>30 These characteristics are especially important for a domain-specific bilingual concordancer to spot translation pairs of low-frequency and semantically non-compositional terms. [sent-59, score-0.888]
</p><p>31 2  The DOMCAT System  Given a query, the DOMCAT bilingual concordancer retrieves sentence pairs and spots translation equivalents by the following steps: 1. [sent-64, score-0.971]
</p><p>32 Retrieve the sentence pairs whose source sentences contain the query term. [sent-65, score-0.278]
</p><p>33 Extract translation candidate words from the  retrieved sentence pairs by the normalized correlation criterion. [sent-67, score-0.892]
</p><p>34 Spot the candidate words for each target sentence and rank the sentences by normalized the Dice coefficient criterion. [sent-69, score-0.487]
</p><p>35 In step 1, the query term can be a single word, a phrase, a gapped sequence and even a regular expression. [sent-70, score-0.232]
</p><p>36 The parallel corpus is indexed by the suffix array to efficiently retrieve the sentences. [sent-71, score-0.106]
</p><p>37 1  Extract Translation Candidate Words  After the queried sentence pairs retrieved from the parallel corpus, we can extract translation candidate words from the sentence pairs. [sent-76, score-0.678]
</p><p>38 We compute the local normalized correlation with respect to the query term for each word e in each target sentence. [sent-77, score-0.664]
</p><p>39 The local normalized correlation is defined as follows:  lnc(e;q,e,f)ffijqfpp((ee||f ij) ||qf|  (1)  where q denotes the query term, f denotes the source sentence and e denotes the target sentence,  is a small smoothing factor. [sent-78, score-0.774]
</p><p>40 The probability p(e|f) is the word translation probability derived from the entire parallel corpus by IBM Model 1 (Brown et al. [sent-79, score-0.335]
</p><p>41 The sense of local normalized correlation of e can be interpreted as the probability of word e being part of translation of the query term q under the condition of sentence pair (e, f). [sent-81, score-0.98]
</p><p>42 Once the local normalized correlation is computed for each word in retrieved sentences, we compute the normalized correlation on the retrieved sentences. [sent-82, score-1.022]
</p><p>43 The normalized correlation is the average of all lnc values and defined as follows:  nc(e;q)n1in1lnc(e;q,e(i),f(i))  (2)  where n is the number of retrieved sentence pairs. [sent-83, score-0.62]
</p><p>44 After the nc values for the words of the retrieved target sentences are computed, we can obtain a translation candidate list by filtering out the words with lower nc values. [sent-84, score-0.806]
</p><p>45 To compare with the association-based method,  we also sorted the word list by the Dice coefficient defined as follows:  dice(e,q)freq2(fer)eq(ef,rqeq)(q)  (3)  where freq is frequency function which computes frequencies from the parallel corpus. [sent-85, score-0.321]
</p><p>46 Figure 3(a) and (b) illustrate examples of translation candidate words of the query term ‘谿 山 行 圖 ’ (Travelers Among Mountains and Streams) sorted by the nc values, NC, and the Dice coefficients respectively. [sent-92, score-0.734]
</p><p>47 The result shows that the normalized correlation separated the related words  旅  57 from unrelated words much better than the Dice coefficient. [sent-93, score-0.407]
</p><p>48 The rationale behind the normalized correlation is that the nc value is the strength of word e generated by the query compared to that of generated by the whole sentence. [sent-94, score-0.788]
</p><p>49 As a result, the  normalized correlation can easily separate the words generated by the query term from the words generated by the context. [sent-95, score-0.614]
</p><p>50 On the contrary, the Dice coefficient counts the frequency of a co-occurred word without considering the fact that it could be generated by the strongly collocated context. [sent-96, score-0.289]
</p><p>51 2  Translation Spotting  Once we have a translation candidate list and respective nc values, we can spot the translation equivalents by the following spotting algorithm. [sent-98, score-1.548]
</p><p>52 For each target sentence, first, spot the word with highest nc value. [sent-99, score-0.383]
</p><p>53 Then extend the spotted sequence to the neighbors of the word by checking their nc values of neighbor words but skipping function words. [sent-100, score-0.519]
</p><p>54 If the nc value is greater than a threshold θ , add the word into spotted sequence. [sent-101, score-0.433]
</p><p>55 Repeat the extending process until no word can be added to the spotted sequence. [sent-102, score-0.272]
</p><p>56 The following is the pseudo-code for the algorithm: S is the target sentence H is the spotted word sequence θ is the threshold of translation candidate words  Initialize: H←? [sent-103, score-0.663]
</p><p>57 emax←S[0] Foreach ei in S: If nc(ei) > nc(emax): emax ← ei If nc(emax )? [sent-104, score-0.186]
</p><p>58 θ : add emax to H Repeat until no word add to H ej←left neighbor of H If nc(ej )? [sent-105, score-0.174]
</p><p>59 θ : add ej to H ek←right neighbor of H If nc( ek ) ? [sent-106, score-0.139]
</p><p>60 θ : add ek to H Figure 4: Pseudo-code of translation spotting process. [sent-107, score-0.691]
</p><p>61 3 Ranking The ranking mechanism of a bilingual concordancer is used to provide the most related translation of the query on the top of the outputs for the user. [sent-109, score-0.938]
</p><p>62 So, an association metric is needed to evaluate the relations between the query and the spotted translations. [sent-110, score-0.448]
</p><p>63 The Dice coefficient is a widely used measure for assessing the association strength between a multi-word expression and its  translation candidates. [sent-111, score-0.502]
</p><p>64 , 1996; Kitamura and Matsumoto, 1996; Yamamoto and Matsumoto, 2000; Melamed, 2001) The following is the definition of the Dice coefficient:  dice(t,q)freq2(ftr)eq(tf,rqeq)(q)  (4)  where q denotes a multi-word expression to be translated, t denotes a translation candidate of q. [sent-113, score-0.417]
</p><p>65 However, the Dice coefficient has the common subsequence effect (as mentioned in Section 1) due to the fact that the co-occurrence frequency of the common subsequence is usually larger than that of the full translation; hence, the Dice coefficient tends to choose the common subsequence. [sent-114, score-0.595]
</p><p>66 To remedy the common subsequence effect, we introduce a normalized frequency for a spotted sequence defined as follows:  nf(t,q)nlnf(t;q,e(i),f(i))  (5)  i1  where lnf is a function which compute normalized  frequency locally in each sentence. [sent-115, score-1.039]
</p><p>67 The following is the definition of lnf:  lnf(t;q,e,f)eH t(1lnc(e;q,e,f))  (6)  where H is the spotted sequence of the sentence pair (e,f), H-t are the words in H but not in t. [sent-116, score-0.321]
</p><p>68 The rationale behind lnf function is that: when counting the local frequency of t in a sentence pair, if t is a subsequence of H, then the count of t should be reasonably reduced by considering the strength of the correlation between the words in H-t and the query. [sent-117, score-0.59]
</p><p>69 1 Experimental Setting We use the Chinese/English web pages of the National Palace Museum as our underlying parallel corpus. [sent-120, score-0.065]
</p><p>70 The English sentences are tokenized and lemmatized by using the NLTK (Bird and Loper, 2004) and the Chinese sentences are segmented by the CKIP Chinese segmenter (Ma and Chen, 2003). [sent-124, score-0.05]
</p><p>71 To evaluate the performance of the translation spotting, we selected 12 domain-specific terms to query the concordancer. [sent-125, score-0.446]
</p><p>72 Then, the returned spotted translation equivalents are evaluated against a manually annotated gold standard in terms of recall and precision metrics. [sent-126, score-0.772]
</p><p>73 We also build two different translation spotting modules by using the GIZA++ toolkit (Och and Ney, 2000) with the intersection/union of the bidirectional word alignment as baseline systems. [sent-127, score-0.677]
</p><p>74 To evaluate the performance of the ranking  2  criterion, we compiled a reference translation set for each query by collecting the manually annotated translation spotting set and selecting 1 to 3 frequently used translations. [sent-128, score-1.159]
</p><p>75 Then, the outputs of each query are ranked by the nf_dice function and evaluated against the reference translation set. [sent-129, score-0.54]
</p><p>76 We also compared the ranking performance with the Dice coefficient. [sent-130, score-0.068]
</p><p>77 2 Evaluation of Translation Spotting We evaluate the translation spotting in terms of the Recall and Precision metrics defined as follows: 2 http://www. [sent-132, score-0.645]
</p><p>78 tw  Recallin1|Hin1g(|iH)(giH)|(i)| Precisionni1|Hin1g(|iH)(iH)|(i)|  (8) (9)  where i denotes the index of the retrieved sentence, is the spotted sequences of the ith  H(i)  sentence returned by the concordancer, and H(gi) is the gold standard spotted sequences of the ith sentence. [sent-135, score-0.749]
</p><p>79 Table 1 shows the evaluation of translation spotting for normalized correlation, NC, compared with the intersection and union of GIZA++ word alignment. [sent-136, score-0.848]
</p><p>80 The F-score of the normalized correlation is much higher than that of the word alignment methods. [sent-137, score-0.414]
</p><p>81 It is noteworthy that the normalized correlation increased the recall rate without losing the precision rate. [sent-138, score-0.409]
</p><p>82 This may indicate that the normalized correlation can effectively conquer the drawbacks of the word alignmentbased translation spotting and the associationbased translation spotting mentioned in Section 1. [sent-139, score-1.791]
</p><p>83 We also evaluate the queried results of each term individually (as shown in Table 2). [sent-145, score-0.11]
</p><p>84 As it shows, the normalized correlation is quite stable  for translation spotting. [sent-146, score-0.652]
</p><p>85 3 Evaluation of Ranking To evaluate the performance of a ranking function, we ranked the retrieved sentences of the queries by the function. [sent-157, score-0.269]
</p><p>86 1, as acceptable translation set for each query of our experiment. [sent-160, score-0.499]
</p><p>87 Table 3 shows the coverage rate of the nf_dice function compared with the Dice coefficient. [sent-161, score-0.096]
</p><p>88 As it shows, in the  outputs ranked by the Dice coefficient, uses usually have to look up more than 3 sentences to find an acceptable translation; while in the outputs ranked by the nf_dice function, users can find an acceptable translation in top-2 sentences. [sent-162, score-0.525]
</p><p>89 4  Conclusion and Future Works  In this paper, we proposed a bilingual concordancer, DOMCAT, designed as a domainspecific computer assisted translation tool. [sent-167, score-0.501]
</p><p>90 We exploited a normalized correlation which incorporate lexical level information into association-based method that effectively avoid the drawbacks of the word alignment-based translation spotting as well as the association-based translation spotting. [sent-168, score-1.383]
</p><p>91 In the future, it would be interesting to extend the parallel corpus to the internet to retrieve more rich data for the computer assisted translation. [sent-169, score-0.207]
</p><p>92 TRANSSEARCH: from a bilingual concordancer to a translation finder. [sent-181, score-0.664]
</p><p>93 Exploring the effects and use of a Chinese–English parallel concordancer. [sent-202, score-0.065]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spotting', 0.375), ('dice', 0.303), ('spotted', 0.272), ('translation', 0.27), ('concordancer', 0.264), ('equivalents', 0.23), ('normalized', 0.203), ('spot', 0.196), ('domcat', 0.192), ('correlation', 0.179), ('query', 0.176), ('nc', 0.161), ('coefficient', 0.138), ('bilingual', 0.13), ('emax', 0.12), ('retrieved', 0.117), ('assisted', 0.101), ('lnf', 0.096), ('criterion', 0.091), ('subsequence', 0.08), ('bourdaillet', 0.072), ('lnc', 0.072), ('mountains', 0.072), ('qingming', 0.072), ('ranking', 0.068), ('parallel', 0.065), ('collocated', 0.063), ('frequency', 0.061), ('ini', 0.057), ('term', 0.056), ('neighbor', 0.054), ('river', 0.054), ('bai', 0.054), ('ih', 0.054), ('queried', 0.054), ('acceptable', 0.053), ('fan', 0.05), ('sentence', 0.049), ('associationbased', 0.048), ('bowker', 0.048), ('champollion', 0.048), ('concordancers', 0.048), ('kitamura', 0.048), ('smadja', 0.048), ('travelers', 0.048), ('ek', 0.046), ('drawbacks', 0.046), ('candidate', 0.046), ('streams', 0.044), ('translations', 0.042), ('melamed', 0.042), ('ckip', 0.042), ('kupiec', 0.042), ('retrieve', 0.041), ('tends', 0.041), ('exploited', 0.04), ('denotes', 0.039), ('ej', 0.039), ('yamamoto', 0.038), ('hin', 0.038), ('strength', 0.037), ('coverage', 0.037), ('subsequences', 0.036), ('assessing', 0.034), ('incorrectly', 0.033), ('jason', 0.033), ('ei', 0.033), ('alignment', 0.032), ('ranked', 0.032), ('encounter', 0.032), ('rationale', 0.032), ('nltk', 0.032), ('function', 0.032), ('effect', 0.032), ('rare', 0.031), ('remedy', 0.031), ('bird', 0.031), ('correspondences', 0.031), ('outputs', 0.03), ('ma', 0.03), ('chinese', 0.029), ('pairs', 0.028), ('taiwan', 0.028), ('queries', 0.027), ('rate', 0.027), ('della', 0.027), ('strongly', 0.027), ('target', 0.026), ('pietra', 0.026), ('jian', 0.025), ('mentioned', 0.025), ('sentences', 0.025), ('unrelated', 0.025), ('sorted', 0.025), ('moore', 0.024), ('repeat', 0.024), ('local', 0.024), ('follows', 0.024), ('expression', 0.023), ('interpreted', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="66-tfidf-1" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>Author: Ming-Hong Bai ; Yu-Ming Hsieh ; Keh-Jiann Chen ; Jason S. Chang</p><p>Abstract: In this paper, we propose a web-based bilingual concordancer, DOMCAT , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient 1 for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively. 1</p><p>2 0.1451759 <a title="66-tfidf-2" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>Author: Joseph Z. Chang ; Jason S. Chang ; Roger Jyh-Shing Jang</p><p>Abstract: Jason S. Chang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j s chang@ c s .nthu . edu .tw Jyh-Shing Roger Jang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j ang@ c s .nthu .edu .tw identifying such translation counterparts Web, we can cope with the OOV problem. In this paper, we present a new method on the for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 1</p><p>3 0.14165263 <a title="66-tfidf-3" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>4 0.13551436 <a title="66-tfidf-4" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>5 0.12138501 <a title="66-tfidf-5" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>Author: Jinsong Su ; Hua Wu ; Haifeng Wang ; Yidong Chen ; Xiaodong Shi ; Huailin Dong ; Qun Liu</p><p>Abstract: To adapt a translation model trained from the data in one domain to another, previous works paid more attention to the studies of parallel corpus while ignoring the in-domain monolingual corpora which can be obtained more easily. In this paper, we propose a novel approach for translation model adaptation by utilizing in-domain monolingual topic information instead of the in-domain bilingual corpora, which incorporates the topic information into translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system.</p><p>6 0.12033494 <a title="66-tfidf-6" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>7 0.11897064 <a title="66-tfidf-7" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>8 0.11892465 <a title="66-tfidf-8" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>9 0.11092253 <a title="66-tfidf-9" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>10 0.1072842 <a title="66-tfidf-10" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>11 0.10603198 <a title="66-tfidf-11" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>12 0.1042814 <a title="66-tfidf-12" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>13 0.097384579 <a title="66-tfidf-13" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>14 0.094248824 <a title="66-tfidf-14" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>15 0.08678779 <a title="66-tfidf-15" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>16 0.086479381 <a title="66-tfidf-16" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>17 0.084487349 <a title="66-tfidf-17" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>18 0.083970755 <a title="66-tfidf-18" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>19 0.081769086 <a title="66-tfidf-19" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>20 0.077357672 <a title="66-tfidf-20" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.199), (1, -0.169), (2, 0.103), (3, 0.082), (4, 0.088), (5, 0.003), (6, -0.0), (7, -0.027), (8, -0.029), (9, -0.017), (10, 0.07), (11, 0.017), (12, 0.002), (13, 0.071), (14, 0.019), (15, -0.089), (16, 0.044), (17, 0.057), (18, 0.046), (19, 0.003), (20, -0.01), (21, 0.094), (22, 0.13), (23, 0.037), (24, -0.026), (25, -0.106), (26, 0.073), (27, -0.036), (28, 0.061), (29, -0.041), (30, -0.009), (31, 0.013), (32, 0.015), (33, -0.046), (34, -0.047), (35, 0.142), (36, 0.02), (37, -0.028), (38, 0.007), (39, 0.014), (40, 0.034), (41, -0.091), (42, 0.0), (43, 0.084), (44, 0.058), (45, -0.034), (46, -0.037), (47, -0.016), (48, 0.039), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94600922 <a title="66-lsi-1" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>Author: Ming-Hong Bai ; Yu-Ming Hsieh ; Keh-Jiann Chen ; Jason S. Chang</p><p>Abstract: In this paper, we propose a web-based bilingual concordancer, DOMCAT , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient 1 for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively. 1</p><p>2 0.72877938 <a title="66-lsi-2" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>Author: Joseph Z. Chang ; Jason S. Chang ; Roger Jyh-Shing Jang</p><p>Abstract: Jason S. Chang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j s chang@ c s .nthu . edu .tw Jyh-Shing Roger Jang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j ang@ c s .nthu .edu .tw identifying such translation counterparts Web, we can cope with the OOV problem. In this paper, we present a new method on the for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 1</p><p>3 0.67106801 <a title="66-lsi-3" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>4 0.66996378 <a title="66-lsi-4" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>5 0.6507557 <a title="66-lsi-5" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>6 0.6335513 <a title="66-lsi-6" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>7 0.62927258 <a title="66-lsi-7" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>8 0.62553364 <a title="66-lsi-8" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>9 0.61730957 <a title="66-lsi-9" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>10 0.60506153 <a title="66-lsi-10" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>11 0.59631097 <a title="66-lsi-11" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>12 0.58316743 <a title="66-lsi-12" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>13 0.57245922 <a title="66-lsi-13" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>14 0.55856806 <a title="66-lsi-14" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>15 0.54955828 <a title="66-lsi-15" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>16 0.52571118 <a title="66-lsi-16" href="./acl-2012-Automatically_Mining_Question_Reformulation_Patterns_from_Search_Log_Data.html">35 acl-2012-Automatically Mining Question Reformulation Patterns from Search Log Data</a></p>
<p>17 0.52183431 <a title="66-lsi-17" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>18 0.50794542 <a title="66-lsi-18" href="./acl-2012-CSNIPER_-_Annotation-by-query_for_Non-canonical_Constructions_in_Large_Corpora.html">44 acl-2012-CSNIPER - Annotation-by-query for Non-canonical Constructions in Large Corpora</a></p>
<p>19 0.49749056 <a title="66-lsi-19" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>20 0.49553159 <a title="66-lsi-20" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.338), (25, 0.019), (26, 0.046), (28, 0.063), (30, 0.023), (37, 0.025), (39, 0.028), (57, 0.015), (59, 0.012), (74, 0.019), (84, 0.026), (85, 0.029), (90, 0.128), (92, 0.056), (94, 0.037), (99, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.71691573 <a title="66-lda-1" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>Author: Emily Pitler</p><p>Abstract: Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.</p><p>same-paper 2 0.71371126 <a title="66-lda-2" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>Author: Ming-Hong Bai ; Yu-Ming Hsieh ; Keh-Jiann Chen ; Jason S. Chang</p><p>Abstract: In this paper, we propose a web-based bilingual concordancer, DOMCAT , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient 1 for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively. 1</p><p>3 0.45345095 <a title="66-lda-3" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>4 0.44973436 <a title="66-lda-4" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong ; Min Zhang ; Qun Liu ; Shouxun Lin</p><p>Abstract: Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</p><p>5 0.44964731 <a title="66-lda-5" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>6 0.44611803 <a title="66-lda-6" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>7 0.44524845 <a title="66-lda-7" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>8 0.44500244 <a title="66-lda-8" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>9 0.44480693 <a title="66-lda-9" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>10 0.44439974 <a title="66-lda-10" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>11 0.44432858 <a title="66-lda-11" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>12 0.4435547 <a title="66-lda-12" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>13 0.44236502 <a title="66-lda-13" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>14 0.44228736 <a title="66-lda-14" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.44160619 <a title="66-lda-15" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>16 0.44061053 <a title="66-lda-16" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>17 0.43992496 <a title="66-lda-17" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>18 0.43955147 <a title="66-lda-18" href="./acl-2012-Text-level_Discourse_Parsing_with_Rich_Linguistic_Features.html">193 acl-2012-Text-level Discourse Parsing with Rich Linguistic Features</a></p>
<p>19 0.43949759 <a title="66-lda-19" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>20 0.43944108 <a title="66-lda-20" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
