<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-124" href="#">acl2012-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</h1>
<br/><p>Source: <a title="acl-2012-124-pdf" href="http://aclweb.org/anthology//P/P12/P12-1055.pdf">pdf</a></p><p>Author: Xiaohua Liu ; Ming Zhou ; Xiangyang Zhou ; Zhongyang Fu ; Furu Wei</p><p>Abstract: Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively.</p><p>Reference: <a title="acl-2012-124-reference" href="../acl2012_reference/acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 iu  ,  fuwei  Abstract Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. [sent-3, score-0.185]
</p><p>2 We study the problem of named entity normalization (NEN) for tweets. [sent-4, score-0.364]
</p><p>3 Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. [sent-5, score-0.34]
</p><p>4 We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. [sent-6, score-0.351]
</p><p>5 Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related  words are mentions of the same entity. [sent-7, score-0.15]
</p><p>6 As a result, the task of named entity recognition (NER) for tweets, which aims to identify mentions of rigid designators from tweets belonging to named-entity types such as persons, organizations and locations (2007), has attracted increasing research interest. [sent-14, score-0.717]
</p><p>7 (201 1) develop a system that exploits a CRF model to segment named 1http://www. [sent-16, score-0.108]
</p><p>8 com  entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. [sent-20, score-0.185]
</p><p>9 (201 1) combine a classifier based on the k-nearest neighbors algorithm with a CRFbased model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. [sent-22, score-0.379]
</p><p>10 However, named entity normalization (NEN) for tweets, which transforms named entities mentioned in tweets to their unambiguous canonical forms, has not been well studied. [sent-23, score-0.993]
</p><p>11 Owing to the informal nature of tweets, there are rich variations of named entities in them. [sent-24, score-0.219]
</p><p>12 (201 1), every named entity in tweets has an average of 3. [sent-26, score-0.604]
</p><p>13 We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. [sent-30, score-0.29]
</p><p>14 (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. [sent-32, score-0.222]
</p><p>15 (2008), most NEN errors are caused 2This data set consists of 12,245 randomly sampled tweets within five days. [sent-39, score-0.351]
</p><p>16 Reportedly, the accuracy of a baseline NEN system based on Wikipedia drops considerably from 94% on edited news to 77% on news comments, a kind of user generated content (UGC) with similar style to tweets (Jijkoun et al. [sent-44, score-0.351]
</p><p>17 We propose jointly conducting NER and NEN on multiple tweets using a graphical model, to address these challenges. [sent-46, score-0.399]
</p><p>18 f Suppose tnhaeld sN,E bNu system b,e pliiezvzeas, tchhaitn “burger king” cannot be mapped to “Burger King” since these two tweets are not similar in content. [sent-57, score-0.351]
</p><p>19 Furthermore, considering multiple tweets simultaneously allows us to exploit the redundancy in tweets, as suggested by Liu et al. [sent-61, score-0.351]
</p><p>20 E RReScOoNgn i sz easy owing to its capitalization and the following word “you”, which in turn helps to identify “bobby shaw” in the  second tweet as a PERSON. [sent-67, score-0.218]
</p><p>21 We adopt a factor graph as our graphical model, which is constructed in the following manner. [sent-68, score-0.134]
</p><p>22 We first introduce a random variable for each word in every tweet, which represents the BILOU (Beginning, the Inside and the Last tokens of multi-token entities as well as Unit-length entities) label of the corresponding word. [sent-69, score-0.077]
</p><p>23 Hereafter, we use tm to denote the mth tweet ,tim and ymi to denote the ith word ofoftm and its BILOU label, respectively, and fmi to denote the factor related to ymi−1 and ymi. [sent-71, score-0.63]
</p><p>24 Next, for each word pair with the same lemma, denoted by tim and we introduce a binary random variable, denoted by zimjn, whose value indicates whether tim and tjn belong to two mentions ofthe same entity. [sent-72, score-0.682]
</p><p>25 Figure )1 s hilalruest trhaetes sa an example aofour factor graph for two tweets. [sent-74, score-0.106]
</p><p>26 fmijn,  Figure 1: A factor graph that jointly conducts NER and NEN on multiple tweets. [sent-76, score-0.195]
</p><p>27 We manually add normalization information to the data set shared by Liu et al. [sent-80, score-0.111]
</p><p>28 We introduce the task of NEN for tweets, and propose jointly conducting NER and NEN for multiple tweets using a factor graph, which leverages redundancy in tweets to make up for the dearth of information in a single tweet and allows these two tasks to inform each other. [sent-89, score-1.039]
</p><p>29 (2010) use Amazons Mechanical Turk service 3 and CrowdFlower 4 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. [sent-115, score-0.536]
</p><p>30 (201 1) re-build the NLP pipeline for tweets beginning with POS tagging, through chunking, to NER, which first exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. [sent-117, score-0.668]
</p><p>31 Unlike this work, our work detects the boundary and type of a named entity simultaneously using sequential labeling techniques. [sent-118, score-0.253]
</p><p>32 (201 1) combine a classifier based on the k-nearest neighbors algorithm with a CRF-based model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. [sent-120, score-0.379]
</p><p>33 Our method leverages redundance in similar tweets, using a factor graph rather than a two-stage labeling strategy. [sent-121, score-0.106]
</p><p>34 2  NEN  There is a large body of studies into normalizing various types of entities for formally written texts. [sent-124, score-0.077]
</p><p>35 , entity recognition errors, multiple ways ofreferring to the same entity and ambiguous references, and exploit hand-crafted rules to improve the baseline NEN system. [sent-136, score-0.322]
</p><p>36 We introduce the task of NEN for tweets, a new genre of texts with rich entity variations. [sent-137, score-0.145]
</p><p>37 3  Task Definition  A tweet is a short text message with no more than 140 characters. [sent-139, score-0.17]
</p><p>38 , tweets within some period or related to some query, our task is: 1) To recognize each mention of entities of predefined types for each tweet; and 2) to restore each entity mention into its unambiguous canonical form. [sent-147, score-0.756]
</p><p>39 Given each pair of entity mentions, decide  whether they denote the same entity. [sent-153, score-0.171]
</p><p>40 Once this is achieved, we can link all the mentions of the same entity, and choose a representative mention, e. [sent-154, score-0.081]
</p><p>41 4  Our Method  In contrast to existing work, our method jointly conducts NER and NEN for multiple tweets. [sent-160, score-0.089]
</p><p>42 1 Overview Given a set of tweets as input, our method recognizes predefined types ofnamed entities and for each  entity outputs its unambiguous canonical form. [sent-163, score-0.701]
</p><p>43 To resolve NER, we assign a label to each word in a tweet, indicating both the boundary and entity type. [sent-164, score-0.145]
</p><p>44 To resolve NEN, we assign a binary value label to each pair of words tim which share the same lemma. [sent-167, score-0.187]
</p><p>45 zimjn = 1 or -1, indicating whether tim and belong to two mentions of the same entity 5. [sent-168, score-0.591]
</p><p>46 For example, consider the three tweets presented in Section 3. [sent-169, score-0.351]
</p><p>47 “Gaga11” 6 and “Gaga31” will be assigned a “1” label, since they are part oftwo mentions ofthe same entity “Lady Gaga”; similarly, “Lady21” and  zimjn  and tjn  tjn  “Lady31” are connected with a “1” label. [sent-170, score-0.62]
</p><p>48 With NE type and normalization labels obtained, we judge two mentions, denoted by tim1···ik and 5Stop words have no normalization labels. [sent-172, score-0.262]
</p><p>49 , respectively, refer to the same entity if and only if: 1) The two mentions share the same entity type; 2) tim1···ik is a sub-string of or vise versa; and 3) zimjn = 1, i= i1, · · · , ik and j = j1, · · · ,jl, if exists. [sent-179, score-0.554]
</p><p>50 Still take, ·t·he· ,thiree tweets presented in Section 3 for example. [sent-180, score-0.351]
</p><p>51 Suppose “Gaga11” and “Lady Gaga31” are labeled as PERSON, and there is only one related NE normalization label, which is associated with “‘Gaga11” and “Gaga31” and has 1 as its value. [sent-181, score-0.111]
</p><p>52 We then consider that these two mentions can be normalized into the same entity; in a similar way, we can align “Lady21 Gaaaaga” with “Lady31 Gaga”. [sent-182, score-0.081]
</p><p>53 Combining these pieces informa-  tnj1···jl  tnj1···jl  zimjn  tion together, we can infer that “‘Gaga11”, “Lady21 Gaaaaga” and “Lady31 Gaga” are three mentions of the same entity. [sent-183, score-0.233]
</p><p>54 The central problem with our method is inferring all the NE type (y-serial) and normalization (z-serial) variables. [sent-187, score-0.111]
</p><p>55 To achieve this, we construct a factor graph according to the input tweets, which can evaluate the probability of every possible assignment of y-serials and z-serials, by checking the characteristics of the assignment. [sent-188, score-0.106]
</p><p>56 One advantage of our model is that it allows y-serials and z-serials variables to interact with each other to jointly optimize NER and NEN. [sent-194, score-0.087]
</p><p>57 {fmi}  {fmijn}, fmi = fmi(yim−1,ymi)  fmijn =  7If it still ends up as a draw, we will randomly choose one from the best. [sent-196, score-0.129]
</p><p>58 lnP(Y,Z|G,T)  ∑  ∝  ∑lnfmi(ymi−1,yim)+  δmijn∑m· l,nifmijn(ymi,ynj,zmijn) (1)  m∑,n,i,j  δimjn  tjn  where = 1 if and only if tim and have the same lemma and are not stop words, otherwise zero. [sent-198, score-0.381]
</p><p>59 Training is learnt from annotated tweets T, by maximizing the data likelihood, i. [sent-202, score-0.351]
</p><p>60 Inference Supposing the parameters Θ have been set to Θ∗, the inference problem is: Given a set of testing tweets T, output the most probable assignment of Y and Z, i. [sent-208, score-0.351]
</p><p>61 ,  ynj, zimjn  (Y,Z)∗ = argm(Ya,Zx)lnP(Y,Z|Θ∗,T)  (6)  We adopt the max-product algorithm to solve this inference problem. [sent-210, score-0.18]
</p><p>62 Note that in both the training and testing stage, the factor graph is constructed in the same way as described in Section 1. [sent-212, score-0.106]
</p><p>63 Firstly, we manually compile a comprehensive named entity dictionary from various sources including Wikipedia, Freebase 8, news articles and the gazetteers shared by Ratinov and Roth (2009). [sent-214, score-0.286]
</p><p>64 Secondly, in the testing phase, we introduce three rules related to 1) = 1, which says two words sharing the same lemma in the same tweet denote the same entity; 2) set to 1, if the similarity between tm and tn is above a threshold (0. [sent-222, score-0.244]
</p><p>65 8 in our work), or tm and tn share one hash tag; and 3)zmnij = −1, if the similarity between tm and tn is biejlo w= a th1r,es ifh tohlde (0. [sent-223, score-0.089]
</p><p>66 Teeon compute  zimjn:  zimjm  zimjn  8http://freebase. [sent-225, score-0.152]
</p><p>67 531 the similarity, each tweet is represented as a bag-ofwords vector with the stop words removed, and the cosine similarity is adopted, as defined in Formula 7. [sent-228, score-0.2]
</p><p>68 ture in concerns a pair of distant NE-type ltaurbeel isn {aϕnd it}s associated normalization label, i. [sent-239, score-0.111]
</p><p>69 them with ymi−1 and ymi constitutes Orthographic featuresc: Wnshtiettuhteesr {timϕ is capitalized or upper case; whether it is alphanumeric or contains any slashes; wether it is a stop word; word prefixes and suffixes. [sent-249, score-0.389]
</p><p>70 Lexical features: Lemma of tim, tim−1 and tim+1, respectively; whether tim is an out-of-vocabulary  (OOV) word 11; POS of tim, tim−1 and tim+1, respectively; whether tim is a hash tag, a link, or a user account. [sent-250, score-0.453]
</p><p>71 2  Feature Set Two:  {ϕ(k2)}kK=21  Similarly, we define orthographic, lexical features and gazetteer-related features on the observation, ymi 11We first conduct a simple dictionary-lookup based normalization with the incorrect/correct word pair list provided by Han et al. [sent-256, score-0.416]
</p><p>72 and and then we combine these features with forming Orthographic features: Whether tim / is capitalized or upper case; whether tim / tjn is alphanumeric or contains any slashes; prefixes and suffixes of tim. [sent-259, score-0.549]
</p><p>73 Lexical features: Lemma of tim; whether tim is  zimjn,  ynj;  {ϕ(k2)}kK=21. [sent-260, score-0.213]
</p><p>74 tjn  tjn tjn+1 tjn−1  OOV; whether tim / tim+1 / tim−1 and / / have the same POS; whether ymi and ynj have the same label/entity type. [sent-261, score-0.911]
</p><p>75 We show that our method outperforms the baseline, a cascaded system that conducts NER and NEN individually. [sent-264, score-0.091]
</p><p>76 (201 1), which consists of 12,245 tweets with four types of entities annotated: PERSON, LOCATION, ORGANIZATION and PRODUCT. [sent-267, score-0.428]
</p><p>77 We enrich this data set by adding entity normalization information. [sent-268, score-0.256]
</p><p>78 For any entity mention, two annotators independently annotate its canonical form. [sent-270, score-0.233]
</p><p>79 2, 245 tweets are used for development, and the remainder  are used for 5-fold cross validation. [sent-274, score-0.351]
</p><p>80 3  Baseline  We develop a cascaded system as the baseline, which conducts NER and NEN sequentially. [sent-286, score-0.091]
</p><p>81 As a case study, we show that our system successfully identified “jaxon11” as a PERSON in the tweet “· · · come to see jaxon11 someday· · · ”, which tiws mistakenly leab toel seede as a LOsoCmAeTdIaOyN·· by SBR. [sent-297, score-0.194]
</p><p>82 This is largely owing to the fact that our system aligns “jaxon11” with “Jaxson21” in the tweet “· · ·I loveJaxson21,Heslikemylittleb”ro itnhe thr·e  · ·  ”, eientw “·hi·c·hI  “Jaxson21” is Hideesnltiikfieedm as a PeEbRroSthOeNr·. [sent-298, score-0.218]
</p><p>83 Table 3 reports the NER performance of our method for each entity type, from which we see that our system consistently yields better F1 on all entity types than SBR. [sent-302, score-0.29]
</p><p>84 8 Table 3: F1 (%) of NER on different entity types. [sent-322, score-0.145]
</p><p>85 For instance, our method recognizes “California11” in the tweet “· · · And Now, He Lives All The Way In tChaeli tfwoerneita11 “·· ····”A as a LOCATION, however, Wit myi Isntakenly ide·n·t·if”ie as “ aC LalOi21C” iTn OthNe tw hoewete “e·r ·,· iit mloviseCali so much· · · ” as a PE”R iSnO tNh. [sent-336, score-0.205]
</p><p>86 A more complicated case is “BS11” in the tweet “· · · I, bobby shaw, am gonna put BS11 on 533 everything· · · ”, in which “BS11” is the abbreviation eofv “bobby s·h·”aw,” in. [sent-339, score-0.253]
</p><p>87 There are two possible ways to fix these errors: 1) Extending the scope of z-serial variables to each word pairs with a common prefix; and 2) developing advanced normalization components to restore such slang expressions and informal abbreviations into their canonical forms. [sent-341, score-0.283]
</p><p>88 This explains the cases where our system correctly links multiple entity mentions but fails to generate canonical forms. [sent-343, score-0.29]
</p><p>89 Take the following two tweets for example: “· · · nitip link win711 sp1 · · · ” tawnde t“s· · · Hrit e xtahme p3leTB: “w··al·ln on S liRnTk installing 1f·r·es·h” Wandin7 “1·2·· · · H”. [sent-344, score-0.351]
</p><p>90 i hOeu r3 system recognizes s“twalilnin7g11” f aensdh “Win7·21”· as two mentions of the same product, but cannot output their canonical forms “Windows 7”. [sent-345, score-0.18]
</p><p>91 One possible solution is to exploit Wikipedia to compile a dictionary consisting of entities and their variations. [sent-346, score-0.11]
</p><p>92 Two challenges of this task are the dearth of information in a single tweet and errors propagated from the NER component. [sent-348, score-0.225]
</p><p>93 We propose jointly conducting NER and NEN for multiple tweets using a factor graph, to address these challenges. [sent-349, score-0.463]
</p><p>94 One unique characteristic of our model is that a NE normalization variable is introduced to indicate whether a word pair belongs to the mentions of the same entity. [sent-350, score-0.218]
</p><p>95 First, we are going to develop advanced tweet normalization technologies to resolve slang expressions and informal abbreviations. [sent-354, score-0.351]
</p><p>96 Unsupervised gene/protein named entity normalization using automatically extracted dictionaries. [sent-363, score-0.364]
</p><p>97 The impact of named entity normalization on information retrieval for question answering. [sent-416, score-0.364]
</p><p>98 Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. [sent-439, score-0.285]
</p><p>99 Extracting personal names from email: applying named entity recognition to informal text. [sent-445, score-0.346]
</p><p>100 Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition. [sent-474, score-0.253]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nen', 0.531), ('tweets', 0.351), ('ymi', 0.305), ('ner', 0.242), ('tim', 0.187), ('gaga', 0.18), ('tweet', 0.17), ('zimjn', 0.152), ('entity', 0.145), ('lady', 0.138), ('ynj', 0.125), ('tjn', 0.121), ('normalization', 0.111), ('named', 0.108), ('jijkoun', 0.084), ('bobby', 0.083), ('yim', 0.083), ('mentions', 0.081), ('kk', 0.079), ('entities', 0.077), ('fmijn', 0.069), ('person', 0.066), ('conducts', 0.066), ('canonical', 0.064), ('factor', 0.064), ('bilou', 0.06), ('fmi', 0.06), ('burger', 0.055), ('dearth', 0.055), ('gaaaaga', 0.055), ('sbr', 0.055), ('owing', 0.048), ('mention', 0.045), ('ratinov', 0.044), ('lemma', 0.043), ('graph', 0.042), ('sbn', 0.042), ('withouto', 0.042), ('khalid', 0.041), ('wikipedia', 0.041), ('orthographic', 0.04), ('denoted', 0.04), ('ritter', 0.039), ('king', 0.039), ('variables', 0.038), ('ne', 0.037), ('lnp', 0.036), ('slang', 0.036), ('recognizes', 0.035), ('informal', 0.034), ('dictionary', 0.033), ('maarten', 0.033), ('recognition', 0.032), ('tm', 0.031), ('downey', 0.031), ('ik', 0.031), ('organization', 0.031), ('stop', 0.03), ('loopy', 0.029), ('unambiguous', 0.029), ('liu', 0.028), ('adopt', 0.028), ('alphanumeric', 0.028), ('anneke', 0.028), ('gagal', 0.028), ('goldman', 0.028), ('imjn', 0.028), ('iphone', 0.028), ('iphoneu', 0.028), ('jl', 0.028), ('labeledlda', 0.028), ('ladyb', 0.028), ('lnfmi', 0.028), ('magdy', 0.028), ('mahboob', 0.028), ('mijn', 0.028), ('mycraftingworld', 0.028), ('sang', 0.028), ('slashes', 0.028), ('names', 0.027), ('hash', 0.027), ('whether', 0.026), ('dai', 0.026), ('interact', 0.026), ('cascaded', 0.025), ('conducting', 0.025), ('murphy', 0.024), ('sachs', 0.024), ('finin', 0.024), ('mistakenly', 0.024), ('chiticariu', 0.024), ('jansche', 0.024), ('krupka', 0.024), ('shanghai', 0.024), ('yoshida', 0.024), ('zhongyang', 0.024), ('beginning', 0.024), ('annotators', 0.024), ('belief', 0.024), ('jointly', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="124-tfidf-1" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>Author: Xiaohua Liu ; Ming Zhou ; Xiangyang Zhou ; Zhongyang Fu ; Furu Wei</p><p>Abstract: Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively.</p><p>2 0.34889501 <a title="124-tfidf-2" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>Author: Xiaohua Liu ; Furu Wei ; Ming Zhou ; QuickView Team Microsoft</p><p>Abstract: Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.</p><p>3 0.2986708 <a title="124-tfidf-3" href="./acl-2012-Tweet_Recommendation_with_Graph_Co-Ranking.html">205 acl-2012-Tweet Recommendation with Graph Co-Ranking</a></p>
<p>Author: Rui Yan ; Mirella Lapata ; Xiaoming Li</p><p>Abstract: Mirella Lapata‡ Xiaoming Li†, \ ‡Institute for Language, \State Key Laboratory of Software Cognition and Computation, Development Environment, University of Edinburgh, Beihang University, Edinburgh EH8 9AB, UK Beijing 100083, China mlap@ inf .ed .ac .uk lxm@pku .edu .cn 2012.1 Twitter enables users to send and read textbased posts ofup to 140 characters, known as tweets. As one of the most popular micro-blogging services, Twitter attracts millions of users, producing millions of tweets daily. Shared information through this service spreads faster than would have been possible with traditional sources, however the proliferation of user-generation content poses challenges to browsing and finding valuable information. In this paper we propose a graph-theoretic model for tweet recommendation that presents users with items they may have an interest in. Our model ranks tweets and their authors simultaneously using several networks: the social network connecting the users, the network connecting the tweets, and a third network that ties the two together. Tweet and author entities are ranked following a co-ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweets and their authors that could be reflected in the rankings. We show that this framework can be parametrized to take into account user preferences, the popularity of tweets and their authors, and diversity. Experimental evaluation on a large dataset shows that our model out- performs competitive approaches by a large margin.</p><p>4 0.18965234 <a title="124-tfidf-4" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>Author: Hao Wang ; Dogan Can ; Abe Kazemzadeh ; Francois Bar ; Shrikanth Narayanan</p><p>Abstract: This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a microblogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion. 1</p><p>5 0.14252979 <a title="124-tfidf-5" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>Author: Jennifer Williams ; Graham Katz</p><p>Abstract: We seek to automatically estimate typical durations for events and habits described in Twitter tweets. A corpus of more than 14 million tweets containing temporal duration information was collected. These tweets were classified as to their habituality status using a bootstrapped, decision tree. For each verb lemma, associated duration information was collected for episodic and habitual uses of the verb. Summary statistics for 483 verb lemmas and their typical habit and episode durations has been compiled and made available. This automatically generated duration information is broadly comparable to hand-annotation. 1</p><p>6 0.11099153 <a title="124-tfidf-6" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>7 0.10589394 <a title="124-tfidf-7" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>8 0.10203619 <a title="124-tfidf-8" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>9 0.094732068 <a title="124-tfidf-9" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>10 0.093085766 <a title="124-tfidf-10" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>11 0.08461725 <a title="124-tfidf-11" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>12 0.082080916 <a title="124-tfidf-12" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>13 0.079540633 <a title="124-tfidf-13" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>14 0.070606701 <a title="124-tfidf-14" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>15 0.062102199 <a title="124-tfidf-15" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>16 0.054924484 <a title="124-tfidf-16" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>17 0.054815792 <a title="124-tfidf-17" href="./acl-2012-Personalized_Normalization_for_a_Multilingual_Chat_System.html">160 acl-2012-Personalized Normalization for a Multilingual Chat System</a></p>
<p>18 0.053990096 <a title="124-tfidf-18" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<p>19 0.052771091 <a title="124-tfidf-19" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>20 0.051497586 <a title="124-tfidf-20" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.161), (1, 0.155), (2, 0.018), (3, 0.12), (4, 0.116), (5, -0.027), (6, 0.375), (7, 0.096), (8, 0.195), (9, 0.158), (10, 0.129), (11, -0.135), (12, 0.01), (13, 0.004), (14, 0.137), (15, 0.073), (16, -0.013), (17, -0.042), (18, -0.018), (19, -0.013), (20, 0.011), (21, -0.038), (22, -0.011), (23, 0.029), (24, 0.002), (25, -0.023), (26, -0.022), (27, -0.038), (28, -0.041), (29, 0.052), (30, -0.025), (31, 0.066), (32, -0.023), (33, -0.023), (34, -0.069), (35, -0.01), (36, -0.004), (37, 0.067), (38, 0.029), (39, -0.025), (40, 0.05), (41, 0.063), (42, -0.046), (43, 0.018), (44, -0.077), (45, 0.017), (46, -0.001), (47, 0.036), (48, 0.057), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94085461 <a title="124-lsi-1" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>Author: Xiaohua Liu ; Ming Zhou ; Xiangyang Zhou ; Zhongyang Fu ; Furu Wei</p><p>Abstract: Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively.</p><p>2 0.88687289 <a title="124-lsi-2" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>Author: Xiaohua Liu ; Furu Wei ; Ming Zhou ; QuickView Team Microsoft</p><p>Abstract: Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.</p><p>3 0.83328021 <a title="124-lsi-3" href="./acl-2012-Tweet_Recommendation_with_Graph_Co-Ranking.html">205 acl-2012-Tweet Recommendation with Graph Co-Ranking</a></p>
<p>Author: Rui Yan ; Mirella Lapata ; Xiaoming Li</p><p>Abstract: Mirella Lapata‡ Xiaoming Li†, \ ‡Institute for Language, \State Key Laboratory of Software Cognition and Computation, Development Environment, University of Edinburgh, Beihang University, Edinburgh EH8 9AB, UK Beijing 100083, China mlap@ inf .ed .ac .uk lxm@pku .edu .cn 2012.1 Twitter enables users to send and read textbased posts ofup to 140 characters, known as tweets. As one of the most popular micro-blogging services, Twitter attracts millions of users, producing millions of tweets daily. Shared information through this service spreads faster than would have been possible with traditional sources, however the proliferation of user-generation content poses challenges to browsing and finding valuable information. In this paper we propose a graph-theoretic model for tweet recommendation that presents users with items they may have an interest in. Our model ranks tweets and their authors simultaneously using several networks: the social network connecting the users, the network connecting the tweets, and a third network that ties the two together. Tweet and author entities are ranked following a co-ranking algorithm based on the intuition that that there is a mutually reinforcing relationship between tweets and their authors that could be reflected in the rankings. We show that this framework can be parametrized to take into account user preferences, the popularity of tweets and their authors, and diversity. Experimental evaluation on a large dataset shows that our model out- performs competitive approaches by a large margin.</p><p>4 0.57787848 <a title="124-lsi-4" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>Author: Jennifer Williams ; Graham Katz</p><p>Abstract: We seek to automatically estimate typical durations for events and habits described in Twitter tweets. A corpus of more than 14 million tweets containing temporal duration information was collected. These tweets were classified as to their habituality status using a bootstrapped, decision tree. For each verb lemma, associated duration information was collected for episodic and habitual uses of the verb. Summary statistics for 483 verb lemmas and their typical habit and episode durations has been compiled and made available. This automatically generated duration information is broadly comparable to hand-annotation. 1</p><p>5 0.57472354 <a title="124-lsi-5" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>Author: Hao Wang ; Dogan Can ; Abe Kazemzadeh ; Francois Bar ; Shrikanth Narayanan</p><p>Abstract: This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a microblogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion. 1</p><p>6 0.44248345 <a title="124-lsi-6" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>7 0.37881181 <a title="124-lsi-7" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>8 0.3746424 <a title="124-lsi-8" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>9 0.37181425 <a title="124-lsi-9" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>10 0.30603614 <a title="124-lsi-10" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>11 0.29790434 <a title="124-lsi-11" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>12 0.29649961 <a title="124-lsi-12" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>13 0.29294169 <a title="124-lsi-13" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<p>14 0.28647104 <a title="124-lsi-14" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>15 0.28262538 <a title="124-lsi-15" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>16 0.27256238 <a title="124-lsi-16" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<p>17 0.25919259 <a title="124-lsi-17" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>18 0.23461613 <a title="124-lsi-18" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>19 0.22794165 <a title="124-lsi-19" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>20 0.22437148 <a title="124-lsi-20" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.02), (26, 0.065), (28, 0.029), (30, 0.02), (37, 0.023), (39, 0.049), (59, 0.017), (74, 0.033), (82, 0.019), (84, 0.022), (85, 0.014), (89, 0.274), (90, 0.129), (92, 0.089), (94, 0.026), (98, 0.022), (99, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75593579 <a title="124-lda-1" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>Author: Xiaohua Liu ; Ming Zhou ; Xiangyang Zhou ; Zhongyang Fu ; Furu Wei</p><p>Abstract: Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively.</p><p>2 0.67588371 <a title="124-lda-2" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>3 0.66691697 <a title="124-lda-3" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>Author: Andrea Gesmundo ; Tanja Samardzic</p><p>Abstract: We present a novel approach to the task of word lemmatisation. We formalise lemmatisation as a category tagging task, by describing how a word-to-lemma transformation rule can be encoded in a single label and how a set of such labels can be inferred for a specific language. In this way, a lemmatisation system can be trained and tested using any supervised tagging model. In contrast to previous approaches, the proposed technique allows us to easily integrate relevant contextual information. We test our approach on eight languages reaching a new state-of-the-art level for the lemmatisation task.</p><p>4 0.57009214 <a title="124-lda-4" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>Author: Xiaohua Liu ; Furu Wei ; Ming Zhou ; QuickView Team Microsoft</p><p>Abstract: Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.</p><p>5 0.55799603 <a title="124-lda-5" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart Shieber</p><p>Abstract: We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data.</p><p>6 0.55283916 <a title="124-lda-6" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>7 0.55008715 <a title="124-lda-7" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>8 0.54470307 <a title="124-lda-8" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>9 0.54400241 <a title="124-lda-9" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>10 0.54390585 <a title="124-lda-10" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>11 0.54381508 <a title="124-lda-11" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>12 0.54381418 <a title="124-lda-12" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>13 0.54335713 <a title="124-lda-13" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>14 0.54191369 <a title="124-lda-14" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>15 0.54054648 <a title="124-lda-15" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>16 0.54037213 <a title="124-lda-16" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>17 0.53968614 <a title="124-lda-17" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>18 0.5383352 <a title="124-lda-18" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>19 0.53551644 <a title="124-lda-19" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>20 0.5348832 <a title="124-lda-20" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
