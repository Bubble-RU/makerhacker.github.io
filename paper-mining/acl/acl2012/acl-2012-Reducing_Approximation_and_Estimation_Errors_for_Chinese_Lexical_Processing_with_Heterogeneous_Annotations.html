<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-168" href="#">acl2012-168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</h1>
<br/><p>Source: <a title="acl-2012-168-pdf" href="http://aclweb.org/anthology//P/P12/P12-1025.pdf">pdf</a></p><p>Author: Weiwei Sun ; Xiaojun Wan</p><p>Abstract: We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</p><p>Reference: <a title="acl-2012-168-reference" href="../acl2012_reference/acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. [sent-3, score-0.717]
</p><p>2 The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. [sent-7, score-0.849]
</p><p>3 Nowadays, for many tasks, multiple heterogeneous annotated corpora have been built and publicly available. [sent-10, score-0.491]
</p><p>4 The co-existence of heterogeneous annotation data therefore presents a new challenge to the consumers of such resources. [sent-16, score-0.553]
</p><p>5 There are two essential characteristics of heterogeneous annotations that can be utilized to reduce two main types of errors in statistical NLP, i. [sent-17, score-0.588]
</p><p>6 the approximation error that is due to the intrinsic suboptimality of a model and the estimation error that is due to having only finite training data. [sent-19, score-0.322]
</p><p>7 First, heterogeneous annotations are (similar but) different as a result of different annotation schemata. [sent-20, score-0.639]
</p><p>8 Systems respectively trained on heterogeneous annotation data can produce different but relevant linguistic analysis. [sent-21, score-0.607]
</p><p>9 This suggests that complementary features from heterogeneous analysis can be derived for disambiguation, and therefore the approximation error can be reduced. [sent-22, score-0.627]
</p><p>10 Second, heterogeneous annotations are (different but) similar because their linguistic analysis is highly correlated. [sent-23, score-0.566]
</p><p>11 This implies that appropriate conversions between heterogeneous corpora could be reasonably accurate, and therefore the estimation error can be reduced by reason of the increase of reliable training data. [sent-24, score-0.617]
</p><p>12 This paper explores heterogeneous annotations  to reduce both approximation and estimation errors for Chinese word segmentation and part-of-speech (POS) tagging, which are fundamental steps for more advanced Chinese language processing tasks. [sent-25, score-0.899]
</p><p>13 We empirically analyze the diversity between two representative popular heterogeneous corpora, i. [sent-26, score-0.561]
</p><p>14 1 Our analysis confirms the aforementioned two properties of heterogeneous annotations. [sent-32, score-0.479]
</p><p>15 Inspired by the sub-word tagging method introduced in (Sun, 2011), we propose a structure-based stacking model to fully utilize heterogeneous word structures to reduce the approximation error. [sent-33, score-1.168]
</p><p>16 In particular, joint word segmentation and POS tagging is addressed as a two step process. [sent-34, score-0.368]
</p><p>17 First, character-based taggers are respectively trained on heterogeneous annotations to produce multiple analysis. [sent-35, score-0.603]
</p><p>18 The sub-word tagger is de-  signed to refine the tagging result with the help of heterogeneous annotations. [sent-37, score-0.746]
</p><p>19 To reduce the estimation error, we employ a learning-based approach to convert complementary heterogeneous data to increase labeled training data for the target task. [sent-38, score-0.703]
</p><p>20 Both the character-based tagger and the sub-word tagger can be refined by re-training with automatically converted data. [sent-39, score-0.289]
</p><p>21 Our structure-based stacking model achieves an f-score of 94. [sent-41, score-0.395]
</p><p>22 36, which is superior to a feature-based stacking model introduced in (Jiang et al. [sent-42, score-0.371]
</p><p>23 words, word segmentation and POS tagging are important initial steps for Chinese language processing. [sent-57, score-0.321]
</p><p>24 Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al. [sent-60, score-0.365]
</p><p>25 Two kinds of approaches are popular for joint word segmentation and POS tagging. [sent-62, score-0.238]
</p><p>26 The second kind of solution is the “word-based” method, also known as semi-Markov tagging (Zhang and Clark, 2008; Zhang and Clark, 2010), where the basic predicting units are words themselves. [sent-68, score-0.184]
</p><p>27 In addition, we proposed an effective and efficient stacked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). [sent-71, score-0.237]
</p><p>28 First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. [sent-72, score-0.316]
</p><p>29 Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. [sent-74, score-0.56]
</p><p>30 Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. [sent-75, score-0.316]
</p><p>31 In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. [sent-82, score-0.825]
</p><p>32 Therefore, it is not only interesting but also important to study how to fully utilize heterogeneous resources to improve Chinese lexical processing. [sent-92, score-0.453]
</p><p>33 There are two main types of errors in statistical NLP: (1) the approximation error that is due to the intrinsic suboptimality of a model and (2) the estimation error that is due to having only finite training data. [sent-93, score-0.322]
</p><p>34 The two essential characteristics about systematic diversity of heterogeneous annota234 tions can be utilized to reduce both approximation and estimation errors. [sent-97, score-0.699]
</p><p>35 Specially, we employ a PPD-style segmentation and tagging system to automatically label these 200 sentences. [sent-101, score-0.342]
</p><p>36 The statistics indicates that the two heterogenous segmented corpora are systematically different, and confirms the aforementioned two properties of heterogeneous annotations. [sent-116, score-0.577]
</p><p>37 From this table, we can see that (1) there is no one-to-one mapping between their heterogeneous word classification but (2) the mapping between heterogeneous tags is not very uncertain. [sent-120, score-0.971]
</p><p>38 that the two POS tagged corpora also hold the two properties of heterogeneous annotations. [sent-133, score-0.491]
</p><p>39 With different training data, we can construct multiple heterogeneous systems. [sent-146, score-0.453]
</p><p>40 A very simple idea to take advantage of heterogeneous structures is to design a predictor which can predict a more accurate target structure based  on the input, the less accurate target structure and complementary structures. [sent-148, score-0.529]
</p><p>41 The  only difference between model ensemble and annotation ensemble is that the output spaces of model ensemble are the same while the output spaces of annotation ensemble are different. [sent-166, score-0.468]
</p><p>42 2 A Character-based Tagger With IOB2 representation (Ramshaw and Marcus, 1995), the problem of joint segmentation and tagging can be regarded as a character classification task. [sent-169, score-0.46]
</p><p>43 Both of our feature- and structure-based stacking models employ base character-based taggers to generate multiple segmentation and tagging results. [sent-171, score-0.723]
</p><p>44 Each character can be assigned one of two possible boundary tags: “B” for a character that begins a word and “I” for a character that occurs in the middle of a word. [sent-173, score-0.451]
</p><p>45 (2009) introduced a feature-based stacking solution for annotation ensemble. [sent-178, score-0.498]
</p><p>46 In their solution, an auxiliary tagger CTagppd is trained on a complementary corpus, i. [sent-179, score-0.213]
</p><p>47 To refine the character-based tagger CTagctb, PPD-style character labels are directly incorporated as new features. [sent-182, score-0.26]
</p><p>48 The stacking model relies on the ability of discriminative learning method to explore informative features, which play central role to boost the tagging performance. [sent-183, score-0.528]
</p><p>49 To compare their feature-based stacking model and our structure-based model, we implement a similar system CTagppd→ctb. [sent-184, score-0.371]
</p><p>50 4 Structure-based Stacking We propose a novel structured-based stacking model for the task, in which heterogeneous word structures are used not only to generate features but also to derive a sub-word structure. [sent-187, score-0.887]
</p><p>51 Our work is inspired by the stacked sub-word tagging model introduced in (Sun, 2011). [sent-188, score-0.237]
</p><p>52 Their work is motivated by the diversity of heterogeneous models, while our work is motivated by the diversity of heterogeneous annotations. [sent-189, score-1.006]
</p><p>53 In the second phase, this system first combines the two segmentation and tagging results to get sub-words which maximize the agreement about word boundaries. [sent-193, score-0.321]
</p><p>54 This strategy makes sure that it is still possible to correctly re-segment the strings of which the boundaries are disagreed with by the heterogeneous segmenters in the sub-word tagging stage. [sent-200, score-0.684]
</p><p>55 To construct training data for the new heterogeneous subword tagger, a 10-fold cross-validation on the original CTB data is performed too. [sent-215, score-0.453]
</p><p>56 5  Data-driven Annotation Conversion  It is possible to acquire high quality labeled data for a specific annotation standard by exploring existing heterogeneous corpora, since the annotations are normally highly compatible. [sent-216, score-0.697]
</p><p>57 Moreover, the exploitation of additional (pseudo) labeled data aims to reduce the estimation error and enhances a NLP system in a different way from stacking. [sent-217, score-0.203]
</p><p>58 The stacking models can be viewed as annotation converters: They take as input complementary structures and produce as output target structures. [sent-219, score-0.574]
</p><p>59 In other words, the stacking models actually learn statistical models to transform the lexical representations. [sent-220, score-0.371]
</p><p>60 We can acquire informative extra samples  by processing the PPD data with our stacking models. [sent-221, score-0.401]
</p><p>61 Though the converted annotations are imperfect, they are still helpful to reduce the estimation error. [sent-222, score-0.272]
</p><p>62 237 Character-based Conversion The feature-based stacking model CTagppd→ctb maps the input character sequence c and its PPD-style character label sequence to the corresponding CTB-style character label sequence. [sent-223, score-0.797]
</p><p>63 Sub-word-based Conversion Similarly, the structure-based stacking model can be also taken as a corpus conversion model. [sent-227, score-0.436]
</p><p>64 1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. [sent-240, score-0.368]
</p><p>65 In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. [sent-249, score-0.2]
</p><p>66 A token is considered to be correct if its boundaries match the boundaries of a word in the gold standard and their POS tags are  identical. [sent-257, score-0.192]
</p><p>67 2 Results of Stacking Table 2 summarizes the segmentation and tagging performance of the baseline and different stacking models. [sent-259, score-0.66]
</p><p>68 By using the character labels from a heterogeneous solver (CTagppd), which is trained on the PPD data set, the performance of this character-based system (CTagppd→ctb) is improved to 93. [sent-262, score-0.614]
</p><p>69 This result confirms the importance of a heterogeneous structure. [sent-264, score-0.479]
</p><p>70 Our structure-based stacking solution is effective and outperforms the featurebased stacking. [sent-265, score-0.398]
</p><p>71 By better exploiting the heterogeneous word boundary structures, our sub-word tagging model achieves an f-score of 94. [sent-266, score-0.713]
</p><p>72 On one hand, the heterogeneous solver provides structural information, which is the basis to construct the sub-word sequence. [sent-269, score-0.49]
</p><p>73 60973 Table 2: Performance of different stacking models on the development data. [sent-280, score-0.371]
</p><p>74 uate these two contributions, we do another experiment byjust using the heterogeneous word boundary structures without the POS information. [sent-281, score-0.563]
</p><p>75 3 Learning Curves We do additional experiments to evaluate the effect of heterogeneous features as the amount of PPD data is varied. [sent-286, score-0.453]
</p><p>76 The feature-based model works well only when a considerable amount of heterogeneous data is available. [sent-288, score-0.453]
</p><p>77 The structurebased stacking model is more robust and obtains consistent gains regardless of the size of the complementary data. [sent-291, score-0.416]
</p><p>78 4  Results of Annotation Conversion  The stacking models can be viewed as data-driven annotation converting models. [sent-307, score-0.471]
</p><p>79 To make clear whether these stacking models trained with noisy inputs can tolerate perfect inputs, we evaluate the two stacking models on our manually converted data. [sent-310, score-0.807]
</p><p>80 The accuracies presented in Table 4 indicate that though the conversion models are learned by applying noisy data, they can refine target tagging with gold auxiliary tagging. [sent-311, score-0.335]
</p><p>81 Another interesting thing is that the gold PPD-style analysis does not help the sub-word tagging model as much as the character tagging model. [sent-312, score-0.471]
</p><p>82 P7109PD  Table 4: F-scores with gold PPD-style tagging on the manually converted data. [sent-315, score-0.255]
</p><p>83 Note that a sub-word tagger is built on character taggers, so when we re-train a sub-word system, we should  consider whether or not re-training base character taggers. [sent-318, score-0.36]
</p><p>84 The error rates decrease as automatically converted data is added to the training pool, especially for the character-based tagger CTagctb. [sent-319, score-0.231]
</p><p>85 When the base CTB-style tagging is improved, the final tagging is improved in the end. [sent-320, score-0.314]
</p><p>86 By better using the heterogeneous word boundary structures, our sub-word tagging model achieves an f-score of 94. [sent-330, score-0.713]
</p><p>87 Both character and  sub-word tagging model can be enhanced with automatically converted corpus. [sent-332, score-0.346]
</p><p>88 7  Conclusion  Our theoretical and empirical analysis of two representative popular corpora highlights two essential  characteristics of heterogeneous annotations which are explored to reduce approximation and estimation errors for Chinese word segmentation and POS tagging. [sent-346, score-0.995]
</p><p>89 We employ stacking models to incorporate features derived from heterogeneous analysis and apply them to convert heterogeneous labeled data for re-training. [sent-347, score-1.361]
</p><p>90 The appropriate application of heterogeneous annotations leads to a significant improvement (a relative error reduction of 11%) over the best performance for this task. [sent-348, score-0.593]
</p><p>91 Although our discussion is for a specific task, the key idea to leverage heterogeneous annotations to reduce the approximation error with stacking models and the estimation error with automatically converted corpora is very general and applicable to other NLP tasks. [sent-349, score-1.317]
</p><p>92 Automatic annotation of the penn treebank with lfg f-structure information. [sent-356, score-0.212]
</p><p>93 A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. [sent-364, score-0.211]
</p><p>94 Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging a case study. [sent-374, score-0.522]
</p><p>95 An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging. [sent-379, score-0.312]
</p><p>96 Discriminative parse reranking for Chinese with homogeneous and heterogeneous annotations. [sent-419, score-0.493]
</p><p>97 A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. [sent-428, score-0.291]
</p><p>98 Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. [sent-440, score-0.531]
</p><p>99 Joint word segmentation and POS tagging using a single perceptron. [sent-455, score-0.321]
</p><p>100 A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. [sent-460, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('heterogeneous', 0.453), ('ppd', 0.411), ('stacking', 0.371), ('ctb', 0.245), ('ctagppd', 0.161), ('tagging', 0.157), ('segmentation', 0.132), ('ctagctb', 0.125), ('tctb', 0.125), ('character', 0.124), ('tagger', 0.112), ('chinese', 0.109), ('tppd', 0.107), ('sk', 0.102), ('pos', 0.101), ('annotation', 0.1), ('sun', 0.09), ('annotations', 0.086), ('jiang', 0.085), ('stacked', 0.08), ('approximation', 0.075), ('estimation', 0.072), ('lppd', 0.071), ('ltppd', 0.071), ('ensemble', 0.067), ('pku', 0.066), ('converted', 0.065), ('conversion', 0.065), ('lc', 0.063), ('auxiliary', 0.056), ('error', 0.054), ('lptpd', 0.054), ('ltctb', 0.054), ('stagctb', 0.054), ('weiwei', 0.053), ('diversity', 0.05), ('standards', 0.05), ('reduce', 0.049), ('joint', 0.047), ('boundary', 0.047), ('boundaries', 0.047), ('penn', 0.046), ('complementary', 0.045), ('si', 0.045), ('wenbin', 0.04), ('saarland', 0.04), ('homogeneous', 0.04), ('corpora', 0.038), ('taggers', 0.037), ('solver', 0.037), ('dppd', 0.036), ('suboptimality', 0.036), ('torres', 0.036), ('treebank', 0.035), ('gold', 0.033), ('tags', 0.033), ('segmented', 0.032), ('word', 0.032), ('converters', 0.031), ('lfg', 0.031), ('representative', 0.031), ('structures', 0.031), ('intrinsic', 0.031), ('fk', 0.031), ('convert', 0.03), ('daily', 0.03), ('acquire', 0.03), ('nlp', 0.03), ('heterogenous', 0.028), ('dfki', 0.028), ('finished', 0.028), ('vn', 0.028), ('hp', 0.028), ('labeled', 0.028), ('linguistic', 0.027), ('ohio', 0.027), ('suntec', 0.027), ('solution', 0.027), ('popular', 0.027), ('zhang', 0.027), ('produce', 0.027), ('specially', 0.027), ('positional', 0.027), ('lavergne', 0.027), ('segmenters', 0.027), ('label', 0.027), ('pages', 0.026), ('clark', 0.026), ('confirms', 0.026), ('employ', 0.026), ('columbus', 0.025), ('yiou', 0.025), ('martins', 0.025), ('kruengkrai', 0.025), ('ichi', 0.025), ('qun', 0.025), ('merged', 0.024), ('enhance', 0.024), ('refine', 0.024), ('achieves', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="168-tfidf-1" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>Author: Weiwei Sun ; Xiaojun Wan</p><p>Abstract: We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</p><p>2 0.18552411 <a title="168-tfidf-2" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>3 0.17284301 <a title="168-tfidf-3" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>4 0.15181586 <a title="168-tfidf-4" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, leftto-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. this model selection approach to more sophisticated tagging improve their robustness even We believe that can be applied algorithms and further.</p><p>5 0.13921088 <a title="168-tfidf-5" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>Author: Zhenghua Li ; Ting Liu ; Wanxiang Che</p><p>Abstract: We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing. Several types of transformation patterns (TP) are designed to capture the systematic annotation inconsistencies among different treebanks. Based on such TPs, we design quasisynchronous grammar features to augment the baseline parsing models. Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target treebanks (Penn Chinese Treebank 5. 1 and 6.0) using the Chinese Dependency Treebank as the source treebank. The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags. Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion.</p><p>6 0.13045312 <a title="168-tfidf-6" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>7 0.12760039 <a title="168-tfidf-7" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>8 0.10673575 <a title="168-tfidf-8" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>9 0.10485695 <a title="168-tfidf-9" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>10 0.10037006 <a title="168-tfidf-10" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>11 0.090453111 <a title="168-tfidf-11" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>12 0.090022512 <a title="168-tfidf-12" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>13 0.084024273 <a title="168-tfidf-13" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>14 0.080737345 <a title="168-tfidf-14" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>15 0.079460889 <a title="168-tfidf-15" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>16 0.076002561 <a title="168-tfidf-16" href="./acl-2012-A_Meta_Learning_Approach_to_Grammatical_Error_Correction.html">15 acl-2012-A Meta Learning Approach to Grammatical Error Correction</a></p>
<p>17 0.071089134 <a title="168-tfidf-17" href="./acl-2012-Chinese_Comma_Disambiguation_for_Discourse_Analysis.html">47 acl-2012-Chinese Comma Disambiguation for Discourse Analysis</a></p>
<p>18 0.071067818 <a title="168-tfidf-18" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>19 0.068224154 <a title="168-tfidf-19" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>20 0.066715874 <a title="168-tfidf-20" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.208), (1, -0.003), (2, -0.13), (3, -0.105), (4, 0.009), (5, 0.134), (6, 0.089), (7, -0.199), (8, -0.062), (9, 0.018), (10, -0.01), (11, 0.029), (12, 0.068), (13, -0.005), (14, 0.075), (15, 0.139), (16, 0.042), (17, -0.047), (18, 0.077), (19, 0.033), (20, -0.077), (21, 0.132), (22, -0.068), (23, 0.098), (24, 0.081), (25, 0.048), (26, -0.049), (27, 0.01), (28, 0.102), (29, -0.078), (30, 0.045), (31, 0.12), (32, 0.019), (33, -0.062), (34, 0.039), (35, -0.082), (36, -0.065), (37, -0.026), (38, 0.103), (39, -0.005), (40, -0.029), (41, -0.002), (42, -0.015), (43, -0.053), (44, 0.071), (45, 0.03), (46, -0.07), (47, 0.116), (48, -0.044), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9509353 <a title="168-lsi-1" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>Author: Weiwei Sun ; Xiaojun Wan</p><p>Abstract: We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</p><p>2 0.7914359 <a title="168-lsi-2" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun ; Hans Uszkoreit</p><p>Abstract: From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on large-scale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by constituent parsing and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated approaches yield a relative error reduction of 18% in total over a stateof-the-art baseline.</p><p>3 0.73765308 <a title="168-lsi-3" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>Author: Jun Hatori ; Takuya Matsuzaki ; Yusuke Miyao ; Jun'ichi Tsujii</p><p>Abstract: We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</p><p>4 0.67530644 <a title="168-lsi-4" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, leftto-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. this model selection approach to more sophisticated tagging improve their robustness even We believe that can be applied algorithms and further.</p><p>5 0.63459736 <a title="168-lsi-5" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>Author: Qiuye Zhao ; Mitch Marcus</p><p>Abstract: We show for both English POS tagging and Chinese word segmentation that with proper representation, large number of deterministic constraints can be learned from training examples, and these are useful in constraining probabilistic inference. For tagging, learned constraints are directly used to constrain Viterbi decoding. For segmentation, character-based tagging constraints can be learned with the same templates. However, they are better applied to a word-based model, thus an integer linear programming (ILP) formulation is proposed. For both problems, the corresponding constrained solutions have advantages in both efficiency and accuracy. 1 introduction In recent work, interesting results are reported for applications of integer linear programming (ILP) such as semantic role labeling (SRL) (Roth and Yih, 2005), dependency parsing (Martins et al., 2009) and so on. In an ILP formulation, ’non-local’ deterministic constraints on output structures can be naturally incorporated, such as ”a verb cannot take two subject arguments” for SRL, and the projectivity constraint for dependency parsing. In contrast to probabilistic constraints that are estimated from training examples, this type of constraint is usually hand-written reflecting one’s linguistic knowledge. Dynamic programming techniques based on Markov assumptions, such as Viterbi decoding, cannot handle those ’non-local’ constraints as discussed above. However, it is possible to constrain Viterbi 1054 decoding by ’local’ constraints, e.g. ”assign label t to word w” for POS tagging. This type of constraint may come from human input solicited in interactive inference procedure (Kristjansson et al., 2004). In this work, we explore deterministic constraints for two fundamental NLP problems, English POS tagging and Chinese word segmentation. We show by experiments that, with proper representation, large number of deterministic constraints can be learned automatically from training data, which can then be used to constrain probabilistic inference. For POS tagging, the learned constraints are directly used to constrain Viterbi decoding. The corresponding constrained tagger is 10 times faster than searching in a raw space pruned with beam-width 5. Tagging accuracy is moderately improved as well. For Chinese word segmentation (CWS), which can be formulated as character tagging, analogous constraints can be learned with the same templates as English POS tagging. High-quality constraints can be learned with respect to a special tagset, however, with this tagset, the best segmentation accuracy is hard to achieve. Therefore, these character-based constraints are not directly used for determining predictions as in English POS tagging. We propose an ILP formulation of the CWS problem. By adopting this ILP formulation, segmentation F-measure is increased from 0.968 to 0.974, as compared to Viterbi decoding with the same feature set. Moreover, the learned constraints can be applied to reduce the number of possible words over a character sequence, i.e. to reduce the number of variables to set. This reduction of problem size immediately speeds up an ILP solver by more than 100 times. ProceediJnegjus, o Rfe thpeu 5bl0icth o Afn Knouraela M, 8e-e1t4in Jgul oyf t 2h0e1 A2.s ?oc c2ia0t1io2n A fsosro Cciaotmiopnu ftaotrio Cnoamlp Luintagtuioisntaicls L,i pnaggueis t 1i0c5s4–1062, 2 English POS tagging 2.1 Explore deterministic constraints Suppose that, following (Chomsky, 1970), we distinguish major lexical categories (Noun, Verb, Adjective and Preposition) by two binary features: + |− N and +|− V. Let (+N −V) =Noun, (−N +V) =Verb, (+N, +V) =Adjective, aonudn (−N, −V) =preposition. A word occurring in betw(e−eNn a preceding wosoitrdio nth.e Aand w a following wgo irnd of always bears the feature +N. On the other hand, consider the annotation guideline of English Treebank (Marcus et al., 1993) instead. Part-of-speech (POS) tags are used to categorize words, for example, the POS tag VBG tags verbal gerunds, NNS tags nominal plurals, DT tags determiners and so on. Following this POS representation, there are as many as 10 possible POS tags that may occur in between the–of, as estimated from the WSJ corpus of Penn Treebank. , 2.1.1 Templates of deterministic constraints , To explore determinacy in the distribution of POS tags in Penn Treebank, we need to consider that a POS tag marks the basic syntactic category of a word as well as its morphological inflection. A constraint that may determine the POS category should reflect both the context and the morphological feature of the corresponding word. The practical difficulty in representing such deterministic constraints is that we do not have a perfect mechanism to analyze morphological features of a word. Endings or prefixes of English words do not deterministically mark their morphological inflections. We propose to compute the morph feature of a word as the set of all of its possible tags, i.e. all tag types that are assigned to the word in training data. Furthermore, we approximate unknown words in testing data by rare words in training data. For a word that occurs less than 5 times in the training corpus, we compute its morph feature as its last two characters, which is also conjoined with binary features indicating whether the rare word contains digits, hyphens or upper-case characters respectively. See examples of morph features in Table 1. We consider bigram and trigram templates for generating potentially deterministic constraints. Let denote the ith word relative to the current word w0; and mi denote the morph feature of wi. A wi 1055 w(fr0e=qtruaednets)(set of pmos0s=ib{lNeN taSg,s V oBfZ th}e word) w0=t(imraere-s)hares(thme0 l=as{t- tewso, c HhYaPraHcEteNrs}. .) Table 1: Morph features offrequent words and rare words as computed from the WSJ Corpus of Penn Treebank. -gtbr ai -m w −1w 0w−mw1 m,wm 0−, 1mw1 0 w mw1 , mw m− 1m 1mw0m0w,1 wm, m0 −m1 m 0wm1 Table 2: The templates for generating potentially deterministic constraints of English POS tagging. bigram constraint includes one contextual word (w−1 |w1) or the corresponding morph feature; and a trigram constraint includes both contextual words or their morph features. Each constraint is also con- joined with w0 or m0, as described in Table 2. 2.1.2 Learning of deterministic constraints In the above section, we explore templates for potentially deterministic constraints that may determine POS category. With respect to a training corpus, if a constraint C relative to w0 ’always’ assigns a certain POS category t∗ to w0 in its context, i.e. > thr, and this constraint occurs more than a cutoff number, we consider it as a deterministic constraint. The threshold thr is a real number just under 1.0 and the cutoff number is empirically set to 5 in our experiments. counctou(Cnt∧(tC0)=t∗) 2.1.3 Decoding of deterministic constraints By the above definition, the constraint of w−1 = the, m0 = {NNS VBZ } and w1 = of is deterministic. It det=er{mNiNneSs, ,the V BPZO}S category of w0 to be NNS. There are at least two ways of decoding these constraints during POS tagging. Take the word trades for example, whose morph feature is {NNS, VBZ}. fOonre e xaaltemrnplaet,ive w hiso sthea tm as long as rtera dises { occurs Zb e}-. tween the-of, it is tagged with NNS. The second alternative is that the tag decision is made only if all deterministic constraints relative to this occurrence , of trades agree on the same tag. Both ways of decoding are purely rule-based and involve no probabilistic inference. In favor of a higher precision, we adopt the latter one in our experiments. tTchoe/nDscrotTamwSpci&lnoeLmxpd;/–fiulenbtaxp/i–cloufntg/aNpnlOci(amgnw/1–tOhNTpe(lanS+Ti&/m2cNL)lubTdaien2ls/)IoVNuBtlZamwn.1=ic2l3ud,ems.2=1 Table 3: Comparison of raw input and constrained input. 2.2 Search in a constrained space Following most previous work, we consider POS tagging as a sequence classification problem and de- compose the overall sequence scnore over the linear structure, i.e. ˆt =t∈atraggGmENa(xw)Xi=1score(ti) where function tagGEN maps input seXntence w = w1...wn to the set of all tag sequences that are of length n. If a POS tagger takes raw input only, i.e. for every word, the number of possible tags is a constant T, the space of tagGEN is as large as Tn. On the other hand, if we decode deterministic constraints first be- fore a probabilistic search, i.e. for some words, the number of possible tags is reduced to 1, the search space is reduced to Tm, where m is the number of (unconstrained) words that are not subject to any deterministic constraints. Viterbi algorithm is widely used for tagging, and runs in O(nT2) when searching in an unconstrained space. On the other hand, consider searching in a constrained space. Suppose that among the m unconstrained words, m1 of them follow a word that has been tagged by deterministic constraints and m2 (=m-m1) of them follow another unconstrained word. Viterbi decoder runs in O(m1T + m2T2) while searching in such a constrained space. The example in Table 3 shows raw and constrained input with respect to a typical input sentence. Lookahead features The score of tag predictions are usually computed in a high-dimensional feature space. We adopt the basic feature set used in (Ratnaparkhi, 1996) and (Collins, 2002). Moreover, when deterministic constraints have applied to contextual words of w0, it is also possible to include some lookahead feature templates, such as: t0&t1; , t0&t1;&t2; , and t−1&t0;&t1; where ti represents the tag of the ith word relative 1056 to the current word w0. As discussed in (Shen et al., 2007), categorical information of neighbouring words on both sides of w0 help resolve POS ambiguity of w0. In (Shen et al., 2007), lookahead features may be available for use during decoding since searching is bidirectional instead of left-to-right as in Viterbi decoding. In this work, deterministic constraints are decoded before the application of probabilistic models, therefore lookahead features are made available during Viterbi decoding. 3 Chinese Word Segmentation (CWS) 3.1 Word segmentation as character tagging Considering the ambiguity problem that a Chinese character may appear in any relative position in a word and the out-of-vocabulary (OOV) problem that it is impossible to observe all words in training data, CWS is widely formulated as a character tagging problem (Xue, 2003). A character-based CWS decoder is to find the highest scoring tag sequence tˆ over the input character sequence c, i.e. Xn tˆ =t∈ atraggGmEaNx(c)Xi=1score(ti) . This is the same formulation as POS tagging. The Viterbi algorithm is also widely used for decoding. The tag of each character represents its relative position in a word. Two popular tagsets include 1) IB: where B tags the beginning of a word and I all other positions; and 2) BMES: where B, M and E represent the beginning, middle and end of a multicharacter word respectively, and S tags a singlecharacter word. For example, after decoding with BMES, 4 consecutive characters associated with the tag sequence BMME compose a word. However, after decoding with IB, characters associated with BIII may compose a word if the following tag is B or only form part of a word if the following tag is I. Even though character tagging accuracy is higher with tagset IB, tagset BMES is more popular in use since better performance of the original problem CWS can be achieved by this tagset. Character-based feature templates We adopt the ’non-lexical-target’ feature templates in (Jiang et al., 2008a). Let ci denote the ith character relative to the current character c0 and t0 denote the tag assigned to c0. The following templates are used: ci&t0; (i=-2...2), cici+1&t0; (i=-2...1) and c−1c1&t0.; Character-based deterministic constraints We can use the same templates as described in Table 2 to generate potentially deterministic constraints for CWS character tagging, except that there are no morph features computed for Chinese characters. As we will show with experimental results in Section 5.2, useful deterministic constraints for CWS can be learned with tagset IB but not with tagset BMES. It is interesting but not surprising to notice, again, that the determinacy of a problem is sensitive to its representation. Since it is hard to achieve the best segmentations with tagset IB, we propose an indirect way to use these constraints in the following section, instead of applying these constraints as straightforwardly as in English POS tagging. 3.2 Word-based word segmentation A word-based CWS decoder finds the highest scoring segmentation sequence wˆ that is composed by the input character sequence c, i.e. wˆ =w∈arseggGmEaNx(c)Xi|=w1|score(wi) . where function segGEN maps character sequence c to the set of all possible segmentations of c. For example, w = (c1. .cl1 ) ...(cn−lk+1 ...cn) represents a segmentation of k words and the lengths of the first and last word are l1 and lk respectively. In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996). Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in (Zhang and Clark, 2007) and (Jiang et al., 2008a). We propose an Integer Linear Programming (ILP) formulation of word segmentation, which is naturally viewed as a word-based model for CWS. Character-based deterministic constraints, as discussed in Section 3.1, can be easily applied. 3.3 ILP formulation of CWS Given a character sequence c=c1 ...cn, there are s(= n(n + 1)/2) possible words that are contiguous subsets of c, i.e. w1, ..., ws ⊆ c. Our goal is to find 1057 Table 4: Comparison of raw input and constrained input. an optimal solution x = ...xs that maximizes x1 Xs Xscore(wi) · xi, subject to Xi= X1 (1) X xi = 1, ∀c ∈ c; (2) ix:Xic∈∈wi {0,1},1 ≤i≤s The boolean value of xi, as guaranteed by constraint (2), indicates whether wi is selected in the segmentation solution or not. Constraint (1) requires every character to be included in exactly one selected word, thus guarantees a proper segmentation of the whole sequence. This resembles the ILP formulation of the set cover problem, though the first con- straint is different. Take n = 2 for example, i.e. c = c1c2, the set of possible words is {c1, c2 , c1c2}, i.e. s = |x| = t3 o. T pohesrseib are only t iwso { possible soli.uet.ion ss = subject t o3 .co Tnhsetrreain artse (1) yan tdw (2), x = 1 s1o0giving an output set {c1, c2}, or x = 001 giving an output asent {c1c2}. tTphuet efficiency o.f solving this problem depends on the number of possible words (contiguous subsets) over a character sequence, i.e. the number of variables in x. So as to reduce |x|, we apply determiniasbtlice sc ionn xs.tra Sinots a predicting I |xB| tags first, w dehtiecrhm are learned as described in Section 3.1. Possible words are generated with respect to the partially tagged character sequence. A character tagged with B always occurs at the beginning of a possible word. Table 4 illustrates the constrained and raw input with respect to a typical character sequence. 3.4 Character- and word-based features As studied in previous work, word-based feature templates usually include the word itself, sub-words contained in the word, contextual characters/words and so on. It has been shown that combining the use of character- and word-based features helps improve performance. However, in the character tag- ging formulation, word-based features are non-local. To incorporate these non-local features and make the search tractable, various efforts have been made. For example, Jiang et al. (2008a) combine different levels of knowledge in an outside linear model of a twolayer cascaded model; Jiang et al. (2008b) uses the forest re-ranking technique (Huang, 2008); and in (Kruengkrai et al., 2009), only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes. We propose to incorporate character-based features in word-based models. Consider a characterbased feature function φ(c, t,c) that maps a character-tag pair to a high-dimensional feature space, with respect to an input character sequence c. For a possible word over c of length l , wi = ci0 ...ci0+l−1, tag each character cij in this word with a character-based tag tij . Character-based features of wi can be computed as {φ(cij , tij , c) |0 ≤ j < l}. The ficrsant row oofm pTautbeled a5s i {llφus(tcrates c,ch)a|r0ac ≤ter j-b</p><p>6 0.62336284 <a title="168-lsi-6" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>7 0.60455257 <a title="168-lsi-7" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>8 0.58622426 <a title="168-lsi-8" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>9 0.53916639 <a title="168-lsi-9" href="./acl-2012-Applications_of_GPC_Rules_and_Character_Structures_in_Games_for_Learning_Chinese_Characters.html">26 acl-2012-Applications of GPC Rules and Character Structures in Games for Learning Chinese Characters</a></p>
<p>10 0.53841281 <a title="168-lsi-10" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>11 0.53794843 <a title="168-lsi-11" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>12 0.44281021 <a title="168-lsi-12" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>13 0.43713289 <a title="168-lsi-13" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>14 0.43078974 <a title="168-lsi-14" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>15 0.42388546 <a title="168-lsi-15" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>16 0.42365792 <a title="168-lsi-16" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>17 0.41085657 <a title="168-lsi-17" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>18 0.40952492 <a title="168-lsi-18" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>19 0.38349828 <a title="168-lsi-19" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>20 0.38112849 <a title="168-lsi-20" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.016), (25, 0.027), (26, 0.038), (28, 0.065), (30, 0.031), (37, 0.03), (39, 0.036), (62, 0.24), (71, 0.014), (74, 0.03), (82, 0.015), (84, 0.029), (85, 0.031), (90, 0.151), (92, 0.046), (94, 0.034), (99, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81793439 <a title="168-lda-1" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>Author: Kevin Duh ; Katsuhito Sudoh ; Xianchao Wu ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</p><p>same-paper 2 0.77295727 <a title="168-lda-2" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>Author: Weiwei Sun ; Xiaojun Wan</p><p>Abstract: We address the issue of consuming heterogeneous annotation data for Chinese word segmentation and part-of-speech tagging. We empirically analyze the diversity between two representative corpora, i.e. Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD), on manually mapped data, and show that their linguistic annotations are systematically different and highly compatible. The analysis is further exploited to improve processing accuracy by (1) integrating systems that are respectively trained on heterogeneous annotations to reduce the approximation error, and (2) re-training models with high quality automatically converted data to reduce the estimation error. Evaluation on the CTB and PPD data shows that our novel model achieves a relative error reduction of 11% over the best reported result in the literature.</p><p>3 0.63574165 <a title="168-lda-3" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>Author: Ce Zhang ; Feng Niu ; Christopher Re ; Jude Shavlik</p><p>Abstract: Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.</p><p>4 0.62783557 <a title="168-lda-4" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>Author: Xinfan Meng ; Furu Wei ; Xiaohua Liu ; Ming Zhou ; Ge Xu ; Houfeng Wang</p><p>Abstract: The amount of labeled sentiment data in English is much larger than that in other languages. Such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) using labeled data in the source language (e.g. English). Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language. This approach suffers from the limited coverage of vocabulary in the machine translation results. In this paper, we propose a generative cross-lingual mixture model (CLMM) to leverage unlabeled bilingual parallel data. By fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage signifi- cantly. Experiments on multiple data sets show that CLMM is consistently effective in two settings: (1) labeled data in the target language are unavailable; and (2) labeled data in the target language are also available.</p><p>5 0.62722397 <a title="168-lda-5" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>6 0.62372094 <a title="168-lda-6" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>7 0.62202144 <a title="168-lda-7" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>8 0.62191719 <a title="168-lda-8" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>9 0.62081802 <a title="168-lda-9" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>10 0.62051064 <a title="168-lda-10" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>11 0.61954963 <a title="168-lda-11" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>12 0.6182673 <a title="168-lda-12" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>13 0.6180324 <a title="168-lda-13" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>14 0.61707139 <a title="168-lda-14" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>15 0.61650831 <a title="168-lda-15" href="./acl-2012-Text-level_Discourse_Parsing_with_Rich_Linguistic_Features.html">193 acl-2012-Text-level Discourse Parsing with Rich Linguistic Features</a></p>
<p>16 0.61552858 <a title="168-lda-16" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>17 0.61460876 <a title="168-lda-17" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>18 0.61345315 <a title="168-lda-18" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>19 0.61334544 <a title="168-lda-19" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>20 0.61253119 <a title="168-lda-20" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
