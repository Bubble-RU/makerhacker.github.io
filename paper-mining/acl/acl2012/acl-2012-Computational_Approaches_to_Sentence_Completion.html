<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 acl-2012-Computational Approaches to Sentence Completion</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-56" href="#">acl2012-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 acl-2012-Computational Approaches to Sentence Completion</h1>
<br/><p>Source: <a title="acl-2012-56-pdf" href="http://aclweb.org/anthology//P/P12/P12-1063.pdf">pdf</a></p><p>Author: Geoffrey Zweig ; John C. Platt ; Christopher Meek ; Christopher J.C. Burges ; Ainur Yessenalina ; Qiang Liu</p><p>Abstract: This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</p><p>Reference: <a title="acl-2012-56-reference" href="../acl2012_reference/acl-2012-Computational_Approaches_to_Sentence_Completion_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsa', 0.43), ('sat', 0.364), ('mikolov', 0.233), ('recur', 0.184), ('holm', 0.182), ('zweig', 0.181), ('reconstruct', 0.173), ('burg', 0.163), ('answ', 0.123), ('turney', 0.121), ('neur', 0.116), ('quest', 0.112), ('semev', 0.095), ('bellegard', 0.09), ('coccaro', 0.09), ('msr', 0.086), ('deerwest', 0.082), ('complet', 0.082), ('synonym', 0.079), ('comprehend', 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="56-tfidf-1" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>Author: Geoffrey Zweig ; John C. Platt ; Christopher Meek ; Christopher J.C. Burges ; Ainur Yessenalina ; Qiang Liu</p><p>Abstract: This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</p><p>2 0.16635117 <a title="56-tfidf-2" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: Sentence Similarity is the process of computing a similarity score between two sentences. Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences. In this paper, we show that by carefully handling words that are not in the sentences (missing words), we can train a reliable latent variable model on sentences. In the process, we propose a new evaluation framework for sentence similarity: Concept Definition Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.</p><p>3 0.15121484 <a title="56-tfidf-3" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>Author: Wen Chan ; Xiangdong Zhou ; Wei Wang ; Tat-Seng Chua</p><p>Abstract: We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of “incomplete answer”, i.e., the “best answer” of a complex multi-sentence question misses valuable information that is contained in other answers. In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary method with group L1 regularization. Various textual and non-textual QA features are explored. Specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. To further unleash the potential of the abundant cQA features, we introduce the group L1 regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task.</p><p>4 0.13702475 <a title="56-tfidf-4" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>5 0.11971331 <a title="56-tfidf-5" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.</p><p>6 0.083289124 <a title="56-tfidf-6" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>7 0.078579932 <a title="56-tfidf-7" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>8 0.076929428 <a title="56-tfidf-8" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>9 0.076161146 <a title="56-tfidf-9" href="./acl-2012-Modeling_Review_Comments.html">144 acl-2012-Modeling Review Comments</a></p>
<p>10 0.071142562 <a title="56-tfidf-10" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>11 0.067064486 <a title="56-tfidf-11" href="./acl-2012-Deep_Learning_for_NLP_%28without_Magic%29.html">69 acl-2012-Deep Learning for NLP (without Magic)</a></p>
<p>12 0.065790437 <a title="56-tfidf-12" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>13 0.064755753 <a title="56-tfidf-13" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>14 0.063081197 <a title="56-tfidf-14" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>15 0.062845737 <a title="56-tfidf-15" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>16 0.062650427 <a title="56-tfidf-16" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>17 0.061845694 <a title="56-tfidf-17" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>18 0.061560832 <a title="56-tfidf-18" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>19 0.061232992 <a title="56-tfidf-19" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>20 0.061166156 <a title="56-tfidf-20" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.208), (1, 0.083), (2, -0.018), (3, 0.044), (4, 0.065), (5, 0.007), (6, 0.046), (7, 0.001), (8, -0.017), (9, -0.025), (10, -0.024), (11, -0.025), (12, -0.011), (13, 0.011), (14, 0.027), (15, 0.046), (16, 0.062), (17, 0.058), (18, -0.034), (19, 0.061), (20, -0.097), (21, 0.047), (22, -0.138), (23, -0.055), (24, -0.172), (25, 0.048), (26, -0.084), (27, -0.016), (28, -0.007), (29, 0.055), (30, -0.14), (31, 0.093), (32, -0.01), (33, 0.097), (34, 0.05), (35, -0.047), (36, 0.017), (37, 0.075), (38, -0.026), (39, 0.009), (40, 0.042), (41, 0.024), (42, -0.124), (43, 0.048), (44, -0.165), (45, -0.236), (46, 0.004), (47, -0.014), (48, 0.039), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90566278 <a title="56-lsi-1" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>Author: Geoffrey Zweig ; John C. Platt ; Christopher Meek ; Christopher J.C. Burges ; Ainur Yessenalina ; Qiang Liu</p><p>Abstract: This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</p><p>2 0.77714646 <a title="56-lsi-2" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: Sentence Similarity is the process of computing a similarity score between two sentences. Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences. In this paper, we show that by carefully handling words that are not in the sentences (missing words), we can train a reliable latent variable model on sentences. In the process, we propose a new evaluation framework for sentence similarity: Concept Definition Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.</p><p>3 0.74223793 <a title="56-lsi-3" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>4 0.67077518 <a title="56-lsi-4" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>Author: Wen Chan ; Xiangdong Zhou ; Wei Wang ; Tat-Seng Chua</p><p>Abstract: We present a novel answer summarization method for community Question Answering services (cQAs) to address the problem of “incomplete answer”, i.e., the “best answer” of a complex multi-sentence question misses valuable information that is contained in other answers. In order to automatically generate a novel and non-redundant community answer summary, we segment the complex original multi-sentence question into several sub questions and then propose a general Conditional Random Field (CRF) based answer summary method with group L1 regularization. Various textual and non-textual QA features are explored. Specifically, we explore four different types of contextual factors, namely, the information novelty and non-redundancy modeling for local and non-local sentence interactions under question segmentation. To further unleash the potential of the abundant cQA features, we introduce the group L1 regularization for feature learning. Experimental results on a Yahoo! Answers dataset show that our proposed method significantly outperforms state-of-the-art methods on cQA summarization task.</p><p>5 0.55302542 <a title="56-lsi-5" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.</p><p>6 0.54848903 <a title="56-lsi-6" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>7 0.51973909 <a title="56-lsi-7" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>8 0.46592423 <a title="56-lsi-8" href="./acl-2012-Humor_as_Circuits_in_Semantic_Networks.html">112 acl-2012-Humor as Circuits in Semantic Networks</a></p>
<p>9 0.46210244 <a title="56-lsi-9" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>10 0.45510262 <a title="56-lsi-10" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>11 0.45091748 <a title="56-lsi-11" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>12 0.45031622 <a title="56-lsi-12" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>13 0.44837964 <a title="56-lsi-13" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>14 0.44668391 <a title="56-lsi-14" href="./acl-2012-Distributional_Semantics_in_Technicolor.html">76 acl-2012-Distributional Semantics in Technicolor</a></p>
<p>15 0.42615953 <a title="56-lsi-15" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>16 0.41154832 <a title="56-lsi-16" href="./acl-2012-Automatically_Mining_Question_Reformulation_Patterns_from_Search_Log_Data.html">35 acl-2012-Automatically Mining Question Reformulation Patterns from Search Log Data</a></p>
<p>17 0.40729472 <a title="56-lsi-17" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>18 0.40649641 <a title="56-lsi-18" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>19 0.39751974 <a title="56-lsi-19" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>20 0.39660841 <a title="56-lsi-20" href="./acl-2012-Automated_Essay_Scoring_Based_on_Finite_State_Transducer%3A_towards_ASR_Transcription_of_Oral_English_Speech.html">32 acl-2012-Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (8, 0.078), (22, 0.068), (31, 0.027), (38, 0.043), (47, 0.047), (55, 0.276), (58, 0.061), (61, 0.021), (77, 0.036), (91, 0.097), (92, 0.065), (96, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71643203 <a title="56-lda-1" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>Author: Geoffrey Zweig ; John C. Platt ; Christopher Meek ; Christopher J.C. Burges ; Ainur Yessenalina ; Qiang Liu</p><p>Abstract: This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</p><p>2 0.6730119 <a title="56-lda-2" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>Author: Micha Elsner ; Sharon Goldwater ; Jacob Eisenstein</p><p>Abstract: ILCC, School of Informatics School of Interactive Computing University of Edinburgh Georgia Institute of Technology Edinburgh, EH8 9AB, UK Atlanta, GA, 30308, USA (a) intended: /ju want w2n/ /want e kUki/ (b) surface: [j@ w a?P w2n] [wan @ kUki] During early language acquisition, infants must learn both a lexicon and a model of phonetics that explains how lexical items can vary in pronunciation—for instance “the” might be realized as [Di] or [D@]. Previous models of acquisition have generally tackled these problems in isolation, yet behavioral evidence suggests infants acquire lexical and phonetic knowledge simultaneously. We present a Bayesian model that clusters together phonetic variants of the same lexical item while learning both a language model over lexical items and a log-linear model of pronunciation variability based on articulatory features. The model is trained on transcribed surface pronunciations, and learns by bootstrapping, without access to the true lexicon. We test the model using a corpus of child-directed speech with realistic phonetic variation and either gold standard or automatically induced word boundaries. In both cases modeling variability improves the accuracy of the learned lexicon over a system that assumes each lexical item has a unique pronunciation.</p><p>3 0.56326103 <a title="56-lda-3" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>4 0.55674177 <a title="56-lda-4" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>Author: Shomir Wilson</p><p>Abstract: Metalanguage is an essential linguistic mechanism which allows us to communicate explicit information about language itself. However, it has been underexamined in research in language technologies, to the detriment of the performance of systems that could exploit it. This paper describes the creation of the first tagged and delineated corpus of English metalanguage, accompanied by an explicit definition and a rubric for identifying the phenomenon in text. This resource will provide a basis for further studies of metalanguage and enable its utilization in language technologies.</p><p>5 0.55633473 <a title="56-lda-5" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>Author: Sungchul Kim ; Kristina Toutanova ; Hwanjo Yu</p><p>Abstract: In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.</p><p>6 0.55631495 <a title="56-lda-6" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>7 0.55355901 <a title="56-lda-7" href="./acl-2012-FLOW%3A_A_First-Language-Oriented_Writing_Assistant_System.html">92 acl-2012-FLOW: A First-Language-Oriented Writing Assistant System</a></p>
<p>8 0.55166346 <a title="56-lda-8" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>9 0.55138648 <a title="56-lda-9" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>10 0.55011702 <a title="56-lda-10" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>11 0.54938948 <a title="56-lda-11" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>12 0.54889858 <a title="56-lda-12" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>13 0.54764754 <a title="56-lda-13" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>14 0.54601967 <a title="56-lda-14" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>15 0.54549754 <a title="56-lda-15" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>16 0.5454641 <a title="56-lda-16" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>17 0.54505807 <a title="56-lda-17" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>18 0.54487807 <a title="56-lda-18" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>19 0.54465079 <a title="56-lda-19" href="./acl-2012-Personalized_Normalization_for_a_Multilingual_Chat_System.html">160 acl-2012-Personalized Normalization for a Multilingual Chat System</a></p>
<p>20 0.54435062 <a title="56-lda-20" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
