<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-115" href="#">acl2012-115</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</h1>
<br/><p>Source: <a title="acl-2012-115-pdf" href="http://aclweb.org/anthology//P/P12/P12-2066.pdf">pdf</a></p><p>Author: Zhaopeng Tu ; Yifan He ; Jennifer Foster ; Josef van Genabith ; Qun Liu ; Shouxun Lin</p><p>Abstract: Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. 1</p><p>Reference: <a title="acl-2012-115-reference" href="../acl2012_reference/acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ie  Abstract Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. [sent-9, score-0.527]
</p><p>2 However, such models are highly sensitive to the type and size of syntactic structure used. [sent-10, score-0.055]
</p><p>3 In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. [sent-12, score-1.376]
</p><p>4 We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1. [sent-13, score-0.485]
</p><p>5 45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. [sent-14, score-0.284]
</p><p>6 1  Introduction  An important subtask in sentiment analysis is sentiment classification. [sent-15, score-0.568]
</p><p>7 Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraphlevel, sentence-level and phrase-level. [sent-16, score-0.053]
</p><p>8 There has been a substantial amount of work on document-level sentiment classification. [sent-18, score-0.284]
</p><p>9 In early pioneering work, Pang and Lee (2004) use a flat feature vector (e. [sent-19, score-0.029]
</p><p>10 A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the doc338 uments. [sent-22, score-0.027]
</p><p>11 More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). [sent-23, score-0.061]
</p><p>12 In this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. [sent-25, score-1.307]
</p><p>13 It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. [sent-30, score-1.059]
</p><p>14 It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al. [sent-31, score-0.211]
</p><p>15 , 2009): a film review often uses lengthy objective  paragraphs to simply describe the plot. [sent-32, score-0.039]
</p><p>16 Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifiProce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-33, score-0.388]
</p><p>17 Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al. [sent-36, score-0.226]
</p><p>18 Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words. [sent-39, score-0.462]
</p><p>19 More specifically, we use the terms in the lexicon constructed from (Wilson et al. [sent-40, score-0.052]
</p><p>20 , 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). [sent-41, score-0.803]
</p><p>21 An empirical evaluation on a widely used sentiment corpus shows an improvement of 1. [sent-42, score-0.284]
</p><p>22 45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). [sent-43, score-0.028]
</p><p>23 2  Related Work  Our research builds on previous work in the field of sentiment classification and convolution kernels. [sent-44, score-0.676]
</p><p>24 For sentiment classification, the design of lexical and syntactic features is an important first step. [sent-45, score-0.339]
</p><p>25 (2003) represent a document as a bag-of-words; Matsumoto et al. [sent-48, score-0.041]
</p><p>26 , (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. [sent-49, score-0.416]
</p><p>27 Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. [sent-54, score-1.817]
</p><p>28 noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. [sent-58, score-0.349]
</p><p>29 1 Linguistic Representations We explore both sequence and convolution kernels to exploit information on surface and syntactic levels. [sent-63, score-0.899]
</p><p>30 For sequence kernels, we make use of lexical words with some syntactic information in the form  of part-of-speech (POS) tags. [sent-64, score-0.088]
</p><p>31 More specifically, we define three types of sequences: •  •  •  SW, a sequence of lexical words, e. [sent-65, score-0.033]
</p><p>32 : A tragic waste of talent ea ondf ienxcicreadli bwloer dvsis,u ea. [sent-67, score-0.457]
</p><p>33 In addition, we experiment with constituency tree kernels (CON), and dependency tree kernels (D), which capture hierarchical constituency structure and labeled dependency relations between words, respectively. [sent-75, score-1.453]
</p><p>34 For dependency kernels, we test with word (DW), POS (DP), and combined word-andPOS settings (DWP), and similarly for simple sequence kernels (SW, SP and SWP). [sent-76, score-0.607]
</p><p>35 We also use a vector kernel (VK) in a bag-of-words baseline. [sent-77, score-0.094]
</p><p>36 Figure 1 shows the constituent and dependency structure for the above sentence. [sent-78, score-0.175]
</p><p>37 2 Settings As kernel-based algorithms inherently explore the whole feature space to weight the features, it is important to choose appropriate substructures to remove unnecessary features as much as possible. [sent-80, score-0.281]
</p><p>38 (a) Constituent parse tree (CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b) with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP). [sent-82, score-0.323]
</p><p>39 wa msoted  NP DT A  Figure 2: Illustration  JJ tragic (a)  NN waste  JJ tragic (b)  of the different  stituency (CON) and dependency tragic as the indicator word. [sent-83, score-1.064]
</p><p>40 settings  on con-  (DWP) parse trees with  Unfortunately, in our task there exist several cues indicating the polarity of the document, which are distributed in different sentences. [sent-84, score-0.209]
</p><p>41 To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al. [sent-85, score-0.354]
</p><p>42 For each polarity indicator, we define the “scope” (the minimal syntactic structure containing at least one subjective word) of each indicator for different representations as follows: For a constituent tree, a node and its children correspond to a grammatical production. [sent-87, score-0.445]
</p><p>43 Therefore, considering the terminal node tragic in the constituent structure tree in Figure 1(a), we extract the subtree rooted at the grandparent of the terminal, see Figure 2(a). [sent-88, score-0.501]
</p><p>44 We also use the corresponding sequence 340  average number of trees, and Size denotes the averaged number of words in each tree. [sent-89, score-0.062]
</p><p>45 of words in the subtree for the sequential kernel. [sent-90, score-0.063]
</p><p>46 For a dependency tree, we only consider the subtree containing the lexical items that are directly connected to the subjective word. [sent-91, score-0.257]
</p><p>47 For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). [sent-92, score-0.561]
</p><p>48 We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. [sent-93, score-0.24]
</p><p>49 To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). [sent-96, score-0.109]
</p><p>50 To obtain dependency trees, we passed  the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). [sent-97, score-0.222]
</p><p>51 We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1 , respectively. [sent-98, score-0.675]
</p><p>52 We use a manually constructed polarity lexicon (Wilson et al. [sent-102, score-0.181]
</p><p>53 , 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). [sent-103, score-0.441]
</p><p>54 We only take into account the subjective terms with the degree of strong subjectivity. [sent-104, score-0.12]
</p><p>55 , 2006) Rand: a number of randomly selected substructures nsuimmilbaerr to th raen ndoummblyer s eolfe cetxedtrac sutebdsubstructures defined in Section 3. [sent-106, score-0.248]
</p><p>56 2 Results and Discussions Table 2 lists the results of the different kernel type combinations. [sent-110, score-0.094]
</p><p>57 As far as PT kernels are concerned, we find dependency trees with simple words (DW) outperform both dependency trees with POS (DP) and those with both words and POS (DWP). [sent-113, score-0.78]
</p><p>58 it/moschitti/ 341  document of the text, Sent denotes the sentences that contains subjective terms in the lexicon, Rand denotes randomly selected substructures, and Sub denotes the substructures defined in Section 3. [sent-117, score-0.496]
</p><p>59 the dependency representation, POS tags can introduce little new information, and will add unnecessary complexity. [sent-122, score-0.162]
</p><p>60 For example, given the substructure (waste (amod (JJ (tragic)))), the PT kernel will use both (waste (amod (JJ))) and (waste (amod (JJ (tragic)))). [sent-123, score-0.094]
</p><p>61 In contrast, words are good indicators for sentiment polarity. [sent-129, score-0.337]
</p><p>62 Firstly, it clearly demonstrates the value of incorporating syntactic information into the document-level sentiment classifier, as the tree kernels (CON and D*) generally outperforms vector and sequence kernels (VK and S*). [sent-131, score-1.388]
</p><p>63 5  Conclusion and Future Work  We studied the impact of syntactic information on document-level sentiment classification using convolution kernels, and reduced the complexity of the kernels by extracting minimal high-impact substructures, guided by a polarity lexicon. [sent-134, score-1.414]
</p><p>64 Our research focuses on identifying and using high-impact substructures for convolution kernels in document-level sentiment classification. [sent-137, score-1.343]
</p><p>65 We expect our method to be complementary with sophisticated methods used in state-of-the-art sentiment classification systems, which is to be explored in future work. [sent-138, score-0.337]
</p><p>66 In Proceedings of the Workshop on Languages in Social Media, pages 30–38. [sent-150, score-0.039]
</p><p>67 In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724–73 1, Vancouver, British Columbia, Canada, oct. [sent-155, score-0.039]
</p><p>68 In Y Weiss, B Sch o lkopf, and J Platt, editors, Proceedings of the 19th Conference on Neural Information Processing Systems, pages 171–178, Cambridge, MA. [sent-160, score-0.039]
</p><p>69 In Proceedings of Neural Information Processing Systems, pages 625–632. [sent-165, score-0.039]
</p><p>70 Exploring the use of paragraph-level annotations for sentiment analysis of financial blogs. [sent-174, score-0.284]
</p><p>71 In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 67–76, Uppsala, Sweden, July. [sent-179, score-0.039]
</p><p>72 In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 3 13–3 16, Suntec, Singapore, jul. [sent-183, score-0.039]
</p><p>73 In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, jul. [sent-188, score-0.039]
</p><p>74 In Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI) 2005, pages 1616–1616. [sent-193, score-0.039]
</p><p>75 Mining the peanut gallery: Opinion extraction and se-  mantic classification of product reviews. [sent-196, score-0.053]
</p><p>76 In Proceedings of the 12th International Conference on World Wide Web, pages 519–528, ACM. [sent-197, score-0.039]
</p><p>77 In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161– 169, Singapore, aug. [sent-202, score-0.039]
</p><p>78 In Proceedings of ACL-08: HLT, Short Papers, pages 113–1 16, Columbus, Ohio, jun. [sent-211, score-0.039]
</p><p>79 In Proceedings of the 1 European Conference on 7th Machine Learning, pages 3 18–329, Berlin, Germany, sep. [sent-220, score-0.039]
</p><p>80 In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611–618, Sydney, Australia, jul. [sent-225, score-0.039]
</p><p>81 Convolution kernels on constituent, dependency and sequential structures for relation extraction. [sent-229, score-0.652]
</p><p>82 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387. [sent-230, score-0.039]
</p><p>83 In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 271–278, Barcelona, Spain, jun. [sent-234, score-0.039]
</p><p>84 In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 795–803, Los Angeles, California, jun. [sent-239, score-0.039]
</p><p>85 In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347–354, Vancouver, British Columbia, Canada, oct. [sent-244, score-0.039]
</p><p>86 Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. [sent-248, score-0.337]
</p><p>87 In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 129–136, Association for Computational Linguistics. [sent-249, score-0.039]
</p><p>88 In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 825–832, Sydney, Australia, jul. [sent-254, score-0.039]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernels', 0.472), ('convolution', 0.339), ('sentiment', 0.284), ('tragic', 0.265), ('substructures', 0.248), ('amod', 0.19), ('jj', 0.167), ('vk', 0.159), ('moschitti', 0.141), ('waste', 0.139), ('polarity', 0.129), ('subjective', 0.12), ('dwp', 0.106), ('opinion', 0.104), ('dependency', 0.102), ('kernel', 0.094), ('pang', 0.09), ('wiegand', 0.08), ('dw', 0.079), ('alessandro', 0.074), ('constituent', 0.073), ('tree', 0.072), ('con', 0.071), ('constituency', 0.068), ('agarwal', 0.063), ('bunescu', 0.061), ('wilson', 0.059), ('ferguson', 0.056), ('syntactic', 0.055), ('classification', 0.053), ('conj', 0.053), ('koppel', 0.053), ('detamodprep', 0.053), ('incredible', 0.053), ('klakow', 0.053), ('swp', 0.053), ('talent', 0.053), ('indicators', 0.053), ('trees', 0.052), ('lexicon', 0.052), ('pos', 0.048), ('dublin', 0.046), ('zhaopeng', 0.046), ('johansson', 0.043), ('holder', 0.042), ('yifan', 0.042), ('guided', 0.042), ('document', 0.041), ('lee', 0.041), ('joshi', 0.041), ('minimal', 0.04), ('seneff', 0.039), ('dave', 0.039), ('schler', 0.039), ('paragraphs', 0.039), ('rand', 0.039), ('pages', 0.039), ('visual', 0.038), ('dp', 0.038), ('pt', 0.036), ('razvan', 0.035), ('subtree', 0.035), ('dt', 0.034), ('nns', 0.034), ('encouraging', 0.034), ('hatzivassiloglou', 0.034), ('sequence', 0.033), ('unnecessary', 0.033), ('sydney', 0.032), ('stanford', 0.032), ('neutral', 0.031), ('separating', 0.031), ('yu', 0.031), ('sub', 0.03), ('illustration', 0.03), ('tu', 0.03), ('nguyen', 0.03), ('extract', 0.03), ('marneffe', 0.029), ('denotes', 0.029), ('flat', 0.029), ('nn', 0.028), ('indicator', 0.028), ('parse', 0.028), ('conference', 0.028), ('sp', 0.028), ('subjectivity', 0.028), ('sequential', 0.028), ('association', 0.027), ('linguistic', 0.027), ('vancouver', 0.027), ('suntec', 0.027), ('tags', 0.027), ('twitter', 0.026), ('british', 0.026), ('matsumoto', 0.026), ('terminal', 0.026), ('relations', 0.025), ('structures', 0.025), ('relation', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="115-tfidf-1" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>Author: Zhaopeng Tu ; Yifan He ; Jennifer Foster ; Josef van Genabith ; Qun Liu ; Shouxun Lin</p><p>Abstract: Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. 1</p><p>2 0.33581024 <a title="115-tfidf-2" href="./acl-2012-State-of-the-Art_Kernels_for_Natural_Language_Processing.html">183 acl-2012-State-of-the-Art Kernels for Natural Language Processing</a></p>
<p>Author: Alessandro Moschitti</p><p>Abstract: unkown-abstract</p><p>3 0.29970631 <a title="115-tfidf-3" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>4 0.24138862 <a title="115-tfidf-4" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>5 0.21802711 <a title="115-tfidf-5" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>6 0.18184189 <a title="115-tfidf-6" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<p>7 0.16944019 <a title="115-tfidf-7" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>8 0.16626242 <a title="115-tfidf-8" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>9 0.15386665 <a title="115-tfidf-9" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>10 0.1452297 <a title="115-tfidf-10" href="./acl-2012-Multilingual_Subjectivity_and_Sentiment_Analysis.html">151 acl-2012-Multilingual Subjectivity and Sentiment Analysis</a></p>
<p>11 0.14342684 <a title="115-tfidf-11" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<p>12 0.12264478 <a title="115-tfidf-12" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>13 0.1217566 <a title="115-tfidf-13" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>14 0.11655024 <a title="115-tfidf-14" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>15 0.11631646 <a title="115-tfidf-15" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>16 0.11418148 <a title="115-tfidf-16" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>17 0.10492446 <a title="115-tfidf-17" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>18 0.092081986 <a title="115-tfidf-18" href="./acl-2012-Subgroup_Detector%3A_A_System_for_Detecting_Subgroups_in_Online_Discussions.html">188 acl-2012-Subgroup Detector: A System for Detecting Subgroups in Online Discussions</a></p>
<p>19 0.091170475 <a title="115-tfidf-19" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>20 0.088888958 <a title="115-tfidf-20" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.218), (1, 0.186), (2, 0.039), (3, -0.329), (4, 0.044), (5, -0.142), (6, 0.006), (7, 0.2), (8, -0.282), (9, -0.131), (10, -0.005), (11, -0.132), (12, 0.124), (13, 0.03), (14, 0.073), (15, 0.101), (16, -0.196), (17, 0.124), (18, 0.054), (19, -0.025), (20, 0.001), (21, -0.008), (22, 0.119), (23, -0.032), (24, -0.071), (25, -0.062), (26, -0.021), (27, 0.072), (28, -0.042), (29, -0.094), (30, -0.012), (31, 0.041), (32, 0.019), (33, 0.017), (34, 0.006), (35, -0.022), (36, -0.002), (37, -0.033), (38, 0.05), (39, -0.005), (40, -0.037), (41, -0.045), (42, -0.017), (43, -0.003), (44, -0.014), (45, 0.005), (46, -0.014), (47, -0.031), (48, 0.031), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93776846 <a title="115-lsi-1" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>Author: Zhaopeng Tu ; Yifan He ; Jennifer Foster ; Josef van Genabith ; Qun Liu ; Shouxun Lin</p><p>Abstract: Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. 1</p><p>2 0.79714233 <a title="115-lsi-2" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>3 0.78530359 <a title="115-lsi-3" href="./acl-2012-State-of-the-Art_Kernels_for_Natural_Language_Processing.html">183 acl-2012-State-of-the-Art Kernels for Natural Language Processing</a></p>
<p>Author: Alessandro Moschitti</p><p>Abstract: unkown-abstract</p><p>4 0.6329397 <a title="115-lsi-4" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>Author: Danilo Croce ; Alessandro Moschitti ; Roberto Basili ; Martha Palmer</p><p>Abstract: In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture mean- ingful syntactic/semantic structures, which allows for improving the state-of-the-art.</p><p>5 0.52606958 <a title="115-lsi-5" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<p>Author: Fan Bu ; Hang Li ; Xiaoyan Zhu</p><p>Abstract: Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval. In this paper, we propose a new class of kernel functions, referred to as string re-writing kernel, to address the problem. A string re-writing kernel measures the similarity between two pairs of strings, each pair representing re-writing of a string. It can capture the lexical and structural similarity between two pairs of sentences without the need of constructing syntactic trees. We further propose an instance of string rewriting kernel which can be computed efficiently. Experimental results on benchmark datasets show that our method can achieve better results than state-of-the-art methods on two sentence re-writing learning tasks: paraphrase identification and recognizing textual entailment.</p><p>6 0.51045048 <a title="115-lsi-6" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>7 0.49899375 <a title="115-lsi-7" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>8 0.48325452 <a title="115-lsi-8" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>9 0.45896211 <a title="115-lsi-9" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>10 0.45895037 <a title="115-lsi-10" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<p>11 0.44682601 <a title="115-lsi-11" href="./acl-2012-Multilingual_Subjectivity_and_Sentiment_Analysis.html">151 acl-2012-Multilingual Subjectivity and Sentiment Analysis</a></p>
<p>12 0.3973712 <a title="115-lsi-12" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>13 0.36170614 <a title="115-lsi-13" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>14 0.33280692 <a title="115-lsi-14" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>15 0.32409993 <a title="115-lsi-15" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>16 0.31479549 <a title="115-lsi-16" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>17 0.29414594 <a title="115-lsi-17" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>18 0.28610882 <a title="115-lsi-18" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>19 0.2778452 <a title="115-lsi-19" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>20 0.26966617 <a title="115-lsi-20" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.013), (26, 0.026), (28, 0.023), (30, 0.027), (37, 0.439), (39, 0.066), (74, 0.023), (82, 0.022), (84, 0.019), (85, 0.031), (90, 0.094), (92, 0.045), (94, 0.018), (99, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86856925 <a title="115-lda-1" href="./acl-2012-IRIS%3A_a_Chat-oriented_Dialogue_System_based_on_the_Vector_Space_Model.html">114 acl-2012-IRIS: a Chat-oriented Dialogue System based on the Vector Space Model</a></p>
<p>Author: Rafael E. Banchs ; Haizhou Li</p><p>Abstract: This system demonstration paper presents IRIS (Informal Response Interactive System), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed. 1</p><p>2 0.8647027 <a title="115-lda-2" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<p>Author: Max Whitney ; Anoop Sarkar</p><p>Abstract: Bootstrapping a classifier from a small set of seed rules can be viewed as the propagation of labels between examples via features shared between them. This paper introduces a novel variant of the Yarowsky algorithm based on this view. It is a bootstrapping learning method which uses a graph propagation algorithm with a well defined objective function. The experimental results show that our proposed bootstrapping algorithm achieves state of the art performance or better on several different natural language data sets.</p><p>3 0.84275013 <a title="115-lda-3" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>same-paper 4 0.83963072 <a title="115-lda-4" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>Author: Zhaopeng Tu ; Yifan He ; Jennifer Foster ; Josef van Genabith ; Qun Liu ; Shouxun Lin</p><p>Abstract: Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in document-level sentiment classification. We show that minimal sub-structures extracted from constituency and dependency trees guided by a polarity lexicon show 1.45 pointabsoluteimprovementinaccuracy overa bag-of-words classifier on a widely used sentiment corpus. 1</p><p>5 0.80513901 <a title="115-lda-5" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>Author: Dominick Ng ; James R. Curran</p><p>Abstract: Optimising for one grammatical representation, but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing. We find that this mismatch causes many n-best CCG parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best F-score by 0.7% and reranking accuracy by 0.4%. We also present a comprehensive analysis of errors made by the C&C; CCG parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy.</p><p>6 0.53324604 <a title="115-lda-6" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>7 0.5318743 <a title="115-lda-7" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>8 0.49814266 <a title="115-lda-8" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>9 0.48666662 <a title="115-lda-9" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>10 0.48271808 <a title="115-lda-10" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>11 0.46972179 <a title="115-lda-11" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>12 0.46688706 <a title="115-lda-12" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>13 0.46609294 <a title="115-lda-13" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<p>14 0.46547383 <a title="115-lda-14" href="./acl-2012-State-of-the-Art_Kernels_for_Natural_Language_Processing.html">183 acl-2012-State-of-the-Art Kernels for Natural Language Processing</a></p>
<p>15 0.46341342 <a title="115-lda-15" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>16 0.45809761 <a title="115-lda-16" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>17 0.454189 <a title="115-lda-17" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>18 0.45344594 <a title="115-lda-18" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>19 0.45100507 <a title="115-lda-19" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<p>20 0.45001519 <a title="115-lda-20" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
