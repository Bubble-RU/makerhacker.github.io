<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 acl-2012-Automatically Learning Measures of Child Language Development</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-34" href="#">acl2012-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 acl-2012-Automatically Learning Measures of Child Language Development</h1>
<br/><p>Source: <a title="acl-2012-34-pdf" href="http://aclweb.org/anthology//P/P12/P12-2019.pdf">pdf</a></p><p>Author: Sam Sahakian ; Benjamin Snyder</p><p>Abstract: We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language development, showing improvements in age prediction performance.</p><p>Reference: <a title="acl-2012-34-reference" href="../acl2012_reference/acl-2012-Automatically_Learning_Measures_of_Child_Language_Development_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a new approach for the creation of child language development metrics. [sent-3, score-0.539]
</p><p>2 A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. [sent-4, score-0.851]
</p><p>3 In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. [sent-5, score-0.407]
</p><p>4 We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. [sent-6, score-0.718]
</p><p>5 In both cases we compare our results with established measures of language development, showing improvements in age prediction performance. [sent-7, score-0.301]
</p><p>6 1 Introduction The rapid childhood  development  from a seem-  ingly blank slate to language mastery is a puzzle that linguists and psychologists continue to ponder. [sent-8, score-0.271]
</p><p>7 While the precise mechanism of language learning remains poorly understood, researchers have developed measures of developmental language progress using child speech patterns. [sent-9, score-0.972]
</p><p>8 These metrics provide a means of diagnosing early language disorders. [sent-10, score-0.148]
</p><p>9 Besides this practical benefit, precisely measuring grammatical development is a step towards understanding the underlying language learning process. [sent-11, score-0.11]
</p><p>10 Previous NLP work has sought to automate the calculation of handcrafted developmental metrics proposed by psychologists and linguists. [sent-12, score-0.653]
</p><p>11 If so, how well would such a measure generalize across  children? [sent-16, score-0.086]
</p><p>12 This last question touches on an underlying assumption made in much of the child language literature– that while children progress grammatically at different rates, they follow fixed stages in their development. [sent-17, score-0.682]
</p><p>13 If a developmental index automatically learned from one set of children could be accurately applied to others, it would vindicate this assumption of shared developmental paths. [sent-18, score-1.0]
</p><p>14 Several metrics of language development have been set forth in the psycholinguistics literature. [sent-19, score-0.286]
</p><p>15 , 2006)– a score for individual sentences based on the observed presence of key syntactic structures. [sent-22, score-0.08]
</p><p>16 Today, these hand-crafted metrics persist as measurements of child language development, each taking a slightly different angle to assess the same question: Exactly how much grammatical knowledge does a young learner possess? [sent-23, score-0.598]
</p><p>17 NLP technology has been applied to help automate the otherwise tedious calculation of these measures. [sent-24, score-0.113]
</p><p>18 In response to its limited depth of analysis and the necessity for human supervision in CP, there have since Proce Jedijung, sR oefpu thbeli c50 othf K Aonrneua,a8l -M14e Jtiunlgy o 2f0 t1h2e. [sent-26, score-0.054]
</p><p>19 In this and all following tables, traditional developmental metrics are shaded. [sent-29, score-0.509]
</p><p>20 Likewise, in the ESL domain, Chen and Zechner (201 1) automate the evaluation of syntactic complexity of non-native speech. [sent-32, score-0.151]
</p><p>21 However, the definition of first-language developmental metrics has as yet been left up to human reasoning. [sent-34, score-0.475]
</p><p>22 In this paper, we consider the automatic induction of more accurate developmental metrics using child language data. [sent-35, score-0.934]
</p><p>23 We extract features from longitudinal child language data and conduct two sets of experiments. [sent-36, score-0.543]
</p><p>24 For individual children, we use least-squares regression over our features to predict the age of a held-out language sample. [sent-37, score-0.378]
</p><p>25 We find that on average, existing single metrics of development are outperformed by a weighted combination of our features. [sent-38, score-0.189]
</p><p>26 In our second set of experiments, we investigate whether metrics can be learned across children. [sent-39, score-0.208]
</p><p>27 To do so, we consider a speech sample ordering task. [sent-40, score-0.12]
</p><p>28 We use optimization techniques to learn weight-  ings over features that allow generalization across children. [sent-41, score-0.189]
</p><p>29 Although traditional measures like MLU and D-level perform well on this task, we find that a learned combination of features outperforms any single pre-defined developmental score. [sent-42, score-0.557]
</p><p>30 2  Data  To identify trends in child language learning we need a corpus of child speech samples, which we 96 9,000  Utcreants264,7250 0142 8354295637PRA NS0 eobdiantsearo asm rh7 i Age (months)  Figure  1:  Number  of utterances  across  ages of  each child in our corpus. [sent-43, score-1.644]
</p><p>31 CHILDES is a collection of corpora from many studies of child language based on episodic speech data. [sent-47, score-0.514]
</p><p>32 Since we are interested in development over time, our corpus consists of seven longitudinal studies of individual children. [sent-48, score-0.177]
</p><p>33 Data for each child is grouped and sorted by the child’s age in months, so that we have a single data point for each month in which a child was observed. [sent-49, score-1.127]
</p><p>34 , 2007) and harvest features that should be informative and complementary in assessing grammatical knowledge. [sent-52, score-0.066]
</p><p>35 Beyond the three traditional developmental metrics, we record five additional features. [sent-54, score-0.4]
</p><p>36 We count two of Brown’s (1973) obligatory morphemes articles and contracted auxiliary “be” verbs as well as occurrences of any preposition. [sent-55, score-0.165]
</p><p>37 These counted features are normalized by a child’s total number of utterances at a given age. [sent-56, score-0.105]
</p><p>38 Finally, we include two vocabulary-centric features: Average word fre—  —  tion of linear regression on individual children. [sent-57, score-0.133]
</p><p>39 The lowest error for each child is shown in bold. [sent-58, score-0.488]
</p><p>40 how often a word is used in a stan-  dard corpus) as indicated by CELEX (Baayen et al. [sent-61, score-0.028]
</p><p>41 , 1995), and the child’s ratio of function words (determiners, pronouns, prepositions, auxiliaries and conjunctions) to content words. [sent-62, score-0.028]
</p><p>42 To validate a developmental measure, we rely on the assumption that a perfect metric should increase monotonically over time. [sent-63, score-0.429]
</p><p>43 We therefore calculate Kendall’s Tau coefficient (τ) between an ordering of each child’s speech samples by age, and an ordering by the given scoring metric. [sent-64, score-0.341]
</p><p>44 The τ coefficient is a measure of rank correlation where two identical orderings receive a τ of 1, complete opposite orderings receive a τ of -1, and independent orderings are expected to receive a τ of zero. [sent-65, score-0.663]
</p><p>45 The τ coefficients for each of our 8 features individually applied to the 7 children are shown in Table 1. [sent-66, score-0.193]
</p><p>46 We note that the pre-defined indices of language development MLU, tree depth and D-Level perform the ordering task most accurately. [sent-67, score-0.199]
</p><p>47 To illustrate the degree of variance between children and features, we also include plots of each child’s DLevel and contracted auxiliary “be” usage in Figure 2. [sent-68, score-0.318]
</p><p>48 —  —  3  Experiments  Learning Individual Child Metrics Our first task is to predict the age at which a held-out speech sample was produced, given a set of age-stamped samples from the same child. [sent-69, score-0.356]
</p><p>49 We perform a least squares regression on each child, treating age as the dependent variable, and our features as independent variables. [sent-70, score-0.329]
</p><p>50 Each data set is split into 10 random folds of 90% training and 10% test data. [sent-71, score-0.04]
</p><p>51 8 /0 Content Table 3: Average τ of orderings produced by MLU (the best traditional index) and our learned metric, versus true chronological order. [sent-77, score-0.276]
</p><p>52 achieves lower error than any individual feature by itself. [sent-79, score-0.078]
</p><p>53 Learning General Metrics Across Children  To  produce a universal metric of language development like MLU or D-Level, we train on data pooled across many children. [sent-80, score-0.182]
</p><p>54 For each of 7 folds, a single child’s data is separated as a test set while the remaining children are used for training. [sent-81, score-0.157]
</p><p>55 Since Ross is the only child with samples beyond 62 months, we do not attempt to learn a general measure of language development at these ages, but rather remove these data points. [sent-82, score-0.731]
</p><p>56 Unlike the individual-child case, we do not predict absolute ages based on speech samples, as each child is expected to learn at a different rate. [sent-83, score-0.711]
</p><p>57 Instead, we learn an ordering model which attempts to place each sample in its relative place in time. [sent-84, score-0.118]
</p><p>58 The model computes a score from a weighted quadratic combination of our features and orders the samples based on their computed scores. [sent-85, score-0.168]
</p><p>59 To learn the parameters of the model, we seek to maximize the Kendall τ between true and predicted orderings, summed over the training children. [sent-86, score-0.053]
</p><p>60 We pass this objective function to Nelder-Mead (Nelder and Mead, 1965), a standard gradient-free optimization algorithm. [sent-87, score-0.061]
</p><p>61 NelderMead constructs a simplex at its initial guess of parameter values and iteratively makes small shifts in  the simplex to satisfy a descent condition until a local maximum is reached. [sent-88, score-0.154]
</p><p>62 We report the average Kendall τ achieved by this algorithm over several feature combinations in Table 3. [sent-89, score-0.032]
</p><p>63 Because we modify our data set in this experiment, for comparison we also show the average Kendall τ achieved by MLU on the truncated data. [sent-90, score-0.032]
</p><p>64 4  Discussion  Our first set of experiments verified that we can achieve a decrease in mean squared error over existing metrics in a child-specific age prediction task. [sent-91, score-0.441]
</p><p>65 Figure 2: Child age plotted against D-Level (top) and counts of contracted auxiliary “be” (bottom) with best fit lines. [sent-106, score-0.421]
</p><p>66 Since our regression predicts child age, age in months is plotted on the y-axis. [sent-107, score-0.868]
</p><p>67 in favor of the learned metric by the apparent difficulty of predicting Ross’s age. [sent-108, score-0.123]
</p><p>68 As demonstrated in Figure 2, Ross’s data exhibits major variance, and also includes data from later ages than that of the other children. [sent-109, score-0.144]
</p><p>69 It is well known that MLU’s per-  formance as a measure of linguistic ability quickly drops off with age. [sent-110, score-0.047]
</p><p>70 During our first experiment, we also attempted to capture more nuanced learning curves than the linear case. [sent-111, score-0.035]
</p><p>71 Specifically, we anticipated that learning over time should follow an S-shaped curve. [sent-112, score-0.028]
</p><p>72 This follows from observations of a “fast mapping” spurt in child word learning (Woodward et al. [sent-113, score-0.459]
</p><p>73 , 1994), and the idea that learning must eventually level off as mastery is attained. [sent-114, score-0.065]
</p><p>74 To allow our model to capture non-linear learning rates, we fit logit and quadratic functions to the data. [sent-115, score-0.077]
</p><p>75 With every other child, these functions fit the data to a linear section of the curve and yielded much larger errors than simple linear regression. [sent-117, score-0.037]
</p><p>76 In our second set of experiments, we attempted to learn a general metric across children. [sent-120, score-0.19]
</p><p>77 Here we also achieved positive results with simple methods, just edging out established measures of language de-  velopment. [sent-121, score-0.092]
</p><p>78 The generality of our learned metric supports the hypothesis that children follow similar paths of language development. [sent-122, score-0.28]
</p><p>79 Although our learned solution is slightly more favorable than preexisting metrics, it performs very little learning. [sent-123, score-0.06]
</p><p>80 Using all features, learned parameter weights remain at or extremely close to the starting point of 1. [sent-124, score-0.06]
</p><p>81 98 Through trial and error, we discovered we could improve performance by omitting certain features. [sent-125, score-0.031]
</p><p>82 In Table 3, we report the best discovered feature combination including only two relatively uncorrelated features, MLU and function/content word ratio. [sent-126, score-0.031]
</p><p>83 If downweighting some features yields a better result, we would expect to discover that with our optimization algorithm, but this evidently not the case, perhaps due to our limited sample of 7 children. [sent-127, score-0.097]
</p><p>84 The fact that weights move so little suggests that our best result is stuck in a local maximum. [sent-128, score-0.056]
</p><p>85 To investigate this, we also experimented with Differential Evolution (Storn and Price, 1997) and SVMranking (Joachims, 2002), the former a global optimization technique, and the latter a method de-  veloped specifically to learn orderings. [sent-129, score-0.114]
</p><p>86 Although these algorithms are more willing to adjust parameter weights and theoretically should not get stuck in local maxima, they are still edged out in performance by Nelder-Mead. [sent-130, score-0.056]
</p><p>87 It may be that the early stopping of Nelder-Mead serves as a sort of smoothing in this very small data-set of 7 children. [sent-131, score-0.039]
</p><p>88 Our improvements over hand-crafted measures of language development show promise. [sent-132, score-0.141]
</p><p>89 In the case of individual children, we outperform existing measures of development, especially past the early stages of development when MLU ceases to correlate with age. [sent-133, score-0.292]
</p><p>90 Our attempts to learn a metric across children met with more limited success. [sent-134, score-0.312]
</p><p>91 However, when we restricted our regression to two of the least correlated features, MLU and the function/content word ratio, we were able to beat manually created metrics. [sent-135, score-0.084]
</p><p>92 These results suggest that more sophisticated models and techniques combined with more data could lead to more accurate metrics as well as insights into the language learning process. [sent-136, score-0.109]
</p><p>93 Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. [sent-173, score-0.133]
</p><p>94 Automatic measurement of syntactic complexity in child language acquisition. [sent-209, score-0.567]
</p><p>95 Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults. [sent-227, score-0.08]
</p><p>96 Differential evolution–a simple and efficient heuristic for global optimization over continuous spaces. [sent-262, score-0.061]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('child', 0.459), ('developmental', 0.366), ('mlu', 0.355), ('age', 0.209), ('children', 0.157), ('orderings', 0.154), ('ages', 0.144), ('childes', 0.141), ('bloom', 0.112), ('metrics', 0.109), ('psycholinguistics', 0.097), ('rosenberg', 0.097), ('ross', 0.096), ('samples', 0.092), ('kendall', 0.091), ('automate', 0.084), ('contracted', 0.084), ('macwhinney', 0.084), ('regression', 0.084), ('development', 0.08), ('simplex', 0.077), ('sagae', 0.074), ('months', 0.068), ('ordering', 0.065), ('abbeduto', 0.065), ('dlevel', 0.065), ('ipsyn', 0.065), ('mastery', 0.065), ('psychologists', 0.065), ('storn', 0.065), ('wisconsin', 0.065), ('woodward', 0.065), ('metric', 0.063), ('brown', 0.062), ('measures', 0.061), ('optimization', 0.061), ('learned', 0.06), ('differential', 0.056), ('baayen', 0.056), ('celex', 0.056), ('nelder', 0.056), ('nina', 0.056), ('squared', 0.056), ('stuck', 0.056), ('speech', 0.055), ('depth', 0.054), ('learn', 0.053), ('psychology', 0.052), ('hood', 0.051), ('covington', 0.051), ('cp', 0.051), ('madison', 0.051), ('productive', 0.051), ('index', 0.051), ('individual', 0.049), ('competence', 0.048), ('longitudinal', 0.048), ('plotted', 0.048), ('measure', 0.047), ('auxiliary', 0.043), ('measurement', 0.041), ('receive', 0.04), ('counted', 0.04), ('quadratic', 0.04), ('folds', 0.04), ('early', 0.039), ('across', 0.039), ('morphemes', 0.038), ('mean', 0.038), ('fit', 0.037), ('features', 0.036), ('complexity', 0.036), ('rates', 0.036), ('lavie', 0.035), ('attempted', 0.035), ('stages', 0.035), ('traditional', 0.034), ('coefficient', 0.034), ('variance', 0.034), ('rapid', 0.033), ('evolution', 0.033), ('conversational', 0.032), ('average', 0.032), ('progress', 0.031), ('syntactic', 0.031), ('discovered', 0.031), ('established', 0.031), ('scoring', 0.03), ('grammatical', 0.03), ('calculation', 0.029), ('utterance', 0.029), ('error', 0.029), ('utterances', 0.029), ('dard', 0.028), ('zechner', 0.028), ('anticipated', 0.028), ('auxiliaries', 0.028), ('blank', 0.028), ('ceases', 0.028), ('chronological', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="34-tfidf-1" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>Author: Sam Sahakian ; Benjamin Snyder</p><p>Abstract: We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language development, showing improvements in age prediction performance.</p><p>2 0.087829851 <a title="34-tfidf-2" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>Author: Mark Johnson ; Katherine Demuth ; Michael Frank</p><p>Abstract: This paper uses an unsupervised model of grounded language acquisition to study the role that social cues play in language acquisition. The input to the model consists of (orthographically transcribed) child-directed utterances accompanied by the set of objects present in the non-linguistic context. Each object is annotated by social cues, indicating e.g., whether the caregiver is looking at or touching the object. We show how to model the task of inferring which objects are being talked about (and which words refer to which objects) as standard grammatical inference, and describe PCFG-based unigram models and adaptor grammar-based collocation models for the task. Exploiting social cues improves the performance of all models. Our models learn the relative importance of each social cue jointly with word-object mappings and collocation structure, consis- tent with the idea that children could discover the importance of particular social information sources during word learning.</p><p>3 0.068338022 <a title="34-tfidf-3" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>Author: Prasanth Kolachina ; Nicola Cancedda ; Marc Dymetman ; Sriram Venkatapathy</p><p>Abstract: Parallel data in the domain of interest is the key resource when training a statistical machine translation (SMT) system for a specific purpose. Since ad-hoc manual translation can represent a significant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. In this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data. We consider two scenarios, 1) Monolingual samples in the source and target languages are available and 2) An additional small amount of parallel corpus is also available. We propose methods for predicting learning curves in both these scenarios.</p><p>4 0.068164393 <a title="34-tfidf-4" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>5 0.064259008 <a title="34-tfidf-5" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>6 0.062298276 <a title="34-tfidf-6" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>7 0.058230422 <a title="34-tfidf-7" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>8 0.057911143 <a title="34-tfidf-8" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>9 0.057309143 <a title="34-tfidf-9" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>10 0.056843068 <a title="34-tfidf-10" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>11 0.055404864 <a title="34-tfidf-11" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>12 0.05521699 <a title="34-tfidf-12" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>13 0.050755952 <a title="34-tfidf-13" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>14 0.050568093 <a title="34-tfidf-14" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>15 0.050121095 <a title="34-tfidf-15" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>16 0.049847674 <a title="34-tfidf-16" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>17 0.049200799 <a title="34-tfidf-17" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>18 0.049040273 <a title="34-tfidf-18" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>19 0.043990999 <a title="34-tfidf-19" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>20 0.043638192 <a title="34-tfidf-20" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, 0.018), (2, -0.059), (3, -0.007), (4, -0.041), (5, 0.011), (6, 0.0), (7, 0.009), (8, -0.012), (9, 0.014), (10, -0.041), (11, -0.008), (12, -0.046), (13, 0.025), (14, -0.013), (15, -0.011), (16, 0.027), (17, -0.003), (18, -0.029), (19, -0.045), (20, 0.057), (21, -0.107), (22, 0.034), (23, -0.058), (24, -0.038), (25, 0.062), (26, 0.026), (27, 0.108), (28, 0.092), (29, 0.053), (30, -0.078), (31, 0.027), (32, 0.068), (33, -0.007), (34, 0.045), (35, 0.023), (36, -0.063), (37, -0.063), (38, -0.098), (39, -0.071), (40, 0.131), (41, 0.052), (42, 0.113), (43, 0.013), (44, 0.148), (45, 0.032), (46, 0.059), (47, -0.032), (48, -0.14), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93163025 <a title="34-lsi-1" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>Author: Sam Sahakian ; Benjamin Snyder</p><p>Abstract: We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language development, showing improvements in age prediction performance.</p><p>2 0.54668236 <a title="34-lsi-2" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>Author: Kevin Duh ; Katsuhito Sudoh ; Xianchao Wu ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</p><p>3 0.51556331 <a title="34-lsi-3" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>Author: Prasanth Kolachina ; Nicola Cancedda ; Marc Dymetman ; Sriram Venkatapathy</p><p>Abstract: Parallel data in the domain of interest is the key resource when training a statistical machine translation (SMT) system for a specific purpose. Since ad-hoc manual translation can represent a significant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. In this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data. We consider two scenarios, 1) Monolingual samples in the source and target languages are available and 2) An additional small amount of parallel corpus is also available. We propose methods for predicting learning curves in both these scenarios.</p><p>4 0.45917752 <a title="34-lsi-4" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>Author: Boxing Chen ; Roland Kuhn ; Samuel Larkin</p><p>Abstract: Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT , a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves 1 consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties). 1</p><p>5 0.45781052 <a title="34-lsi-5" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>Author: Shay B. Cohen ; Karl Stratos ; Michael Collins ; Dean P. Foster ; Lyle Ungar</p><p>Abstract: We introduce a spectral learning algorithm for latent-variable PCFGs (Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides consistent parameter estimates.</p><p>6 0.43197343 <a title="34-lsi-6" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>7 0.42805994 <a title="34-lsi-7" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>8 0.42427278 <a title="34-lsi-8" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>9 0.40944606 <a title="34-lsi-9" href="./acl-2012-You_Had_Me_at_Hello%3A_How_Phrasing_Affects_Memorability.html">218 acl-2012-You Had Me at Hello: How Phrasing Affects Memorability</a></p>
<p>10 0.38559416 <a title="34-lsi-10" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>11 0.37105054 <a title="34-lsi-11" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>12 0.35448059 <a title="34-lsi-12" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>13 0.35080895 <a title="34-lsi-13" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>14 0.34993556 <a title="34-lsi-14" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>15 0.34806305 <a title="34-lsi-15" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>16 0.34480032 <a title="34-lsi-16" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>17 0.34340766 <a title="34-lsi-17" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>18 0.34079367 <a title="34-lsi-18" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>19 0.33694926 <a title="34-lsi-19" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>20 0.33311975 <a title="34-lsi-20" href="./acl-2012-CSNIPER_-_Annotation-by-query_for_Non-canonical_Constructions_in_Large_Corpora.html">44 acl-2012-CSNIPER - Annotation-by-query for Non-canonical Constructions in Large Corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.024), (26, 0.039), (28, 0.042), (30, 0.041), (37, 0.024), (39, 0.045), (56, 0.355), (59, 0.011), (74, 0.044), (82, 0.029), (84, 0.026), (85, 0.037), (90, 0.079), (92, 0.073), (94, 0.019), (99, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72753066 <a title="34-lda-1" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>Author: Sam Sahakian ; Benjamin Snyder</p><p>Abstract: We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language development, showing improvements in age prediction performance.</p><p>2 0.50100541 <a title="34-lda-2" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>3 0.37666309 <a title="34-lda-3" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>Author: Bevan Jones ; Mark Johnson ; Sharon Goldwater</p><p>Abstract: Many semantic parsing models use tree transformations to map between natural language and meaning representation. However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.</p><p>4 0.37265223 <a title="34-lda-4" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart Shieber</p><p>Abstract: We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data.</p><p>5 0.37161708 <a title="34-lda-5" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>Author: Jonathan Berant ; Ido Dagan ; Meni Adler ; Jacob Goldberger</p><p>Abstract: Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm.</p><p>6 0.36810961 <a title="34-lda-6" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>7 0.36685219 <a title="34-lda-7" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>8 0.36592966 <a title="34-lda-8" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>9 0.36494943 <a title="34-lda-9" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>10 0.36408663 <a title="34-lda-10" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>11 0.36388463 <a title="34-lda-11" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>12 0.36178666 <a title="34-lda-12" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>13 0.36106908 <a title="34-lda-13" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>14 0.36088586 <a title="34-lda-14" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>15 0.35865921 <a title="34-lda-15" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>16 0.35810804 <a title="34-lda-16" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>17 0.3580814 <a title="34-lda-17" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>18 0.35736492 <a title="34-lda-18" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>19 0.35735038 <a title="34-lda-19" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>20 0.35729948 <a title="34-lda-20" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
