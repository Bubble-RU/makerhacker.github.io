<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-117" href="#">acl2012-117</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</h1>
<br/><p>Source: <a title="acl-2012-117-pdf" href="http://aclweb.org/anthology//P/P12/P12-1092.pdf">pdf</a></p><p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>Reference: <a title="acl-2012-117-reference" href="../acl2012_reference/acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. [sent-6, score-0.348]
</p><p>2 However, most of these models are built with only local context and one representation per word. [sent-7, score-0.301]
</p><p>3 This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. [sent-8, score-0.586]
</p><p>4 We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. [sent-9, score-1.94]
</p><p>5 We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. [sent-10, score-0.785]
</p><p>6 1  1 Introduction Vector-space models (VSM) represent word meanings with vectors that capture semantic and syntactic information of words. [sent-11, score-0.372]
</p><p>7 These representations can be used to induce similarity measures by computing distances between the vectors, leading to many useful applications, such as information retrieval (Manning et al. [sent-12, score-0.261]
</p><p>8 1The dataset and word vectors can be downloaded at http : / / ai . [sent-15, score-0.286]
</p><p>9 873 Despite their usefulness, most VSMs share a common problem that each word is only represented with one vector, which clearly fails to capture homonymy and polysemy. [sent-18, score-0.219]
</p><p>10 Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. [sent-19, score-0.402]
</p><p>11 While many approaches use local contexts to disambiguate word meaning, global contexts can also provide useful topical information (Ng and Zelle, 1997). [sent-21, score-0.587]
</p><p>12 Several studies in psychology have also shown that global context can help language comprehension (Hess et al. [sent-22, score-0.337]
</p><p>13 We introduce a new neural-network-based language model that distinguishes and uses both local and global context via a joint training objective. [sent-25, score-0.401]
</p><p>14 The model learns word representations that better capture the semantics of words, while still keeping syntactic information. [sent-26, score-0.336]
</p><p>15 These improved representations can be used to represent contexts for clustering word instances, which is used in the multi-prototype version of our model that accounts for words with multiple senses. [sent-27, score-0.467]
</p><p>16 , 2001) dataset that includes human similarity judgments on pairs of words, showing that combining both local and global context outperforms using only local or global context alone, and is competitive with stateof-the-art methods. [sent-29, score-1.304]
</p><p>17 However, one limitation of this  evaluation is that the human judgments are on pairs ProceedJienjgus, R ofep thueb 5lic0t hof A Knonrueaa,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-30, score-0.206]
</p><p>18 c so2c0ia1t2io Ans fsoorc Ciatoiomnp fuotart Cioonmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi8c 7s3–882, Local ContextscoreGlobal Context  Figure 1: An overview of our neural language model. [sent-32, score-0.227]
</p><p>19 The model makes use ofboth local and global context to compute a score that should be large for the actual next word (bank in the example), compared to the score for other words. [sent-33, score-0.578]
</p><p>20 When word meaning is still ambiguous given local context, information in global context can help disambiguation. [sent-34, score-0.558]
</p><p>21 Since word interpretation in context is important especially for homonymous and polysemous words, we introduce a new dataset with human judgments on similarity between pairs of words in sentential context. [sent-36, score-0.992]
</p><p>22 To capture interesting word pairs, we sample different senses of words using WordNet (Miller, 1995). [sent-37, score-0.214]
</p><p>23 We show that our multi-prototype model improves upon the single-prototype version and outperforms other neural language models and baselines on this dataset. [sent-39, score-0.307]
</p><p>24 2  Global Context-Aware Neural Language Model  In this section, we describe the training objective of our model, followed by a description of the neural network architecture, ending with a brief description of our model’s training method. [sent-40, score-0.317]
</p><p>25 1 Training Objective Our model jointly learns word representations while learning to discriminate the next word given a short word sequence (local context) and the document (global context) in which the word sequence occurs. [sent-42, score-0.655]
</p><p>26 Because our goal is to learn useful word representations and not the probability of the next word given previous words (which prohibits looking ahead), our model can utilize the entire document to provide 874 global context. [sent-43, score-0.612]
</p><p>27 Given a word sequence s and document d in which the sequence occurs, our goal is to discriminate the correct last word in s from other random words. [sent-44, score-0.268]
</p><p>28 We compute scores g(s, d) and g(sw, d) where is s with the last word replaced by word w, and g(·, ·) is the scoring function that represents the anenudra gl( ·n,e·)tw iso trhkes sucsoedrin. [sent-45, score-0.206]
</p><p>29 The scoring components are computed by two neural networks, one capturing local context and the other global context, as shown in Figure 1. [sent-49, score-0.628]
</p><p>30 The score of local context uses the local word sequence s. [sent-51, score-0.515]
</p><p>31 To compute the score of local context, scorel, we use a neural network with one hidden layer: a1 scorel  = =  f(W1 [x1; x2; . [sent-58, score-0.544]
</p><p>32 For the score of the global context, we represent the document also as an ordered list of word embeddings, d = (d1, d2, . [sent-65, score-0.345]
</p><p>33 875 The final score is the sum of the two scores:  score =  scorel  + scoreg  (7)  The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document. [sent-73, score-0.657]
</p><p>34 Note that Collobert and Weston (2008)’s language model corresponds to the network using only local context. [sent-74, score-0.207]
</p><p>35 We found that word embeddings move to good positions in the vector space faster when using mini-batch L-BFGS (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochas-  tic gradient descent. [sent-79, score-0.682]
</p><p>36 Moreover, using all contexts of a homonymous or polysemous word to build a single prototype could hurt the representation, which cannot represent any one of the meanings well as it is influenced by all meanings of the word. [sent-83, score-0.64]
</p><p>37 Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multiprototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. [sent-84, score-0.263]
</p><p>38 We present a way to use our learned  single-prototype embeddings to represent each context window, which can then be used by clustering to perform word sense discrimination (Sch u¨tze, 1998). [sent-86, score-0.799]
</p><p>39 In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). [sent-87, score-0.406]
</p><p>40 Each context is represented by a weighted average of the context words’ vectors, where again, we use idf-weighting as the weighting function, similar to the document context representation described in Section 2. [sent-88, score-0.485]
</p><p>41 We then use spherical k-means to cluster these context representations, which has been shown to model semantic relations well (Dhillon and Modha, 2001). [sent-90, score-0.257]
</p><p>42 Finally, each word occurrence in the corpus is re-labeled to its associated cluster and is used to train the word representation for that cluster. [sent-91, score-0.273]
</p><p>43 The similarity measure can be computed in absence of context by assuming uniform p(c, w, i) over i. [sent-93, score-0.26]
</p><p>44 4  Experiments  In this section, we first present a qualitative analysis comparing the nearest neighbors of our model’s embeddings with those of others, showing our embeddings better capture the semantics of words, with the use of global context. [sent-94, score-1.376]
</p><p>45 Our model also improves the correlation with human judgments on a word similarity task. [sent-95, score-0.363]
</p><p>46 Because word interpretation in context is 876  important, we introduce a new dataset with human judgments on similarity of pairs of words in sentential context. [sent-96, score-0.803]
</p><p>47 We chose Wikipedia as the corpus to train all models because of its wide range of topics and word usages, and its clean organization of document by topic. [sent-98, score-0.208]
</p><p>48 We use 10-word windows of text as the local context, 100 hidden units, and no  weight regularization for both neural networks. [sent-107, score-0.344]
</p><p>49 1 Qualitative Evaluations In order to show that our model learns more semantic word representations with global context, we give the nearest neighbors of our single-prototype model versus C&W;’s, which only uses local context. [sent-110, score-0.753]
</p><p>50 The nearest neighbors of a word are computed by comparing the cosine similarity between the center word and all other words in the dictionary. [sent-111, score-0.593]
</p><p>51 Table 1 shows the nearest neighbors of some words. [sent-112, score-0.209]
</p><p>52 The nearest neighbors of “market” that C&W;’s embeddings give are more constrained by the syntactic constraint that words in plural form are only close to other words in plural form, whereas our model captures that the singular and plural forms of a word are similar in meaning. [sent-113, score-1.024]
</p><p>53 Other examples show that our model induces nearest neighbors that better capture semantics. [sent-114, score-0.261]
</p><p>54 Table 2 shows the nearest neighbors of our model using the multi-prototype approach. [sent-115, score-0.209]
</p><p>55 meanings of a word into separate groups, allowing our model to learn multiple meaningful representations of a word. [sent-120, score-0.332]
</p><p>56 2  WordSim-353  A standard dataset for evaluating vector-space models is the WordSim-353 dataset (Finkelstein et al. [sent-122, score-0.235]
</p><p>57 Each pair is presented without context and associated with 13 to 16 human judgments on similarity and relatedness on a scale from 0 to 10. [sent-124, score-0.401]
</p><p>58 Table 3 shows our results compared to previous methods, including C&W;’s language model and the hierarchical log-bilinear (HLBL) model (Mnih and Hinton, 2008), which is a probabilistic, linear neural model. [sent-128, score-0.227]
</p><p>59 These embeddings were trained on the smaller corpus RCV1 that contains one year of Reuters English newswire, and show similar cor-  relations on the dataset. [sent-131, score-0.465]
</p><p>60 9i45m-3 ,  showing our model’s improvement over previous neural models for learning word embeddings. [sent-138, score-0.415]
</p><p>61 C&W;* is the word embeddings trained and provided by C&W. [sent-139, score-0.568]
</p><p>62 Our model is able to learn more semantic word embeddings and noticeably improves upon C&W;’s model. [sent-143, score-0.568]
</p><p>63 3 New Dataset: Word Similarity in Context The many previous datasets that associate human judgments on similarity between pairs of words, such as WordSim-353, MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have helped to advance the development of vectorspace models. [sent-152, score-0.325]
</p><p>64 However, common to all datasets is that similarity scores are given to pairs of words in isolation. [sent-153, score-0.243]
</p><p>65 This is problematic because the mean-  ings of homonymous and polysemous words depend highly on the words’ contexts. [sent-154, score-0.297]
</p><p>66 It is unclear how this variation in meaning is accounted for in human judgments of words presented without context. [sent-157, score-0.254]
</p><p>67 1 Dataset Construction Our procedure of constructing the dataset consists of three steps: 1) select a list a words, 2) for each word, select another word to form a pair, 3) for each word in a pair, find a sentential context. [sent-163, score-0.381]
</p><p>68 (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word’s chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. [sent-172, score-0.234]
</p><p>69 We randomly select a word from this set of words as the second word in the pair. [sent-173, score-0.265]
</p><p>70 We end up with pairs of words as well as the one chosen synset for each word in the pairs. [sent-176, score-0.353]
</p><p>71 AvgSim calculates similarity with each prototype contributing equally, while AvgSimC weighs the prototypes according to probability of the word belonging to that prototype’s cluster. [sent-185, score-0.381]
</p><p>72 word usages that correspond to the chosen synset, we first construct a set of related words of the chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. [sent-186, score-0.4]
</p><p>73 Single-prototype models would give the max similarity score for those pairs, which can be problematic depending on the words’ contexts. [sent-191, score-0.248]
</p><p>74 This dataset requires models to examine context when determining word meaning. [sent-192, score-0.383]
</p><p>75 We obtained a total of 2,003 word pairs and their sentential contexts. [sent-197, score-0.247]
</p><p>76 We compare against the following baselines: tf-idf represents words in a word-word matrix capturing co-occurrence counts in all 10-word context windows. [sent-205, score-0.2]
</p><p>77 We tried the same multi-prototype approach and used spherical k-means3 to cluster the contexts using tf-idf representations, but obtained lower numbers than singleprototype (55. [sent-208, score-0.277]
</p><p>78 We then tried using pruned tf-idf representations on contexts with our clustering assignments (included in Table 5), but still got results worse than the single-prototype version of the pruned tf-idf model (60. [sent-210, score-0.417]
</p><p>79 This suggests that the pruned tf-idf representations might be more susceptible to noise or mistakes in context clustering. [sent-212, score-0.339]
</p><p>80 By utilizing global context, our model outperforms C&W;’s vectors and the above baselines on this dataset. [sent-213, score-0.267]
</p><p>81 With multiple representations per word, we show that the multi-prototype approach can improve over the single-prototype version without using context (62. [sent-214, score-0.283]
</p><p>82 words which model words’ similarity, this type of models addresses the data sparseness problem that n-gram models encounter when large contexts are used. [sent-226, score-0.257]
</p><p>83 Most of these models used relative local contexts of between 2 to 10 words. [sent-227, score-0.272]
</p><p>84 Schwenk and Gauvain (2002) tried to incorporate larger context by combining partial parses of past word sequences and a neural language model. [sent-228, score-0.471]
</p><p>85 Our model uses a similar neural network architecture as these models and uses the ranking-loss training objective proposed by Collobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings. [sent-230, score-0.918]
</p><p>86 Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al. [sent-231, score-0.838]
</p><p>87 However, they have not been directly evaluated on word similarity tasks, which are important for tasks such as information retrieval and summa-  rization. [sent-236, score-0.222]
</p><p>88 Our experiments show that our word embeddings are competitive in word similarity tasks. [sent-237, score-0.829]
</p><p>89 Most of the previous vector-space models use a single vector to represent a word even though many words have multiple meanings. [sent-238, score-0.205]
</p><p>90 , 2009), while Sch u¨tze (1998) used clustering of contexts to perform word sense discrimination. [sent-240, score-0.266]
</p><p>91 , 2011) present models for constructing word representations that deal with context. [sent-244, score-0.288]
</p><p>92 (201 1) and Dinu and Lapata (2010) evaluated word similarity in context with a modified task where systems are to rerank gold-standard paraphrase candidates given the SemEval 2007 Lexical Substitution Task dataset. [sent-251, score-0.363]
</p><p>93 6  Conclusion  We presented a new neural network architecture that learns more semantic word representations by using both local and global context in learning. [sent-253, score-1.056]
</p><p>94 These learned word embeddings can be used to represent word contexts as low-dimensional weighted average vectors, which are then clustered to form different meaning groups and used to learn multi-prototype vectors. [sent-254, score-0.837]
</p><p>95 We introduced a new dataset with human  judgments on similarity between pairs of words in context, so as to evaluate model’s abilities to capture homonymy and polysemy of words in context. [sent-255, score-0.655]
</p><p>96 Our new multi-prototype neural language model outperforms previous neural models and competitive baselines on this new dataset. [sent-256, score-0.573]
</p><p>97 A unified architecture for natural language processing: deep neural networks with multitask learning. [sent-266, score-0.281]
</p><p>98 Effects of global and local context on lexical processing during language comprehension. [sent-316, score-0.401]
</p><p>99 The acquisition of word meaning through global lexical cooccurrences. [sent-320, score-0.3]
</p><p>100 Parsing natural scenes and natural language with recursive neural networks. [sent-427, score-0.265]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('embeddings', 0.465), ('neural', 0.227), ('reisinger', 0.182), ('global', 0.143), ('representations', 0.142), ('context', 0.141), ('judgments', 0.141), ('socher', 0.127), ('similarity', 0.119), ('collobert', 0.117), ('local', 0.117), ('contexts', 0.112), ('neighbors', 0.109), ('word', 0.103), ('nearest', 0.1), ('avgsimc', 0.098), ('homonymous', 0.098), ('mnih', 0.098), ('prototypes', 0.097), ('weston', 0.097), ('dataset', 0.096), ('polysemous', 0.091), ('rh', 0.091), ('mooney', 0.091), ('network', 0.09), ('vectors', 0.087), ('meanings', 0.087), ('finkelstein', 0.085), ('dhillon', 0.085), ('embedding', 0.082), ('sentential', 0.079), ('xm', 0.078), ('layer', 0.075), ('rubenstein', 0.073), ('scoreg', 0.073), ('scorel', 0.073), ('usages', 0.069), ('cluster', 0.067), ('synset', 0.066), ('pairs', 0.065), ('bat', 0.064), ('homonymy', 0.064), ('turian', 0.064), ('miller', 0.062), ('prototype', 0.062), ('document', 0.062), ('chosen', 0.06), ('words', 0.059), ('schwenk', 0.058), ('activation', 0.058), ('gabrilovich', 0.058), ('pruned', 0.056), ('meaning', 0.054), ('architecture', 0.054), ('psychology', 0.053), ('capture', 0.052), ('clustering', 0.051), ('stroudsburg', 0.05), ('andriy', 0.049), ('avgsim', 0.049), ('corrupt', 0.049), ('dinu', 0.049), ('gauvain', 0.049), ('goodenough', 0.049), ('hess', 0.049), ('holonyms', 0.049), ('klapaftis', 0.049), ('manandhar', 0.049), ('meronyms', 0.049), ('shaoul', 0.049), ('singleprototype', 0.049), ('spherical', 0.049), ('westbury', 0.049), ('problematic', 0.049), ('wikipedia', 0.048), ('synsets', 0.047), ('plural', 0.043), ('models', 0.043), ('autoencoders', 0.043), ('pennington', 0.043), ('ford', 0.043), ('thater', 0.043), ('reddy', 0.043), ('evgeniy', 0.043), ('suresh', 0.043), ('connectionist', 0.043), ('showing', 0.042), ('sch', 0.042), ('learns', 0.039), ('vsm', 0.039), ('emami', 0.039), ('erk', 0.039), ('discrimination', 0.039), ('tellex', 0.039), ('competitive', 0.039), ('recursive', 0.038), ('ng', 0.038), ('ny', 0.037), ('score', 0.037), ('baselines', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="117-tfidf-1" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>2 0.12708808 <a title="117-tfidf-2" href="./acl-2012-Deep_Learning_for_NLP_%28without_Magic%29.html">69 acl-2012-Deep Learning for NLP (without Magic)</a></p>
<p>Author: Richard Socher ; Yoshua Bengio ; Christopher D. Manning</p><p>Abstract: unkown-abstract</p><p>3 0.11635908 <a title="117-tfidf-3" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>Author: Geoffrey Zweig ; John C. Platt ; Christopher Meek ; Christopher J.C. Burges ; Ainur Yessenalina ; Qiang Liu</p><p>Abstract: This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</p><p>4 0.10740124 <a title="117-tfidf-4" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>Author: Limin Yao ; Sebastian Riedel ; Andrew McCallum</p><p>Abstract: To discover relation types from text, most methods cluster shallow or syntactic patterns of relation mentions, but consider only one possible sense per pattern. In practice this assumption is often violated. In this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns. In particular, we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features. We merge these sense clusters into semantic relations using hierarchical agglomerative clustering. We compare against several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial.</p><p>5 0.081966467 <a title="117-tfidf-5" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<p>Author: Ingrid Falk ; Claire Gardent ; Jean-Charles Lamirel</p><p>Abstract: We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an Fmeasure of 0.70.</p><p>6 0.080332182 <a title="117-tfidf-6" href="./acl-2012-Fast_Online_Lexicon_Learning_for_Grounded_Language_Acquisition.html">93 acl-2012-Fast Online Lexicon Learning for Grounded Language Acquisition</a></p>
<p>7 0.075679101 <a title="117-tfidf-7" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>8 0.074741848 <a title="117-tfidf-8" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>9 0.073319107 <a title="117-tfidf-9" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>10 0.071250632 <a title="117-tfidf-10" href="./acl-2012-Distributional_Semantics_in_Technicolor.html">76 acl-2012-Distributional Semantics in Technicolor</a></p>
<p>11 0.071240857 <a title="117-tfidf-11" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>12 0.070946172 <a title="117-tfidf-12" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>13 0.070806399 <a title="117-tfidf-13" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>14 0.068464935 <a title="117-tfidf-14" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>15 0.068365991 <a title="117-tfidf-15" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>16 0.067040704 <a title="117-tfidf-16" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>17 0.064704277 <a title="117-tfidf-17" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>18 0.064408787 <a title="117-tfidf-18" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>19 0.064113043 <a title="117-tfidf-19" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>20 0.0639681 <a title="117-tfidf-20" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.217), (1, 0.069), (2, 0.007), (3, 0.007), (4, 0.002), (5, 0.106), (6, -0.011), (7, 0.046), (8, -0.034), (9, -0.033), (10, 0.031), (11, -0.026), (12, 0.059), (13, 0.085), (14, -0.074), (15, 0.067), (16, 0.06), (17, 0.056), (18, -0.062), (19, 0.041), (20, 0.061), (21, -0.09), (22, -0.105), (23, 0.052), (24, 0.075), (25, 0.032), (26, -0.018), (27, -0.08), (28, 0.034), (29, 0.118), (30, -0.008), (31, -0.115), (32, -0.016), (33, -0.066), (34, -0.031), (35, 0.023), (36, 0.119), (37, -0.066), (38, -0.024), (39, 0.129), (40, 0.081), (41, -0.024), (42, -0.114), (43, 0.036), (44, 0.043), (45, -0.023), (46, -0.01), (47, -0.079), (48, 0.002), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93766046 <a title="117-lsi-1" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>2 0.73797035 <a title="117-lsi-2" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>Author: Geoffrey Zweig ; John C. Platt ; Christopher Meek ; Christopher J.C. Burges ; Ainur Yessenalina ; Qiang Liu</p><p>Abstract: This paper studies the problem of sentencelevel semantic coherence by answering SATstyle sentence completion questions. These questions test the ability of algorithms to distinguish sense from nonsense based on a variety of sentence-level phenomena. We tackle the problem with two approaches: methods that use local lexical information, such as the n-grams of a classical language model; and methods that evaluate global coherence, such as latent semantic analysis. We evaluate these methods on a suite of practice SAT questions, and on a recently released sentence completion task based on data taken from five Conan Doyle novels. We find that by fusing local and global information, we can exceed 50% on this task (chance baseline is 20%), and we suggest some avenues for further research.</p><p>3 0.60580981 <a title="117-lsi-3" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: Sentence Similarity is the process of computing a similarity score between two sentences. Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences. In this paper, we show that by carefully handling words that are not in the sentences (missing words), we can train a reliable latent variable model on sentences. In the process, we propose a new evaluation framework for sentence similarity: Concept Definition Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.</p><p>4 0.59003645 <a title="117-lsi-4" href="./acl-2012-Deep_Learning_for_NLP_%28without_Magic%29.html">69 acl-2012-Deep Learning for NLP (without Magic)</a></p>
<p>Author: Richard Socher ; Yoshua Bengio ; Christopher D. Manning</p><p>Abstract: unkown-abstract</p><p>5 0.56546044 <a title="117-lsi-5" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>Author: Rada Mihalcea ; Vivi Nastase</p><p>Abstract: In this paper we introduce the novel task of “word epoch disambiguation,” defined as the problem of identifying changes in word usage over time. Through experiments run using word usage examples collected from three major periods of time (1800, 1900, 2000), we show that the task is feasible, and significant differences can be observed between occurrences of words in different periods of time.</p><p>6 0.54429841 <a title="117-lsi-6" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>7 0.51428938 <a title="117-lsi-7" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>8 0.51250827 <a title="117-lsi-8" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<p>9 0.49216512 <a title="117-lsi-9" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>10 0.49113426 <a title="117-lsi-10" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>11 0.48797667 <a title="117-lsi-11" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>12 0.48267612 <a title="117-lsi-12" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>13 0.46029556 <a title="117-lsi-13" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<p>14 0.45790321 <a title="117-lsi-14" href="./acl-2012-Distributional_Semantics_in_Technicolor.html">76 acl-2012-Distributional Semantics in Technicolor</a></p>
<p>15 0.45538574 <a title="117-lsi-15" href="./acl-2012-Fast_Online_Lexicon_Learning_for_Grounded_Language_Acquisition.html">93 acl-2012-Fast Online Lexicon Learning for Grounded Language Acquisition</a></p>
<p>16 0.45286667 <a title="117-lsi-16" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>17 0.45135346 <a title="117-lsi-17" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>18 0.44855124 <a title="117-lsi-18" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<p>19 0.44595307 <a title="117-lsi-19" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>20 0.44532681 <a title="117-lsi-20" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.025), (26, 0.039), (28, 0.037), (30, 0.05), (37, 0.045), (39, 0.044), (56, 0.236), (59, 0.056), (74, 0.035), (82, 0.026), (84, 0.036), (85, 0.038), (90, 0.132), (92, 0.075), (94, 0.014), (99, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87101835 <a title="117-lda-1" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>Author: Sam Sahakian ; Benjamin Snyder</p><p>Abstract: We propose a new approach for the creation of child language development metrics. A set of linguistic features is computed on child speech samples and used as input in two age prediction experiments. In the first experiment, we learn a child-specific metric and predicts the ages at which speech samples were produced. We then learn a more general developmental index by applying our method across children, predicting relative temporal orderings of speech samples. In both cases we compare our results with established measures of language development, showing improvements in age prediction performance.</p><p>same-paper 2 0.72857863 <a title="117-lda-2" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>Author: Eric Huang ; Richard Socher ; Christopher Manning ; Andrew Ng</p><p>Abstract: Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1</p><p>3 0.59502888 <a title="117-lda-3" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>Author: Bevan Jones ; Mark Johnson ; Sharon Goldwater</p><p>Abstract: Many semantic parsing models use tree transformations to map between natural language and meaning representation. However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.</p><p>4 0.5937559 <a title="117-lda-4" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>5 0.59260899 <a title="117-lda-5" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: In this paper we study unsupervised word sense disambiguation (WSD) based on sense definition. We learn low-dimensional latent semantic vectors of concept definitions to construct a more robust sense similarity measure wmfvec. Experiments on four all-words WSD data sets show significant improvement over the baseline WSD systems and LDA based similarity measures, achieving results comparable to state of the art WSD systems.</p><p>6 0.59198713 <a title="117-lda-6" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>7 0.59192979 <a title="117-lda-7" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>8 0.59028274 <a title="117-lda-8" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>9 0.58690304 <a title="117-lda-9" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>10 0.58477491 <a title="117-lda-10" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>11 0.58474493 <a title="117-lda-11" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>12 0.58419973 <a title="117-lda-12" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<p>13 0.5834657 <a title="117-lda-13" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>14 0.58334279 <a title="117-lda-14" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>15 0.58311027 <a title="117-lda-15" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>16 0.58293349 <a title="117-lda-16" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>17 0.58241183 <a title="117-lda-17" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>18 0.58207554 <a title="117-lda-18" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>19 0.58184326 <a title="117-lda-19" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>20 0.58181101 <a title="117-lda-20" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
