<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-46" href="#">acl2012-46</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</h1>
<br/><p>Source: <a title="acl-2012-46-pdf" href="http://aclweb.org/anthology//P/P12/P12-1097.pdf">pdf</a></p><p>Author: Chang Liu ; Hwee Tou Ng</p><p>Abstract: In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. –</p><p>Reference: <a title="acl-2012-46-reference" href="../acl2012_reference/acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 s g  Abstract In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. [sent-4, score-0.242]
</p><p>2 For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. [sent-5, score-0.135]
</p><p>3 By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. [sent-6, score-0.268]
</p><p>4 We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. [sent-7, score-0.223]
</p><p>5 , 2002), automatic machine translation (MT) evaluation has received a lot of research interest. [sent-9, score-0.2]
</p><p>6 The Workshop on Statistical Machine Translation (WMT) hosts regular campaigns comparing different machine translation evaluation metrics (Callison-Burch et al. [sent-10, score-0.267]
</p><p>7 , 2010) have consistently outperformed BLEU as judged by the correlations with human judgments. [sent-16, score-0.118]
</p><p>8 921 The research on automatic machine translation evaluation is important for a number of reasons. [sent-17, score-0.2]
</p><p>9 Automatic translation evaluation gives machine translation researchers a cheap and reproducible way to guide their research and makes it possible to compare machine translation methods across different  studies. [sent-18, score-0.456]
</p><p>10 In addition, machine translation system parameters are tuned by maximizing the automatic scores. [sent-19, score-0.167]
</p><p>11 , 2011) has shown evidence that replacing BLEU by a newer metric, TESLA, can improve the human judged translation quality. [sent-21, score-0.189]
</p><p>12 The only prior work attempting to address the problem of word segmentation in automatic MT evaluation for Chinese that we are aware of is Li et Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-30, score-0.1]
</p><p>13 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi9c 2s1–929, 买 buy  伞 umbrella  买 buy  雨伞 umbrella  买  雨  伞  buy  rain  umbrella  Figure 1: Three forms of the same expression buy umbrella in Chinese al. [sent-32, score-1.399]
</p><p>14 The work compared various MT evaluation metrics (BLEU, NIST, METEOR, GTM, 1 − TER) with different segmentation schemes, and f−ou TndE Rth)a wt treating every single tcahtaioranc stcehr as a t,o akennd (character-level MT evaluation) gives the best correlation with human judgments. [sent-34, score-0.241]
</p><p>15 (201 1) identify two reasons that characterbased metrics outperform word-based metrics. [sent-36, score-0.093]
</p><p>16 Character-based metrics award the match between characters 伞 and 伞. [sent-45, score-0.252]
</p><p>17 Character-based metrics do not suffer from errors and differences in word segmentation, so 买_雨伞 and 买_雨_伞 would be judged exactly equal. [sent-47, score-0.141]
</p><p>18 (201 1) conduct empirical experiments to show that character-based metrics consistently out-  perform their word-based counterparts. [sent-49, score-0.093]
</p><p>19 Although partial matches are partially awarded, the mechanism breaks down for n-grams where 1Literally, rain umbrella. [sent-51, score-0.153]
</p><p>20 For example, between 买_雨_伞 and 买_伞, higher-order n-grams such as 买_雨 and 雨_伞 still have no match, and will be penalized accordingly, even though 买_雨_伞 and 买_伞 should match exactly. [sent-53, score-0.094]
</p><p>21 Character-level metrics can utilize only a small part ofthe Chinese synonym dictionary, such as 你 and (you). [sent-56, score-0.171]
</p><p>22 The majority of Chinese synonyms involve more than one character, such as 雨伞 and 伞 (umbrella), and 儿童 and 小孩 (child). [sent-57, score-0.184]
</p><p>23 您  In this work, we attempt to address both of these issues by introducing TESLA-CELAB, a characterlevel metric that also models word-level linguistic phenomenon. [sent-58, score-0.124]
</p><p>24 We formulate the n-gram matching process as a real-valued linear programming problem, which can be solved efficiently. [sent-59, score-0.201]
</p><p>25 The metric is based on the TESLA automatic MT evaluation framework (Liu et al. [sent-60, score-0.134]
</p><p>26 1 Basic Matching We illustrate our matching algorithm using the examples in Figure 1. [sent-64, score-0.117]
</p><p>27 We use Cilin (同 义词 词林)2 as our synonym dictionary. [sent-66, score-0.078]
</p><p>28 The basic n-gram matching problem is shown in Figure 2. [sent-67, score-0.117]
</p><p>29 Two n-grams are connected if they are identical, or if they are identified as synonyms by Cilin. [sent-68, score-0.184]
</p><p>30 Notice that all n-grams are put in the same matching problem regardless of n, unlike in translation evaluation metrics designed for European languages. [sent-69, score-0.384]
</p><p>31 module=pagemaster &PAGE;_user_op=view_page&PAGE;_id=162  买买雨买伞伞买雨买雨伞伞买雨伞 Figure 2: The basic n-gram matching problem  买买雨买伞伞买雨买雨伞伞买雨伞 Figure 3: The n-gram matching problem after phrase matching  3. [sent-78, score-0.351]
</p><p>32 2 Phrase Matching We note in Figure 2 that the trigram 买雨伞 and the bigram 买 伞 are still unmatched, even though the match between 雨伞 and 伞 should imply the match between 买雨伞 and 买伞. [sent-79, score-0.112]
</p><p>33 We infer the matching of such phrases using a dynamic programming algorithm. [sent-80, score-0.172]
</p><p>34 Two n-grams are considered synonyms if they can be segmented into synonyms that are aligned. [sent-81, score-0.368]
</p><p>35 With this extension, we are able to match 买 雨伞 and 买 伞 (since 买 matches 买 and 雨伞 matches 伞). [sent-82, score-0.142]
</p><p>36 The matching problem is now depicted by Figure 3. [sent-83, score-0.117]
</p><p>37 The linear programming problem is mathematically described as follows. [sent-84, score-0.084]
</p><p>38 3 Covered Matching In Figure 3, n-grams 雨 and 买雨 in the reference remain impossible to match, which implies misguided penalty for the candidate translation. [sent-89, score-0.081]
</p><p>39 This relationship is implicit in the matching problem for English translation evaluation metrics where words are well delimited. [sent-92, score-0.384]
</p><p>40 But with phrase matching in Chinese, it must be modeled explicitly. [sent-93, score-0.117]
</p><p>41 However, we cannot simply perform covered ngram matching as a post processing step. [sent-94, score-0.187]
</p><p>42 As an example, suppose we are matching phrases 雨伞 and 伞, as shown in Figure 4. [sent-95, score-0.117]
</p><p>43 The linear programming solver may come up with any of the solutions where  w(伞, 伞) + w(雨伞, 伞) = 1, w(伞, 伞) ∈ [0, 1] awn(d伞 w,( 伞雨)伞 +, 伞w(雨) ∈ [0, 1] . [sent-96, score-0.084]
</p><p>44 This indicates the need to model covered n-gram matching in the linear programming problem itself. [sent-99, score-0.271]
</p><p>45 We return to the matching of the reference 买雨 伞 and the candidate 买伞 in Figure 3. [sent-100, score-0.198]
</p><p>46 For such a solution, the 924 max and the sum forms are equivalent, since the cref(·) and ccand(·) variables are also constrained to the range [0, 1] . [sent-112, score-0.146]
</p><p>47 The maximum flow equivalence breaks down when the c(·) variables are introduced, so in the genewrahle case, replacing max wrei ithnt sum cise only an approximation. [sent-113, score-0.147]
</p><p>48 Returning to our sample problem, the linear programming solver simply needs to assign: w(买雨伞, 买伞) = 1 wref(买雨伞)  = 1  wcand(买伞) = 1 Consequently, due to the maximum covering weights constraint, we can give the following value assignment, implying that all n-grams have been matched. [sent-114, score-0.084]
</p><p>49 The recall is a function of PPX cref(X), abnleds the precision is a function of PPY ccand(Y ), where X is the set of all n-grams of tPPhe reference, and Y is the set of all n-grams of the cPandidate translation. [sent-117, score-0.095]
</p><p>50 Many prior translation evaluation metrics such as MAXSIM (Chan and Ng, 2008) and TESLA (Liu et al. [sent-118, score-0.267]
</p><p>51 We are also constrained by the linear programming —  —  framework, hence we set the objective function as  Z1 XXcref(X) + fXYccand(Y )! [sent-127, score-0.157]
</p><p>52 25 so that our objective function is also four times as sensitive to recall than to precision. [sent-129, score-0.102]
</p><p>53 Similar to the other TESLA metrics, when there are N multiple references, we match the candidate translation against each of them and use the average of the N objective function values as the segment level score. [sent-131, score-0.351]
</p><p>54 4 Experiments In this section, we test the effectiveness of TESLACELAB on some real-world English-Chinese translation tasks. [sent-134, score-0.141]
</p><p>55 8 words long and the average Chinese reference translation is 9. [sent-138, score-0.214]
</p><p>56 The test set was translated by seven MT systems, and each translation has been manually judged for adequacy and fluency. [sent-141, score-0.265]
</p><p>57 Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent, whereas fluency measures whether a translation is fluent, regardless of whether the meaning is correct. [sent-142, score-0.548]
</p><p>58 Due to high evaluation costs, adequacy and fluency assessments were limited to the translation outputs of four systems. [sent-143, score-0.313]
</p><p>59 In addition, the translation outputs of the MT systems are also manually ranked according to their translation quality. [sent-144, score-0.282]
</p><p>60 5 words long and the average Chinese reference translation is 43. [sent-163, score-0.214]
</p><p>61 Since no manual evaluation is given for the data set, we recruited twelve bilingual judges to evaluate the first thirty documents for adequacy and fluency (355 segments for a total of 355 11 = 3, 905 etrnacnysl (a3te5d5 segments). [sent-165, score-0.195]
</p><p>62 Tr ahe to ftianla ol score ×of1 a s=en 3t,e9n0ce5 is the average of its adequacy and fluency scores. [sent-166, score-0.162]
</p><p>63 Each judge works on one quarter of the sentences so that each translation is judged by three judges. [sent-167, score-0.189]
</p><p>64 1 BLEU Although word-level BLEU has often been found  inferior to the new-generation metrics when the target language is English or other European languages, prior research has shown that character-level BLEU is highly competitive when the target language is Chinese (Li et al. [sent-175, score-0.093]
</p><p>65 Therefore, we Segment Pearson Spearman rank Metric Type consistency correlation correlation BLEUcharacter-level0. [sent-177, score-0.2]
</p><p>66 Segment Pearson Spearman rank Metric Type consistency correlation correlation BLEUcharacter-level0. [sent-192, score-0.2]
</p><p>67 The correlations of character-level BLEU and the average human judgments are shown in the first row of Tables 2 and 3 for the IWSLT and the NIST data set, respectively. [sent-207, score-0.168]
</p><p>68 We also report the Pearson correlation and the Spearman rank correlation of the system-level  scores. [sent-210, score-0.172]
</p><p>69 Note that in the IWSLT data set, the Spearman rank correlation is highly unstable due to the small number of participating systems. [sent-211, score-0.098]
</p><p>70 2 TESLA-M In addition to character-level BLEU, we also present the correlations for the word-level metric TESLA. [sent-214, score-0.145]
</p><p>71 Compared to BLEU, TESLA allows more sophisticated weighting of n-grams and measures of word similarity including synonym relations. [sent-215, score-0.109]
</p><p>72 It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al. [sent-216, score-0.118]
</p><p>73 However, its use of POS tags and synonym dictionaries prevents its use at the character-level. [sent-218, score-0.078]
</p><p>74 , 2003) for preprocessing and Cilin for synonym 926 definition during matching. [sent-222, score-0.078]
</p><p>75 The various correlations are reported in the second row of Tables 2  and 3. [sent-224, score-0.102]
</p><p>76 The correlations between the TESLA-CELAB scores and human judgments are shown in the last row of Tables 2 and 3. [sent-230, score-0.145]
</p><p>77 Entries that outperform the BLEU baseline at 5% significance level are marked with ‘*’, and those that outperform at the 1% significance level are marked with ‘**’ . [sent-232, score-0.082]
</p><p>78 This disables TESLA-  CELAB’s ability to detect word-level synonyms and turns TESLA-CELAB into a linear programming based character-level metric. [sent-235, score-0.268]
</p><p>79 TESLA-CELAB can process word-level synonyms a-nCdE can Baw caarnd cp hraocraecstser w-leovrde-ll mevaetlch seyns. [sent-239, score-0.184]
</p><p>80 The covered n-gram matching rule is then able to award tricky n-grams such as 下星, 个女, 个 闺, 作吗 and 班吗, which are covered by 下星期, 个女儿, 个闺女, 工作吗 and 上班吗 respectively. [sent-245, score-0.328]
</p><p>81 The TESLACELAB algorithm does not need pre-segmented  这  927 Reference:  下 next  周 week  下 next  星期 week  我 I 我 I  有 have 有 have  一个 a 个 a  女儿 daughter 闺女 daughter  Reference:  你 you  在 at  这儿 here  工作 work  吗 qn  ？ ? [sent-247, score-0.215]
</p><p>82 Table 4: Sample sentences from the IWSLT 2008 test set Schirm umbrella  kaufen buy  Regenschirm umbrella Regen rain  schirm umbrella  kaufen buy kaufen buy  Figure 5: Three forms of buy umbrella in German sentences, and essentially finds multi-character synonyms opportunistically. [sent-253, score-1.852]
</p><p>83 1 Other Languages with Ambiguous Word Boundaries Although our experiments here are limited to Chinese, many other languages have similarly ambiguous word boundaries. [sent-255, score-0.086]
</p><p>84 Since compound nouns such as Regenschirm are very common in German and generate many out-of-vocabulary words, a common preprocessing step in German translation (and translation evaluation to a lesser extent) is to split  compound words, and we end up with the last form Regen schirm kaufen. [sent-259, score-0.446]
</p><p>85 2 Fractional Similarity Measures In the current formulation of TESLA-CELAB, two n-grams X and Y are either synonyms which completely match each other, or are completely unrelated. [sent-263, score-0.3]
</p><p>86 In contrast, the linear-programming based TESLA metric allows fractional similarity measures between 0 (completely unrelated) and 1 (exact synonyms). [sent-264, score-0.188]
</p><p>87 We can then award partial scores for related words, such as those identified as such by WordNet or those with the same POS tags. [sent-265, score-0.101]
</p><p>88 Supporting fractional similarity measures is nontrivial in the TESLA-CELAB framework. [sent-266, score-0.113]
</p><p>89 3 Fractional Weights for N-grams The TESLA-M metric allows each n-gram to have a weight, which is primarily used to discount function words. [sent-269, score-0.108]
</p><p>90 TESLA-CELAB can support fractional  weights for n-grams as well by the following extension. [sent-270, score-0.082]
</p><p>91 In contrast, TESLA-M found discounting function words very effective for English and other European languages such as German. [sent-276, score-0.081]
</p><p>92 Our metric combines the advantages of characterlevel and word-level metrics: 1. [sent-279, score-0.124]
</p><p>93 TESLA-CELAB is able to award scores for partial word-level matches. [sent-280, score-0.101]
</p><p>94 TESLA-CELAB does not have a segmentation step, hence it will not introduce word segmentation errors. [sent-282, score-0.082]
</p><p>95 TESLA-CELAB is able to take full advantage of the synonym dictionary, even when the synonyms differ in the number of characters. [sent-284, score-0.262]
</p><p>96 METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. [sent-294, score-0.208]
</p><p>97 Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. [sent-303, score-0.234]
</p><p>98 MAXSIM: A maximum similarity metric for machine translation evaluation. [sent-311, score-0.216]
</p><p>99 Automatic evaluation of Chinese translation output: word-level or character-level? [sent-323, score-0.174]
</p><p>100 A study of translation edit rate with targeted human annotation. [sent-347, score-0.141]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wref', 0.599), ('cref', 0.268), ('wcand', 0.227), ('umbrella', 0.206), ('tesla', 0.197), ('synonyms', 0.184), ('ccand', 0.165), ('bleu', 0.145), ('translation', 0.141), ('buy', 0.124), ('matching', 0.117), ('chinese', 0.109), ('teslacelab', 0.103), ('iwslt', 0.098), ('metrics', 0.093), ('max', 0.087), ('mt', 0.084), ('cilin', 0.083), ('schirm', 0.083), ('fractional', 0.082), ('synonym', 0.078), ('adequacy', 0.076), ('metric', 0.075), ('correlation', 0.074), ('award', 0.071), ('covered', 0.07), ('correlations', 0.07), ('dahlmeier', 0.066), ('kappa', 0.065), ('hwee', 0.064), ('tou', 0.064), ('fluency', 0.063), ('kaufen', 0.062), ('regenschirm', 0.062), ('match', 0.056), ('spearman', 0.055), ('programming', 0.055), ('boundaries', 0.054), ('daughter', 0.054), ('rain', 0.054), ('reference', 0.05), ('characterlevel', 0.049), ('nist', 0.048), ('judged', 0.048), ('languages', 0.048), ('christof', 0.043), ('judgments', 0.043), ('matches', 0.043), ('sighan', 0.041), ('regen', 0.041), ('significance', 0.041), ('segmentation', 0.041), ('objective', 0.04), ('monz', 0.04), ('german', 0.038), ('ambiguous', 0.038), ('week', 0.038), ('penalized', 0.038), ('meteor', 0.037), ('maxsim', 0.036), ('dictionary', 0.036), ('pearson', 0.035), ('liu', 0.035), ('variables', 0.034), ('evaluation', 0.033), ('singapore', 0.033), ('function', 0.033), ('characters', 0.032), ('european', 0.032), ('chang', 0.032), ('row', 0.032), ('qn', 0.031), ('measures', 0.031), ('candidate', 0.031), ('completely', 0.03), ('partial', 0.03), ('recall', 0.029), ('linear', 0.029), ('tables', 0.029), ('consistency', 0.028), ('tseng', 0.028), ('banerjee', 0.028), ('fuzzy', 0.028), ('philipp', 0.027), ('segment', 0.027), ('breaks', 0.026), ('automatic', 0.026), ('omar', 0.025), ('fluent', 0.025), ('forms', 0.025), ('findings', 0.025), ('compound', 0.024), ('snover', 0.024), ('li', 0.024), ('rank', 0.024), ('koehn', 0.024), ('average', 0.023), ('judges', 0.023), ('segmenter', 0.023), ('highlights', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="46-tfidf-1" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>Author: Chang Liu ; Hwee Tou Ng</p><p>Abstract: In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. –</p><p>2 0.1581645 <a title="46-tfidf-2" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>3 0.13487417 <a title="46-tfidf-3" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>4 0.13221845 <a title="46-tfidf-4" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>Author: Boxing Chen ; Roland Kuhn ; Samuel Larkin</p><p>Abstract: Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT , a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves 1 consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties). 1</p><p>5 0.10640571 <a title="46-tfidf-5" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>6 0.10366805 <a title="46-tfidf-6" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>7 0.096960172 <a title="46-tfidf-7" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>8 0.096906431 <a title="46-tfidf-8" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>9 0.094790623 <a title="46-tfidf-9" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>10 0.094399661 <a title="46-tfidf-10" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>11 0.091703616 <a title="46-tfidf-11" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>12 0.090588376 <a title="46-tfidf-12" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>13 0.090513125 <a title="46-tfidf-13" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>14 0.081036992 <a title="46-tfidf-14" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>15 0.080361903 <a title="46-tfidf-15" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>16 0.07841415 <a title="46-tfidf-16" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>17 0.076279022 <a title="46-tfidf-17" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>18 0.076044895 <a title="46-tfidf-18" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>19 0.07459975 <a title="46-tfidf-19" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>20 0.074496306 <a title="46-tfidf-20" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.203), (1, -0.139), (2, 0.062), (3, 0.051), (4, 0.071), (5, 0.029), (6, -0.006), (7, -0.061), (8, -0.067), (9, 0.045), (10, -0.029), (11, 0.012), (12, 0.006), (13, 0.036), (14, 0.016), (15, 0.004), (16, 0.051), (17, 0.04), (18, -0.055), (19, 0.053), (20, 0.079), (21, -0.07), (22, 0.13), (23, 0.024), (24, 0.061), (25, 0.034), (26, -0.009), (27, 0.108), (28, 0.112), (29, 0.003), (30, 0.01), (31, 0.086), (32, 0.046), (33, -0.087), (34, 0.034), (35, -0.098), (36, -0.064), (37, 0.021), (38, 0.081), (39, -0.076), (40, 0.092), (41, 0.035), (42, 0.001), (43, 0.104), (44, 0.132), (45, 0.08), (46, -0.103), (47, 0.117), (48, 0.038), (49, 0.102)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92223626 <a title="46-lsi-1" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>Author: Chang Liu ; Hwee Tou Ng</p><p>Abstract: In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. –</p><p>2 0.82225955 <a title="46-lsi-2" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>Author: Boxing Chen ; Roland Kuhn ; Samuel Larkin</p><p>Abstract: Many machine translation (MT) evaluation metrics have been shown to correlate better with human judgment than BLEU. In principle, tuning on these metrics should yield better systems than tuning on BLEU. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents PORT , a new MT evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning MT systems. PORT does not require external resources and is quick to compute. It has a better correlation with human judgment than BLEU. We compare PORT-tuned MT systems to BLEU-tuned baselines in five experimental conditions involving four language pairs. PORT tuning achieves 1 consistently better performance than BLEU tuning, according to four automated metrics (including BLEU) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the PORT-tuned output 45.3% of the time (vs. 32.7% BLEU tuning preferences and 22.0% ties). 1</p><p>3 0.75563622 <a title="46-lsi-3" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>Author: Kevin Duh ; Katsuhito Sudoh ; Xianchao Wu ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</p><p>4 0.56551677 <a title="46-lsi-4" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>5 0.53965533 <a title="46-lsi-5" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>Author: Meritxell Gonzalez ; Jesus Gimenez ; Lluis Marquez</p><p>Abstract: Error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the MT systems under development and allow fair comparisons among them. This work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations. To do so, we have set up an online graphical interface for the ASIYA toolkit, a rich repository of evaluation measures working at different linguistic levels. The current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations, and word alignments. The intelligent visualization of the linguistic structures used by the metrics, as well as a set of navigational functionalities, may lead towards advanced methods for automatic error analysis.</p><p>6 0.53905827 <a title="46-lsi-6" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>7 0.5352546 <a title="46-lsi-7" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>8 0.51505452 <a title="46-lsi-8" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>9 0.49103275 <a title="46-lsi-9" href="./acl-2012-Unsupervized_Word_Segmentation%3A_the_Case_for_Mandarin_Chinese.html">210 acl-2012-Unsupervized Word Segmentation: the Case for Mandarin Chinese</a></p>
<p>10 0.45617208 <a title="46-lsi-10" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>11 0.4546411 <a title="46-lsi-11" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>12 0.44971219 <a title="46-lsi-12" href="./acl-2012-Applications_of_GPC_Rules_and_Character_Structures_in_Games_for_Learning_Chinese_Characters.html">26 acl-2012-Applications of GPC Rules and Character Structures in Games for Learning Chinese Characters</a></p>
<p>13 0.44934624 <a title="46-lsi-13" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>14 0.4422327 <a title="46-lsi-14" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>15 0.43287909 <a title="46-lsi-15" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>16 0.42558628 <a title="46-lsi-16" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>17 0.41934544 <a title="46-lsi-17" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>18 0.4185071 <a title="46-lsi-18" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>19 0.40195253 <a title="46-lsi-19" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>20 0.3972365 <a title="46-lsi-20" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.24), (25, 0.034), (26, 0.056), (28, 0.06), (30, 0.036), (37, 0.029), (39, 0.059), (52, 0.011), (59, 0.015), (74, 0.051), (82, 0.012), (84, 0.022), (85, 0.059), (90, 0.137), (92, 0.028), (94, 0.022), (99, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74008405 <a title="46-lda-1" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>Author: Chang Liu ; Hwee Tou Ng</p><p>Abstract: In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. –</p><p>2 0.60379457 <a title="46-lda-2" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>3 0.60170001 <a title="46-lda-3" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>4 0.59819603 <a title="46-lda-4" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>Author: Yashar Mehdad ; Matteo Negri ; Marcello Federico</p><p>Abstract: We address a core aspect of the multilingual content synchronization task: the identification of novel, more informative or semantically equivalent pieces of information in two documents about the same topic. This can be seen as an application-oriented variant of textual entailment recognition where: i) T and H are in different languages, and ii) entailment relations between T and H have to be checked in both directions. Using a combination of lexical, syntactic, and semantic features to train a cross-lingual textual entailment system, we report promising results on different datasets.</p><p>5 0.59716129 <a title="46-lda-5" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>Author: Gerard de Melo ; Gerhard Weikum</p><p>Abstract: We present UWN, a large multilingual lexical knowledge base that describes the meanings and relationships of words in over 200 languages. This paper explains how link prediction, information integration and taxonomy induction methods have been used to build UWN based on WordNet and extend it with millions of named entities from Wikipedia. We additionally introduce extensions to cover lexical relationships, frame-semantic knowledge, and language data. An online interface provides human access to the data, while a software API enables applications to look up over 16 million words and names.</p><p>6 0.59706903 <a title="46-lda-6" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>7 0.596008 <a title="46-lda-7" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>8 0.59450793 <a title="46-lda-8" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>9 0.5932889 <a title="46-lda-9" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>10 0.59262908 <a title="46-lda-10" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>11 0.59140801 <a title="46-lda-11" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>12 0.59049809 <a title="46-lda-12" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>13 0.59018385 <a title="46-lda-13" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>14 0.58966881 <a title="46-lda-14" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>15 0.58866364 <a title="46-lda-15" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>16 0.58837062 <a title="46-lda-16" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>17 0.58790576 <a title="46-lda-17" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>18 0.58729374 <a title="46-lda-18" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>19 0.58697778 <a title="46-lda-19" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>20 0.58411223 <a title="46-lda-20" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
