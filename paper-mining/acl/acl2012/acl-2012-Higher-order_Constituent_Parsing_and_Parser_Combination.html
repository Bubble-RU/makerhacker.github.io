<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 acl-2012-Higher-order Constituent Parsing and Parser Combination</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-109" href="#">acl2012-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 acl-2012-Higher-order Constituent Parsing and Parser Combination</h1>
<br/><p>Source: <a title="acl-2012-109-pdf" href="http://aclweb.org/anthology//P/P12/P12-2001.pdf">pdf</a></p><p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>Reference: <a title="acl-2012-109-reference" href="../acl2012_reference/acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Higher-Order Constituent Parsing and Parser Combination∗ Xiao Chen and Chunyu Kit Department of Chinese, Translation and Linguistics City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong SAR, China {cxiao2 ct ckit }@ cityu . [sent-1, score-0.074]
</p><p>2 hk  ,  Abstract This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. [sent-3, score-1.149]
</p><p>3 Experiments on English and Chinese treebanks confirm its advantage over its first-order version. [sent-4, score-0.053]
</p><p>4 58% on the two languages, respectively, and further pushes them to 92. [sent-7, score-0.044]
</p><p>5 Previous discriminative parsing models usually factor a parse tree into a set of parts. [sent-11, score-0.587]
</p><p>6 In dependency parsing (DP), the number of dependencies in a part  is called the order of a DP model (Koo and Collins, 2010). [sent-13, score-0.322]
</p><p>7 Accordingly, existing graph-based DP models can be categorized into tree groups, namely, the first-order (Eisner, 1996; McDonald et al. [sent-14, score-0.054]
</p><p>8 Similarly, we can define the order of constituent parsing in terms of the number of grammar rules in a part. [sent-17, score-0.649]
</p><p>9 Then, the previous discriminative constituent parsing models (Johnson, 2001 ; Henderson, 2004; Taskar et al. [sent-18, score-0.691]
</p><p>10 , 2008) are the first-order ones, because there is only one grammar rule in a part. [sent-21, score-0.256]
</p><p>11 The discriminative re-scoring models (Collins, 2000; Collins and Duffy, 2002; Charniak and Johnson, 2005; Huang, 2008) can be viewed as previous attempts to higher-order constituent parsing, using some parts containing more than one  grammar rule as non-local features. [sent-22, score-0.726]
</p><p>12 In this paper, we present a higher-order constituent parsing model1 based on these previous works. [sent-23, score-0.538]
</p><p>13 It allows multiple adjacent grammar rules in each part of a parse tree, so as to utilize more local structural context to decide the plausibility of a grammar rule instance. [sent-24, score-0.766]
</p><p>14 Combined with other high-performance parsers under the framework of constituent recombination (Sagae and Lavie, 2006; Fossum and Knight, 2009), this model further enhances the F1 scores to 92. [sent-28, score-0.455]
</p><p>15 2  Higher-order Constituent Parsing  Discriminative parsing is aimed to learn a function f : S → T from a set of sentences S to a set of valid parses →T according etot a given CFG, w toh aic she maps an input sentence s ∈ So ato g a snet C FofG c,a wndhiidchate m parses T (s). [sent-31, score-0.266]
</p><p>16 com/p/gazaparser/ ProceJe jdui,n gRsep ouf tbhliec 5 o0fth K Aornenau,8a l-1 M4e Jeutilnyg 2 o0f1 t2h. [sent-35, score-0.088]
</p><p>17 c As2s0o1c2ia Atisosno fcoiart Cionom fopru Ctaotmiopnuatla Lti on gaulis Lti cnsg,u pisatgices 1–5,  begin(b)  split(m)  Figure 1: A part of a parse tree centered at NP  end(e) → NP  VP  where g(t, s) is a scoring function to evaluate the event that t is the parse of s. [sent-37, score-0.652]
</p><p>18 this model is factorized as  To ensure tractability,  g(t,s) = Xg(Q(r),s) =Xθ · Φ(Q(r),s), Xr∈t  (3)  Xr∈t  where g(Q(r) , s) scores Q(r), a part centered at grammar Q ru(lre i,nss)tan sccoer r sin Q t, a)n,d a Φ( Q(r) , s) eisd dth aet gveracmtomr aofr rfuelaetu irnests fnocre Q(r). [sent-39, score-0.215]
</p><p>19 A E part Qin( a parse str ietse is illustrated in Figure 1. [sent-41, score-0.206]
</p><p>20 It consists of the center grammar rule instance NP → NP VP and a set of immediate neighbors, i. [sent-42, score-0.376]
</p><p>21 , its parent PP → IN NP, its children NP → DT QP and VP → VBN PP, and its sibling IN → of. [sent-44, score-0.216]
</p><p>22 This set of neighboring rule instances forms a local structural context to provide useful information to determine the plausibility of the center rule instance. [sent-45, score-0.608]
</p><p>23 All features extracted from the part in Figure 1 are demonstrated in Table 1. [sent-52, score-0.047]
</p><p>24 Some back-off structural features are used for smoothing, which cannot be presented due to limited space. [sent-53, score-0.106]
</p><p>25 With only lexical features in a part, this parsing model backs off to a first-order one similar to those in the previous works. [sent-54, score-0.221]
</p><p>26 Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. [sent-55, score-0.522]
</p><p>27 2 Decoding The factorization of the parsing model allows us to develop an exact decoding algorithm for it. [sent-57, score-0.353]
</p><p>28 Following Huang (2008), this algorithm traverses a parse forest in a bottom-up manner. [sent-58, score-0.256]
</p><p>29 However, it determines and keeps the best derivation for every gram-  mar rule instance instead of for each node. [sent-59, score-0.248]
</p><p>30 Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e. [sent-60, score-0.296]
</p><p>31 , parent and sibling features, has to be delayed until it joins an upper level structure. [sent-62, score-0.216]
</p><p>32 For example, when computing the score of a derivation under the center rule NP → NP VP in Figure 1, the algorithm will extract child features from its children NP → DT QP and VP → VBN PP. [sent-63, score-0.278]
</p><p>33 The parent and sibling features of the two child rules can also be extracted from the current derivation and used to calculate the score of this derivation. [sent-64, score-0.274]
</p><p>34 But parent and sibling features for the center rule will not be computed until the decoding process reaches the rule above, i. [sent-65, score-0.651]
</p><p>35 This algorithm is more complex than the approximate decoding algorithm of Huang (2008). [sent-68, score-0.07]
</p><p>36 However, its efficiency heavily depends on the size of the parse forest it has to handle. [sent-69, score-0.322]
</p><p>37 Forest pruning (Charφ0(Q(r),s)  =  PP PO(Ax, b, e)P(Ax → By Cz)I(By, b,m)I(Cz,m, e) PxPyPzI(S,0,n) 2  (4)  niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. [sent-70, score-0.11]
</p><p>38 A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. [sent-71, score-0.136]
</p><p>39 3  Constituent Recombination  Following Fossum and Knight (2009), our constituent weighting scheme for parser combination uses multiple outputs of independent parsers. [sent-73, score-0.539]
</p><p>40 The weight of a recombined parse is defined as the sum of weights of all constituents in the parse. [sent-75, score-0.329]
</p><p>41 However, this definition has a systematic bias towards selecting a parse with as many constituents as possible 3  Train. [sent-76, score-0.229]
</p><p>42 A pruning threshold  ρ,  simi-  lar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. [sent-81, score-0.17]
</p><p>43 The parameters λi and ρ are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al. [sent-82, score-0.047]
</p><p>44 4  Experiment  Our parsing models are evaluated on both English and Chinese treebanks, i. [sent-84, score-0.221]
</p><p>45 For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. [sent-90, score-0.19]
</p><p>46 A factor λ is introduced to balance the two models. [sent-92, score-0.046]
</p><p>47 It is tuned on a development set using the gold sec-  SystemF1(%)EX(%)  C B ehao rdkre( nr2ila0 esky0e3 p(t)a2 r0ls . [sent-93, score-0.047]
</p><p>48 The parameters θ of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). [sent-106, score-0.288]
</p><p>49 The performance of our first- and higher-order parsing models on all sentences of the two test sets is presented in Table 3, where λ indicates a tuned balance factor. [sent-107, score-0.314]
</p><p>50 This parser is also combined with the parser of Charniak and Johnson (2005)2 and the Stanford. [sent-108, score-0.268]
</p><p>51 parser3 The best combination results in Table 3 are achieved with k=70 for English and k=100 for Chinese for selecting the k-best parses. [sent-109, score-0.088]
</p><p>52 5  Conclusion  This paper has presented a higher-order model for constituent parsing that factorizes a parse tree into larger parts than before, in hopes of increasing its power of discriminating the true parse from the others without losing tractability. [sent-123, score-0.91]
</p><p>53 Including a PCFG-based model as its basic feature, this model achieves a better performance than previous single and re-scoring parsers, and its combination with other parsers per-  forms even better (by about 1%). [sent-127, score-0.139]
</p><p>54 More importantly, it extends the existing works into a more general framework of constituent parsing to utilize more lexical and structural context and incorporate more strength of various parsing techniques. [sent-128, score-0.865]
</p><p>55 However, higher-order constituent parsing inevitably leads to a high computational complexity. [sent-129, score-0.582]
</p><p>56 We intend to deal with the efficiency problem of our model with some advanced parallel computing technologies in our future works. [sent-130, score-0.066]
</p><p>57 In Proceedings of DARPA Speech and Natural Language Workshop, pages 306–3 11. [sent-151, score-0.113]
</p><p>58 TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. [sent-162, score-0.067]
</p><p>59 New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. [sent-188, score-0.221]
</p><p>60 Discriminative training methods for hidden Markov models: Theory and experiments  with perceptron algorithms. [sent-196, score-0.067]
</p><p>61 Three new probabilistic models for dependency parsing: An exploration. [sent-201, score-0.054]
</p><p>62 Joint and conditional estimation  of tagging and parsing models. [sent-231, score-0.221]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('constituent', 0.317), ('parsing', 0.221), ('fossum', 0.199), ('parse', 0.159), ('np', 0.156), ('discriminative', 0.153), ('rule', 0.145), ('collins', 0.142), ('petrov', 0.136), ('parser', 0.134), ('sibling', 0.134), ('mcdonald', 0.121), ('vp', 0.114), ('pages', 0.113), ('grammar', 0.111), ('structural', 0.106), ('recombined', 0.1), ('forest', 0.097), ('slav', 0.096), ('combination', 0.088), ('recombination', 0.087), ('plausibility', 0.087), ('xg', 0.087), ('koo', 0.086), ('sagae', 0.086), ('parent', 0.082), ('ex', 0.081), ('klein', 0.08), ('chunyu', 0.08), ('powell', 0.08), ('qp', 0.08), ('xiao', 0.079), ('products', 0.077), ('center', 0.075), ('cityu', 0.074), ('vbn', 0.074), ('knight', 0.074), ('pp', 0.074), ('huang', 0.072), ('chinese', 0.072), ('dp', 0.071), ('johnson', 0.07), ('lti', 0.07), ('ax', 0.07), ('constituents', 0.07), ('wsj', 0.07), ('decoding', 0.07), ('perceptron', 0.067), ('efficiency', 0.066), ('hong', 0.065), ('fernando', 0.064), ('kit', 0.064), ('charniak', 0.063), ('terry', 0.062), ('xr', 0.062), ('factorization', 0.062), ('ryan', 0.061), ('xavier', 0.059), ('latent', 0.059), ('derivation', 0.058), ('emnlp', 0.058), ('tan', 0.057), ('centered', 0.057), ('treebank', 0.056), ('carreras', 0.056), ('tree', 0.054), ('lavie', 0.054), ('taskar', 0.054), ('dependency', 0.054), ('treebanks', 0.053), ('parsers', 0.051), ('kong', 0.05), ('neighboring', 0.05), ('chen', 0.048), ('pereira', 0.048), ('finkel', 0.048), ('tuned', 0.047), ('part', 0.047), ('balance', 0.046), ('aimed', 0.045), ('instance', 0.045), ('eugene', 0.045), ('atisosno', 0.044), ('cionom', 0.044), ('ctaotmiopnuatla', 0.044), ('fcoiart', 0.044), ('fopru', 0.044), ('gaulis', 0.044), ('grsep', 0.044), ('jeutilnyg', 0.044), ('ouf', 0.044), ('pisatgices', 0.044), ('tbhliec', 0.044), ('niak', 0.044), ('xixk', 0.044), ('inevitably', 0.044), ('grf', 0.044), ('chee', 0.044), ('char', 0.044), ('pushes', 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="109-tfidf-1" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>2 0.25400159 <a title="109-tfidf-2" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>3 0.22259112 <a title="109-tfidf-3" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>4 0.20205224 <a title="109-tfidf-4" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>Author: Katsuhiko Hayashi ; Taro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms.</p><p>5 0.20103805 <a title="109-tfidf-5" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>6 0.17481659 <a title="109-tfidf-6" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>7 0.17302477 <a title="109-tfidf-7" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>8 0.16004193 <a title="109-tfidf-8" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>9 0.15773006 <a title="109-tfidf-9" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>10 0.15006971 <a title="109-tfidf-10" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>11 0.14936037 <a title="109-tfidf-11" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>12 0.14779189 <a title="109-tfidf-12" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>13 0.14147808 <a title="109-tfidf-13" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>14 0.13546418 <a title="109-tfidf-14" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>15 0.13328388 <a title="109-tfidf-15" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>16 0.12624519 <a title="109-tfidf-16" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>17 0.12580104 <a title="109-tfidf-17" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>18 0.10984815 <a title="109-tfidf-18" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>19 0.10979837 <a title="109-tfidf-19" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>20 0.10706897 <a title="109-tfidf-20" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.296), (1, -0.084), (2, -0.241), (3, -0.234), (4, -0.161), (5, -0.139), (6, 0.005), (7, 0.008), (8, 0.067), (9, 0.014), (10, 0.106), (11, 0.026), (12, -0.064), (13, 0.026), (14, 0.094), (15, -0.051), (16, 0.052), (17, 0.073), (18, -0.043), (19, 0.009), (20, 0.002), (21, 0.006), (22, -0.011), (23, -0.016), (24, -0.005), (25, 0.018), (26, -0.029), (27, -0.059), (28, 0.055), (29, -0.042), (30, -0.003), (31, -0.061), (32, 0.001), (33, 0.035), (34, -0.152), (35, 0.033), (36, 0.042), (37, -0.011), (38, 0.056), (39, 0.031), (40, 0.164), (41, -0.014), (42, -0.022), (43, -0.14), (44, 0.053), (45, 0.044), (46, 0.038), (47, -0.075), (48, -0.04), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97854209 <a title="109-lsi-1" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>2 0.76094216 <a title="109-lsi-2" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>Author: Emily Pitler</p><p>Abstract: Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.</p><p>3 0.75084853 <a title="109-lsi-3" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>Author: Wenliang Chen ; Min Zhang ; Haizhou Li</p><p>Abstract: Most previous graph-based parsing models increase decoding complexity when they use high-order features due to exact-inference decoding. In this paper, we present an approach to enriching high-orderfeature representations for graph-based dependency parsing models using a dependency language model and beam search. The dependency language model is built on a large-amount of additional autoparsed data that is processed by a baseline parser. Based on the dependency language model, we represent a set of features for the parsing model. Finally, the features are efficiently integrated into the parsing model during decoding using beam search. Our approach has two advantages. Firstly we utilize rich high-order features defined over a view of large scope and additional large raw corpus. Secondly our approach does not increase the decoding complexity. We evaluate the proposed approach on English and Chinese data. The experimental results show that our new parser achieves the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.</p><p>4 0.7491402 <a title="109-lsi-4" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>5 0.74879897 <a title="109-lsi-5" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>Author: Zhenghua Li ; Ting Liu ; Wanxiang Che</p><p>Abstract: We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing. Several types of transformation patterns (TP) are designed to capture the systematic annotation inconsistencies among different treebanks. Based on such TPs, we design quasisynchronous grammar features to augment the baseline parsing models. Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target treebanks (Penn Chinese Treebank 5. 1 and 6.0) using the Chinese Dependency Treebank as the source treebank. The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags. Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion.</p><p>6 0.69336617 <a title="109-lsi-6" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>7 0.66459113 <a title="109-lsi-7" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>8 0.66418266 <a title="109-lsi-8" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>9 0.63556057 <a title="109-lsi-9" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>10 0.63515306 <a title="109-lsi-10" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>11 0.6309731 <a title="109-lsi-11" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>12 0.62347275 <a title="109-lsi-12" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>13 0.59669065 <a title="109-lsi-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.57824391 <a title="109-lsi-14" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>15 0.57594389 <a title="109-lsi-15" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>16 0.5748226 <a title="109-lsi-16" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>17 0.54237473 <a title="109-lsi-17" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>18 0.51074201 <a title="109-lsi-18" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>19 0.50795442 <a title="109-lsi-19" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>20 0.49310401 <a title="109-lsi-20" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.308), (25, 0.016), (26, 0.018), (28, 0.069), (30, 0.037), (37, 0.049), (39, 0.035), (59, 0.016), (71, 0.023), (74, 0.044), (82, 0.04), (84, 0.018), (85, 0.018), (90, 0.128), (92, 0.061), (94, 0.018), (99, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78586465 <a title="109-lda-1" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>2 0.65798169 <a title="109-lda-2" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>3 0.51062024 <a title="109-lda-3" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>Author: Dave Golland ; John DeNero ; Jakob Uszkoreit</p><p>Abstract: We present LLCCM, a log-linear variant ofthe constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</p><p>4 0.49093789 <a title="109-lda-4" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>Author: Xinyan Xiao ; Deyi Xiong ; Min Zhang ; Qun Liu ; Shouxun Lin</p><p>Abstract: Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.</p><p>5 0.49062249 <a title="109-lda-5" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>6 0.48606452 <a title="109-lda-6" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>7 0.48576236 <a title="109-lda-7" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>8 0.48558524 <a title="109-lda-8" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>9 0.48544395 <a title="109-lda-9" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>10 0.48493901 <a title="109-lda-10" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>11 0.48432747 <a title="109-lda-11" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>12 0.48373896 <a title="109-lda-12" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>13 0.48350012 <a title="109-lda-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.48297817 <a title="109-lda-14" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>15 0.48165354 <a title="109-lda-15" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>16 0.48073819 <a title="109-lda-16" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.47978643 <a title="109-lda-17" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>18 0.4795908 <a title="109-lda-18" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>19 0.47944573 <a title="109-lda-19" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>20 0.4791263 <a title="109-lda-20" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
