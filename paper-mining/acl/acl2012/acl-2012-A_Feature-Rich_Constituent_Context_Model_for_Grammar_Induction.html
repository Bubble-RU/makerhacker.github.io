<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-11" href="#">acl2012-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</h1>
<br/><p>Source: <a title="acl-2012-11-pdf" href="http://aclweb.org/anthology//P/P12/P12-2004.pdf">pdf</a></p><p>Author: Dave Golland ; John DeNero ; Jakob Uszkoreit</p><p>Abstract: We present LLCCM, a log-linear variant ofthe constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</p><p>Reference: <a title="acl-2012-11-reference" href="../acl2012_reference/acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ccm', 0.804), ('llccm', 0.322), ('constitu', 0.177), ('distitu', 0.138), ('xsn', 0.138), ('span', 0.113), ('klein', 0.104), ('bracket', 0.091), ('xij', 0.085), ('sn', 0.08), ('gramm', 0.076), ('dmv', 0.073), ('unsuperv', 0.065), ('fals', 0.054), ('induc', 0.053), ('pt', 0.052), ('naseem', 0.051), ('denero', 0.051), ('grady', 0.049), ('xy', 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="11-tfidf-1" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>Author: Dave Golland ; John DeNero ; Jakob Uszkoreit</p><p>Abstract: We present LLCCM, a log-linear variant ofthe constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</p><p>2 0.1189238 <a title="11-tfidf-2" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>3 0.093728855 <a title="11-tfidf-3" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>Author: Wanxiang Che ; Valentin Spitkovsky ; Ting Liu</p><p>Abstract: Stanford dependencies are widely used in natural language processing as a semanticallyoriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) partof-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice.</p><p>4 0.083364673 <a title="11-tfidf-4" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>Author: Tahira Naseem ; Regina Barzilay ; Amir Globerson</p><p>Abstract: We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language. Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource-rich languages. The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly. The model factorizes the process of generating a dependency tree into two steps: selection of syntactic dependents and their ordering. Being largely languageuniversal, the selection component is learned in a supervised fashion from all the training languages. In contrast, the ordering decisions are only influenced by languages with similar properties. We systematically model this cross-lingual sharing using typological features. In our experiments, the model consistently outperforms a state-of-the-art multilingual parser. The largest improvement is achieved on the non Indo-European languages yielding a gain of 14.4%.1</p><p>5 0.072335869 <a title="11-tfidf-5" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.</p><p>6 0.056378834 <a title="11-tfidf-6" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>7 0.056113545 <a title="11-tfidf-7" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>8 0.054878931 <a title="11-tfidf-8" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>9 0.051171094 <a title="11-tfidf-9" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>10 0.043295037 <a title="11-tfidf-10" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>11 0.042829651 <a title="11-tfidf-11" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>12 0.04238008 <a title="11-tfidf-12" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>13 0.042190142 <a title="11-tfidf-13" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>14 0.041213691 <a title="11-tfidf-14" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>15 0.041151728 <a title="11-tfidf-15" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>16 0.040497374 <a title="11-tfidf-16" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>17 0.039464779 <a title="11-tfidf-17" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>18 0.039423954 <a title="11-tfidf-18" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>19 0.039313659 <a title="11-tfidf-19" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>20 0.038898941 <a title="11-tfidf-20" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.126), (1, -0.003), (2, -0.075), (3, -0.064), (4, -0.016), (5, 0.006), (6, 0.002), (7, -0.007), (8, 0.013), (9, -0.016), (10, 0.003), (11, -0.036), (12, -0.009), (13, -0.01), (14, 0.007), (15, -0.027), (16, -0.005), (17, -0.008), (18, -0.032), (19, 0.005), (20, 0.017), (21, 0.007), (22, -0.007), (23, -0.022), (24, 0.015), (25, -0.012), (26, 0.037), (27, -0.037), (28, 0.036), (29, 0.043), (30, -0.024), (31, 0.012), (32, 0.042), (33, 0.061), (34, -0.008), (35, 0.017), (36, 0.061), (37, -0.007), (38, 0.02), (39, -0.088), (40, -0.01), (41, 0.016), (42, -0.011), (43, 0.028), (44, -0.097), (45, -0.199), (46, 0.043), (47, -0.157), (48, -0.112), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84128404 <a title="11-lsi-1" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>Author: Dave Golland ; John DeNero ; Jakob Uszkoreit</p><p>Abstract: We present LLCCM, a log-linear variant ofthe constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</p><p>2 0.56635493 <a title="11-lsi-2" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>Author: Tahira Naseem ; Regina Barzilay ; Amir Globerson</p><p>Abstract: We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language. Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource-rich languages. The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly. The model factorizes the process of generating a dependency tree into two steps: selection of syntactic dependents and their ordering. Being largely languageuniversal, the selection component is learned in a supervised fashion from all the training languages. In contrast, the ordering decisions are only influenced by languages with similar properties. We systematically model this cross-lingual sharing using typological features. In our experiments, the model consistently outperforms a state-of-the-art multilingual parser. The largest improvement is achieved on the non Indo-European languages yielding a gain of 14.4%.1</p><p>3 0.52417392 <a title="11-lsi-3" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>4 0.50156516 <a title="11-lsi-4" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.</p><p>5 0.50040722 <a title="11-lsi-5" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>Author: Emily Pitler</p><p>Abstract: Prepositions and conjunctions are two of the largest remaining bottlenecks in parsing. Across various existing parsers, these two categories have the lowest accuracies, and mistakes made have consequences for downstream applications. Prepositions and conjunctions are often assumed to depend on lexical dependencies for correct resolution. As lexical statistics based on the training set only are sparse, unlabeled data can help ameliorate this sparsity problem. By including unlabeled data features into a factorization of the problem which matches the representation of prepositions and conjunctions, we achieve a new state-of-the-art for English dependencies with 93.55% correct attachments on the current standard. Furthermore, conjunctions are attached with an accuracy of 90.8%, and prepositions with an accuracy of 87.4%.</p><p>6 0.47728825 <a title="11-lsi-6" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>7 0.45455608 <a title="11-lsi-7" href="./acl-2012-Syntactic_Stylometry_for_Deception_Detection.html">190 acl-2012-Syntactic Stylometry for Deception Detection</a></p>
<p>8 0.4392767 <a title="11-lsi-8" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>9 0.43735337 <a title="11-lsi-9" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>10 0.43361923 <a title="11-lsi-10" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>11 0.42908743 <a title="11-lsi-11" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>12 0.42118242 <a title="11-lsi-12" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>13 0.41116378 <a title="11-lsi-13" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>14 0.40795946 <a title="11-lsi-14" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>15 0.40640861 <a title="11-lsi-15" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>16 0.40288737 <a title="11-lsi-16" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>17 0.39603084 <a title="11-lsi-17" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>18 0.39107984 <a title="11-lsi-18" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>19 0.38670546 <a title="11-lsi-19" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>20 0.37158611 <a title="11-lsi-20" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (8, 0.056), (22, 0.067), (26, 0.011), (31, 0.035), (38, 0.022), (46, 0.248), (47, 0.048), (58, 0.047), (77, 0.069), (91, 0.11), (92, 0.085), (94, 0.017), (96, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78389335 <a title="11-lda-1" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>Author: Preethi Raghavan ; Albert Lai ; Eric Fosler-Lussier</p><p>Abstract: We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen’s temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the Timebank corpus. This finding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range ofdomains to fully understand the nature of temporal ordering.</p><p>same-paper 2 0.74757922 <a title="11-lda-2" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>Author: Dave Golland ; John DeNero ; Jakob Uszkoreit</p><p>Abstract: We present LLCCM, a log-linear variant ofthe constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% bracketing F1 and outperforms a right-branching baseline in regimes where CCM does not.</p><p>3 0.72707021 <a title="11-lda-3" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>Author: Andrejs Vasiljevs ; Raivis Skadins ; Jorg Tiedemann</p><p>Abstract: Machine Translation Raivis Skadiņš TILDE Vienbas gatve 75a, Riga LV-1004, LATVIA raivi s . s kadins @ ti lde . lv Jörg Tiedemann Uppsala University Box 635, Uppsala SE-75 126, SWEDEN j org .t iedemann@ l ingfi l .uu . se the Universities of Copenhagen, and Uppsala. Edinburgh, Zagreb, To facilitate the creation and usage of custom SMT systems we have created a cloud-based platform for do-it-yourself MT. The platform is developed in the EU collaboration project LetsMT! . This system demonstration paper presents the motivation in developing the LetsMT! platform, its main features, architecture, and an evaluation in a practical use case. 1</p><p>4 0.70015758 <a title="11-lda-4" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>Author: Chang Liu ; Hwee Tou Ng</p><p>Abstract: In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. –</p><p>5 0.59698045 <a title="11-lda-5" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>Author: Sungchul Kim ; Kristina Toutanova ; Hwanjo Yu</p><p>Abstract: In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.</p><p>6 0.59539676 <a title="11-lda-6" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>7 0.5915857 <a title="11-lda-7" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>8 0.59075344 <a title="11-lda-8" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>9 0.58998191 <a title="11-lda-9" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>10 0.58525443 <a title="11-lda-10" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>11 0.58380353 <a title="11-lda-11" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>12 0.58243674 <a title="11-lda-12" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>13 0.58218849 <a title="11-lda-13" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>14 0.58201706 <a title="11-lda-14" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>15 0.58109099 <a title="11-lda-15" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>16 0.58081698 <a title="11-lda-16" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>17 0.58080322 <a title="11-lda-17" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>18 0.58019257 <a title="11-lda-18" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>19 0.5792467 <a title="11-lda-19" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>20 0.57913792 <a title="11-lda-20" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
