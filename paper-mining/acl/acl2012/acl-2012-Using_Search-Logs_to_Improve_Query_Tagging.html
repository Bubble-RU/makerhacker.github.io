<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>212 acl-2012-Using Search-Logs to Improve Query Tagging</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-212" href="#">acl2012-212</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>212 acl-2012-Using Search-Logs to Improve Query Tagging</h1>
<br/><p>Source: <a title="acl-2012-212-pdf" href="http://aclweb.org/anthology//P/P12/P12-2047.pdf">pdf</a></p><p>Author: Kuzman Ganchev ; Keith Hall ; Ryan McDonald ; Slav Petrov</p><p>Abstract: Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.</p><p>Reference: <a title="acl-2012-212-reference" href="../acl2012_reference/acl-2012-Using_Search-Logs_to_Improve_Query_Tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com av}  Abstract Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. [sent-3, score-0.757]
</p><p>2 We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. [sent-4, score-0.888]
</p><p>3 Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. [sent-6, score-0.024]
</p><p>4 Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis. [sent-7, score-0.802]
</p><p>5 1 Introduction Syntactic analysis of search queries is important for a variety of tasks including better query refinement, improved matching and better ad targeting (Barr  et al. [sent-8, score-0.839]
</p><p>6 However, search queries differ substantially from traditional forms of written language (e. [sent-10, score-0.45]
</p><p>7 In this paper we focus on part-of-speech (POS) tagging queries entered into commercial search engines and compare different strategies for learning from search logs. [sent-15, score-0.717]
</p><p>8 The search logs consist of user queries and relevant search results retrieved by a search engine. [sent-16, score-0.909]
</p><p>9 We use a supervised POS tagger to label the result snippets and then transfer the tags to the queries, producing a set of noisy labeled queries. [sent-17, score-0.916]
</p><p>10 These labeled queries are then added to the training data and 238 the tagger is retrained. [sent-18, score-0.567]
</p><p>11 We evaluate different strategies for selecting which annotation to transfer and find that using the result that was clicked by the user gives comparable performance to using just the top result or to aggregating over the top-k results. [sent-19, score-0.609]
</p><p>12 In their work, un-  igram POS tag priors generated from a large corpus are blended with information from the top-50 results from a search engine at prediction time. [sent-22, score-0.287]
</p><p>13 Such an approach has the disadvantage that it necessitates access to a search engine at run-time and is computationally very expensive. [sent-23, score-0.261]
</p><p>14 We re-implement their method and show that our direct transfer approach is more effective, while being simpler to instrument: since we use information from the search engine only during training, we can train a stand-alone POS tagger that can be run without access to additional resources. [sent-24, score-0.691]
</p><p>15 We also perform an error analysis and find that most of the remaining errors are due to errors in POS tagging of the snippets. [sent-25, score-0.145]
</p><p>16 (201 1), is that standard NLP annotation tools work better on snippets returned by a search engine than on user supplied queries. [sent-28, score-0.726]
</p><p>17 This is because snippets are typically well-formed English sentences, while queries are not. [sent-29, score-0.747]
</p><p>18 Our goal is to leverage this observation and use a supervised POS tagger trained on regular English sentences to gen-  erate annotations for a large set of queries that can be used for training a query-specific model. [sent-30, score-0.567]
</p><p>19 Perhaps the simplest approach but also a surprisingly powerful one is to POS tag some relevant snippets for –  –  Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-31, score-0.583]
</p><p>20 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi2c 3s8–242, a given query, and then to transfer the tags from the snippet tokens to matching query tokens. [sent-33, score-1.03]
</p><p>21 This “direct” transfer idea is at the core of all our experiments. [sent-34, score-0.215]
</p><p>22 In this work, we provide a comparison of techniques for selecting snippets associated with the query, as well as an evaluation of methods for aligning the matching words in the query to those in the selected snippets. [sent-35, score-0.886]
</p><p>23 Specifically, for each query1 with a corresponding set of “relevant snippets,” we first apply the baseline tagger to the query and all the snippets. [sent-36, score-0.534]
</p><p>24 We match any query terms in these snippets, and copy over the POS tag to the matching query term. [sent-37, score-0.794]
</p><p>25 Note that this can produce multiple labelings as the relevant snippet set can be very diverse and varies even for the same query. [sent-38, score-0.385]
</p><p>26 We choose the most frequent tagging as the canonical one and add it to our training set. [sent-39, score-0.096]
</p><p>27 We then train a query tagger on all our training data:  the original human annotated English sentences and also the automatically generated query training set. [sent-40, score-0.816]
</p><p>28 The simplest way to match query tokens to snippet tokens is to allow a query token to match any snippet token. [sent-41, score-1.385]
</p><p>29 This can be problematic when we have queries that have a token repeated with different parts-of-speech such as in “tie a tie. [sent-42, score-0.338]
</p><p>30 ” To make a more precise matching we try a sequence of matching rules: First, exact match of the query n-gram. [sent-43, score-0.557]
</p><p>31 Then matching the terms in order, so the query “tiea a tieb” matched to the snippet “to tie1 a neck tie2” would match tiea:tie1 and tieb:tie2. [sent-44, score-0.815]
</p><p>32 Finally, we match as many query terms as possible. [sent-45, score-0.339]
</p><p>33 An early observation showed that when a query term occurs in the result URL, e. [sent-46, score-0.28]
</p><p>34 , searching for “irs mileage rate” results in the page i . [sent-48, score-0.029]
</p><p>35 gov, the query term rs matching the URL domain name is usually a proper noun. [sent-49, score-0.421]
</p><p>36 In the context of search logs, a relevant snippet set can refer to the top k snippets (including the case where k = 1) or the snippet(s) associated with results clicked by users that issued the query. [sent-51, score-1.092]
</p><p>37 In our experiments we found that different strategies for selecting relevant snippets, such as selecting the snip-  pets of the clicked results, using the top-10 results or using only the top result, perform similarly (see Table 1). [sent-52, score-0.417]
</p><p>38 com, since syntactic analysis of such queries is not useful. [sent-55, score-0.362]
</p><p>39 (2010) use a linear interpolation between a prior probability and the snippet tagging. [sent-60, score-0.349]
</p><p>40 They define π(t|w) as the relative frequency ogifn tag Tt given by th π(et |bwas)e alsin eth tagger vtoe word w in some corpus and ψ(t|w, s) as the indicawtoro rfdun wct iinon s fmore w coorrpdu w nind t ψhe( tc|won,tsex)t a so tfh snippet s  ×  has tag t. [sent-61, score-0.701]
</p><p>41 The numbered rows of the table correspond to three snippets (with non-query terms elided). [sent-65, score-0.434]
</p><p>42 The strategy that uses the clicks to select the tagging would count two examples of “Budget/NNP Rent/NNP A/NNP Car/NNP” and one for each of two other taggings. [sent-66, score-0.117]
</p><p>43 Note that snippet 1 and the query get different taggings primarily due to orthographic variations. [sent-67, score-0.592]
</p><p>44 It would then add “budget/NNP rent/NNP a/NNP car/NNP” to its training set. [sent-68, score-0.027]
</p><p>45 (2010) would tag the query as “budget/NNP rent/VB a/DET car/NN”. [sent-70, score-0.346]
</p><p>46 3  Experimental Setup  We assume that we have access to labeled English sentences from the PennTreebank (Marcus et al. [sent-76, score-0.046]
</p><p>47 , 2006), as well as large amounts of unlabeled search queries. [sent-78, score-0.136]
</p><p>48 Each query is paired with a set of relevant results represented by snippets (sentence fragments containing the search terms), as well as information about the order in which the results were shown to the user and possibly the result the user clicked on. [sent-79, score-1.14]
</p><p>49 Note that different sets of results are possible for the same query, because of personalization and ranking changes over time. [sent-80, score-0.027]
</p><p>50 The first is the set of 251 queries from Microsoft search logs (MS25 1) used in Bendersky et al. [sent-83, score-0.547]
</p><p>51 The queries are annotated with three POS tags representing nouns, verbs and “other” tags (MS-251 NVX). [sent-85, score-0.518]
</p><p>52 We additionally refine the annotation to cover 14 POS tags comprising the 12 universal tags of Petrov et al. [sent-86, score-0.283]
</p><p>53 (2012), as well as proper nouns and a special tag for search operator symbols such as “-” (for excluding the subsequent word). [sent-87, score-0.242]
</p><p>54 We had two annotators annotate the whole of the MS25 1 data set. [sent-89, score-0.072]
</p><p>55 We then examined all the instances where the annotators disagreed, and corrected the discrepancy. [sent-95, score-0.052]
</p><p>56 These are queries that occurred rarely in the search logs, and are typically difficult to tag because they are searching for lessfrequent information. [sent-100, score-0.545]
</p><p>57 2 Baseline Model We use a linear chain tagger trained with the averaged perceptron (Collins, 2002). [sent-103, score-0.232]
</p><p>58 We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. [sent-104, score-0.035]
</p><p>59 When training on Sections 1-18 of the Penn Treebank and testing on sections 22-24, our tagger achieves 97. [sent-105, score-0.259]
</p><p>60 22% accuracy with the Penn Treebank tag set, which is state-of-the-art for this data set. [sent-106, score-0.066]
</p><p>61 When we  evaluate only on the 14 tags used in our experiments, the accuracy increases to 97. [sent-107, score-0.09]
</p><p>62 We experimented with 4 baseline taggers (see Table 2). [sent-109, score-0.088]
</p><p>63 WSJ corresponds to training on only the standard training sections of Wall Street Journal portion of the Penn Treebank. [sent-110, score-0.084]
</p><p>64 WSJ NOCASE and WSJ+QTB NOCASE use case-insensitive version of the tagger (conceptually lowercasing the text before training and before applying the tagger). [sent-116, score-0.229]
</p><p>65 As we will see, all our baseline models are better than the baseline reported in Bendersky et al. [sent-117, score-0.104]
</p><p>66 (2010); our lowercased baseline model significantly outperforms even their best model. [sent-118, score-0.076]
</p><p>67 4  Experiments  First, we compared different strategies for selecting relevant snippets from which to transfer the tags. [sent-119, score-0.812]
</p><p>68 These systems are: DIRECT-CLICK, which uses snippets clicked on by users; DIRECT-ALL, which uses all the returned snippets seen by the user;2 and DIRECT-TOP- 1, which uses just the snippet in the top result. [sent-120, score-1.298]
</p><p>69 While DIRECT-ALL and DIRECT-TOP- 1 perform best on the MS-251 data sets, DIRECT-CLICK has an advantage on the long tail queries. [sent-122, score-0.051]
</p><p>70 6%) suggesting that any strategy for selecting relevant snippet sets will return comparable results when aggregated over large amounts of data. [sent-124, score-0.497]
</p><p>71 We then compared our method to the baseline models and a re-implementation of Bendersky et al. [sent-125, score-0.052]
</p><p>72 We use the same matching scheme for both BSC and our system, including the URL matching described in Section 2. [sent-127, score-0.218]
</p><p>73 6% for their final system, which is comparable to our implementation of their system when the baseline tagger is trained on just the WSJ corpus. [sent-134, score-0.279]
</p><p>74 Some other trends become appar2Usually 10 results, but more if the user viewed the second page of results. [sent-137, score-0.065]
</p><p>75 Column MS-251 NVX evaluates with tags from Bendersky et al. [sent-140, score-0.09]
</p><p>76 We observe snippets for 2/500 long-tail queries and 3 1/25 1 MS-25 1 queries. [sent-147, score-0.747]
</p><p>77 Firstly, a large part of the benefit of transfer has to do with case information that is available in the snippets but is missing in the query. [sent-149, score-0.624]
</p><p>78 The uncased tagger is insensitive to this mismatch and achieves significantly better results than the cased taggers. [sent-150, score-0.27]
</p><p>79 However, transferring information from the snippets provides additional benefits, significantly improving even the uncased baseline taggers. [sent-151, score-0.577]
</p><p>80 Finally, we see that the direct transfer method from Section 2 significantly outperforms the method described in Bendersky et al. [sent-154, score-0.252]
</p><p>81 Table 3 confirms this trend when focusing on proper nouns, which are particularly difficult to identify in queries. [sent-156, score-0.032]
</p><p>82 We also manually examined a set of 40 queries with their associated snippets, for which our best DIRECT-CLICK system made mistakes. [sent-157, score-0.391]
</p><p>83 In 32 cases, the errors in the query tagging could be traced back  to errors in the snippet tagging. [sent-158, score-0.738]
</p><p>84 In the remaining 8 cases there were problems with the matching either the mis-tagged word was not found at all, or it was matched incorrectly. [sent-160, score-0.134]
</p><p>85 For example one of the results for the query “bell helmet” had a snippet containing “Bell cycling helmets” and we failed to match helmet to helmets. [sent-161, score-0.719]
</p><p>86 e30471l28ong-  241  tail data for the best baseline method and the three transfer methods using that baseline. [sent-165, score-0.318]
</p><p>87 (2008) manually annotate a corpus of  2722 queries with 19 POS tags and use it to train and evaluate POS taggers, and also describe the linguistic structures they find. [sent-167, score-0.476]
</p><p>88 (201 1) create features based on search engine results, that they use in an NER system applied to queries. [sent-170, score-0.191]
</p><p>89 They report report significant improvements when incorporating features from the snippets. [sent-171, score-0.056]
</p><p>90 In particular, they exploit capitalization and query terms matching URL components; both of which we have used in this work. [sent-172, score-0.434]
</p><p>91 (2009) use clicks in a product data base to train a tagger for product queries, but they do not use snippets and do not annotate syntax. [sent-174, score-0.707]
</p><p>92 Li (2010) and Manshadi and Li (2009) also work on adding tags to queries, but do not use snippets or search logs as a source of information. [sent-175, score-0.708]
</p><p>93 6  Conclusions  We described a simple method for training a searchquery POS tagger from search-logs by transferring context from relevant snippet sets to query terms. [sent-176, score-0.942]
</p><p>94 We compared our approach to previous work, achieving an error reduction of 20%. [sent-177, score-0.024]
</p><p>95 (2010), our approach does not require access to the search engine or index when tagging a new query. [sent-179, score-0.306]
</p><p>96 By explicitly re-training our final model, it has the ability to pool knowledge from several related queries and incorporate the information into the model parameters. [sent-180, score-0.362]
</p><p>97 An area for future work is to transfer other syntactic information, such as parse structures or supertags using a similar transfer approach. [sent-181, score-0.481]
</p><p>98 Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. [sent-200, score-0.057]
</p><p>99 Extracting structured information from user queries with semisupervised conditional random fields. [sent-214, score-0.403]
</p><p>100 Piggyback: Using search engines for robust cross-domain named entity recognition. [sent-246, score-0.146]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('snippets', 0.409), ('bendersky', 0.356), ('queries', 0.338), ('snippet', 0.312), ('query', 0.28), ('transfer', 0.215), ('tagger', 0.202), ('barr', 0.178), ('clicked', 0.136), ('search', 0.112), ('matching', 0.109), ('logs', 0.097), ('url', 0.094), ('tags', 0.09), ('questionbank', 0.089), ('pos', 0.087), ('engine', 0.079), ('wsj', 0.079), ('relevant', 0.073), ('tagging', 0.069), ('helmet', 0.068), ('manshadi', 0.068), ('nocase', 0.068), ('nvx', 0.068), ('qtb', 0.068), ('tieb', 0.068), ('uncased', 0.068), ('tag', 0.066), ('user', 0.065), ('selecting', 0.063), ('tiea', 0.059), ('match', 0.059), ('baseline', 0.052), ('strategies', 0.052), ('tail', 0.051), ('annotate', 0.048), ('clicks', 0.048), ('transferring', 0.048), ('access', 0.046), ('uszkoreit', 0.045), ('capitalization', 0.045), ('penn', 0.044), ('bell', 0.043), ('navigational', 0.042), ('universal', 0.039), ('direct', 0.037), ('interpolation', 0.037), ('taggers', 0.036), ('petrov', 0.035), ('additionally', 0.035), ('simplest', 0.035), ('engines', 0.034), ('nouns', 0.032), ('proper', 0.032), ('returned', 0.032), ('perceptron', 0.03), ('sections', 0.03), ('iinon', 0.03), ('neck', 0.03), ('rent', 0.03), ('piggyback', 0.03), ('pets', 0.03), ('blended', 0.03), ('pseudorelevance', 0.03), ('annotation', 0.029), ('searching', 0.029), ('report', 0.028), ('examined', 0.028), ('judge', 0.028), ('training', 0.027), ('supertags', 0.027), ('instrument', 0.027), ('personalization', 0.027), ('li', 0.027), ('marcus', 0.026), ('errors', 0.026), ('tics', 0.025), ('issued', 0.025), ('disagreed', 0.025), ('aoife', 0.025), ('rosie', 0.025), ('nind', 0.025), ('traced', 0.025), ('numbered', 0.025), ('associated', 0.025), ('matched', 0.025), ('comparable', 0.025), ('error', 0.024), ('tokens', 0.024), ('pool', 0.024), ('kuzman', 0.024), ('lowercased', 0.024), ('necessitates', 0.024), ('uller', 0.024), ('aggregating', 0.024), ('irs', 0.024), ('transferred', 0.024), ('amounts', 0.024), ('annotators', 0.024), ('syntactic', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="212-tfidf-1" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>Author: Kuzman Ganchev ; Keith Hall ; Ryan McDonald ; Slav Petrov</p><p>Abstract: Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.</p><p>2 0.25879842 <a title="212-tfidf-2" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>Author: Patrick Pantel ; Thomas Lin ; Michael Gamon</p><p>Abstract: We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent significantly improves entity type resolution for head queries over the state ofthe art, on several metrics, without degradation in tail query performance.</p><p>3 0.24217007 <a title="212-tfidf-3" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>Author: Joseph Z. Chang ; Jason S. Chang ; Roger Jyh-Shing Jang</p><p>Abstract: Jason S. Chang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j s chang@ c s .nthu . edu .tw Jyh-Shing Roger Jang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j ang@ c s .nthu .edu .tw identifying such translation counterparts Web, we can cope with the OOV problem. In this paper, we present a new method on the for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 1</p><p>4 0.15830116 <a title="212-tfidf-4" href="./acl-2012-CSNIPER_-_Annotation-by-query_for_Non-canonical_Constructions_in_Large_Corpora.html">44 acl-2012-CSNIPER - Annotation-by-query for Non-canonical Constructions in Large Corpora</a></p>
<p>Author: Richard Eckart de Castilho ; Sabine Bartsch ; Iryna Gurevych</p><p>Abstract: We present CSNIPER (Corpus Sniper), a tool that implements (i) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and (ii) evaluation of annotation quality by measuring inter-rater agreement. This annotationby-query approach efficiently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools.</p><p>5 0.15651685 <a title="212-tfidf-5" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>Author: Zhi Zhong ; Hwee Tou Ng</p><p>Abstract: Previous research has conflicting conclusions on whether word sense disambiguation (WSD) systems can improve information retrieval (IR) performance. In this paper, we propose a method to estimate sense distributions for short queries. Together with the senses predicted for words in documents, we propose a novel approach to incorporate word senses into the language modeling approach to IR and also exploit the integration of synonym relations. Our experimental results on standard TREC collections show that using the word senses tagged by a supervised WSD system, we obtain significant improvements over a state-of-the-art IR system.</p><p>6 0.12419735 <a title="212-tfidf-6" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>7 0.11969363 <a title="212-tfidf-7" href="./acl-2012-Automatically_Mining_Question_Reformulation_Patterns_from_Search_Log_Data.html">35 acl-2012-Automatically Mining Question Reformulation Patterns from Search Log Data</a></p>
<p>8 0.10745711 <a title="212-tfidf-8" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>9 0.10584085 <a title="212-tfidf-9" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>10 0.094798028 <a title="212-tfidf-10" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>11 0.093083918 <a title="212-tfidf-11" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>12 0.088832989 <a title="212-tfidf-12" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>13 0.079460889 <a title="212-tfidf-13" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>14 0.07739412 <a title="212-tfidf-14" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>15 0.077043682 <a title="212-tfidf-15" href="./acl-2012-A_Joint_Model_for_Discovery_of_Aspects_in_Utterances.html">14 acl-2012-A Joint Model for Discovery of Aspects in Utterances</a></p>
<p>16 0.076484226 <a title="212-tfidf-16" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>17 0.072359741 <a title="212-tfidf-17" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>18 0.070960447 <a title="212-tfidf-18" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>19 0.064809643 <a title="212-tfidf-19" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>20 0.063847445 <a title="212-tfidf-20" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.182), (1, 0.046), (2, -0.056), (3, -0.001), (4, 0.081), (5, 0.196), (6, 0.076), (7, -0.068), (8, -0.0), (9, -0.055), (10, 0.144), (11, 0.098), (12, 0.079), (13, 0.086), (14, 0.053), (15, -0.098), (16, 0.116), (17, 0.036), (18, 0.128), (19, -0.108), (20, 0.049), (21, 0.382), (22, 0.15), (23, 0.085), (24, -0.013), (25, -0.112), (26, 0.264), (27, 0.09), (28, -0.011), (29, 0.08), (30, -0.079), (31, 0.044), (32, 0.049), (33, -0.092), (34, 0.038), (35, -0.037), (36, -0.04), (37, -0.054), (38, -0.057), (39, 0.032), (40, 0.087), (41, -0.007), (42, -0.04), (43, -0.038), (44, 0.071), (45, 0.051), (46, 0.036), (47, -0.019), (48, 0.017), (49, -0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96466947 <a title="212-lsi-1" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>Author: Kuzman Ganchev ; Keith Hall ; Ryan McDonald ; Slav Petrov</p><p>Abstract: Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.</p><p>2 0.79135203 <a title="212-lsi-2" href="./acl-2012-CSNIPER_-_Annotation-by-query_for_Non-canonical_Constructions_in_Large_Corpora.html">44 acl-2012-CSNIPER - Annotation-by-query for Non-canonical Constructions in Large Corpora</a></p>
<p>Author: Richard Eckart de Castilho ; Sabine Bartsch ; Iryna Gurevych</p><p>Abstract: We present CSNIPER (Corpus Sniper), a tool that implements (i) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and (ii) evaluation of annotation quality by measuring inter-rater agreement. This annotationby-query approach efficiently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools.</p><p>3 0.777776 <a title="212-lsi-3" href="./acl-2012-Mining_Entity_Types_from_Query_Logs_via_User_Intent_Modeling.html">142 acl-2012-Mining Entity Types from Query Logs via User Intent Modeling</a></p>
<p>Author: Patrick Pantel ; Thomas Lin ; Michael Gamon</p><p>Abstract: We predict entity type distributions in Web search queries via probabilistic inference in graphical models that capture how entitybearing queries are generated. We jointly model the interplay between latent user intents that govern queries and unobserved entity types, leveraging observed signals from query formulations and document clicks. We apply the models to resolve entity types in new queries and to assign prior type distributions over an existing knowledge base. Our models are efficiently trained using maximum likelihood estimation over millions of real-world Web search queries. We show that modeling user intent significantly improves entity type resolution for head queries over the state ofthe art, on several metrics, without degradation in tail query performance.</p><p>4 0.64326495 <a title="212-lsi-4" href="./acl-2012-Automatically_Mining_Question_Reformulation_Patterns_from_Search_Log_Data.html">35 acl-2012-Automatically Mining Question Reformulation Patterns from Search Log Data</a></p>
<p>Author: Xiaobing Xue ; Yu Tao ; Daxin Jiang ; Hang Li</p><p>Abstract: Natural language questions have become popular in web search. However, various questions can be formulated to convey the same information need, which poses a great challenge to search systems. In this paper, we automatically mined 5w1h question reformulation patterns from large scale search log data. The question reformulations generated from these patterns are further incorporated into the retrieval model. Experiments show that using question reformulation patterns can significantly improve the search performance of natural language questions.</p><p>5 0.61723137 <a title="212-lsi-5" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>Author: Wan-Yu Lin ; Nanyun Peng ; Chun-Chao Yen ; Shou-de Lin</p><p>Abstract: In this paper, we introduce a framework that identifies online plagiarism by exploiting lexical, syntactic and semantic features that includes duplication-gram, reordering and alignment of words, POS and phrase tags, and semantic similarity of sentences. We establish an ensemble framework to combine the predictions of each model. Results demonstrate that our system can not only find considerable amount of real-world online plagiarism cases but also outperforms several state-of-the-art algorithms and commercial software. Keywords Plagiarism Detection, Lexical, Syntactic, Semantic 1.</p><p>6 0.50324732 <a title="212-lsi-6" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>7 0.47468331 <a title="212-lsi-7" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>8 0.40357941 <a title="212-lsi-8" href="./acl-2012-A_Joint_Model_for_Discovery_of_Aspects_in_Utterances.html">14 acl-2012-A Joint Model for Discovery of Aspects in Utterances</a></p>
<p>9 0.38077417 <a title="212-lsi-9" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>10 0.36469334 <a title="212-lsi-10" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>11 0.36198056 <a title="212-lsi-11" href="./acl-2012-Reducing_Approximation_and_Estimation_Errors_for_Chinese_Lexical_Processing_with_Heterogeneous_Annotations.html">168 acl-2012-Reducing Approximation and Estimation Errors for Chinese Lexical Processing with Heterogeneous Annotations</a></p>
<p>12 0.35519955 <a title="212-lsi-12" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>13 0.35310987 <a title="212-lsi-13" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>14 0.35114011 <a title="212-lsi-14" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>15 0.30811012 <a title="212-lsi-15" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>16 0.29418278 <a title="212-lsi-16" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>17 0.28007469 <a title="212-lsi-17" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<p>18 0.27416566 <a title="212-lsi-18" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>19 0.27285302 <a title="212-lsi-19" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>20 0.26107192 <a title="212-lsi-20" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.018), (26, 0.037), (28, 0.017), (30, 0.017), (37, 0.014), (39, 0.031), (74, 0.029), (85, 0.013), (90, 0.644), (92, 0.034), (94, 0.014), (99, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99812108 <a title="212-lda-1" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>Author: Kuzman Ganchev ; Keith Hall ; Ryan McDonald ; Slav Petrov</p><p>Abstract: Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.</p><p>2 0.99745107 <a title="212-lda-2" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.</p><p>3 0.99632496 <a title="212-lda-3" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; George Foster ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</p><p>4 0.99425733 <a title="212-lda-4" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>5 0.99023253 <a title="212-lda-5" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Xiao Jiang</p><p>Abstract: Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a . 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</p><p>6 0.96414053 <a title="212-lda-6" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>7 0.96195483 <a title="212-lda-7" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>8 0.94915795 <a title="212-lda-8" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>9 0.94861823 <a title="212-lda-9" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>10 0.94706339 <a title="212-lda-10" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>11 0.94319624 <a title="212-lda-11" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>12 0.94034928 <a title="212-lda-12" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>13 0.93989986 <a title="212-lda-13" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>14 0.93988961 <a title="212-lda-14" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>15 0.9357332 <a title="212-lda-15" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>16 0.92840266 <a title="212-lda-16" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>17 0.92797142 <a title="212-lda-17" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>18 0.92630941 <a title="212-lda-18" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>19 0.9212417 <a title="212-lda-19" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>20 0.91800958 <a title="212-lda-20" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
