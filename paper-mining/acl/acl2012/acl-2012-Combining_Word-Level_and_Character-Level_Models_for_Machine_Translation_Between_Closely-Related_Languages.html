<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-54" href="#">acl2012-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</h1>
<br/><p>Source: <a title="acl-2012-54-pdf" href="http://aclweb.org/anthology//P/P12/P12-2059.pdf">pdf</a></p><p>Author: Preslav Nakov ; Jorg Tiedemann</p><p>Abstract: We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.</p><p>Reference: <a title="acl-2012-54-reference" href="../acl2012_reference/acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 qa  Abstract We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. [sent-5, score-0.321]
</p><p>2 We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. [sent-6, score-0.756]
</p><p>3 The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2. [sent-7, score-0.085]
</p><p>4 1 Introduction  Statistical machine translation (SMT) systems, require parallel corpora of sentences and their translations, called bitexts, which are often not sufficiently large. [sent-9, score-0.19]
</p><p>5 However, for many closely-related languages, SMT can be carried out even with small bitexts by exploring relations below the word level. [sent-10, score-0.082]
</p><p>6 Closely-related languages such as Macedonian and Bulgarian exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. [sent-11, score-0.101]
</p><p>7 Spelling conventions in such related languages can still be different, and they may diverge more substantially at the level of morphology. [sent-12, score-0.056]
</p><p>8 However, the differences often constitute consistent regularities that can be generalized when translating. [sent-13, score-0.077]
</p><p>9 The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al. [sent-14, score-0.454]
</p><p>10 301 J o¨rg Tiedemann Department of Linguistics and Philology Uppsala University  Uppsala, Sweden j org . [sent-16, score-0.028]
</p><p>11 s e  Table1:Exadvme vpMkrlameucase vdfrtomadn omiea jn chBa u,vhrqlagcmaetr eiatv-nledoav mei$lphrasetbl  (without scores): mappings can cover words and phrases. [sent-19, score-0.064]
</p><p>12 Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. [sent-20, score-0.221]
</p><p>13 , 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. [sent-22, score-0.085]
</p><p>14 This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. [sent-23, score-0.064]
</p><p>15 Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. [sent-24, score-0.147]
</p><p>16 Thus, character-level phrase-based SMT models combine the generality of character-by-character transliteration and lexical mappings of larger units  that could possibly refer to morphemes, words or phrases, as well as to various combinations thereof. [sent-25, score-0.446]
</p><p>17 2  Training Character-level SMT Models  We treat sentences as sequences of characters instead of words, as shown in Figure 1. [sent-26, score-0.116]
</p><p>18 In our case, we opted for a 10-character language model and a maximum phrase length of 10 (based on initial experiments). [sent-28, score-0.051]
</p><p>19 However, word alignment models are not fit for character-level SMT, where the vocabulary shrinks. [sent-29, score-0.087]
</p><p>20 s n  character bigrams: MK: na av vi is st ti in na a ? [sent-36, score-0.199]
</p><p>21 Statistical word alignment models heavily rely on  context-independent lexical translation parameters and, therefore, are unable to properly distinguish character mapping differences in various contexts. [sent-41, score-0.289]
</p><p>22 The alignment models used in the transliteration literature have the same problem as they are usually based on edit distance operations and finite-state automata without contextual history (Jiampojamarn et al. [sent-42, score-0.369]
</p><p>23 We, thus, transformed the input to sequences ofcharacter n-grams as suggested by Tiedemann (2012); examples are shown in Figure 1. [sent-45, score-0.048]
</p><p>24 This artificially increases the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. [sent-46, score-0.245]
</p><p>25 Macedonian Bulgarian single characters99101 character bigrams 1,851 1,893 character trigrams 13,794 14,305  words41,81630,927  Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. [sent-47, score-0.268]
</p><p>26 It turns out that bigrams constitute a good compromise between generality and contextual speci-  ficity, which yields useful character alignments with good performance in terms of phrase-based translation. [sent-48, score-0.272]
</p><p>27 In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al. [sent-49, score-0.073]
</p><p>28 1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al. [sent-53, score-0.039]
</p><p>29 1Note that the extracted phrase table does not include sequences of character n-grams. [sent-55, score-0.188]
</p><p>30 We map character n-gram alignments to links between single characters before extraction. [sent-56, score-0.23]
</p><p>31 302 Since BLEU over matching character sequences does not make much sense, especially if the k-gram size is limited to small values of k (usually, 4 or less), we post-processed n-best lists in each tuning step to calculate the usual word-based BLEU score. [sent-57, score-0.137]
</p><p>32 1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al. [sent-60, score-0.227]
</p><p>33 Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we caltciuolnaste fdo trh teh quantity Piv(m, b) = Pr(m|b) Pr(b|m). [sent-66, score-0.238]
</p><p>34 cWulea tceadlc tuhleat qeuda a istiym Piliavr( quantity Dir(m, b), (wb|hmer)e. [sent-67, score-0.035]
</p><p>35 Here we give equal weight to Dir(m, b) and Piv(m, b) ; we also give equal weights to the translational similarity (the sum of the first two terms) and to the spelling similarity (twice LCSR). [sent-73, score-0.141]
</p><p>36 Finally, using S(m, b), we induced a weighted bipartite graph, and we performed a greedy approximation to the maximum weighted bipartite matching  in that graph using competitive linking (Melamed, 2000), to produce the final list of cognate pairs. [sent-78, score-0.298]
</p><p>37 Note that the above-described cognate extraction algorithm has three important components: (1) orthographic, based on LCSR, (2) semantic, based on word alignments and pivoting over English, and (3) competitive linking. [sent-79, score-0.246]
</p><p>38 The orthographic component is essential when looking for cognates since they must have similar spelling by definition, while the semantic component prevents the extraction of false friends like vreden, which means ‘valuable’ in Macedonian but ‘harmful’ in Bulgarian. [sent-80, score-0.196]
</p><p>39 Finally, competitive linking helps prevent issues related to word inflection that cannot be handled using the semantic component alone. [sent-81, score-0.037]
</p><p>40 2 Transliteration Training For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both words, and we further appended special start and end characters. [sent-83, score-0.191]
</p><p>41 We split the resulting list into training, development and testing parts and we trained and tuned a character-level MacedonianBulgarian phrase-based monotone SMT system similar to that in (Finch and Sumita, 2008; Tiedemann  and Nabende, 2009; Nakov and Ng, 2009; Nakov and Ng, 2012). [sent-84, score-0.039]
</p><p>42 We set the maximum phrase length and the language model order to 10, and we tuned the system using MERT. [sent-86, score-0.09]
</p><p>43 4  Experiments and Evaluation  For our experiments, we used translated movie subtitles from the OPUS corpus (Tiedemann, 2009b). [sent-90, score-0.085]
</p><p>44 Thus, we realigned the corpus using hunal ign and we removed some Bulgarian files that were misclassified as Macedonian and vice versa, using a BLEU-filter. [sent-96, score-0.032]
</p><p>45 Furthermore, we also removed sentence pairs containing language-specific characters on the wrong side. [sent-97, score-0.1]
</p><p>46 81  Merged phrase tables m1 m2  w1 + c2 w2 + c2  33. [sent-153, score-0.083]
</p><p>47 First, we can see that the BLEU score for the original Macedonian testset evaluated against the Bulgarian reference is 10. [sent-167, score-0.047]
</p><p>48 The next line (t1) shows that many differences between Macedonian and Bulgarian stem from mere differences in  orthography: we mapped the six letters in the Macedonian alphabet that do not exist in the Bulgarian alphabet to corresponding Bulgarian letters and letter sequences, gaining over 1. [sent-169, score-0.213]
</p><p>49 The following line (t2) shows the results using the sophisticated transliteration described in Section 3, which takes two kinds of context into account: (1) wordinternal letter context, and (2) sentence-level word context. [sent-171, score-0.32]
</p><p>50 We generated a lattice for each Macedonian test sentence, which included the original Macedonian words and the 1-best2 Bulgarian transliteration option from the character-level transliteration model. [sent-172, score-0.703]
</p><p>51 We then decoded the lattice using a Bulgarian language model; this increased BLEU to 22. [sent-173, score-0.125]
</p><p>52 Naturally, lattice-based transliteration cannot really compete against standard word-level translation (w1), which is better by 8 BLEU points. [sent-176, score-0.447]
</p><p>53 Still, as line (w2) shows, using the 1-best transliteration lattice as an input to (w1) yields3 consistent improvement over (w1) for four evaluation metrics: BLEU (Papineni et al. [sent-177, score-0.414]
</p><p>54 org/), a shallow transfer-rulebased MT system that is optimized for closelyrelated languages (accessed on 2012/05/02). [sent-188, score-0.119]
</p><p>55 Here, Apertium suffers badly from a large number of unknown words in our testset (ca. [sent-189, score-0.047]
</p><p>56 , simply treating characters as separate words, performs significantly better than word-level SMT. [sent-194, score-0.068]
</p><p>57 Using bigram-based character alignments yields further improvement of +0. [sent-195, score-0.162]
</p><p>58 3The decoder can choose between (a) translating a Macedonian word and (b) using its 1-best Bulgarian transliteration. [sent-198, score-0.041]
</p><p>59 Since word-level and character-level models have different strengths and weaknesses, we further tried to combine them. [sent-200, score-0.03]
</p><p>60 We used MEMT, a state-of-the-art Multi-Engine Machine Translation system (Heafield and Lavie, 2010), to combine the outputs of (c3) with the out-  put of (w1) and of (w2). [sent-201, score-0.03]
</p><p>61 Both combinations improved over the individual systems, but (w1)+(c2) performed better, by +0. [sent-202, score-0.031]
</p><p>62 Finally, we also combined (w1) with (c3) in a more direct way: by merging their phrase tables. [sent-205, score-0.051]
</p><p>63 First, we split the phrases in the word-level phrase tables of (w1) to characters as in character-level models. [sent-206, score-0.151]
</p><p>64 Then, we generated four versions of each phrase pair: with/without “ ” at the beginning/end of the phrase. [sent-207, score-0.051]
</p><p>65 Finally, we merged these phrase pairs with those in the phrase table of (c3), adding two extra features indicating each phrase pair’s origin: the first/second feature is 1 if the pair came from the first/second table, and 0. [sent-208, score-0.153]
</p><p>66 Finally, we experimented with a 1-best character-level lattice input that encodes the same options and weights as for (w2). [sent-212, score-0.159]
</p><p>67 84  BLEU points of absolute improvement over the (w1) baseline, and +1. [sent-215, score-0.036]
</p><p>68 4 5  Conclusion and Future Work  We have explored several combinations ofcharacterand word-level translation models for translating between closely-related languages with scarce resources. [sent-217, score-0.326]
</p><p>69 In future work, we want to use such a model for pivot-based translations from the resource-poor language (Macedonian) to other languages (such as English) via the related language (Bulgarian). [sent-218, score-0.093]
</p><p>70 97 BLEU points are statistically significant according to Collins’ sign test (Collins et al. [sent-222, score-0.036]
</p><p>71 In Proceedings of HLT-NAACL ’06, pages 17–24, New York, NY. [sent-231, score-0.039]
</p><p>72 In Proceedings of ACL ’05, pages 53 1– 540, Ann Arbor, MI. [sent-235, score-0.039]
</p><p>73 In Proceedings of the Workshop on Technologies and Corpora for Asia-Pacific Speech Translation, pages 13–18, Hyderabad, India. [sent-243, score-0.039]
</p><p>74 Combining machine translation output with open source: The Carnegie Mellon multi-engine machine translation scheme. [sent-246, score-0.38]
</p><p>75 Applying many-to-many alignments and hidden Markov models to letter-to-phoneme conversion. [sent-250, score-0.073]
</p><p>76 In Proceedings of NAACL-HLT ’07, pages 372–379, Rochester, New York. [sent-251, score-0.039]
</p><p>77 In Proceedings of NAACL ’03, pages 48–54, Edmonton, Canada. [sent-255, score-0.039]
</p><p>78 In Proceedings of ACL ’07, pages 177–180, Prague, Czech Republic. [sent-259, score-0.039]
</p><p>79 In Proceedings of NAACL ’03, pages 46–48, Edmonton, Canada. [sent-263, score-0.039]
</p><p>80 The Meteor metric for automatic evaluation of machine translation. [sent-266, score-0.032]
</p><p>81 Improved statistical machine translation for resource-poor languages  using related resource-rich languages. [sent-278, score-0.281]
</p><p>82 Improving statistical machine translation for a resource-poor language using related resource-rich languages. [sent-282, score-0.225]
</p><p>83 In Proceedings of ACL ’03, pages 160–167, Sapporo, Japan. [sent-291, score-0.039]
</p><p>84 BLEU: a method for automatic evaluation of machine translation. [sent-294, score-0.032]
</p><p>85 In Proceedings of ACL ’02, pages 3 11–3 18, Philadelphia, PA. [sent-295, score-0.039]
</p><p>86 A study of translation edit rate with targeted human annotation. [sent-302, score-0.196]
</p><p>87 In Proceedings of EAMT ’09, pages 12–19, Barcelona, Spain. [sent-311, score-0.039]
</p><p>88 In Recent Advances in Natural Language Processing, volume V, pages 237–248. [sent-315, score-0.039]
</p><p>89 In Proceedings of EACL ’12, pages 141–15 1, Avignon, France. [sent-320, score-0.039]
</p><p>90 A comparison of pivot methods for phrase-based statistical machine translation. [sent-323, score-0.125]
</p><p>91 In Proceedings of NAACL-HLT ’07, pages 484–491, Rochester, NY. [sent-324, score-0.039]
</p><p>92 In Proceedings of WMT ’07, pages 33–39, Prague, Czech Republic. [sent-328, score-0.039]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('macedonian', 0.409), ('bulgarian', 0.329), ('transliteration', 0.289), ('bg', 0.251), ('bleu', 0.181), ('pr', 0.177), ('mk', 0.167), ('translation', 0.158), ('smt', 0.153), ('cognate', 0.136), ('lcsr', 0.125), ('lattice', 0.125), ('tiedemann', 0.12), ('nakov', 0.11), ('piv', 0.109), ('kondrak', 0.1), ('apertium', 0.094), ('dir', 0.093), ('spelling', 0.091), ('character', 0.089), ('rg', 0.087), ('bitexts', 0.082), ('cognates', 0.075), ('preslav', 0.075), ('alignments', 0.073), ('characters', 0.068), ('qatar', 0.066), ('mappings', 0.064), ('arpe', 0.063), ('closelyrelated', 0.063), ('damper', 0.063), ('memt', 0.063), ('pivot', 0.058), ('languages', 0.056), ('meteor', 0.056), ('ristad', 0.055), ('subtitles', 0.055), ('utiyama', 0.055), ('letters', 0.055), ('na', 0.055), ('en', 0.054), ('koehn', 0.054), ('phrase', 0.051), ('lavie', 0.051), ('jiampojamarn', 0.05), ('translational', 0.05), ('bigrams', 0.048), ('sequences', 0.048), ('regularities', 0.047), ('testset', 0.047), ('finch', 0.047), ('heafield', 0.047), ('opus', 0.047), ('induced', 0.045), ('vocabulary', 0.045), ('vilar', 0.044), ('alignment', 0.042), ('grzegorz', 0.042), ('ict', 0.042), ('philipp', 0.041), ('translating', 0.041), ('bipartite', 0.04), ('scarce', 0.04), ('pages', 0.039), ('tuned', 0.039), ('papineni', 0.039), ('alon', 0.038), ('peter', 0.038), ('edit', 0.038), ('och', 0.037), ('competitive', 0.037), ('rochester', 0.037), ('snover', 0.037), ('translations', 0.037), ('prague', 0.036), ('points', 0.036), ('alternatives', 0.036), ('alphabet', 0.036), ('della', 0.035), ('quantity', 0.035), ('statistical', 0.035), ('josef', 0.034), ('edmonton', 0.034), ('pietra', 0.034), ('options', 0.034), ('tables', 0.032), ('removed', 0.032), ('generality', 0.032), ('machine', 0.032), ('franz', 0.031), ('letter', 0.031), ('combinations', 0.031), ('orthographic', 0.03), ('movie', 0.03), ('constitute', 0.03), ('combine', 0.03), ('alexandra', 0.03), ('chris', 0.03), ('collins', 0.03), ('org', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="54-tfidf-1" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>Author: Preslav Nakov ; Jorg Tiedemann</p><p>Abstract: We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.</p><p>2 0.27933666 <a title="54-tfidf-2" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>3 0.20354275 <a title="54-tfidf-3" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>4 0.20282073 <a title="54-tfidf-4" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>5 0.1492857 <a title="54-tfidf-5" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>Author: Sungchul Kim ; Kristina Toutanova ; Hwanjo Yu</p><p>Abstract: In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.</p><p>6 0.14815156 <a title="54-tfidf-6" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>7 0.1284637 <a title="54-tfidf-7" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>8 0.12508619 <a title="54-tfidf-8" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>9 0.12482926 <a title="54-tfidf-9" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>10 0.11902348 <a title="54-tfidf-10" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>11 0.1183587 <a title="54-tfidf-11" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>12 0.11784817 <a title="54-tfidf-12" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>13 0.11706816 <a title="54-tfidf-13" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>14 0.11529802 <a title="54-tfidf-14" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>15 0.1150079 <a title="54-tfidf-15" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>16 0.1095342 <a title="54-tfidf-16" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>17 0.10878155 <a title="54-tfidf-17" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>18 0.10481273 <a title="54-tfidf-18" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>19 0.10195508 <a title="54-tfidf-19" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>20 0.10026331 <a title="54-tfidf-20" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.247), (1, -0.237), (2, 0.121), (3, 0.073), (4, 0.127), (5, 0.025), (6, 0.02), (7, -0.046), (8, -0.011), (9, -0.047), (10, -0.005), (11, -0.025), (12, -0.02), (13, 0.011), (14, 0.03), (15, 0.006), (16, -0.012), (17, -0.008), (18, 0.009), (19, 0.172), (20, -0.124), (21, -0.025), (22, 0.154), (23, -0.09), (24, -0.058), (25, 0.109), (26, -0.048), (27, 0.021), (28, -0.101), (29, 0.088), (30, -0.116), (31, -0.101), (32, -0.104), (33, -0.077), (34, 0.208), (35, -0.118), (36, 0.057), (37, 0.055), (38, -0.015), (39, -0.022), (40, -0.038), (41, -0.004), (42, 0.166), (43, -0.182), (44, -0.05), (45, 0.038), (46, 0.003), (47, -0.054), (48, 0.08), (49, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89382178 <a title="54-lsi-1" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>Author: Preslav Nakov ; Jorg Tiedemann</p><p>Abstract: We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.</p><p>2 0.8779164 <a title="54-lsi-2" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results.</p><p>3 0.57484609 <a title="54-lsi-3" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>Author: Joseph Z. Chang ; Jason S. Chang ; Roger Jyh-Shing Jang</p><p>Abstract: Jason S. Chang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j s chang@ c s .nthu . edu .tw Jyh-Shing Roger Jang Department of Computer Science, National Tsing Hua University 101, Kuangfu Road, Hsinchu, 300, Taiwan j ang@ c s .nthu .edu .tw identifying such translation counterparts Web, we can cope with the OOV problem. In this paper, we present a new method on the for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At runtime, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work. 1</p><p>4 0.56308067 <a title="54-lsi-4" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>5 0.54471499 <a title="54-lsi-5" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>6 0.48352307 <a title="54-lsi-6" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>7 0.4804318 <a title="54-lsi-7" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>8 0.4598667 <a title="54-lsi-8" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>9 0.45542777 <a title="54-lsi-9" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>10 0.45478293 <a title="54-lsi-10" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>11 0.44494364 <a title="54-lsi-11" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>12 0.44332296 <a title="54-lsi-12" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>13 0.43309003 <a title="54-lsi-13" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>14 0.42503521 <a title="54-lsi-14" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>15 0.42390135 <a title="54-lsi-15" href="./acl-2012-Private_Access_to_Phrase_Tables_for_Statistical_Machine_Translation.html">164 acl-2012-Private Access to Phrase Tables for Statistical Machine Translation</a></p>
<p>16 0.41500649 <a title="54-lsi-16" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>17 0.40786257 <a title="54-lsi-17" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>18 0.39245248 <a title="54-lsi-18" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>19 0.38835841 <a title="54-lsi-19" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>20 0.38627401 <a title="54-lsi-20" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.024), (26, 0.069), (28, 0.067), (30, 0.012), (37, 0.025), (38, 0.262), (39, 0.041), (57, 0.034), (59, 0.014), (74, 0.059), (82, 0.014), (84, 0.036), (85, 0.049), (90, 0.113), (92, 0.033), (94, 0.044), (99, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.74833661 <a title="54-lda-1" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>Author: S.R.K. Branavan ; Nate Kushman ; Tao Lei ; Regina Barzilay</p><p>Abstract: Comprehending action preconditions and effects is an essential step in modeling the dynamics of the world. In this paper, we express the semantics of precondition relations extracted from text in terms of planning operations. The challenge of modeling this connection is to ground language at the level of relations. This type of grounding enables us to create high-level plans based on language abstractions. Our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations. We implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempts. When applied to a complex virtual world and text describing that world, our relation extraction technique performs on par with a supervised baseline, yielding an F-measure of 66% compared to the baseline’s 65%. Additionally, we show that a high-level planner utilizing these extracted relations significantly outperforms a strong, text unaware baseline successfully completing 80% of planning tasks as compared to 69% for the baseline.1 –</p><p>same-paper 2 0.7277056 <a title="54-lda-2" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>Author: Preslav Nakov ; Jorg Tiedemann</p><p>Abstract: We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources. We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model. The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.</p><p>3 0.53301388 <a title="54-lda-3" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>4 0.52686739 <a title="54-lda-4" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>Author: Kevin Duh ; Katsuhito Sudoh ; Xianchao Wu ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</p><p>5 0.52525681 <a title="54-lda-5" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>6 0.52472645 <a title="54-lda-6" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>7 0.52348667 <a title="54-lda-7" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>8 0.5211342 <a title="54-lda-8" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>9 0.52026451 <a title="54-lda-9" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>10 0.51994991 <a title="54-lda-10" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>11 0.51972234 <a title="54-lda-11" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>12 0.51939327 <a title="54-lda-12" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>13 0.51552385 <a title="54-lda-13" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>14 0.5139848 <a title="54-lda-14" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>15 0.51367486 <a title="54-lda-15" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>16 0.51172829 <a title="54-lda-16" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>17 0.51152289 <a title="54-lda-17" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>18 0.51126719 <a title="54-lda-18" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>19 0.51029617 <a title="54-lda-19" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>20 0.50975126 <a title="54-lda-20" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
