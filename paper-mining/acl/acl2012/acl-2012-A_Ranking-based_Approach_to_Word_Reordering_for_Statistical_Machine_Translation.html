<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-19" href="#">acl2012-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2012-19-pdf" href="http://aclweb.org/anthology//P/P12/P12-1096.pdf">pdf</a></p><p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>Reference: <a title="acl-2012-19-reference" href="../acl2012_reference/acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  ,  Abstract Long distance word reordering is a major challenge in statistical machine translation research. [sent-5, score-0.58]
</p><p>2 Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. [sent-6, score-0.248]
</p><p>3 In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. [sent-7, score-0.94]
</p><p>4 The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. [sent-8, score-0.587]
</p><p>5 1 Introduction Modeling word reordering between source and target sentences has been a research focus since the emerging of statistical machine translation. [sent-10, score-0.696]
</p><p>6 The most notable solution to this problem is adopting syntax-based SMT models, especially methods making use of source side syntactic parse trees. [sent-18, score-0.226]
</p><p>7 , 2006) which directly uses source parse trees to derive a large set of translation rules and associated model parameters. [sent-22, score-0.31]
</p><p>8 The other is called syntax pre-reordering an approach that re-positions source words to approximate target language word order as much as possible based on the features from source syntactic parse trees. [sent-23, score-0.481]
</p><p>9 In this paper, we continue this line of work and address the problem of word reordering based on –  source syntactic parse trees for SMT. [sent-25, score-0.753]
</p><p>10 Similar to most previous work, our approach tries to rearrange the source tree nodes sharing a common parent to mimic Proce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-26, score-0.428]
</p><p>11 The ranking model is automatically derived from the word aligned parallel data, viewing the source tree nodes to be reordered as list items to be ranked. [sent-30, score-0.997]
</p><p>12 The ranks oftree nodes are determined by their relative positions in the target language the node in the most front gets the highest rank, while the ending word in the target sentence gets the lowest rank. [sent-31, score-0.444]
</p><p>13 The ranking model is trained to directly minimize the mis-ordering of tree nodes, which differs from the prior work based on maximum likelihood estimations of reordering patterns (Li et al. [sent-32, score-0.993]
</p><p>14 The ranking model can not only be used in a pre-reordering based SMT system, but also be integrated into a phrasebased decoder serving as additional distortion features. [sent-34, score-0.466]
</p><p>15 In the rest of the paper, we will first formally present our ranking-based word reordering model, then followed by detailed steps of modeling training and integration into a phrase-based SMT system. [sent-36, score-0.516]
</p><p>16 2  Word Reordering as Syntax Tree Node Ranking  Given a source side parse tree Te, the task of word reordering is to transform Te to Te0, so that e0 can match the word order in target language as much as possible. [sent-39, score-1.019]
</p><p>17 In this work, we only focus on reordering that can be obtained by permuting children of every tree nodes in Te. [sent-40, score-0.946]
</p><p>18 We use children to denote direct descendants of tree nodes for constituent trees; while for dependency trees, children of a node include not  only all direct dependents, but also the head word itself. [sent-41, score-0.883]
</p><p>19 Figure 1 gives a simple example showing the word reordering between English and Japanese. [sent-42, score-0.516]
</p><p>20 By permuting tree nodes in the parse tree, the source sentence is reordered into the target language order. [sent-44, score-0.787]
</p><p>21 Constituent tree is shown above the source sentence; arrows below the source sentences show head-dependent arcs for dependency tree; word alignment links are lines without arrow between the source and target sentences. [sent-45, score-0.748]
</p><p>22 Following this principle, the word reordering task can be broken into sub-tasks, in which we only need to determine the order of children nodes for all non-leaf nodes in the source parse tree. [sent-50, score-1.072]
</p><p>23 If we treat the reordered position π(i) of child ci as ifts w “rank”, hthee r roerodredreerding problem is naturally translated into a ranking problem: to reorder, we determine a “rank” for each child, then the children are sorted according to their “ranks”. [sent-58, score-0.732]
</p><p>24 As it is often impractical to directly assign a score for each permutation due to huge number of possible permutations, a widely used method is to use a real valued function f to assign a value to each node, which is called a ranking function (Herbrich et al. [sent-59, score-0.421]
</p><p>25 For example, con−  sider the node rooted at trying in the dependency tree in Figure 1. [sent-62, score-0.408]
</p><p>26 Assuming ranking ,f aumn,ct tiroynf can assign bvea lruaensk {0. [sent-65, score-0.277]
</p><p>27 More formally, for a tree node t with children {c1, c2, . [sent-70, score-0.413]
</p><p>28 , cn}, our ranking model assigns a rank f(ci, t) for each} ,c ohiulrd ci, tkhineng t mheo cdehil d arssenig are aso rratendk according to the rank in a descending order. [sent-73, score-0.407]
</p><p>29 The ranking function f has the following form: f(ci,t) = Xθj(ci,t) Xj  · wj  (1)  where the θj is a feature representing the tree node t and its child ci, and wj is the corresponding feature weight. [sent-74, score-0.634]
</p><p>30 3  Ranking Model Training  To learn ranking function in Equation (1), we need to determine the feature set θ and learn weight vector w from reorder examples. [sent-75, score-0.669]
</p><p>31 In this section, we first describe how to extract reordering examples from  parallel corpus; then we show our features for ranking function; finally, we discuss how to train the model from the extracted examples. [sent-76, score-0.81]
</p><p>32 1 Reorder Example Acquisition For a sentence pair (e, f,a) with syntax tree Te on the source side, we need to determine which reordered tree Te00 best represents the word order in target sentence f. [sent-78, score-0.899]
</p><p>33 With the gold alignment, “Problem” is aligned to the 5th target word, and “with latter procedure” are aligned to target span [1, 3], thus we can simply put “Problem” after “with latter procedure”. [sent-84, score-0.358]
</p><p>34 , 2007), in practice, nodes often have overlapping target spans due to erroneous word alignment or different syntactic structures between source and target sentences. [sent-87, score-0.613]
</p><p>35 The word  “with” is incorrectly aligned to the 6th Japanese word “ha”; as a result, “with latter procedure” now has target span [1, 6], while “Problem” aligns to [5, 5]. [sent-89, score-0.268]
</p><p>36 Due to this overlapping, it becomes unclear which permutation of “Problem” and “with latter procedure” is a better match of the target phrase; we need a better metric to measure word order similarity between reordered source and target sentences. [sent-90, score-0.634]
</p><p>37 We choose to find the tree Te00 with minimal alignment crossing-link number (CLN) (Genzel, 2010) to f as our golden reordered tree. [sent-91, score-0.534]
</p><p>38 CLN reaches zero if f is monotonically aligned to e0, and increases as there are more word reordering between e0 and f. [sent-94, score-0.561]
</p><p>39 CLN for the reordered tree is 0 as there are no crossing-links. [sent-96, score-0.372]
</p><p>40 We need to find the reordered tree with minimal CLN among all reorder candidates. [sent-98, score-0.77]
</p><p>41 •  •  CLN(t) : the number of crossing-links (i1j1, i2j2) whose source words ei01 and ei02 both fall under sub span of the tree node t. [sent-102, score-0.445]
</p><p>42 CCLN(t) : the number of crossing-links (i1j1, i2j2) whose source words ei01 and ei02 fall under sub span of t’s two different children nodes c1 and c2 respectively. [sent-103, score-0.428]
</p><p>43 Apparently CLN of a tree T0 equals to CLN(root of T0), and CLN(t) can be recursively expressed as: CLN(t) = CCLN(t) +  X  CLN(c)  childX Xc of t  Take the original tree in Figure 1for example. [sent-104, score-0.348]
</p><p>44 From the definition, we can easily see that CCLN(t) can be determined solely by the order of t’s direct children, and CLN(t) is only affected by discarded too many training instances and led to degraded reordering performance. [sent-107, score-0.539]
</p><p>45 2In our experiments, there are nodes with more than 10 children for English dependency trees. [sent-108, score-0.32]
</p><p>46 This observation enables us to divide the task of finding the reordered tree Te00 with minimal CLN into independently finding the children permutation of each node with minimal CCLN. [sent-110, score-0.767]
</p><p>47 As pointed out by (Tromble, 2009), the ITG neighbor-  hood is large enough for reordering task, and can be searched through efficiently using a CKY decoder. [sent-114, score-0.482]
</p><p>48 After finding the best reordered tree Te00, we can extract one reorder example from every node with more than one child. [sent-115, score-0.832]
</p><p>49 2 Features Features for the ranking model are extracted from source syntax trees. [sent-117, score-0.423]
</p><p>50 , 2006), including lexicons, Part-of-Speech tags, dependency labels, punctuations and tree distance between head and dependent. [sent-119, score-0.265]
</p><p>51 For Japanese-to-English task, we use a chunkbased Japanese dependency tree (Kudo and Matsumoto, 2002). [sent-120, score-0.237]
</p><p>52 3 Learning Method There are many well studied methods available to  learn the ranking function from extracted examples. [sent-125, score-0.309]
</p><p>53 , 2000), a pair-wised ranking method, for its simplicity and good performance. [sent-129, score-0.277]
</p><p>54 For every reorder example t with children {c1, c2, . [sent-130, score-0.499]
</p><p>55 Fosore any ttow oa children nodes ci and cj with i < j , we extract a positive instance if π(i) < π(j), otherwise we extract a negative instance. [sent-137, score-0.385]
</p><p>56 The feature vector for both positive instance and negative instance is (θci −θcj ), where θci and θcj are feature vectors for ci −anθd cj  cJ t- f E· rchlfe tx·dstc t f · lhdc slftex·ds tc tl f ·r hc l tfe ·x dst  Table 1: Feature templates for ranking function. [sent-138, score-0.48]
</p><p>57 c: child to be ranked; h: head node lc: left sibling of c; rc: right sibling of c l: dependency label; t: pos tag lex: top frequency lexicons f: Japanese function word dst: tree distance between c and h pct: punctuation node between c and h respectively. [sent-140, score-0.672]
</p><p>58 In this way, ranking function learning is turned into a simple binary classification problem, which can be easily solved by a two-class linear support vector machine. [sent-141, score-0.309]
</p><p>59 4  Integration into SMT system  There are two ways to integrate the ranking reordering model into a phrase-based SMT system: the prereorder method, and the decoding time constraint method. [sent-142, score-0.787]
</p><p>60 For pre-reorder method, ranking reorder model is applied to reorder source sentences during both training and decoding. [sent-143, score-1.097]
</p><p>61 The ranking reorder model can also be integrated into a phrase based decoder. [sent-145, score-0.714]
</p><p>62 Integrated method takes the original source sentence e as input, and ranking model generates a reordered e0 as a word order ref916 erence for the decoder. [sent-146, score-0.67]
</p><p>63 A simple penalty scheme is utilized to penalize decoder reordering violating ranking reorder model’s prediction e0. [sent-147, score-1.24]
</p><p>64 •  •  Wrong straight are not straight  straight rule penalty fst: it fires for srutrlaei gwhthe rnu source spans of A1 and A2 mapped to two adjacent spans in e0 in order. [sent-152, score-0.558]
</p><p>65 Wrong inverse rule penalty fiv: it fires for inverse gr u ilenv werhseen r source spans of A1 and A2 are not mapped to two adjacent spans in e0 in inverse order. [sent-153, score-0.516]
</p><p>66 Essentially they are soft constraints to encourage the decoder to choose translations with word order similar to the prediction of ranking reorder model. [sent-155, score-0.743]
</p><p>67 5 Experiments To test our ranking reorder model, we carry out experiments on large scale English-To-Japanese, and Japanese-To-English translation tasks. [sent-156, score-0.701]
</p><p>68 1 Baseline System We use a BTG phrase-based system with a MaxEnt based lexicalized reordering model (Wu, 1997; Xiong et al. [sent-183, score-0.482]
</p><p>69 2 Ranking Reordering System Ranking reordering model is learned from the same parallel corpus as phrase table. [sent-191, score-0.56]
</p><p>70 For efficiency reason, we only use 25% of the corpus to train our reordering model. [sent-192, score-0.482]
</p><p>71 Baseline is the BTG phrase system system;  ManR-PR is pre-reorder with manual rule; Rank-PR is pre-reorder with ranking reorder model; Rank-IT is system with integrated ranking reorder model. [sent-199, score-1.394]
</p><p>72 From Table 2, we can see our ranking reordering model significantly improves the performance for both English-to-Japanese and Japanese-to-English experiments over the BTG baseline system. [sent-200, score-0.759]
</p><p>73 5 Reordering Performance In order to show whether the improved performance is really due to improved reordering, we would like to measure the reorder performance directly. [sent-203, score-0.389]
</p><p>74 As we do not have access to a golden reordered sentence set, we decide to use the alignment crossing-link numbers between aligned sentence pairs as the measure for reorder performance. [sent-204, score-0.791]
</p><p>75 We train the ranking model on 25% of our parallel corpus, and use the rest 75% as test data (auto). [sent-205, score-0.328]
</p><p>76 None means the original sentences without reordering; Oracle means the best permutation allowed by the source parse tree; ManR refers to manual reorder rules; Rank means ranking reordering model. [sent-212, score-1.394]
</p><p>77 our ranking reordering model indeed significantly reduces the crossing-link numbers over the original sentence pairs. [sent-213, score-0.791]
</p><p>78 On the other hand, the performance of the ranking reorder model still fall far short of oracle, which is the lowest crossing-link number of all possible permutations allowed by the parse tree. [sent-214, score-0.759]
</p><p>79 By manual analysis, we find that the gap is due to both errors of the ranking reorder model and errors from  word alignment and parser. [sent-215, score-0.811]
</p><p>80 6 Effect of Ranking Features Here we examine the effect of features for ranking reorder model. [sent-219, score-0.637]
</p><p>81 8946 3127 ,362195 3k76 k  Table 4: Effect of ranking features. [sent-229, score-0.277]
</p><p>82 Our explanation is that less frequent lexicons tend to help local reordering only,  which is already handled by the underlying phrasebased system. [sent-235, score-0.551]
</p><p>83 6  Discussion on Related Work  There have been several studies focusing on compiling hand-crafted syntactic reorder rules. [sent-241, score-0.4]
</p><p>84 (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. [sent-248, score-0.62]
</p><p>85 Automatically learning syntactic reordering rules have also been explored in several work. [sent-250, score-0.571]
</p><p>86 (2010) learned probability of reordering patterns from constituent trees using either Maximum Entropy or maximum likelihood estimation. [sent-253, score-0.559]
</p><p>87 Since reordering patterns are matched against a tree node together with all its direct children, data sparseness problem will arise when tree nodes have many children (Li et al. [sent-254, score-1.215]
</p><p>88 Genzel (2010) dealt with the data sparseness problem by using window heuristic, and learned reordering pattern sequence from dependency trees. [sent-257, score-0.545]
</p><p>89 Different from the previous approaches, we treat syntax-based reordering  as a ranking problem between different source tree nodes. [sent-259, score-1.033]
</p><p>90 , 2006) model syntactic reordering using minimal or composed translation rules, which may contain reordering involving tree nodes from multiple tree 919 levels. [sent-263, score-1.572]
</p><p>91 For a tree-tostring rule with multiple tree levels, instead of ranking the direct children of the root node, we rank all leaf nodes (Most are frontier nodes (Galley et al. [sent-265, score-0.957]
</p><p>92 We need to redesign our ranking feature templates to encode the reordering information in the source part of the translation rules. [sent-267, score-0.957]
</p><p>93 7  Conclusion and Future Work  In this paper we present a ranking based reordering method to reorder source language to match the word order of target language given the source side parse tree. [sent-269, score-1.548]
</p><p>94 Reordering is formulated as a task to rank different nodes in the source side syntax tree according to their relative position in the target language. [sent-270, score-0.617]
</p><p>95 The ranking model is automatically trained to minimize the mis-ordering of tree nodes in the training data. [sent-271, score-0.599]
</p><p>96 Large scale experiment shows improvement on both reordering metric and SMT performance, with up to 1. [sent-272, score-0.482]
</p><p>97 In future work, we plan to extend the ranking model to handle reordering between multiple levels of source trees. [sent-274, score-0.859]
</p><p>98 We also expect to explore better way to integrate ranking reorder model into SMT system instead of a simple penalty scheme. [sent-275, score-0.715]
</p><p>99 Along the research direction of preprocessing the source  language to facilitate translation, we consider to not only change the order of the source language, but also inject syntactic structure of the target language into source language by adding pseudo words into source sentences. [sent-276, score-0.549]
</p><p>100 Constituent reordering and syntax models for Englishto-Japanese statistical machine translation. [sent-341, score-0.528]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reordering', 0.482), ('reorder', 0.36), ('cln', 0.349), ('ranking', 0.277), ('reordered', 0.198), ('tree', 0.174), ('children', 0.139), ('nodes', 0.118), ('japanese', 0.114), ('ccln', 0.103), ('source', 0.1), ('node', 0.1), ('alignment', 0.097), ('genzel', 0.082), ('rankingsvm', 0.082), ('visweswariah', 0.082), ('btg', 0.082), ('target', 0.08), ('smt', 0.08), ('permutation', 0.08), ('penalty', 0.078), ('trying', 0.071), ('ci', 0.067), ('rank', 0.065), ('spans', 0.064), ('translation', 0.064), ('distortion', 0.063), ('dependency', 0.063), ('herbrich', 0.062), ('cj', 0.061), ('straight', 0.055), ('parse', 0.052), ('child', 0.051), ('parallel', 0.051), ('integrated', 0.05), ('fires', 0.049), ('xiong', 0.049), ('rules', 0.049), ('syntax', 0.046), ('trees', 0.045), ('aligned', 0.045), ('decoder', 0.043), ('manual', 0.043), ('span', 0.042), ('kudo', 0.041), ('permutations', 0.041), ('cabocha', 0.041), ('dst', 0.041), ('nenghai', 0.041), ('ramanathan', 0.041), ('scholz', 0.041), ('inverse', 0.041), ('parser', 0.04), ('syntactic', 0.04), ('bleu', 0.039), ('minimal', 0.038), ('rule', 0.038), ('te', 0.038), ('precedence', 0.036), ('rearrange', 0.036), ('sov', 0.036), ('galley', 0.036), ('lexicons', 0.036), ('marneffe', 0.034), ('transduction', 0.034), ('word', 0.034), ('side', 0.034), ('templates', 0.034), ('latter', 0.033), ('phrasebased', 0.033), ('auto', 0.033), ('permuting', 0.033), ('unaligned', 0.033), ('constituent', 0.032), ('function', 0.032), ('quirk', 0.032), ('li', 0.032), ('play', 0.032), ('sentence', 0.032), ('english', 0.031), ('informal', 0.031), ('estimations', 0.03), ('tromble', 0.03), ('minimize', 0.03), ('order', 0.029), ('koehn', 0.029), ('xu', 0.029), ('dongdong', 0.029), ('fall', 0.029), ('decoding', 0.028), ('head', 0.028), ('direct', 0.028), ('franz', 0.027), ('golden', 0.027), ('cky', 0.027), ('sibling', 0.027), ('subtree', 0.027), ('phrase', 0.027), ('procedure', 0.027), ('philipp', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="19-tfidf-1" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>2 0.39305165 <a title="19-tfidf-2" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>3 0.24717472 <a title="19-tfidf-3" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English transla- , tion tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</p><p>4 0.24085668 <a title="19-tfidf-4" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>5 0.22061759 <a title="19-tfidf-5" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Reordering is a difficult task in translating between widely different languages such as Japanese and English. We employ the postordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language.</p><p>6 0.1376911 <a title="19-tfidf-6" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>7 0.13242909 <a title="19-tfidf-7" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>8 0.13175334 <a title="19-tfidf-8" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>9 0.12924854 <a title="19-tfidf-9" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>10 0.12508179 <a title="19-tfidf-10" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>11 0.1209085 <a title="19-tfidf-11" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>12 0.11961356 <a title="19-tfidf-12" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>13 0.11941727 <a title="19-tfidf-13" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>14 0.11149015 <a title="19-tfidf-14" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>15 0.10919608 <a title="19-tfidf-15" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>16 0.10534809 <a title="19-tfidf-16" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>17 0.099576727 <a title="19-tfidf-17" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>18 0.096266441 <a title="19-tfidf-18" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>19 0.09541893 <a title="19-tfidf-19" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>20 0.092912927 <a title="19-tfidf-20" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.298), (1, -0.213), (2, -0.01), (3, -0.039), (4, 0.029), (5, -0.193), (6, -0.022), (7, 0.104), (8, 0.045), (9, 0.008), (10, 0.028), (11, -0.02), (12, 0.056), (13, -0.069), (14, -0.113), (15, -0.027), (16, -0.018), (17, -0.058), (18, 0.037), (19, -0.286), (20, 0.073), (21, 0.091), (22, -0.129), (23, 0.032), (24, -0.312), (25, 0.16), (26, -0.115), (27, 0.105), (28, 0.108), (29, 0.14), (30, -0.073), (31, 0.106), (32, -0.006), (33, -0.063), (34, 0.017), (35, -0.044), (36, 0.048), (37, -0.042), (38, 0.038), (39, 0.123), (40, -0.089), (41, 0.064), (42, 0.005), (43, -0.001), (44, -0.03), (45, -0.006), (46, 0.031), (47, 0.051), (48, -0.006), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93537933 <a title="19-lsi-1" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>2 0.90330178 <a title="19-lsi-2" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>3 0.79060525 <a title="19-lsi-3" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Reordering is a difficult task in translating between widely different languages such as Japanese and English. We employ the postordering framework proposed by (Sudoh et al., 2011b) for Japanese to English translation and improve upon the reordering method. The existing post-ordering method reorders a sequence of target language words in a source language word order via SMT, while our method reorders the sequence by: 1) parsing the sequence to obtain syntax structures similar to a source language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language.</p><p>4 0.60609293 <a title="19-lsi-4" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>Author: Junhui Li ; Zhaopeng Tu ; Guodong Zhou ; Josef van Genabith</p><p>Abstract: This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1</p><p>5 0.56428295 <a title="19-lsi-5" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>6 0.54124153 <a title="19-lsi-6" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>7 0.51501513 <a title="19-lsi-7" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>8 0.41544795 <a title="19-lsi-8" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>9 0.41006845 <a title="19-lsi-9" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>10 0.40005928 <a title="19-lsi-10" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>11 0.38387206 <a title="19-lsi-11" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>12 0.37725386 <a title="19-lsi-12" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>13 0.37625185 <a title="19-lsi-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.34597573 <a title="19-lsi-14" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>15 0.33900401 <a title="19-lsi-15" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>16 0.33649752 <a title="19-lsi-16" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>17 0.33160987 <a title="19-lsi-17" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>18 0.32866514 <a title="19-lsi-18" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>19 0.32506019 <a title="19-lsi-19" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>20 0.32326391 <a title="19-lsi-20" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(26, 0.032), (28, 0.053), (30, 0.34), (37, 0.042), (39, 0.034), (57, 0.029), (74, 0.041), (82, 0.021), (84, 0.011), (85, 0.045), (90, 0.122), (92, 0.059), (94, 0.039), (99, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95474017 <a title="19-lda-1" href="./acl-2012-Crowdsourcing_Inference-Rule_Evaluation.html">65 acl-2012-Crowdsourcing Inference-Rule Evaluation</a></p>
<p>Author: Naomi Zeichner ; Jonathan Berant ; Ido Dagan</p><p>Abstract: The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.</p><p>2 0.88490903 <a title="19-lda-2" href="./acl-2012-Modeling_Review_Comments.html">144 acl-2012-Modeling Review Comments</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Writing comments about news articles, blogs, or reviews have become a popular activity in social media. In this paper, we analyze reader comments about reviews. Analyzing review comments is important because reviews only tell the experiences and evaluations of reviewers about the reviewed products or services. Comments, on the other hand, are readers’ evaluations of reviews, their questions and concerns. Clearly, the information in comments is valuable for both future readers and brands. This paper proposes two latent variable models to simultaneously model and extract these key pieces of information. The results also enable classification of comments accurately. Experiments using Amazon review comments demonstrate the effectiveness of the proposed models.</p><p>3 0.84703267 <a title="19-lda-3" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>Author: Matthieu Constant ; Anthony Sigogne ; Patrick Watrin</p><p>Abstract: and Parsing Anthony Sigogne Universit e´ Paris-Est LIGM, CNRS France s igogne @univ-mlv . fr Patrick Watrin Universit e´ de Louvain CENTAL Belgium pat rick .wat rin @ ucl ouvain .be view, their incorporation has also been considered The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.</p><p>same-paper 4 0.79929829 <a title="19-lda-4" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>5 0.67167538 <a title="19-lda-5" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>6 0.61824793 <a title="19-lda-6" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.59148961 <a title="19-lda-7" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>8 0.58044034 <a title="19-lda-8" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>9 0.57235825 <a title="19-lda-9" href="./acl-2012-Syntactic_Stylometry_for_Deception_Detection.html">190 acl-2012-Syntactic Stylometry for Deception Detection</a></p>
<p>10 0.57117856 <a title="19-lda-10" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<p>11 0.57020795 <a title="19-lda-11" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>12 0.56618321 <a title="19-lda-12" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>13 0.55932987 <a title="19-lda-13" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>14 0.54566312 <a title="19-lda-14" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>15 0.54418337 <a title="19-lda-15" href="./acl-2012-Tokenization%3A_Returning_to_a_Long_Solved_Problem__A_Survey%2C_Contrastive_Experiment%2C_Recommendations%2C_and_Toolkit_.html">197 acl-2012-Tokenization: Returning to a Long Solved Problem  A Survey, Contrastive Experiment, Recommendations, and Toolkit </a></p>
<p>16 0.53980619 <a title="19-lda-16" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>17 0.53923464 <a title="19-lda-17" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>18 0.53828174 <a title="19-lda-18" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>19 0.53793764 <a title="19-lda-19" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>20 0.53738189 <a title="19-lda-20" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
