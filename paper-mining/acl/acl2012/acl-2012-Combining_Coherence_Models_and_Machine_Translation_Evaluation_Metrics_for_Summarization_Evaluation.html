<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-52" href="#">acl2012-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</h1>
<br/><p>Source: <a title="acl-2012-52-pdf" href="http://aclweb.org/anthology//P/P12/P12-1106.pdf">pdf</a></p><p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>Reference: <a title="acl-2012-52-reference" href="../acl2012_reference/acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  ,  ,  Abstract An ideal summarization system should produce summaries that have high content coverage and linguistic quality. [sent-6, score-0.526]
</p><p>2 Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. [sent-7, score-0.313]
</p><p>3 The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. [sent-9, score-0.32]
</p><p>4 In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. [sent-10, score-0.911]
</p><p>5 1 Introduction Research and development on automatic and manual evaluation of summarization systems have been mainly focused on content coverage (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hovy et al. [sent-12, score-0.313]
</p><p>6 However, users may still find it difficult to read such high-content coverage summaries as they lack fluency. [sent-15, score-0.26]
</p><p>7 To promote research on automatic evaluation of summary readability, the Text Analysis Conference (TAC) (Owczarzak and Dang, 2011) introduced a new subtask on readability to its Automatically Evaluating Summaries of Peers (AESOP) task. [sent-16, score-0.464]
</p><p>8 1006 Most of the state-of-the-art summarization systems (Ng et al. [sent-17, score-0.153]
</p><p>9 Knott (1996) argued that when the sentences of a text are randomly or-  dered, the text becomes difficult to understand, as its discourse structure is disturbed. [sent-23, score-0.216]
</p><p>10 (201 1) validated this argument by using a trained model to differentiate an original text from a randomlyordered permutation of its sentences by looking at their discourse structures. [sent-25, score-0.216]
</p><p>11 This prior work leads us to believe that we can apply such discourse models to evaluate the readability of extract-based summaries. [sent-26, score-0.475]
</p><p>12 ’s discourse coherence model to evaluate readability of machine generated summaries. [sent-28, score-0.633]
</p><p>13 There are parallels between evaluations of machine translation (MT) and summarization with respect to textual content. [sent-30, score-0.182]
</p><p>14 For instance, the widely used ROUGE (Lin and Hovy, 2003) metrics are influenced by BLEU (Papineni et al. [sent-31, score-0.134]
</p><p>15 , 2010), to evaluate the content  coverage of summaries. [sent-34, score-0.16]
</p><p>16 TAC’s overall responsiveness metric evaluates the ProceediJnegjus, o Rfe thpeu 5bl0icth o Afn Knouraela M, 8e-e1t4in Jgul oyf t 2h0e1 A2. [sent-35, score-0.287]
</p><p>17 oc c2ia0t1io2n A fsosro Cciaotmiopnu ftaotrio Cnoamlp Luintagtuioisntaicls L,i pnaggueis t 1i0c0s6–1014, quality of a summary with regard to both its content and readability. [sent-37, score-0.318]
</p><p>18 Given this, we combine our two component coherence and content models into an SVM-trained regression model as our surrogate to overall responsiveness. [sent-38, score-0.432]
</p><p>19 Our experiments show that the coherence model significantly outperforms all AESOP 2011 submissions on both initial and update tasks, while the adapted MT evaluation metric and the combined model significantly outperform all submissions on the initial task. [sent-39, score-0.595]
</p><p>20 To the best of our knowledge, this is the first work that applies a discourse coherence model to measure the readability of summaries in the AESOP task. [sent-40, score-0.846]
</p><p>21 2  Related Work  Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. [sent-41, score-0.174]
</p><p>22 Lin and Hovy (2003) introduced an automatic summarization evaluation metric, called ROUGE, which was motivated by the MT evaluation metric, BLEU (Papineni et al. [sent-43, score-0.184]
</p><p>23 It automatically determines the content quality of a summary by comparing it to the model summaries and counting the overlapping n-gram units. [sent-45, score-0.5]
</p><p>24 Both ROUGE and BE have been implemented and included in the ROUGE/BE evaluation toolkit1, which has been used as the default evaluation tool –  –  in the summarization track in the Document Un1http : / /berouge . [sent-51, score-0.184]
</p><p>25 DUC and TAC also manually evaluated machine generated summaries by adopting the Pyramid method. [sent-54, score-0.213]
</p><p>26 Besides evaluating with ROUGE/BE and Pyramid, DUC and TAC also asked human judges to score every candidate summary with regard to its content, readability, and overall responsiveness. [sent-55, score-0.419]
</p><p>27 Conroy and Dang (2008) combined two manual linguistic scores grammaticality and focus with various ROUGE/BE metrics, and showed this helps better –  –  predict the responsiveness of the summarizers. [sent-60, score-0.289]
</p><p>28 AESOP 2009 and 2010 focused on two summary qualities: content and overall responsiveness. [sent-62, score-0.332]
</p><p>29 Summary content is measured by comparing the output of an automatic metric with the manual Pyramid score. [sent-63, score-0.202]
</p><p>30 Overall responsiveness measures a combination of content and linguistic quality. [sent-64, score-0.266]
</p><p>31 In AESOP 2011 (Owczarzak and Dang, 2011), automatic metrics are also evaluated for their ability to assess summary readability, i. [sent-65, score-0.308]
</p><p>32 , to measure how linguistically readable a machine generated summary is. [sent-67, score-0.214]
</p><p>33 de Oliveira (201 1)  modeled the similarity between the model and candidate summaries as a maximum bipartite matching problem, where the two summaries are represented as two sets of nodes and precision and recall are cal-  sw=01. [sent-73, score-0.501]
</p><p>34 However, none of the AESOP metrics currently apply deep linguistic analysis, which includes discourse analysis. [sent-92, score-0.35]
</p><p>35 Motivated by the parallels between summarization and MT evaluation, we will adapt a state-ofthe-art MT evaluation metric to measure summary content quality. [sent-93, score-0.558]
</p><p>36 To apply deep linguistic analysis, we also enhance an existing discourse coherence model to evaluate summary readability. [sent-94, score-0.548]
</p><p>37 , metrics that can rank a set of machine summarizers correctly (human summarizers are not included in the list). [sent-97, score-0.226]
</p><p>38 A similarity score s(xi, yj) between all ngrams xi and yj . [sent-110, score-0.148]
</p><p>39 When multiple model summaries are provided, TESLA matches the candidate BNG with each of the model BNGs. [sent-118, score-0.25]
</p><p>40 The similarity score s(xi, yj) is 1if the word surface forms of xi and yj are identical, and 0 otherwise. [sent-124, score-0.148]
</p><p>41 However, the majority of current state-of-the-art summarization systems are extraction-based systems, which do not generate new words. [sent-126, score-0.153]
</p><p>42 This reflects a major difference between MT and summarization evaluation: while MT systems  always generate new sentences, most summarization systems focus on locating existing salient sentences. [sent-128, score-0.306]
</p><p>43 Evaluate the list of summaries from Step 2 with the two evaluation metrics under comparison. [sent-139, score-0.347]
</p><p>44 As we have 44 topics in TAC 2011 summarization –  track, n = 44. [sent-144, score-0.153]
</p><p>45 The percentage of times metric a gives higher correlation than metric b is said to be the significance level at which a outperforms b. [sent-145, score-0.369]
</p><p>46 The results for the initial and update tasks are reported in Table 1. [sent-152, score-0.155]
</p><p>47 We show the three baselines (ROUGE-2, ROUGE-SU4, and BE) and submitted metrics with correlations among the top three scores, which are underlined. [sent-153, score-0.235]
</p><p>48 On the initial task, TESLA-S outperforms all metrics on all three correlation measures. [sent-157, score-0.322]
</p><p>49 Both readability and coherence indicate how fluent a text is. [sent-165, score-0.417]
</p><p>50 (201 1) introduced discourse role matrix to represent discourse coherence of a text. [sent-168, score-0.621]
</p><p>51 We then apply the models and evaluate summary readability. [sent-170, score-0.174]
</p><p>52 ’s Discourse Coherence Model First, a free text in Figure 2 is parsed by a discourse parser to derive its discourse relations, which are shown in Figure 3. [sent-173, score-0.432]
</p><p>53 However, simply using such patterns to measure the coherence of a text can result in feature sparseness. [sent-176, score-0.158]
</p><p>54 To solve this problem, they expand the relation sequence into a discourse role matrix, as shown in Table 2. [sent-177, score-0.26]
</p><p>55 Next, the discourse role transition probabilities of lengths 2 and 3 (e. [sent-180, score-0.216]
</p><p>56 permuted), and trained a SVM preference ranking model with discourse role S1 Japan normally depends heavily on the Highland Valley and Cananea mines as well as the Bougainville mine in Papua New Guinea. [sent-198, score-0.248]
</p><p>57 Rows correspond to sentences, columns to stemmed terms, and cells contain extracted discourse roles. [sent-225, score-0.216]
</p><p>58 Explicit relation and Non-Explicit relation have different distributions on each discourse relation (PDTB-Group, 2007). [sent-233, score-0.348]
</p><p>59 In addition to the set of the discourse roles of “Relation type . [sent-235, score-0.254]
</p><p>60 The other information that is not in the discourse role matrix is the discourse hierarchy structure,  i. [sent-249, score-0.432]
</p><p>61 These dependencies are important for us to know how well-structured a summary is. [sent-254, score-0.174]
</p><p>62 It is represented by the multiple discourse roles in each cell of the matrix. [sent-255, score-0.295]
</p><p>63 For example, the multiple discourse roles in the cell Ccananea,S3 capture the three dependencies just mentioned. [sent-256, score-0.295]
</p><p>64 We introduce intra-cell bigrams as a new set of features to the original model: for a cell with multiple discourse roles, we sort them by their surface strings and multiply to obtain the bigrams. [sent-257, score-0.286]
</p><p>65 1011 In the TAC summarization track, human judges scored each model and candidate summary with a readability score from 1 to 5 (5 means most readable). [sent-273, score-0.693]
</p><p>66 Thus in our setting, instead of a pair of texts, the training input consists of a list of model and candidate summaries from each topic, with their annotated scores as the rankings. [sent-274, score-0.318]
</p><p>67 This score essentially is the readability ranking of the test summary. [sent-276, score-0.329]
</p><p>68 As Pearson’s r measures linear correlation and we do not know whether the real number score follows a linear function, we take the logarithm of this score as the readability score for this instance. [sent-278, score-0.52]
</p><p>69 To obtain the discourse relations of a summary, we use the discourse parser2 developed in Lin et al. [sent-280, score-0.432]
</p><p>70 The last four rows show the correlation scores for our coherence model: LIN is the default model by (Lin et al. [sent-284, score-0.352]
</p><p>71 LIN outperforms all metrics on all correlations on both tasks. [sent-289, score-0.215]
</p><p>72 On the initial task, it outperforms the best scores by 3. [sent-290, score-0.169]
</p><p>73 Table 3 shows that summarization evaluation Metric 4 tops all other AESOP metrics, except in the case of Spearman’s ρ on the initial task. [sent-318, score-0.21]
</p><p>74 5  CREMER: Evaluating Overall  Responsiveness With TESLA-S measuring content coverage and DICOMER measuring readability, it is feasible to combine them to predict the overall responsiveness of a summary. [sent-322, score-0.39]
</p><p>75 There exist many ways to combine two variables mathematically: we can combine them in a linear function or polynomial function, or in a way 1012  judgment on summarizer level. [sent-323, score-0.166]
</p><p>76 We use SVMlight with the regression configuration, testing three kernels: linear function, polynomial function, and radial basis function. [sent-329, score-0.201]
</p><p>77 The DICOMER model that is trained in Section 4 is used to predict the readability scores on all AESOP 2009, 2010, and 2011 summaries. [sent-332, score-0.327]
</p><p>78 We apply TESLA-S to predict content scores on all AESOP 2009, 2010, and 2011 summaries. [sent-333, score-0.181]
</p><p>79 1 Experiments The last three rows in Table 5 show the correlation scores of our regression model trained with SVM linear function (LF), polynomial function (PF), and radial basis function (RBF). [sent-335, score-0.395]
</p><p>80 PF performs better than  LF, suggesting that content and readability scores should not be linearly combined. [sent-336, score-0.44]
</p><p>81 RBF gives better performances than both LF and PF, suggesting that RBF better models the way humans combine content and readability. [sent-337, score-0.145]
</p><p>82 On the initial task, the model trained with RBF outperforms all submitted metrics. [sent-338, score-0.165]
</p><p>83 All three regression models do not perform as well on the update task. [sent-343, score-0.182]
</p><p>84 Koehn’s significance test shows that when trained with RBF, CREMER outperforms ROUGE-2 and ROUGE-SU4 on the initial task at a significance level of 99% for all three correlation measures. [sent-344, score-0.308]
</p><p>85 6  Discussion  The intuition behind the combined regression model is that combining the readability and content scores will give an overall good responsiveness score. [sent-345, score-0.752]
</p><p>86 Human judges were told to rate  summaries by their overall qualities. [sent-348, score-0.29]
</p><p>87 Given CREMER did not perform well on the update task, we hypothesize that human judgment of update summaries may involve more complicated rankings or factor in additional input that CREMER currently does not model. [sent-350, score-0.409]
</p><p>88 We plan to devise a better responsiveness metric in our future work, beyond using a simple combination. [sent-351, score-0.242]
</p><p>89 Figure 4 shows a complete picture of Pearson’s r for all AESOP 2011 metrics and our three metrics on both initial and update tasks. [sent-352, score-0.423]
</p><p>90 On the initial task, correlation scores for content are consistently higher than those for responsiveness with small gaps, whereas on the update task, they are almost overlapping. [sent-354, score-0.576]
</p><p>91 On the other hand, correlation scores for readability are much lower than those for content and responsiveness, with a gap of about 0. [sent-355, score-0.527]
</p><p>92 Comparing Figure 4a and 4b, evaluation metrics always correlate better on the initial task than on the update task. [sent-357, score-0.289]
</p><p>93 This suggests that there is much room for improvement for readability metrics, and metrics need to consider update information when evaluating update summarizers. [sent-358, score-0.651]
</p><p>94 7  Conclusion  We proposed TESLA-S by adapting an MT evaluation metric to measure summary content coverage, and introduced DICOMER by applying a dis1013  task. [sent-359, score-0.407]
</p><p>95 course coherence model with newly introduced features to evaluate summary readability. [sent-363, score-0.363]
</p><p>96 We combined these two metrics in the CREMER metric an SVM-trained regression model for automatic summarization overall responsiveness evaluation. [sent-364, score-0.688]
</p><p>97 Experimental results on AESOP 2011 show that DICOMER significantly outperforms all submitted metrics on both initial and update tasks with –  –  large gaps, while TESLA-S and CREMER significantly outperform all metrics on the initial task. [sent-365, score-0.588]
</p><p>98 Mind the gap: Dangers of divorcing evaluations of summary content from linguistic quality. [sent-379, score-0.287]
</p><p>99 Evaluating content selection in summarization: The pyramid  method. [sent-440, score-0.18]
</p><p>100 Overview of the TAC 2011 summarization track: Guided task and AESOP task. [sent-448, score-0.153]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aesop', 0.435), ('readability', 0.259), ('discourse', 0.216), ('summaries', 0.213), ('tac', 0.212), ('dicomer', 0.192), ('tesla', 0.183), ('summary', 0.174), ('coherence', 0.158), ('cremer', 0.154), ('responsiveness', 0.153), ('summarization', 0.153), ('lin', 0.145), ('bng', 0.134), ('metrics', 0.134), ('spearman', 0.128), ('content', 0.113), ('pearson', 0.11), ('kendall', 0.108), ('conroy', 0.107), ('update', 0.098), ('metric', 0.089), ('correlation', 0.087), ('duc', 0.084), ('rbf', 0.084), ('regression', 0.084), ('gaithersburg', 0.081), ('rouge', 0.071), ('maryland', 0.068), ('scores', 0.068), ('pyramid', 0.067), ('ziheng', 0.067), ('submitted', 0.064), ('yj', 0.064), ('evaluating', 0.062), ('significance', 0.06), ('cananea', 0.058), ('giannakopoulos', 0.058), ('radial', 0.058), ('initial', 0.057), ('mt', 0.052), ('guided', 0.05), ('bolded', 0.05), ('bes', 0.05), ('highland', 0.05), ('dang', 0.049), ('hovy', 0.048), ('coverage', 0.047), ('summarizers', 0.046), ('valley', 0.046), ('xi', 0.046), ('singapore', 0.046), ('comp', 0.045), ('overall', 0.045), ('outperforms', 0.044), ('relation', 0.044), ('summarizer', 0.043), ('owczarzak', 0.043), ('cell', 0.041), ('nenkova', 0.04), ('readable', 0.04), ('rows', 0.039), ('grammaticality', 0.038), ('bngs', 0.038), ('ihen', 0.038), ('karkaletsis', 0.038), ('oliveira', 0.038), ('peers', 0.038), ('sap', 0.038), ('xiw', 0.038), ('roles', 0.038), ('matching', 0.038), ('score', 0.038), ('explicit', 0.037), ('correlations', 0.037), ('usa', 0.037), ('lf', 0.037), ('candidate', 0.037), ('pf', 0.035), ('gaps', 0.035), ('hwee', 0.034), ('tou', 0.034), ('ranking', 0.032), ('judges', 0.032), ('combine', 0.032), ('regard', 0.031), ('track', 0.031), ('bleu', 0.031), ('submissions', 0.031), ('permuted', 0.031), ('shortened', 0.031), ('introduced', 0.031), ('linear', 0.03), ('combined', 0.03), ('polynomial', 0.029), ('bigrams', 0.029), ('passonneau', 0.029), ('trang', 0.029), ('pitler', 0.029), ('parallels', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="52-tfidf-1" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>2 0.2968629 <a title="52-tfidf-2" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>Author: Karolina Owczarzak ; Peter A. Rankel ; Hoa Trang Dang ; John M. Conroy</p><p>Abstract: We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.</p><p>3 0.22318566 <a title="52-tfidf-3" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>4 0.20106934 <a title="52-tfidf-4" href="./acl-2012-Text-level_Discourse_Parsing_with_Rich_Linguistic_Features.html">193 acl-2012-Text-level Discourse Parsing with Rich Linguistic Features</a></p>
<p>Author: Vanessa Wei Feng ; Graeme Hirst</p><p>Abstract: In this paper, we develop an RST-style textlevel discourse parser, based on the HILDA discourse parser (Hernault et al., 2010b). We significantly improve its tree-building step by incorporating our own rich linguistic features. We also analyze the difficulty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourseparsing performance under different discourse conditions.</p><p>5 0.14056963 <a title="52-tfidf-5" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>Author: Christian Chiarcos</p><p>Abstract: This paper describes a novel approach towards the empirical approximation of discourse relations between different utterances in texts. Following the idea that every pair of events comes with preferences regarding the range and frequency of discourse relations connecting both parts, the paper investigates whether these preferences are manifested in the distribution of relation words (that serve to signal these relations). Experiments on two large-scale English web corpora show that significant correlations between pairs of adjacent events and relation words exist, that they are reproducible on different data sets, and for three relation words, that their distribution corresponds to theorybased assumptions. 1 Motivation Texts are not merely accumulations of isolated utterances, but the arrangement of utterances conveys meaning; human text understanding can thus be described as a process to recover the global structure of texts and the relations linking its different parts (Vallduv ı´ 1992; Gernsbacher et al. 2004). To capture these aspects of meaning in NLP, it is necessary to develop operationalizable theories, and, within a supervised approach, large amounts of annotated training data. To facilitate manual annotation, weakly supervised or unsupervised techniques can be applied as preprocessing step for semimanual annotation, and this is part of the motivation of the approach described here. 213 Discourse relations involve different aspects of meaning. This may include factual knowledge about the connected discourse segments (a ‘subjectmatter’ relation, e.g., if one utterance represents the cause for another, Mann and Thompson 1988, p.257), argumentative purposes (a ‘presentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., adverbials (e.g., however), conjunctions (e.g., but), or complex phrases (e.g., in contrast to what Peter said a minute ago). Here, these cues are referred to as relation words. Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a background knowledge base (BKB) that hosts pairs of events (here heuristically represented by verbs) along with distributional profiles for relation words. The primary data structure of the BKB is a triple where one event (type) is connected with a particular relation word to another event (type). Triples are further augmented with a frequency score (expressing the likelihood of the triple to be observed), a significance score (see below), and a correlation score (indicating whether a pair of events has a positive or negative correlation with a particular relation word). ProceedJienjgus, R ofep thueb 5lic0t hof A Knonrueaa,l M 8-e1e4ti Jnugly o f2 t0h1e2 A.s ?c so2c0ia1t2io Ans fsoorc Ciatoiomnp fuotart Cioonmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi2c 1s3–217, Triples can be easily acquired from automatically parsed corpora. While the relation word is usually part of the utterance that represents the source of the relation, determining the appropriate target (antecedent) of the relation may be difficult to achieve. As a heuristic, an adjacency preference is adopted, i.e., the target is identified with the main event of the preceding utterance.1 The BKB can be constructed from a sufficiently large corpus as follows: • • identify event types and relation words for every utterance create a candidate triple consisting of the event type of the utterance, the relation word, and the event type of the preceding utterance. add the candidate triple to the BKB, if it found in the BKB, increase its score by (or initialize it with) 1, – – • perform a pruning on all candidate triples, calcpuerlaftoer significance aonnd a lclo crarneldaitdioante scores Pruning uses statistical significance tests to evaluate whether the relative frequency of a relation word for a pair of events is significantly higher or lower than the relative frequency of the relation word in the entire corpus. Assuming that incorrect candidate triples (i.e., where the factual target of the relation was non-adjacent) are equally distributed, they should be filtered out by the significance tests. The goal of this paper is to evaluate the validity of this approach. 2 Experimental Setup By generalizing over multiple occurrences of the same events (or, more precisely, event types), one can identify preferences of event pairs for one or several relation words. These preferences capture context-invariant characteristics of pairs of events and are thus to considered to reflect a semantic predisposition for a particular discourse relation. Formally, an event is the semantic representation of the meaning conveyed in the utterance. We 1Relations between non-adjacent utterances are constrained by the structure of discourse (Webber 1991), and thus less likely than relations between adjacent utterances. 214 assume that the same event can reoccur in different contexts, we are thus studying relations between types of events. For the experiment described here, events are heuristically identified with the main predicates of a sentence, i.e., non-auxiliar, noncausative, non-modal verbal lexemes that serve as heads of main clauses. The primary data structure of the approach described here is a triple consisting of a source event, a relation word and a target (antecedent) event. These triples are harvested from large syntactically annotated corpora. For intersentential relations, the target is identified with the event of the immediately preceding main clause. These extraction preferences are heuristic approximations, and thus, an additional pruning step is necessary. For this purpose, statistical significance tests are adopted (χ2 for triples of frequent events and relation words, t-test for rare events and/or relation words) that compare the relative frequency of a rela- tion word given a pair of events with the relative frequency of the relation word in the entire corpus. All results with p ≥ .05 are excluded, i.e., only triples are preserved pfo ≥r w .0h5ic ahr teh eex xocblsuedrevde,d i positive or negative correlation between a pair of events and a relation word is not due to chance with at least 95% probability. Assuming an even distribution of incorrect target events, this should rule these out. Additionally, it also serves as a means of evaluation. Using statistical significance tests as pruning criterion entails that all triples eventually confirmed are statistically significant.2 This setup requires immense amounts of data: We are dealing with several thousand events (theoretically, the total number of verbs of a language). The chance probability for two events to occur in adjacent position is thus far below 10−6, and it decreases further if the likelihood of a relation word is taken into consideration. All things being equal, we thus need millions of sentences to create the BKB. Here, two large-scale corpora of English are employed, PukWaC and Wackypedia EN (Baroni et al. 2009). PukWaC is a 2G-token web corpus of British English crawled from the uk domain (Ferraresi et al. 2Subsequent studies may employ less rigid pruning criteria. For the purpose of the current paper, however, the statistical significance of all extracted triples serves as an criterion to evaluate methodological validity. 2008), and parsed with MaltParser (Nivre et al. 2006). It is distributed in 5 parts; Only PukWaC1 to PukWaC-4 were considered here, constituting 82.2% (72.5M sentences) of the entire corpus, PukWaC-5 is left untouched for forthcoming evaluation experiments. Wackypedia EN is a 0.8G-token dump of the English Wikipedia, annotated with the same tools. It is distributed in 4 different files; the last portion was left untouched for forthcoming evaluation experiments. The portion analyzed here comprises 33.2M sentences, 75.9% of the corpus. The extraction of events in these corpora uses simple patterns that combine dependency information and part-of-speech tags to retrieve the main verbs and store their lemmata as event types. The target (antecedent) event was identified with the last main event of the preceding sentence. As relation words, only sentence-initial children of the source event that were annotated as adverbial modifiers, verb modifiers or conjunctions were considered. 3 Evaluation To evaluate the validity of the approach, three fundamental questions need to be addressed: significance (are there significant correlations between pairs of events and relation words ?), reproducibility (can these correlations confirmed on independent data sets ?), and interpretability (can these correlations be interpreted in terms of theoretically-defined discourse relations ?). 3.1 Significance and Reproducibility Significance tests are part of the pruning stage of the algorithm. Therefore, the number of triples eventually retrieved confirms the existence of statistically significant correlations between pairs of events and relation words. The left column of Tab. 1 shows the number of triples obtained from PukWaC subcorpora of different size. For reproducibility, compare the triples identified with Wackypedia EN and PukWaC subcorpora of different size: Table 1 shows the number of triples found in both Wackypedia EN and PukWaC, and the agreement between both resources. For two triples involving the same events (event types) and the same relation word, agreement means that the relation word shows either positive or negative correlation 215 TasPbe13u7l4n2k98t. We254Mn1a c:CeAs(gurb42)et760cr8m,iop3e61r4l28np0st6uwicho21rm9W,e2673mas048p7c3okenytpdoagi21p8r,o35eE0s29Nit36nvgreipol8796r50s9%.n3509egative correlation of event pairs and relation words between Wackypedia EN and PukWaC subcorpora of different size TBH: thb ouetwnev r17 t1,o27,t0a95P41 ul2kWv6aCs,8.0 Htr5iple1v s, 45.12T35av9sg7.reH7em nv6 ts62(. %.9T2) Table 2: Agreement between but (B), however (H) and then (T) on PukWaC in both corpora, disagreement means positive correlation in one corpus and negative correlation in the other. Table 1 confirms that results obtained on one resource can be reproduced on another. This indicates that triples indeed capture context-invariant, and hence, semantic, characteristics of the relation between events. The data also indicates that reproducibility increases with the size of corpora from which a BKB is built. 3.2 Interpretability Any theory of discourse relations would predict that relation words with similar function should have similar distributions, whereas one would expect different distributions for functionally unrelated relation words. These expectations are tested here for three of the most frequent relation words found in the corpora, i.e., but, then and however. But and however can be grouped together under a generalized notion of contrast (Knott and Dale 1994; Prasad et al. 2008); then, on the other hand, indicates a tem- poral and/or causal relation. Table 2 confirms the expectation that event pairs that are correlated with but tend to show the same correlation with however, but not with then. 4 Discussion and Outlook This paper described a novel approach towards the unsupervised acquisition of discourse relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are possible between specific types of events; on this basis, a background knowledge base (BKB) was created that can be used to predict an appropriatediscoursemarkertoconnecttwoutterances with no overt relation word. This information can be used, for example, to facilitate the semiautomated annotation of discourse relations, by pointing out the ‘default’ relation word for a given pair of events. Similarly, Zhou et al. (2010) used a language model to predict discourse markers for implicitly realized discourse relations. As opposed to this shallow, n-gram-based approach, here, the internal structure of utterances is exploited: based on semantic considerations, syntactic patterns have been devised that extract triples of event pairs and relation words. The resulting BKB provides a distributional approximation of the discourse relations that can hold between two specific event types. Both approaches exploit complementary sources of knowledge, and may be combined with each other to achieve a more precise prediction of implicit discourse connectives. The validity of the approach was evaluated with respect to three evaluation criteria: The extracted associations between relation words and event pairs could be shown to be statistically significant, and to be reproducible on other corpora; for three highly frequent relation words, theoretical predictions about their relative distribution could be confirmed, indicating their interpretability in terms of presupposed taxonomies of discourse relations. Another prospective field of application can be seen in NLP applications, where selection preferences for relation words may serve as a cheap replacement for full-fledged discourse parsing. In the Natural Language Understanding domain, the BKB may help to disambiguate or to identify discourse relations between different events; in the context of Machine Translation, it may represent a factor guid- ing the insertion of relation words, a task that has been found to be problematic for languages that dif216 fer in their inventory and usage of discourse markers, e.g., German and English (Stede and Schmitz 2000). The approach is language-independent (except for the syntactic extraction patterns), and it does not require manually annotated data. It would thus be easy to create background knowledge bases with relation words for other languages or specific domains given a sufficient amount of textual data. – Related research includes, for example, the unsupervised recognition of causal and temporal relationships, as required, for example, for the recognition of textual entailment. Riaz and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire band- width of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with information provided by Riaz and Girju (2010), Chambers and Jurafsky (2009) and Kasch and Oates (2010). Other directions of subsequent research may include address more elaborate models of events, and the investigation of the relationship between relation words and taxonomies of discourse relations. Acknowledgments This work was supported by a fellowship within the Postdoc program of the German Academic Exchange Service (DAAD). Initial experiments were conducted at the Collaborative Research Center (SFB) 632 “Information Structure” at the University of Potsdam, Germany. Iwould also like to thank three anonymous reviewers for valuable comments and feedback, as well as Manfred Stede and Ed Hovy whose work on discourse relations on the one hand and proposition stores on the other hand have been the main inspiration for this paper. References M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226, 2009. N. Chambers and D. Jurafsky. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 602–610. Association for Computational Linguistics, 2009. A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini. Introducing and evaluating ukwac, a very large web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54, 2008. Morton Ann Gernsbacher, Rachel R. W. Robertson, Paola Palladino, and Necia K. Werner. Managing mental representations during narrative comprehension. Discourse Processes, 37(2): 145–164, 2004. N. Kasch and T. Oates. Mining script-like structures from the web. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 34–42. Association for Computational Linguistics, 2010. A. Knott and R. Dale. Using linguistic phenomena to motivate a set ofcoherence relations. Discourse processes, 18(1):35–62, 1994. 217 J. van Kuppevelt and R. Smith, editors. Current Directions in Discourse andDialogue. Kluwer, Dordrecht, 2003. William C. Mann and Sandra A. Thompson. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243–281, 1988. J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-generator for dependency parsing. In Proc. of LREC, pages 2216–2219. Citeseer, 2006. R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. The penn discourse treebank 2.0. In Proc. 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco, 2008. M. Riaz and R. Girju. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on, pages 361–368. IEEE, 2010. M. Stede and B. Schmitz. Discourse particles and discourse functions. Machine translation, 15(1): 125–147, 2000. Enric Vallduv ı´. The Informational Component. Garland, New York, 1992. Bonnie L. Webber. Structure and ostension in the interpretation of discourse deixis. Natural Language and Cognitive Processes, 2(6): 107–135, 1991. Bonnie L. Webber, Matthew Stone, Aravind K. Joshi, and Alistair Knott. Anaphora and discourse structure. Computational Linguistics, 4(29):545– 587, 2003. Z.-M. Zhou, Y. Xu, Z.-Y. Niu, M. Lan, J. Su, and C.L. Tan. Predicting discourse connectives for implicit discourse relation recognition. In COLING 2010, pages 1507–15 14, Beijing, China, August 2010.</p><p>6 0.14020312 <a title="52-tfidf-6" href="./acl-2012-PDTB-style_Discourse_Annotation_of_Chinese_Text.html">157 acl-2012-PDTB-style Discourse Annotation of Chinese Text</a></p>
<p>7 0.13487417 <a title="52-tfidf-7" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>8 0.10940783 <a title="52-tfidf-8" href="./acl-2012-Chinese_Comma_Disambiguation_for_Discourse_Analysis.html">47 acl-2012-Chinese Comma Disambiguation for Discourse Analysis</a></p>
<p>9 0.092117287 <a title="52-tfidf-9" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>10 0.084066361 <a title="52-tfidf-10" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>11 0.082256369 <a title="52-tfidf-11" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>12 0.075671144 <a title="52-tfidf-12" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>13 0.06324847 <a title="52-tfidf-13" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>14 0.062740609 <a title="52-tfidf-14" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>15 0.062616706 <a title="52-tfidf-15" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>16 0.057911143 <a title="52-tfidf-16" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>17 0.05450587 <a title="52-tfidf-17" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>18 0.053887784 <a title="52-tfidf-18" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<p>19 0.053301398 <a title="52-tfidf-19" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>20 0.052923806 <a title="52-tfidf-20" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.184), (1, 0.038), (2, -0.034), (3, 0.107), (4, 0.029), (5, -0.03), (6, -0.128), (7, -0.058), (8, -0.196), (9, 0.29), (10, -0.06), (11, -0.027), (12, 0.004), (13, -0.006), (14, 0.009), (15, 0.057), (16, 0.122), (17, -0.017), (18, -0.036), (19, -0.019), (20, 0.242), (21, -0.149), (22, 0.335), (23, 0.037), (24, 0.049), (25, 0.113), (26, -0.051), (27, 0.012), (28, 0.183), (29, 0.083), (30, 0.044), (31, 0.148), (32, 0.016), (33, -0.112), (34, -0.117), (35, -0.081), (36, -0.018), (37, 0.105), (38, -0.135), (39, 0.076), (40, -0.037), (41, -0.03), (42, -0.021), (43, 0.013), (44, -0.031), (45, -0.026), (46, -0.034), (47, -0.07), (48, 0.034), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95374846 <a title="52-lsi-1" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>2 0.89074689 <a title="52-lsi-2" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>Author: Karolina Owczarzak ; Peter A. Rankel ; Hoa Trang Dang ; John M. Conroy</p><p>Abstract: We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.</p><p>3 0.74980527 <a title="52-lsi-3" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>4 0.42163742 <a title="52-lsi-4" href="./acl-2012-Text-level_Discourse_Parsing_with_Rich_Linguistic_Features.html">193 acl-2012-Text-level Discourse Parsing with Rich Linguistic Features</a></p>
<p>Author: Vanessa Wei Feng ; Graeme Hirst</p><p>Abstract: In this paper, we develop an RST-style textlevel discourse parser, based on the HILDA discourse parser (Hernault et al., 2010b). We significantly improve its tree-building step by incorporating our own rich linguistic features. We also analyze the difficulty of extending traditional sentence-level discourse parsing to text-level parsing by comparing discourseparsing performance under different discourse conditions.</p><p>5 0.41707107 <a title="52-lsi-5" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>Author: Chang Liu ; Hwee Tou Ng</p><p>Abstract: In this work, we introduce the TESLACELAB metric (Translation Evaluation of Sentences with Linear-programming-based Analysis Character-level Evaluation for Languages with Ambiguous word Boundaries) for automatic machine translation evaluation. For languages such as Chinese where words usually have meaningful internal structure and word boundaries are often fuzzy, TESLA-CELAB acknowledges the advantage of character-level evaluation over word-level evaluation. By reformulating the problem in the linear programming framework, TESLACELAB addresses several drawbacks of the character-level metrics, in particular the modeling of synonyms spanning multiple characters. We show empirically that TESLACELAB significantly outperforms characterlevel BLEU in the English-Chinese translation evaluation tasks. –</p><p>6 0.35892516 <a title="52-lsi-6" href="./acl-2012-Chinese_Comma_Disambiguation_for_Discourse_Analysis.html">47 acl-2012-Chinese Comma Disambiguation for Discourse Analysis</a></p>
<p>7 0.35567531 <a title="52-lsi-7" href="./acl-2012-PDTB-style_Discourse_Annotation_of_Chinese_Text.html">157 acl-2012-PDTB-style Discourse Annotation of Chinese Text</a></p>
<p>8 0.34059116 <a title="52-lsi-8" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>9 0.32997841 <a title="52-lsi-9" href="./acl-2012-Sentence_Simplification_by_Monolingual_Machine_Translation.html">178 acl-2012-Sentence Simplification by Monolingual Machine Translation</a></p>
<p>10 0.3239679 <a title="52-lsi-10" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>11 0.32353663 <a title="52-lsi-11" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>12 0.32158539 <a title="52-lsi-12" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>13 0.31614017 <a title="52-lsi-13" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>14 0.24737915 <a title="52-lsi-14" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<p>15 0.22722992 <a title="52-lsi-15" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<p>16 0.22355333 <a title="52-lsi-16" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>17 0.21717376 <a title="52-lsi-17" href="./acl-2012-Information-theoretic_Multi-view_Domain_Adaptation.html">120 acl-2012-Information-theoretic Multi-view Domain Adaptation</a></p>
<p>18 0.21175501 <a title="52-lsi-18" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>19 0.21100971 <a title="52-lsi-19" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>20 0.20838976 <a title="52-lsi-20" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(22, 0.247), (25, 0.044), (26, 0.037), (28, 0.035), (30, 0.032), (37, 0.033), (39, 0.038), (74, 0.032), (81, 0.015), (82, 0.018), (84, 0.017), (85, 0.059), (90, 0.106), (92, 0.053), (94, 0.023), (99, 0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77046049 <a title="52-lda-1" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>Author: Ziheng Lin ; Chang Liu ; Hwee Tou Ng ; Min-Yen Kan</p><p>Abstract: An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure content coverage, apply an enhanced discourse coherence model to evaluate summary readability, and combine both in a trained regression model to evaluate overall responsiveness. The results show significantly improved performance over AESOP 2011 submitted metrics.</p><p>2 0.71486247 <a title="52-lda-2" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>3 0.67253202 <a title="52-lda-3" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>Author: Karolina Owczarzak ; Peter A. Rankel ; Hoa Trang Dang ; John M. Conroy</p><p>Abstract: We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.</p><p>4 0.59304154 <a title="52-lda-4" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>5 0.59211802 <a title="52-lda-5" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein ; James R. Curran</p><p>Abstract: We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (5 1.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.</p><p>6 0.57717758 <a title="52-lda-6" href="./acl-2012-Movie-DiC%3A_a_Movie_Dialogue_Corpus_for_Research_and_Development.html">149 acl-2012-Movie-DiC: a Movie Dialogue Corpus for Research and Development</a></p>
<p>7 0.57665229 <a title="52-lda-7" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>8 0.57640177 <a title="52-lda-8" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>9 0.57394636 <a title="52-lda-9" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>10 0.57389486 <a title="52-lda-10" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>11 0.57321191 <a title="52-lda-11" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>12 0.57283509 <a title="52-lda-12" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>13 0.57144964 <a title="52-lda-13" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>14 0.55381936 <a title="52-lda-14" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>15 0.5524525 <a title="52-lda-15" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>16 0.55158472 <a title="52-lda-16" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>17 0.5495581 <a title="52-lda-17" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>18 0.54916501 <a title="52-lda-18" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>19 0.5485059 <a title="52-lda-19" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>20 0.54221606 <a title="52-lda-20" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
