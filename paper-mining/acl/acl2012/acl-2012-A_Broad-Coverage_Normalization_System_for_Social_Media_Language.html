<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-2" href="#">acl2012-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</h1>
<br/><p>Source: <a title="acl-2012-2-pdf" href="http://aclweb.org/anthology//P/P12/P12-1109.pdf">pdf</a></p><p>Author: Fei Liu ; Fuliang Weng ; Xiao Jiang</p><p>Abstract: Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a . 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</p><p>Reference: <a title="acl-2012-2-reference" href="../acl2012_reference/acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 u  xliiaaon  Abstract Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. [sent-7, score-0.729]
</p><p>2 It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. [sent-8, score-0.81]
</p><p>3 , for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. [sent-11, score-0.672]
</p><p>4 In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. [sent-12, score-1.365]
</p><p>5 1 Introduction The amount of user generated content has increased drastically in the past few years, driven by the prosperous development of the social media websites such as Twitter, Facebook, and Google+. [sent-16, score-0.157]
</p><p>6 Yet existing systems often perform poorly in this domain due the to extensive use of the nonstandard tokens, emoticons, incomplete and ungrammatical sentences, etc. [sent-28, score-0.604]
</p><p>7 Text normalization is also crucial for building robust text-to-speech (TTS) systems, which need to determine the pronunciations for nonstandard words in the social text. [sent-37, score-0.866]
</p><p>8 The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. [sent-38, score-0.886]
</p><p>9 We aim for a robust text normalization system with “broad coverage”, i. [sent-39, score-0.18]
</p><p>10 , for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3, 10. [sent-41, score-0.782]
</p><p>11 This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. [sent-45, score-0.604]
</p><p>12 , 2010); second, the nonstandard tokens consist ProceediJnegjus, o Rfe thpeu 5bl0icth o Afn Knouraela M, 8e-e1t4in Jgul oyf t 2h0e1 A2. [sent-48, score-0.697]
</p><p>13 of a mixture of both unintentional misspellings and intentionally-created tokens for various reasons1 ,including the needs for speed, ease of typing (Crystal, 2009), sentiment expressing (e. [sent-52, score-0.191]
</p><p>14 Existing spell checkers and normalization systems rely heavily on lexical/phonetic similarity to select the correct candidate words. [sent-57, score-0.356]
</p><p>15 , (tomorrow, “tmrw”)2), yet the number of candidates increases dramatically as the system strives to increase the coverage by enlarging the threshold. [sent-60, score-0.204]
</p><p>16 (Han and Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%. [sent-61, score-0.888]
</p><p>17 Different from previous work, we tackle the text normalization problem from a cognitive-sensitive perspective and investigate the human rationales for normalizing the nonstandard tokens. [sent-63, score-0.854]
</p><p>18 We argue that there exists a set of letter transformation patterns that humans use to decipher the nonstandard tokens. [sent-64, score-0.986]
</p><p>19 In this paper, we propose a broad-coverage normalization system by integrating three human per1For this reason, we will use the term “nonstandard tokens” instead of “ill-formed tokens” throughout the paper. [sent-68, score-0.205]
</p><p>20 2We use the form (standard word, “nonstandard token”) to denote an example nonstandard token and its corresponding standard word. [sent-69, score-0.751]
</p><p>21 1036 spectives, including the enhanced letter transformation, visual priming, and the string and phonetic similarity. [sent-70, score-0.483]
</p><p>22 For an arbitrary nonstandard token, the three subnormalizers each suggest their most confident candidates from a different perspective. [sent-71, score-0.817]
</p><p>23 Results show that our system can achieve over 90% wordcoverage with limited number of candidates and the broad word-coverage can be successfully translated into message-level performance gain. [sent-74, score-0.187]
</p><p>24 , 1991 ; Brill and Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. [sent-80, score-0.193]
</p><p>25 , 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. [sent-82, score-0.301]
</p><p>26 With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. [sent-83, score-1.205]
</p><p>27 , 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French  SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. [sent-86, score-0.177]
</p><p>28 The text normalization problem was also tackled under the machine translation (MT) or speech recognition (ASR) framework. [sent-87, score-0.138]
</p><p>29 , 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. [sent-96, score-0.697]
</p><p>30 In this paper, we propose a novel cognitively-driven text normalization system that robustly tackle both the unintentional misspellings and the intentionally-created noisy tokens. [sent-104, score-0.367]
</p><p>31 We propose a global context-based approach to purify the automatically collected training data and learn the letter transformation patterns without human supervision. [sent-105, score-0.386]
</p><p>32 To the best of our knowledge, we are the first to integrate these human perspectives in the text normalization system. [sent-108, score-0.179]
</p><p>33 3  Broad-Coverage Normalization System  In this section, we describe our broad-coverage normalization system, which consists of four key com-  ponents. [sent-109, score-0.138]
</p><p>34 candidates from a different “Enhanced Letter Transformation” automatically learns a set of letter transformation patterns and is most effective in normalizing the intentionally created nonstandard tokens through letter insertion, repetition, deletion, and substitution (Section 3. [sent-111, score-1.56]
</p><p>35 1); “Visual Priming” proposes candidates based on the visual cues and a primed perspective (Section 3. [sent-112, score-0.286]
</p><p>36 Note that it is crucial to integrate different human perspectives so that the system is flexible in pro-  perspective3:  cessing both unintentional misspellings and various intentionally-created noisy tokens. [sent-117, score-0.294]
</p><p>37 We formulate the process of generating a nonstandard token ti from the dictionary word si using a letter transformation model, and use the model confidence as the probability p(ti |si). [sent-123, score-1.434]
</p><p>38 To form a nonstandard token, each letter in the dictionary word can be labeled with: (a) one of the 0-9 digits; (b) one of the 26 characters including itself; (c) the null character “-”; (d) a letter combination4. [sent-125, score-1.238]
</p><p>39 This transformation process from dictio-  3For the dictionary word, we allow the subnormalizers to either return the word itself or candidates that are the possibly intended words in the given context (e. [sent-126, score-0.436]
</p><p>40 4The set of letter combinations used in this work are {ah, ai, aw, ay, cek s,e ea, ey, teier, ou, te, nwathio} nary words to nonstandard tokens will be learned by a character-level sequence labeling system using the automatically collected (word, token) pairs. [sent-129, score-1.006]
</p><p>41 Next, we create a large lookup table by applying the character-level labeling system to the standard dictionary words and generate multiple variations for each word using the n-best labeling output, the labeling confidence is used as p(ti |si). [sent-130, score-0.253]
</p><p>42 During testing, we search this lookup table to fin|ds the best candidate words for the nonstandard tokens. [sent-131, score-0.688]
</p><p>43 For tokens with letter repetition, we first generate a set of variants by varying the repetitive letters (e. [sent-132, score-0.361]
</p><p>44 1 Context-Aware Training Pair Selection Manual annotation of the noisy nonstandard tokens takes a lot of time and effort. [sent-139, score-0.786]
</p><p>45 The ideal training data should consist of the most frequent nonstandard tokens paired with the corresponding corrections, so that the system can learn from the most representative letter transformation patterns. [sent-143, score-1.096]
</p><p>46 Motivated by research on word sense disambiguation (WSD) (Mihalcea, 2007), we hypothesize the nonstandard token and the standard word share a lot of common terms in their global context. [sent-144, score-0.803]
</p><p>47 To the best of our knowledge, we are the first to explore this global contextual similarity for the text normalization task. [sent-148, score-0.165]
</p><p>48 The term weights are defined using a  normalized TF-IDF method:  wi,k=TTFFii,k× log(DNFk) where TFi,k is the count of term tk appearing within the context of term ti; TFi is the total count of ti in the corpus. [sent-153, score-0.211]
</p><p>49 TTFFii,k is therefore the relative frequency of tk appearing in the context of ti; log(DNFk) denotes the inverse document frequency of tk, calculated as the logarithm of total tweets (N) divided by the number of tweets containing tk. [sent-154, score-0.248]
</p><p>50 To select the most representative (word, token) pairs for training, we rank the automatically collected 46,288 pairs by the token frequency, filter out pairs whose contextual similarity lower than a threshold θ (set empirically at 0. [sent-155, score-0.174]
</p><p>51 , 2011b), then con-  struct a feature vector for each letter of the dictionary word, using its mapped character as the reference label. [sent-161, score-0.366]
</p><p>52 , 2011b), but develop a set of boundary features to effectively characterize the letter transformation process. [sent-166, score-0.429]
</p><p>53 We notice that in creating the nonstandard tokens, humans tend to drop certain letter units from the word or replace them with other letters. [sent-167, score-0.922]
</p><p>54 On top of the boundary tags, we develop a set of conjunction features to accurately pinpoint the current character position. [sent-172, score-0.148]
</p><p>55 We consider conjunction features formed by concatenating character position in syllable and current syllable position in the word (e. [sent-173, score-0.204]
</p><p>56 , conjunction feature “L B” for the letter “d” in Table 2). [sent-175, score-0.276]
</p><p>57 We consider conjunction of character/vowel feature and their boundary tags on the syllable/morpheme/word level; conjunction of phoneme and phoneme boundary tags, and absolute position of current character within the corre5Phoneme decomposition is generated using the (Jiampojamarn et al. [sent-177, score-0.358]
</p><p>58 , 2007) algorithm to map up to two letters to phonemes (2-to-2 alignment); syllable boundary acquired by the hyphenation algorithm (Liang, 1983); morpheme boundary determined by toolkit Morfessor 1. [sent-178, score-0.259]
</p><p>59 We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. [sent-182, score-0.3]
</p><p>60 When a nonstandard token is seen during testing, we apply the noisy channel to generate a list of best candidate words: = arg maxsip(ti |si)p(si). [sent-183, score-0.964]
</p><p>61 2 Visual Priming Approach A second key component of the broad-coverage normalization system is a novel “Visual Priming” subnormalizer. [sent-185, score-0.18]
</p><p>62 A priming effect is observed when  participants complete stems with words on the study list more often than with the novel words. [sent-189, score-0.324]
</p><p>63 A person familiarized with the “social talk” is highly primed with the most commonly used words; later when a nonstandard token shows only minor visual cues or visual stimulus, it can still be quickly recognized by the person. [sent-192, score-1.106]
</p><p>64 In this process, the first letter or first few letters of the word serve as a very important visual stimulus. [sent-193, score-0.441]
</p><p>65 Based on this assumption, we introduce the “priming” subnormalizer based only on the word frequency and the minor visual stimulus. [sent-194, score-0.349]
</p><p>66 Note that the first character has been shown to be a crucial visual cue for the brain to understand jumbled words (Davis, ), we therefore consider as candidates only those words si that start with the same character as ti. [sent-199, score-0.517]
</p><p>67 In the case that the nonstandard token ti starts with a digit (e. [sent-200, score-0.843]
</p><p>68 , “2moro”), we use the mostly likely corresponding letter to search the candidates (those starting with letter “t”). [sent-202, score-0.594]
</p><p>69 The “visual priming” subnormalizer promotes the candidate words that are frequently used in the social talk and also bear visual similarity with the given noisy token. [sent-204, score-0.51]
</p><p>70 This approach also inherently follows the noisy channel framework, with p(ti |si) represents the visual stimulus and p(si) being |thse logarithm of frequency. [sent-206, score-0.316]
</p><p>71 3 Spell Checker The third subnormalizer is the spell checker, which combines the string and phonetic similarity algorithms and is most effective in normalizing the misspellings. [sent-210, score-0.395]
</p><p>72 We use the Jazzy spell checker (Idzelis, 2005) that integrates the DoubleMetaphone phonetic matching algorithm and the Levenshtein distance using the near-miss strategy, which enables the interchange of two adjacent letters, and the replacing/deleting/adding of letters. [sent-211, score-0.294]
</p><p>73 4  Candidate Combination  Each of the three subnormalizers is a stand-alone system and can suggest corrections for the nonstandard tokens. [sent-213, score-0.812]
</p><p>74 Yet we show that each subnormalizer mimics a different perspective that humans use to decode the nonstandard tokens, as a result, our broad-coverage normalization system is built by integrating candidates from the three subnormalizers using various strategies. [sent-214, score-1.14]
</p><p>75 For a noisy token seen in the informal text, the most convenient way of system combination is to  harvest up to n candidates from each of the subnormalizers, and use the pool of candidates (up to 1040 3n) as the system output. [sent-215, score-0.54]
</p><p>76 In Table 3, we also present the number of distinct nonstandard tokens found in each data set, and notice that only a small portion of the nonstandard tokens correspond to multiple standard words. [sent-231, score-1.419]
</p><p>77 We calculate the dictionary coverage of the manually annotated words since this sets an upper bound for any normalization system. [sent-232, score-0.247]
</p><p>78 8 The nonstandard tokens may consist of both numbers/characters and apostrophe. [sent-235, score-0.697]
</p><p>79 8The dictionary is created by combining the CMU (CMU, 2007) and Aspell (Atkinson, 2006) dictionaries and dropping words with frequency < 20 in the background corpus. [sent-236, score-0.16]
</p><p>80 The goal of word-level normalization is to convert the list of distinct nonstandard tokens into standard words. [sent-246, score-0.835]
</p><p>81 For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. [sent-247, score-0.646]
</p><p>82 On message-level, we evaluate the 1-best system output using precision, recall, and f-score, calculated respective to the nonstandard tokens. [sent-249, score-0.67]
</p><p>83 We present the n-best accuracy (n = 1, 3, 10, 20) of the system as well as the “Oracle” results generated by pooling the top-20 candidates from each of the three subnormalizers. [sent-252, score-0.152]
</p><p>84 This is of crucial importance to a normalization system, since the high accuracy and limited number of candidates will enable more sophisticated reranking or supervised learning techniques to select the best candidate. [sent-257, score-0.272]
</p><p>85 These are out of the capabilities of the current text normalization system and partly explains the remaining 5% gap. [sent-262, score-0.18]
</p><p>86 Regarding the subnormalizer performance, the spell checker yields only 50% to 60% accuracy on all data sets, indicating that the vast amount of the intentionally created nonstandard tokens can hardly be tackled by a system relies solely on the lexical/phonetic similarity. [sent-263, score-1.19]
</p><p>87 A minor sideeffect is that the candidates were restricted to have  the same first letter with the noisy token, this sets the upper bound of the approach to 89. [sent-293, score-0.473]
</p><p>88 ” is effective at normalizing intentionally created tokens and has better precision regarding its top candidate (n = 1). [sent-298, score-0.303]
</p><p>89 We notice that the system can effectively learn the letter transformation patterns from a small number of high quality training pairs. [sent-300, score-0.424]
</p><p>90 The final system was trained using the top 5,000 pairs and the lookup table was created by generating 50 variations for each dictionary word. [sent-301, score-0.179]
</p><p>91 3 Message-level Results The goal of message-level normalization is to replace each occurrence of the nonstandard token with the candidate word that best fits the local context. [sent-303, score-0.971]
</p><p>92 Amount of Training Pairs  (~45K)  Figure 2: Learning curve of the enhanced letter transformation system using random training pair selection or the context-aware approach. [sent-316, score-0.464]
</p><p>93 Following research in (Han and Baldwin, 2011), we focus on the the normalization task and assume perfect nonstandard token detection. [sent-321, score-0.889]
</p><p>94 The “Word-level w/o Context” results are generated by replacing each nonstandard token using the 1-best word-level candidate. [sent-322, score-0.751]
</p><p>95 5  Conclusion  In this paper, we propose a broad-coverage normalization system for the social media language without using the human annotations. [sent-346, score-0.337]
</p><p>96 It integrates three key components: the enhanced letter transformation, visual priming, and string/phonetic similarity. [sent-347, score-0.454]
</p><p>97 We observe that the social media is an emotion-rich language, therefore future normalization system will need to address various sentimentrelated expressions, such as emoticons (“:d”, “X8”), interjections (“bwahaha”, “brrrr”), acronyms (“lol”, “lmao”), etc. [sent-350, score-0.372]
</p><p>98 A hybrid rule/model-based finite-state framework for normalizing sms messages. [sent-368, score-0.241]
</p><p>99 Exploring multiple text sources  for twitter topic summarization. [sent-479, score-0.15]
</p><p>100 A characterlevel machine translation approach for normalization of sms abbreviations. [sent-508, score-0.293]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nonstandard', 0.604), ('priming', 0.324), ('letter', 0.242), ('spell', 0.162), ('sms', 0.155), ('twitter', 0.15), ('visual', 0.147), ('token', 0.147), ('normalization', 0.138), ('si', 0.126), ('subnormalizer', 0.118), ('transformation', 0.115), ('candidates', 0.11), ('subnormalizers', 0.103), ('checker', 0.103), ('social', 0.1), ('tokens', 0.093), ('ti', 0.092), ('noisy', 0.089), ('pennell', 0.088), ('normalizing', 0.086), ('dictionary', 0.082), ('boundary', 0.072), ('enhanced', 0.065), ('tweets', 0.064), ('corrections', 0.063), ('messagelevel', 0.059), ('misspellings', 0.059), ('media', 0.057), ('candidate', 0.056), ('messages', 0.053), ('phoneme', 0.052), ('liu', 0.052), ('bilou', 0.051), ('syllable', 0.051), ('lm', 0.044), ('deana', 0.044), ('fuliang', 0.044), ('maxsip', 0.044), ('visualprim', 0.044), ('tk', 0.044), ('character', 0.042), ('system', 0.042), ('intentionally', 0.041), ('perspectives', 0.041), ('channel', 0.041), ('petrovic', 0.039), ('stimulus', 0.039), ('unintentional', 0.039), ('morpheme', 0.038), ('han', 0.037), ('acronyms', 0.035), ('broad', 0.035), ('crf', 0.034), ('conjunction', 0.034), ('viterbi', 0.034), ('baldwin', 0.034), ('minor', 0.032), ('completion', 0.031), ('beaufort', 0.029), ('brill', 0.029), ('dnfk', 0.029), ('gouws', 0.029), ('isualprim', 0.029), ('jazzy', 0.029), ('kobus', 0.029), ('mays', 0.029), ('microtext', 0.029), ('primed', 0.029), ('purify', 0.029), ('subramaniam', 0.029), ('tulving', 0.029), ('phonetic', 0.029), ('lookup', 0.028), ('arg', 0.027), ('contextual', 0.027), ('coverage', 0.027), ('created', 0.027), ('edinburgh', 0.026), ('frequency', 0.026), ('advertisements', 0.026), ('celikyilmaz', 0.026), ('choudhury', 0.026), ('hogan', 0.026), ('jumbled', 0.026), ('morfessor', 0.026), ('rationales', 0.026), ('weng', 0.026), ('letters', 0.026), ('word', 0.026), ('aw', 0.025), ('pages', 0.025), ('humans', 0.025), ('notice', 0.025), ('labeling', 0.025), ('term', 0.025), ('background', 0.025), ('yet', 0.025), ('crucial', 0.024), ('calculated', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="2-tfidf-1" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Xiao Jiang</p><p>Abstract: Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a . 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</p><p>2 0.13189167 <a title="2-tfidf-2" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>Author: Hao Wang ; Dogan Can ; Abe Kazemzadeh ; Francois Bar ; Shrikanth Narayanan</p><p>Abstract: This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a microblogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion. 1</p><p>3 0.11778974 <a title="2-tfidf-3" href="./acl-2012-Personalized_Normalization_for_a_Multilingual_Chat_System.html">160 acl-2012-Personalized Normalization for a Multilingual Chat System</a></p>
<p>Author: Ai Ti Aw ; Lian Hau Lee</p><p>Abstract: This paper describes the personalized normalization of a multilingual chat system that supports chatting in user defined short-forms or abbreviations. One of the major challenges for multilingual chat realized through machine translation technology is the normalization of non-standard, self-created short-forms in the chat message to standard words before translation. Due to the lack of training data and the variations of short-forms used among different social communities, it is hard to normalize and translate chat messages if user uses vocabularies outside the training data and create short-forms freely. We develop a personalized chat normalizer for English and integrate it with a multilingual chat system, allowing user to create and use personalized short-forms in multilingual chat. 1</p><p>4 0.0947593 <a title="2-tfidf-4" href="./acl-2012-Distributional_Semantics_in_Technicolor.html">76 acl-2012-Distributional Semantics in Technicolor</a></p>
<p>Author: Elia Bruni ; Gemma Boleda ; Marco Baroni ; Nam Khanh Tran</p><p>Abstract: Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.</p><p>5 0.079540633 <a title="2-tfidf-5" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>Author: Xiaohua Liu ; Ming Zhou ; Xiangyang Zhou ; Zhongyang Fu ; Furu Wei</p><p>Abstract: Tweets represent a critical source of fresh information, in which named entities occur frequently with rich variations. We study the problem of named entity normalization (NEN) for tweets. Two main challenges are the errors propagated from named entity recognition (NER) and the dearth of information in a single tweet. We propose a novel graphical model to simultaneously conduct NER and NEN on multiple tweets to address these challenges. Particularly, our model introduces a binary random variable for each pair of words with the same lemma across similar tweets, whose value indicates whether the two related words are mentions of the same entity. We evaluate our method on a manually annotated data set, and show that our method outperforms the baseline that handles these two tasks separately, boosting the F1 from 80.2% to 83.6% for NER, and the Accuracy from 79.4% to 82.6% for NEN, respectively.</p><p>6 0.078983687 <a title="2-tfidf-6" href="./acl-2012-Tweet_Recommendation_with_Graph_Co-Ranking.html">205 acl-2012-Tweet Recommendation with Graph Co-Ranking</a></p>
<p>7 0.076181658 <a title="2-tfidf-7" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>8 0.072656006 <a title="2-tfidf-8" href="./acl-2012-Decoding_Running_Key_Ciphers.html">68 acl-2012-Decoding Running Key Ciphers</a></p>
<p>9 0.071243852 <a title="2-tfidf-9" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>10 0.068464711 <a title="2-tfidf-10" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>11 0.068079576 <a title="2-tfidf-11" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>12 0.06529431 <a title="2-tfidf-12" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>13 0.062766381 <a title="2-tfidf-13" href="./acl-2012-Arabic_Retrieval_Revisited%3A_Morphological_Hole_Filling.html">27 acl-2012-Arabic Retrieval Revisited: Morphological Hole Filling</a></p>
<p>14 0.058551196 <a title="2-tfidf-14" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>15 0.058463514 <a title="2-tfidf-15" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>16 0.057966929 <a title="2-tfidf-16" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>17 0.057353918 <a title="2-tfidf-17" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>18 0.057036582 <a title="2-tfidf-18" href="./acl-2012-Tokenization%3A_Returning_to_a_Long_Solved_Problem__A_Survey%2C_Contrastive_Experiment%2C_Recommendations%2C_and_Toolkit_.html">197 acl-2012-Tokenization: Returning to a Long Solved Problem  A Survey, Contrastive Experiment, Recommendations, and Toolkit </a></p>
<p>19 0.056261979 <a title="2-tfidf-19" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<p>20 0.056254718 <a title="2-tfidf-20" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.171), (1, 0.056), (2, 0.026), (3, 0.027), (4, 0.034), (5, 0.074), (6, 0.187), (7, 0.008), (8, 0.025), (9, 0.08), (10, -0.029), (11, -0.022), (12, -0.001), (13, 0.03), (14, 0.019), (15, -0.012), (16, 0.007), (17, 0.03), (18, 0.025), (19, 0.002), (20, -0.026), (21, -0.081), (22, 0.036), (23, -0.007), (24, -0.108), (25, 0.112), (26, 0.093), (27, -0.014), (28, -0.058), (29, -0.118), (30, 0.068), (31, -0.128), (32, -0.006), (33, 0.092), (34, 0.055), (35, 0.022), (36, 0.033), (37, -0.049), (38, -0.043), (39, 0.084), (40, -0.069), (41, -0.089), (42, -0.109), (43, -0.027), (44, -0.11), (45, -0.052), (46, -0.02), (47, 0.12), (48, 0.074), (49, -0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91993713 <a title="2-lsi-1" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Xiao Jiang</p><p>Abstract: Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a . 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</p><p>2 0.62599832 <a title="2-lsi-2" href="./acl-2012-Personalized_Normalization_for_a_Multilingual_Chat_System.html">160 acl-2012-Personalized Normalization for a Multilingual Chat System</a></p>
<p>Author: Ai Ti Aw ; Lian Hau Lee</p><p>Abstract: This paper describes the personalized normalization of a multilingual chat system that supports chatting in user defined short-forms or abbreviations. One of the major challenges for multilingual chat realized through machine translation technology is the normalization of non-standard, self-created short-forms in the chat message to standard words before translation. Due to the lack of training data and the variations of short-forms used among different social communities, it is hard to normalize and translate chat messages if user uses vocabularies outside the training data and create short-forms freely. We develop a personalized chat normalizer for English and integrate it with a multilingual chat system, allowing user to create and use personalized short-forms in multilingual chat. 1</p><p>3 0.56142831 <a title="2-lsi-3" href="./acl-2012-Decoding_Running_Key_Ciphers.html">68 acl-2012-Decoding Running Key Ciphers</a></p>
<p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. We consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using Gibbs sampling with a word language model. We evaluate our method on synthetic ciphertexts of different lengths, and find that it outperforms previous work that employs Viterbi decoding with character-based models.</p><p>4 0.55438167 <a title="2-lsi-4" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>Author: Marco Guerini ; Carlo Strapparava ; Oliviero Stock</p><p>Abstract: In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach.</p><p>5 0.52978593 <a title="2-lsi-5" href="./acl-2012-Named_Entity_Disambiguation_in_Streaming_Data.html">153 acl-2012-Named Entity Disambiguation in Streaming Data</a></p>
<p>Author: Alexandre Davis ; Adriano Veloso ; Altigran Soares ; Alberto Laender ; Wagner Meira Jr.</p><p>Abstract: The named entity disambiguation task is to resolve the many-to-many correspondence between ambiguous names and the unique realworld entity. This task can be modeled as a classification problem, provided that positive and negative examples are available for learning binary classifiers. High-quality senseannotated data, however, are hard to be obtained in streaming environments, since the training corpus would have to be constantly updated in order to accomodate the fresh data coming on the stream. On the other hand, few positive examples plus large amounts of unlabeled data may be easily acquired. Producing binary classifiers directly from this data, however, leads to poor disambiguation performance. Thus, we propose to enhance the quality of the classifiers using finer-grained variations of the well-known ExpectationMaximization (EM) algorithm. We conducted a systematic evaluation using Twitter streaming data and the results show that our classifiers are extremely effective, providing improvements ranging from 1% to 20%, when compared to the current state-of-the-art biased SVMs, being more than 120 times faster.</p><p>6 0.51333362 <a title="2-lsi-6" href="./acl-2012-Distributional_Semantics_in_Technicolor.html">76 acl-2012-Distributional Semantics in Technicolor</a></p>
<p>7 0.45827642 <a title="2-lsi-7" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>8 0.45369044 <a title="2-lsi-8" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>9 0.45238736 <a title="2-lsi-9" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>10 0.451435 <a title="2-lsi-10" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>11 0.43480876 <a title="2-lsi-11" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>12 0.43460548 <a title="2-lsi-12" href="./acl-2012-Joint_Inference_of_Named_Entity_Recognition_and_Normalization_for_Tweets.html">124 acl-2012-Joint Inference of Named Entity Recognition and Normalization for Tweets</a></p>
<p>13 0.4242782 <a title="2-lsi-13" href="./acl-2012-Collective_Generation_of_Natural_Image_Descriptions.html">51 acl-2012-Collective Generation of Natural Image Descriptions</a></p>
<p>14 0.4199641 <a title="2-lsi-14" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<p>15 0.41036218 <a title="2-lsi-15" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>16 0.3994596 <a title="2-lsi-16" href="./acl-2012-Tweet_Recommendation_with_Graph_Co-Ranking.html">205 acl-2012-Tweet Recommendation with Graph Co-Ranking</a></p>
<p>17 0.38529578 <a title="2-lsi-17" href="./acl-2012-Demonstration_of_IlluMe%3A_Creating_Ambient_According_to_Instant_Message_Logs.html">70 acl-2012-Demonstration of IlluMe: Creating Ambient According to Instant Message Logs</a></p>
<p>18 0.38361114 <a title="2-lsi-18" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>19 0.38303941 <a title="2-lsi-19" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>20 0.37634099 <a title="2-lsi-20" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.018), (26, 0.033), (28, 0.038), (30, 0.02), (37, 0.018), (39, 0.048), (74, 0.026), (75, 0.018), (81, 0.011), (82, 0.013), (84, 0.019), (85, 0.019), (90, 0.508), (92, 0.049), (94, 0.022), (99, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99791443 <a title="2-lda-1" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Xiao Jiang</p><p>Abstract: Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a . 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</p><p>2 0.99706405 <a title="2-lda-2" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>3 0.99644792 <a title="2-lda-3" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; George Foster ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</p><p>4 0.99632573 <a title="2-lda-4" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>Author: Kuzman Ganchev ; Keith Hall ; Ryan McDonald ; Slav Petrov</p><p>Abstract: Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.</p><p>5 0.99465024 <a title="2-lda-5" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.</p><p>6 0.97822767 <a title="2-lda-6" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>7 0.9779157 <a title="2-lda-7" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>8 0.96996057 <a title="2-lda-8" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>9 0.96948075 <a title="2-lda-9" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>10 0.96760541 <a title="2-lda-10" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>11 0.96550536 <a title="2-lda-11" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>12 0.96542203 <a title="2-lda-12" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>13 0.9647007 <a title="2-lda-13" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>14 0.96414059 <a title="2-lda-14" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>15 0.95910496 <a title="2-lda-15" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>16 0.95507228 <a title="2-lda-16" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>17 0.95273906 <a title="2-lda-17" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>18 0.95059597 <a title="2-lda-18" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>19 0.94752079 <a title="2-lda-19" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>20 0.9455933 <a title="2-lda-20" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
