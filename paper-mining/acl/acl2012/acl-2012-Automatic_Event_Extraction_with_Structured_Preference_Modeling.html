<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-33" href="#">acl2012-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</h1>
<br/><p>Source: <a title="acl-2012-33-pdf" href="http://aclweb.org/anthology//P/P12/P12-1088.pdf">pdf</a></p><p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>Reference: <a title="acl-2012-33-reference" href="../acl2012_reference/acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  Abstract This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. [sent-2, score-0.393]
</p><p>2 The model takes in coarse mention and type information and predicts argument roles for a given event template. [sent-3, score-0.898]
</p><p>3 This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. [sent-4, score-0.55]
</p><p>4 Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. [sent-5, score-1.283]
</p><p>5 We show empirically that the structured preferences are crucial to the success of our task. [sent-7, score-0.301]
</p><p>6 1 Introduction Automatic template-filling-based event extraction is an important and challenging task. [sent-9, score-0.4]
</p><p>7 Each row shows an 835 argument for the event, together with a set of its acceptable mention types, where the type specifies a high-level semantic class a mention belongs to. [sent-15, score-0.742]
</p><p>8 One typical assumption is that certain coarse mention-level information, such as mention boundaries and their semantic class (a. [sent-19, score-0.428]
</p><p>9 However, in practice, outputs from existing mention identification and typing systems can be far from ideal. [sent-31, score-0.381]
</p><p>10 Instead of obtaining the above ideal annotation, one might observe the following noisy and ambiguous annotation for the given event span:  O[f. [sent-32, score-0.352]
</p><p>11 stC ao rktyfeh]OicplReGa[iolmt]spvP|aeFElyRAfrCh]|sOLVaREvieGCHdlct  mentions in an event span and assign them with corresponding argument information, given such coarse ProceedJienjgus, R ofep thueb 5lic0t hof A Knonrueaa,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-36, score-0.862]
</p><p>12 kdl’isvTcmeonpxt(ralfy),  and the correct event template annotation for the example event span given in Sec 1 (right). [sent-40, score-0.911]
</p><p>13 This motivates us to build a novel latentvariable semi-Markov conditional random fields model (Sarawagi and Cohen, 2004) for such an event extraction task. [sent-44, score-0.564]
</p><p>14 The learned model takes in coarse information as produced by existing mention identification and typing modules, and jointly outputs selected mentions and their corresponding argument roles. [sent-45, score-0.759]
</p><p>15 We propose a novel general learning framework called structured preference modeling (or preference modeling, PM), which encompasses both the fully supervised and the latent-variable conditional models as special cases. [sent-47, score-1.229]
</p><p>16 The framework allows arbitrary declarative structured preference knowledge to be introduced to guide the learning procedure in a primarily unsupervised setting. [sent-48, score-0.915]
</p><p>17 We present our semi-Markov model and discuss our preference modeling framework in Section 2 and 3 respectively. [sent-49, score-0.558]
</p><p>18 Finally, we demonstrate through experiments that structured preference information is crucial to model and present empirical results on a standard dataset in Section 5. [sent-51, score-0.597]
</p><p>19 IMn a ksouvpe CrvRiFse,d u nsedtetirn ag ,s poneclyif icco srreegmct arguments are Cobserved but their associated correct mention types are hidden (shaded). [sent-59, score-0.386]
</p><p>20 This motivates us to build a joint model for extracting the event structures from the text. [sent-61, score-0.413]
</p><p>21 Cn refer to a particular segmentation of the event span, where C1, C3 . [sent-66, score-0.386]
</p><p>22 correspond to in-between mention word sequences (we call them gaps) (e. [sent-74, score-0.258]
</p><p>23 refer to event arguments that carry specific roles (e. [sent-86, score-0.447]
</p><p>24 The event span is split into segments, where each segment is either linked to a mention type (Ti; these segments can be referred to as “argument segments”), or directly linked to an inter-argument gap (Bj ; they can be referred to as “gap segments”). [sent-93, score-1.126]
</p><p>25 In the figure, for example, the segments C1 and C3 are identified as two argument segments (which are mentions of types T1 and T3 respectively) and are  mapped to two “nodes”, and the segment C2 is identified as a gap segment that connects the two arguments A1 and A3. [sent-95, score-0.969]
</p><p>26 We use s to denote an event span and t to denote a specific realization (filling) of the event template. [sent-97, score-0.899]
</p><p>27 Denote by h a particular mention boundary and type assignment for an event span, which gives us a specific segmentation of the given span. [sent-99, score-0.733]
</p><p>28 Following the conditional 1Extending the model to support certain argument overlapping is possible – we leave it for future work. [sent-100, score-0.299]
</p><p>29 , 2001), we parameterize the conditional probability of the (t, h) pair given an event span s as follows:  PΘ(t,h|s) =Pet,hf(se,fh(s,t,)h·Θ,t)·Θ  (1)  where f gives the featuPre functions defined on the tuple (s, h, t), and Θ defines the parameter vector. [sent-102, score-0.679]
</p><p>30 Our objective function is the logarithm of the joint conditional probability of observing the template realization for the observed event span s: L(Θ)  =  XlogPΘ(ti|si) Xi  =  XilogPPth,heef(f(sis,ih,h,t,it) · ΘΘ  (2)  This function is not convePx due to the summation over the hidden variable h. [sent-103, score-0.988]
</p><p>31 Inference involves computing the most probable template realization t for a given event span:  argtmaxPΘ(t|s) = argtmaxXhPΘ(t,h|s)  (4)  where the possible hidden assignments h need to be marginalized out. [sent-111, score-0.523]
</p><p>32 In this task, a particular realization t already uniquely defines a particular segmentation (mention boundaries) of the event span, thus the h only contributes type information to t. [sent-112, score-0.532]
</p><p>33 Since one primary assumption is that we have access to the output of existing mention identification and typing systems, the set ofall possible mentions defines a lattice representation containing the set of all possible segmentations that comply with such mention-level information. [sent-118, score-0.554]
</p><p>34 Assuming there are A possible arguments for the event and K annotated mentions, the complexity of the forwardbackward style algorithm is in O(A3K2) under the “second-order” setting that we will discuss in Sec-  tion 2. [sent-119, score-0.49]
</p><p>35 Our model will need to disambiguate the mention boundaries as well as their types. [sent-126, score-0.309]
</p><p>36 Since we are only interested in modeling dependencies between adjacent argument segments, we assign hard labels to each gap segment based on its contextual argument information. [sent-130, score-0.629]
</p><p>37 Specifically, the label of each gap segment  ×  is uniquely determined by its surrounding argument segments with a list representation. [sent-131, score-0.522]
</p><p>38 For example, in a “first-order” setting, the gap segment that appears between its previous argument segment “ATTACKER” and its next argument segment “INSTRUMENT” is annotated as the list consisting of two elements: [ATTACKER, INSTRUMENT]. [sent-132, score-0.735]
</p><p>39 To capture longer-range dependencies, in this work we use a “second-order” setting (as shown in Figure 2), 2The length of a gap segment is arbitrary (including zero), unlike the seminal semi-Markov CRF model of Sarawagi and Cohen (2004). [sent-133, score-0.258]
</p><p>40 which means each gap segment is annotated with a list that consists of its previous two argument segments as well as its subsequent one. [sent-134, score-0.522]
</p><p>41 Indicator function for the combination of its immediate two left arguments and its immediate right argument. [sent-139, score-0.281]
</p><p>42 For argument segments, we also define the same input feature templates as above, with the following additional ones to capture contextual information: CWORDS : CPOS :  Indicator function for the previous and next k (= 1, 2, 3) words. [sent-140, score-0.321]
</p><p>43 and we define the following output feature template: ARGTYPE:  Indicator function for the combination of the argument and its associated type. [sent-142, score-0.284]
</p><p>44 We introduce a novel general learning framework called structured preference modeling, which allows arbitrary prior knowledge about structures to be introduced to the learning process in a declarative manner. [sent-150, score-0.826]
</p><p>45 he following objective function:  Lu(Θ) =XilogPypPΘ(xyip,Θy)(x ×i, κy()xi,y)  (5)  Intuitively, optimizing suPch an objective function is equivalent to pushing the probability mass from bad structures to good structures corresponding to the same input. [sent-166, score-0.47]
</p><p>46 When the preference function κ is defined as the indicator function for the correct structure (xi, yi),  ×  the numerator terms of the above formula are simply of the forms pΘ (xi, yi), and the model corresponds to the fully supervised CRF model. [sent-167, score-0.842]
</p><p>47 The preference function κ serves as a source from which certain prior knowledge about the structure can be injected into our model in a principled way. [sent-176, score-0.618]
</p><p>48 This allows us to incorporate both local and arbitrary global structured information into the preference function. [sent-178, score-0.65]
</p><p>49 The preference function κ is defined at the complete structure level. [sent-182, score-0.616]
</p><p>50 In this work, we exploit a specific form of the preference function κ. [sent-185, score-0.522]
</p><p>51 We show some actual κp functions usedP Pfor a particular event in Section 5. [sent-191, score-0.403]
</p><p>52 3 Event Extraction Now we can obtain the objective function for our event extraction task. [sent-195, score-0.571]
</p><p>53 Constraints  Note that the objective function in Equation 5, if written in the additive form, leads to a cost function reminiscent of the one used in constraint-driven learning algorithm (CoDL) (Chang et al. [sent-200, score-0.273]
</p><p>54 Specifically, in CoDL, the following cost function is involved in its EM-like inference procedure:  argymaxΘ · f(x,y) − ρXcd(y,Yc)  (14)  where Yc defines the set of y’s that all satisfy a certwaihne rceon Ystraint c, and d defines a distance function from y to that set. [sent-203, score-0.282]
</p><p>55 There are some important distinctions between structured preference modeling (PM) and CoDL. [sent-205, score-0.657]
</p><p>56 Constraints are typically useful when one works on structured prediction problems for data with certain (often rigid) regularities, such as citations, advertisements, or POS tagging for complete sentences. [sent-208, score-0.289]
</p><p>57 For example, there is no guarantee that a certain argument will always be present in the event span, nor should a particular mention, if appeared, always be selected and assigned to a specific argument. [sent-217, score-0.591]
</p><p>58 For example, in the example event span given 840 in Section 1, both “March” and “Tuesday” are valid candidate mentions for the TIME-WITHIN argument given their annotated type TME. [sent-218, score-0.844]
</p><p>59 In this work, our preference function is related to another function that can be decomposed into a collection of property functions κp. [sent-222, score-0.675]
</p><p>60 This formulation gives us a complete flexibility to assign arbitrary structured preferences, where positive scores can be assigned to good properties, and negative scores to bad ones. [sent-224, score-0.398]
</p><p>61 To summarize, preferences are an effective way to “define” the event structure to the learner, which is essential in an unsupervised setting, which may not be easy to do with other forms of constraints. [sent-226, score-0.584]
</p><p>62 To present general results while making minimal assumptions, our primary event extraction results 3http://www. [sent-232, score-0.4]
</p><p>63 1 6 1945  Table 1: Performance for different events under different experimental settings, with gold mention boundaries and types. [sent-244, score-0.356]
</p><p>64 ) are independent of mention identification and typing modules, which are based on the gold mention information as given by the dataset. [sent-247, score-0.639]
</p><p>65 Additionally, we present results obtained by exploiting our in-house automatic mention identification and typing module, which is a hybrid system that combines statistical and rule-based approaches. [sent-248, score-0.381]
</p><p>66 In these approaches, we treat each argument of the template as one possi-  ble output class, plus a special “NONE” class for not selecting it as an argument. [sent-255, score-0.257]
</p><p>67 We train and apply the classifiers on argument segments (i. [sent-256, score-0.317]
</p><p>68 In the simplest baseline approach MaxEnt-b, type information for each mention is simply treated as one special feature. [sent-260, score-0.302]
</p><p>69 To assess the importance of structured preference, we also perform experiments where structured preference information is incorporated at the inference time of the MaxEnt classifiers. [sent-265, score-0.774]
</p><p>70 Next, we re-rank this list based on scores from our structured preference functions (we used  the same preferences as to be discussed in the next section). [sent-267, score-0.772]
</p><p>71 Note that no structured preference information is used when training and evaluating our semi-CRF model. [sent-270, score-0.597]
</p><p>72 This clearly indicates that structured preference information is crucial to model. [sent-273, score-0.597]
</p><p>73 We first build our simplest baseline by randomly assigning arguments to each mention with mention type information serving as constraints. [sent-276, score-0.655]
</p><p>74 Figure 3: The complete list of preference patterns used for the “Die” and “Transport” event. [sent-279, score-0.517]
</p><p>75 However, to demonstrate its general effectiveness, in this work we only choose a minimal amount of general preference patterns for evaluations. [sent-293, score-0.462]
</p><p>76 We make our preference patterns as general as possible. [sent-294, score-0.462]
</p><p>77 As shown in the last column (#P) of Table 2, we use only 7 preference patterns each for the “Attack” and “Meet” events, and 6 patterns each for the other two events. [sent-295, score-0.545]
</p><p>78 In Figure 3, we show the complete list of the 6 preference patterns for the “Die” and “Transport” event used for our experiments. [sent-296, score-0.869]
</p><p>79 On the other hand, a completely unsupervised approach where structured preferences are not specified, performs substantially worse. [sent-302, score-0.37]
</p><p>80 To run such completely unsupervised models, we essentially follow the same training procedure as that of the preference modeling, except that structured preference information is not in place when generating the n-best list. [sent-303, score-1.086]
</p><p>81 As a result, the 842 unsupervised model without preference information can even perform worse than the random baseline 4. [sent-307, score-0.535]
</p><p>82 Such an approach performs worse than our approach with preference modeling. [sent-311, score-0.42]
</p><p>83 However, we also note that the performance of preference modeling depends on the actual quality and amount of preferences used for learning. [sent-314, score-0.604]
</p><p>84 In the extreme case, where only few preferences are used, the performance of preference modeling will be close to that of the unsupervised approach, while the rulebased approach will yield performance close to that of the random baseline. [sent-315, score-0.719]
</p><p>85 The results with automatically predicted mention boundaries and types are given in Table 3. [sent-316, score-0.309]
</p><p>86 Similar observations can be made when comparing the performance of preference modeling with other approaches. [sent-317, score-0.48]
</p><p>87 This set of results further confirms the effectiveness of our approach using preference modeling for the event extraction task. [sent-318, score-0.88]
</p><p>88 93152487 Table 3: Event extraction performance with automatic mention identifier and typer. [sent-329, score-0.306]
</p><p>89 We report F1 percentage scores for preference modeling (PM) as well as two baseline approaches. [sent-330, score-0.48]
</p><p>90 Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. [sent-342, score-0.362]
</p><p>91 , 2010) is another recently proposed framework for unsupervised structured prediction. [sent-352, score-0.281]
</p><p>92 Empirically the model is effective in various unsupervised structured prediction tasks, and outperforms the globally normalized model. [sent-354, score-0.246]
</p><p>93 Although modeling the semi-Markov properties of our segments (especially the gap segments) in our task is potentially challenging, we plan to investigate in the future the feasibility for our task with such a framework. [sent-355, score-0.317]
</p><p>94 7  Conclusions  In this paper, we present a novel model based on the semi-Markov conditional random fields for the challenging event extraction task. [sent-356, score-0.564]
</p><p>95 The model takes in coarse mention boundary and type information  and predicts complete structures indicating the corresponding argument role for each mention. [sent-357, score-0.662]
</p><p>96 To learn the model in an unsupervised manner, we further develop a novel learning approach called structured preference modeling that allows structured knowledge to be incorporated effectively in a declarative manner. [sent-358, score-0.983]
</p><p>97 Empirically, we show that knowledge about structured preference is crucial to model and the preference modeling is an effective way to guide learning in this setting. [sent-359, score-1.077]
</p><p>98 Trained in a primarily unsupervised manner, our model incorporating structured preference information exhibits performance that is competitive to that of some supervised baseline approaches. [sent-360, score-0.804]
</p><p>99 Our event extraction system and code will be available for download from our group web page. [sent-361, score-0.4]
</p><p>100 Acknowledgments We would like to thank Yee Seng Chan, Mark Sammons, and Quang Xuan Do for their help with the mention identification and typing system used in this paper. [sent-362, score-0.381]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('preference', 0.42), ('event', 0.352), ('mention', 0.258), ('argument', 0.182), ('structured', 0.177), ('codl', 0.16), ('segments', 0.135), ('mentions', 0.134), ('span', 0.132), ('xi', 0.129), ('preferences', 0.124), ('gap', 0.122), ('function', 0.102), ('arguments', 0.095), ('pm', 0.092), ('crf', 0.089), ('typing', 0.084), ('segment', 0.083), ('indicator', 0.082), ('primarily', 0.081), ('declarative', 0.08), ('template', 0.075), ('ratinov', 0.073), ('military', 0.073), ('die', 0.073), ('unsupervised', 0.069), ('equation', 0.069), ('objective', 0.069), ('attacker', 0.069), ('xep', 0.069), ('xiep', 0.069), ('bad', 0.068), ('korea', 0.065), ('si', 0.065), ('realization', 0.063), ('yi', 0.062), ('coarse', 0.062), ('structures', 0.061), ('attack', 0.061), ('conditional', 0.06), ('modeling', 0.06), ('constraints', 0.059), ('fields', 0.058), ('supervised', 0.057), ('certain', 0.057), ('roth', 0.056), ('locally', 0.056), ('chang', 0.056), ('complete', 0.055), ('arbitrary', 0.053), ('fk', 0.052), ('posterior', 0.051), ('boundaries', 0.051), ('functions', 0.051), ('transport', 0.051), ('ef', 0.051), ('sarawagi', 0.048), ('extraction', 0.048), ('events', 0.047), ('random', 0.046), ('neighborhood', 0.046), ('ganchev', 0.046), ('byy', 0.046), ('decomposable', 0.046), ('fired', 0.046), ('imn', 0.046), ('laser', 0.046), ('lecun', 0.046), ('samdani', 0.046), ('tgen', 0.046), ('gives', 0.045), ('contrastive', 0.044), ('type', 0.044), ('discuss', 0.043), ('patterns', 0.042), ('immediate', 0.042), ('column', 0.041), ('gpe', 0.04), ('numerator', 0.04), ('pushing', 0.04), ('defines', 0.039), ('structure', 0.039), ('fj', 0.039), ('smith', 0.039), ('identification', 0.039), ('templates', 0.037), ('guiding', 0.036), ('goldwasser', 0.036), ('instrument', 0.036), ('str', 0.036), ('unless', 0.036), ('framework', 0.035), ('okanohara', 0.034), ('semimarkov', 0.034), ('derivatives', 0.034), ('march', 0.034), ('regularities', 0.034), ('segmentation', 0.034), ('eisner', 0.033), ('hidden', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="33-tfidf-1" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>2 0.29233903 <a title="33-tfidf-2" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>Author: Joel Nothman ; Matthew Honnibal ; Ben Hachey ; James R. Curran</p><p>Abstract: Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement.</p><p>3 0.18573263 <a title="33-tfidf-3" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>4 0.1795374 <a title="33-tfidf-4" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>5 0.15240978 <a title="33-tfidf-5" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>Author: Einat Minkov ; Luke Zettlemoyer</p><p>Abstract: This paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention de- tection and template filling tasks.</p><p>6 0.14314452 <a title="33-tfidf-6" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>7 0.13005283 <a title="33-tfidf-7" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>8 0.12341409 <a title="33-tfidf-8" href="./acl-2012-A_Novel_Burst-based_Text_Representation_Model_for_Scalable_Event_Detection.html">17 acl-2012-A Novel Burst-based Text Representation Model for Scalable Event Detection</a></p>
<p>9 0.12078315 <a title="33-tfidf-9" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>10 0.12036063 <a title="33-tfidf-10" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>11 0.11016322 <a title="33-tfidf-11" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>12 0.099527277 <a title="33-tfidf-12" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>13 0.098916605 <a title="33-tfidf-13" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>14 0.0945848 <a title="33-tfidf-14" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>15 0.091094881 <a title="33-tfidf-15" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>16 0.090184063 <a title="33-tfidf-16" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>17 0.088245012 <a title="33-tfidf-17" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>18 0.086897038 <a title="33-tfidf-18" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>19 0.086320177 <a title="33-tfidf-19" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>20 0.085987903 <a title="33-tfidf-20" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.273), (1, 0.125), (2, -0.111), (3, 0.161), (4, 0.013), (5, -0.004), (6, -0.003), (7, 0.006), (8, 0.025), (9, -0.038), (10, -0.105), (11, -0.152), (12, -0.049), (13, -0.111), (14, -0.16), (15, 0.099), (16, 0.112), (17, 0.11), (18, -0.15), (19, 0.058), (20, -0.042), (21, 0.123), (22, -0.007), (23, -0.0), (24, -0.108), (25, -0.054), (26, -0.126), (27, 0.042), (28, -0.102), (29, -0.004), (30, 0.13), (31, -0.074), (32, 0.162), (33, -0.056), (34, -0.042), (35, 0.049), (36, -0.11), (37, -0.043), (38, -0.091), (39, -0.102), (40, -0.011), (41, -0.096), (42, -0.024), (43, -0.055), (44, 0.034), (45, 0.053), (46, 0.057), (47, 0.073), (48, 0.146), (49, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97125095 <a title="33-lsi-1" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>2 0.76397479 <a title="33-lsi-2" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>Author: Joel Nothman ; Matthew Honnibal ; Ben Hachey ; James R. Curran</p><p>Abstract: Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement.</p><p>3 0.56425524 <a title="33-lsi-3" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>Author: Einat Minkov ; Luke Zettlemoyer</p><p>Abstract: This paper presents a joint model for template filling, where the goal is to automatically specify the fields of target relations such as seminar announcements or corporate acquisition events. The approach models mention detection, unification and field extraction in a flexible, feature-rich model that allows for joint modeling of interdependencies at all levels and across fields. Such an approach can, for example, learn likely event durations and the fact that start times should come before end times. While the joint inference space is large, we demonstrate effective learning with a Perceptron-style approach that uses simple, greedy beam decoding. Empirical results in two benchmark domains demonstrate consistently strong performance on both mention de- tection and template filling tasks.</p><p>4 0.53012478 <a title="33-lsi-4" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>Author: Dani Yogatama ; Yanchuan Sim ; Noah A. Smith</p><p>Abstract: We present a statistical model for canonicalizing named entity mentions into a table whose rows represent entities and whose columns are attributes (or parts of attributes). The model is novel in that it incorporates entity context, surface features, firstorder dependencies among attribute-parts, and a notion of noise. Transductive learning from a few seeds and a collection of mention tokens combines Bayesian inference and conditional estimation. We evaluate our model and its components on two datasets collected from political blogs and sports news, finding that it outperforms a simple agglomerative clustering approach and previous work.</p><p>5 0.49409366 <a title="33-lsi-5" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<p>Author: Katja Markert ; Yufang Hou ; Michael Strube</p><p>Abstract: Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classification and focuses on conversational dialogue. We here introduce the task of classifying finegrained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status ofall mentions. Our approach strongly outperforms reimplementations of previous work.</p><p>6 0.4899655 <a title="33-lsi-6" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>7 0.4785741 <a title="33-lsi-7" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>8 0.46670979 <a title="33-lsi-8" href="./acl-2012-A_Novel_Burst-based_Text_Representation_Model_for_Scalable_Event_Detection.html">17 acl-2012-A Novel Burst-based Text Representation Model for Scalable Event Detection</a></p>
<p>9 0.45365462 <a title="33-lsi-9" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>10 0.43414986 <a title="33-lsi-10" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>11 0.42911071 <a title="33-lsi-11" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>12 0.42822921 <a title="33-lsi-12" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>13 0.4146761 <a title="33-lsi-13" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>14 0.3923701 <a title="33-lsi-14" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>15 0.38674581 <a title="33-lsi-15" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>16 0.38142902 <a title="33-lsi-16" href="./acl-2012-Coreference_Semantics_from_Web_Features.html">58 acl-2012-Coreference Semantics from Web Features</a></p>
<p>17 0.37902886 <a title="33-lsi-17" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>18 0.36851889 <a title="33-lsi-18" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>19 0.36647299 <a title="33-lsi-19" href="./acl-2012-The_Creation_of_a_Corpus_of_English_Metalanguage.html">195 acl-2012-The Creation of a Corpus of English Metalanguage</a></p>
<p>20 0.36340022 <a title="33-lsi-20" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(26, 0.031), (28, 0.043), (30, 0.014), (37, 0.041), (39, 0.03), (74, 0.021), (82, 0.024), (84, 0.019), (85, 0.016), (90, 0.579), (92, 0.036), (94, 0.02), (99, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99936473 <a title="33-lda-1" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>Author: Wei Lu ; Dan Roth</p><p>Abstract: This paper presents a novel sequence labeling model based on the latent-variable semiMarkov conditional random fields for jointly extracting argument roles of events from texts. The model takes in coarse mention and type information and predicts argument roles for a given event template. This paper addresses the event extraction problem in a primarily unsupervised setting, where no labeled training instances are available. Our key contribution is a novel learning framework called structured preference modeling (PM), that allows arbitrary preference to be assigned to certain structures during the learning procedure. We establish and discuss connections between this framework and other existing works. We show empirically that the structured preferences are crucial to the success of our task. Our model, trained without annotated data and with a small number of structured preferences, yields performance competitive to some baseline supervised approaches.</p><p>2 0.99698466 <a title="33-lda-2" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; George Foster ; Baskaran Sankaran ; Anoop Sarkar</p><p>Abstract: Statistical machine translation is often faced with the problem of combining training data from many diverse sources into a single translation model which then has to translate sentences in a new domain. We propose a novel approach, ensemble decoding, which combines a number of translation systems dynamically at the decoding step. In this paper, we evaluate performance on a domain adaptation setting where we translate sentences from the medical domain. Our experimental results show that ensemble decoding outperforms various strong baselines including mixture models, the current state-of-the-art for domain adaptation in machine translation.</p><p>3 0.99647981 <a title="33-lda-3" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: Online forums are becoming a popular resource in the state of the art question answering (QA) systems. Because of its nature as an online community, it contains more updated knowledge than other places. However, going through tedious and redundant posts to look for answers could be very time consuming. Most prior work focused on extracting only question answering sentences from user conversations. In this paper, we introduce the task of sentence dependency tagging. Finding dependency structure can not only help find answer quickly but also allow users to trace back how the answer is concluded through user conversations. We use linear-chain conditional random fields (CRF) for sentence type tagging, and a 2D CRF to label the dependency relation between sentences. Our experimental results show that our proposed approach performs well for sentence dependency tagging. This dependency information can benefit other tasks such as thread ranking and answer summarization in online forums.</p><p>4 0.99612427 <a title="33-lda-4" href="./acl-2012-Using_Search-Logs_to_Improve_Query_Tagging.html">212 acl-2012-Using Search-Logs to Improve Query Tagging</a></p>
<p>Author: Kuzman Ganchev ; Keith Hall ; Ryan McDonald ; Slav Petrov</p><p>Abstract: Syntactic analysis of search queries is important for a variety of information-retrieval tasks; however, the lack of annotated data makes training query analysis models difficult. We propose a simple, efficient procedure in which part-of-speech tags are transferred from retrieval-result snippets to queries at training time. Unlike previous work, our final model does not require any additional resources at run-time. Compared to a state-ofthe-art approach, we achieve more than 20% relative error reduction. Additionally, we annotate a corpus of search queries with partof-speech tags, providing a resource for future work on syntactic query analysis.</p><p>5 0.99513942 <a title="33-lda-5" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Xiao Jiang</p><p>Abstract: Social media language contains huge amount and wide variety of nonstandard tokens, created both intentionally and unintentionally by the users. It is of crucial importance to normalize the noisy nonstandard tokens before applying other NLP techniques. A major challenge facing this task is the system coverage, i.e., for any user-created nonstandard term, the system should be able to restore the correct word within its top n output candidates. In this paper, we propose a cognitivelydriven normalization system that integrates different human perspectives in normalizing the nonstandard tokens, including the enhanced letter transformation, visual priming, and string/phonetic similarity. The system was evaluated on both word- and messagelevel using four SMS and Twitter data sets. Results show that our system achieves over 90% word-coverage across all data sets (a . 10% absolute increase compared to state-ofthe-art); the broad word-coverage can also successfully translate into message-level performance gain, yielding 6% absolute increase compared to the best prior approach.</p><p>6 0.9746049 <a title="33-lda-6" href="./acl-2012-A_Two-step_Approach_to_Sentence_Compression_of_Spoken_Utterances.html">23 acl-2012-A Two-step Approach to Sentence Compression of Spoken Utterances</a></p>
<p>7 0.97094077 <a title="33-lda-7" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>8 0.96672529 <a title="33-lda-8" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>9 0.96640933 <a title="33-lda-9" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>10 0.95989895 <a title="33-lda-10" href="./acl-2012-Community_Answer_Summarization_for_Multi-Sentence_Question_with_Group_L1_Regularization.html">55 acl-2012-Community Answer Summarization for Multi-Sentence Question with Group L1 Regularization</a></p>
<p>11 0.95969409 <a title="33-lda-11" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>12 0.9584052 <a title="33-lda-12" href="./acl-2012-Selective_Sharing_for_Multilingual_Dependency_Parsing.html">172 acl-2012-Selective Sharing for Multilingual Dependency Parsing</a></p>
<p>13 0.95736188 <a title="33-lda-13" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>14 0.95732194 <a title="33-lda-14" href="./acl-2012-Multilingual_Named_Entity_Recognition_using_Parallel_Data_and_Metadata_from_Wikipedia.html">150 acl-2012-Multilingual Named Entity Recognition using Parallel Data and Metadata from Wikipedia</a></p>
<p>15 0.9553324 <a title="33-lda-15" href="./acl-2012-Exploring_Deterministic_Constraints%3A_from_a_Constrained_English_POS_Tagger_to_an_Efficient_ILP_Solution_to_Chinese_Word_Segmentation.html">89 acl-2012-Exploring Deterministic Constraints: from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation</a></p>
<p>16 0.94453287 <a title="33-lda-16" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>17 0.94296867 <a title="33-lda-17" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>18 0.94290316 <a title="33-lda-18" href="./acl-2012-A_Statistical_Model_for_Unsupervised_and_Semi-supervised_Transliteration_Mining.html">20 acl-2012-A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining</a></p>
<p>19 0.94244915 <a title="33-lda-19" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>20 0.93826252 <a title="33-lda-20" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
