<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-90" href="#">acl2012-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</h1>
<br/><p>Source: <a title="acl-2012-90-pdf" href="http://aclweb.org/anthology//P/P12/P12-1010.pdf">pdf</a></p><p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions.</p><p>Reference: <a title="acl-2012-90-reference" href="../acl2012_reference/acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 be Abstract We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. [sent-4, score-1.351]
</p><p>2 We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0. [sent-5, score-0.696]
</p><p>3 We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0. [sent-9, score-1.421]
</p><p>4 Our analysis of the dependency parser errors gives some insights into future research directions. [sent-12, score-0.398]
</p><p>5 1 Introduction There has been much recent interest in identifying events, times and their relations along the timeline, from event and time ordering problems in the TempEval shared tasks (Verhagen et al. [sent-13, score-0.344]
</p><p>6 , 2010), to identifying time arguments of event structures in the Automated Content Extraction program (Linguistic Data Consortium, 2005; Gupta and Ji, 2009), to timestamping event intervals in the Knowledge Base Population shared task (Artiles et al. [sent-15, score-0.389]
</p><p>7 However, to date, this research has produced fragmented document timelines, because only specific types of temporal relations in specific contexts have  88 Steven Bethard University of Colorado Campus Box 594 Boulder, CO 80309, USA St even . [sent-18, score-0.562]
</p><p>8 For example, the TempEval tasks only looked at relations between events in the same or adjacent sentences (Verhagen et al. [sent-24, score-0.383]
</p><p>9 In this article, we propose an approach to temporal information extraction that identifies a single connected timeline for a text. [sent-27, score-0.631]
</p><p>10 We construct an evaluation corpus by annotating such  temporal dependency trees over a set of children’s stories. [sent-29, score-0.678]
</p><p>11 We then demonstrate how to train a timeline extraction system based on dependency parsing techniques instead of the pair-wise classification approaches typical of prior work. [sent-30, score-0.423]
</p><p>12 The main contributions of this article are: •  •  •  We propose a new approach to characterizing temporal structure via dependency trees. [sent-31, score-0.636]
</p><p>13 We produce an annotated corpus of temporal dependency trees in children’s stories. [sent-32, score-0.676]
</p><p>14 We design a non-projective dependency parser for inferring timelines from text. [sent-33, score-0.502]
</p><p>15 c s 2o0c1ia2ti Aosns fo cria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsteiscs 8 –97, 2  Related Work  Much prior work on the annotation of temporal information has constructed corpora with incomplete  timelines. [sent-37, score-0.514]
</p><p>16 , 2003a) provided a corpus annotated for all events and times, but temporal relations were only annotated when the relation was judged to be salient by the annotator. [sent-40, score-0.957]
</p><p>17 , 2010), annotated texts were provided for a few different event and time configurations, for example, an event and a time in the same sentence, or two main-clause events from adjacent sentences. [sent-43, score-0.685]
</p><p>18 (2007) proposed to annotate temporal relations one syntactic construction at a time, producing an initial corpus of only verbal events linked to events in subordinated clauses. [sent-45, score-1.137]
</p><p>19 (2006) where temporal structures were annotated as directed acyclic graphs. [sent-47, score-0.549]
</p><p>20 In part because of the structure of the available training corpora, most existing temporal information extraction models formulate temporal linking as a pair-wise classification task, where each pair  of events and/or times is examined and classified as having a temporal relation or not. [sent-49, score-1.731]
</p><p>21 Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. [sent-50, score-0.342]
</p><p>22 Most of the topperforming systems in the TempEval competitions also took this pair-wise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al. [sent-51, score-0.562]
</p><p>23 Systems have also tried to take advantage of more global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks such as integer linear programming and Markov logic networks (Bramsen et al. [sent-54, score-0.472]
</p><p>24 Yet the basic approach is still centered around pair-wise classifications, not the complete temporal structure of a document. [sent-57, score-0.472]
</p><p>25 Our work builds upon this prior research, both 89 improving the annotation approach to generate the fully connected timeline of a story, and improving  the models for timeline extraction using dependency parsing techniques. [sent-58, score-0.624]
</p><p>26 (2012), which proposes to annotate temporal relations as dependency links between head events and dependent events. [sent-61, score-1.185]
</p><p>27 These connected timelines allow us to design new models for timeline extraction in which we jointly infer the temporal structure of the text and the labeled temporal relations. [sent-63, score-1.294]
</p><p>28 We employ methods from syntactic dependency parsing, adapting them to our task by including features typical of temporal relation labeling models. [sent-64, score-0.699]
</p><p>29 In this section we illustrate the main annotation principles for coherent temporal annotation. [sent-67, score-0.514]
</p><p>30 txt ] Figure 1 shows the temporal dependency structure that we expect our annotators to identify in this story. [sent-74, score-0.69]
</p><p>31 The annotators were provided with guidelines both for which kinds of words should be identified as events, and for which kinds of events should be linked by temporal relations. [sent-75, score-0.887]
</p><p>32 For identifying event words, the standard TimeML guidelines for annotating events (Pustejovsky et al. [sent-76, score-0.508]
</p><p>33 Edges denote temporal relations signaled by linguistic cues in the text. [sent-83, score-0.562]
</p><p>34 in used to snap the event should be snap, in kept perfectly still the event should be still). [sent-90, score-0.391]
</p><p>35 the ordering relations between event words), the annotators were instructed to link each event in the story to a single nearby event, similar to what has been observed in reading comprehension studies (JohnsonLaird, 1980; Brewer and Lichtenstein, 1982). [sent-93, score-0.639]
</p><p>36 When there were several reasonable nearby events to choose from, the annotators were instructed to choose the temporal relation that was easiest to infer from the  text (e. [sent-94, score-0.841]
</p><p>37 A set of six temporal relations was used: BEFORE, AFTER, INCLUDES, IS-INCLUDED, IDENTITY or OVERLAP. [sent-97, score-0.562]
</p><p>38 Two annotators annotated temporal dependency structures in the first 100 fables of the McIntyreLapata collection and measured inter-annotator agreement by Krippendorff’s Alpha for nominal data (Krippendorff, 2004; Hayes and Krippendorff, 2007). [sent-98, score-0.884]
</p><p>39 Thus, we concluded that the temporal dependency annotation paradigm was reliable, and the resulting corpus of 100 fables2 could be used to 2Available from http : / /www . [sent-103, score-0.678]
</p><p>40 4  Parsing Models  We consider two different approaches to learning a temporal dependency parser: a shift-reduce model  (Nivre, 2008) and a graph-based model (McDonald et al. [sent-107, score-0.636]
</p><p>41 Both models take as input a sequence of event words and produce as output a tree structure where the events are linked via temporal relations. [sent-109, score-1.097]
</p><p>42 wn is a sequence Wof →eve Πnt) words, and π ∈ Π is a dependency tree π = (V, E) where: • V = W ∪ {Root}, that is, the vertex set of the graph Wis t∪he { Rseot ootf} words in W plus an artificial root node. [sent-113, score-0.353]
</p><p>43 • E = {(wh, r, wd) : wh ∈ V, wd ∈ V, r ∈ R = {BEFORE, AFTER, INCLUDES, IS INCLUDED, IDENTITY, OVERLAP}}, that is, in the edge set of the graph, each edge ,i tsh a tli ins,k bne tthwee eedng a d seetpendent word and its head word, labeled with a temporal relation. [sent-114, score-0.974]
</p><p>44 ], E) Move the head of L1 to the head of L2 ([a1 . [sent-139, score-0.31]
</p><p>45 bj], Q, E) Create a relation where the head of L1 depends on the head of Q Not applicable if ai+1 is the root or already has a head, or if there is a path connecting wk and ai+1 ([a1 . [sent-151, score-0.58]
</p><p>46 ], E ∪ (wk, r, ai+1) Create a relation where the head of Q depends on the head of L1 Not applicable if wk is the root or already has a head, or if there is a path connecting wk and ai+1 ([a1 . [sent-169, score-0.724]
</p><p>47 1 Shift-Reduce Parsing Model Shift-reduce dependency parsers start with an input queue of unlinked words, and link them into a tree by repeatedly choosing and performing actions like shifting a node to a stack, or popping two nodes from the stack and linking them. [sent-189, score-0.38]
</p><p>48 For temporal dependency parsing, we adopt the Covington set of transitions (Covington, 2001) as it allows for parsing the non-projective trees, which may also contain “crossing” edges, that occasionally occur in our annotated corpus. [sent-192, score-0.776]
</p><p>49 c ← INIT(W) cw ←hile I c ∈/ CF do hti  ←  o(c)  c  ←  t(c)  endc ←wh til(ec return TREE(c)  •  •  •  •  c = (L1, L2, Q, E) is a parser configuration, cwh =ere ( L1 and L2 are lists for temporary storage, Q is the queue of input words, and E is the set of identified edges of the dependency tree. [sent-194, score-0.482]
</p><p>50 For temporal dependency parsing, we learn a Support Vector Machine classifier (Yamada and Matsumoto, 2003) using the features described in Section 5. [sent-203, score-0.636]
</p><p>51 2 Graph-Based Parsing Model One shortcoming of the shift-reduce dependency parsing approach is that each transition decision  Figure 2: A setting for the graph-based parsing model: an initial dense graph G (left) with edge scores SCORE(e). [sent-205, score-0.483]
</p><p>52 The resulting dependency tree as a spanning tree with the highest score over the edges (right). [sent-206, score-0.642]
</p><p>53 Graph-based models are an alternative dependency parsing model, which assembles a graph with weighted edges between all pairs of words, and selects the tree-shaped subset of this graph that gives the highest total score (Fig. [sent-208, score-0.38]
</p><p>54 Formally, a graph-based parser follows  Algorithm 2, where: •  •  •  ×  W0 = W ∪ {Root}  SCORE ∈ ((W0 R fSor scoring edges  W)  →  <) is a function  SPANNINGTREE is a function for selecting a sSubset of edges that is a tree that spans over all the nodes of the graph. [sent-210, score-0.467]
</p><p>55 Algorithm 2 Graph-based dependency parsing E ← {(e,SCORE(e)) : e ∈ (W0×R×W))} GE ← (W0, E) rGet ←urn ( SPANNINGTREE(G)  The SPANNINGTREE function is usually defined using one of the efficient search techniques for finding a maximum spanning tree. [sent-211, score-0.374]
</p><p>56 For temporal dependency parsing, we use the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) which solves this problem by iteratively selecting the edge with the highest weight and removing edges that would create cycles. [sent-212, score-0.788]
</p><p>57 For temporal dependency parsing, we learn a model to predict edge scores via the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al. [sent-215, score-0.71]
</p><p>58 The shift-reduce parser (SRP) trains a machine learning classifier as the oracle o ∈ (C → T) to predict a transition t from a parser configuration c = (L1, L2, Q, E), using node features such as the heads of L1, L2 and Q, and edge features from the already predicted temporal relations in E. [sent-218, score-1.17]
</p><p>59 The graph-based maximum spanning tree (MST) parser trains a machine learning model to predict SCORE(e) for an edge e = (wi, rj , wk),  using features of the nodes wi and wk. [sent-219, score-0.532]
</p><p>60 The full set of features proposed for both parsing models, derived from the state-of-the-art systems for temporal relation labeling, is presented in Table 2. [sent-220, score-0.635]
</p><p>61 The corpus contains 100 fables, a total of 14,279 tokens and a total of 1136 annotated temporal relations. [sent-223, score-0.512]
</p><p>62 Both are based on the assumption of linear temporal structures of narratives as the temporal ordering process that was evidenced by studies in human text rewriting (Hickmann, 2003). [sent-229, score-1.059]
</p><p>63 The proposed baselines are: •  •  LinearSeq: A model that assumes all events occur rinS ethqe order they are written, adding links between each pair of adjacent events, and labeling all links with the relation BEFORE. [sent-230, score-0.46]
</p><p>64 This is an approximation of prior work, where the pairs of events to classify with a temporal relation were given as an input to the system. [sent-233, score-0.787]
</p><p>65 1) and the graph-based, maximum spanning tree parser (MST; Section 4. [sent-237, score-0.421]
</p><p>66 1 Evaluation Criteria and Metrics Model performance was evaluated using standard evaluation criteria for parser evaluations: Unlabeled Attachment Score (UAS) The fraction of events whose head events were correctly predicted. [sent-240, score-0.844]
</p><p>67 This measures whether the correct pairs of events were linked, but not if they were linked by the correct relations. [sent-241, score-0.323]
</p><p>68 Labeled Attachment Score (LAS) The fraction of events whose head events were correctly predicted with the correct relations. [sent-242, score-0.659]
</p><p>69 This measures both whether the correct pairs of events were linked and whether their temporal ordering is correct. [sent-243, score-0.873]
</p><p>70 Tree Edit Distance In addition to the UAS and LAS the tree edit distance score has been recently introduced for evaluating dependency structures (Tsarfaty et al. [sent-244, score-0.491]
</p><p>71 Taking the shortest such sequence, the tree edit distance is calculated as the sum of the edit operation costs divided by the size of the tree (i. [sent-260, score-0.464]
</p><p>72 For temporal dependency trees, we assume each operation costs 1. [sent-263, score-0.636]
</p><p>73 The final score subtracts the edit distance from 1 so that a perfect tree has score 1. [sent-265, score-0.328]
</p><p>74 The labeled tree edit distance score (LTEDS) calculates sequences over the tree with all its labeled temporal relations, while the unlabeled tree edit distance score (UTEDS) treats all edges as if they had the same label. [sent-267, score-1.332]
</p><p>75 In terms of labeled attachment score, both dependency parsing models outperformed the baseline models the maximum spanning tree parser achieved 0. [sent-275, score-0.785]
</p><p>76 The shift-reduce parser also outperformed the baseline models in terms of labeled tree edit distance, achieving 0. [sent-278, score-0.435]
</p><p>77 These results indicate that dependency parsing models are a good fit to our wholestory timeline extraction task. [sent-282, score-0.423]
</p><p>78 Finally, in comparing the two different depen–  dency parsing models, we observe that the shiftreduce parser outperforms the maximum spanning  Table4:OAEVtrhaocReELdtAryisoTpPfrenu→itsbTaloyhBrpfieEnFhrfOeoRamsdEtheN251un6748mal. [sent-283, score-0.395]
</p><p>79 tree parser in terms of labeled attachment score (0. [sent-286, score-0.449]
</p><p>80 It has been argued that graphbased models like the maximum spanning tree parser should be able to produce more globally consistent and correct dependency trees, yet we do not observe that here. [sent-290, score-0.585]
</p><p>81 A likely explanation for this phenomenon is that the shift-reduce parsing model allows for features describing previous parse decisions (similar to the incremental nature of human parse decisions),  while the joint nature of the maximum spanning tree parser does not. [sent-291, score-0.521]
</p><p>82 Attach to further head: The model predicts tAhett wrong head, ran hde predicts as the head an event that is further away than the true head. [sent-294, score-0.459]
</p><p>83 Attach to nearer head: The model predicts the wrong head, aanredr predicts as the head an event that is closer than the true head. [sent-295, score-0.459]
</p><p>84 An analysis of these OVERLAP → BEFORE errors suggests that they occur in scenario→s like this one, where the duration of one event is significantly longer than the duration of another, but there are no direct cues for these duration differences. [sent-305, score-0.441]
</p><p>85 We also observe these types of errors when one event has many sub-events, and therefore the duration of the main event typically includes the durations of all the sub-events. [sent-306, score-0.512]
</p><p>86 The second most common error type of the model is the prediction of a head event that is further away than the head identified by the annotators. [sent-310, score-0.486]
</p><p>87 7  Discussion and Conclusions  In this article, we have presented an approach to temporal information extraction that represents the time95 line of a story as a temporal dependency tree. [sent-315, score-1.173]
</p><p>88 We have constructed an evaluation corpus where such temporal dependencies have been annotated over a set of 100 children’s stories. [sent-316, score-0.512]
</p><p>89 We have introduced two dependency parsing techniques for extracting story timelines and have shown that both outperform a rulebased baseline and a prior-work-inspired pair-wise classification baseline. [sent-317, score-0.482]
</p><p>90 Comparing the two dependency parsing models, we have found that a shiftreduce parser, which more closely mirrors the incremental processing of our human annotators, outperforms a graph-based maximum spanning tree parser. [sent-318, score-0.5]
</p><p>91 Our error analysis of the shift-reduce parser revealed that being able to estimate differences in event durations may play a key role in improving parse quality. [sent-319, score-0.4]
</p><p>92 We have focused on children’s stories in this study,  in part because they typically have simpler temporal structures (though not so simple that our rule-based baseline could parse them accurately). [sent-320, score-0.569]
</p><p>93 One approach might be to first group events into their narrative containers (Pustejovsky and Stubbs, 2011), for example, grouping together all events linked to the time of a patient’s examination. [sent-325, score-0.64]
</p><p>94 Then within each narrative container, our dependency parsing approach could be applied. [sent-326, score-0.329]
</p><p>95 Another approach might be to join the individual timeline trees into a document-wide tree via discourse relations or relations to the document creation time. [sent-327, score-0.465]
</p><p>96 Finding temporal structure in text: Machine learning of syntactic temporal relations. [sent-351, score-0.944]
</p><p>97 ACE (Automatic Content Extraction) English annotation guidelines for events version 5. [sent-469, score-0.332]
</p><p>98 TimeML: Robust specification of event and temporal expressions in text. [sent-519, score-0.648]
</p><p>99 TRIPS and TRIOS system for TempEval2: Extracting temporal information from text. [sent-537, score-0.472]
</p><p>100 SemEval2007 Task 15: TempEval temporal rela97  tion identification. [sent-543, score-0.472]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('temporal', 0.472), ('events', 0.252), ('bethard', 0.188), ('parser', 0.185), ('event', 0.176), ('dependency', 0.164), ('timeline', 0.159), ('head', 0.155), ('timelines', 0.153), ('wk', 0.144), ('tree', 0.126), ('verhagen', 0.125), ('fables', 0.117), ('pustejovsky', 0.115), ('spanning', 0.11), ('bj', 0.101), ('parsing', 0.1), ('wh', 0.092), ('relations', 0.09), ('edit', 0.086), ('cf', 0.084), ('ai', 0.083), ('overlap', 0.083), ('children', 0.08), ('ordering', 0.078), ('mst', 0.078), ('linearseq', 0.078), ('uzzaman', 0.078), ('edges', 0.078), ('edge', 0.074), ('duration', 0.072), ('linked', 0.071), ('wd', 0.069), ('bramsen', 0.068), ('krippendorff', 0.068), ('artiles', 0.068), ('srp', 0.068), ('tempeval', 0.065), ('narrative', 0.065), ('story', 0.065), ('predicts', 0.064), ('root', 0.063), ('crammer', 0.063), ('relation', 0.063), ('attachment', 0.062), ('stories', 0.06), ('aiai', 0.058), ('brewer', 0.058), ('llorens', 0.058), ('mcintyre', 0.058), ('spanningtree', 0.058), ('init', 0.058), ('queue', 0.055), ('annotators', 0.054), ('links', 0.052), ('hayes', 0.051), ('boguraev', 0.051), ('chu', 0.051), ('gusev', 0.051), ('errors', 0.049), ('configuration', 0.047), ('alpha', 0.047), ('tsarfaty', 0.047), ('transition', 0.045), ('chambers', 0.045), ('gupta', 0.043), ('attach', 0.043), ('heng', 0.043), ('timebank', 0.043), ('steven', 0.043), ('annotating', 0.042), ('annotation', 0.042), ('adjacent', 0.041), ('timeml', 0.041), ('yoshikawa', 0.041), ('annotated', 0.04), ('distance', 0.04), ('durations', 0.039), ('pan', 0.039), ('celestijnenlaan', 0.039), ('classifyseq', 0.039), ('heverlee', 0.039), ('kolomiyets', 0.039), ('lteds', 0.039), ('oleksandr', 0.039), ('pretended', 0.039), ('relabel', 0.039), ('saury', 0.039), ('snap', 0.039), ('travellers', 0.039), ('score', 0.038), ('james', 0.038), ('guidelines', 0.038), ('labeled', 0.038), ('las', 0.037), ('trains', 0.037), ('roser', 0.037), ('structures', 0.037), ('evaluations', 0.037), ('node', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="90-tfidf-1" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions.</p><p>2 0.42194211 <a title="90-tfidf-2" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>Author: Guillermo Garrido ; Anselmo Penas ; Bernardo Cabaleiro ; Alvaro Rodrigo</p><p>Abstract: Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported.</p><p>3 0.29658058 <a title="90-tfidf-3" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>Author: Preethi Raghavan ; Albert Lai ; Eric Fosler-Lussier</p><p>Abstract: We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen’s temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the Timebank corpus. This finding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range ofdomains to fully understand the nature of temporal ordering.</p><p>4 0.26063067 <a title="90-tfidf-4" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>Author: Joel Nothman ; Matthew Honnibal ; Ben Hachey ; James R. Curran</p><p>Abstract: Interpreting news requires identifying its constituent events. Events are complex linguistically and ontologically, so disambiguating their reference is challenging. We introduce event linking, which canonically labels an event reference with the article where it was first reported. This implicitly relaxes coreference to co-reporting, and will practically enable augmenting news archives with semantic hyperlinks. We annotate and analyse a corpus of 150 documents, extracting 501 links to a news archive with reasonable inter-annotator agreement.</p><p>5 0.25372803 <a title="90-tfidf-5" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>Author: Remy Kessler ; Xavier Tannier ; Caroline Hagege ; Veronique Moriceau ; Andre Bittar</p><p>Abstract: We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related.</p><p>6 0.23525903 <a title="90-tfidf-6" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>7 0.23319708 <a title="90-tfidf-7" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>8 0.22316976 <a title="90-tfidf-8" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>9 0.17414621 <a title="90-tfidf-9" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>10 0.16952914 <a title="90-tfidf-10" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>11 0.16501904 <a title="90-tfidf-11" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>12 0.16422851 <a title="90-tfidf-12" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>13 0.15232083 <a title="90-tfidf-13" href="./acl-2012-A_Novel_Burst-based_Text_Representation_Model_for_Scalable_Event_Detection.html">17 acl-2012-A Novel Burst-based Text Representation Model for Scalable Event Detection</a></p>
<p>14 0.12556444 <a title="90-tfidf-14" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>15 0.12078315 <a title="90-tfidf-15" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>16 0.11964308 <a title="90-tfidf-16" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>17 0.11725818 <a title="90-tfidf-17" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>18 0.10649288 <a title="90-tfidf-18" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>19 0.10476479 <a title="90-tfidf-19" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>20 0.10162035 <a title="90-tfidf-20" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.279), (1, 0.185), (2, -0.305), (3, 0.188), (4, -0.025), (5, -0.415), (6, 0.001), (7, -0.125), (8, 0.027), (9, -0.229), (10, -0.127), (11, 0.133), (12, 0.023), (13, 0.047), (14, 0.001), (15, -0.103), (16, -0.007), (17, 0.056), (18, 0.015), (19, 0.061), (20, 0.05), (21, -0.046), (22, -0.008), (23, -0.062), (24, -0.014), (25, 0.018), (26, 0.023), (27, 0.013), (28, -0.023), (29, 0.029), (30, -0.117), (31, 0.063), (32, 0.026), (33, -0.004), (34, 0.046), (35, 0.084), (36, -0.025), (37, -0.02), (38, 0.028), (39, -0.049), (40, -0.045), (41, -0.03), (42, 0.049), (43, 0.007), (44, -0.013), (45, 0.004), (46, -0.031), (47, 0.032), (48, 0.053), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96313244 <a title="90-lsi-1" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions.</p><p>2 0.88211024 <a title="90-lsi-2" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>Author: Preethi Raghavan ; Albert Lai ; Eric Fosler-Lussier</p><p>Abstract: We investigate the problem of ordering medical events in unstructured clinical narratives by learning to rank them based on their time of occurrence. We represent each medical event as a time duration, with a corresponding start and stop, and learn to rank the starts/stops based on their proximity to the admission date. Such a representation allows us to learn all of Allen’s temporal relations between medical events. Interestingly, we observe that this methodology performs better than a classification-based approach for this domain, but worse on the relationships found in the Timebank corpus. This finding has important implications for styles of data representation and resources used for temporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range ofdomains to fully understand the nature of temporal ordering.</p><p>3 0.79159415 <a title="90-lsi-3" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>Author: Remy Kessler ; Xavier Tannier ; Caroline Hagege ; Veronique Moriceau ; Andre Bittar</p><p>Abstract: We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related.</p><p>4 0.77077371 <a title="90-lsi-4" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>Author: Guillermo Garrido ; Anselmo Penas ; Bernardo Cabaleiro ; Alvaro Rodrigo</p><p>Abstract: Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported.</p><p>5 0.68418276 <a title="90-lsi-5" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>Author: Yafang Wang ; Maximilian Dylla ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: The Web and digitized text sources contain a wealth of information about named entities such as politicians, actors, companies, or cultural landmarks. Extracting this information has enabled the automated construction oflarge knowledge bases, containing hundred millions of binary relationships or attribute values about these named entities. However, in reality most knowledge is transient, i.e. changes over time, requiring a temporal dimension in fact extraction. In this paper we develop a methodology that combines label propagation with constraint reasoning for temporal fact extraction. Label propagation aggressively gathers fact candidates, and an Integer Linear Program is used to clean out false hypotheses that violate temporal constraints. Our method is able to improve on recall while keeping up with precision, which we demonstrate by experiments with biography-style Wikipedia pages and a large corpus of news articles.</p><p>6 0.62607586 <a title="90-lsi-6" href="./acl-2012-Extracting_and_modeling_durations_for_habits_and_events_from_Twitter.html">91 acl-2012-Extracting and modeling durations for habits and events from Twitter</a></p>
<p>7 0.60209423 <a title="90-lsi-7" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>8 0.56207949 <a title="90-lsi-8" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>9 0.50295544 <a title="90-lsi-9" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>10 0.46328461 <a title="90-lsi-10" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>11 0.43825826 <a title="90-lsi-11" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>12 0.42984381 <a title="90-lsi-12" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>13 0.41916296 <a title="90-lsi-13" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>14 0.41788921 <a title="90-lsi-14" href="./acl-2012-A_Novel_Burst-based_Text_Representation_Model_for_Scalable_Event_Detection.html">17 acl-2012-A Novel Burst-based Text Representation Model for Scalable Event Detection</a></p>
<p>15 0.41075784 <a title="90-lsi-15" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>16 0.39793932 <a title="90-lsi-16" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>17 0.36956725 <a title="90-lsi-17" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>18 0.35307845 <a title="90-lsi-18" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>19 0.35026532 <a title="90-lsi-19" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>20 0.34632316 <a title="90-lsi-20" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.023), (26, 0.057), (28, 0.029), (30, 0.034), (37, 0.032), (39, 0.036), (49, 0.019), (71, 0.013), (74, 0.03), (82, 0.383), (84, 0.044), (85, 0.022), (90, 0.103), (92, 0.042), (94, 0.013), (99, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84241712 <a title="90-lda-1" href="./acl-2012-Extracting_Narrative_Timelines_as_Temporal_Dependency_Structures.html">90 acl-2012-Extracting Narrative Timelines as Temporal Dependency Structures</a></p>
<p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We propose a new approach to characterizing the timeline of a text: temporal dependency structures, where all the events of a narrative are linked via partial ordering relations like BEFORE, AFTER, OVERLAP and IDENTITY. We annotate a corpus of children’s stories with temporal dependency trees, achieving agreement (Krippendorff’s Alpha) of 0.856 on the event words, 0.822 on the links between events, and of 0.700 on the ordering relation labels. We compare two parsing models for temporal dependency structures, and show that a deterministic non-projective dependency parser outperforms a graph-based maximum spanning tree parser, achieving labeled attachment accuracy of 0.647 and labeled tree edit distance of 0.596. Our analysis of the dependency parser errors gives some insights into future research directions.</p><p>2 0.83534658 <a title="90-lda-2" href="./acl-2012-Subgroup_Detector%3A_A_System_for_Detecting_Subgroups_in_Online_Discussions.html">188 acl-2012-Subgroup Detector: A System for Detecting Subgroups in Online Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: We present Subgroup Detector, a system for analyzing threaded discussions and identifying the attitude of discussants towards one another and towards the discussion topic. The system uses attitude predictions to detect the split of discussants into subgroups of opposing views. The system uses an unsupervised approach based on rule-based opinion target detecting and unsupervised clustering techniques. The system is open source and is freely available for download. An online demo of the system is available at: http://clair.eecs.umich.edu/SubgroupDetector/</p><p>3 0.80163437 <a title="90-lda-3" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>Author: Ioannis Konstas ; Mirella Lapata</p><p>Abstract: This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our approach is to reduce the tasks of content selection (“what to say”) and surface realization (“how to say”) into a common parsing problem. We define a probabilistic context-free grammar that describes the structure of the input (a corpus of database records and text describing some of them) and represent it compactly as a weighted hypergraph. The hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.</p><p>4 0.7906633 <a title="90-lda-4" href="./acl-2012-A_Graph-based_Cross-lingual_Projection_Approach_for_Weakly_Supervised_Relation_Extraction.html">12 acl-2012-A Graph-based Cross-lingual Projection Approach for Weakly Supervised Relation Extraction</a></p>
<p>Author: Seokhwan Kim ; Gary Geunbae Lee</p><p>Abstract: Although researchers have conducted extensive studies on relation extraction in the last decade, supervised approaches are still limited because they require large amounts of training data to achieve high performances. To build a relation extractor without significant annotation effort, we can exploit cross-lingual annotation projection, which leverages parallel corpora as external resources for supervision. This paper proposes a novel graph-based projection approach and demonstrates the merits of it by using a Korean relation extraction system based on projected dataset from an English-Korean parallel corpus.</p><p>5 0.65322137 <a title="90-lda-5" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Pradeep Dasigi ; Mona Diab ; Dragomir Radev</p><p>Abstract: The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.</p><p>6 0.55858123 <a title="90-lda-6" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>7 0.5235849 <a title="90-lda-7" href="./acl-2012-Learning_to_Temporally_Order_Medical_Events_in_Clinical_Text.html">135 acl-2012-Learning to Temporally Order Medical Events in Clinical Text</a></p>
<p>8 0.51549542 <a title="90-lda-8" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>9 0.51428169 <a title="90-lda-9" href="./acl-2012-Event_Linking%3A_Grounding_Event_Reference_in_a_News_Archive.html">85 acl-2012-Event Linking: Grounding Event Reference in a News Archive</a></p>
<p>10 0.50290209 <a title="90-lda-10" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>11 0.50255454 <a title="90-lda-11" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>12 0.49391374 <a title="90-lda-12" href="./acl-2012-Finding_Salient_Dates_for_Building_Thematic_Timelines.html">99 acl-2012-Finding Salient Dates for Building Thematic Timelines</a></p>
<p>13 0.48170042 <a title="90-lda-13" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>14 0.46903488 <a title="90-lda-14" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>15 0.46846887 <a title="90-lda-15" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>16 0.45960546 <a title="90-lda-16" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>17 0.45894861 <a title="90-lda-17" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>18 0.45605314 <a title="90-lda-18" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>19 0.4525528 <a title="90-lda-19" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>20 0.45077041 <a title="90-lda-20" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
