<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 acl-2012-Strong Lexicalization of Tree Adjoining Grammars</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-185" href="#">acl2012-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 acl-2012-Strong Lexicalization of Tree Adjoining Grammars</h1>
<br/><p>Source: <a title="acl-2012-185-pdf" href="http://aclweb.org/anthology//P/P12/P12-1053.pdf">pdf</a></p><p>Author: Andreas Maletti ; Joost Engelfriet</p><p>Abstract: Recently, it was shown (KUHLMANN, SATTA: Tree-adjoining grammars are not closed under strong lexicalization. Comput. Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. A more powerful model, the simple context-free tree grammar, admits such a normal form. It can be effectively constructed and the maximal rank of the nonterminals only increases by 1. Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.</p><p>Reference: <a title="acl-2012-185-reference" href="../acl2012_reference/acl-2012-Strong_Lexicalization_of_Tree_Adjoining_Grammars_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Recently, it was shown (KUHLMANN, SATTA: Tree-adjoining grammars are not closed under strong lexicalization. [sent-3, score-0.147]
</p><p>2 , 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. [sent-6, score-0.757]
</p><p>3 A more powerful model, the simple context-free tree grammar, admits such a normal form. [sent-7, score-0.177]
</p><p>4 Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves. [sent-9, score-0.72]
</p><p>5 1 Introduction  Tree adjoining grammars [TAG] (Joshi et al. [sent-10, score-0.213]
</p><p>6 A good overview on TAG, their formal properties, their linguistic motivation, and their applications is presented by Joshi and Schabes (1992) and Joshi and Schabes (1997), in which also strong lexicalization is discussed. [sent-13, score-0.18]
</p><p>7 In general, lexicalization is the process of transforming a grammar into an equivalent one (potentially expressed in another formalism) such that each production contains a lexical item (or anchor). [sent-14, score-0.453]
</p><p>8 Each production can then be viewed as lexical information on its anchor. [sent-15, score-0.216]
</p><p>9 nl i alphabet, each production of a lexicalized grammar produces at least one letter of the generated string. [sent-21, score-0.313]
</p><p>10 Consequently, lexicalized grammars offer significant parsing benefits (Schabes et al. [sent-22, score-0.188]
</p><p>11 , 1988) as the number of applications of productions (i. [sent-23, score-0.335]
</p><p>12 In addition, the lexical items in the productions guide the production selection in a derivation, which works especially well in scenarios with large alphabets. [sent-26, score-0.551]
</p><p>13 , 1960) only requires that the generated string languages coincide, whereas strong equivalence (Chomsky, 1963) requires that even the generated tree languages coincide. [sent-30, score-0.172]
</p><p>14 Correspondingly, we obtain weak and strong lexicalization based on the required equivalence. [sent-31, score-0.175]
</p><p>15 The GREIBACH normal form shows that CFG  can weakly lexicalize themselves, but they cannot strongly lexicalize themselves (Schabes, 1990). [sent-32, score-0.397]
</p><p>16 It is a prominent feature of tree adjoining grammars that they can strongly lexicalize CFG (Schabes, 1990),2 and it was claimed and widely believed that they can strongly lexicalize themselves. [sent-33, score-0.668]
</p><p>17 Recently, Kuhlmann and Satta (2012) proved that TAG actually cannot strongly lexicalize themselves. [sent-34, score-0.175]
</p><p>18 In fact, they prove that TAG cannot even strongly lexicalize the weaker tree insertion grammars (Schabes and Waters, 1995). [sent-35, score-0.402]
</p><p>19 , linear and nondeleting) context-free tree grammars [CFTG] (Rounds, 1969; Rounds, 1970) are a more powerful grammar formalism than TAG (M¨ onnich, 1997). [sent-43, score-0.284]
</p><p>20 However, the monadic variant is strongly equivalent to a slightly extended version of TAG, which is called non-strict TAG (Kepser and Rogers, 2011). [sent-44, score-0.157]
</p><p>21 In particular, they also demonstrate that monadic CFTG can strongly lexicalize regular tree grammars (G´ ecseg and Steinby, 1984; G ´ecseg and Steinby, 1997). [sent-47, score-0.527]
</p><p>22 CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al. [sent-48, score-0.222]
</p><p>23 In this contribution, we show that CFTG can strongly lexicalize TAG and also themselves, thus answering the second question in the conclusion of Kuhlmann and Satta (2012). [sent-53, score-0.175]
</p><p>24 This is achieved by a series of normalization steps (see Section 4) and a final lexicalization step (see Section 5), in which a lexical item is guessed for each production that does not already contain one. [sent-54, score-0.412]
</p><p>25 This item is then transported in an additional argument until it is exchanged for the same item in a terminal pro-  duction. [sent-55, score-0.192]
</p><p>26 The lexicalization is effective and increases the maximal rank (number of arguments) of the nonterminals by at most 1. [sent-56, score-0.214]
</p><p>27 In contrast to a transformation into GREIBACH normal form, our lexicalization does not radically change the structure of the derivations. [sent-57, score-0.199]
</p><p>28 Moreover, t[u]p denotes the tree obtained from t by replacing the subtree at p by the tree u ∈ TΣ (X). [sent-91, score-0.21]
</p><p>29 3  Context-free tree grammars  In this section, we recall linear and nondeleting context-free tree grammars [CFTG] (Rounds, 1969; Rounds, 1970). [sent-154, score-0.454]
</p><p>30 The nonterminals of regular tree grammars only occur at the leaves and are replaced using first-order substitution. [sent-156, score-0.288]
</p><p>31 In the left-hand sides of productions we write A(x1, . [sent-159, score-0.361]
</p><p>32 , xk) for a nonterminal A ∈ Nk to indicate the variab)l feosr th aa nt ohnotledr mthien dli Arect ∈ ∈su Nbtrees of a particular occurrence of A. [sent-162, score-0.172]
</p><p>33 S ∈ N0 is the start nonterminal of rank 0, and • PS ∈is a finite set of productions of the form A(x1, . [sent-166, score-0.57]
</p><p>34 The components ‘ and r are called left- and righthand side of the production ‘ → r in P. [sent-170, score-0.253]
</p><p>35 T sahey right-hand side is simply a tree using terminal and nonterminal symbols according to their rank. [sent-175, score-0.423]
</p><p>36 We use lower-case Greek letters for terminal symbols and upper-case Latin letters for nonterminals. [sent-179, score-0.157]
</p><p>37 As a running example, we consider the CFTG Gex = ({S(0), Σ, S, P) where •  •  A(2)}, Σ = {=σ(2 (){, Sα(0), β(0)}} ,aΣn,dS  •  PΣ c=o {nσtains the productions (see Figure 3):8 S  →  A(α, α) | A(β, β) | σ(α, β)  A(x1, x2)  →  A(σ(x1 , S) , σ(x2 , S)) | σ(x1 , x2) . [sent-181, score-0.335]
</p><p>38 A derivation to a tree of TΣ is illustrated in Figure 4. [sent-205, score-0.149]
</p><p>39 It demonstrates that the final tree in that derivation is in the language JGexK generated by Gex. [sent-206, score-0.149]
</p><p>40 A major tool is a simple production elimination 9For all k ∈ N and ξ ⇒G ζ we note that ξ ∈ CN∪Σ (Xk) if and only ilfl ζ ∈ C NN an∪dΣ (Xk). [sent-218, score-0.252]
</p><p>41 The CFTG G is start-separated if posS(r) = ∅  for every production ‘ → r ∈ P. [sent-222, score-0.242]
</p><p>42 In such a CFTG we call each production of the form S → r initial. [sent-226, score-0.216]
</p><p>43 It requires that the right-hand side of each non-initial production contains at least two terminal or nonterminal symbols. [sent-232, score-0.477]
</p><p>44 In particular, it eliminates projection productions A(x1) → x1 and unit productions, in which the right-hand si dxe has the same shape as the lefthand side (potentially with a different root symbol and a different order of the variables). [sent-233, score-0.415]
</p><p>45 A production ‘ → r is growing if |posN∪Σ(r) | ≥ 2 p. [sent-235, score-0.256]
</p><p>46 oTdhuec iCoFnTG ‘ →G →i sr growing nifg a illf |opfo oistsN n∪oΣn(-rin)i|ti ≥al productions are growing. [sent-236, score-0.375]
</p><p>47 The tree language L ⊆ TΣ has finite ∆-ambiguity . [sent-262, score-0.19]
</p><p>48 This is typically called strong lexicalization (Schabes, 1990; Joshi and Schabes, 1992; Kuhlmann and Satta, 2012) because we require strong equiva-  lence. [sent-267, score-0.177]
</p><p>49 The production ‘ → r is ∆-lexicalDizeefdi nifit pos∆(r) e∅ . [sent-270, score-0.216]
</p><p>50 p rTohdeu CctiFoTnG ‘ G → →is r r∆ is-l ∆ex-ilceaxliiczaeldif all its non-(inr)iti6 =al productions are ∆ is- ∆lex-liceaxliiczaelidz. [sent-271, score-0.335]
</p><p>51 First, we define our simple production elimination scheme, which we will use in the following. [sent-276, score-0.252]
</p><p>52 Roughly speaking, a non-initial Aproduction such that A does not occur in its righthand side can be eliminated from G by applying it in  =  12The corresponding notion for weak equivalence is called weak lexicalization (Joshi and Schabes, 1992). [sent-277, score-0.252]
</p><p>53 , xk) → r in P be a non-initial production such that posA(r) = ∅P. [sent-283, score-0.216]
</p><p>54 For every other production ρ0 = ‘0 → r0 (inr )P = =an ∅d. [sent-284, score-0.242]
</p><p>55 In particular, ρ0∅ = ρ0 for every production ρ0, so every productio∅n besides the eliminated production ρ is preserved. [sent-288, score-0.484]
</p><p>56 Conversely, a derivation of G can be simulated directly by G0ρ except for derivation steps ⇒ρG,p using the eliminated production ρ. [sent-294, score-0.304]
</p><p>57 tSiionnce s eSp ⇒A, we know that the nonterminal at posSiitniocne p was generated by aanto tthheer n production ρ0. [sent-295, score-0.34]
</p><p>58 =  In the next normalization step we use our production elimination scheme. [sent-300, score-0.252]
</p><p>59 The goal is to make sure that non-initial monic productions (i. [sent-301, score-0.472]
</p><p>60 , productions of which the right-hand side contains at most one nonterminal) contain at least one lexical symbol. [sent-303, score-0.372]
</p><p>61 A sentential form ξ ∈ SF(G) is monic if |posN (ξ) | ≤ 1n. [sent-305, score-0.168]
</p><p>62 us T rheme onveaxlt of epsilon-productions A → ε and unit productions oAf → Bilo fno-rp rcoodnutcetxito-fnrsee A grammars (Hopcroft cetti al. [sent-309, score-0.457]
</p><p>63 , xk) ⇒+G0 ξ} , where ⇒+G0 is the transitive closure of ⇒G0 and the wCFheTrGe ⇒G0 = (N, Σ, S, P0) i sc osusuchre th ofat ⇒ ⇒P0 contains exactly the non-∆-lexicalized productions of P. [sent-328, score-0.36]
</p><p>64 The set ΞA is finite since only finitely many non∆-lexicalized productions can be used due to the finite ∆-ambiguity of JGK . [sent-329, score-0.551]
</p><p>65 Next, we eliminate all productions of P1 from G1 using Lemma 12 to obtain an equivalent CFTG G2 with the productions P2. [sent-338, score-0.72]
</p><p>66 In the final step, we drop all non-∆lexicalized monic productions of P2 to obtain the CFTG G, in which all monic productions are ∆lexicalized. [sent-339, score-0.944]
</p><p>67 The CFTG G0e0x only has {α, β}-lexicalized noninitial monic productions, so we use a new example. [sent-341, score-0.171]
</p><p>68 c hL tth (at{ SΣ = {σ(2), α}(0,)Σ, ,βS(0,)P} )a bnde 511  xA1⇒G0βσxB1⇒G0βxσ1σβ  Bx1⇒G0x1σβ  Figure 5: The relevant derivations using only productions that are not ∆-lexicalized (see Example 14). [sent-344, score-0.335]
</p><p>69 P contains the productions A(x1) B(x1)  → →  σ(β, B(x1)) σ(α, A(x1))  B(x1) S  → →  σ(x1 , β) A(α) . [sent-345, score-0.335]
</p><p>70 Moreover, all its productions are monic, and JGex2K iMs ofirneoitevleyr, ∆al-la imtsb pirgoudouucst ofnors tahree mseotn ∆ic, a=n { JαG} ofK ilMesxo firicneaiotl seyrm, abllo iltss. [sent-347, score-0.335]
</p><p>71 The relevant derivations using only non-∆-lexicalized productions are shown in Figure 5. [sent-350, score-0.335]
</p><p>72 We call a production ‘ → r terminal if r ∈ TΣ (X) ; i. [sent-353, score-0.316]
</p><p>73 | ≥f J G2 Kfo hra sal fli nititse n ∆on--aimnibtiiaglu tietyr,m tihneanl productions r‘) → r ∈ Pfor0. [sent-361, score-0.335]
</p><p>74 Moreover, we assume that each nonterminal is use-  ful and that each of its non-initial monic productions is ∆-lexicalized by Theorem 13. [sent-364, score-0.596]
</p><p>75 We obtain the desired CFTG by simply eliminating each noninitial terminal production ‘ → r ∈ P such that |pos∆ (r) | = 1l. [sent-365, score-0.35]
</p><p>76 on R-eicnaitlilal t htaetrm ∆ina =l production uthctaito vnio (l4a)te iss the requirement of Theorem 15. [sent-374, score-0.216]
</p><p>77 We eliminate it and obtain the CFTG with the productions  S S B(x1) B(x1) 5  → → → →  σ(β, σ(β, σ(α, σ(α,  B(α)) | σ(β, σ(α, β)) σ(α, σ(β, σ(α, β)))) σ(β, B(x1))) σ(β, σ(α, σ(β, σ(x1 ,β))))) . [sent-375, score-0.335]
</p><p>78 This is the reason why we made sure that we have two occurrences of lexical symbols in the terminal productions. [sent-381, score-0.157]
</p><p>79 After we exchanged one  ∆-  ×  for a parameter, the resulting terminal production is 512 still ∆-lexicalized. [sent-382, score-0.316]
</p><p>80 Lexical items that are guessed for distinct (occurrences of) productions are transported to distinct (occurrences of) terminal productions [cf. [sent-383, score-0.844]
</p><p>81 We let N0 = N ∆ be a new set of nonterminals such that rk(hA, δi) = rk(A) + 1t foofr n every mAin ∈ N su cahnd th aδt ∈ (∆hA. [sent-392, score-0.162]
</p><p>82 This parameter is handed to the (lexicographically) first nonterminal in the right-hand side until it is resolved in a terminal production. [sent-395, score-0.261]
</p><p>83 F)o wr teha cBh nonterminal A-production ρ = ‘ → r in P let ρδ  = hA, δi (x1, . [sent-412, score-0.174]
</p><p>84 Roughly speaking, we select the lexicographically smallest occurrence of a nonterminal in the right-hand side and pass the lexical symbol δ in the extra parameter to it. [sent-417, score-0.299]
</p><p>85 The extra parameter is used in terminal productions, so let ρ = ‘ → r in P  S→  ασα  hSx,1αi → x1σα  Figure 7: Original terminal production and the production ρ (see Theorem 17). [sent-418, score-0.682]
</p><p>86 With these productions we obtain the CFTG G0 = (N ∪ N0, S,P), where  Σ,  P = P ∪ P0 ∪ P00 and= P0 =  [  {ρδ | δ ∈ ∆}  P00 =  ρ=‘→[r∈P ‘6=ρS=,p‘[o→sNr  [  {ρ} . [sent-425, score-0.335]
</p><p>87 ρ=‘→[r∈P  ∈(rP)6=∅  ‘6=ρS=,p‘[o→sNr  ∈(rP)=∅  It is easy to prove that those new productions manage the desired transport of the extra parameter if it holds the value indicated in the nonterminal. [sent-426, score-0.335]
</p><p>88 Finally, we replace each non-initial non-∆-lexicalized production in G0 by new productions that guess a lexical symbol and add it to the new parameter of the (lexicographically) first nonterminal of N in the right-hand side. [sent-427, score-0.718]
</p><p>89 Note that  =  each production ‘ → r ∈ Pnil contains at least one occurrence cotfio a nno ‘n →ter rmi ∈na Pl of N (because all monic productions of G are ∆-lexicalized). [sent-429, score-0.688]
</p><p>90 Now all noninitial non-∆-lexicalized productions from P can be removed, so we obtain the CFTG G00, which is given by (N ∪ N0, Σ, S, R) with R = (P ∪ P000) \ Pnil. [sent-430, score-0.369]
</p><p>91 only transported until it is resolved in a production with at least two lexical items. [sent-435, score-0.25]
</p><p>92 If we change δthi ea ldex Sica=liz haSti,oδni construction as indicated before this example, then all the productions Sδ(x1) → Aδ(δ0, δ0, x1) are replaced by the productions Sδ(x1) → A(x1, δ). [sent-440, score-0.698]
</p><p>93 Moreover, the productions (5) can b e→ replaced by the productions A(x1, x2) → A(σ(x1 , Sδ(δ)) , σ(x2, S)), and then the nonter)m →inal As Aδ xand their productions can be removed, which leaves only 15 productions. [sent-441, score-1.005]
</p><p>94 sSein CceF TthGe normal form constructions preserve the nonterminal rank, the proof of Theorem 17 shows that CFTG(k)  are strongly lexicalized by CFTG(k+1). [sent-443, score-0.312]
</p><p>95 (1980) that the classes CFTG(k) with k ∈ N in(d1u9c8e0 an ihnaftin tihtee hierarchy FofT string languages, N but in nitremains an open problem whether the rank increase in our lexicalization construction is necessary. [sent-447, score-0.181]
</p><p>96 Epsilon-free grammars and lexicalized grammars that generate the class of the mildly context-sensitive languages. [sent-497, score-0.342]
</p><p>97 The equivalence of tree adjoining grammars and monadic linear context-free tree grammars. [sent-591, score-0.522]
</p><p>98 Adjunction as substitution: An algebraic formulation of regular, context-free and tree adjoining languages. [sent-626, score-0.223]
</p><p>99 Parsing strategies with ‘lexicalized’ grammars: Application to tree adjoining grammars. [sent-682, score-0.196]
</p><p>100 Tree insertion grammar: A cubic-time parsable formalism that lexicalizes context-free grammars without changing the trees produced. [sent-691, score-0.148]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cftg', 0.673), ('productions', 0.335), ('production', 0.216), ('monic', 0.137), ('schabes', 0.127), ('lexicalization', 0.127), ('lexicalize', 0.125), ('nonterminal', 0.124), ('grammars', 0.122), ('xk', 0.119), ('kuhlmann', 0.114), ('tree', 0.105), ('theorem', 0.104), ('terminal', 0.1), ('jgk', 0.091), ('adjoining', 0.091), ('finite', 0.085), ('joshi', 0.081), ('rk', 0.08), ('posn', 0.08), ('normal', 0.072), ('ecseg', 0.068), ('lexicographically', 0.068), ('lexicalized', 0.066), ('nonterminals', 0.061), ('tk', 0.059), ('symbols', 0.057), ('greibach', 0.057), ('kepser', 0.057), ('monadic', 0.057), ('posa', 0.057), ('stamer', 0.057), ('strongly', 0.05), ('let', 0.05), ('gex', 0.05), ('equivalent', 0.05), ('alphabet', 0.046), ('finitely', 0.046), ('rogers', 0.046), ('steinby', 0.046), ('derivation', 0.044), ('satta', 0.044), ('symbol', 0.043), ('equivalence', 0.042), ('ha', 0.041), ('automata', 0.041), ('sf', 0.04), ('yd', 0.04), ('guessed', 0.04), ('growing', 0.04), ('pos', 0.039), ('tag', 0.039), ('side', 0.037), ('yves', 0.036), ('elimination', 0.036), ('elim', 0.034), ('engelfriet', 0.034), ('fujiyoshi', 0.034), ('hopcroft', 0.034), ('noninitial', 0.034), ('onnich', 0.034), ('pnil', 0.034), ('rozenberg', 0.034), ('transported', 0.034), ('aravind', 0.033), ('mildly', 0.032), ('grammar', 0.031), ('sentential', 0.031), ('guez', 0.03), ('kanazawa', 0.03), ('treeadjoining', 0.03), ('rn', 0.029), ('item', 0.029), ('rounds', 0.028), ('construction', 0.028), ('formal', 0.028), ('stuttgart', 0.027), ('algebraic', 0.027), ('smallest', 0.027), ('marco', 0.027), ('rank', 0.026), ('sides', 0.026), ('formalism', 0.026), ('every', 0.026), ('th', 0.025), ('cfg', 0.025), ('strong', 0.025), ('formally', 0.025), ('weakly', 0.025), ('rewriting', 0.025), ('positions', 0.024), ('categorial', 0.024), ('position', 0.023), ('weak', 0.023), ('grzegorz', 0.023), ('akio', 0.023), ('arto', 0.023), ('baader', 0.023), ('dit', 0.023), ('feosr', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="185-tfidf-1" href="./acl-2012-Strong_Lexicalization_of_Tree_Adjoining_Grammars.html">185 acl-2012-Strong Lexicalization of Tree Adjoining Grammars</a></p>
<p>Author: Andreas Maletti ; Joost Engelfriet</p><p>Abstract: Recently, it was shown (KUHLMANN, SATTA: Tree-adjoining grammars are not closed under strong lexicalization. Comput. Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. A more powerful model, the simple context-free tree grammar, admits such a normal form. It can be effectively constructed and the maximal rank of the nonterminals only increases by 1. Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.</p><p>2 0.12043514 <a title="185-tfidf-2" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>Author: Makoto Kanazawa ; Sylvain Salvati</p><p>Abstract: The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal n-luemttebrer a olpfh occurrences }o tfh heaatch c olentttaeinr. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.</p><p>3 0.10334387 <a title="185-tfidf-3" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>Author: Hiroyuki Shindo ; Yusuke Miyao ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers.</p><p>4 0.089319855 <a title="185-tfidf-4" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.</p><p>5 0.075832501 <a title="185-tfidf-5" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>Author: Shay B. Cohen ; Karl Stratos ; Michael Collins ; Dean P. Foster ; Lyle Ungar</p><p>Abstract: We introduce a spectral learning algorithm for latent-variable PCFGs (Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides consistent parameter estimates.</p><p>6 0.069837794 <a title="185-tfidf-6" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>7 0.067253076 <a title="185-tfidf-7" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>8 0.064510927 <a title="185-tfidf-8" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>9 0.056849975 <a title="185-tfidf-9" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>10 0.049725518 <a title="185-tfidf-10" href="./acl-2012-Syntactic_Stylometry_for_Deception_Detection.html">190 acl-2012-Syntactic Stylometry for Deception Detection</a></p>
<p>11 0.048405778 <a title="185-tfidf-11" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>12 0.042615924 <a title="185-tfidf-12" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>13 0.042208634 <a title="185-tfidf-13" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>14 0.037061572 <a title="185-tfidf-14" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>15 0.034874689 <a title="185-tfidf-15" href="./acl-2012-The_OpenGrm_open-source_finite-state_grammar_software_libraries.html">196 acl-2012-The OpenGrm open-source finite-state grammar software libraries</a></p>
<p>16 0.032404158 <a title="185-tfidf-16" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>17 0.031995177 <a title="185-tfidf-17" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>18 0.031300955 <a title="185-tfidf-18" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>19 0.031277388 <a title="185-tfidf-19" href="./acl-2012-Heuristic_Cube_Pruning_in_Linear_Time.html">107 acl-2012-Heuristic Cube Pruning in Linear Time</a></p>
<p>20 0.03121857 <a title="185-tfidf-20" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.097), (1, -0.006), (2, -0.067), (3, -0.062), (4, -0.072), (5, -0.005), (6, -0.007), (7, 0.103), (8, 0.013), (9, 0.016), (10, -0.051), (11, -0.086), (12, -0.04), (13, 0.03), (14, 0.044), (15, -0.113), (16, 0.023), (17, -0.037), (18, 0.03), (19, 0.019), (20, 0.041), (21, 0.016), (22, -0.062), (23, -0.003), (24, -0.023), (25, 0.057), (26, 0.039), (27, -0.038), (28, 0.021), (29, 0.068), (30, 0.004), (31, 0.079), (32, -0.12), (33, 0.06), (34, 0.037), (35, -0.025), (36, -0.09), (37, -0.011), (38, -0.128), (39, -0.154), (40, 0.09), (41, -0.023), (42, 0.101), (43, 0.1), (44, 0.008), (45, -0.002), (46, -0.039), (47, 0.107), (48, 0.248), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96385664 <a title="185-lsi-1" href="./acl-2012-Strong_Lexicalization_of_Tree_Adjoining_Grammars.html">185 acl-2012-Strong Lexicalization of Tree Adjoining Grammars</a></p>
<p>Author: Andreas Maletti ; Joost Engelfriet</p><p>Abstract: Recently, it was shown (KUHLMANN, SATTA: Tree-adjoining grammars are not closed under strong lexicalization. Comput. Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. A more powerful model, the simple context-free tree grammar, admits such a normal form. It can be effectively constructed and the maximal rank of the nonterminals only increases by 1. Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.</p><p>2 0.82345861 <a title="185-lsi-2" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>Author: Makoto Kanazawa ; Sylvain Salvati</p><p>Abstract: The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal n-luemttebrer a olpfh occurrences }o tfh heaatch c olentttaeinr. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.</p><p>3 0.54832214 <a title="185-lsi-3" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>Author: Shay B. Cohen ; Karl Stratos ; Michael Collins ; Dean P. Foster ; Lyle Ungar</p><p>Abstract: We introduce a spectral learning algorithm for latent-variable PCFGs (Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides consistent parameter estimates.</p><p>4 0.52121937 <a title="185-lsi-4" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>Author: Bevan Jones ; Mark Johnson ; Sharon Goldwater</p><p>Abstract: Many semantic parsing models use tree transformations to map between natural language and meaning representation. However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.</p><p>5 0.45551684 <a title="185-lsi-5" href="./acl-2012-The_OpenGrm_open-source_finite-state_grammar_software_libraries.html">196 acl-2012-The OpenGrm open-source finite-state grammar software libraries</a></p>
<p>Author: Brian Roark ; Richard Sproat ; Cyril Allauzen ; Michael Riley ; Jeffrey Sorensen ; Terry Tai</p><p>Abstract: In this paper, we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules into finite-state transducers, and for n-gram language modeling. The OpenGrm libraries use the OpenFst library to provide an efficient encoding of grammars and general algorithms for building, modifying and applying models.</p><p>6 0.42118496 <a title="185-lsi-6" href="./acl-2012-Heuristic_Cube_Pruning_in_Linear_Time.html">107 acl-2012-Heuristic Cube Pruning in Linear Time</a></p>
<p>7 0.41954127 <a title="185-lsi-7" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>8 0.41077837 <a title="185-lsi-8" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>9 0.3868064 <a title="185-lsi-9" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>10 0.36724436 <a title="185-lsi-10" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>11 0.35364181 <a title="185-lsi-11" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>12 0.3227984 <a title="185-lsi-12" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>13 0.30201125 <a title="185-lsi-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.29988918 <a title="185-lsi-14" href="./acl-2012-A_Cost_Sensitive_Part-of-Speech_Tagging%3A_Differentiating_Serious_Errors_from_Minor_Errors.html">9 acl-2012-A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors</a></p>
<p>15 0.28797749 <a title="185-lsi-15" href="./acl-2012-Syntactic_Stylometry_for_Deception_Detection.html">190 acl-2012-Syntactic Stylometry for Deception Detection</a></p>
<p>16 0.26034781 <a title="185-lsi-16" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<p>17 0.21342623 <a title="185-lsi-17" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>18 0.20060325 <a title="185-lsi-18" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>19 0.19687109 <a title="185-lsi-19" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>20 0.19605578 <a title="185-lsi-20" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.4), (23, 0.019), (26, 0.026), (28, 0.025), (30, 0.029), (35, 0.018), (37, 0.018), (39, 0.034), (74, 0.021), (82, 0.018), (84, 0.039), (85, 0.034), (90, 0.071), (92, 0.073), (94, 0.016), (99, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81241357 <a title="185-lda-1" href="./acl-2012-Strong_Lexicalization_of_Tree_Adjoining_Grammars.html">185 acl-2012-Strong Lexicalization of Tree Adjoining Grammars</a></p>
<p>Author: Andreas Maletti ; Joost Engelfriet</p><p>Abstract: Recently, it was shown (KUHLMANN, SATTA: Tree-adjoining grammars are not closed under strong lexicalization. Comput. Linguist., 2012) that finitely ambiguous tree adjoining grammars cannot be transformed into a normal form (preserving the generated tree language), in which each production contains a lexical symbol. A more powerful model, the simple context-free tree grammar, admits such a normal form. It can be effectively constructed and the maximal rank of the nonterminals only increases by 1. Thus, simple context-free tree grammars strongly lexicalize tree adjoining grammars and themselves.</p><p>2 0.65595782 <a title="185-lda-2" href="./acl-2012-Text_Segmentation_by_Language_Using_Minimum_Description_Length.html">194 acl-2012-Text Segmentation by Language Using Minimum Description Length</a></p>
<p>Author: Hiroshi Yamaguchi ; Kumiko Tanaka-Ishii</p><p>Abstract: The problem addressed in this paper is to segment a given multilingual document into segments for each language and then identify the language of each segment. The problem was motivated by an attempt to collect a large amount of linguistic data for non-major languages from the web. The problem is formulated in terms of obtaining the minimum description length of a text, and the proposed solution finds the segments and their languages through dynamic programming. Empirical results demonstrating the potential of this approach are presented for experiments using texts taken from the Universal Declaration of Human Rights and Wikipedia, covering more than 200 languages.</p><p>3 0.6466217 <a title="185-lda-3" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>Author: Eduard Dragut ; Hong Wang ; Clement Yu ; Prasad Sistla ; Weiyi Meng</p><p>Abstract: Polarity classification of words is important for applications such as Opinion Mining and Sentiment Analysis. A number of sentiment word/sense dictionaries have been manually or (semi)automatically constructed. The dictionaries have substantial inaccuracies. Besides obvious instances, where the same word appears with different polarities in different dictionaries, the dictionaries exhibit complex cases, which cannot be detected by mere manual inspection. We introduce the concept of polarity consistency of words/senses in sentiment dictionaries in this paper. We show that the consistency problem is NP-complete. We reduce the polarity consistency problem to the satisfiability problem and utilize a fast SAT solver to detect inconsistencies in a sentiment dictionary. We perform experiments on four sentiment dictionaries and WordNet.</p><p>4 0.3393392 <a title="185-lda-4" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>Author: Makoto Kanazawa ; Sylvain Salvati</p><p>Abstract: The language MIX consists of all strings over the three-letter alphabet {a, b, c} that contain an equal n-luemttebrer a olpfh occurrences }o tfh heaatch c olentttaeinr. We prove Joshi’s (1985) conjecture that MIX is not a tree-adjoining language.</p><p>5 0.33728683 <a title="185-lda-5" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart Shieber</p><p>Abstract: We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data.</p><p>6 0.30238575 <a title="185-lda-6" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>7 0.29648262 <a title="185-lda-7" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>8 0.29551315 <a title="185-lda-8" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>9 0.29519275 <a title="185-lda-9" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>10 0.2945911 <a title="185-lda-10" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>11 0.29158676 <a title="185-lda-11" href="./acl-2012-Tweet_Recommendation_with_Graph_Co-Ranking.html">205 acl-2012-Tweet Recommendation with Graph Co-Ranking</a></p>
<p>12 0.29143402 <a title="185-lda-12" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>13 0.28877014 <a title="185-lda-13" href="./acl-2012-QuickView%3A_NLP-based_Tweet_Search.html">167 acl-2012-QuickView: NLP-based Tweet Search</a></p>
<p>14 0.28788623 <a title="185-lda-14" href="./acl-2012-Native_Language_Detection_with_Tree_Substitution_Grammars.html">154 acl-2012-Native Language Detection with Tree Substitution Grammars</a></p>
<p>15 0.28685713 <a title="185-lda-15" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>16 0.28630036 <a title="185-lda-16" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>17 0.28566331 <a title="185-lda-17" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>18 0.28516161 <a title="185-lda-18" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>19 0.28501391 <a title="185-lda-19" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>20 0.28402433 <a title="185-lda-20" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
