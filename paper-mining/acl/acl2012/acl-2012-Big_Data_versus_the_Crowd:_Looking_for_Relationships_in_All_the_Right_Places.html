<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-40" href="#">acl2012-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</h1>
<br/><p>Source: <a title="acl-2012-40-pdf" href="http://aclweb.org/anthology//P/P12/P12-1087.pdf">pdf</a></p><p>Author: Ce Zhang ; Feng Niu ; Christopher Re ; Jude Shavlik</p><p>Abstract: Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.</p><p>Reference: <a title="acl-2012-40-reference" href="../acl2012_reference/acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. [sent-4, score-1.114]
</p><p>2 There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. [sent-5, score-0.329]
</p><p>3 We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. [sent-7, score-0.232]
</p><p>4 Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). [sent-8, score-1.476]
</p><p>5 In contrast, human feedback has a positive and statistically significant, but lower,  impact on precision and recall. [sent-9, score-0.683]
</p><p>6 1 Introduction Relation extraction is the problem of populating a target relation (representing an entity-level relationship or attribute) with facts extracted from naturallanguage text. [sent-10, score-0.216]
</p><p>7 To remedy these problems, recent years have seen interest in the distant supervision approach for rela825 tion extraction (Wu and Weld, 2007; Mintz et al. [sent-13, score-0.994]
</p><p>8 The input to distant supervision is a set of seed facts for the target relation together with an (unlabeled) text corpus, and the output is a set of (noisy) annotations that can be used by any machine learning technique to train a statistical model for the target relation. [sent-15, score-1.193]
</p><p>9 For example, given the target relation birthP lace(person, place) and a seed  fact birthP lace(John, Springfield), the sentence “John and his wife were born in Springfield in 1946” (S1) would qualify as a positive training example. [sent-16, score-0.232]
</p><p>10 Distant supervision replaces the expensive process of manually acquiring annotations that is required by direct supervision with resources that already exist in many scenarios (seed facts and a text corpus). [sent-17, score-0.897]
</p><p>11 For example, “John left Springfield when he was 16” (S2) would also be considered a positive example about place of birth by distant supervision as it contains both John and Springfield. [sent-19, score-1.008]
</p><p>12 For example, with a large enough corpus, a distant supervision system may find that patterns in the sentence S1 strongly correlate with seed facts of birthP lace whereas patterns in S2 do not qualify as a strong indicator. [sent-21, score-1.169]
</p><p>13 Thus, intuitively the quality of distant supervision should improve as we use larger corpora. [sent-22, score-1.027]
</p><p>14 However, there has been no study on the impact of corpus size on distant supervision for relation extraction. [sent-23, score-1.4]
</p><p>15 Besides “big data,” another resource that may be valuable to distant supervision is crowdsourcProce dJienjgus, R ofep thueb 5lic0t hof A Knonruea ,l M 8-e1e4ti Jnugly o f2 t0h1e2 A. [sent-25, score-0.952]
</p><p>16 For example, one could employ crowd workers to provide feedback on whether distant supervision examples are correct or not (Gormley et al. [sent-28, score-1.491]
</p><p>17 Intuitively the crowd workforce is a perfect fit for such tasks since many erroneous distant labels could be easily identified and corrected by humans. [sent-30, score-0.782]
</p><p>18 For example, distant supervision may mistakenly consider “Obama took a vacation in Hawaii” a positive example for birthP lace simply because a database says that Obama was born in Hawaii; a crowd worker would correctly point out that this sentence is not actually indicative of this relation. [sent-31, score-1.243]
</p><p>19 Our primary contribution is to empirically assess how scaling these inputs to distant supervision impacts its result quality. [sent-33, score-0.977]
</p><p>20 While the largest corpus (Wikipedia and New York Times) employed  by recent work on distant supervision (Mintz et al. [sent-35, score-1.015]
</p><p>21 , 2010) on crowdsourcing for distant supervision used thousands of human feedback units, we acquire tens of thousands of human-provided labels. [sent-40, score-1.428]
</p><p>22 Despite the large scale, we follow state-of-the-art distant supervision approaches and use deep linguistic features, e. [sent-41, score-0.952]
</p><p>23 How does increasing the corpus size impact the quality of distant supervision? [sent-45, score-1.009]
</p><p>24 For a given corpus size, how does increasing the amount of human feedback impact the quality of distant supervision? [sent-47, score-1.337]
</p><p>25 We found that increasing corpus size consistently and significantly improves recall and F1, despite reducing precision on small corpora; in contrast, human feedback has relatively small impact on precision and recall. [sent-48, score-1.049]
</p><p>26 8M documents, we found that increasing the corpus size ten-fold consistently results in statistically  1http : / / lemurpro j e ct . [sent-50, score-0.269]
</p><p>27 On the other hand, increasing human feedback amount ten-fold results in statistically significant improvement on F1 only when the corpus contains at least 1M documents; and the magnitude of such improvement was only one fifth compared to the impact of corpus-size increment. [sent-55, score-0.776]
</p><p>28 We find that the quality of distant supervision tends to be recall gated, that is, for any given relation, distant supervision fails to find all possible linguistic signals that indicate a relation. [sent-56, score-2.045]
</p><p>29 Thus, as a rule ofthumb for developing distant supervision systems, one should first attempt to expand the training corpus and then worry about precision of labels only after having obtained a broad-coverage corpus. [sent-58, score-1.169]
</p><p>30 More closely related to relation extraction is the work of Lin and Patel (2001) that uses dependency paths to find answers that express the same relation as in a question. [sent-69, score-0.294]
</p><p>31 For example, distant supervision has been used for the TAC-KBP slot-filling tasks (Surdeanu et al. [sent-72, score-0.952]
</p><p>32 In contrast, we study how increas-  ing input size (and incorporating human feedback) improves the result quality of distant supervision. [sent-76, score-0.883]
</p><p>33 Hdubmackn Figure 1: The workflow of our distant supervision system. [sent-90, score-0.978]
</p><p>34 abilistic models; such models have recently been used to relax various assumptions of distant supervision (Riedel et al. [sent-93, score-0.952]
</p><p>35 Specifically, they address the noisy assumption that, if two entities participate in a relation in a knowledge base, then all co-occurrences of these entities express this relation. [sent-97, score-0.288]
</p><p>36 Their focus is the collection process; in contrast, our goal is to quantify the impact of this additional data source on distant-  supervision quality. [sent-102, score-0.567]
</p><p>37 (2009) study how to acquire end-user feedback on relation-extraction results posted on an augmented Wikipedia site; it is interesting future work to integrate this source in our experiments. [sent-105, score-0.339]
</p><p>38 An example relation is that two persons are married, which for mentions of entities x and y is denoted R(x, y). [sent-109, score-0.279]
</p><p>39 Figure 1 illustrates the overall workflow of a distant supervision system. [sent-117, score-0.978]
</p><p>40 At each step of the distant supervision process, we closely follow the recent literature (Mintz et al. [sent-118, score-0.952]
</p><p>41 1 Distant Supervision Distant supervision compensates for a lack of training examples by generating what are known as silver-standard examples (Wu and Weld, 2007). [sent-122, score-0.53]
</p><p>42 The observation is that we are often able to obtain a structured, but incomplete, database D that instantiates relations ofinterest and a text corpus C that contains mentions of the entities in our database. [sent-123, score-0.321]
</p><p>43 We denote the relation that describes this mapping as the relation EL(e, m) where e ∈ E is an entity in the database D and m is  a mheernetei on ∈ i En t ihse a corpus yC in. [sent-146, score-0.331]
</p><p>44 Following recent work on distant supervision (Mintz et al. [sent-157, score-0.952]
</p><p>45 Specifically, for each pair of entity mentions (m1, m2) in a sentence, we extract the following features F(m1 , m2): (1) the word sequence  (including POS tags) between these mentions after normalizing entity mentions (e. [sent-174, score-0.378]
</p><p>46 In this work, we specifically examine feedback on the result of distant supervision. [sent-181, score-0.847]
</p><p>47 (2009), we use logistic regression classifiers to represent relation extractors. [sent-207, score-0.237]
</p><p>48 We compensate for the different sizes of distant and human labeled examples by training an objective function that allows to tune the weight of human versus distant labeling. [sent-213, score-1.401]
</p><p>49 12 4  Experiments  We describe our experiments to test the hypotheses that the following two factors improve distantsupervision quality: increasing the (1) corpus size, and (2) the amount of crowd-sourced feedback. [sent-215, score-0.23]
</p><p>50 Specifically, when using logistic regression to train relation extractors, increasing corpus size improves, consistently and significantly,  the precision and recall produced by distant supervision, regardless of human feedback levels. [sent-217, score-1.561]
</p><p>51 Using the 11We obtain the gold standard from a separate MTurk submission by taking examples that at least 10 out of 11 turkers answered yes, and then negate half of these examples by altering the relation names (e. [sent-218, score-0.25]
</p><p>52 829 methodology described in Section 3, human feedback has limited impact on the precision and recall produced from distant supervision by itself. [sent-223, score-1.648]
</p><p>53 There are two key parameters: the corpus size (#docs) M and human feedback budget (#examples) N. [sent-242, score-0.601]
</p><p>54 For each training corpus DiM, we perform ≤d iist ≤an 3t supervision rtoa tnrianing a srpetu osf D logistic regression classifiers. [sent-246, score-0.609]
</p><p>55 From the full corpus, distant supervision creates around 72K training examples. [sent-247, score-0.952]
</p><p>56 To evaluate the impact of human feedback, we randomly sample 20K examples from the input corpus (we remove any portion of the corpus that is  ×  used in an evaluation). [sent-248, score-0.45]
</p><p>57 3), and use them as the pool of human feedback that we run experiments with. [sent-252, score-0.397]
</p><p>58 Denote by N the number of examples that  Figure 2: Impact of input sizes under the TAC-KBP metric, which uses documents mentioning 100 predefined entities as testing corpus with entity-level ground truth. [sent-254, score-0.326]
</p><p>59 We vary the sizes of the training corpus and human feedback while measuring the scores (F1, recall, and precision) on the TAC-KBP benchmark. [sent-255, score-0.567]
</p><p>60 we want to incorporate human feedback for; we vary N in the range of 0, 10, 102, 103, 104, and 2 104. [sent-256, score-0.442]
</p><p>61 For each selected corpus and value o,f a N, we perform without-replacement sampling from examples of this corpus to select feedback for up to N examples. [sent-257, score-0.493]
</p><p>62 After incorporating human feedback, we evaluate the relation extractors on the TAC-KBP benchmark. [sent-261, score-0.245]
</p><p>63 Using the same metrics, we show at a larger scale that increasing corpus size can significantly improve both precision and recall. [sent-269, score-0.323]
</p><p>64 As shown in Figure 2, the F1 graph closely tracks the recall graph, which supports our earlier claim that quality is recall gated (Section 1). [sent-272, score-0.263]
</p><p>65 While increasing the corpus size improves F1 at a roughly log-linear rate, human feedback has little impact until both corpus size and human feedback size approch maximum M, N values. [sent-273, score-1.505]
</p><p>66 13 We observe that increasing the corpus size significant improves per-relation recall 13When the  corpus  size is small, the total number of exam-  ples with feedback can be smaller than the budget size N for example, when M = 103 there are on average 10 examples with feedback even if N = 104. [sent-275, score-1.316]
</p><p>67 and F1 on 17 out of TAC-KBP’s 20 relations; in contrast, human feedback has little impact on recall, and only significantly improves the precision and F1 of 9 relations while hurting F1 of 2 relations (i. [sent-281, score-0.794]
</p><p>68 0 8456e + 0 10+ 01+ 0 21+ 0+ 31+ 0+ 42+ e+4 (b) Impact of feedback size changes. [sent-286, score-0.406]
</p><p>69 05 on the impact of corpus size and feedback size changes respectively. [sent-291, score-0.719]
</p><p>70 Figure 3: Projections of Figure 2 to show the impact of corpus size and human feedback amount on TAC-KBP F1,  recall, and precision. [sent-304, score-0.738]
</p><p>71 4  Impact of Corpus Size  In Figure 3(a) we plot a projection of the graphs in Figure 2 to show the impact of corpus size on distant-supervision quality. [sent-306, score-0.343]
</p><p>72 The two curves correspond to when there is no human feedback and when we use all applicable human feedback. [sent-307, score-0.514]
</p><p>73 The fact that the two curves almost overlap indicates that human feedback had little impact on precision or recall. [sent-308, score-0.687]
</p><p>74 To measure the statistical significance of changes in F1, we calculate t-test results to compare adjacent corpus size levels given each fixed human feedback level. [sent-311, score-0.557]
</p><p>75 As shown in Table 2(a), increasing the corpus size by a factor of 10 consistently and significantly improves F1. [sent-312, score-0.273]
</p><p>76 5  Impact of Human Feedback  Figure 3(b) provides another perspective on the results under the TAC metric: We fix a corpus size and plot the F1, recall, and precision as functions of human-feedback amount. [sent-316, score-0.27]
</p><p>77 Confirming the trend in Figure 2, we see that human feedback has little 831  Figure 4: TAC-KBP quality of relation extractors trained using different amounts of human labels. [sent-317, score-0.745]
</p><p>78 impact on precision or recall with both corpus sizes. [sent-319, score-0.362]
</p><p>79 We calculate t-tests to compare adjacent human feedback levels given each fixed corpus size level. [sent-320, score-0.557]
</p><p>80 Table 2(b)’s last row reports the comparison, for various corpus sizes (and, hence, number of distant labels), of (i) using no human feedback and (ii) using all of the human feedback we collected. [sent-321, score-1.457]
</p><p>81 When the corpus size is small (fewer than 105 docs), human feedback has no statistically significant impact on F1. [sent-322, score-0.736]
</p><p>82 The locations of +’s suggest that the influence of human feedback becomes notable only when the corpus is very large (say with 106 docs). [sent-323, score-0.46]
</p><p>83 However, comparing the slopes of the curves in Figure 3(b) against Figure 3(a), the impact of human feedback is substantially smaller. [sent-324, score-0.579]
</p><p>84 The precision graph in Figure 3(b) suggests that human feedback does not no-  Figure 5: Impact of input sizes under the Freebase heldout metric. [sent-325, score-0.564]
</p><p>85 Note that the human feedback axis is in the reverse order compared to Figure 2. [sent-326, score-0.397]
</p><p>86 To assess the quality of human labels, we train extraction models with human labels only (on examples obtained from distant supervision). [sent-328, score-0.963]
</p><p>87 We vary the amount of human labels and plot the F1 changes in Figure 4. [sent-329, score-0.265]
</p><p>88 Although the F1 improves as we use more human labels, the best model has roughly the same performance as those trained from distant labels (with or without human labels). [sent-330, score-0.818]
</p><p>89 This suggests that the accuracy of human labels is not substantially better than distant labels. [sent-331, score-0.7]
</p><p>90 In Figure 5, we vary the size of the corpus in the train pair and the number of human labels; the precision reaches a dramatic peak when we the corpus size is above 105 and uses little human feedback. [sent-340, score-0.649]
</p><p>91 This suggests that this Freebase held-out metric is biased toward solely relying on distant labels alone. [sent-341, score-0.644]
</p><p>92 7 Web-scale Corpora To study how a Web corpus impacts distantsupervision quality, we select the first 100M English webpages from the ClueWeb09 dataset and measure how distant-supervision quality changes as we vary the number of webpages used. [sent-343, score-0.365]
</p><p>93 As shown in Figure 6, increasing the corpus size improves F1 up to  832  Figure 6: Impact of corpus size on the TAC-KBP quality with the ClueWeb dataset. [sent-344, score-0.508]
</p><p>94 We found that text corpus size has a stronger impact on precision and recall than human feedback. [sent-353, score-0.547]
</p><p>95 It was initially counter-intuitive to us that human labels did not have a large impact on precision. [sent-355, score-0.315]
</p><p>96 One reason is that human labels acquired from crowdsourcing have comparable noise level as distant labels as shown by Figure 4. [sent-356, score-0.774]
</p><p>97 We used a particular form of human input (yes/no votes on distant labels) and a particular statistical model to incorporate this information (logistic regression). [sent-358, score-0.651]
</p><p>98 Knowledge-based weak supervision for information extraction of overlapping relations. [sent-428, score-0.456]
</p><p>99 End-to-end relation extraction using distant supervision from external semantic repositories. [sent-460, score-1.099]
</p><p>100 A simple distant supervision approach for the TAC-KBP slot filling task. [sent-521, score-0.952]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distant', 0.538), ('supervision', 0.414), ('feedback', 0.309), ('mintz', 0.179), ('impact', 0.153), ('hoffmann', 0.142), ('crowd', 0.133), ('freebase', 0.111), ('clueweb', 0.111), ('gormley', 0.111), ('mentions', 0.108), ('yao', 0.106), ('relation', 0.105), ('tac', 0.102), ('size', 0.097), ('human', 0.088), ('increasing', 0.083), ('precision', 0.08), ('quality', 0.075), ('labels', 0.074), ('birthp', 0.074), ('lace', 0.074), ('facts', 0.069), ('logistic', 0.068), ('recall', 0.066), ('entities', 0.066), ('regression', 0.064), ('corpus', 0.063), ('sizes', 0.062), ('maltparser', 0.062), ('docs', 0.059), ('mturk', 0.059), ('ri', 0.059), ('examples', 0.058), ('distantsupervision', 0.056), ('gated', 0.056), ('springfield', 0.056), ('relations', 0.053), ('extractors', 0.052), ('condor', 0.048), ('protocols', 0.048), ('webpages', 0.048), ('surdeanu', 0.047), ('vary', 0.045), ('distantly', 0.044), ('budget', 0.044), ('nguyen', 0.042), ('seed', 0.042), ('answers', 0.042), ('extraction', 0.042), ('ensemble', 0.04), ('workers', 0.039), ('dim', 0.037), ('thain', 0.037), ('workforce', 0.037), ('scarce', 0.035), ('married', 0.032), ('patel', 0.032), ('qualify', 0.032), ('weld', 0.032), ('zhang', 0.032), ('metric', 0.032), ('database', 0.031), ('obama', 0.031), ('proceeding', 0.031), ('improves', 0.03), ('plot', 0.03), ('places', 0.03), ('mention', 0.03), ('study', 0.03), ('answered', 0.029), ('birth', 0.029), ('settles', 0.029), ('labeled', 0.029), ('curves', 0.029), ('amount', 0.028), ('little', 0.028), ('tibshirani', 0.027), ('tens', 0.027), ('trials', 0.027), ('positive', 0.027), ('entity', 0.027), ('riedel', 0.027), ('ground', 0.027), ('el', 0.026), ('statistically', 0.026), ('technical', 0.026), ('questions', 0.026), ('workflow', 0.026), ('participate', 0.026), ('born', 0.026), ('thousands', 0.026), ('magnitude', 0.026), ('big', 0.026), ('input', 0.025), ('noisy', 0.025), ('documents', 0.025), ('niu', 0.025), ('scaling', 0.025), ('pages', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="40-tfidf-1" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>Author: Ce Zhang ; Feng Niu ; Christopher Re ; Jude Shavlik</p><p>Abstract: Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.</p><p>2 0.39663377 <a title="40-tfidf-2" href="./acl-2012-Reducing_Wrong_Labels_in_Distant_Supervision_for_Relation_Extraction.html">169 acl-2012-Reducing Wrong Labels in Distant Supervision for Relation Extraction</a></p>
<p>Author: Shingo Takamatsu ; Issei Sato ; Hiroshi Nakagawa</p><p>Abstract: In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision. When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. However, this heuristic can fail with the result that some sentences are labeled wrongly. This noisy labeled data causes poor extraction performance. In this paper, we propose a method to reduce the number of wrong labels. We present a novel generative model that directly models the heuristic labeling process of distant supervision. The model predicts whether assigned labels are correct or wrong via its hidden variables. Our experimental results show that this model detected wrong labels with higher performance than baseline methods. In the ex- periment, we also found that our wrong label reduction boosted the performance of relation extraction.</p><p>3 0.33757812 <a title="40-tfidf-3" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>Author: Enrique Alfonseca ; Katja Filippova ; Jean-Yves Delort ; Guillermo Garrido</p><p>Abstract: We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision.</p><p>4 0.19381328 <a title="40-tfidf-4" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>Author: Guillermo Garrido ; Anselmo Penas ; Bernardo Cabaleiro ; Alvaro Rodrigo</p><p>Abstract: Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported.</p><p>5 0.1059195 <a title="40-tfidf-5" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>Author: Limin Yao ; Sebastian Riedel ; Andrew McCallum</p><p>Abstract: To discover relation types from text, most methods cluster shallow or syntactic patterns of relation mentions, but consider only one possible sense per pattern. In practice this assumption is often violated. In this paper we overcome this issue by inducing clusters of pattern senses from feature representations of patterns. In particular, we employ a topic model to partition entity pairs associated with patterns into sense clusters using local and global features. We merge these sense clusters into semantic relations using hierarchical agglomerative clustering. We compare against several baselines: a generative latent-variable model, a clustering method that does not disambiguate between path senses, and our own approach but with only local features. Experimental results show our proposed approach discovers dramatically more accurate clusters than models without sense disambiguation, and that incorporating global features, such as the document theme, is crucial.</p><p>6 0.097570777 <a title="40-tfidf-6" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>7 0.095139399 <a title="40-tfidf-7" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>8 0.087460823 <a title="40-tfidf-8" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>9 0.082344227 <a title="40-tfidf-9" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>10 0.080917649 <a title="40-tfidf-10" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>11 0.076189205 <a title="40-tfidf-11" href="./acl-2012-A_Graph-based_Cross-lingual_Projection_Approach_for_Weakly_Supervised_Relation_Extraction.html">12 acl-2012-A Graph-based Cross-lingual Projection Approach for Weakly Supervised Relation Extraction</a></p>
<p>12 0.075184129 <a title="40-tfidf-12" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>13 0.068180494 <a title="40-tfidf-13" href="./acl-2012-Fast_Online_Lexicon_Learning_for_Grounded_Language_Acquisition.html">93 acl-2012-Fast Online Lexicon Learning for Grounded Language Acquisition</a></p>
<p>14 0.067729898 <a title="40-tfidf-14" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>15 0.067386195 <a title="40-tfidf-15" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>16 0.064726032 <a title="40-tfidf-16" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>17 0.063671261 <a title="40-tfidf-17" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>18 0.062740609 <a title="40-tfidf-18" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>19 0.057820372 <a title="40-tfidf-19" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>20 0.057397578 <a title="40-tfidf-20" href="./acl-2012-Crowdsourcing_Inference-Rule_Evaluation.html">65 acl-2012-Crowdsourcing Inference-Rule Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.196), (1, 0.14), (2, -0.069), (3, 0.171), (4, 0.058), (5, 0.043), (6, -0.135), (7, 0.023), (8, 0.009), (9, -0.086), (10, 0.241), (11, -0.041), (12, -0.212), (13, -0.11), (14, 0.098), (15, 0.149), (16, -0.234), (17, -0.237), (18, 0.128), (19, -0.019), (20, 0.138), (21, -0.13), (22, 0.031), (23, -0.011), (24, -0.003), (25, -0.037), (26, 0.022), (27, 0.069), (28, 0.164), (29, -0.118), (30, 0.184), (31, -0.098), (32, -0.082), (33, -0.075), (34, 0.132), (35, -0.027), (36, -0.004), (37, -0.098), (38, -0.032), (39, -0.069), (40, 0.068), (41, -0.013), (42, 0.023), (43, -0.062), (44, 0.049), (45, -0.002), (46, 0.116), (47, -0.093), (48, 0.012), (49, -0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96616215 <a title="40-lsi-1" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>Author: Ce Zhang ; Feng Niu ; Christopher Re ; Jude Shavlik</p><p>Abstract: Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.</p><p>2 0.94818962 <a title="40-lsi-2" href="./acl-2012-Reducing_Wrong_Labels_in_Distant_Supervision_for_Relation_Extraction.html">169 acl-2012-Reducing Wrong Labels in Distant Supervision for Relation Extraction</a></p>
<p>Author: Shingo Takamatsu ; Issei Sato ; Hiroshi Nakagawa</p><p>Abstract: In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision. When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. However, this heuristic can fail with the result that some sentences are labeled wrongly. This noisy labeled data causes poor extraction performance. In this paper, we propose a method to reduce the number of wrong labels. We present a novel generative model that directly models the heuristic labeling process of distant supervision. The model predicts whether assigned labels are correct or wrong via its hidden variables. Our experimental results show that this model detected wrong labels with higher performance than baseline methods. In the ex- periment, we also found that our wrong label reduction boosted the performance of relation extraction.</p><p>3 0.77608645 <a title="40-lsi-3" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>Author: Enrique Alfonseca ; Katja Filippova ; Jean-Yves Delort ; Guillermo Garrido</p><p>Abstract: We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision.</p><p>4 0.43593583 <a title="40-lsi-4" href="./acl-2012-Learning_High-Level_Planning_from_Text.html">129 acl-2012-Learning High-Level Planning from Text</a></p>
<p>Author: S.R.K. Branavan ; Nate Kushman ; Tao Lei ; Regina Barzilay</p><p>Abstract: Comprehending action preconditions and effects is an essential step in modeling the dynamics of the world. In this paper, we express the semantics of precondition relations extracted from text in terms of planning operations. The challenge of modeling this connection is to ground language at the level of relations. This type of grounding enables us to create high-level plans based on language abstractions. Our model jointly learns to predict precondition relations from text and to perform high-level planning guided by those relations. We implement this idea in the reinforcement learning framework using feedback automatically obtained from plan execution attempts. When applied to a complex virtual world and text describing that world, our relation extraction technique performs on par with a supervised baseline, yielding an F-measure of 66% compared to the baseline’s 65%. Additionally, we show that a high-level planner utilizing these extracted relations significantly outperforms a strong, text unaware baseline successfully completing 80% of planning tasks as compared to 69% for the baseline.1 –</p><p>5 0.41119421 <a title="40-lsi-5" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>Author: Guillermo Garrido ; Anselmo Penas ; Bernardo Cabaleiro ; Alvaro Rodrigo</p><p>Abstract: Although much work on relation extraction has aimed at obtaining static facts, many of the target relations are actually fluents, as their validity is naturally anchored to a certain time period. This paper proposes a methodological approach to temporally anchored relation extraction. Our proposal performs distant supervised learning to extract a set of relations from a natural language corpus, and anchors each of them to an interval of temporal validity, aggregating evidence from documents supporting the relation. We use a rich graphbased document-level representation to generate novel features for this task. Results show that our implementation for temporal anchoring is able to achieve a 69% of the upper bound performance imposed by the relation extraction step. Compared to the state of the art, the overall system achieves the highest precision reported.</p><p>6 0.39476168 <a title="40-lsi-6" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>7 0.3832016 <a title="40-lsi-7" href="./acl-2012-Unsupervised_Relation_Discovery_with_Sense_Disambiguation.html">208 acl-2012-Unsupervised Relation Discovery with Sense Disambiguation</a></p>
<p>8 0.35403264 <a title="40-lsi-8" href="./acl-2012-Discriminative_Learning_for_Joint_Template_Filling.html">73 acl-2012-Discriminative Learning for Joint Template Filling</a></p>
<p>9 0.34463757 <a title="40-lsi-9" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>10 0.31727681 <a title="40-lsi-10" href="./acl-2012-A_Graph-based_Cross-lingual_Projection_Approach_for_Weakly_Supervised_Relation_Extraction.html">12 acl-2012-A Graph-based Cross-lingual Projection Approach for Weakly Supervised Relation Extraction</a></p>
<p>11 0.29475155 <a title="40-lsi-11" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>12 0.29410642 <a title="40-lsi-12" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>13 0.28596649 <a title="40-lsi-13" href="./acl-2012-A_Corpus_of_Textual_Revisions_in_Second_Language_Writing.html">8 acl-2012-A Corpus of Textual Revisions in Second Language Writing</a></p>
<p>14 0.27944273 <a title="40-lsi-14" href="./acl-2012-Labeling_Documents_with_Timestamps%3A_Learning_from_their_Time_Expressions.html">126 acl-2012-Labeling Documents with Timestamps: Learning from their Time Expressions</a></p>
<p>15 0.25858667 <a title="40-lsi-15" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>16 0.24785659 <a title="40-lsi-16" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>17 0.24505234 <a title="40-lsi-17" href="./acl-2012-A_Meta_Learning_Approach_to_Grammatical_Error_Correction.html">15 acl-2012-A Meta Learning Approach to Grammatical Error Correction</a></p>
<p>18 0.23237091 <a title="40-lsi-18" href="./acl-2012-Sentence_Dependency_Tagging_in_Online_Question_Answering_Forums.html">177 acl-2012-Sentence Dependency Tagging in Online Question Answering Forums</a></p>
<p>19 0.22792879 <a title="40-lsi-19" href="./acl-2012-Toward_Automatically_Assembling_Hittite-Language_Cuneiform_Tablet_Fragments_into_Larger_Texts.html">200 acl-2012-Toward Automatically Assembling Hittite-Language Cuneiform Tablet Fragments into Larger Texts</a></p>
<p>20 0.22505125 <a title="40-lsi-20" href="./acl-2012-Crowdsourcing_Inference-Rule_Evaluation.html">65 acl-2012-Crowdsourcing Inference-Rule Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.012), (26, 0.044), (28, 0.05), (30, 0.036), (37, 0.031), (39, 0.067), (46, 0.056), (49, 0.038), (59, 0.019), (71, 0.012), (74, 0.033), (82, 0.014), (84, 0.034), (85, 0.026), (90, 0.146), (92, 0.059), (94, 0.03), (96, 0.024), (99, 0.166)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92003924 <a title="40-lda-1" href="./acl-2012-Big_Data_versus_the_Crowd%3A_Looking_for_Relationships_in_All_the_Right_Places.html">40 acl-2012-Big Data versus the Crowd: Looking for Relationships in All the Right Places</a></p>
<p>Author: Ce Zhang ; Feng Niu ; Christopher Re ; Jude Shavlik</p><p>Abstract: Classically, training relation extractors relies on high-quality, manually annotated training data, which can be expensive to obtain. To mitigate this cost, NLU researchers have considered two newly available sources of less expensive (but potentially lower quality) labeled data from distant supervision and crowd sourcing. There is, however, no study comparing the relative impact of these two sources on the precision and recall of post-learning answers. To fill this gap, we empirically study how state-of-the-art techniques are affected by scaling these two sources. We use corpus sizes of up to 100 million documents and tens of thousands of crowd-source labeled examples. Our experiments show that increasing the corpus size for distant supervision has a statistically significant, positive impact on quality (F1 score). In contrast, human feedback has a positive and statistically significant, but lower, impact on precision and recall.</p><p>2 0.91430604 <a title="40-lda-2" href="./acl-2012-Robust_Conversion_of_CCG_Derivations_to_Phrase_Structure_Trees.html">170 acl-2012-Robust Conversion of CCG Derivations to Phrase Structure Trees</a></p>
<p>Author: Jonathan K. Kummerfeld ; Dan Klein ; James R. Curran</p><p>Abstract: We propose an improved, bottom-up method for converting CCG derivations into PTB-style phrase structure trees. In contrast with past work (Clark and Curran, 2009), which used simple transductions on category pairs, our approach uses richer transductions attached to single categories. Our conversion preserves more sentences under round-trip conversion (5 1.1% vs. 39.6%) and is more robust. In particular, unlike past methods, ours does not require ad-hoc rules over non-local features, and so can be easily integrated into a parser.</p><p>3 0.90845233 <a title="40-lda-3" href="./acl-2012-Pattern_Learning_for_Relation_Extraction_with_a_Hierarchical_Topic_Model.html">159 acl-2012-Pattern Learning for Relation Extraction with a Hierarchical Topic Model</a></p>
<p>Author: Enrique Alfonseca ; Katja Filippova ; Jean-Yves Delort ; Guillermo Garrido</p><p>Abstract: We describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations. We leverage distant supervision using relations from the knowledge base FreeBase, but do not require any manual heuristic nor manual seed list selections. Results show that the learned patterns can be used to extract new relations with good precision.</p><p>4 0.90667033 <a title="40-lda-4" href="./acl-2012-Fully_Abstractive_Approach_to_Guided_Summarization.html">101 acl-2012-Fully Abstractive Approach to Guided Summarization</a></p>
<p>Author: Pierre-Etienne Genest ; Guy Lapalme</p><p>Abstract: This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</p><p>5 0.89380229 <a title="40-lda-5" href="./acl-2012-Towards_the_Unsupervised_Acquisition_of_Discourse_Relations.html">201 acl-2012-Towards the Unsupervised Acquisition of Discourse Relations</a></p>
<p>Author: Christian Chiarcos</p><p>Abstract: This paper describes a novel approach towards the empirical approximation of discourse relations between different utterances in texts. Following the idea that every pair of events comes with preferences regarding the range and frequency of discourse relations connecting both parts, the paper investigates whether these preferences are manifested in the distribution of relation words (that serve to signal these relations). Experiments on two large-scale English web corpora show that significant correlations between pairs of adjacent events and relation words exist, that they are reproducible on different data sets, and for three relation words, that their distribution corresponds to theorybased assumptions. 1 Motivation Texts are not merely accumulations of isolated utterances, but the arrangement of utterances conveys meaning; human text understanding can thus be described as a process to recover the global structure of texts and the relations linking its different parts (Vallduv ı´ 1992; Gernsbacher et al. 2004). To capture these aspects of meaning in NLP, it is necessary to develop operationalizable theories, and, within a supervised approach, large amounts of annotated training data. To facilitate manual annotation, weakly supervised or unsupervised techniques can be applied as preprocessing step for semimanual annotation, and this is part of the motivation of the approach described here. 213 Discourse relations involve different aspects of meaning. This may include factual knowledge about the connected discourse segments (a ‘subjectmatter’ relation, e.g., if one utterance represents the cause for another, Mann and Thompson 1988, p.257), argumentative purposes (a ‘presentational’ relation, e.g., one utterance motivates the reader to accept a claim formulated in another utterance, ibid., p.257), or relations between entities mentioned in the connected discourse segments (anaphoric relations, Webber et al. 2003). Discourse relations can be indicated explicitly by optional cues, e.g., adverbials (e.g., however), conjunctions (e.g., but), or complex phrases (e.g., in contrast to what Peter said a minute ago). Here, these cues are referred to as relation words. Assuming that relation words are associated with specific discourse relations (Knott and Dale 1994; Prasad et al. 2008), the distribution of relation words found between two (types of) events can yield insights into the range of discourse relations possible at this occasion and their respective likeliness. For this purpose, this paper proposes a background knowledge base (BKB) that hosts pairs of events (here heuristically represented by verbs) along with distributional profiles for relation words. The primary data structure of the BKB is a triple where one event (type) is connected with a particular relation word to another event (type). Triples are further augmented with a frequency score (expressing the likelihood of the triple to be observed), a significance score (see below), and a correlation score (indicating whether a pair of events has a positive or negative correlation with a particular relation word). ProceedJienjgus, R ofep thueb 5lic0t hof A Knonrueaa,l M 8-e1e4ti Jnugly o f2 t0h1e2 A.s ?c so2c0ia1t2io Ans fsoorc Ciatoiomnp fuotart Cioonmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi2c 1s3–217, Triples can be easily acquired from automatically parsed corpora. While the relation word is usually part of the utterance that represents the source of the relation, determining the appropriate target (antecedent) of the relation may be difficult to achieve. As a heuristic, an adjacency preference is adopted, i.e., the target is identified with the main event of the preceding utterance.1 The BKB can be constructed from a sufficiently large corpus as follows: • • identify event types and relation words for every utterance create a candidate triple consisting of the event type of the utterance, the relation word, and the event type of the preceding utterance. add the candidate triple to the BKB, if it found in the BKB, increase its score by (or initialize it with) 1, – – • perform a pruning on all candidate triples, calcpuerlaftoer significance aonnd a lclo crarneldaitdioante scores Pruning uses statistical significance tests to evaluate whether the relative frequency of a relation word for a pair of events is significantly higher or lower than the relative frequency of the relation word in the entire corpus. Assuming that incorrect candidate triples (i.e., where the factual target of the relation was non-adjacent) are equally distributed, they should be filtered out by the significance tests. The goal of this paper is to evaluate the validity of this approach. 2 Experimental Setup By generalizing over multiple occurrences of the same events (or, more precisely, event types), one can identify preferences of event pairs for one or several relation words. These preferences capture context-invariant characteristics of pairs of events and are thus to considered to reflect a semantic predisposition for a particular discourse relation. Formally, an event is the semantic representation of the meaning conveyed in the utterance. We 1Relations between non-adjacent utterances are constrained by the structure of discourse (Webber 1991), and thus less likely than relations between adjacent utterances. 214 assume that the same event can reoccur in different contexts, we are thus studying relations between types of events. For the experiment described here, events are heuristically identified with the main predicates of a sentence, i.e., non-auxiliar, noncausative, non-modal verbal lexemes that serve as heads of main clauses. The primary data structure of the approach described here is a triple consisting of a source event, a relation word and a target (antecedent) event. These triples are harvested from large syntactically annotated corpora. For intersentential relations, the target is identified with the event of the immediately preceding main clause. These extraction preferences are heuristic approximations, and thus, an additional pruning step is necessary. For this purpose, statistical significance tests are adopted (χ2 for triples of frequent events and relation words, t-test for rare events and/or relation words) that compare the relative frequency of a rela- tion word given a pair of events with the relative frequency of the relation word in the entire corpus. All results with p ≥ .05 are excluded, i.e., only triples are preserved pfo ≥r w .0h5ic ahr teh eex xocblsuedrevde,d i positive or negative correlation between a pair of events and a relation word is not due to chance with at least 95% probability. Assuming an even distribution of incorrect target events, this should rule these out. Additionally, it also serves as a means of evaluation. Using statistical significance tests as pruning criterion entails that all triples eventually confirmed are statistically significant.2 This setup requires immense amounts of data: We are dealing with several thousand events (theoretically, the total number of verbs of a language). The chance probability for two events to occur in adjacent position is thus far below 10−6, and it decreases further if the likelihood of a relation word is taken into consideration. All things being equal, we thus need millions of sentences to create the BKB. Here, two large-scale corpora of English are employed, PukWaC and Wackypedia EN (Baroni et al. 2009). PukWaC is a 2G-token web corpus of British English crawled from the uk domain (Ferraresi et al. 2Subsequent studies may employ less rigid pruning criteria. For the purpose of the current paper, however, the statistical significance of all extracted triples serves as an criterion to evaluate methodological validity. 2008), and parsed with MaltParser (Nivre et al. 2006). It is distributed in 5 parts; Only PukWaC1 to PukWaC-4 were considered here, constituting 82.2% (72.5M sentences) of the entire corpus, PukWaC-5 is left untouched for forthcoming evaluation experiments. Wackypedia EN is a 0.8G-token dump of the English Wikipedia, annotated with the same tools. It is distributed in 4 different files; the last portion was left untouched for forthcoming evaluation experiments. The portion analyzed here comprises 33.2M sentences, 75.9% of the corpus. The extraction of events in these corpora uses simple patterns that combine dependency information and part-of-speech tags to retrieve the main verbs and store their lemmata as event types. The target (antecedent) event was identified with the last main event of the preceding sentence. As relation words, only sentence-initial children of the source event that were annotated as adverbial modifiers, verb modifiers or conjunctions were considered. 3 Evaluation To evaluate the validity of the approach, three fundamental questions need to be addressed: significance (are there significant correlations between pairs of events and relation words ?), reproducibility (can these correlations confirmed on independent data sets ?), and interpretability (can these correlations be interpreted in terms of theoretically-defined discourse relations ?). 3.1 Significance and Reproducibility Significance tests are part of the pruning stage of the algorithm. Therefore, the number of triples eventually retrieved confirms the existence of statistically significant correlations between pairs of events and relation words. The left column of Tab. 1 shows the number of triples obtained from PukWaC subcorpora of different size. For reproducibility, compare the triples identified with Wackypedia EN and PukWaC subcorpora of different size: Table 1 shows the number of triples found in both Wackypedia EN and PukWaC, and the agreement between both resources. For two triples involving the same events (event types) and the same relation word, agreement means that the relation word shows either positive or negative correlation 215 TasPbe13u7l4n2k98t. We254Mn1a c:CeAs(gurb42)et760cr8m,iop3e61r4l28np0st6uwicho21rm9W,e2673mas048p7c3okenytpdoagi21p8r,o35eE0s29Nit36nvgreipol8796r50s9%.n3509egative correlation of event pairs and relation words between Wackypedia EN and PukWaC subcorpora of different size TBH: thb ouetwnev r17 t1,o27,t0a95P41 ul2kWv6aCs,8.0 Htr5iple1v s, 45.12T35av9sg7.reH7em nv6 ts62(. %.9T2) Table 2: Agreement between but (B), however (H) and then (T) on PukWaC in both corpora, disagreement means positive correlation in one corpus and negative correlation in the other. Table 1 confirms that results obtained on one resource can be reproduced on another. This indicates that triples indeed capture context-invariant, and hence, semantic, characteristics of the relation between events. The data also indicates that reproducibility increases with the size of corpora from which a BKB is built. 3.2 Interpretability Any theory of discourse relations would predict that relation words with similar function should have similar distributions, whereas one would expect different distributions for functionally unrelated relation words. These expectations are tested here for three of the most frequent relation words found in the corpora, i.e., but, then and however. But and however can be grouped together under a generalized notion of contrast (Knott and Dale 1994; Prasad et al. 2008); then, on the other hand, indicates a tem- poral and/or causal relation. Table 2 confirms the expectation that event pairs that are correlated with but tend to show the same correlation with however, but not with then. 4 Discussion and Outlook This paper described a novel approach towards the unsupervised acquisition of discourse relations, with encouraging preliminary results: Large collections of parsed text are used to assess distributional profiles of relation words that indicate discourse relations that are possible between specific types of events; on this basis, a background knowledge base (BKB) was created that can be used to predict an appropriatediscoursemarkertoconnecttwoutterances with no overt relation word. This information can be used, for example, to facilitate the semiautomated annotation of discourse relations, by pointing out the ‘default’ relation word for a given pair of events. Similarly, Zhou et al. (2010) used a language model to predict discourse markers for implicitly realized discourse relations. As opposed to this shallow, n-gram-based approach, here, the internal structure of utterances is exploited: based on semantic considerations, syntactic patterns have been devised that extract triples of event pairs and relation words. The resulting BKB provides a distributional approximation of the discourse relations that can hold between two specific event types. Both approaches exploit complementary sources of knowledge, and may be combined with each other to achieve a more precise prediction of implicit discourse connectives. The validity of the approach was evaluated with respect to three evaluation criteria: The extracted associations between relation words and event pairs could be shown to be statistically significant, and to be reproducible on other corpora; for three highly frequent relation words, theoretical predictions about their relative distribution could be confirmed, indicating their interpretability in terms of presupposed taxonomies of discourse relations. Another prospective field of application can be seen in NLP applications, where selection preferences for relation words may serve as a cheap replacement for full-fledged discourse parsing. In the Natural Language Understanding domain, the BKB may help to disambiguate or to identify discourse relations between different events; in the context of Machine Translation, it may represent a factor guid- ing the insertion of relation words, a task that has been found to be problematic for languages that dif216 fer in their inventory and usage of discourse markers, e.g., German and English (Stede and Schmitz 2000). The approach is language-independent (except for the syntactic extraction patterns), and it does not require manually annotated data. It would thus be easy to create background knowledge bases with relation words for other languages or specific domains given a sufficient amount of textual data. – Related research includes, for example, the unsupervised recognition of causal and temporal relationships, as required, for example, for the recognition of textual entailment. Riaz and Girju (2010) exploit distributional information about pairs of utterances. Unlike approach described here, they are not restricted to adjacent utterances, and do not rely on explicit and recurrent relation words. Their approach can thus be applied to comparably small data sets. However, they are restricted to a specific type of relations whereas here the entire band- width of discourse relations that are explicitly realized in a language are covered. Prospectively, both approaches could be combined to compensate their respective weaknesses. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. However, as their approach extends beyond pairs of events to complex event chains, it seems that both approaches provide complementary types of information and their results could also be combined in a fruitful way to achieve a more detailed assessment of discourse relations. The goal of this paper was to evaluate the methdological validity of the approach. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with information provided by Riaz and Girju (2010), Chambers and Jurafsky (2009) and Kasch and Oates (2010). Other directions of subsequent research may include address more elaborate models of events, and the investigation of the relationship between relation words and taxonomies of discourse relations. Acknowledgments This work was supported by a fellowship within the Postdoc program of the German Academic Exchange Service (DAAD). Initial experiments were conducted at the Collaborative Research Center (SFB) 632 “Information Structure” at the University of Potsdam, Germany. Iwould also like to thank three anonymous reviewers for valuable comments and feedback, as well as Manfred Stede and Ed Hovy whose work on discourse relations on the one hand and proposition stores on the other hand have been the main inspiration for this paper. References M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226, 2009. N. Chambers and D. Jurafsky. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 602–610. Association for Computational Linguistics, 2009. A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernardini. Introducing and evaluating ukwac, a very large web-derived corpus of english. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54, 2008. Morton Ann Gernsbacher, Rachel R. W. Robertson, Paola Palladino, and Necia K. Werner. Managing mental representations during narrative comprehension. Discourse Processes, 37(2): 145–164, 2004. N. Kasch and T. Oates. Mining script-like structures from the web. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 34–42. Association for Computational Linguistics, 2010. A. Knott and R. Dale. Using linguistic phenomena to motivate a set ofcoherence relations. Discourse processes, 18(1):35–62, 1994. 217 J. van Kuppevelt and R. Smith, editors. Current Directions in Discourse andDialogue. Kluwer, Dordrecht, 2003. William C. Mann and Sandra A. Thompson. Rhetorical Structure Theory: Toward a functional theory of text organization. Text, 8(3):243–281, 1988. J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-generator for dependency parsing. In Proc. of LREC, pages 2216–2219. Citeseer, 2006. R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. The penn discourse treebank 2.0. In Proc. 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco, 2008. M. Riaz and R. Girju. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on, pages 361–368. IEEE, 2010. M. Stede and B. Schmitz. Discourse particles and discourse functions. Machine translation, 15(1): 125–147, 2000. Enric Vallduv ı´. The Informational Component. Garland, New York, 1992. Bonnie L. Webber. Structure and ostension in the interpretation of discourse deixis. Natural Language and Cognitive Processes, 2(6): 107–135, 1991. Bonnie L. Webber, Matthew Stone, Aravind K. Joshi, and Alistair Knott. Anaphora and discourse structure. Computational Linguistics, 4(29):545– 587, 2003. Z.-M. Zhou, Y. Xu, Z.-Y. Niu, M. Lan, J. Su, and C.L. Tan. Predicting discourse connectives for implicit discourse relation recognition. In COLING 2010, pages 1507–15 14, Beijing, China, August 2010.</p><p>6 0.89306241 <a title="40-lda-6" href="./acl-2012-Movie-DiC%3A_a_Movie_Dialogue_Corpus_for_Research_and_Development.html">149 acl-2012-Movie-DiC: a Movie Dialogue Corpus for Research and Development</a></p>
<p>7 0.88726276 <a title="40-lda-7" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>8 0.87735701 <a title="40-lda-8" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>9 0.87716746 <a title="40-lda-9" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>10 0.87351561 <a title="40-lda-10" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>11 0.85251212 <a title="40-lda-11" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>12 0.84633881 <a title="40-lda-12" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>13 0.84554666 <a title="40-lda-13" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>14 0.8442508 <a title="40-lda-14" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>15 0.84021372 <a title="40-lda-15" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>16 0.83478129 <a title="40-lda-16" href="./acl-2012-Online_Plagiarized_Detection_Through_Exploiting_Lexical%2C_Syntax%2C_and_Semantic_Information.html">156 acl-2012-Online Plagiarized Detection Through Exploiting Lexical, Syntax, and Semantic Information</a></p>
<p>17 0.83453721 <a title="40-lda-17" href="./acl-2012-BIUTEE%3A_A_Modular_Open-Source_System_for_Recognizing_Textual_Entailment.html">36 acl-2012-BIUTEE: A Modular Open-Source System for Recognizing Textual Entailment</a></p>
<p>18 0.83259678 <a title="40-lda-18" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>19 0.82857835 <a title="40-lda-19" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>20 0.82696122 <a title="40-lda-20" href="./acl-2012-Collective_Classification_for_Fine-grained_Information_Status.html">50 acl-2012-Collective Classification for Fine-grained Information Status</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
