<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-147" href="#">acl2012-147</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</h1>
<br/><p>Source: <a title="acl-2012-147-pdf" href="http://aclweb.org/anthology//P/P12/P12-1095.pdf">pdf</a></p><p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English transla- , tion tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</p><p>Reference: <a title="acl-2012-147-reference" href="../acl2012_reference/acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. [sent-2, score-1.606]
</p><p>2 The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. [sent-3, score-1.901]
</p><p>3 The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. [sent-4, score-1.928]
</p><p>4 We believe that the translation of predicates and reordering of arguments are the two central ∗Corresponding author 902 hli i r . [sent-10, score-0.969]
</p><p>5 This suggests that conventional lexical and phrasal translation models adopted in those SMT systems are not sufficient to correctly translate predicates in source sentences. [sent-16, score-0.705]
</p><p>6 Thus we propose a discriminative, feature-based predicate translation model that captures not only lexical information (i. [sent-17, score-0.837]
</p><p>7 One common error in translating arguments is about their reorderings: arguments are placed at incorrect positions after translation. [sent-21, score-0.424]
</p><p>8 In order to reduce such errors, we introduce a discriminative argument reordering model that uses the position of a predicate as the reference axis to estimate positions of its associated arguments on the target side. [sent-22, score-1.562]
</p><p>9 In this way, the model predicts moving directions of arguments relative to their predicates with semantic features. [sent-23, score-0.59]
</p><p>10 Our analysis on system outputs further reveals that they can indeed help reduce errors in predicate translations and argument reorderings. [sent-26, score-0.951]
</p><p>11 1We only consider verbal predicates in this  paper. [sent-27, score-0.505]
</p><p>12 In Section 3 and 4, we will elaborate the proposed predicate translation model and argument reordering model respectively, including details about modeling, features and training procedure. [sent-32, score-1.532]
</p><p>13 Therefore they either postpone the integration of target side PASs until the whole decoding procedure is completed (Wu and Fung, 2009b), or directly project semantic roles from the source side to the target side through word alignments during decoding (Liu and Gildea, 2010). [sent-39, score-0.828]
</p><p>14 Komachi and Matsumoto (2006) reorder arguments in source language (Japanese) sentences using heuristic rules defined on source side predicate-argument structures in a pre-processing step. [sent-41, score-0.553]
</p><p>15 (201 1) automate this procedure by automatically extracting reordering rules from predicate-argument structures and applying these rules to reorder source language sentences. [sent-43, score-0.435]
</p><p>16 Our predicate translation model is also related to previous discriminative lexicon translation models (Berger et al. [sent-47, score-1.182]
</p><p>17 While previous models predict translations for all words in vocabulary, we only focus on verbal predicates. [sent-50, score-0.425]
</p><p>18 Furthermore, the proposed translation model also dif-  fers from previous lexicon translation models in that we use both lexical and semantic features. [sent-53, score-0.73]
</p><p>19 3  Predicate Translation Model  In this section, we present the features and the training process of the predicate translation model. [sent-55, score-0.784]
</p><p>20 The essential component of our model is a maximum entropy classifier pt(e|C(v)) that predicts the target translation e for a v(ee|rCba(vl predicate v given i ttsa surrounding ncon ete fxotr C(v). [sent-59, score-0.988]
</p><p>21 Given a source sentence which contains N verbal predicates {vi}1N, our predicate translation  mNo vdeerlb Mt can cbaet dese {novt}ed as YN  Mt =  Ypt(evi|C(vi))  (2)  iY= Y1  Note that we do not restrict the target translation e to be a single word. [sent-62, score-1.616]
</p><p>22 We allow e to be a phrase of length up to 4 words so as to capture multi-word translations for a verbal predicate. [sent-63, score-0.423]
</p><p>23 If a verbal predicate is not translated, we set e = NULL so that we can also capture null translations for verbal predicates. [sent-68, score-1.255]
</p><p>24 2  Features  The apparent advantage of discriminative lexicon translation models over generative translation models (e. [sent-70, score-0.673]
</p><p>25 , 2003)) is that discriminative models allow us to integrate richer contexts  (lexical, syntactic or semantic) into target translation prediction. [sent-73, score-0.507]
</p><p>26 We use two kinds of features to predict translations for verbal predicates: 1) lexical features and 2) semantic features. [sent-74, score-0.589]
</p><p>27 ♥ = ♠  (3) where the symbol ♣ is a placeholder for a possible target t trahnes slyatmiobno (up t ios 4 a words), tldhee symbol ♥os sinibdlietcaartgese a caonnsltaetxitounal ( (lexical or semantic) ebleoml ♥en itn fdoirthe verbal predicate v, and the symbol ♠ represents tthhee vvearlbuae lo pf r♥ed. [sent-78, score-0.868]
</p><p>28 tures: The lexical element ♥ is extracted from the surrounding awlo erdlesm eofn tv ♥erba isl predicate v. [sent-80, score-0.595]
</p><p>29 We use the preceding 3 words and the succeeding 3 words to define the lexical context for the verbal predicate v. [sent-81, score-0.866]
</p><p>30 antic element ♥ is exStreamcteadn tficrom Fe atthue surrounding arguments noft verbal predicate v. [sent-84, score-1.07]
</p><p>31 In particular, we define a semantic window centered at the verbal predicate with 6 arguments {A−3, A−2 , A−1 , A1, A2, A3} where  A−3 Ae−nt1s are arguments on the left si}de w ohfe v while− −A 1A − A3 are those on the right side. [sent-85, score-1.405]
</p><p>32 Different verbal predicates have different number of arguments in different linguistic scenarios. [sent-86, score-0.717]
</p><p>33 5% verbal predicates on each side (left/right) is not larger than 3. [sent-88, score-0.668]
</p><p>34 Therefore the defined 6-argument semantic window is sufficient to describe argument contexts for predicates. [sent-89, score-0.461]
</p><p>35 For each argument Ai in the defined seman−  904 f(e, C(v)) = 1if and only if e = adjourn and C(v). [sent-90, score-0.467]
</p><p>36 If an argument  Ai dinoees se not nexticist c fonort etxhet evleemrbaeln predicate v 2r,g we set the value of both Air and Aih to null. [sent-101, score-0.844]
</p><p>37 The verbal predicate “>‹/adjourn” (in bold) has 4 arguments: one in an ARG0 agent role, one in an ARGM-ADV adverbial modifier role, one in an ARGM-TMP temporal modifier role and the last one in an ARG1 patient role. [sent-103, score-0.903]
</p><p>38 3 Training In order to train the discriminative predicate translation model, we first parse source sentences and labeled semantic roles for all verbal predicates (see details in Section 6. [sent-106, score-1.528]
</p><p>39 Then we extract all training events for verbal predicates which occur at least 10 times in the training data. [sent-108, score-0.505]
</p><p>40 A training event for a verbal predicate v consists of all contextual elements C(v) (e. [sent-109, score-0.818]
</p><p>41 Using these events, we train one maximum entropy classifier per verbal predicate (16,121 verbs in total) via the off-the-shelf MaxEnt toolkit3. [sent-112, score-0.904]
</p><p>42 We perform 100 iterations of the L-BFGS algorithm implemented in the training toolkit for each verbal predicate with both Gaussian prior and event cutoff set to 1 to avoid overfitting. [sent-113, score-0.84]
</p><p>43 After event cutoff, we have an average of 140 classes (target translations) per verbal predicate with the maximum number of classes being 9,226. [sent-114, score-0.843]
</p><p>44 4  Argument Reordering Model  In this section we introduce the discriminative argument reordering model, features and the training procedure. [sent-129, score-0.764]
</p><p>45 1 Model Since the predicate determines what arguments are  involved in its semantic frame and semantic frames tend to be cohesive across languages (Fung et al. [sent-131, score-0.931]
</p><p>46 Therefore we consider the reordering of an argument as the motion of the argument relative to its predicate. [sent-133, score-1.021]
</p><p>47 In particular, we use the position of the predicate as the reference axis. [sent-134, score-0.507]
</p><p>48 The ARG0, ARGM-ADV and ARG1 are located at the same side oftheir predicate after being translated into English, therefore the reordering category of these three arguments is assigned as “NC”. [sent-137, score-1.302]
</p><p>49 The ARGM-TMP is moved from the left side of “>‹/adjourn” to the right side of “adjourn” after translation, thus its reordering category is L2R. [sent-138, score-0.758]
</p><p>50 In order to predict the reordering category for  an argument, we propose a discriminative argument reordering model that uses a maximum en4Here we assume that the translations of arguments are not interrupted by their predicates, other arguments or any words outside the arguments in question. [sent-139, score-1.827]
</p><p>51 905 tropy classifier to calculate the reordering category m ∈ {NC, L2R, R2L} for an argument A as folmlows ∈. [sent-141, score-0.677]
</p><p>52 Given a source sentence with labeled arguments our discriminative argument reordering mntsod {eAl Mr is formulated as  {Ai}1N,  YN  Mr =  Ypr(mAi|C(Ai)) iY= Y1  (5)  4. [sent-145, score-1.047]
</p><p>53 2 Features The features fi used in the argument reordering model still takes the binary form as in Eq. [sent-146, score-0.742]
</p><p>54 Table 2 shows the features that are used in the argument reordering model. [sent-148, score-0.681]
</p><p>55 On the source side, the features include the verbal predicate, the semantic role of the argument, the head word and the boundary words of the argument. [sent-150, score-0.525]
</p><p>56 On the target side, the translation of the verbal predicate, the translation of the head word of the argument, as well as the boundary words of the translation of the argument are used as features. [sent-151, score-1.442]
</p><p>57 3 Training To train the argument reordering model, we first extract features defined in the last section from our bilingual training data where source sentences are annotated with predicate-argument structures. [sent-153, score-0.739]
</p><p>58 We also study the distribution of argument reordering categories (i. [sent-154, score-0.652]
</p><p>59 43%, are on the same side of their verbal predicates after translation. [sent-158, score-0.668]
</p><p>60 57%) are moved either from the left side of their predicates to the right side after transla-  tion (accounting for 11. [sent-164, score-0.637]
</p><p>61 19%) or from the right side to the left side of their translated predicates (accounting for 6. [sent-165, score-0.676]
</p><p>62 1 Integrating the Predicate Translation Model It is straightforward to integrate the predicate translation model into phrase-based SMT (Koehn et al. [sent-176, score-0.849]
</p><p>63 Given a source sentence with its predicateargument structure, we detect all verbal predicates and load trained predicate translation classifiers for these verbs. [sent-180, score-1.415]
</p><p>64 Whenever a hypothesis covers a new verbal predicate v, we find the target translation e for v through word alignments and then calculate its translation probability pt(e|C(v)) according to Eq. [sent-181, score-1.395]
</p><p>65 While the lexical translation model calculates the probability of a verbal predicate being translated given its local lexical context, the discriminative predicate translation model is able to employ both lexical and semantic contexts to predict translations for verbs. [sent-187, score-2.405]
</p><p>66 2  Integrating the Argument Reordering Model Before we introduce the integration algorithm for the argument reordering model, we define two functions A and N on a source sentence and its predicate-argument Nstr ounct aur eso τ as efo slelnotwensc. [sent-189, score-0.733]
</p><p>67 For example, in Figure 1, A(3, 6, τ) = {(>‹, ARGM-TMP)} wFihgiulere A(2, 3, τ) = {}, A(1, 5, τ) = {} b-TecMauPs)e} wtheh vlee Arba(l2 predicate }“,> A‹(1,” i,sτ l)oc =at {e}d boeuctasuidsee the span (2,3) and (1,5). [sent-191, score-0.507]
</p><p>68 +  We then define another funcStion Pr to calculate theW argument reordering mero fduenlc probability on all arguments which are found by the previous two functions A and N as follows. [sent-194, score-0.864]
</p><p>69 The algorithm integrates the argument reordering model into a CKY-style decoder (Xiong et al. [sent-198, score-0.686]
</p><p>70 For notational convenience, we only show the argument reordering model probability for each item, ignoring all other sub-model probabilities such as the language model probability. [sent-201, score-0.72]
</p><p>71 (7) shows how we calculate the argument reordering model probability when a lexical rule is applied to translate a source phrase c to a target phrase e. [sent-203, score-0.946]
</p><p>72 (8) shows how we compute the argument reordering model probability for a span (i, j) in a dynamic programming manner when a merging rule is applied to combine its two subspans in a straight (X → [X1, X2]) or inverted ordspera (X → hX1, X2i). [sent-205, score-0.686]
</p><p>73 The experiments are aimed at measuring the effectiveness ofthe proposed discriminative predicate translation model and argument reordering model. [sent-210, score-1.524]
</p><p>74 To train the proposed predicate translation model and argument reordering model, we first parsed all source sentences using the Berkeley Chinese parser (Petrov et al. [sent-222, score-1.499]
</p><p>75 , 2010) on all source parse trees to annotate semantic roles for all verbal predicates. [sent-224, score-0.496]
</p><p>76 2  Results  Our first group of experiments is to investigate whether the predicate translation model is able to improve translation accuracy in terms of BLEU and whether semantic features are useful. [sent-235, score-1.156]
</p><p>77 •  The proposed predicate translation models aTchheiev per an average improvement oatfi 0o. [sent-238, score-0.787]
</p><p>78 This shows that not only verbal predicates are semantically important, they also form a major part of the sentences. [sent-247, score-0.505]
</p><p>79 Therefore, whether verbal predicates are translated correctly or not has a great impact on the translation accuracy of the whole sentence 7. [sent-248, score-0.859]
</p><p>80 [X,i,jX] : P →r( cA/e(i,j,τ)) X → [X1,X2[]X o,ri h,Xj]1, :X Pr2i(A ([Xi,k1,,iτ,)k)] · : P Prr((AA((ki, +k, 1τ,)j),τ) [X) ·2P, kr( +N 1(,ij,]k :,j P,τr()A)(k + 1,j,τ)) Figure  2: Integrating  (7)  (8)  the argument reordering model into a BTG-style decoder. [sent-258, score-0.686]
</p><p>81 PTM (lex): predicate translation model with lexical features; PTM (lex+sem): predicate translation model with both lexical and semantic features; +/++: better than the baseline (p < 0. [sent-262, score-1.764]
</p><p>82 Our second group of experiments is to validate whether the argument reordering model is capable of improving translation quality. [sent-277, score-0.934]
</p><p>83 4 BLEU points on the two test sets over the baseline when we incorporate the proposed argument reordering model into our system. [sent-280, score-0.686]
</p><p>84 Finally, we integrate both the predicate translation model and argument reordering model into the final system. [sent-283, score-1.535]
</p><p>85 7  Analysis  In this section, we conduct some case studies to show how the proposed models improve translation accuracy by looking into the differences that they make on translation hypotheses. [sent-286, score-0.528]
</p><p>86 Table 6 displays a translation example which shows the difference between the baseline and the system enhanced with the predicate translation model. [sent-287, score-1.003]
</p><p>87 There are two verbal predicates “‘ /head to” and “º \/attend” in the source sentence. [sent-288, score-0.563]
</p><p>88 In order to get the most appropriate translations for these two verbal predicates, we should adopt different ways to translate them. [sent-289, score-0.437]
</p><p>89 This indicates that verbal predicate translation errors may lead to more errors, such as inappropriate reorder-  ings or lexical choices for neighboring words. [sent-295, score-1.185]
</p><p>90 On the contrary, we can see that our predicate translation model is able to help select appropriate words for both verbs. [sent-296, score-0.789]
</p><p>91 Table 7 shows another example to demonstrate how the argument reordering model improve reorderings. [sent-298, score-0.686]
</p><p>92 The ARG1 argument should be moved from the right side of the predicate to its left side after translation. [sent-301, score-1.287]
</p><p>93 The ARG0 argument can either stay on the left side or move to right side of the predicate. [sent-302, score-0.736]
</p><p>94 All of these 3 errors are avoided in the Base+ARM system output as a result of the argument reordering model that correctly identifies arguments and moves them in the right directions. [sent-309, score-0.978]
</p><p>95 The first model is the predicate translation model which employs both lexical and semantic contexts to translate verbal predicates. [sent-312, score-1.35]
</p><p>96 909  The second model is the argument reordering model which estimates the direction of argument movement relative to its predicate after translation. [sent-313, score-1.587]
</p><p>97 In the future work, we will extend our predicate translation model to translate both verbal and nominal predicates. [sent-315, score-1.144]
</p><p>98 We also want to address another translation issue of arguments as shown in Table 7: arguments are wrongly translated into separate groups instead of a cohesive unit (Wu and Fung, 2009a). [sent-317, score-0.81]
</p><p>99 Phrase reordering for statistical machine translation based on predicate-argument structure. [sent-350, score-0.563]
</p><p>100 Maximum entropy based phrase reordering model for statistical machine translation. [sent-405, score-0.415]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predicate', 0.507), ('argument', 0.337), ('reordering', 0.315), ('verbal', 0.311), ('translation', 0.248), ('arguments', 0.212), ('predicates', 0.194), ('side', 0.163), ('adjourn', 0.13), ('predicateargument', 0.097), ('ptm', 0.093), ('semantic', 0.09), ('xiong', 0.088), ('fung', 0.087), ('translated', 0.083), ('discriminative', 0.083), ('translations', 0.082), ('smt', 0.072), ('lex', 0.071), ('arm', 0.065), ('wu', 0.061), ('integrate', 0.06), ('source', 0.058), ('aanndd', 0.055), ('reorderings', 0.052), ('target', 0.05), ('berger', 0.049), ('lexical', 0.048), ('neighboring', 0.046), ('chinese', 0.045), ('pr', 0.045), ('pascale', 0.044), ('translate', 0.044), ('null', 0.044), ('moved', 0.044), ('formulated', 0.042), ('bleu', 0.042), ('left', 0.041), ('surrounding', 0.04), ('sem', 0.039), ('deyi', 0.039), ('aih', 0.037), ('aziz', 0.037), ('motions', 0.037), ('ypr', 0.037), ('moving', 0.037), ('role', 0.037), ('roles', 0.037), ('dekai', 0.036), ('entropy', 0.036), ('mauser', 0.036), ('accounting', 0.036), ('structures', 0.036), ('integrating', 0.034), ('pas', 0.034), ('contexts', 0.034), ('model', 0.034), ('phrasal', 0.033), ('tchoen', 0.032), ('cohesive', 0.032), ('motion', 0.032), ('venkatapathy', 0.032), ('right', 0.032), ('models', 0.032), ('alignments', 0.031), ('lexicon', 0.03), ('phrase', 0.03), ('btg', 0.03), ('features', 0.029), ('issued', 0.028), ('elaborate', 0.028), ('spoken', 0.027), ('fi', 0.027), ('koehn', 0.027), ('ai', 0.027), ('komachi', 0.026), ('reorder', 0.026), ('ito', 0.026), ('classifier', 0.025), ('pt', 0.025), ('errors', 0.025), ('conventional', 0.025), ('maximum', 0.025), ('bold', 0.024), ('modifier', 0.024), ('axis', 0.024), ('nc', 0.023), ('correctly', 0.023), ('unit', 0.023), ('predicts', 0.023), ('integration', 0.023), ('vi', 0.023), ('mr', 0.023), ('movement', 0.023), ('iy', 0.023), ('located', 0.022), ('cutoff', 0.022), ('pages', 0.022), ('nist', 0.022), ('air', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="147-tfidf-1" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English transla- , tion tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</p><p>2 0.35484561 <a title="147-tfidf-2" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>3 0.26345769 <a title="147-tfidf-3" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>4 0.24717472 <a title="147-tfidf-4" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>5 0.20068274 <a title="147-tfidf-5" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>6 0.17834839 <a title="147-tfidf-6" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>7 0.17576471 <a title="147-tfidf-7" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>8 0.16050683 <a title="147-tfidf-8" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>9 0.15838985 <a title="147-tfidf-9" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>10 0.15143356 <a title="147-tfidf-10" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>11 0.1495612 <a title="147-tfidf-11" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>12 0.147172 <a title="147-tfidf-12" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>13 0.14095636 <a title="147-tfidf-13" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>14 0.13073106 <a title="147-tfidf-14" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>15 0.12786545 <a title="147-tfidf-15" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>16 0.1256932 <a title="147-tfidf-16" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>17 0.12555908 <a title="147-tfidf-17" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>18 0.1242551 <a title="147-tfidf-18" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>19 0.12036063 <a title="147-tfidf-19" href="./acl-2012-Automatic_Event_Extraction_with_Structured_Preference_Modeling.html">33 acl-2012-Automatic Event Extraction with Structured Preference Modeling</a></p>
<p>20 0.11848637 <a title="147-tfidf-20" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.298), (1, -0.232), (2, 0.036), (3, 0.055), (4, 0.065), (5, -0.175), (6, -0.056), (7, 0.118), (8, 0.04), (9, 0.095), (10, 0.05), (11, -0.027), (12, 0.154), (13, -0.144), (14, -0.421), (15, 0.059), (16, -0.007), (17, -0.098), (18, 0.042), (19, -0.03), (20, -0.019), (21, 0.176), (22, -0.091), (23, 0.019), (24, -0.102), (25, 0.02), (26, 0.018), (27, 0.051), (28, -0.003), (29, -0.029), (30, 0.032), (31, 0.015), (32, 0.05), (33, -0.073), (34, 0.06), (35, 0.021), (36, -0.032), (37, 0.048), (38, 0.042), (39, 0.008), (40, 0.013), (41, -0.138), (42, -0.051), (43, 0.046), (44, 0.009), (45, 0.042), (46, -0.08), (47, -0.035), (48, 0.052), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94190049 <a title="147-lsi-1" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English transla- , tion tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</p><p>2 0.73136729 <a title="147-lsi-2" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>3 0.64588672 <a title="147-lsi-3" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>Author: Nikhil Garg ; James Henserdon</p><p>Abstract: We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering.</p><p>4 0.62393409 <a title="147-lsi-4" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Arianna Bisazza ; Marcello Federico</p><p>Abstract: This paper presents a novel method to suggest long word reorderings to a phrase-based SMT decoder. We address language pairs where long reordering concentrates on few patterns, and use fuzzy chunk-based rules to predict likely reorderings for these phenomena. Then we use reordered n-gram LMs to rank the resulting permutations and select the n-best for translation. Finally we encode these reorderings by modifying selected entries of the distortion cost matrix, on a per-sentence basis. In this way, we expand the search space by a much finer degree than if we simply raised the distortion limit. The proposed techniques are tested on Arabic-English and German-English using well-known SMT benchmarks.</p><p>5 0.58473492 <a title="147-lsi-5" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>6 0.56605566 <a title="147-lsi-6" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>7 0.56602567 <a title="147-lsi-7" href="./acl-2012-Combining_Textual_Entailment_and_Argumentation_Theory_for_Supporting_Online_Debates_Interactions.html">53 acl-2012-Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions</a></p>
<p>8 0.54246616 <a title="147-lsi-8" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>9 0.48939309 <a title="147-lsi-9" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>10 0.46442401 <a title="147-lsi-10" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>11 0.44581538 <a title="147-lsi-11" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>12 0.43909603 <a title="147-lsi-12" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>13 0.4335531 <a title="147-lsi-13" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>14 0.41485572 <a title="147-lsi-14" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>15 0.41436076 <a title="147-lsi-15" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>16 0.41396424 <a title="147-lsi-16" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>17 0.4067004 <a title="147-lsi-17" href="./acl-2012-A_Comparative_Study_of_Target_Dependency_Structures_for_Statistical_Machine_Translation.html">4 acl-2012-A Comparative Study of Target Dependency Structures for Statistical Machine Translation</a></p>
<p>18 0.39751047 <a title="147-lsi-18" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>19 0.39438254 <a title="147-lsi-19" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>20 0.38309249 <a title="147-lsi-20" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.015), (26, 0.063), (28, 0.061), (30, 0.067), (37, 0.102), (39, 0.029), (57, 0.013), (59, 0.023), (74, 0.055), (79, 0.125), (82, 0.014), (84, 0.024), (85, 0.037), (90, 0.131), (92, 0.026), (94, 0.052), (99, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83849084 <a title="147-lda-1" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>Author: Prasanth Kolachina ; Nicola Cancedda ; Marc Dymetman ; Sriram Venkatapathy</p><p>Abstract: Parallel data in the domain of interest is the key resource when training a statistical machine translation (SMT) system for a specific purpose. Since ad-hoc manual translation can represent a significant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. In this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data. We consider two scenarios, 1) Monolingual samples in the source and target languages are available and 2) An additional small amount of parallel corpus is also available. We propose methods for predicting learning curves in both these scenarios.</p><p>2 0.83582902 <a title="147-lda-2" href="./acl-2012-Qualitative_Modeling_of_Spatial_Prepositions_and_Motion_Expressions.html">166 acl-2012-Qualitative Modeling of Spatial Prepositions and Motion Expressions</a></p>
<p>Author: Inderjeet Mani ; James Pustejovsky</p><p>Abstract: unkown-abstract</p><p>same-paper 3 0.8291955 <a title="147-lda-3" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Predicate-argument structure contains rich semantic information of which statistical machine translation hasn’t taken full advantage. In this paper, we propose two discriminative, feature-based models to exploit predicateargument structures for statistical machine translation: 1) a predicate translation model and 2) an argument reordering model. The predicate translation model explores lexical and semantic contexts surrounding a verbal predicate to select desirable translations for the predicate. The argument reordering model automatically predicts the moving direction of an argument relative to its predicate after translation using semantic features. The two models are integrated into a state-of-theart phrase-based machine translation system and evaluated on Chinese-to-English transla- , tion tasks with large-scale training data. Experimental results demonstrate that the two models significantly improve translation accuracy.</p><p>4 0.78899568 <a title="147-lda-4" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>Author: Dominick Ng ; James R. Curran</p><p>Abstract: Optimising for one grammatical representation, but evaluating over a different one is a particular challenge for parsers and n-best CCG parsing. We find that this mismatch causes many n-best CCG parses to be semantically equivalent, and describe a hashing technique that eliminates this problem, improving oracle n-best F-score by 0.7% and reranking accuracy by 0.4%. We also present a comprehensive analysis of errors made by the C&C; CCG parser, providing the first breakdown of the impact of implementation decisions, such as supertagging, on parsing accuracy.</p><p>5 0.78648949 <a title="147-lda-5" href="./acl-2012-Crosslingual_Induction_of_Semantic_Roles.html">64 acl-2012-Crosslingual Induction of Semantic Roles</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We argue that multilingual parallel data provides a valuable source of indirect supervision for induction of shallow semantic representations. Specifically, we consider unsupervised induction of semantic roles from sentences annotated with automatically-predicted syntactic dependency representations and use a stateof-the-art generative Bayesian non-parametric model. At inference time, instead of only seeking the model which explains the monolingual data available for each language, we regularize the objective by introducing a soft constraint penalizing for disagreement in argument labeling on aligned sentences. We propose a simple approximate learning algorithm for our set-up which results in efficient inference. When applied to German-English parallel data, our method obtains a substantial improvement over a model trained without using the agreement signal, when both are tested on non-parallel sentences.</p><p>6 0.77120405 <a title="147-lda-6" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>7 0.76449722 <a title="147-lda-7" href="./acl-2012-Bootstrapping_via_Graph_Propagation.html">42 acl-2012-Bootstrapping via Graph Propagation</a></p>
<p>8 0.76289535 <a title="147-lda-8" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>9 0.75990796 <a title="147-lda-9" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>10 0.75974232 <a title="147-lda-10" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>11 0.75803375 <a title="147-lda-11" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>12 0.75712395 <a title="147-lda-12" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>13 0.75360632 <a title="147-lda-13" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>14 0.75274271 <a title="147-lda-14" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>15 0.75224131 <a title="147-lda-15" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>16 0.75020862 <a title="147-lda-16" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>17 0.75008768 <a title="147-lda-17" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>18 0.74584103 <a title="147-lda-18" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>19 0.74567908 <a title="147-lda-19" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>20 0.744039 <a title="147-lda-20" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
