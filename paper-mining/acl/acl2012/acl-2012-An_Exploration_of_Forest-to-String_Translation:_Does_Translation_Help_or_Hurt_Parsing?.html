<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-25" href="#">acl2012-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</h1>
<br/><p>Source: <a title="acl-2012-25-pdf" href="http://aclweb.org/anthology//P/P12/P12-2062.pdf">pdf</a></p><p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>Reference: <a title="acl-2012-25-reference" href="../acl2012_reference/acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. [sent-3, score-0.604]
</p><p>2 In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. [sent-4, score-0.741]
</p><p>3 We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank. [sent-5, score-0.886]
</p><p>4 , 2006) typically employ a pipeline of  two stages: a syntactic parser for the source language, and a decoder that translates source-language trees into target-language strings. [sent-8, score-0.547]
</p><p>5 Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chineseto-English translation (Liu et al. [sent-9, score-1.21]
</p><p>6 More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al. [sent-11, score-0.889]
</p><p>7 In this paper, we investigate two hypotheses to explain why. [sent-13, score-0.046]
</p><p>8 One hypothesis is that forest-to-string translation selects worse parses. [sent-14, score-0.546]
</p><p>9 Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al. [sent-15, score-0.564]
</p><p>10 For example, suppose that a tree-to-string system 317 David Chiang University of Southern California Information Sciences Institute  chiang@isi  . [sent-18, score-0.038]
</p><p>11 Because this subphrase is not a syntactic unit in sentence (1), the system will be unable to translate it. [sent-20, score-0.123]
</p><p>12 But a forest-tostring system would be free to choose another (incorrect but plausible) bracketing: (2) j¯ ıngj ı` [NP z¯ engzh aˇng de s `ud `u] economy growth DE rate and successfully translate it using rules learned from observed data. [sent-21, score-0.429]
</p><p>13 The other hypothesis is that forest-to-string translation selects better parses. [sent-22, score-0.461]
</p><p>14 While the parser prefers structure (3), an n-gram language model would easily prefer translation (4) and, therefore, its corresponding Chinese parse. [sent-24, score-0.604]
</p><p>15 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi3c 1s7–321,  (b)  fff  −p − −ar −s →er f f . [sent-27, score-0.085]
</p><p>16 f −d −e −c −o d−e →r  source string  (a)  source tree  target string  fff  −p − −ar −s →er f. [sent-29, score-0.458]
</p><p>17 f f −d −e −c −o d−e →r  eeee  source  source  target  eeee  string  forest  string  Figure 1: (a) In tree-to-string translation, the parser generates a single tree which the decoder must use to generate a translation. [sent-31, score-1.038]
</p><p>18 (b) In forest-to-string translation, the parser generates a forest of possible trees, any of which the decoder can use to generate a translation. [sent-32, score-0.495]
</p><p>19 Previous work has shown that an observed targetlanguage translation can improve parsing of sourcelanguage text (Burkett and Klein, 2008; Huang et al. [sent-33, score-0.647]
</p><p>20 (201 1) have explored the case where the target-language translation is unobserved. [sent-35, score-0.387]
</p><p>21 We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. [sent-37, score-0.789]
</p><p>22 Our basic conclusion, then, is that the parses that help translation (according to B ) are, on average, worse parses. [sent-38, score-0.604]
</p><p>23 Neither labeled-bracket F1 nor B is a perfect metric of the phenomena it is meant to measure, and our translation system is op-  timized to maximize B . [sent-41, score-0.604]
</p><p>24 If we optimize our system to maximize labeled-bracket F1 instead, we find that our translation system selects parses that score higher than the baseline parser’s. [sent-42, score-0.861]
</p><p>25 The parser stage can be any phrasestructure parser; it computes a parse for each sourcelanguage string. [sent-49, score-0.483]
</p><p>26 The decoder stage translates the 318 source-language tree into a target-language string, using a synchronous tree-substitution grammar. [sent-50, score-0.381]
</p><p>27 In forest-to-string translation (Figure 1b), the parser outputs a forest of possible parses of each source-language string. [sent-51, score-0.92]
</p><p>28 The decoder uses the same rules as in tree-to-string translation, but is free to select any of the trees contained in the parse forest. [sent-52, score-0.289]
</p><p>29 3  Translation hurts parsing  The simplest experiment to carry out is to examine the parses actually selected by the decoder, and see whether they are better or worse than the parses selected by the parser. [sent-53, score-0.547]
</p><p>30 If they are worse, this supports the hypothesis that syntax can hurt translation. [sent-54, score-0.122]
</p><p>31 If they are better, we can conclude that translation can help parsing. [sent-55, score-0.387]
</p><p>32 1 Setup The baseline parser is the Charniak parser (Charniak, 2000). [sent-58, score-0.434]
</p><p>33 1 The parser outputs a parse forest annotated with head words and other information. [sent-62, score-0.491]
</p><p>34 Since the decoder does not use these annotations, we use the max-rule algorithm (Petrov et al. [sent-63, score-0.134]
</p><p>35 As a side benefit, this improves parsing accuracy from 77. [sent-65, score-0.261]
</p><p>36 The weight of a hyperedge in this forest is its posterior probability, given the input string. [sent-68, score-0.181]
</p><p>37 We retain these weights as a feature in the translation  model. [sent-69, score-0.387]
</p><p>38 The decoder stage is a forest-to-string system (Liu et al. [sent-70, score-0.263]
</p><p>39 We parsed the Chinese side using the Charniak parser as described above, and performed forest-based rule extraction (Mi and Huang, 2008) with a maximum height of 3 nodes. [sent-75, score-0.497]
</p><p>40 TDersavitnPC aT rB s in 91 8g–10438617– 9 8531 514376TFNrBIaSnTsla2t0io32n  Table 1: Data used for training and testing the parsing and translation models. [sent-78, score-0.525]
</p><p>41 la01u6t7%80ion Table 2: Forest-to-string translation outperforms tree-tostring translation according to B , but the decreases parsing accuracy according to labeled-bracket F1. [sent-81, score-1.022]
</p><p>42 However, when we train to maximize labeled-bracket F1, forest-to-string translation yields better parses than both tree-to-string translation and the original parser. [sent-82, score-1.033]
</p><p>43 We used minimum-errorrate (MER) training to optimize the feature weights (Och, 2003) to maximize B . [sent-84, score-0.192]
</p><p>44 At decoding time, we select the best derivation and extract its source tree. [sent-85, score-0.074]
</p><p>45 In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried (n-best list crunching, max-rule decoding, minimum Bayes risk) did  not appear to help. [sent-86, score-0.166]
</p><p>46 In the second and third line, we see that the forestto-string system outperforms the tree-to-string system by 1. [sent-89, score-0.076]
</p><p>47 However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. [sent-93, score-0.103]
</p><p>48 This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints imposed by the syntax of the source language. [sent-94, score-0.333]
</p><p>49 319 4  Translation helps parsing  We have found that better translations can be obtained by settling for worse parses. [sent-95, score-0.363]
</p><p>50 However, translation accuracy is measured using B and parsing accuracy is measured using labeled-bracket F1, and neither of these is a perfect metric of the phe-  nomenon it is meant to measure. [sent-96, score-0.683]
</p><p>51 Moreover, we optimized the translation model in order to maximize B . [sent-97, score-0.514]
</p><p>52 It is known that when MER training is used to optimize one translation metric, other translation metrics suffer (Och, 2003); much more, then, can we expect that optimizing B will cause labeledbracket F1 to suffer. [sent-98, score-0.96]
</p><p>53 In this section, we try optimizing labeled-bracket F1, and find that, in this case, the translation model does indeed select parses that are better on average. [sent-99, score-0.555]
</p><p>54 At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n-best list of possible translation derivations (Huang and Chiang, 2005). [sent-102, score-0.83]
</p><p>55 For each derivation, we extract its Chinese parse tree and compute the number of brackets guessed and the number matched against the gold-standard parse tree. [sent-103, score-0.316]
</p><p>56 A trivial modification of the MER trainer then optimizes the feature weights to maximize labeledbracket F1. [sent-104, score-0.286]
</p><p>57 The MER trainer requires that each list contain enough unique translations (when maximizing B ) or source trees (when maximizing labeled-bracket F1). [sent-106, score-0.349]
</p><p>58 However, because one source tree may lead to many translation derivations, the n-best list may contain only a few unique source trees, or in the extreme case, the derivations may all have the same source tree. [sent-107, score-0.8]
</p><p>59 We use a variant of the n-best algorithm that allows efficient generation of equivalence classes of derivations (Huang et al. [sent-108, score-0.092]
</p><p>60 The standard algorithm works by generating, at each node of the forest, a list of the best subderivations at that node; the variant drops a subderivation if it has the same source tree as a higherscoring subderivation. [sent-110, score-0.21]
</p><p>61 (a) Increasing the maximum translation rule height increases parsing accuracy further. [sent-116, score-0.788]
</p><p>62 (b) Increasing/decreasing  the language model size increases/decreases  parsing accuracy. [sent-117, score-0.138]
</p><p>63 (c) Decreasing the parallel text size decreases parsing accuracy. [sent-118, score-0.233]
</p><p>64 (d) Removing all bilingual features decreases parsing  accuracy, but only slightly. [sent-119, score-0.241]
</p><p>65 The system trained to optimize labeled-bracket F1 (max-F1) obtains a much lower B score than the one trained to maximize B (max-B )—unsurprisingly, because a single source-side parse can yield many different translations, but the objective function scores them equally. [sent-122, score-0.32]
</p><p>66 What is more interesting is that the max-F1 system obtains a higher F1 score, not only compared with the max-B system but also the original parser. [sent-123, score-0.076]
</p><p>67 We then tried various settings to investigate what factors affect parsing performance. [sent-124, score-0.184]
</p><p>68 First, we found that increasing the maximum rule height increases F1 further (Table 3a). [sent-125, score-0.21]
</p><p>69 One of the motivations of our method is that bilingual information (especially the language model) can help disambiguate the source side structures. [sent-126, score-0.19]
</p><p>70 To test this, we varied the size ofthe corpus used to train the language model (keeping a maximum rule height of 5 from the previous experiment). [sent-127, score-0.252]
</p><p>71 In Table 3b we see that the parsing performance does increase with the language model size, with the largest language model yielding a net im-  provement of 0. [sent-129, score-0.138]
</p><p>72 To test further the importance of bilingual information, we compared against a system built only from the Chinese side of the parallel text (with each word aligned to itself). [sent-131, score-0.192]
</p><p>73 We removed all features that use bilingual information, retaining only the parser probability and the phrase penalty. [sent-132, score-0.263]
</p><p>74 In their place we added a new feature, the probability of a rule’s source side tree given its root label, which is essen320 tially the same model used in Data-Oriented Parsing (Bod, 1992). [sent-133, score-0.243]
</p><p>75 Table 3c shows that this system still outperforms the original parser. [sent-134, score-0.038]
</p><p>76 In other words, part of the gain is not attributable to translation, but additional source-side context and data that the translation model happens to capture. [sent-135, score-0.424]
</p><p>77 Finally, we varied the size of the parallel text (keeping a maximum rule height of 5 and the largest language model) and found that, as expected, parsing performance correlates with parallel data size (Table 3d). [sent-136, score-0.466]
</p><p>78 5  Conclusion  We set out to investigate why forest-to-string translation outperforms tree-to-string translation. [sent-137, score-0.433]
</p><p>79 By comparing their performance as Chinese parsers, we found that forest-to-string translation sacrifices parsing accuracy, suggesting that forest-to-string translation works by overriding constraints imposed by syntax. [sent-138, score-1.003]
</p><p>80 But when we optimized the system to maximize labeled-bracket F1, we found that, in fact, forest-to-string translation is able to achieve higher accuracy, by 0. [sent-139, score-0.552]
</p><p>81 82 F1%, than the baseline Chinese parser, demonstrating that, to a certain extent, forestto-string translation is able to correct parsing errors. [sent-140, score-0.525]
</p><p>82 Two statistical parsing models applied to the Chinese Treebank. [sent-146, score-0.138]
</p><p>83 Second Chinese Language Processing Work-  shop, pages 1–6. [sent-148, score-0.063]
</p><p>84 Is it harder to parse Chinese, or the Chinese Treebank? [sent-202, score-0.09]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('translation', 0.387), ('mer', 0.255), ('parser', 0.217), ('wedding', 0.17), ('height', 0.149), ('cousin', 0.148), ('forest', 0.144), ('parsing', 0.138), ('decoder', 0.134), ('huang', 0.133), ('parses', 0.132), ('maximize', 0.127), ('attend', 0.127), ('engzh', 0.127), ('nji', 0.127), ('unl', 0.127), ('mi', 0.125), ('chinese', 0.121), ('ud', 0.111), ('tree', 0.099), ('derivations', 0.092), ('stage', 0.091), ('parse', 0.09), ('growth', 0.087), ('worse', 0.085), ('aoji', 0.085), ('eeee', 0.085), ('fff', 0.085), ('labeledbracket', 0.085), ('ngj', 0.085), ('sourcelanguage', 0.085), ('subphrase', 0.085), ('de', 0.082), ('qun', 0.079), ('bi', 0.078), ('source', 0.074), ('trainer', 0.074), ('selects', 0.074), ('zhang', 0.073), ('liang', 0.072), ('liu', 0.071), ('side', 0.07), ('syntax', 0.068), ('aiti', 0.068), ('haizhou', 0.067), ('trees', 0.065), ('optimize', 0.065), ('hui', 0.063), ('kneser', 0.063), ('levy', 0.063), ('string', 0.063), ('pages', 0.063), ('translations', 0.062), ('rule', 0.061), ('bikel', 0.06), ('haitao', 0.06), ('hurts', 0.06), ('isi', 0.06), ('chew', 0.057), ('duan', 0.057), ('translates', 0.057), ('decreases', 0.057), ('imposed', 0.054), ('burkett', 0.054), ('hurt', 0.054), ('accuracy', 0.053), ('charniak', 0.053), ('meant', 0.052), ('southern', 0.052), ('chiang', 0.051), ('aw', 0.049), ('ctb', 0.049), ('rate', 0.048), ('economy', 0.047), ('bilingual', 0.046), ('investigate', 0.046), ('bracketing', 0.046), ('shouxun', 0.046), ('lim', 0.044), ('np', 0.044), ('varied', 0.042), ('helps', 0.041), ('chen', 0.041), ('min', 0.041), ('outputs', 0.04), ('keeping', 0.038), ('system', 0.038), ('parallel', 0.038), ('hmat', 0.037), ('attributable', 0.037), ('forestbased', 0.037), ('guessed', 0.037), ('higherscoring', 0.037), ('hyperedge', 0.037), ('sacrifices', 0.037), ('settling', 0.037), ('soften', 0.037), ('targetlanguage', 0.037), ('maximizing', 0.037), ('optimizing', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="25-tfidf-1" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>2 0.20531382 <a title="25-tfidf-2" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>3 0.20229247 <a title="25-tfidf-3" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>4 0.20103805 <a title="25-tfidf-4" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>5 0.19944832 <a title="25-tfidf-5" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>6 0.18077265 <a title="25-tfidf-6" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>7 0.17525113 <a title="25-tfidf-7" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>8 0.17386161 <a title="25-tfidf-8" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>9 0.17243892 <a title="25-tfidf-9" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>10 0.167649 <a title="25-tfidf-10" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>11 0.16697603 <a title="25-tfidf-11" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>12 0.16050683 <a title="25-tfidf-12" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>13 0.16004267 <a title="25-tfidf-13" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>14 0.158508 <a title="25-tfidf-14" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>15 0.15766585 <a title="25-tfidf-15" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>16 0.15755005 <a title="25-tfidf-16" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>17 0.15170446 <a title="25-tfidf-17" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>18 0.14496754 <a title="25-tfidf-18" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>19 0.14132561 <a title="25-tfidf-19" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>20 0.13937467 <a title="25-tfidf-20" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.346), (1, -0.299), (2, -0.023), (3, -0.085), (4, -0.026), (5, -0.176), (6, -0.009), (7, -0.023), (8, 0.058), (9, 0.006), (10, 0.101), (11, 0.024), (12, -0.051), (13, 0.026), (14, 0.074), (15, -0.06), (16, 0.011), (17, 0.071), (18, -0.009), (19, 0.015), (20, -0.049), (21, 0.018), (22, 0.014), (23, 0.031), (24, 0.044), (25, -0.038), (26, 0.005), (27, -0.067), (28, 0.055), (29, -0.098), (30, 0.045), (31, 0.008), (32, -0.07), (33, -0.007), (34, -0.111), (35, 0.098), (36, 0.045), (37, 0.032), (38, 0.032), (39, -0.071), (40, 0.131), (41, -0.032), (42, -0.031), (43, 0.004), (44, 0.05), (45, -0.039), (46, -0.109), (47, 0.014), (48, 0.01), (49, -0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97846413 <a title="25-lsi-1" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>2 0.80281132 <a title="25-lsi-2" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>3 0.77270555 <a title="25-lsi-3" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>4 0.76970917 <a title="25-lsi-4" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>5 0.76734877 <a title="25-lsi-5" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>Author: Junhui Li ; Zhaopeng Tu ; Guodong Zhou ; Josef van Genabith</p><p>Abstract: This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1</p><p>6 0.7272709 <a title="25-lsi-6" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>7 0.72304201 <a title="25-lsi-7" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>8 0.7032423 <a title="25-lsi-8" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>9 0.70088959 <a title="25-lsi-9" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>10 0.69628584 <a title="25-lsi-10" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>11 0.67048496 <a title="25-lsi-11" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>12 0.65417087 <a title="25-lsi-12" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>13 0.64797074 <a title="25-lsi-13" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>14 0.63205075 <a title="25-lsi-14" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>15 0.61579776 <a title="25-lsi-15" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>16 0.59886521 <a title="25-lsi-16" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>17 0.59288371 <a title="25-lsi-17" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>18 0.59080702 <a title="25-lsi-18" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>19 0.59003639 <a title="25-lsi-19" href="./acl-2012-Learning_to_Find_Translations_and_Transliterations_on_the_Web.html">134 acl-2012-Learning to Find Translations and Transliterations on the Web</a></p>
<p>20 0.5768159 <a title="25-lsi-20" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(9, 0.179), (15, 0.013), (26, 0.046), (28, 0.073), (30, 0.04), (37, 0.071), (39, 0.037), (57, 0.038), (71, 0.015), (74, 0.03), (82, 0.065), (84, 0.015), (85, 0.018), (90, 0.132), (92, 0.05), (94, 0.069), (99, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78380668 <a title="25-lda-1" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>2 0.6720916 <a title="25-lda-2" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Pradeep Dasigi ; Mona Diab ; Dragomir Radev</p><p>Abstract: The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.</p><p>3 0.66956472 <a title="25-lda-3" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>4 0.66909975 <a title="25-lda-4" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>5 0.66778052 <a title="25-lda-5" href="./acl-2012-Modeling_Topic_Dependencies_in_Hierarchical_Text_Categorization.html">146 acl-2012-Modeling Topic Dependencies in Hierarchical Text Categorization</a></p>
<p>Author: Alessandro Moschitti ; Qi Ju ; Richard Johansson</p><p>Abstract: In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role ofcategory relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</p><p>6 0.66488212 <a title="25-lda-6" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>7 0.66306937 <a title="25-lda-7" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>8 0.66199768 <a title="25-lda-8" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>9 0.66175473 <a title="25-lda-9" href="./acl-2012-A_Graph-based_Cross-lingual_Projection_Approach_for_Weakly_Supervised_Relation_Extraction.html">12 acl-2012-A Graph-based Cross-lingual Projection Approach for Weakly Supervised Relation Extraction</a></p>
<p>10 0.66168809 <a title="25-lda-10" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>11 0.65919662 <a title="25-lda-11" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>12 0.65908116 <a title="25-lda-12" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>13 0.65908051 <a title="25-lda-13" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>14 0.65894818 <a title="25-lda-14" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>15 0.65878755 <a title="25-lda-15" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>16 0.65797049 <a title="25-lda-16" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>17 0.65754926 <a title="25-lda-17" href="./acl-2012-Authorship_Attribution_with_Author-aware_Topic_Models.html">31 acl-2012-Authorship Attribution with Author-aware Topic Models</a></p>
<p>18 0.65619749 <a title="25-lda-18" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>19 0.65603846 <a title="25-lda-19" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>20 0.65553051 <a title="25-lda-20" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
