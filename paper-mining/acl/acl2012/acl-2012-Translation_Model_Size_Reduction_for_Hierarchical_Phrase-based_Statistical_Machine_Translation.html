<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-204" href="#">acl2012-204</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2012-204-pdf" href="http://aclweb.org/anthology//P/P12/P12-2057.pdf">pdf</a></p><p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>Reference: <a title="acl-2012-204-reference" href="../acl2012_reference/acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. [sent-10, score-1.172]
</p><p>2 Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. [sent-11, score-1.82]
</p><p>3 On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. [sent-12, score-1.442]
</p><p>4 Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance. [sent-13, score-1.419]
</p><p>5 1 Introduction  Statistical Machine Translation (SMT) has gained considerable attention during last decades. [sent-14, score-0.169]
</p><p>6 From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. [sent-15, score-0.433]
</p><p>7 , 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. [sent-17, score-0.227]
</p><p>8 This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. [sent-18, score-0.691]
</p><p>9 However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. [sent-19, score-0.946]
</p><p>10 According to Bloodgood and Callison-Burch 291 (2010) and our own preliminary experiments, the size of phrase table and hierarchical rule table consistently increases linearly with the growth of training size, while the translation performance tends to gain minor improvement after a certain point. [sent-20, score-1.253]
</p><p>11 Consequently, the model size reduction is necessary and meaningful for SMT systems if it can be performed without significant performance degradation. [sent-21, score-0.413]
</p><p>12 The  smaller the model size is, the faster the SMT decoding speed is, because there are fewer hypotheses to be investigated during decoding. [sent-22, score-0.486]
</p><p>13 Especially, in a limited environment, such as mobile device, and for a time-urgent task, such as speech-to-speech translation, the compact size oftranslation rules is required. [sent-23, score-0.357]
</p><p>14 In this case, the model reduction would be the one of the main techniques we have to consider. [sent-24, score-0.167]
</p><p>15 Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al. [sent-25, score-0.954]
</p><p>16 Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al. [sent-27, score-0.755]
</p><p>17 These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. [sent-30, score-0.963]
</p><p>18 The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. [sent-31, score-0.598]
</p><p>19 In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. [sent-32, score-0.887]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('translation', 0.303), ('unreliable', 0.296), ('entries', 0.285), ('smt', 0.259), ('translational', 0.212), ('hierarchical', 0.192), ('size', 0.184), ('redundancy', 0.177), ('infrequent', 0.163), ('reducing', 0.155), ('korea', 0.152), ('prune', 0.152), ('reduction', 0.132), ('try', 0.132), ('deteriorating', 0.116), ('seoul', 0.116), ('ineffective', 0.116), ('harm', 0.106), ('achievement', 0.106), ('entry', 0.103), ('rim', 0.099), ('device', 0.099), ('tiny', 0.099), ('phrase', 0.098), ('dongdong', 0.093), ('hn', 0.093), ('zollmann', 0.093), ('micro', 0.089), ('tries', 0.085), ('south', 0.085), ('xiang', 0.085), ('decoders', 0.082), ('unnecessary', 0.082), ('chiang', 0.08), ('degradation', 0.079), ('mu', 0.079), ('pruned', 0.076), ('bilingual', 0.072), ('minor', 0.072), ('phrasebased', 0.072), ('mobile', 0.072), ('contrary', 0.07), ('zheng', 0.07), ('linearly', 0.068), ('growth', 0.068), ('huge', 0.068), ('preserve', 0.068), ('ming', 0.066), ('gained', 0.065), ('considerable', 0.063), ('meaningful', 0.062), ('acquired', 0.058), ('investigated', 0.058), ('tends', 0.057), ('environment', 0.056), ('compact', 0.056), ('rule', 0.055), ('examined', 0.055), ('encoded', 0.055), ('cause', 0.049), ('zhou', 0.049), ('suppose', 0.049), ('caused', 0.048), ('consequently', 0.047), ('hypotheses', 0.047), ('johnson', 0.047), ('half', 0.047), ('pairs', 0.046), ('detect', 0.045), ('rules', 0.045), ('sufficient', 0.044), ('china', 0.044), ('sometimes', 0.044), ('speed', 0.043), ('translated', 0.043), ('fewer', 0.041), ('preliminary', 0.041), ('attention', 0.041), ('faster', 0.041), ('coverage', 0.041), ('gain', 0.04), ('mainly', 0.04), ('estimation', 0.039), ('koehn', 0.038), ('significance', 0.038), ('huang', 0.038), ('consistently', 0.038), ('scores', 0.038), ('benefit', 0.037), ('decoding', 0.037), ('increases', 0.037), ('noisy', 0.036), ('side', 0.036), ('yang', 0.035), ('model', 0.035), ('lee', 0.034), ('estimated', 0.034), ('engineering', 0.033), ('reduce', 0.033), ('almost', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="204-tfidf-1" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>2 0.17513779 <a title="204-tfidf-2" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>3 0.17386161 <a title="204-tfidf-3" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>4 0.16247655 <a title="204-tfidf-4" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>5 0.15130609 <a title="204-tfidf-5" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: In this paper, we demonstrate that accurate machine translation is possible without the concept of “words,” treating MT as a problem of transformation between character strings. We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character-based translation model, and using this in the phrase-based MT framework. We also propose a look-ahead parsing algorithm and substring-informed prior probabilities to achieve more effective and efficient alignment. In an evaluation, we demonstrate that character-based translation can achieve results that compare to word-based systems while effectively translating unknown and uncommon words over several language pairs.</p><p>6 0.15017906 <a title="204-tfidf-6" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>7 0.14941765 <a title="204-tfidf-7" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>8 0.1481183 <a title="204-tfidf-8" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>9 0.13700458 <a title="204-tfidf-9" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>10 0.1256932 <a title="204-tfidf-10" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>11 0.12505993 <a title="204-tfidf-11" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>12 0.12444445 <a title="204-tfidf-12" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>13 0.12350089 <a title="204-tfidf-13" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>14 0.12300408 <a title="204-tfidf-14" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>15 0.1183587 <a title="204-tfidf-15" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>16 0.11808179 <a title="204-tfidf-16" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>17 0.11716302 <a title="204-tfidf-17" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>18 0.11666989 <a title="204-tfidf-18" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>19 0.11488294 <a title="204-tfidf-19" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>20 0.11313762 <a title="204-tfidf-20" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.226), (1, -0.264), (2, 0.142), (3, 0.066), (4, 0.057), (5, -0.095), (6, -0.013), (7, 0.028), (8, 0.002), (9, -0.006), (10, 0.022), (11, 0.016), (12, -0.023), (13, 0.017), (14, 0.053), (15, 0.02), (16, -0.027), (17, 0.01), (18, 0.01), (19, -0.046), (20, -0.024), (21, 0.051), (22, -0.059), (23, 0.044), (24, 0.043), (25, -0.032), (26, -0.02), (27, -0.015), (28, 0.025), (29, -0.147), (30, 0.093), (31, -0.012), (32, -0.038), (33, 0.027), (34, -0.024), (35, 0.023), (36, 0.026), (37, 0.017), (38, 0.002), (39, -0.062), (40, -0.002), (41, -0.052), (42, 0.066), (43, -0.069), (44, -0.001), (45, -0.143), (46, -0.09), (47, 0.027), (48, 0.05), (49, -0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97544605 <a title="204-lsi-1" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>2 0.7632212 <a title="204-lsi-2" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>Author: Junhui Li ; Zhaopeng Tu ; Guodong Zhou ; Josef van Genabith</p><p>Abstract: This paper presents an extension of Chiang’s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space. Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang’s model with average gains of 1.91 points absolute in BLEU. 1</p><p>3 0.75771433 <a title="204-lsi-3" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>Author: Hui Zhang ; David Chiang</p><p>Abstract: Syntax-based translation models that operate on the output of a source-language parser have been shown to perform better if allowed to choose from a set of possible parses. In this paper, we investigate whether this is because it allows the translation stage to overcome parser errors or to override the syntactic structure itself. We find that it is primarily the latter, but that under the right conditions, the translation stage does correct parser errors, improving parsing accuracy on the Chinese Treebank.</p><p>4 0.73849082 <a title="204-lsi-4" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao ; Chunliang Zhang</p><p>Abstract: This paper presents an unsupervised approach to learning translation span alignments from parallel data that improves syntactic rule extraction by deleting spurious word alignment links and adding new valuable links based on bilingual translation span correspondences. Experiments on Chinese-English translation demonstrate improvements over standard methods for tree-to-string and tree-to-tree translation. 1</p><p>5 0.72483391 <a title="204-lsi-5" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>Author: Yang Feng ; Dongdong Zhang ; Mu Li ; Qun Liu</p><p>Abstract: We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.</p><p>6 0.72217667 <a title="204-lsi-6" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>7 0.71634316 <a title="204-lsi-7" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>8 0.69811094 <a title="204-lsi-8" href="./acl-2012-DOMCAT%3A_A_Bilingual_Concordancer_for_Domain-Specific_Computer_Assisted_Translation.html">66 acl-2012-DOMCAT: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation</a></p>
<p>9 0.68774623 <a title="204-lsi-9" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>10 0.6875295 <a title="204-lsi-10" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>11 0.67407405 <a title="204-lsi-11" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>12 0.67202628 <a title="204-lsi-12" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>13 0.66926962 <a title="204-lsi-13" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>14 0.66091073 <a title="204-lsi-14" href="./acl-2012-Private_Access_to_Phrase_Tables_for_Statistical_Machine_Translation.html">164 acl-2012-Private Access to Phrase Tables for Statistical Machine Translation</a></p>
<p>15 0.58791465 <a title="204-lsi-15" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>16 0.58134568 <a title="204-lsi-16" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>17 0.58120048 <a title="204-lsi-17" href="./acl-2012-ACCURAT_Toolkit_for_Multi-Level_Alignment_and_Information_Extraction_from_Comparable_Corpora.html">1 acl-2012-ACCURAT Toolkit for Multi-Level Alignment and Information Extraction from Comparable Corpora</a></p>
<p>18 0.57199311 <a title="204-lsi-18" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>19 0.56848985 <a title="204-lsi-19" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>20 0.56162304 <a title="204-lsi-20" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(28, 0.036), (37, 0.012), (39, 0.015), (74, 0.028), (85, 0.012), (90, 0.064), (92, 0.02), (94, 0.693), (99, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92437202 <a title="204-lda-1" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>Author: Seung-Wook Lee ; Dongdong Zhang ; Mu Li ; Ming Zhou ; Hae-Chang Rim</p><p>Abstract: In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-toEnglish machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</p><p>2 0.7348125 <a title="204-lda-2" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>3 0.72018576 <a title="204-lda-3" href="./acl-2012-Sentence_Compression_with_Semantic_Role_Constraints.html">176 acl-2012-Sentence Compression with Semantic Role Constraints</a></p>
<p>Author: Katsumasa Yoshikawa ; Ryu Iida ; Tsutomu Hirao ; Manabu Okumura</p><p>Abstract: For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques.</p><p>4 0.69964343 <a title="204-lda-4" href="./acl-2012-A_Comprehensive_Gold_Standard_for_the_Enron_Organizational_Hierarchy.html">6 acl-2012-A Comprehensive Gold Standard for the Enron Organizational Hierarchy</a></p>
<p>Author: Apoorv Agarwal ; Adinoyi Omuya ; Aaron Harnly ; Owen Rambow</p><p>Abstract: Many researchers have attempted to predict the Enron corporate hierarchy from the data. This work, however, has been hampered by a lack of data. We present a new, large, and freely available gold-standard hierarchy. Using our new gold standard, we show that a simple lower bound for social network-based systems outperforms an upper bound on the approach taken by current NLP systems.</p><p>5 0.68419772 <a title="204-lda-5" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<p>Author: JinYeong Bak ; Suin Kim ; Alice Oh</p><p>Abstract: In social psychology, it is generally accepted that one discloses more of his/her personal information to someone in a strong relationship. We present a computational framework for automatically analyzing such self-disclosure behavior in Twitter conversations. Our framework uses text mining techniques to discover topics, emotions, sentiments, lexical patterns, as well as personally identifiable information (PII) and personally embarrassing information (PEI). Our preliminary results illustrate that in relationships with high relationship strength, Twitter users show significantly more frequent behaviors of self-disclosure.</p><p>6 0.38746196 <a title="204-lda-6" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>7 0.30598503 <a title="204-lda-7" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>8 0.26842344 <a title="204-lda-8" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>9 0.26558459 <a title="204-lda-9" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>10 0.25969818 <a title="204-lda-10" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>11 0.25873807 <a title="204-lda-11" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>12 0.24803229 <a title="204-lda-12" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>13 0.24544072 <a title="204-lda-13" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>14 0.23394261 <a title="204-lda-14" href="./acl-2012-Enhancing_Statistical_Machine_Translation_with_Character_Alignment.html">81 acl-2012-Enhancing Statistical Machine Translation with Character Alignment</a></p>
<p>15 0.23226301 <a title="204-lda-15" href="./acl-2012-Joint_Learning_of_a_Dual_SMT_System_for_Paraphrase_Generation.html">125 acl-2012-Joint Learning of a Dual SMT System for Paraphrase Generation</a></p>
<p>16 0.2271741 <a title="204-lda-16" href="./acl-2012-Improve_SMT_Quality_with_Automatically_Extracted_Paraphrase_Rules.html">116 acl-2012-Improve SMT Quality with Automatically Extracted Paraphrase Rules</a></p>
<p>17 0.22513697 <a title="204-lda-17" href="./acl-2012-A_Discriminative_Hierarchical_Model_for_Fast_Coreference_at_Large_Scale.html">10 acl-2012-A Discriminative Hierarchical Model for Fast Coreference at Large Scale</a></p>
<p>18 0.22476168 <a title="204-lda-18" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>19 0.22457591 <a title="204-lda-19" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>20 0.21556501 <a title="204-lda-20" href="./acl-2012-Using_Rejuvenation_to_Improve_Particle_Filtering_for_Bayesian_Word_Segmentation.html">211 acl-2012-Using Rejuvenation to Improve Particle Filtering for Bayesian Word Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
