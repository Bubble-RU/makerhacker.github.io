<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-165" href="#">acl2012-165</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</h1>
<br/><p>Source: <a title="acl-2012-165-pdf" href="http://aclweb.org/anthology//P/P12/P12-1004.pdf">pdf</a></p><p>Author: Khe Chai Sim</p><p>Abstract: This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition (HVR), a multimodal input method designed to provide efficient text entry on modern mobile devices. HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. Experimental results show that both the word error rate and runtime factor can be re- duced by a factor of two using HVR.</p><p>Reference: <a title="acl-2012-165-reference" href="../acl2012_reference/acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. [sent-2, score-0.647]
</p><p>2 This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. [sent-3, score-0.194]
</p><p>3 In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. [sent-4, score-0.834]
</p><p>4 Currently, text entry using an onscreen virtual keyboard is the most widely adopted input method on these modern mobile devices. [sent-8, score-0.349]
</p><p>5 Unfortunately, typing with a small virtual keyboard can sometimes be cumbersome and frustratingly slow for many people. [sent-9, score-0.21]
</p><p>6 sg a virtual keyboard, it is also possible to use handwriting gestures to input text. [sent-13, score-0.295]
</p><p>7 Handwriting input offers a more convenient input method for writing systems with complex orthography, including many Asian languages such as Chinese, Japanese and Ko-  rean. [sent-14, score-0.14]
</p><p>8 However, handwriting input is not necessarily more efficient compared to keyboard input for English. [sent-15, score-0.436]
</p><p>9 Moreover, handwriting recognition is susceptible to recognition errors, too. [sent-16, score-0.316]
</p><p>10 Evidently, speech input is the preferred text entry method, provided that speech signals can be reliably and efficiently converted into texts. [sent-27, score-0.287]
</p><p>11 Unfortunately, voice input relies on automatic speech recognition (ASR) (Rabiner, 1989) technology, which requires high computational resources and is susceptible to performance degradation due to acoustic interference, such as the presence of noise. [sent-28, score-0.6]
</p><p>12 In order to improve the reliability and efficiency of ASR, Haptic Voice Recognition (HVR) was proposed by Sim (2010) as a novel multimodal input –  –  method combining both speech and touch inputs. [sent-29, score-0.311]
</p><p>13 Touch inputs are used to generate haptic events, which correspond to the initial letters of the words in the spoken utterance. [sent-30, score-0.531]
</p><p>14 , 1997), search paths which are inconsistent with the haptic events are also pruned away to achieve further reduction in the recognition search space. [sent-34, score-0.509]
</p><p>15 Furthermore, haptic events are not susceptible to acoustic distortion, making HVR more robust to noise. [sent-36, score-0.576]
</p><p>16 This paper proposes a probabilistic framework that encompasses multiple knowledge sources for combining the speech and touch inputs. [sent-37, score-0.228]
</p><p>17 In addition to the acoustic model and language model used in ASR, haptic model and partial lexical model are also introduced to facilitate the integration of more sophisticated haptic events, such as the keystrokes, into HVR. [sent-39, score-0.976]
</p><p>18 Section 2 gives an overview of existing techniques in the literature that aim at improving noise robustness for automatic speech recognition. [sent-41, score-0.208]
</p><p>19 2  Noise Robust ASR  As previously mentioned, the process of converting speech into text using ASR is error-prone, where significant performance degradation is often due to the presence of noise or other acoustic interference. [sent-47, score-0.337]
</p><p>20 Therefore, it is crucial to improve the robustness of voice input in noisy environment. [sent-48, score-0.321]
</p><p>21 Similar concept can also be applied to improve the robustness of voice input in noisy environment. [sent-53, score-0.321]
</p><p>22 Additional complementary information can be provided using other input modalities to provide cues (redundancies) to boost the recognition performance. [sent-54, score-0.189]
</p><p>23 The next section will introduce a multimodal interface that combines speech and touch inputs to improve the efficiency and noise robustness for text entry using a technique known as Haptic Voice Recognition (Sim, 2010). [sent-55, score-0.476]
</p><p>24 3  Haptic Voice Recognition (HVR)  For many voice-enabled applications, users often find voice input to be a black box that captures the users’ voice and automatically converts it into texts using ASR. [sent-56, score-0.427]
</p><p>25 This motivates the investigation of multimodal interface that tightly integrates speech input with other modalities. [sent-61, score-0.28]
</p><p>26 Haptic Voice Recognition (HVR) is a multimodal interface designed to offer users the opportunity to add his or her ‘magic touch’ in order to improve the accuracy, efficiency and robustness of voice input. [sent-62, score-0.36]
</p><p>27 HVR is designed for modern mobile devices equipped with an embedded microphone to capture speech signals and a touchscreen display to receive  touch events. [sent-63, score-0.314]
</p><p>28 The HVR interface aims to combine both speech and touch modalities to enhance speech recognition. [sent-64, score-0.326]
</p><p>29 When using an HVR interface, users will input text verbally, at the same time provide additional cues in the form of Partial Lexical Information (PLI) to guide the recognition search. [sent-65, score-0.194]
</p><p>30 Preliminary simulated experiments conducted by Sim (2010) show that potential performance improvements both in terms of recognition speed and noise robustness can be achieved using the initial letters as PLIs. [sent-69, score-0.323]
</p><p>31 For example, to enter the text “Henry will be in Boston next Friday”, the user will speak the sentence and enter the following letter sequence: ‘H’, ‘W’, ‘B’, ‘I’, ‘B’, ‘N’ and ‘F’ . [sent-70, score-0.184]
</p><p>32 These additional letter sequence is simple enough to be entered whilst speaking; and yet they provide crucial information that can significantly improve the efficiency and robustness of speech recognition. [sent-71, score-0.374]
</p><p>33 For instance, the number of letters entered can be used to constrain the number of words in the recognition output, thereby suppressing spurious insertion and deletion errors, which are  commonly observed in noisy environment. [sent-72, score-0.212]
</p><p>34 Furthermore, the identity of the letters themselves can be used to guide the search process so that partial word sequences in the search graph that do not conform to the PLIs provided by the users can be pruned away. [sent-73, score-0.175]
</p><p>35 This paper considers two methods of generating the initial letter sequence using a touchscreen. [sent-78, score-0.193]
</p><p>36 The first method requires the user to tap on the appropriate keys on an onscreen virtual keyboard to generate the desired letter sequence. [sent-79, score-0.33]
</p><p>37 Alternatively, the initial letters can be entered using handwriting gestures. [sent-82, score-0.282]
</p><p>38 In order to achieve high recognition accuracy, each letter is represented by a single-stroke gesture, so that isolated letter recognition can be performed. [sent-84, score-0.406]
</p><p>39 , hN} denote a sequence of aNn haptic f {eahtures. [sent-92, score-0.424]
</p><p>40 For the case nooft keyboard input, each hi is a discrete symbol representing one of the 26 letters. [sent-93, score-0.236]
</p><p>41 On the other hand, for handwriting input, each hi represents a sequence of 2-dimensional vectors that corresponds to the coordinates of the points of the keystroke. [sent-94, score-0.272]
</p><p>42 Therefore, the haptic voice recog-  nition problem can be defined as finding the joint optimal solution for both the word sequence, and the PLI sequence, given O and H. [sent-95, score-0.554]
</p><p>43 eS,i mWi-, larly, P(H|L) aisc tohues haptic luirkeel siehqouoedn cofe ,t Ohe. [sent-101, score-0.392]
</p><p>44 The probabilistic formulation of HVR incorporated two additional probabilities: haptic model score,  ×  p(H|L) and PLI model score, P(L|W). [sent-108, score-0.42]
</p><p>45 The role pof( Hthe|L haptic m PLoIde ml aodnde lP sLcoI mre,od Pe(l wL|iWll b)e. [sent-109, score-0.392]
</p><p>46 1 Haptic Model Similar to having an acoustic model as a statistical representation of the phoneme sequence generating the observed acoustic features, a haptic model is used to model the PLI sequence generating the observed haptic inputs, H. [sent-112, score-1.128]
</p><p>47 The haptic likelihood can sbeer vfaecdto hraipsetdic as YN  p(H|L) =  Yp(hi|li) Yi=1  (5)  where L = {li : 1 ≤ i ≤ N}. [sent-113, score-0.392]
</p><p>48 li is the ith PLI winh Ler ean Ld hi {isl th:e 1ith ≤ haptic input feature. [sent-114, score-0.588]
</p><p>49 In this work, enadc hh PLI represent the initial letter of a word. [sent-115, score-0.161]
</p><p>50 As previously mentioned, for keyboard input, hi are discrete features whose values are also one of the 26 letters. [sent-117, score-0.236]
</p><p>51 =  34 For handwriting input, hi denote a sequence of 2dimensional feature vectors, which can be modelled using Hidden Markov Models (HMMs) (Rabiner, 1989). [sent-122, score-0.272]
</p><p>52 01 oift lhier=w i nseitial letter of wi =  On the other hand, if N M, insertions and deleOtionn sth hea oveth etor hbae ntda,ke ifn Nint6o = cMo n,sid inesreartitoionn: P(li = ? [sent-130, score-0.165]
</p><p>53 This means that the word count of the HVR output matches the length of the initial letter sequence entered by the user. [sent-136, score-0.236]
</p><p>54 Furthermore, HMM-based haptic models are also used in this work to represent the single-stroke letters shown in Fig. [sent-146, score-0.452]
</p><p>55 P¯ can be viewed as a merger that defines the possible alignments w beedtween the speech and haptic inputs. [sent-153, score-0.485]
</p><p>56 Speech input and haptic input are processed separately by the ASR system and the haptic model respectively. [sent-161, score-0.924]
</p><p>57 The ASR system may  35  Figure 3: Screenshot depicting the HVR prototype operating with keyboard input  Figure 4: Screenshot depicting the HVR prototype oper-  ating with keystroke input generate multiple hypotheses of word sequences in the form of a lattice. [sent-162, score-0.44]
</p><p>58 Similarly, the haptic model may also generate a lattice containing the most probably letter sequences. [sent-163, score-0.564]
</p><p>59 This may happen if the lattices generated by the ASR system or the haptic model are not large enough to produce any valid pair of W and L. [sent-166, score-0.392]
</p><p>60 This prototype HVR interface allows both speech and haptic input data to be captured either synchronously or asynchronously and the partial lexical information can  be entered using either a soft keyboard or handwriting gestures. [sent-168, score-1.003]
</p><p>61 200 sentences were used as test data while the remaining 50 sentences were used for acoustic model adaptation. [sent-176, score-0.14]
</p><p>62 In order to investigate the robustness of HVR in noisy environment, the collected speech  data were also artificially corrupted with additive babble noise from the NOISEX database (Varga and Steeneken, 1993) to synthesise noisy speech signalto-noise (SNR) levels of 20 and 10 decibels2. [sent-179, score-0.377]
</p><p>63 1 Comparison of Input Speed Table 2 shows the speech, letter and total input speed using different input configurations. [sent-188, score-0.32]
</p><p>64 For synchronous HVR, the total input speed is the same as the speech and letter input speed since both the speech and haptic inputs are provided concurrently. [sent-189, score-1.08]
</p><p>65 According to this study, synchronous keyboard in-  put speed is 86 words per minutes (WPM). [sent-190, score-0.278]
</p><p>66 This is 2Higher SNR indicates a better speech quality 3A single Gaussian component system was used as a compromise between speed and accuracy for mobile apps. [sent-191, score-0.166]
</p><p>67 This is not surprising since key taps are much quicker to generate compared to handwriting gestures. [sent-193, score-0.15]
</p><p>68 On  the other hand, the individual speech and letter input speed are faster for asynchronous mode because users do not need to multi-task. [sent-194, score-0.515]
</p><p>69 However, since the speech and haptic inputs are provided concurrently, the resulting total input speed for asynchronous HVR is much slower compared to synchronous HVR. [sent-195, score-0.876]
</p><p>70 Therefore, synchronous HVR is potentially more efficient than asynchronous HVR. [sent-196, score-0.223]
</p><p>71 305  Table 3: WER and LER performance of ASR in different noise conditions First of all, the Word Error Rate (WER) and Letter Error Rate (LER) performances for standard ASR systems in different noise conditions are summarized in Table 3. [sent-204, score-0.198]
</p><p>72 These are results using pure ASR, without adding the haptic inputs. [sent-205, score-0.412]
</p><p>73 Table 3 also shows the system performance considering on the initial letter sequence of the recognition output. [sent-213, score-0.264]
</p><p>74 Note that the pure ASR system output contains substantial initial letter errors. [sent-215, score-0.181]
</p><p>75 For synchronous HVR, the recorded speech is expected to exhibit different characteristics since it may be influenced by concurrent haptic input. [sent-216, score-0.569]
</p><p>76 Also, the degradations caused by simultaneous keystroke input are greater. [sent-218, score-0.17]
</p><p>77 039  Table 4: WER and LER performance of synchronous HVR in different noise conditions  The performance of synchronous HVR is shown in Table 4. [sent-227, score-0.267]
</p><p>78 For keystroke input, top five letter candidates returned 37 by the handwriting recognizer were used. [sent-233, score-0.362]
</p><p>79 Therefore, in clean condition, the acoustic models are able to recover some of the errors introduced by the handwriting recognizer, bringing the LER down to as low as 0. [sent-234, score-0.314]
</p><p>80 However, in noisy conditions, the LER performance is similar to those using keyboard input. [sent-236, score-0.184]
</p><p>81 Overall, synchronous and asynchronous HVR achieved WER comparable performance. [sent-237, score-0.223]
</p><p>82 140  Table 5: WER and LER performance of asynchronous HVR in different noise conditions Similar to synchronous HVR, asynchronous HVR also achieved significant performance improvements over the pure ASR systems. [sent-243, score-0.481]
</p><p>83 Table 5 shows the WER and LER performance of asynchronous HVR in different noise conditions. [sent-244, score-0.203]
</p><p>84 The WER performance of asynchronous HVR is consistently better than that of synchronous HVR (comparing Tables 4 and 5). [sent-245, score-0.223]
</p><p>85 This is expected since the speech quality for asynchronous HVR is higher. [sent-246, score-0.232]
</p><p>86 Table 2) and the marginal WER improvements for asynchronous  HVR, synchronous HVR appears to be a better configuration. [sent-249, score-0.223]
</p><p>87 nB01chronus  HVR using integrated decoding and lattice rescoring As previously mentioned in Section 5, HVR can also be performed in two stages using lattice rescoring technique. [sent-255, score-0.215]
</p><p>88 2514 4190659 90  Table 7: WER and LER performance of integrated and rescoring synchronous HVR in different noise conditions Table 7 shows the comparison of the runtime factors and the average number of active tokens per frame for ASR and HVR systems. [sent-274, score-0.316]
</p><p>89 The runtime factor increases with de4Runtime factor is computed as the ratio between the recognition duration and the input speech duration 38 creasing SNR because the presence of noise introduces more confusions, which renders beam pruning (Ortmanns et al. [sent-279, score-0.413]
</p><p>90 In particular, synchronous HVR achieved the best runtime performance, which is roughly consistent across different noise conditions (approximately 1. [sent-283, score-0.233]
</p><p>91 Therefore, the synchronous HVR using keyboard input is robust to noisy environment, both in terms of WER and runtime performance. [sent-286, score-0.388]
</p><p>92 The runtime performance using keystroke input is also comparable to that using keyboard input (only slightly worse). [sent-287, score-0.416]
</p><p>93 Therefore, both keyboard and keystroke inputs are effective ways for entering the initial letters for HVR. [sent-288, score-0.388]
</p><p>94 The size of the iPad screen is sufficiently large to allow efficient keyboard entry. [sent-290, score-0.146]
</p><p>95 However, for devices with smaller screen, keystroke inputs may be easier to use and less error-prone. [sent-291, score-0.17]
</p><p>96 HVR offers users the option to interact with the system using touchscreen during voice input so that additional cues can be provided to improve the efficiency and robustness of voice recognition. [sent-293, score-0.556]
</p><p>97 Partial Lexical Information (PLI), such as the initial letter of the words, are used as cues to guide the recognition search process. [sent-294, score-0.252]
</p><p>98 Therefore, apart from the acoustic and language models used in conventional ASR, HVR also combines the haptic model as well as the PLI model to yield an integrated probabilistic model. [sent-295, score-0.612]
</p><p>99 Experimental results show that this framework can be used to achieve a more efficient and robust multimodal interface for text entry on modern portable devices. [sent-298, score-0.147]
</p><p>100 Haptic voice recognition: Augmenting speech modality with touch events for efficient speech recognition. [sent-369, score-0.441]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hvr', 0.658), ('haptic', 0.392), ('pli', 0.211), ('asr', 0.194), ('voice', 0.162), ('handwriting', 0.15), ('keyboard', 0.146), ('acoustic', 0.14), ('asynchronous', 0.139), ('ler', 0.137), ('letter', 0.132), ('wer', 0.108), ('speech', 0.093), ('hi', 0.09), ('synchronous', 0.084), ('keystroke', 0.08), ('touch', 0.073), ('recognition', 0.071), ('input', 0.07), ('noise', 0.064), ('letters', 0.06), ('multimodal', 0.052), ('rescoring', 0.052), ('robustness', 0.051), ('runtime', 0.05), ('ortmanns', 0.05), ('inputs', 0.05), ('speed', 0.048), ('cdel', 0.046), ('gestures', 0.046), ('wpm', 0.046), ('wfst', 0.046), ('entered', 0.043), ('devices', 0.04), ('snr', 0.04), ('lattice', 0.04), ('interface', 0.039), ('ab', 0.038), ('noisy', 0.038), ('prototype', 0.037), ('li', 0.036), ('typing', 0.035), ('le', 0.035), ('conditions', 0.035), ('cins', 0.035), ('confusions', 0.035), ('ipad', 0.035), ('plis', 0.035), ('touchscreen', 0.035), ('sources', 0.034), ('partial', 0.033), ('users', 0.033), ('wi', 0.033), ('sequence', 0.032), ('entry', 0.031), ('integrated', 0.031), ('sim', 0.031), ('acero', 0.03), ('virtual', 0.029), ('initial', 0.029), ('probabilistic', 0.028), ('modalities', 0.028), ('pruned', 0.026), ('tightly', 0.026), ('mohri', 0.026), ('enter', 0.026), ('modern', 0.025), ('mobile', 0.025), ('pruning', 0.025), ('susceptible', 0.024), ('clean', 0.024), ('conform', 0.023), ('entering', 0.023), ('gesture', 0.023), ('hinapputitcsnrwer', 0.023), ('kesy', 0.023), ('microphone', 0.023), ('onscreen', 0.023), ('plp', 0.023), ('vts', 0.023), ('xrt', 0.023), ('efficiency', 0.023), ('screenshot', 0.021), ('beam', 0.021), ('degradation', 0.021), ('conventional', 0.021), ('smartphones', 0.02), ('gales', 0.02), ('enhancement', 0.02), ('redundancies', 0.02), ('degradations', 0.02), ('varga', 0.02), ('wfsts', 0.02), ('cues', 0.02), ('pure', 0.02), ('events', 0.02), ('hmm', 0.019), ('presence', 0.019), ('integration', 0.019), ('clarkson', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="165-tfidf-1" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>Author: Khe Chai Sim</p><p>Abstract: This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition (HVR), a multimodal input method designed to provide efficient text entry on modern mobile devices. HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. Experimental results show that both the word error rate and runtime factor can be re- duced by a factor of two using HVR.</p><p>2 0.11655039 <a title="165-tfidf-2" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>3 0.10528067 <a title="165-tfidf-3" href="./acl-2012-Automated_Essay_Scoring_Based_on_Finite_State_Transducer%3A_towards_ASR_Transcription_of_Oral_English_Speech.html">32 acl-2012-Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech</a></p>
<p>Author: Xingyuan Peng ; Dengfeng Ke ; Bo Xu</p><p>Abstract: Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters.</p><p>4 0.10357515 <a title="165-tfidf-4" href="./acl-2012-A_Nonparametric_Bayesian_Approach_to_Acoustic_Model_Discovery.html">16 acl-2012-A Nonparametric Bayesian Approach to Acoustic Model Discovery</a></p>
<p>Author: Chia-ying Lee ; James Glass</p><p>Abstract: We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1% and outper- forms a language-mismatched acoustic model.</p><p>5 0.072726473 <a title="165-tfidf-5" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>Author: Yukino Baba ; Hisami Suzuki</p><p>Abstract: This paper presents a comparative study of spelling errors that are corrected as you type, vs. those that remain uncorrected. First, we generate naturally occurring online error correction data by logging users’ keystrokes, and by automatically deriving pre- and postcorrection strings from them. We then perform an analysis of this data against the errors that remain in the final text as well as across languages. Our analysis shows a clear distinction between the types of errors that are generated and those that remain uncorrected, as well as across languages.</p><p>6 0.057966929 <a title="165-tfidf-6" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>7 0.050054077 <a title="165-tfidf-7" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>8 0.049284451 <a title="165-tfidf-8" href="./acl-2012-A_Web-based_Evaluation_Framework_for_Spatial_Instruction-Giving_Systems.html">24 acl-2012-A Web-based Evaluation Framework for Spatial Instruction-Giving Systems</a></p>
<p>9 0.03915102 <a title="165-tfidf-9" href="./acl-2012-Decoding_Running_Key_Ciphers.html">68 acl-2012-Decoding Running Key Ciphers</a></p>
<p>10 0.036922481 <a title="165-tfidf-10" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>11 0.036757711 <a title="165-tfidf-11" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>12 0.036177244 <a title="165-tfidf-12" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>13 0.034446232 <a title="165-tfidf-13" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>14 0.033664823 <a title="165-tfidf-14" href="./acl-2012-Unsupervised_Semantic_Role_Induction_with_Global_Role_Ordering.html">209 acl-2012-Unsupervised Semantic Role Induction with Global Role Ordering</a></p>
<p>15 0.03285224 <a title="165-tfidf-15" href="./acl-2012-INPROwidth.3emiSS%3A_A_Component_for_Just-In-Time_Incremental_Speech_Synthesis.html">113 acl-2012-INPROwidth.3emiSS: A Component for Just-In-Time Incremental Speech Synthesis</a></p>
<p>16 0.03068438 <a title="165-tfidf-16" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>17 0.03012383 <a title="165-tfidf-17" href="./acl-2012-Efficient_Search_for_Transformation-based_Inference.html">78 acl-2012-Efficient Search for Transformation-based Inference</a></p>
<p>18 0.028827488 <a title="165-tfidf-18" href="./acl-2012-The_OpenGrm_open-source_finite-state_grammar_software_libraries.html">196 acl-2012-The OpenGrm open-source finite-state grammar software libraries</a></p>
<p>19 0.027108433 <a title="165-tfidf-19" href="./acl-2012-Spectral_Learning_of_Latent-Variable_PCFGs.html">181 acl-2012-Spectral Learning of Latent-Variable PCFGs</a></p>
<p>20 0.027084462 <a title="165-tfidf-20" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.094), (1, 0.005), (2, -0.023), (3, 0.003), (4, -0.015), (5, 0.06), (6, 0.042), (7, 0.009), (8, 0.023), (9, 0.02), (10, -0.063), (11, -0.005), (12, -0.044), (13, 0.039), (14, -0.04), (15, 0.018), (16, -0.023), (17, 0.086), (18, 0.031), (19, 0.015), (20, 0.028), (21, -0.13), (22, -0.068), (23, -0.01), (24, -0.122), (25, -0.002), (26, 0.148), (27, 0.077), (28, -0.046), (29, 0.053), (30, 0.104), (31, -0.039), (32, -0.077), (33, -0.024), (34, -0.051), (35, 0.006), (36, -0.113), (37, 0.128), (38, 0.121), (39, 0.056), (40, -0.044), (41, -0.112), (42, -0.019), (43, -0.074), (44, 0.077), (45, -0.116), (46, -0.005), (47, 0.085), (48, -0.043), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93224531 <a title="165-lsi-1" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>Author: Khe Chai Sim</p><p>Abstract: This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition (HVR), a multimodal input method designed to provide efficient text entry on modern mobile devices. HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. Experimental results show that both the word error rate and runtime factor can be re- duced by a factor of two using HVR.</p><p>2 0.76071674 <a title="165-lsi-2" href="./acl-2012-Automated_Essay_Scoring_Based_on_Finite_State_Transducer%3A_towards_ASR_Transcription_of_Oral_English_Speech.html">32 acl-2012-Automated Essay Scoring Based on Finite State Transducer: towards ASR Transcription of Oral English Speech</a></p>
<p>Author: Xingyuan Peng ; Dengfeng Ke ; Bo Xu</p><p>Abstract: Conventional Automated Essay Scoring (AES) measures may cause severe problems when directly applied in scoring Automatic Speech Recognition (ASR) transcription as they are error sensitive and unsuitable for the characteristic of ASR transcription. Therefore, we introduce a framework of Finite State Transducer (FST) to avoid the shortcomings. Compared with the Latent Semantic Analysis with Support Vector Regression (LSA-SVR) method (stands for the conventional measures), our FST method shows better performance especially towards the ASR transcription. In addition, we apply the synonyms similarity to expand the FST model. The final scoring performance reaches an acceptable level of 0.80 which is only 0.07 lower than the correlation (0.87) between human raters.</p><p>3 0.63774031 <a title="165-lsi-3" href="./acl-2012-The_OpenGrm_open-source_finite-state_grammar_software_libraries.html">196 acl-2012-The OpenGrm open-source finite-state grammar software libraries</a></p>
<p>Author: Brian Roark ; Richard Sproat ; Cyril Allauzen ; Michael Riley ; Jeffrey Sorensen ; Terry Tai</p><p>Abstract: In this paper, we present a new collection of open-source software libraries that provides command line binary utilities and library classes and functions for compiling regular expression and context-sensitive rewrite rules into finite-state transducers, and for n-gram language modeling. The OpenGrm libraries use the OpenFst library to provide an efficient encoding of grammars and general algorithms for building, modifying and applying models.</p><p>4 0.50996888 <a title="165-lsi-4" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><p>5 0.43177167 <a title="165-lsi-5" href="./acl-2012-A_Nonparametric_Bayesian_Approach_to_Acoustic_Model_Discovery.html">16 acl-2012-A Nonparametric Bayesian Approach to Acoustic Model Discovery</a></p>
<p>Author: Chia-ying Lee ; James Glass</p><p>Abstract: We investigate the problem of acoustic modeling in which prior language-specific knowledge and transcribed data are unavailable. We present an unsupervised model that simultaneously segments the speech, discovers a proper set of sub-word units (e.g., phones) and learns a Hidden Markov Model (HMM) for each induced acoustic unit. Our approach is formulated as a Dirichlet process mixture model in which each mixture is an HMM that represents a sub-word unit. We apply our model to the TIMIT corpus, and the results demonstrate that our model discovers sub-word units that are highly correlated with English phones and also produces better segmentation than the state-of-the-art unsupervised baseline. We test the quality of the learned acoustic models on a spoken term detection task. Compared to the baselines, our model improves the relative precision of top hits by at least 22.1% and outper- forms a language-mismatched acoustic model.</p><p>6 0.39469421 <a title="165-lsi-6" href="./acl-2012-Decoding_Running_Key_Ciphers.html">68 acl-2012-Decoding Running Key Ciphers</a></p>
<p>7 0.38896221 <a title="165-lsi-7" href="./acl-2012-Bootstrapping_a_Unified_Model_of_Lexical_and_Phonetic_Acquisition.html">41 acl-2012-Bootstrapping a Unified Model of Lexical and Phonetic Acquisition</a></p>
<p>8 0.35156402 <a title="165-lsi-8" href="./acl-2012-A_Broad-Coverage_Normalization_System_for_Social_Media_Language.html">2 acl-2012-A Broad-Coverage Normalization System for Social Media Language</a></p>
<p>9 0.32415318 <a title="165-lsi-9" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>10 0.31326795 <a title="165-lsi-10" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>11 0.29819193 <a title="165-lsi-11" href="./acl-2012-INPROwidth.3emiSS%3A_A_Component_for_Just-In-Time_Incremental_Speech_Synthesis.html">113 acl-2012-INPROwidth.3emiSS: A Component for Just-In-Time Incremental Speech Synthesis</a></p>
<p>12 0.28080386 <a title="165-lsi-12" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>13 0.27920642 <a title="165-lsi-13" href="./acl-2012-Iterative_Viterbi_A%2A_Algorithm_for_K-Best_Sequential_Decoding.html">121 acl-2012-Iterative Viterbi A* Algorithm for K-Best Sequential Decoding</a></p>
<p>14 0.25559539 <a title="165-lsi-14" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>15 0.24755095 <a title="165-lsi-15" href="./acl-2012-Personalized_Normalization_for_a_Multilingual_Chat_System.html">160 acl-2012-Personalized Normalization for a Multilingual Chat System</a></p>
<p>16 0.24431288 <a title="165-lsi-16" href="./acl-2012-Modeling_Sentences_in_the_Latent_Space.html">145 acl-2012-Modeling Sentences in the Latent Space</a></p>
<p>17 0.23769458 <a title="165-lsi-17" href="./acl-2012-Computational_Approaches_to_Sentence_Completion.html">56 acl-2012-Computational Approaches to Sentence Completion</a></p>
<p>18 0.23563401 <a title="165-lsi-18" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>19 0.22584657 <a title="165-lsi-19" href="./acl-2012-Private_Access_to_Phrase_Tables_for_Statistical_Machine_Translation.html">164 acl-2012-Private Access to Phrase Tables for Statistical Machine Translation</a></p>
<p>20 0.22429392 <a title="165-lsi-20" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.012), (26, 0.031), (28, 0.027), (30, 0.019), (37, 0.018), (39, 0.039), (69, 0.012), (74, 0.072), (82, 0.016), (84, 0.023), (85, 0.415), (90, 0.084), (92, 0.042), (94, 0.023), (98, 0.013), (99, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89267761 <a title="165-lda-1" href="./acl-2012-Entailment-based_Text_Exploration_with_Application_to_the_Health-care_Domain.html">82 acl-2012-Entailment-based Text Exploration with Application to the Health-care Domain</a></p>
<p>Author: Meni Adler ; Jonathan Berant ; Ido Dagan</p><p>Abstract: We present a novel text exploration model, which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration. The proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process. A user of our system can explore the result space of a query by drilling down/up from one statement to another, according to entailment relations specified by an entailment graph and an optional concept taxonomy. As a prominent use case, we apply our exploration system and illustrate its benefit on the health-care domain. To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation. 1</p><p>2 0.8717193 <a title="165-lda-2" href="./acl-2012-A_Graphical_Interface_for_MT_Evaluation_and_Error_Analysis.html">13 acl-2012-A Graphical Interface for MT Evaluation and Error Analysis</a></p>
<p>Author: Meritxell Gonzalez ; Jesus Gimenez ; Lluis Marquez</p><p>Abstract: Error analysis in machine translation is a necessary step in order to investigate the strengths and weaknesses of the MT systems under development and allow fair comparisons among them. This work presents an application that shows how a set of heterogeneous automatic metrics can be used to evaluate a test bed of automatic translations. To do so, we have set up an online graphical interface for the ASIYA toolkit, a rich repository of evaluation measures working at different linguistic levels. The current implementation of the interface shows constituency and dependency trees as well as shallow syntactic and semantic annotations, and word alignments. The intelligent visualization of the linguistic structures used by the metrics, as well as a set of navigational functionalities, may lead towards advanced methods for automatic error analysis.</p><p>same-paper 3 0.8443889 <a title="165-lda-3" href="./acl-2012-Probabilistic_Integration_of_Partial_Lexical_Information_for_Noise_Robust_Haptic_Voice_Recognition.html">165 acl-2012-Probabilistic Integration of Partial Lexical Information for Noise Robust Haptic Voice Recognition</a></p>
<p>Author: Khe Chai Sim</p><p>Abstract: This paper presents a probabilistic framework that combines multiple knowledge sources for Haptic Voice Recognition (HVR), a multimodal input method designed to provide efficient text entry on modern mobile devices. HVR extends the conventional voice input by allowing users to provide complementary partial lexical information via touch input to improve the efficiency and accuracy of voice recognition. This paper investigates the use of the initial letter of the words in the utterance as the partial lexical information. In addition to the acoustic and language models used in automatic speech recognition systems, HVR uses the haptic and partial lexical models as additional knowledge sources to reduce the recognition search space and suppress confusions. Experimental results show that both the word error rate and runtime factor can be re- duced by a factor of two using HVR.</p><p>4 0.82868069 <a title="165-lda-4" href="./acl-2012-Multilingual_WSD_with_Just_a_Few_Lines_of_Code%3A_the_BabelNet_API.html">152 acl-2012-Multilingual WSD with Just a Few Lines of Code: the BabelNet API</a></p>
<p>Author: Roberto Navigli ; Simone Paolo Ponzetto</p><p>Abstract: In this paper we present an API for programmatic access to BabelNet a wide-coverage multilingual lexical knowledge base and multilingual knowledge-rich Word Sense Disambiguation (WSD). Our aim is to provide the research community with easy-to-use tools to perform multilingual lexical semantic analysis and foster further research in this direction. – –</p><p>5 0.79626656 <a title="165-lda-5" href="./acl-2012-Fast_and_Robust_Part-of-Speech_Tagging_Using_Dynamic_Model_Selection.html">96 acl-2012-Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selection approach, coupled with a one-pass, leftto-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. this model selection approach to more sophisticated tagging improve their robustness even We believe that can be applied algorithms and further.</p><p>6 0.70198631 <a title="165-lda-6" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>7 0.54503363 <a title="165-lda-7" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>8 0.52298564 <a title="165-lda-8" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>9 0.51474416 <a title="165-lda-9" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>10 0.50110984 <a title="165-lda-10" href="./acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</a></p>
<p>11 0.49761534 <a title="165-lda-11" href="./acl-2012-Combining_Coherence_Models_and_Machine_Translation_Evaluation_Metrics_for_Summarization_Evaluation.html">52 acl-2012-Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation</a></p>
<p>12 0.49047512 <a title="165-lda-12" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>13 0.48680377 <a title="165-lda-13" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>14 0.48456052 <a title="165-lda-14" href="./acl-2012-A_Web-based_Evaluation_Framework_for_Spatial_Instruction-Giving_Systems.html">24 acl-2012-A Web-based Evaluation Framework for Spatial Instruction-Giving Systems</a></p>
<p>15 0.47781736 <a title="165-lda-15" href="./acl-2012-LetsMT%21%3A_Cloud-Based_Platform_for_Do-It-Yourself_Machine_Translation.html">138 acl-2012-LetsMT!: Cloud-Based Platform for Do-It-Yourself Machine Translation</a></p>
<p>16 0.47560206 <a title="165-lda-16" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>17 0.47254008 <a title="165-lda-17" href="./acl-2012-Learning_the_Latent_Semantics_of_a_Concept_from_its_Definition.html">132 acl-2012-Learning the Latent Semantics of a Concept from its Definition</a></p>
<p>18 0.46516913 <a title="165-lda-18" href="./acl-2012-How_Are_Spelling_Errors_Generated_and_Corrected%3F_A_Study_of_Corrected_and_Uncorrected_Spelling_Errors_Using_Keystroke_Logs.html">111 acl-2012-How Are Spelling Errors Generated and Corrected? A Study of Corrected and Uncorrected Spelling Errors Using Keystroke Logs</a></p>
<p>19 0.46397537 <a title="165-lda-19" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>20 0.45846248 <a title="165-lda-20" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
