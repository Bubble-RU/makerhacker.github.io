<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-28" href="#">acl2012-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</h1>
<br/><p>Source: <a title="acl-2012-28-pdf" href="http://aclweb.org/anthology//P/P12/P12-1036.pdf">pdf</a></p><p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Aspect extraction is a central problem in sentiment analysis. Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. By categorizing, we mean the synonymous aspects should be clustered into the same category. In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. This setting is important because categorizing aspects is a subjective task. For different application purposes, different categorizations may be needed. Some form of user guidance is desired. In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. Our experimental results show that the two proposed models are indeed able to perform the task effectively. 1</p><p>Reference: <a title="acl-2012-28-reference" href="../acl2012_reference/acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com Abstract Aspect extraction is a central problem in sentiment analysis. [sent-2, score-0.265]
</p><p>2 Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. [sent-3, score-0.293]
</p><p>3 In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. [sent-5, score-1.416]
</p><p>4 In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. [sent-9, score-0.309]
</p><p>5 1  Introduction  Aspect-based sentiment analysis is one of the main frameworks for sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008; Liu, 2012). [sent-11, score-0.484]
</p><p>6 A key task of the framework is to extract aspects of entities that have been commented in opinion documents. [sent-12, score-0.274]
</p><p>7 The first subtask extracts aspect terms from an opinion corpus. [sent-14, score-0.689]
</p><p>8 The second sub-task clusters synonymous aspect terms into categories where each category 339 Bing Liu Department of Computer Science University of Illinois at Chicago Chicago, IL 60607, USA l iub@ c s . [sent-15, score-0.623]
</p><p>9 edu  represents a single aspect, which we call an aspect category. [sent-17, score-0.523]
</p><p>10 Existing research has proposed many methods for aspect extraction. [sent-18, score-0.523]
</p><p>11 The first type only extracts aspect terms without grouping them into categories (although a subsequent step may be used for the grouping, see Section 2). [sent-20, score-0.662]
</p><p>12 The second type uses statistical topic models to extract aspects and group them at the same time in an unsupervised manner. [sent-21, score-0.292]
</p><p>13 In this work, we propose two novel statistical  models to extract and categorize aspect terms automatically given some seeds in the user interested categories. [sent-31, score-1.07]
</p><p>14 Our models also jointly model both aspects and aspect specific sentiments. [sent-33, score-0.789]
</p><p>15 ME-SAS improves SAS by using Maximum-Entropy (or Max-Ent for short) priors to help separate aspects and sentiment terms. [sent-35, score-0.497]
</p><p>16 c so2c0ia1t2io Ans fso rc Ciatoiomnp fuotart Cio nmaplu Ltiantgiounisatlic Lsi,n pgaugiestsi3c 3s9–348, In practical applications, asking users to provide some seeds is easy as they are normally experts in their trades and have a good knowledge what are important in their domains. [sent-39, score-0.351]
</p><p>17 , 2003) and joint models of aspects and sentiments in sentiment analysis in specific (e. [sent-41, score-0.662]
</p><p>18 First of all, we jointly model aspect and sentiment, while DF-LDA is only for topics/aspects. [sent-53, score-0.523]
</p><p>19 Joint modeling ensures clear separation of aspects from sentiments producing better results. [sent-54, score-0.336]
</p><p>20 Second, our way of treating seeds is also different from DF-LDA. [sent-55, score-0.351]
</p><p>21 2  Related Work  There are many existing works on aspect extraction. [sent-60, score-0.523]
</p><p>22 However, all these methods do not group extracted aspect terms into categories. [sent-73, score-0.597]
</p><p>23 Although there are works on grouping aspect terms (Carenini et al. [sent-74, score-0.662]
</p><p>24 , 2010), they all assume that aspect terms have been extracted beforehand. [sent-78, score-0.597]
</p><p>25 , 2010; Moghaddam and Ester, 2011), summarizing aspects and sentiments (Lu et al. [sent-84, score-0.336]
</p><p>26 Aspect and sentiment extraction using topic modeling come in two flavors: discovering aspect words sentiment wise (i. [sent-89, score-1.125]
</p><p>27 , discovering positive and negative aspect words and/or sentiments for each aspect without separating aspect and sentiment terms) (Lin and He, 2009; Brody and Elhadad, 2010, Jo and Oh, 2011) and separately discovering both aspects and sentiments (e. [sent-91, score-2.353]
</p><p>28 (2010) used Maximum-Entropy to train a switch variable to separate aspect and sentiment words. [sent-97, score-0.846]
</p><p>29 One problem with these existing models is that many discovered aspects are not understandable/meaningful to users. [sent-99, score-0.246]
</p><p>30 3  Proposed Seeded Models  The standard LDA and existing aspect and sentiment models (ASMs) are mostly governed by the phenomenon called “higher-order cooccurrence” (Heinrich, 2009), i. [sent-119, score-0.806]
</p><p>31 We employ seed sets to address this issue by “guiding” the model to group semantically related terms in the same aspect thus making the aspect more specific and related to the seeds (which reflect the user needs). [sent-122, score-1.81]
</p><p>32 For easy presentation, we will use aspect to mean aspect category from now on. [sent-123, score-1.046]
</p><p>33 We replace the multinomial distribution over words for each aspect (as in ASMs) with a special two-level tree structured distribution. [sent-124, score-0.551]
</p><p>34 , in hotel domain terms like stain, shower, walls in aspect  contexts1. [sent-130, score-0.631]
</p><p>35 341 Maintenance; bed, linens, pillows in aspect Cleanliness) equally probable of emission for any aspect. [sent-132, score-0.617]
</p><p>36 Upon adding the seed sets {bed, linens, pillows} and { staff, service}, the prior structure now changes to the correlated distribution in Figure 1 (b). [sent-134, score-0.243]
</p><p>37 Thus, each aspect has a top level distribution over nonseed words and seed sets. [sent-135, score-0.814]
</p><p>38 Each seed set in each aspect further has a second level distribution over seeds in that seed set. [sent-136, score-1.332]
</p><p>39 The aspect term (word) emission now requires two steps: first sampling at level one to obtain a non-seed word or a seed set. [sent-137, score-0.844]
</p><p>40 If a non-seed word is sampled we emit it else we further sample at the second seed set level and emit a seed word. [sent-138, score-0.601]
</p><p>41 This ensures that seed words together have either all high or low aspect associations. [sent-139, score-0.738]
</p><p>42 Furthermore, seed sets preserve conjugacy between related concepts and also shape more specific aspects by clustering based on higher order cooccurrences with seeds rather than only with standard one level multinomial distribution over words (or terms) alone. [sent-140, score-0.819]
</p><p>43 denote the aspect specific distribution of seeds in the seed set ? [sent-173, score-1.184]
</p><p>44 To distinguish between aspect and sentiment terms, we introduce an indicator (switch) variable ? [sent-200, score-0.765]
</p><p>45 denote the distribution of aspects and sentiments in ? [sent-228, score-0.388]
</p><p>46 Draw a distribution over terms and seed sets  For  a)  ? [sent-241, score-0.317]
</p><p>47 (Figure 1(c)) is crucial as it governs the aspect or sentiment switch. [sent-846, score-0.765]
</p><p>48 However, aspects are often more probable than sentiments in a sentence (e. [sent-886, score-0.361]
</p><p>49 , where we know the per sentence probability of aspect emission (? [sent-899, score-0.56]
</p><p>50 2  ME-SAS Model  We can further improve SAS by employing Maximum Entropy (Max-Ent) priors for aspect and sentiment switching. [sent-980, score-0.805]
</p><p>51 The motivation is that aspect and sentiment terms play different syntactic roles in a sentence. [sent-982, score-0.839]
</p><p>52 Aspect terms tend to be nouns or noun phrases while sentiment terms tend to be adjectives, adverbs, etc. [sent-983, score-0.39]
</p><p>53 Since the focus in this paper is to generate high quality aspects using seeds, we will not evaluate sentiments although both SAS and ME-SAS can also discover sentiments. [sent-1172, score-0.369]
</p><p>54 As discussed in Section 2, there are two main flavors of aspect and sentiment models. [sent-1176, score-0.797]
</p><p>55 The first flavor does not separate aspect and sentiment, and the second flavor uses a switch to perform the separation. [sent-1177, score-0.688]
</p><p>56 We use our seeds to generate constraints for DF-LDA. [sent-1181, score-0.374]
</p><p>57 To make the seeds more effective, we set the seed set worddistribution hyper-parameter γ to be much larger than βA, the hyper-parameter for the distribution over seed sets and aspect terms. [sent-1191, score-1.332]
</p><p>58 We thus computed the per-sentence probability of aspect emission (? [sent-1200, score-0.56]
</p><p>59 Of those 1000 terms if they appeared in the sentiment lexicon, they were treated as sentiment terms, else aspect terms. [sent-1217, score-1.114]
</p><p>60 Clearly, labeling words not in the sentiment lexicon as aspect terms may not always be correct. [sent-1218, score-0.839]
</p><p>61 Since ME-LDA used manually labeled training data for Max-Ent, we again randomly sampled 1000 terms from our corpus appearing at least 20 times and labeled them as aspect terms or sentiment terms, so this labeled data clearly has less noise than our automatically labeled data. [sent-1220, score-1.055]
</p><p>62 However, it is important to note that the proposed models are flexible and do not need to have seeds for every aspect/topic. [sent-1235, score-0.392]
</p><p>63 Our experiments simulate the real-life situation where the user may not know all aspects or have no seeds for some aspects. [sent-1236, score-0.614]
</p><p>64 Thus, we provided seeds only to the first 6 of the 9 aspects/topics. [sent-1237, score-0.351]
</p><p>65 We will see that without seeds for all aspects, our models not only can improve the seeded aspects but also improve the non-seeded aspects. [sent-1238, score-0.728]
</p><p>66 Table 1 shows the aspect terms and sentiment terms discovered by the 4 models for three aspects. [sent-1241, score-0.977]
</p><p>67 It is important to note that we judge the results based on how they are related to the user seeds (which represent the user need). [sent-1245, score-0.513]
</p><p>68 For SAS, ME-SAS and ME-LDA, we mark sentiment terms as errors when they are grouped under aspects as these models are supposed to separate sentiments and aspects. [sent-1248, score-0.726]
</p><p>69 For DF-LDA, the situation is different as it is not meant to separate sentiment and aspect terms, we use red italic font to indicate those adjectives which are aspect specific adjectives (see more discussion below). [sent-1249, score-1.364]
</p><p>70 We see that only providing a handful of seeds (5) for the aspect Staff, ME-SAS can discover highly specific words like manager, attendant, bartender, and janitor. [sent-1254, score-0.95]
</p><p>71 While SAS also discovers specific words benefiting from seeds, relying on Beta priors for aspect and sentiment switching was less effective. [sent-1256, score-0.848]
</p><p>72 Next in performance is ME-LDA which although produces reasonable results in general, several aspect terms are far from what the user wants based on the seeds, e. [sent-1257, score-0.678]
</p><p>73 As DF-LDA is not meant to perform extraction and to group both aspect and sentiment terms, we relax the errors of DF-LDA due to correct aspect specific sentiments (e. [sent-1263, score-1.508]
</p><p>74 , friendly, helpful for Staff  are correct aspect specific  sentiments,  but still  345 regard incorrect sentiments like front, comfortable, large as errors) placed in aspect models. [sent-1265, score-1.243]
</p><p>75 Instead our focus is to evaluate how well our learned aspects perform in clustering specific terms guided by seeds. [sent-1272, score-0.299]
</p><p>76 Note again we do not evaluate sentiment terms as they are not the focus of this paper Since aspects produced by the models are rankings and we do not know the number of correct aspect terms, a natural way to evaluate these rankings is to use precision @ n (or p@n), where n is a rank position. [sent-1274, score-1.062]
</p><p>77 number of seeds, we want to see the effect of the number of seeds on aspect discovery. [sent-1276, score-0.874]
</p><p>78 The first average was taken over all combinations of actual seeds selected for each aspect, e. [sent-1280, score-0.351]
</p><p>79 , when the number of seeds is 3, out of the 5 seeds in each aspect, all ? [sent-1282, score-0.702]
</p><p>80 combinations of seeds were tried and the results averaged. [sent-1284, score-0.351]
</p><p>81 We start with 2 seeds and progressively increase them to 5. [sent-1286, score-0.375]
</p><p>82 Using only 1 seed per seed set (or per aspect) has practically no effect because the top level distribution ? [sent-1287, score-0.458]
</p><p>83 encodes which seed sets (and nonseed words) to include; the lower-level distribution Ω constrains the probabilities of the seed words to be correlated for each of the seed sets. [sent-1289, score-0.721]
</p><p>84 Thus, having only one seed per seed set will result in sampling that single word whenever that seed set is chosen which will not have the effect of correlating seed words so as to pull other words based on cooccurrence with constrained seed words. [sent-1290, score-1.103]
</p><p>85 From Table 2, we can see that for all models p@n progressively improves as the number of seeds increases. [sent-1291, score-0.416]
</p><p>86 6 A qualitative evaluation of sentiment extraction based on Table  1 yields  the following order: ME-SAS, SAS, ME-LDA. [sent-1293, score-0.29]
</p><p>87 7 3206 Effect of seeds on non-seeded aspects: Here we compare all models aspect wise and see the results of seeded models SAS and ME-SAS on nonseeded aspects (Table 3). [sent-1308, score-1.34]
</p><p>88 We see that across all the first 6 aspects with (5) seeds ME-SAS outperforms all other models by large margins in all top 3 ranked buckets p@ 10, p@20 and p@30. [sent-1310, score-0.574]
</p><p>89 For the last three aspects which did not have any seed guidance, we find something interesting. [sent-1312, score-0.425]
</p><p>90 This is because as seeds facilitate  clustering specific and appropriate terms in seeded aspects, which in turn improves precision on nonseeded aspects. [sent-1314, score-0.67]
</p><p>91 In aspect Staff of ME-LDA, we find pillow and linens being clustered. [sent-1316, score-0.733]
</p><p>92 This is not a “flaw” of the model per se, but the point here is pillow and linens happen to co-occur many times with other words like maintenance, staff, and upkeep because “room-service” generally includes staff members coming and replacing linens and pillow covers. [sent-1317, score-0.546]
</p><p>93 Although pillow and linens are related to Staff, strictly speaking they are semantically incorrect because they do not represent the very concept “Staff” based on the seeds (which reflect the user need). [sent-1318, score-0.642]
</p><p>94 346 seed sets in SAS and ME-SAS result in pulling such words as linens and pillow (due to seeds like beds and cleanliness in the aspect Cleanliness) and ranking them higher in the aspect Cleanliness (see Table 1) where they make more sense than Staff. [sent-1321, score-1.931]
</p><p>95 In summary, the averages over all aspects (Table 3 last row) show that the proposed seeded models SAS and ME-SAS outperform ME-LDA, DF-LDA and even DF-LDA-Relaxed considerably. [sent-1323, score-0.377]
</p><p>96 5  Conclusion  This paper studied the issue of using seeds to discover aspects in an opinion corpus. [sent-1324, score-0.658]
</p><p>97 To solve this problem, we proposed two models SAS and MESAS which take seeds reflecting the user needs to discover specific aspects. [sent-1328, score-0.549]
</p><p>98 Aspect and sentiment unification model for online review analysis. [sent-1445, score-0.282]
</p><p>99 Topic sentiment mixture: modeling facets and opinions in weblogs. [sent-1506, score-0.273]
</p><p>100 Latent aspect rating analysis on review text data: a rating regression approach. [sent-1569, score-0.609]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('aspect', 0.523), ('sas', 0.387), ('seeds', 0.351), ('sentiment', 0.242), ('seed', 0.215), ('aspects', 0.182), ('seeded', 0.154), ('sentiments', 0.154), ('linens', 0.129), ('staff', 0.126), ('zhai', 0.111), ('opinion', 0.092), ('andrzejewski', 0.09), ('user', 0.081), ('cleanliness', 0.081), ('pillow', 0.081), ('terms', 0.074), ('topic', 0.069), ('grouping', 0.065), ('asms', 0.064), ('hu', 0.056), ('emit', 0.054), ('blei', 0.053), ('draw', 0.052), ('nonseed', 0.048), ('nonseeded', 0.048), ('switch', 0.048), ('specific', 0.043), ('categorizing', 0.042), ('flavor', 0.042), ('term', 0.041), ('models', 0.041), ('review', 0.04), ('priors', 0.04), ('steyvers', 0.04), ('heinrich', 0.038), ('maintenance', 0.038), ('lda', 0.038), ('www', 0.037), ('emission', 0.037), ('zhao', 0.037), ('reviews', 0.037), ('liu', 0.036), ('beta', 0.036), ('zhu', 0.034), ('asymmetric', 0.034), ('hotel', 0.034), ('wallach', 0.034), ('separate', 0.033), ('discover', 0.033), ('chicago', 0.033), ('else', 0.033), ('craven', 0.032), ('flavors', 0.032), ('mcauliffe', 0.032), ('pillows', 0.032), ('vfm', 0.032), ('lu', 0.032), ('mining', 0.031), ('opinions', 0.031), ('sampled', 0.03), ('griffiths', 0.03), ('labeled', 0.028), ('distribution', 0.028), ('beds', 0.028), ('amenities', 0.028), ('carenini', 0.028), ('ester', 0.028), ('groupings', 0.028), ('moghaddam', 0.028), ('suit', 0.028), ('sampling', 0.028), ('something', 0.028), ('product', 0.027), ('conference', 0.027), ('latent', 0.026), ('world', 0.026), ('dirichlet', 0.026), ('cikm', 0.026), ('discovering', 0.026), ('sauper', 0.026), ('synonymous', 0.026), ('probable', 0.025), ('qualitative', 0.025), ('guo', 0.025), ('denote', 0.024), ('zhuang', 0.024), ('jia', 0.024), ('moments', 0.024), ('emitting', 0.024), ('guidance', 0.024), ('progressively', 0.024), ('extracting', 0.024), ('let', 0.024), ('rating', 0.023), ('extraction', 0.023), ('discovered', 0.023), ('constraints', 0.023), ('brody', 0.023), ('plate', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="28-tfidf-1" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Aspect extraction is a central problem in sentiment analysis. Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. By categorizing, we mean the synonymous aspects should be clustered into the same category. In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. This setting is important because categorizing aspects is a subjective task. For different application purposes, different categorizations may be needed. Some form of user guidance is desired. In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. Our experimental results show that the two proposed models are indeed able to perform the task effectively. 1</p><p>2 0.44186279 <a title="28-tfidf-2" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<p>Author: Lei Fang ; Minlie Huang</p><p>Abstract: In this paper, we present a structural learning model forjoint sentiment classification and aspect analysis of text at various levels of granularity. Our model aims to identify highly informative sentences that are aspect-specific in online custom reviews. The primary advantages of our model are two-fold: first, it performs document-level and sentence-level sentiment polarity classification jointly; second, it is able to find informative sentences that are closely related to some respects in a review, which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization. The proposed method was evaluated with 9,000 Chinese restaurant reviews. Preliminary experiments demonstrate that our model obtains promising performance. 1</p><p>3 0.28318626 <a title="28-tfidf-3" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>4 0.18022636 <a title="28-tfidf-4" href="./acl-2012-Modeling_Review_Comments.html">144 acl-2012-Modeling Review Comments</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Writing comments about news articles, blogs, or reviews have become a popular activity in social media. In this paper, we analyze reader comments about reviews. Analyzing review comments is important because reviews only tell the experiences and evaluations of reviewers about the reviewed products or services. Comments, on the other hand, are readers’ evaluations of reviews, their questions and concerns. Clearly, the information in comments is valuable for both future readers and brands. This paper proposes two latent variable models to simultaneously model and extract these key pieces of information. The results also enable classification of comments accurately. Experiments using Amazon review comments demonstrate the effectiveness of the proposed models.</p><p>5 0.14177676 <a title="28-tfidf-5" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>Author: Xinfan Meng ; Furu Wei ; Xiaohua Liu ; Ming Zhou ; Ge Xu ; Houfeng Wang</p><p>Abstract: The amount of labeled sentiment data in English is much larger than that in other languages. Such a disproportion arouse interest in cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) using labeled data in the source language (e.g. English). Most existing work relies on machine translation engines to directly adapt labeled data from the source language to the target language. This approach suffers from the limited coverage of vocabulary in the machine translation results. In this paper, we propose a generative cross-lingual mixture model (CLMM) to leverage unlabeled bilingual parallel data. By fitting parameters to maximize the likelihood of the bilingual parallel data, the proposed model learns previously unseen sentiment words from the large bilingual parallel data and improves vocabulary coverage signifi- cantly. Experiments on multiple data sets show that CLMM is consistently effective in two settings: (1) labeled data in the target language are unavailable; and (2) labeled data in the target language are also available.</p><p>6 0.1185487 <a title="28-tfidf-6" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>7 0.11368216 <a title="28-tfidf-7" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>8 0.10492446 <a title="28-tfidf-8" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>9 0.10250822 <a title="28-tfidf-9" href="./acl-2012-Multilingual_Subjectivity_and_Sentiment_Analysis.html">151 acl-2012-Multilingual Subjectivity and Sentiment Analysis</a></p>
<p>10 0.096976906 <a title="28-tfidf-10" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>11 0.090862386 <a title="28-tfidf-11" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>12 0.087695971 <a title="28-tfidf-12" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>13 0.085370369 <a title="28-tfidf-13" href="./acl-2012-A_Probabilistic_Model_for_Canonicalizing_Named_Entity_Mentions.html">18 acl-2012-A Probabilistic Model for Canonicalizing Named Entity Mentions</a></p>
<p>14 0.083326012 <a title="28-tfidf-14" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>15 0.082999505 <a title="28-tfidf-15" href="./acl-2012-Tense_and_Aspect_Error_Correction_for_ESL_Learners_Using_Global_Context.html">192 acl-2012-Tense and Aspect Error Correction for ESL Learners Using Global Context</a></p>
<p>16 0.079688028 <a title="28-tfidf-16" href="./acl-2012-SITS%3A_A_Hierarchical_Nonparametric_Model_using_Speaker_Identity_for_Topic_Segmentation_in_Multiparty_Conversations.html">171 acl-2012-SITS: A Hierarchical Nonparametric Model using Speaker Identity for Topic Segmentation in Multiparty Conversations</a></p>
<p>17 0.078654654 <a title="28-tfidf-17" href="./acl-2012-Efficient_Tree-Based_Topic_Modeling.html">79 acl-2012-Efficient Tree-Based Topic Modeling</a></p>
<p>18 0.074612446 <a title="28-tfidf-18" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>19 0.067754708 <a title="28-tfidf-19" href="./acl-2012-Subgroup_Detector%3A_A_System_for_Detecting_Subgroups_in_Online_Discussions.html">188 acl-2012-Subgroup Detector: A System for Detecting Subgroups in Online Discussions</a></p>
<p>20 0.067717858 <a title="28-tfidf-20" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.197), (1, 0.219), (2, 0.227), (3, -0.172), (4, 0.008), (5, -0.043), (6, 0.029), (7, -0.019), (8, -0.208), (9, -0.088), (10, 0.024), (11, 0.028), (12, -0.167), (13, -0.164), (14, -0.085), (15, -0.072), (16, 0.088), (17, -0.053), (18, -0.057), (19, -0.033), (20, 0.035), (21, 0.011), (22, -0.006), (23, -0.005), (24, -0.012), (25, -0.1), (26, -0.004), (27, -0.057), (28, -0.029), (29, -0.024), (30, -0.102), (31, -0.015), (32, -0.063), (33, 0.059), (34, -0.026), (35, -0.034), (36, -0.097), (37, 0.128), (38, -0.023), (39, 0.065), (40, 0.042), (41, 0.039), (42, 0.161), (43, -0.092), (44, -0.155), (45, -0.035), (46, -0.064), (47, 0.039), (48, -0.015), (49, 0.15)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96389574 <a title="28-lsi-1" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Aspect extraction is a central problem in sentiment analysis. Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. By categorizing, we mean the synonymous aspects should be clustered into the same category. In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. This setting is important because categorizing aspects is a subjective task. For different application purposes, different categorizations may be needed. Some form of user guidance is desired. In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. Our experimental results show that the two proposed models are indeed able to perform the task effectively. 1</p><p>2 0.9118374 <a title="28-lsi-2" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<p>Author: Lei Fang ; Minlie Huang</p><p>Abstract: In this paper, we present a structural learning model forjoint sentiment classification and aspect analysis of text at various levels of granularity. Our model aims to identify highly informative sentences that are aspect-specific in online custom reviews. The primary advantages of our model are two-fold: first, it performs document-level and sentence-level sentiment polarity classification jointly; second, it is able to find informative sentences that are closely related to some respects in a review, which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization. The proposed method was evaluated with 9,000 Chinese restaurant reviews. Preliminary experiments demonstrate that our model obtains promising performance. 1</p><p>3 0.63619506 <a title="28-lsi-3" href="./acl-2012-Cross-Domain_Co-Extraction_of_Sentiment_and_Topic_Lexicons.html">61 acl-2012-Cross-Domain Co-Extraction of Sentiment and Topic Lexicons</a></p>
<p>Author: Fangtao Li ; Sinno Jialin Pan ; Ou Jin ; Qiang Yang ; Xiaoyan Zhu</p><p>Abstract: Extracting sentiment and topic lexicons is important for opinion mining. Previous works have showed that supervised learning methods are superior for this task. However, the performance of supervised methods highly relies on manually labeled training data. In this paper, we propose a domain adaptation framework for sentiment- and topic- lexicon co-extraction in a domain of interest where we do not require any labeled data, but have lots of labeled data in another related domain. The framework is twofold. In the first step, we generate a few high-confidence sentiment and topic seeds in the target domain. In the second step, we propose a novel Relational Adaptive bootstraPping (RAP) algorithm to expand the seeds in the target domain by exploiting the labeled source domain data and the relationships between topic and sentiment words. Experimental results show that our domain adaptation framework can extract precise lexicons in the target domain without any annotation.</p><p>4 0.61052889 <a title="28-lsi-4" href="./acl-2012-Modeling_Review_Comments.html">144 acl-2012-Modeling Review Comments</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Writing comments about news articles, blogs, or reviews have become a popular activity in social media. In this paper, we analyze reader comments about reviews. Analyzing review comments is important because reviews only tell the experiences and evaluations of reviewers about the reviewed products or services. Comments, on the other hand, are readers’ evaluations of reviews, their questions and concerns. Clearly, the information in comments is valuable for both future readers and brands. This paper proposes two latent variable models to simultaneously model and extract these key pieces of information. The results also enable classification of comments accurately. Experiments using Amazon review comments demonstrate the effectiveness of the proposed models.</p><p>5 0.5474202 <a title="28-lsi-5" href="./acl-2012-Baselines_and_Bigrams%3A_Simple%2C_Good_Sentiment_and_Topic_Classification.html">37 acl-2012-Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
<p>Author: Sida Wang ; Christopher Manning</p><p>Abstract: Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset. We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets. Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.</p><p>6 0.50281817 <a title="28-lsi-6" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<p>7 0.48652479 <a title="28-lsi-7" href="./acl-2012-Cross-Lingual_Mixture_Model_for_Sentiment_Classification.html">62 acl-2012-Cross-Lingual Mixture Model for Sentiment Classification</a></p>
<p>8 0.45853287 <a title="28-lsi-8" href="./acl-2012-Polarity_Consistency_Checking_for_Sentiment_Dictionaries.html">161 acl-2012-Polarity Consistency Checking for Sentiment Dictionaries</a></p>
<p>9 0.45104039 <a title="28-lsi-9" href="./acl-2012-Structuring_E-Commerce_Inventory.html">186 acl-2012-Structuring E-Commerce Inventory</a></p>
<p>10 0.44212127 <a title="28-lsi-10" href="./acl-2012-Multilingual_Subjectivity_and_Sentiment_Analysis.html">151 acl-2012-Multilingual Subjectivity and Sentiment Analysis</a></p>
<p>11 0.42930242 <a title="28-lsi-11" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>12 0.39057356 <a title="28-lsi-12" href="./acl-2012-Identifying_High-Impact_Sub-Structures_for_Convolution_Kernels_in_Document-level_Sentiment_Classification.html">115 acl-2012-Identifying High-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification</a></p>
<p>13 0.36885089 <a title="28-lsi-13" href="./acl-2012-Social_Event_Radar%3A_A_Bilingual_Context_Mining_and_Sentiment_Analysis_Summarization_System.html">180 acl-2012-Social Event Radar: A Bilingual Context Mining and Sentiment Analysis Summarization System</a></p>
<p>14 0.34899557 <a title="28-lsi-14" href="./acl-2012-Syntactic_Stylometry_for_Deception_Detection.html">190 acl-2012-Syntactic Stylometry for Deception Detection</a></p>
<p>15 0.3106645 <a title="28-lsi-15" href="./acl-2012-A_Joint_Model_for_Discovery_of_Aspects_in_Utterances.html">14 acl-2012-A Joint Model for Discovery of Aspects in Utterances</a></p>
<p>16 0.30837655 <a title="28-lsi-16" href="./acl-2012-Historical_Analysis_of_Legal_Opinions_with_a_Sparse_Mixed-Effects_Latent_Variable_Model.html">110 acl-2012-Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model</a></p>
<p>17 0.30370963 <a title="28-lsi-17" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>18 0.29415029 <a title="28-lsi-18" href="./acl-2012-Efficient_Tree-Based_Topic_Modeling.html">79 acl-2012-Efficient Tree-Based Topic Modeling</a></p>
<p>19 0.28810585 <a title="28-lsi-19" href="./acl-2012-WizIE%3A_A_Best_Practices_Guided_Development_Environment_for_Information_Extraction.html">215 acl-2012-WizIE: A Best Practices Guided Development Environment for Information Extraction</a></p>
<p>20 0.27498367 <a title="28-lsi-20" href="./acl-2012-Self-Disclosure_and_Relationship_Strength_in_Twitter_Conversations.html">173 acl-2012-Self-Disclosure and Relationship Strength in Twitter Conversations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(15, 0.048), (21, 0.059), (25, 0.015), (26, 0.042), (28, 0.036), (30, 0.087), (37, 0.04), (39, 0.123), (43, 0.092), (74, 0.016), (82, 0.021), (84, 0.027), (85, 0.021), (90, 0.112), (92, 0.078), (94, 0.018), (99, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8526535 <a title="28-lda-1" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>Author: Bevan Jones ; Mark Johnson ; Sharon Goldwater</p><p>Abstract: Many semantic parsing models use tree transformations to map between natural language and meaning representation. However, while tree transformations are central to several state-of-the-art approaches, little use has been made of the rich literature on tree automata. This paper makes the connection concrete with a tree transducer based semantic parsing model and suggests that other models can be interpreted in a similar framework, increasing the generality of their contributions. In particular, this paper further introduces a variational Bayesian inference algorithm that is applicable to a wide class of tree transducers, producing state-of-the-art semantic parsing results while remaining applicable to any domain employing probabilistic tree transducers.</p><p>same-paper 2 0.84169811 <a title="28-lda-2" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Aspect extraction is a central problem in sentiment analysis. Current methods either extract aspects without categorizing them, or extract and categorize them using unsupervised topic modeling. By categorizing, we mean the synonymous aspects should be clustered into the same category. In this paper, we solve the problem in a different setting where the user provides some seed words for a few aspect categories and the model extracts and clusters aspect terms into categories simultaneously. This setting is important because categorizing aspects is a subjective task. For different application purposes, different categorizations may be needed. Some form of user guidance is desired. In this paper, we propose two statistical models to solve this seeded problem, which aim to discover exactly what the user wants. Our experimental results show that the two proposed models are indeed able to perform the task effectively. 1</p><p>3 0.79453373 <a title="28-lda-3" href="./acl-2012-Fine_Granular_Aspect_Analysis_using_Latent_Structural_Models.html">100 acl-2012-Fine Granular Aspect Analysis using Latent Structural Models</a></p>
<p>Author: Lei Fang ; Minlie Huang</p><p>Abstract: In this paper, we present a structural learning model forjoint sentiment classification and aspect analysis of text at various levels of granularity. Our model aims to identify highly informative sentences that are aspect-specific in online custom reviews. The primary advantages of our model are two-fold: first, it performs document-level and sentence-level sentiment polarity classification jointly; second, it is able to find informative sentences that are closely related to some respects in a review, which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization. The proposed method was evaluated with 9,000 Chinese restaurant reviews. Preliminary experiments demonstrate that our model obtains promising performance. 1</p><p>4 0.7852692 <a title="28-lda-4" href="./acl-2012-Efficient_Tree-Based_Topic_Modeling.html">79 acl-2012-Efficient Tree-Based Topic Modeling</a></p>
<p>Author: Yuening Hu ; Jordan Boyd-Graber</p><p>Abstract: Topic modeling with a tree-based prior has been used for a variety of applications because it can encode correlations between words that traditional topic modeling cannot. However, its expressive power comes at the cost of more complicated inference. We extend the SPARSELDA (Yao et al., 2009) inference scheme for latent Dirichlet allocation (LDA) to tree-based topic models. This sampling scheme computes the exact conditional distribution for Gibbs sampling much more quickly than enumerating all possible latent variable assignments. We further improve performance by iteratively refining the sampling distribution only when needed. Experiments show that the proposed techniques dramatically improve the computation time.</p><p>5 0.77684736 <a title="28-lda-5" href="./acl-2012-Learning_to_%22Read_Between_the_Lines%22_using_Bayesian_Logic_Programs.html">133 acl-2012-Learning to "Read Between the Lines" using Bayesian Logic Programs</a></p>
<p>Author: Sindhu Raghavan ; Raymond Mooney ; Hyeonseo Ku</p><p>Abstract: Most information extraction (IE) systems identify facts that are explicitly stated in text. However, in natural language, some facts are implicit, and identifying them requires “reading between the lines”. Human readers naturally use common sense knowledge to infer such implicit information from the explicitly stated facts. We propose an approach that uses Bayesian Logic Programs (BLPs), a statistical relational model combining firstorder logic and Bayesian networks, to infer additional implicit information from extracted facts. It involves learning uncertain commonsense knowledge (in the form of probabilistic first-order rules) from natural language text by mining a large corpus of automatically extracted facts. These rules are then used to derive additional facts from extracted information using BLP inference. Experimental evaluation on a benchmark data set for machine reading demonstrates the efficacy of our approach.</p><p>6 0.77640092 <a title="28-lda-6" href="./acl-2012-A_System_for_Real-time_Twitter_Sentiment_Analysis_of_2012_U.S._Presidential_Election_Cycle.html">21 acl-2012-A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle</a></p>
<p>7 0.77308327 <a title="28-lda-7" href="./acl-2012-A_Computational_Approach_to_the_Automation_of_Creative_Naming.html">7 acl-2012-A Computational Approach to the Automation of Creative Naming</a></p>
<p>8 0.77141744 <a title="28-lda-8" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>9 0.76666409 <a title="28-lda-9" href="./acl-2012-Modeling_Review_Comments.html">144 acl-2012-Modeling Review Comments</a></p>
<p>10 0.76525462 <a title="28-lda-10" href="./acl-2012-UWN%3A_A_Large_Multilingual_Lexical_Knowledge_Base.html">206 acl-2012-UWN: A Large Multilingual Lexical Knowledge Base</a></p>
<p>11 0.75727546 <a title="28-lda-11" href="./acl-2012-Genre_Independent_Subgroup_Detection_in_Online_Discussion_Threads%3A_A_Study_of_Implicit_Attitude_using_Textual_Latent_Semantics.html">102 acl-2012-Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics</a></p>
<p>12 0.75599027 <a title="28-lda-12" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>13 0.75482935 <a title="28-lda-13" href="./acl-2012-Estimating_Compact_Yet_Rich_Tree_Insertion_Grammars.html">84 acl-2012-Estimating Compact Yet Rich Tree Insertion Grammars</a></p>
<p>14 0.74743789 <a title="28-lda-14" href="./acl-2012-Temporally_Anchored_Relation_Extraction.html">191 acl-2012-Temporally Anchored Relation Extraction</a></p>
<p>15 0.74546844 <a title="28-lda-15" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<p>16 0.74455684 <a title="28-lda-16" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>17 0.74446672 <a title="28-lda-17" href="./acl-2012-Bayesian_Symbol-Refined_Tree_Substitution_Grammars_for_Syntactic_Parsing.html">38 acl-2012-Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing</a></p>
<p>18 0.7443158 <a title="28-lda-18" href="./acl-2012-Exploiting_Social_Information_in_Grounded_Language_Learning_via_Grammatical_Reduction.html">88 acl-2012-Exploiting Social Information in Grounded Language Learning via Grammatical Reduction</a></p>
<p>19 0.74424106 <a title="28-lda-19" href="./acl-2012-Subgroup_Detection_in_Ideological_Discussions.html">187 acl-2012-Subgroup Detection in Ideological Discussions</a></p>
<p>20 0.74180496 <a title="28-lda-20" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
