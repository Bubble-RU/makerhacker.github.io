<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-123" href="#">acl2012-123</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</h1>
<br/><p>Source: <a title="acl-2012-123-pdf" href="http://aclweb.org/anthology//P/P12/P12-1002.pdf">pdf</a></p><p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>Reference: <a title="acl-2012-123-reference" href="../acl2012_reference/acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ,  Abstract With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. [sent-3, score-0.534]
</p><p>2 We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. [sent-6, score-0.747]
</p><p>3 5 million training sentences, and show significant improvements over tuning discriminative models on small development sets. [sent-8, score-0.276]
</p><p>4 1 Introduction The standard SMT training pipeline combines scores from large count-based translation models and language models with a few other features and tunes these using the well-understood line-search technique for error minimization of Och (2003). [sent-9, score-0.239]
</p><p>5 If only a handful of dense features need to be tuned, minimum error rate training can be done on small tuning sets and is hard to beat in terms of accuracy and efficiency. [sent-10, score-0.372]
</p><p>6 In contrast, the promise of largescale discriminative training for SMT is to scale to arbitrary types and numbers of features and to provide sufficient statistical support by parameter estimation on large sample sizes. [sent-11, score-0.244]
</p><p>7 The modeler’s goals might be to identify complex properties of translations, or to counter errors of pretrained translation models and language models by explicitly down-weighting translations that exhibit certain undesired properties. [sent-16, score-0.226]
</p><p>8 Various approaches to feature engineering for discriminative models have been presented (see Section 2), however, with a few exceptions, discriminative learning in SMT has been confined to training on small tuning sets of a few thousand examples. [sent-17, score-0.539]
</p><p>9 One possible reason why discriminative SMT has mostly been content with small tuning sets lies in the particular design of the features themselves. [sent-20, score-0.35]
</p><p>10 (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. [sent-23, score-0.337]
</p><p>11 These features are specified in handcrafted lists based on a thorough analysis of a tuning set. [sent-24, score-0.186]
</p><p>12 The second type of features deploys external information such as syntactic parses or word alignments to penalize bad reorderings or undesired translations of phrases that cross syntactic constraints. [sent-26, score-0.202]
</p><p>13 c s 2o0c1ia2ti Aosns fo cria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsteiscs 1 –21, (1) X ((12)) XX (3) X  → →→  →  X1 hat X2 versprochen, X1 promised X2 XX1 hat mir X2 versprochen, XX1 promised me X2 X1 versprach X2, X1 promised X2  Figure 1: SCFG rules for translation. [sent-29, score-0.281]
</p><p>14 Another possible reason why large training data did not yet show the expected improvements in discriminative SMT is a special overfitting problem of current popular online learning techniques. [sent-31, score-0.167]
</p><p>15 This is due to stochastic learning on a per-example basis where a weight update on a misclassified example may apply only to a small fraction of data that have been seen before. [sent-32, score-0.195]
</p><p>16 The goal of this paper is to investigate if and how it is possible to benefit from scaling discriminative training for SMT to large training sets. [sent-34, score-0.281]
</p><p>17 Such features include rule ids, rule-local n-grams, or types of rule shapes. [sent-36, score-0.295]
</p><p>18 Another crucial ingredient of our approach is a combination of parallelized stochastic learning with feature selection inspired by multi-task learning. [sent-37, score-0.407]
</p><p>19 Iterative feature selection procedure is the key to both efficiency and improved prediction: Without interleaving parallelized stochastic learning with feature selection our largest experiments would not be feasible. [sent-39, score-0.612]
</p><p>20 Selecting features jointly across shards and averaging does counter the overfitting effect that is inherent  to stochastic updating. [sent-40, score-0.48]
</p><p>21 Our resulting models are learned on large data sets, but they are small and outperform models that tune feature sets of various sizes on small development sets. [sent-41, score-0.157]
</p><p>22 com/ redpony/  cdec  12 2  Related Work  The great promise of discriminative training for SMT is the possibility to design arbitrarily expressive, complex, or overlapping features in great numbers. [sent-44, score-0.377]
</p><p>23 All approaches have been shown to scale to large feature sets and all include some kind of regularization method. [sent-57, score-0.269]
</p><p>24 However, most approaches have been confined to training on small tuning sets. [sent-58, score-0.17]
</p><p>25 Exceptions where discriminative SMT has been used on large training data are Liang et al. [sent-59, score-0.167]
</p><p>26 Local features are designed to be readable directly off the rule at decoding time. [sent-71, score-0.186]
</p><p>27 We use three rule templates in our work: Rule identifiers: These features identify each rule by a unique identifier. [sent-72, score-0.295]
</p><p>28 2 Rule shape: These features are indicators that abstract away from lexical items to templates that  identify the location of sequences of terminal symbols in relation to non-terminal symbols, on both the source- and target-sides of each rule used. [sent-77, score-0.186]
</p><p>29 For example, both rules (1) and (2) map to the same indicator, namely that a rule is being used that consists of a (NT, term*, NT, term*) pattern on its source side, and an (NT, term*, NT) pattern on its target side. [sent-78, score-0.174]
</p><p>30 Let each translation candidate be represented by a feature vector x ∈ IRD where preference pairs dfo bry training are prepared by sorting translations according to smoothed sentence-wise BLEU score (Liang et al. [sent-82, score-0.324]
</p><p>31 13 subgradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005):  ∇lj(w) =(0−¯ xjelsei. [sent-87, score-0.236]
</p><p>32 f hw, x¯ji ≤ 0, Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios. [sent-88, score-0.194]
</p><p>33 The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002) or for voting (Freund and Schapire, 1999). [sent-89, score-0.332]
</p><p>34 w −t, 1i},j:) do endw fto,ir+1,0 ← wt,i,P  wt+1,0,0 ← end for  wt,I,0  return1TtP=T1wt,0,0 While stochastic learning exhibits a runtime behavior that is linear in sample size (Bottou, 2004), very large datasets can make sequential processing infeasible. [sent-101, score-0.26]
</p><p>35 Algorithm 2 MixSGD: int I,T,Z, float η Partition data into Z shards, each of size S dPiastrrtiitbiuonte dtaot maa incthoin Zes s. [sent-103, score-0.295]
</p><p>36 ∇Pl −j( 1w}z:, dt,io,j) ← wz,t,i,P  I/Z;  endw fzo,tr,i+1,0 endw fzo,tr+1,0,0 ← wz,t,S,0  end for Collect final w? [sent-124, score-0.24]
</p><p>37 The key idea of algorithm 2 is to partition the data into disjoint shards, then train SGD on each shard in parallel, and after training mix the final parameters from each shard by averaging. [sent-136, score-0.475]
</p><p>38 (2010) also present an iterative mixing algorithm where weights are mixed from each shard after training a single epoch of the perceptron in parallel on each shard. [sent-139, score-0.672]
</p><p>39 The mixed weight vector is re-sent to each shard to start another epoch of training in parallel on each shard. [sent-140, score-0.49]
</p><p>40 This algorithm  corresponds to our algorithm 3 (IterMixSGD). [sent-141, score-0.16]
</p><p>41 Algorithm 3 IterMixSGD: int I,T,Z, float η Partition data into Z shards, each of size S ← I/Z; dPiastrrtiitbiuonte dtaot maa incthoin Zes s. [sent-142, score-0.295]
</p><p>42 end for return v Parameter mixing by averaging will help to ease the feature sparsity problem, however, keeping feature vectors on the scale of several million features in memory can be prohibitive. [sent-159, score-0.336]
</p><p>43 Our algorithm 4 (IterSelSGD) introduces feature selection into distributed learning for increased efficiency and as a more radical measure against overfitting. [sent-161, score-0.353]
</p><p>44 The key idea is to view shards as tasks, and to apply methods for joint feature selection from multi-task learning to achieve small sets of features that are useful across all tasks or shards. [sent-162, score-0.603]
</p><p>45 We compute the ‘2 norm of the weights in each feature column, sort features by this value, and keep K features in the model. [sent-167, score-0.387]
</p><p>46 This feature selection procedure is done after each epoch. [sent-168, score-0.205]
</p><p>47 Reduced weight vectors are mixed and the result is re-sent to each shard to start another epoch of parallel training on each shard. [sent-169, score-0.49]
</p><p>48 end for return v  This algorithm can be seen as an instance of ‘1/‘2 regularization as follows: Let wd be the dth column vector of W, representing the weights for the dth feature across tasks/shards. [sent-195, score-0.291]
</p><p>49 ‘1/‘2 regularization penalizes weights W by the weighted ‘1/‘2 norm  = λX ||wd||2. [sent-196, score-0.246]
</p><p>50 XD  λ||W||1,2  Xd=1  Each ‘2 norm of a weight column represents the relevance of the corresponding feature across  tasks/shards. [sent-197, score-0.288]
</p><p>51 The ‘1 sum of the ‘2 norms enforces a selection among features based on these norms. [sent-198, score-0.233]
</p><p>52 wz1 wz2  wz3  [ [ [  w1 6 0 0  w2 4 0 0  w3 0 3 0  w4 0 0 2  w5 0 0 3  ] ] ]  6  4  3  2  3 ⇒  18  column ‘2 norm: ‘1 sum:  Figure 2:  ‘1/‘2  w1 6 3 2  w2 4 0 3  w3 0 0 0  w4 0 0 0  w5 0 0 0  ] ] ]  7  [ [ [  5  0  0  0 ⇒  12  regularization enforcing feature selection. [sent-203, score-0.211]
</p><p>53 (2010)’s approach to ‘1/‘2 regularization where feature columns are incrementally selected based on the ‘2 norms of the gradient vectors corresponding to feature columns. [sent-205, score-0.413]
</p><p>54 Their algorithm is itself an extension of gradient-based feature selection based on the ‘1 norm, e. [sent-206, score-0.285]
</p><p>55 4 In contrast to these approaches we approximate the gradient by using the weights given by the ranking algorithm itself. [sent-210, score-0.187]
</p><p>56 Furthermore, algorithm 4 performs feature selection based on a choice of meta-parameter of K features instead of by thresholding a regularization meta-parameter λ, however, these techniques are equivalent and can be transformed into each other. [sent-213, score-0.474]
</p><p>57 Before training, we collect all the grammar rules necessary to 4Note that by definition of |W|| standard ‘1 regularization iNs a special case oinfi ‘tio1 /‘2 regularization for a single task. [sent-223, score-0.289]
</p><p>58 When decoding, cdec loads the appropriate file immediately prior to translation of the sentence. [sent-226, score-0.234]
</p><p>59 Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima’an, 2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well. [sent-229, score-0.256]
</p><p>60 For the 5-gram language models, we replaced every word in the lm training data with   that did not appear in the English part of the parallel training data to build an open vocabulary language model. [sent-231, score-0.187]
</p><p>61 Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed persentence BLEU (Liang et al. [sent-233, score-0.169]
</p><p>62 These level sets are used for multipartite ranking  RTuSloeknCten osucden st14,3 t5,r13025a,39i5n075,9-23n067c3(1G)lm41-,8t30r9a,–46in52-78c2d, 3e7612v,07-5,89n7281cde2v, 3t8612e,042s6,124t596-n4c3t,52e307s,t049 -,847n93c1 RTSuloeknteCnosucedn st203,54 1t5,2r6a32597in2543,2-e693(24p8951. [sent-236, score-0.16]
</p><p>63 News Commentary (nc) and Europarl (ep) training data and also News Crawl (crawl) dev/test data were taken from the WMT1 1 translation task (http : / / st atmt . [sent-238, score-0.215]
</p><p>64 The dev/test data of nc are the sets provided with the WMT07 shared /t task (http : / / st atmt . [sent-241, score-0.231]
</p><p>65 The numbers in brackets for the rule counts of ep/nc training data are total counts of rules in the per-sentence grammars. [sent-247, score-0.235]
</p><p>66 where translation pairs are built between the elements in HI-MID, HI-LOW, and MID-LOW, but not between translations inside sets on the same level. [sent-248, score-0.222]
</p><p>67 All experiments for training on dev sets were carried out on a single computer. [sent-255, score-0.174]
</p><p>68 We split the data into 2290 shards for the ep runs and 141 shards for the nc runs, each shard holding about 1,000 sentences, which corresponds to the dev set size of the nc data set. [sent-257, score-1.154]
</p><p>69 2  Experimental Results  The baseline learner in our experiments is a pairwise ranking perceptron that is used on various features and training data and plugged into various meta16 M  x¯ BLEU[%]23. [sent-259, score-0.348]
</p><p>70 0 Figure 4: Boxplot of BLEU-4 results for 100 runs of MIRA on news commentary data, depicting median (M), mean ( x¯), interquartile range (box), standard deviation (whiskers), outliers (end points). [sent-263, score-0.166]
</p><p>71 The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al. [sent-265, score-0.176]
</p><p>72 Figure 4 gives a boxplot depicting BLEU-4 results for 100 runs ofthe MIRA implementation of the cdec package, tuned on dev-nc, and evaluated on the respective test set test-nc. [sent-267, score-0.23]
</p><p>73 The fluctuation of results is due to sampling training examples from the translation hy6MIRA was used with default meta parameters: 250 hypothesis list to search for oracles, regularization strength C = 0. [sent-271, score-0.274]
</p><p>74 Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape). [sent-287, score-0.587]
</p><p>75 05 of a result difference on the test set to a different algorithm applied to the same feature group is indicated by raised algorithm number. [sent-289, score-0.259]
</p><p>76 e @s s iantdisitcicataelsly yth seig optimal  number of epochs chosen on the devtest set. [sent-292, score-0.167]
</p><p>77 , by using larger language models or by adding news data to the ep training set when evaluating on crawl test sets (see, e. [sent-302, score-0.467]
</p><p>78 17 default features include 12 dense models defined on SCFG  rules;8  The sparse features are the 3 templates  described in Section 3. [sent-306, score-0.221]
</p><p>79 If not indicated otherwise, the perceptron was run for 10 epochs with learning rate η  = 0. [sent-308, score-0.191]
</p><p>80 843 34†1  Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test data. [sent-317, score-0.282]
</p><p>81 Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape). [sent-318, score-0.587]
</p><p>82 05 of a result difference on the test set to a different algorithm applied to the same feature group is indicated by raised algorithm number. [sent-320, score-0.259]
</p><p>83 @ st aintidsti ccaatlelys t shieg optimal dniuf merbeenrc eofs epochs  chosen on the devtest set. [sent-323, score-0.167]
</p><p>84 and tuning the full set of 180,000 features are not significant. [sent-324, score-0.186]
</p><p>85 However, scaling all features to the full training set shows significant improvements for algorithm 3, and especially for algorithm 4, which gains 0. [sent-325, score-0.351]
</p><p>86 8 BLEU points over tuning 12 features on the development set. [sent-326, score-0.186]
</p><p>87 7 million without feature selection, which iteratively selects 100,000 features with best ‘2 norm values across shards. [sent-328, score-0.31]
</p><p>88 Feature templates such as rule n-grams and rule shapes only work if iterative mixing (algorithm 3) or feature selection (algorithm 4) are used. [sent-329, score-0.484]
</p><p>89 Adding rule id features works in combination with other sparse features. [sent-330, score-0.273]
</p><p>90 Testing was done on the Europarl test set and news crawl test data from the years 2010 and 2011. [sent-333, score-0.231]
</p><p>91 Here tuning large feature sets on the respective dev sets yields significant improvements of around 2 BLEU points over tuning the 12 default features on the dev sets. [sent-334, score-0.62]
</p><p>92 3 BLEU points (test-  crawl10) are gained when scaling to the full training set using iterative features selection. [sent-337, score-0.191]
</p><p>93 Algorithms 2 and 3 were infeasible to run on Europarl data beyond one epoch because features vectors grew too large to be kept in memory. [sent-339, score-0.219]
</p><p>94 6  Discussion  We presented an approach to scaling discriminative learning for SMT not only to large feature 18 sets but also to large sets of parallel training data. [sent-340, score-0.5]
</p><p>95 Our approach is made feasible and effective by applying joint feature selection across distributed stochastic learning processes. [sent-342, score-0.413]
</p><p>96 For example, patent data can be characterized along the dimensions of patent classes and patent text fields (W¨ aschle and Riezler, 2012) and thus are well suited for multi-task translation. [sent-347, score-0.3]
</p><p>97 Online large-margin training of syntactic and structural translation features. [sent-366, score-0.162]
</p><p>98 Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. [sent-442, score-0.162]
</p><p>99 Joint covariate selection and joint subspace selection for multiple classification problems. [sent-484, score-0.212]
</p><p>100 Grafting: Fast, incremental feature selection by gradient descent in function space. [sent-505, score-0.258]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shards', 0.263), ('crawl', 0.178), ('shard', 0.167), ('smt', 0.148), ('epoch', 0.142), ('stochastic', 0.14), ('norm', 0.134), ('cdec', 0.133), ('europarl', 0.125), ('nc', 0.12), ('endw', 0.12), ('ep', 0.117), ('regularization', 0.112), ('tuning', 0.109), ('rule', 0.109), ('selection', 0.106), ('discriminative', 0.106), ('sgd', 0.104), ('translation', 0.101), ('feature', 0.099), ('aclhl', 0.096), ('eopr', 0.096), ('float', 0.096), ('iterselsgd', 0.096), ('perceptron', 0.096), ('epochs', 0.095), ('id', 0.087), ('chiang', 0.087), ('dyer', 0.085), ('bleu', 0.084), ('patent', 0.084), ('watanabe', 0.084), ('xj', 0.083), ('algorithm', 0.08), ('features', 0.077), ('riezler', 0.073), ('chapelle', 0.072), ('devtest', 0.072), ('mapreduce', 0.072), ('mixsgd', 0.072), ('perceptrons', 0.072), ('promised', 0.072), ('ramp', 0.072), ('duh', 0.071), ('distributed', 0.068), ('hajime', 0.067), ('tsukada', 0.067), ('dense', 0.067), ('rules', 0.065), ('parallel', 0.065), ('commentary', 0.064), ('translations', 0.063), ('shape', 0.063), ('undesired', 0.062), ('parallelized', 0.062), ('training', 0.061), ('mixing', 0.061), ('scfg', 0.061), ('nt', 0.06), ('pairwise', 0.06), ('mira', 0.059), ('hideki', 0.059), ('sets', 0.058), ('tillmann', 0.057), ('gimpel', 0.057), ('taro', 0.057), ('liang', 0.057), ('dev', 0.055), ('weight', 0.055), ('int', 0.055), ('blunsom', 0.055), ('ranking', 0.054), ('identifiers', 0.053), ('olivier', 0.053), ('atmt', 0.053), ('scaling', 0.053), ('gradient', 0.053), ('news', 0.053), ('algorithms', 0.051), ('norms', 0.05), ('runs', 0.049), ('franz', 0.048), ('aschle', 0.048), ('boxplot', 0.048), ('dpiastrrtiitbiuonte', 0.048), ('dtaot', 0.048), ('dzo', 0.048), ('ieapliozech', 0.048), ('incthoin', 0.048), ('ird', 0.048), ('itermixsgd', 0.048), ('lal', 0.048), ('maa', 0.048), ('multipartite', 0.048), ('obozinski', 0.048), ('parallelization', 0.048), ('perkins', 0.048), ('setfeatures', 0.048), ('simianer', 0.048), ('ssh', 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="123-tfidf-1" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>2 0.20847532 <a title="123-tfidf-2" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>3 0.17170602 <a title="123-tfidf-3" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Hao Zhang ; Qiang Li</p><p>Abstract: We present a new open source toolkit for phrase-based and syntax-based machine translation. The toolkit supports several state-of-the-art models developed in statistical machine translation, including the phrase-based model, the hierachical phrase-based model, and various syntaxbased models. The key innovation provided by the toolkit is that the decoder can work with various grammars and offers different choices of decoding algrithms, such as phrase-based decoding, decoding as parsing/tree-parsing and forest-based decoding. Moreover, several useful utilities were distributed with the toolkit, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1</p><p>4 0.14745566 <a title="123-tfidf-4" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>Author: Rada Mihalcea ; Vivi Nastase</p><p>Abstract: In this paper we introduce the novel task of “word epoch disambiguation,” defined as the problem of identifying changes in word usage over time. Through experiments run using word usage examples collected from three major periods of time (1800, 1900, 2000), we show that the task is feasible, and significant differences can be observed between occurrences of words in different periods of time.</p><p>5 0.14020024 <a title="123-tfidf-5" href="./acl-2012-Smaller_Alignment_Models_for_Better_Translations%3A_Unsupervised_Word_Alignment_with_the_l0-norm.html">179 acl-2012-Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm</a></p>
<p>Author: Ashish Vaswani ; Liang Huang ; David Chiang</p><p>Abstract: Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an ‘0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 B ).</p><p>6 0.13746744 <a title="123-tfidf-6" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>7 0.13575622 <a title="123-tfidf-7" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>8 0.13002442 <a title="123-tfidf-8" href="./acl-2012-Topic_Models_for_Dynamic_Translation_Model_Adaptation.html">199 acl-2012-Topic Models for Dynamic Translation Model Adaptation</a></p>
<p>9 0.12640764 <a title="123-tfidf-9" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>10 0.11893633 <a title="123-tfidf-10" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>11 0.11721632 <a title="123-tfidf-11" href="./acl-2012-Post-ordering_by_Parsing_for_Japanese-English_Statistical_Machine_Translation.html">162 acl-2012-Post-ordering by Parsing for Japanese-English Statistical Machine Translation</a></p>
<p>12 0.11666989 <a title="123-tfidf-12" href="./acl-2012-Translation_Model_Size_Reduction_for_Hierarchical_Phrase-based_Statistical_Machine_Translation.html">204 acl-2012-Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</a></p>
<p>13 0.11631415 <a title="123-tfidf-13" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>14 0.11044347 <a title="123-tfidf-14" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>15 0.11027118 <a title="123-tfidf-15" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>16 0.10878155 <a title="123-tfidf-16" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>17 0.10726315 <a title="123-tfidf-17" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>18 0.10621268 <a title="123-tfidf-18" href="./acl-2012-Learning_Better_Rule_Extraction_with_Translation_Span_Alignment.html">128 acl-2012-Learning Better Rule Extraction with Translation Span Alignment</a></p>
<p>19 0.10449497 <a title="123-tfidf-19" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>20 0.10360116 <a title="123-tfidf-20" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.304), (1, -0.176), (2, 0.058), (3, 0.011), (4, 0.005), (5, -0.017), (6, -0.0), (7, 0.037), (8, 0.002), (9, -0.018), (10, -0.019), (11, -0.043), (12, -0.039), (13, -0.002), (14, 0.015), (15, 0.056), (16, 0.092), (17, 0.087), (18, 0.017), (19, -0.045), (20, 0.061), (21, -0.061), (22, -0.056), (23, 0.024), (24, 0.072), (25, -0.002), (26, -0.037), (27, 0.027), (28, -0.023), (29, 0.013), (30, 0.012), (31, -0.037), (32, 0.121), (33, 0.041), (34, 0.1), (35, -0.0), (36, 0.136), (37, -0.13), (38, -0.083), (39, 0.018), (40, 0.174), (41, 0.082), (42, -0.084), (43, 0.03), (44, -0.061), (45, 0.059), (46, -0.061), (47, -0.14), (48, 0.082), (49, 0.155)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94035244 <a title="123-lsi-1" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>2 0.66792399 <a title="123-lsi-2" href="./acl-2012-Maximum_Expected_BLEU_Training_of_Phrase_and_Lexicon_Translation_Models.html">141 acl-2012-Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</a></p>
<p>Author: Xiaodong He ; Li Deng</p><p>Abstract: This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 201 1 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.</p><p>3 0.66711581 <a title="123-lsi-3" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>Author: Ioannis Konstas ; Mirella Lapata</p><p>Abstract: This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our approach is to reduce the tasks of content selection (“what to say”) and surface realization (“how to say”) into a common parsing problem. We define a probabilistic context-free grammar that describes the structure of the input (a corpus of database records and text describing some of them) and represent it compactly as a weighted hypergraph. The hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.</p><p>4 0.63307679 <a title="123-lsi-4" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.</p><p>5 0.63100141 <a title="123-lsi-5" href="./acl-2012-Prediction_of_Learning_Curves_in_Machine_Translation.html">163 acl-2012-Prediction of Learning Curves in Machine Translation</a></p>
<p>Author: Prasanth Kolachina ; Nicola Cancedda ; Marc Dymetman ; Sriram Venkatapathy</p><p>Abstract: Parallel data in the domain of interest is the key resource when training a statistical machine translation (SMT) system for a specific purpose. Since ad-hoc manual translation can represent a significant investment in time and money, a prior assesment of the amount of training data required to achieve a satisfactory accuracy level can be very useful. In this work, we show how to predict what the learning curve would look like if we were to manually translate increasing amounts of data. We consider two scenarios, 1) Monolingual samples in the source and target languages are available and 2) An additional small amount of parallel corpus is also available. We propose methods for predicting learning curves in both these scenarios.</p><p>6 0.61113685 <a title="123-lsi-6" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>7 0.6094225 <a title="123-lsi-7" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>8 0.60688126 <a title="123-lsi-8" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<p>9 0.60352743 <a title="123-lsi-9" href="./acl-2012-Fast_Online_Training_with_Frequency-Adaptive_Learning_Rates_for_Chinese_Word_Segmentation_and_New_Word_Detection.html">94 acl-2012-Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection</a></p>
<p>10 0.6018241 <a title="123-lsi-10" href="./acl-2012-Mixing_Multiple_Translation_Models_in_Statistical_Machine_Translation.html">143 acl-2012-Mixing Multiple Translation Models in Statistical Machine Translation</a></p>
<p>11 0.5819059 <a title="123-lsi-11" href="./acl-2012-Deciphering_Foreign_Language_by_Combining_Language_Models_and_Context_Vectors.html">67 acl-2012-Deciphering Foreign Language by Combining Language Models and Context Vectors</a></p>
<p>12 0.57219487 <a title="123-lsi-12" href="./acl-2012-NiuTrans%3A_An_Open_Source_Toolkit_for_Phrase-based_and_Syntax-based_Machine_Translation.html">155 acl-2012-NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</a></p>
<p>13 0.55712223 <a title="123-lsi-13" href="./acl-2012-Head-Driven_Hierarchical_Phrase-based_Translation.html">105 acl-2012-Head-Driven Hierarchical Phrase-based Translation</a></p>
<p>14 0.5523994 <a title="123-lsi-14" href="./acl-2012-Learning_Translation_Consensus_with_Structured_Label_Propagation.html">131 acl-2012-Learning Translation Consensus with Structured Label Propagation</a></p>
<p>15 0.54843539 <a title="123-lsi-15" href="./acl-2012-Improving_Word_Representations_via_Global_Context_and_Multiple_Word_Prototypes.html">117 acl-2012-Improving Word Representations via Global Context and Multiple Word Prototypes</a></p>
<p>16 0.54526848 <a title="123-lsi-16" href="./acl-2012-Discriminative_Pronunciation_Modeling%3A_A_Large-Margin%2C_Feature-Rich_Approach.html">74 acl-2012-Discriminative Pronunciation Modeling: A Large-Margin, Feature-Rich Approach</a></p>
<p>17 0.51192451 <a title="123-lsi-17" href="./acl-2012-Character-Level_Machine_Translation_Evaluation_for_Languages_with_Ambiguous_Word_Boundaries.html">46 acl-2012-Character-Level Machine Translation Evaluation for Languages with Ambiguous Word Boundaries</a></p>
<p>18 0.50572127 <a title="123-lsi-18" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>19 0.50365323 <a title="123-lsi-19" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>20 0.50131041 <a title="123-lsi-20" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.045), (25, 0.026), (26, 0.04), (28, 0.062), (30, 0.027), (37, 0.029), (39, 0.038), (55, 0.15), (57, 0.025), (59, 0.02), (74, 0.062), (82, 0.026), (84, 0.016), (85, 0.052), (90, 0.141), (92, 0.053), (94, 0.053), (99, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88890529 <a title="123-lda-1" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>Author: Marco Guerini ; Carlo Strapparava ; Oliviero Stock</p><p>Abstract: In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach.</p><p>2 0.88881952 <a title="123-lda-2" href="./acl-2012-Unsupervised_Morphology_Rivals_Supervised_Morphology_for_Arabic_MT.html">207 acl-2012-Unsupervised Morphology Rivals Supervised Morphology for Arabic MT</a></p>
<p>Author: David Stallard ; Jacob Devlin ; Michael Kayser ; Yoong Keok Lee ; Regina Barzilay</p><p>Abstract: If unsupervised morphological analyzers could approach the effectiveness of supervised ones, they would be a very attractive choice for improving MT performance on low-resource inflected languages. In this paper, we compare performance gains for state-of-the-art supervised vs. unsupervised morphological analyzers, using a state-of-theart Arabic-to-English MT system. We apply maximum marginal decoding to the unsupervised analyzer, and show that this yields the best published segmentation accuracy for Arabic, while also making segmentation output more stable. Our approach gives an 18% relative BLEU gain for Levantine dialectal Arabic. Furthermore, it gives higher gains for Modern Standard Arabic (MSA), as measured on NIST MT-08, than does MADA (Habash and Rambow, 2005), a leading supervised MSA segmenter.</p><p>same-paper 3 0.84312314 <a title="123-lda-3" href="./acl-2012-Joint_Feature_Selection_in_Distributed_Stochastic_Learning_for_Large-Scale_Discriminative_Training_in_SMT.html">123 acl-2012-Joint Feature Selection in Distributed Stochastic Learning for Large-Scale Discriminative Training in SMT</a></p>
<p>Author: Patrick Simianer ; Stefan Riezler ; Chris Dyer</p><p>Abstract: With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies ‘1/‘2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</p><p>4 0.77304494 <a title="123-lda-4" href="./acl-2012-Fast_and_Scalable_Decoding_with_Language_Model_Look-Ahead_for_Phrase-based_Statistical_Machine_Translation.html">97 acl-2012-Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation</a></p>
<p>Author: Joern Wuebker ; Hermann Ney ; Richard Zens</p><p>Abstract: In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our ap- proach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% BLEU, whereas our approach yields 20.0% with identical models.</p><p>5 0.74563718 <a title="123-lda-5" href="./acl-2012-Learning_to_Translate_with_Multiple_Objectives.html">136 acl-2012-Learning to Translate with Multiple Objectives</a></p>
<p>Author: Kevin Duh ; Katsuhito Sudoh ; Xianchao Wu ; Hajime Tsukada ; Masaaki Nagata</p><p>Abstract: We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.</p><p>6 0.74380356 <a title="123-lda-6" href="./acl-2012-PORT%3A_a_Precision-Order-Recall_MT_Evaluation_Metric_for_Tuning.html">158 acl-2012-PORT: a Precision-Order-Recall MT Evaluation Metric for Tuning</a></p>
<p>7 0.74201924 <a title="123-lda-7" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>8 0.7410934 <a title="123-lda-8" href="./acl-2012-Machine_Translation_without_Words_through_Substring_Alignment.html">140 acl-2012-Machine Translation without Words through Substring Alignment</a></p>
<p>9 0.73570567 <a title="123-lda-9" href="./acl-2012-A_Topic_Similarity_Model_for_Hierarchical_Phrase-based_Translation.html">22 acl-2012-A Topic Similarity Model for Hierarchical Phrase-based Translation</a></p>
<p>10 0.73550063 <a title="123-lda-10" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>11 0.73036414 <a title="123-lda-11" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>12 0.72600788 <a title="123-lda-12" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>13 0.72541535 <a title="123-lda-13" href="./acl-2012-Detecting_Semantic_Equivalence_and_Information_Disparity_in_Cross-lingual_Documents.html">72 acl-2012-Detecting Semantic Equivalence and Information Disparity in Cross-lingual Documents</a></p>
<p>14 0.72531706 <a title="123-lda-14" href="./acl-2012-Improving_the_IBM_Alignment_Models_Using_Variational_Bayes.html">118 acl-2012-Improving the IBM Alignment Models Using Variational Bayes</a></p>
<p>15 0.72339678 <a title="123-lda-15" href="./acl-2012-Verb_Classification_using_Distributional_Similarity_in_Syntactic_and_Semantic_Structures.html">214 acl-2012-Verb Classification using Distributional Similarity in Syntactic and Semantic Structures</a></p>
<p>16 0.72301805 <a title="123-lda-16" href="./acl-2012-Translation_Model_Adaptation_for_Statistical_Machine_Translation_with_Monolingual_Topic_Information.html">203 acl-2012-Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information</a></p>
<p>17 0.72176695 <a title="123-lda-17" href="./acl-2012-Combining_Word-Level_and_Character-Level_Models_for_Machine_Translation_Between_Closely-Related_Languages.html">54 acl-2012-Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</a></p>
<p>18 0.7216329 <a title="123-lda-18" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>19 0.72094631 <a title="123-lda-19" href="./acl-2012-Word_Sense_Disambiguation_Improves_Information_Retrieval.html">217 acl-2012-Word Sense Disambiguation Improves Information Retrieval</a></p>
<p>20 0.72033966 <a title="123-lda-20" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
