<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-75" href="#">acl2012-75</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</h1>
<br/><p>Source: <a title="acl-2012-75-pdf" href="http://aclweb.org/anthology//P/P12/P12-1022.pdf">pdf</a></p><p>Author: Matthieu Constant ; Anthony Sigogne ; Patrick Watrin</p><p>Abstract: and Parsing Anthony Sigogne Universit e´ Paris-Est LIGM, CNRS France s igogne @univ-mlv . fr Patrick Watrin Universit e´ de Louvain CENTAL Belgium pat rick .wat rin @ ucl ouvain .be view, their incorporation has also been considered The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.</p><p>Reference: <a title="acl-2012-75-reference" href="../acl2012_reference/acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 be  view, their incorporation has also been considered The integration of multiword expressions in a  parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. [sent-5, score-0.632]
</p><p>2 This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. [sent-6, score-0.539]
</p><p>3 Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. [sent-7, score-1.042]
</p><p>4 However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. [sent-8, score-0.427]
</p><p>5 Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics. [sent-9, score-0.781]
</p><p>6 1 Introduction The integration of Multiword Expressions (MWE) in real-life applications is crucial because such expressions have the particularity of having a certain level of idiomaticity. [sent-10, score-0.106]
</p><p>7 They form complex lexical units which, if they are considered, should significantly help parsing. [sent-11, score-0.069]
</p><p>8 From a theoretical point of view, the integration of multiword expressions in the parsing procedure has been studied for different formalisms: Head-Driven Phrase Structure Grammar (Copestake et al. [sent-12, score-0.526]
</p><p>9 From an empirical point of 204 such as in (Nivre and Nilsson, 2004) for dependency parsing and in (Arun and Keller, 2005) in constituency parsing. [sent-14, score-0.064]
</p><p>10 Although experiments always relied on a corpus where the MWEs were perfectly pre-identified, they showed that pre-grouping such expressions could significantly improve parsing accuracy. [sent-15, score-0.17]
</p><p>11 (201 1) proposed integrating the multiword expressions directly in the grammar without pre-recognizing them. [sent-17, score-0.529]
</p><p>12 The grammar was trained with a reference treebank where MWEs were annotated with a specific non-terminal node. [sent-18, score-0.074]
</p><p>13 , whether incorrect MWER does not negatively impact the overall parsing system. [sent-23, score-0.064]
</p><p>14 (b) is a more innovative approach to MWER (despite not being new in parsing): we select the final MWE segmentation after parsing in order to explore as many parses as possible (as opposed to method (a)). [sent-24, score-0.119]
</p><p>15 1 Overview Multiword expressions are lexical items made up of multiple lexemes that undergo idiosyncratic constraints and therefore offer a certain degree of idiomaticity. [sent-31, score-0.15]
</p><p>16 They are often divided into two main classes: multiword expressions defined through linguistic idiomaticity criteria (lexicalized phrases in the terminology of Sag et al. [sent-38, score-0.49]
</p><p>17 For instance, the utterance at night is a MWE because it does display a strict lexical restriction (*at day, *at afternoon) and it does not  accept any inserting material (*at cold night, *at present night). [sent-43, score-0.134]
</p><p>18 Such linguistically defined expressions may overlap with collocations which are the combinations of two or more words that cooccur more often than by chance. [sent-44, score-0.184]
</p><p>19 In this paper, we focus on contiguous MWEs that form a lexical unit which can be marked by a part-ofspeech tag (e. [sent-47, score-0.071]
</p><p>20 at night is an adverb, because of is a preposition). [sent-49, score-0.066]
</p><p>21 Such expressions can be analyzed at the lexical level. [sent-60, score-0.15]
</p><p>22 In what follows, we use the term compounds to denote such expressions. [sent-61, score-0.145]
</p><p>23 The main drawback is that this procedure entirely relies on a lexicon and is unable to discover unknown MWEs. [sent-67, score-0.066]
</p><p>24 For instance, for each candidate in the text, Watrin and Fran ¸cois (201 1) compute on the fly its association score from an external ngram base learnt from a large raw corpus, and tag it as MWE if the association score is greater than a threshold. [sent-69, score-0.053]
</p><p>25 (2010) developped a Support Vector Machine classifier integrating features corresponding to different collocation association measures. [sent-74, score-0.131]
</p><p>26 A recent trend is to couple MWE recognition with a linguistic analyzer: a POS tagger (Constant and Sigogne, 2011) or a parser (Green et al. [sent-79, score-0.087]
</p><p>27 Constant and Sigogne (201 1) trained a unified Conditional Random Fields model integrating different standard tagging features and features based on external lexical resources. [sent-81, score-0.189]
</p><p>28 Both methods have the advantage of being able to discover new MWEs on the basis of lexical and syntactic contexts. [sent-92, score-0.073]
</p><p>29 In this paper, we will take advantage of the methods described in this section by integrating them as features of a MWER model. [sent-93, score-0.065]
</p><p>30 (201 1) that integrates the MWER in the grammar and allows for discontinuous MWEs. [sent-98, score-0.058]
</p><p>31 Nevertheless, in practice, the compounds we are dealing with are very rarely discontinuous and if so,  they solely contain a single word insert that can be easily integrated in the MWE sequence. [sent-99, score-0.172]
</p><p>32 Constant and Sigogne (201 1) proposed to combine MWE segmentation and part-of-speech tagging into a single sequence labelling task by assigning to each token a tag of the form TAG+X where TAG is the part-ofspeech (POS) of the lexical unit the token belongs to and X is either B (i. [sent-100, score-0.229]
</p><p>33 the token is at the beginning of the lexical unit) or I(i. [sent-102, score-0.07]
</p><p>34 It is based on K features each of them being defined by a binary function fk depending on the current position t in x, the current label yt, the preceding one yt−1 and the whole input sequence x. [sent-117, score-0.099]
</p><p>35 The tokens xi of x integrate the lexical value of this token but can also integrate basic properties which are computable from this value (for example: whether it begins with an upper case, it contains a number, its tags in an external lexicon, etc. [sent-118, score-0.156]
</p><p>36 2 Reranking Discriminative reranking consists in reranking the nbest parses of a baseline parser with a discriminative model, hence integrating features associated with  each node of the candidate parses. [sent-127, score-0.336]
</p><p>37 Charniak and Johnson (2005) introduced different features that showed significant improvement in general parsing accuracy (e. [sent-128, score-0.093]
</p><p>38 Formally, given a sentence s, the reranker selects the best candidate parse p among a set of candidates P(s) with respect to a scoring function Vθ:  p∗ = argmaxp∈P(s)Vθ(p) The set of candidates P(s) corresponds to the n-best parses generated by the baseline parser. [sent-131, score-0.122]
</p><p>39 The vector θ is estimated during the training stage from a reference treebank and the baseline parser ouputs. [sent-136, score-0.091]
</p><p>40 In this paper, we slightly deviate from the original  reranker usage, by focusing on improving MWER in the context of parsing. [sent-137, score-0.098]
</p><p>41 Given the n-best parses, we want to select the one with the best MWE segmentation by keeping the overall parsing accuracy as high as possible. [sent-138, score-0.095]
</p><p>42 One benefit of this corpus is that its compounds are marked. [sent-147, score-0.145]
</p><p>43 HHHH N N P part  de  march ´e  Figure 1: Subtree of MWE part de march ´e (market share): The MWN node indicates that it is a multiword noun; it has a flat internal structure N P N (noun preprosition – noun) –  The French Treebank is composed of435,860 lexical units (34,178 types). [sent-155, score-0.605]
</p><p>44 It is composed of 840,813 lexical entries including 104,350 multiword ones (91,030 multiword nouns). [sent-178, score-0.808]
</p><p>45 The compounds present in the resources respect the linguistic criteria defined in (Gross, 1986). [sent-179, score-0.176]
</p><p>46 The lefff is a freely available dictionary4 that has been automatically compiled by drawing from different sources and that has been manually validated. [sent-180, score-0.056]
</p><p>47 We used a version with 553,138 lexical entries including 26,3 11 multiword ones (22,673 multiword nouns). [sent-181, score-0.784]
</p><p>48 In both, lexical  entries are composed of a inflected form, a lemma, a part-of-speech and morphological features. [sent-183, score-0.068]
</p><p>49 The Dela has an additional feature for most of the multiword entries: their syntactic surface form. [sent-184, score-0.356]
</p><p>50 For instance, eau de vie (brandy) has the feature NDN because it has the internal flat structure noun preposition de noun. [sent-185, score-0.127]
</p><p>51 In order to compare compounds in these lexical resources with the ones in the French Treebank, we applied on the development corpus the dictionaries and the lexicon extracted from the training corpus. [sent-186, score-0.285]
</p><p>52 They show that the use of external resources may improve recall, but they lead –  –  3http://igm. [sent-189, score-0.057]
</p><p>53 09D  Table 1: Simple context-free application of the lexical resources on the development corpus: T is the MWE lexicon of the training corpus, L is the lefff, D is the Dela. [sent-203, score-0.112]
</p><p>54 In terms of statistical collocations, Watrin and Fran ¸cois (201 1) described a system that lists all the potential nominal collocations of a given sentence along with their association measure. [sent-205, score-0.078]
</p><p>55 The authors provided us with a list of 17,3 15 candidate nominal collocations occurring in the French treebank with their log-likelihood and their internal flat structure. [sent-206, score-0.182]
</p><p>56 In order to make these models comparable, we use two comparable sets of feature templates: one adapted to sequence labelling (CRF-based MWER) and the other one adapted to reranking (MaxEnt-based reranker). [sent-208, score-0.077]
</p><p>57 The MWER templates are instantiated at each position of the input sequence. [sent-209, score-0.075]
</p><p>58 The reranker templates are instantiated only for the nodes of the candidate  parse tree, which are leaves dominated by a MWE node (i. [sent-210, score-0.156]
</p><p>59 1 Endogenous Features Endogenous features are features directly extracted from properties of the words themselves or from a tool learnt from the training corpus (e. [sent-215, score-0.083]
</p><p>60 We use word unigrams and bigrams in order to capture multiwords present in the training section and to extract lexical cues to discover new  MWEs. [sent-219, score-0.073]
</p><p>61 For instance, the bigram coup de is often the prefix of compounds such as coup de pied (kick), coup de foudre (love at first sight), coup de main (help). [sent-220, score-0.501]
</p><p>62 For instance, the POS sequence preposition – adverb associated with the compound depuis peu (recently) is very unusual in French. [sent-223, score-0.108]
</p><p>63 In order to deal with unknown words and special tokens, we incorporate standard tagging features in the CRF: lowercase forms of the words, word prefixes of length 1 to 4, word suffice of length 1to 4, whether the word is capitalized, whether the token has a digit, whether it is an hyphen. [sent-227, score-0.08]
</p><p>64 The reranker models integrate features associated with each MWE node, the value of which is the compound itself. [sent-229, score-0.265]
</p><p>65 2 Exogenous Features Exogenous features are features that are not entirely derived from the (reference) corpus itself. [sent-231, score-0.058]
</p><p>66 They are computed from external data (in our case, our lexical resources). [sent-232, score-0.07]
</p><p>67 The lexical resources might be useful to discover new expressions: usually, expressions that have standard syntax like nominal compounds and are difficult to predict from the endogenous features. [sent-233, score-0.467]
</p><p>68 The resources are applied to the corpus through a lexical analysis that generates, for each sentence, a finite-state automaton TFSA which represents all the possible analyses. [sent-234, score-0.075]
</p><p>69 If the word belongs to a compound, the compound tag is also incorporated in the ambiguity class. [sent-239, score-0.16]
</p><p>70 For instance, the word night (either a simple noun or a simple adjective) in the context at night, is associated with the class adj noun adv+I as it is located inside a compound adverb. [sent-240, score-0.174]
</p><p>71 The lexical analysis can lead to a preliminary MWE segmentation  by using a shortest path algorithm that gives priority to compound analyses. [sent-242, score-0.183]
</p><p>72 This segmentation is also a source of features: a word belonging to a compound segment is assigned different properties such as the segment part-of-speech mwt and its syntactic structure mws encoded in the lexical resource, its relative position mwpos in the segment (’B’ or ’I’). [sent-243, score-0.341]
</p><p>73 In our collocation resource, each candidate collocation of the French treebank is associated with its internal syntactic structure and its association score (log-likelihood). [sent-245, score-0.206]
</p><p>74 Therefore, a given word in the corpus can be associated with different properties whether it belongs to a potential collocation: the class c and the internal structure cs of the collocation it belongs to, its position cpos in the collocation (B: beginning; I: remaining positions; O: outside). [sent-247, score-0.259]
</p><p>75 We first tested a standalone MWE recognizer based on CRF. [sent-253, score-0.079]
</p><p>76 We then combined MWE pregrouping based on this recognizer and the Berkeley (Petrov et al. [sent-254, score-0.094]
</p><p>77 , 2006) trained on the FTB where the compounds were concatenated (BKYc). [sent-255, score-0.145]
</p><p>78 Finally, we  parser5  combined the Berkeley parser trained on the FTB where the compounds are annotated with specific non-terminals (BKY), and the reranker. [sent-256, score-0.193]
</p><p>79 In all experiments, we varied the set of features: endo are all endogenous features; coll and lex include all endogenous features plus collocation-based features and lexicon-based ones, respectively; all is composed of both endogenous and exogenous features. [sent-257, score-0.455]
</p><p>80 The unlabeled attachement score [UAS] evaluates the quality of unlabeled 5We  used  the  version  adapted  to  French  in  the software Bonsai (Candito and Crabb ´e, 2009): http://alpage. [sent-265, score-0.054]
</p><p>81 And [LA]11 (Sampson,  2003) computes the similarity between all paths (sequence of nodes) from each terminal node to the root node of the tree. [sent-295, score-0.058]
</p><p>82 We also evaluated the MWE segmentation by using the unlabeled F1 score (U). [sent-299, score-0.058]
</p><p>83 In order to establish the statistical significance of results between two parsing experiments in terms of F1 and UAS, we used a unidirectional t-test for two independent samples12. [sent-301, score-0.064]
</p><p>84 2 Standalone Multiword recognition The results of the standalone MWE recognizer are given in table 3. [sent-306, score-0.118]
</p><p>85 That shows that most  of the work is done by fully automatically acquired features (as opposed to features coming from a manually constructed lexicon). [sent-313, score-0.058]
</p><p>86 The more precise system is the base one because it almost solely detects compounds present in the training corpus; nevertheless, it is unable to capture new MWEs (it has the 10This score is computed by using the tool available at http://ilk. [sent-316, score-0.17]
</p><p>87 it is the best one to discover new compounds as it is able to precisely detect irregular syntactic structures that are likely to be MWEs. [sent-332, score-0.174]
</p><p>88 3058  Table 3: MWE identification with CRF: base are the features corresponding to token properties and word ngrams. [sent-339, score-0.095]
</p><p>89 3  Combination of Multiword Expression Recognition and Parsing We tested and compared the two proposed discriminative strategies by varying the sets of MWEdedicated features. [sent-344, score-0.1]
</p><p>90 Table 5 compares the parsing systems, by showing the score differences between each of the tested system and the BKY parser. [sent-346, score-0.064]
</p><p>91 259834  Table 4: Parsing evaluation: pre indicates a MWE pregrouping strategy, whereas post is a reranking strategy with n = 50. [sent-353, score-0.136]
</p><p>92 s184)t  Table 5: Comparison of the strategies with respect to BKY parser. [sent-361, score-0.064]
</p><p>93 Both strategies also lead to a statistically significant UAS increase. [sent-366, score-0.064]
</p><p>94 Whereas both strategies improve the MWE recognition, pre-grouping is much more accurate (+2-4%); this might be due to the fact that an unlexicalized parser is limited in terms of compound identification, even within nbest analyses (cf. [sent-367, score-0.25]
</p><p>95 The benefits of lexicon-based features are confirmed in this experiment, whereas the use of collocations in the reranking strategy seems to be rejected. [sent-369, score-0.159]
</p><p>96 However, it performs very poorly in multiword verb recognition. [sent-377, score-0.356]
</p><p>97 7  Conclusions and Future Work  In this paper, we evaluated two discriminative strategies to integrate Multiword Expression Recognition in probabilistic parsing: (a) pre-grouping MWEs with a state-of-the-art recognizer and (b) MWE identification with a reranker after parsing. [sent-477, score-0.32]
</p><p>98 We showed that MWE pre-grouping significantly improves compound recognition and unlabeled dependency annotation, which implies that this strategy could be useful for dependency parsing. [sent-478, score-0.174]
</p><p>99 Un syst e`me de dictionnaires e´lectroniques pour les mots simples du fran ¸cais. [sent-560, score-0.102]
</p><p>100 A test of the leafancestor metric for parsing accuracy. [sent-674, score-0.064]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mwe', 0.688), ('multiword', 0.356), ('mwes', 0.267), ('mwer', 0.197), ('compounds', 0.145), ('bky', 0.112), ('endogenous', 0.112), ('compound', 0.108), ('expressions', 0.106), ('reranker', 0.098), ('collocations', 0.078), ('green', 0.076), ('sigogne', 0.07), ('french', 0.067), ('night', 0.066), ('collocation', 0.066), ('strategies', 0.064), ('parsing', 0.064), ('coup', 0.056), ('lefff', 0.056), ('watrin', 0.056), ('gross', 0.056), ('reranking', 0.052), ('recognizer', 0.052), ('yt', 0.05), ('crabb', 0.049), ('ftb', 0.049), ('parser', 0.048), ('position', 0.046), ('fran', 0.045), ('lexical', 0.044), ('treebank', 0.043), ('courtois', 0.042), ('dela', 0.042), ('mwn', 0.042), ('mws', 0.042), ('mwt', 0.042), ('pre', 0.042), ('pregrouping', 0.042), ('unitex', 0.042), ('candito', 0.042), ('abeill', 0.042), ('identification', 0.04), ('recognition', 0.039), ('exogenous', 0.037), ('lexicon', 0.037), ('integrating', 0.036), ('discriminative', 0.036), ('uas', 0.036), ('sag', 0.034), ('arun', 0.034), ('gillick', 0.034), ('de', 0.033), ('baldwin', 0.032), ('expression', 0.032), ('copestake', 0.031), ('segmentation', 0.031), ('grammar', 0.031), ('resources', 0.031), ('constant', 0.031), ('internal', 0.031), ('flat', 0.03), ('integrate', 0.03), ('nbest', 0.03), ('charniak', 0.029), ('discover', 0.029), ('templates', 0.029), ('node', 0.029), ('features', 0.029), ('ancestor', 0.028), ('bkyc', 0.028), ('cox', 0.028), ('idiomaticity', 0.028), ('ligm', 0.028), ('mwpos', 0.028), ('paumier', 0.028), ('ramisch', 0.028), ('sampson', 0.028), ('silberztein', 0.028), ('ones', 0.028), ('crf', 0.027), ('tag', 0.027), ('unlabeled', 0.027), ('discontinuous', 0.027), ('standalone', 0.027), ('token', 0.026), ('external', 0.026), ('tagging', 0.025), ('labelling', 0.025), ('universit', 0.025), ('belongs', 0.025), ('tool', 0.025), ('units', 0.025), ('mots', 0.024), ('villavicencio', 0.024), ('col', 0.024), ('cold', 0.024), ('composed', 0.024), ('parses', 0.024), ('fk', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="75-tfidf-1" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>Author: Matthieu Constant ; Anthony Sigogne ; Patrick Watrin</p><p>Abstract: and Parsing Anthony Sigogne Universit e´ Paris-Est LIGM, CNRS France s igogne @univ-mlv . fr Patrick Watrin Universit e´ de Louvain CENTAL Belgium pat rick .wat rin @ ucl ouvain .be view, their incorporation has also been considered The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.</p><p>2 0.090433933 <a title="75-tfidf-2" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>Author: Seyed Abolghasem Mirroshandel ; Alexis Nasr ; Joseph Le Roux</p><p>Abstract: Treebanks are not large enough to reliably model precise lexical phenomena. This deficiency provokes attachment errors in the parsers trained on such data. We propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser. Experiments on the French Treebank showed a relative decrease ofthe error rate of 7. 1% Labeled Accuracy Score yielding the best parsing results on this treebank.</p><p>3 0.069941655 <a title="75-tfidf-3" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>Author: Xiao Chen ; Chunyu Kit</p><p>Abstract: This paper presents a higher-order model for constituent parsing aimed at utilizing more local structural context to decide the score of a grammar rule instance in a parse tree. Experiments on English and Chinese treebanks confirm its advantage over its first-order version. It achieves its best F1 scores of 91.86% and 85.58% on the two languages, respectively, and further pushes them to 92.80% and 85.60% via combination with other highperformance parsers.</p><p>4 0.064064324 <a title="75-tfidf-4" href="./acl-2012-A_Class-Based_Agreement_Model_for_Generating_Accurately_Inflected_Translations.html">3 acl-2012-A Class-Based Agreement Model for Generating Accurately Inflected Translations</a></p>
<p>Author: Spence Green ; John DeNero</p><p>Abstract: When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders. 1</p><p>5 0.062472127 <a title="75-tfidf-5" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: We propose a simple generative, syntactic language model that conditions on overlapping windows of tree context (or treelets) in the same way that n-gram language models condition on overlapping windows of linear context. We estimate the parameters of our model by collecting counts from automatically parsed text using standard n-gram language model estimation techniques, allowing us to train a model on over one billion tokens of data using a single machine in a matter of hours. We evaluate on perplexity and a range of grammaticality tasks, and find that we perform as well or better than n-gram models and other generative baselines. Our model even competes with state-of-the-art discriminative models hand-designed for the grammaticality tasks, despite training on positive data alone. We also show fluency improvements in a preliminary machine translation experiment.</p><p>6 0.061718933 <a title="75-tfidf-6" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>7 0.061219938 <a title="75-tfidf-7" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>8 0.060991015 <a title="75-tfidf-8" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>9 0.060124051 <a title="75-tfidf-9" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>10 0.060037401 <a title="75-tfidf-10" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>11 0.055409972 <a title="75-tfidf-11" href="./acl-2012-Capturing_Paradigmatic_and_Syntagmatic_Lexical_Relations%3A_Towards_Accurate_Chinese_Part-of-Speech_Tagging.html">45 acl-2012-Capturing Paradigmatic and Syntagmatic Lexical Relations: Towards Accurate Chinese Part-of-Speech Tagging</a></p>
<p>12 0.05518505 <a title="75-tfidf-12" href="./acl-2012-Head-driven_Transition-based_Parsing_with_Top-down_Prediction.html">106 acl-2012-Head-driven Transition-based Parsing with Top-down Prediction</a></p>
<p>13 0.052927464 <a title="75-tfidf-13" href="./acl-2012-Incremental_Joint_Approach_to_Word_Segmentation%2C_POS_Tagging%2C_and_Dependency_Parsing_in_Chinese.html">119 acl-2012-Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese</a></p>
<p>14 0.052177817 <a title="75-tfidf-14" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>15 0.049131144 <a title="75-tfidf-15" href="./acl-2012-An_Exploration_of_Forest-to-String_Translation%3A_Does_Translation_Help_or_Hurt_Parsing%3F.html">25 acl-2012-An Exploration of Forest-to-String Translation: Does Translation Help or Hurt Parsing?</a></p>
<p>16 0.047751229 <a title="75-tfidf-16" href="./acl-2012-Attacking_Parsing_Bottlenecks_with_Unlabeled_Data_and_Relevant_Factorizations.html">30 acl-2012-Attacking Parsing Bottlenecks with Unlabeled Data and Relevant Factorizations</a></p>
<p>17 0.047598209 <a title="75-tfidf-17" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>18 0.046628848 <a title="75-tfidf-18" href="./acl-2012-Coarse_Lexical_Semantic_Annotation_with_Supersenses%3A_An_Arabic_Case_Study.html">49 acl-2012-Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study</a></p>
<p>19 0.046590444 <a title="75-tfidf-19" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>20 0.045522496 <a title="75-tfidf-20" href="./acl-2012-Classifying_French_Verbs_Using_French_and_English_Lexical_Resources.html">48 acl-2012-Classifying French Verbs Using French and English Lexical Resources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2012_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.146), (1, 0.008), (2, -0.103), (3, -0.061), (4, -0.024), (5, 0.02), (6, 0.029), (7, -0.041), (8, 0.008), (9, -0.006), (10, 0.001), (11, 0.007), (12, 0.009), (13, 0.013), (14, 0.004), (15, 0.004), (16, 0.012), (17, -0.015), (18, -0.055), (19, 0.003), (20, -0.017), (21, -0.016), (22, 0.021), (23, -0.017), (24, 0.024), (25, 0.031), (26, 0.034), (27, -0.016), (28, 0.02), (29, -0.012), (30, 0.033), (31, -0.002), (32, -0.014), (33, 0.093), (34, -0.022), (35, 0.017), (36, 0.077), (37, 0.046), (38, 0.013), (39, -0.028), (40, 0.041), (41, 0.079), (42, -0.03), (43, -0.034), (44, -0.045), (45, -0.102), (46, 0.102), (47, -0.057), (48, 0.092), (49, 0.167)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88590026 <a title="75-lsi-1" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>Author: Matthieu Constant ; Anthony Sigogne ; Patrick Watrin</p><p>Abstract: and Parsing Anthony Sigogne Universit e´ Paris-Est LIGM, CNRS France s igogne @univ-mlv . fr Patrick Watrin Universit e´ de Louvain CENTAL Belgium pat rick .wat rin @ ucl ouvain .be view, their incorporation has also been considered The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.</p><p>2 0.60702491 <a title="75-lsi-2" href="./acl-2012-Syntactic_Annotations_for_the_Google_Books_NGram_Corpus.html">189 acl-2012-Syntactic Annotations for the Google Books NGram Corpus</a></p>
<p>Author: Yuri Lin ; Jean-Baptiste Michel ; Erez Aiden Lieberman ; Jon Orwant ; Will Brockman ; Slav Petrov</p><p>Abstract: We present a new edition of the Google Books Ngram Corpus, which describes how often words and phrases were used over a period of five centuries, in eight languages; it reflects 6% of all books ever published. This new edition introduces syntactic annotations: words are tagged with their part-of-speech, and headmodifier relationships are recorded. The annotations are produced automatically with statistical models that are specifically adapted to historical text. The corpus will facilitate the study of linguistic trends, especially those related to the evolution of syntax.</p><p>3 0.58970994 <a title="75-lsi-3" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>Author: Seyed Abolghasem Mirroshandel ; Alexis Nasr ; Joseph Le Roux</p><p>Abstract: Treebanks are not large enough to reliably model precise lexical phenomena. This deficiency provokes attachment errors in the parsers trained on such data. We propose in this paper to compute lexical affinities, on large corpora, for specific lexico-syntactic configurations that are hard to disambiguate and introduce the new information in a parser. Experiments on the French Treebank showed a relative decrease ofthe error rate of 7. 1% Labeled Accuracy Score yielding the best parsing results on this treebank.</p><p>4 0.54074728 <a title="75-lsi-4" href="./acl-2012-Concept-to-text_Generation_via_Discriminative_Reranking.html">57 acl-2012-Concept-to-text Generation via Discriminative Reranking</a></p>
<p>Author: Ioannis Konstas ; Mirella Lapata</p><p>Abstract: This paper proposes a data-driven method for concept-to-text generation, the task of automatically producing textual output from non-linguistic input. A key insight in our approach is to reduce the tasks of content selection (“what to say”) and surface realization (“how to say”) into a common parsing problem. We define a probabilistic context-free grammar that describes the structure of the input (a corpus of database records and text describing some of them) and represent it compactly as a weighted hypergraph. The hypergraph structure encodes exponentially many derivations, which we rerank discriminatively using local and global features. We propose a novel decoding algorithm for finding the best scoring derivation and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study.</p><p>5 0.53941488 <a title="75-lsi-5" href="./acl-2012-Joint_Evaluation_of_Morphological_Segmentation_and_Syntactic_Parsing.html">122 acl-2012-Joint Evaluation of Morphological Segmentation and Syntactic Parsing</a></p>
<p>Author: Reut Tsarfaty ; Joakim Nivre ; Evelina Andersson</p><p>Abstract: We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance ofthe best parsing systems to date in the different scenarios.</p><p>6 0.50479722 <a title="75-lsi-6" href="./acl-2012-Higher-order_Constituent_Parsing_and_Parser_Combination.html">109 acl-2012-Higher-order Constituent Parsing and Parser Combination</a></p>
<p>7 0.48476312 <a title="75-lsi-7" href="./acl-2012-Cross-lingual_Parse_Disambiguation_based_on_Semantic_Correspondence.html">63 acl-2012-Cross-lingual Parse Disambiguation based on Semantic Correspondence</a></p>
<p>8 0.4740907 <a title="75-lsi-8" href="./acl-2012-langid.py%3A_An_Off-the-shelf_Language_Identification_Tool.html">219 acl-2012-langid.py: An Off-the-shelf Language Identification Tool</a></p>
<p>9 0.4667536 <a title="75-lsi-9" href="./acl-2012-Building_Trainable_Taggers_in_a_Web-based%2C_UIMA-Supported_NLP_Workbench.html">43 acl-2012-Building Trainable Taggers in a Web-based, UIMA-Supported NLP Workbench</a></p>
<p>10 0.46185297 <a title="75-lsi-10" href="./acl-2012-Exploiting_Multiple_Treebanks_for_Parsing_with_Quasi-synchronous_Grammars.html">87 acl-2012-Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars</a></p>
<p>11 0.45877755 <a title="75-lsi-11" href="./acl-2012-Utilizing_Dependency_Language_Models_for_Graph-based_Dependency_Parsing_Models.html">213 acl-2012-Utilizing Dependency Language Models for Graph-based Dependency Parsing Models</a></p>
<p>12 0.44871366 <a title="75-lsi-12" href="./acl-2012-Large-Scale_Syntactic_Language_Modeling_with_Treelets.html">127 acl-2012-Large-Scale Syntactic Language Modeling with Treelets</a></p>
<p>13 0.44600797 <a title="75-lsi-13" href="./acl-2012-Learning_Syntactic_Verb_Frames_using_Graphical_Models.html">130 acl-2012-Learning Syntactic Verb Frames using Graphical Models</a></p>
<p>14 0.43990397 <a title="75-lsi-14" href="./acl-2012-Beefmoves%3A_Dissemination%2C_Diversity%2C_and_Dynamics_of_English_Borrowings_in_a_German_Hip_Hop_Forum.html">39 acl-2012-Beefmoves: Dissemination, Diversity, and Dynamics of English Borrowings in a German Hip Hop Forum</a></p>
<p>15 0.4366836 <a title="75-lsi-15" href="./acl-2012-A_Comparison_of_Chinese_Parsers_for_Stanford_Dependencies.html">5 acl-2012-A Comparison of Chinese Parsers for Stanford Dependencies</a></p>
<p>16 0.43451115 <a title="75-lsi-16" href="./acl-2012-A_Feature-Rich_Constituent_Context_Model_for_Grammar_Induction.html">11 acl-2012-A Feature-Rich Constituent Context Model for Grammar Induction</a></p>
<p>17 0.43115851 <a title="75-lsi-17" href="./acl-2012-Dependency_Hashing_for_n-best_CCG_Parsing.html">71 acl-2012-Dependency Hashing for n-best CCG Parsing</a></p>
<p>18 0.41886407 <a title="75-lsi-18" href="./acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</a></p>
<p>19 0.41716123 <a title="75-lsi-19" href="./acl-2012-Lemmatisation_as_a_Tagging_Task.html">137 acl-2012-Lemmatisation as a Tagging Task</a></p>
<p>20 0.41708866 <a title="75-lsi-20" href="./acl-2012-Word_Epoch_Disambiguation%3A_Finding_How_Words_Change_Over_Time.html">216 acl-2012-Word Epoch Disambiguation: Finding How Words Change Over Time</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2012_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.012), (25, 0.025), (26, 0.041), (28, 0.025), (30, 0.379), (37, 0.049), (39, 0.032), (71, 0.012), (74, 0.038), (82, 0.026), (84, 0.033), (85, 0.027), (90, 0.112), (92, 0.032), (94, 0.014), (99, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94022304 <a title="75-lda-1" href="./acl-2012-Crowdsourcing_Inference-Rule_Evaluation.html">65 acl-2012-Crowdsourcing Inference-Rule Evaluation</a></p>
<p>Author: Naomi Zeichner ; Jonathan Berant ; Ido Dagan</p><p>Abstract: The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators.</p><p>2 0.85332578 <a title="75-lda-2" href="./acl-2012-Modeling_Review_Comments.html">144 acl-2012-Modeling Review Comments</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Writing comments about news articles, blogs, or reviews have become a popular activity in social media. In this paper, we analyze reader comments about reviews. Analyzing review comments is important because reviews only tell the experiences and evaluations of reviewers about the reviewed products or services. Comments, on the other hand, are readers’ evaluations of reviews, their questions and concerns. Clearly, the information in comments is valuable for both future readers and brands. This paper proposes two latent variable models to simultaneously model and extract these key pieces of information. The results also enable classification of comments accurately. Experiments using Amazon review comments demonstrate the effectiveness of the proposed models.</p><p>same-paper 3 0.81653923 <a title="75-lda-3" href="./acl-2012-Discriminative_Strategies_to_Integrate_Multiword_Expression_Recognition_and_Parsing.html">75 acl-2012-Discriminative Strategies to Integrate Multiword Expression Recognition and Parsing</a></p>
<p>Author: Matthieu Constant ; Anthony Sigogne ; Patrick Watrin</p><p>Abstract: and Parsing Anthony Sigogne Universit e´ Paris-Est LIGM, CNRS France s igogne @univ-mlv . fr Patrick Watrin Universit e´ de Louvain CENTAL Belgium pat rick .wat rin @ ucl ouvain .be view, their incorporation has also been considered The integration of multiword expressions in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly pre-identified. This paper evaluates two empirical strategies to integrate multiword units in a real constituency parsing context and shows that the results are not as promising as has sometimes been suggested. Firstly, we show that pregrouping multiword expressions before parsing with a state-of-the-art recognizer improves multiword recognition accuracy and unlabeled attachment score. However, it has no statistically significant impact in terms of F-score as incorrect multiword expression recognition has important side effects on parsing. Secondly, integrating multiword expressions in the parser grammar followed by a reranker specific to such expressions slightly improves all evaluation metrics.</p><p>4 0.7433964 <a title="75-lda-4" href="./acl-2012-A_Ranking-based_Approach_to_Word_Reordering_for_Statistical_Machine_Translation.html">19 acl-2012-A Ranking-based Approach to Word Reordering for Statistical Machine Translation</a></p>
<p>Author: Nan Yang ; Mu Li ; Dongdong Zhang ; Nenghai Yu</p><p>Abstract: Long distance word reordering is a major challenge in statistical machine translation research. Previous work has shown using source syntactic trees is an effective way to tackle this problem between two languages with substantial word order difference. In this work, we further extend this line of exploration and propose a novel but simple approach, which utilizes a ranking model based on word order precedence in the target language to reposition nodes in the syntactic parse tree of a source sentence. The ranking model is automatically derived from word aligned parallel data with a syntactic parser for source language based on both lexical and syntactical features. We evaluated our approach on largescale Japanese-English and English-Japanese machine translation tasks, and show that it can significantly outperform the baseline phrase- based SMT system.</p><p>5 0.59856635 <a title="75-lda-5" href="./acl-2012-Error_Mining_on_Dependency_Trees.html">83 acl-2012-Error Mining on Dependency Trees</a></p>
<p>Author: Claire Gardent ; Shashi Narayan</p><p>Abstract: In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data.</p><p>6 0.55322939 <a title="75-lda-6" href="./acl-2012-Modified_Distortion_Matrices_for_Phrase-Based_Statistical_Machine_Translation.html">148 acl-2012-Modified Distortion Matrices for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.5245322 <a title="75-lda-7" href="./acl-2012-Efficient_Tree-based_Approximation_for_Entailment_Graph_Learning.html">80 acl-2012-Efficient Tree-based Approximation for Entailment Graph Learning</a></p>
<p>8 0.52412993 <a title="75-lda-8" href="./acl-2012-Syntactic_Stylometry_for_Deception_Detection.html">190 acl-2012-Syntactic Stylometry for Deception Detection</a></p>
<p>9 0.52323067 <a title="75-lda-9" href="./acl-2012-Aspect_Extraction_through_Semi-Supervised_Modeling.html">28 acl-2012-Aspect Extraction through Semi-Supervised Modeling</a></p>
<p>10 0.5130257 <a title="75-lda-10" href="./acl-2012-Spice_it_up%3F_Mining_Refinements_to_Online_Instructions_from_User_Generated_Content.html">182 acl-2012-Spice it up? Mining Refinements to Online Instructions from User Generated Content</a></p>
<p>11 0.511356 <a title="75-lda-11" href="./acl-2012-Semi-supervised_Dependency_Parsing_using_Lexical_Affinities.html">175 acl-2012-Semi-supervised Dependency Parsing using Lexical Affinities</a></p>
<p>12 0.50922334 <a title="75-lda-12" href="./acl-2012-MIX_Is_Not_a_Tree-Adjoining_Language.html">139 acl-2012-MIX Is Not a Tree-Adjoining Language</a></p>
<p>13 0.4928616 <a title="75-lda-13" href="./acl-2012-Tokenization%3A_Returning_to_a_Long_Solved_Problem__A_Survey%2C_Contrastive_Experiment%2C_Recommendations%2C_and_Toolkit_.html">197 acl-2012-Tokenization: Returning to a Long Solved Problem  A Survey, Contrastive Experiment, Recommendations, and Toolkit </a></p>
<p>14 0.48486939 <a title="75-lda-14" href="./acl-2012-Hierarchical_Chunk-to-String_Translation.html">108 acl-2012-Hierarchical Chunk-to-String Translation</a></p>
<p>15 0.48174828 <a title="75-lda-15" href="./acl-2012-Modeling_the_Translation_of_Predicate-Argument_Structure_for_SMT.html">147 acl-2012-Modeling the Translation of Predicate-Argument Structure for SMT</a></p>
<p>16 0.48126957 <a title="75-lda-16" href="./acl-2012-Automatically_Learning_Measures_of_Child_Language_Development.html">34 acl-2012-Automatically Learning Measures of Child Language Development</a></p>
<p>17 0.47960079 <a title="75-lda-17" href="./acl-2012-Coupling_Label_Propagation_and_Constraints_for_Temporal_Fact_Extraction.html">60 acl-2012-Coupling Label Propagation and Constraints for Temporal Fact Extraction</a></p>
<p>18 0.47871983 <a title="75-lda-18" href="./acl-2012-Semantic_Parsing_with_Bayesian_Tree_Transducers.html">174 acl-2012-Semantic Parsing with Bayesian Tree Transducers</a></p>
<p>19 0.47798988 <a title="75-lda-19" href="./acl-2012-Ecological_Evaluation_of_Persuasive_Messages_Using_Google_AdWords.html">77 acl-2012-Ecological Evaluation of Persuasive Messages Using Google AdWords</a></p>
<p>20 0.47744945 <a title="75-lda-20" href="./acl-2012-String_Re-writing_Kernel.html">184 acl-2012-String Re-writing Kernel</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
