<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-183" href="../acl2011/acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">acl2011-183</a> <a title="acl-2011-183-reference" href="#">acl2011-183-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</h1>
<br/><p>Source: <a title="acl-2011-183-pdf" href="http://aclweb.org/anthology//P/P11/P11-1033.pdf">pdf</a></p><p>Author: Bin Lu ; Chenhao Tan ; Claire Cardie ; Benjamin K. Tsou</p><p>Abstract: Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8. 12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1</p><br/>
<h2>reference text</h2><p>Massih-Reza Amini, Cyril Goutte, and Nicolas Usunier.  2010. Combining coregularization and consensusbased self-training for multilingual text categorization. In Proceeding of SIGIR ’10. Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2010. Multilingual subjectivity: Are more languages better? In Proceedings of COLING ’10. Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proceedings of EMNLP’ ’08. Adam L. Berger, Stephen A. Della Pietra and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1). John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural corresponddence learning. In Proceedings of EMNLP ’06. Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT’98. Jordan Boyd-Graber and Philip Resnik. 2010. Holistic sentiment analysis across languages: Multilingual supervised Latent Dirichlet Allocation. In Proceedings of EMNLP ’10. Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of IJCAI’07. David Burkett, Slav Petrov, John Blitzer, and Dan  Klein. 2010. Learning better monolingual models with unannotated bilingual text. In Proceedings of CoNLL’ ’10. David Burkett and Dan Klein. 2008. Two languages are better than one (for syntactic parsing). In Proceedings of EMNLP ’08. Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of EMNLP’ ’08. Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 2009. Exploiting bilingual information to improve web search. In Proceedings of ACL/IJCNLP ‘09. Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI’04. Ido Dagan, and Alon Itai. 1994. Word sense disambiguation using a second language monolingual corpus, Computational Linguistics, 20(4): 563-596. 329 Thorsten Joachims. 1999a. Making Large-Scale SVM Learning Practical. In: Advances in Kernel Methods Support Vector Learning, B. Schölkopf, C. Burges, and A. Smola (ed.), MIT Press. Thorsten Joachims. 1999b. Transductive inference for text classification using support vector machines. In  Proceedings of ICML ’99. Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL’ ’06. Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, (45): 503–528. Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of CoNLL’ ’02. Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007. Learning multilingual subjective language via cross-lingual projections. In Proceedings of ACL ’07. Dragos S. Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4): 477–504. Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In Proceedings of NAACL/HLT ‘10. Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2): 103–134. Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.  Computational Linguistics, 29(1): 19-5 1. Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval, Now Publishers. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP ’02. Peter Prettenhofer and Benno Stein. 2010. Crosslanguage text classification using structural correspondence learning. In Proceedings of ACL ’10. Adwait Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report 97-08, University of Pennsylvania. Julia M. Schulz, Christa Womser-Hacker, and Thomas Mandl. 2010. Multilingual corpus development for opinion mining. In Proceedings of LREC’ ’10. Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-His Chen, and Noriko Kando. 2008. Overview of multilingual opinion analysis task at NTCIR-7. In Proceedings of the NTCIR-7 Workshop. Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun, Hsin-His Chen, Noriko Kando, and Chin-Yew Lin. 2007. Overview of opinion analysis pilot task at NTCIR-6. In Proceedings of the NTCIR-6 Workshop. Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 2005. A co-regularization approach to semi-  supervised learning with multiple views. In Proceedings of ICML ’05. Noah A. Smith. 2006. Novel estimation methods for unsupervised discovery of latent structure in natural language text. Ph.D. thesis, Department of Computer Science, Johns Hopkins University. Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning. 2005. A conditional random field word segmenter. In Proeedings of the SIGHAN Workshop. Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews, In Proceedings of ACL’ ’02. Xiaojun Wan. 2008. Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis. In Proceedings of EMNLP ’08. Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of ACL/AFNLP ’09.  4th  Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2- 3): 165-210. Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010. Cross-lingual latent topic extraction, In Proceedings of ACL ’10. Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009. Cross language dependency parsing using a  bilingual lexicon. ACL/IJCNLP’09.  In  Proceedings  of  Xiaojin Zhu and Andrew B. Goldberg. 2009. Introduction to Semi-Supervised Learning. Morgan & Claypool Publishers. Appendix A. Equation Deduction In this appendix, we derive the gradient for the objective function in Equation 3, which is used in parameter 330 estimation. As mentioned in Section 3.3, the parameters can be learned by finding:  Since the first term on the right-hand side is just the expression for the standard MaxEnt problem, we will focus on the gradient for the second term, and denote as ( ). Let denote or and be the th weight in the vector For brevity, we drop the in the above  , .  . Then  notation, and write to denote the partial derivative of (*) based on Equation 4 with respect to is as follows: (1)  Further, we obtain:  (2) Merge (2) into (1), we get:</p>
<br/>
<br/><br/><br/></body>
</html>
