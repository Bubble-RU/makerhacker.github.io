<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-207" href="../acl2011/acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">acl2011-207</a> <a title="acl-2011-207-reference" href="#">acl2011-207-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</h1>
<br/><p>Source: <a title="acl-2011-207-pdf" href="http://aclweb.org/anthology//P/P11/P11-1028.pdf">pdf</a></p><p>Author: S.R.K Branavan ; David Silver ; Regina Barzilay</p><p>Abstract: This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built- . in AI of Civilization II. 1</p><br/>
<h2>reference text</h2><p>R. Balla and A. Fern. 2009. UCT for tactical assault planning in real-time strategy games. In 21st International Joint Conference on Artificial Intelligence. Darse Billings, Lourdes Pe˜ na Castillo, Jonathan Schaeffer, and Duane Szafron. 1999. Using probabilistic knowledge and simulation to play poker. In 16th National Conference on Artificial Intelligence, pages 697–703. S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of ACL, pages 82–90. S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of ACL, pages 1268–1277. S.R.K. Branavan, David Silver, and Regina Barzilay. 2011. Non-linear monte-carlo search in civilization ii.  In Proceedings of IJCAI. John S. Bridle. 1990. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In Advances in NIPS, pages 211–217. Arthur E. Bryson and Yu-Chi Ho. 1969. Applied optimal control: optimization, estimation, and control. Blaisdell Publishing Company. Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC 2006. Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of EMNLP, pages 958–967. Michael Fleischman and Deb Roy. 2005. Intentional context in situated natural language learning. In Proceedings of CoNLL, pages 104–1 11. S. Gelly, Y. Wang, R. Munos, and O. Teytaud. 2006. Modification of UCT with patterns in Monte-Carlo Go. Technical Report 6062, INRIA. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):3 13–330. Raymond J. Mooney. 2008a. Learning language from its  perceptual context. In Proceedings of ECML/PKDD. Raymond J. Mooney. 2008b. Learning to connect language and perception. In Proceedings of AAAI, pages 1598–1601. James Timothy Oates. 2001. Grounding knowledge in sensors: Unsupervised learning for language and 277 Ph.D. thesis, University of Massachusetts  planning. Amherst. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning representations by backpropagating errors. Nature, 323:533–536. J. Sch a¨fer. 2008. The UCT algorithm applied to games with imperfect information. Diploma Thesis. Ottovon-Guericke-Universit a¨t Magdeburg. B. Sheppard. 2002. World-championship-caliber Scrabble. Artificial Intelligence, 134(1-2):241–275. D. Silver, R. Sutton, and M. M ¨uller. 2008. Samplebased learning and search with permanent and transient memories. In 25th International Conference on Machine Learning, pages 968–975. D. Silver. 2009. Reinforcement Learning and Simulation-Based Search in the Game of Go. Ph.D. thesis, University of Alberta. Jeffrey Mark Siskind. 2001. Grounding the lexical semantics of verbs in visual perception using force dy-  namics and event logic. Journal of Artificial Intelligence Research, 15:3 1–90. N. Sturtevant. 2008. An analysis of UCT in multi-player games. In 6th International Conference on Computers and Games, pages 37–49. Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning: An Introduction. The MIT Press. G. Tesauro and G. Galperin. 1996. On-line policy improvement using Monte-Carlo search. In Advances in Neural Information Processing 9, pages 1068–1074. Adam Vogel and Daniel Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the ACL, pages 806–814. Chen Yu and Dana H. Ballard. 2004. On the integration of grounding language and learning objects. In Proceedings of AAAI, pages 488–493.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
