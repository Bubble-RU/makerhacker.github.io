<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>189 acl-2011-K-means Clustering with Feature Hashing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-189" href="../acl2011/acl-2011-K-means_Clustering_with_Feature_Hashing.html">acl2011-189</a> <a title="acl-2011-189-reference" href="#">acl2011-189-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>189 acl-2011-K-means Clustering with Feature Hashing</h1>
<br/><p>Source: <a title="acl-2011-189-pdf" href="http://aclweb.org/anthology//P/P11/P11-3022.pdf">pdf</a></p><p>Author: Hajime Senuma</p><p>Abstract: One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</p><br/>
<h2>reference text</h2><p>Dimitris Achlioptas. 2003. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer and System Sciences, 66(4):671– 687, June. David Arthur and Sergei Vassilvitskii. 2007. k-means++ : The Advantages of Careful Seeding. In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1027–1035. Christos Boutsidis, Anastasios Zouzias, and Petros Drineas. 2010. Random Projections for k-means Clustering. In Advances in Neural Information Processing Systems 23, number iii, pages 298–306. Kuzman Ganchev and Mark Dredze. 2008. Small Statistical Models by Random Feature Mixing. In Proceedings of the ACL08 HLT Workshop on Mobile Language Processing, pages 19–20. Stuart P. Lloyd. 1982. Least Squares Quantization in PCM. IEEE Transactions on Information Theory, 28(2): 129–137. J MacQueen. 1967. Some Methods for Classification and Analysis of Multivariate Observations. In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability, pages 281–297. Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch u¨tze. 2008. Introduction to Information Re-  trieval. Cambridge University Press. Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. 2009. Hash Kernels for Structured Data. Journal of Machine Learning Research, 10:2615–2637. Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature Hashing for Large Scale Multitask Learning. In Proceedings of the 26th International Conference on Machine Learning.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
