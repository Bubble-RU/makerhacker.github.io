<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>301 acl-2011-The impact of language models and loss functions on repair disfluency detection</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-301" href="../acl2011/acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">acl2011-301</a> <a title="acl-2011-301-reference" href="#">acl2011-301-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>301 acl-2011-The impact of language models and loss functions on repair disfluency detection</h1>
<br/><p>Source: <a title="acl-2011-301-pdf" href="http://aclweb.org/anthology//P/P11/P11-1071.pdf">pdf</a></p><p>Author: Simon Zwarts ; Mark Johnson</p><p>Abstract: Unrehearsed spoken language often contains disfluencies. In order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, we study the effect of language model and loss function on the performance of a linear reranker that rescores the 25-best output of a noisychannel model. We show that language models trained on large amounts of non-speech data improve performance more than a language model trained on a more modest amount of speech data, and that optimising f-score rather than log loss improves disfluency detection performance. Our approach uses a log-linear reranker, operating on the top n analyses of a noisy channel model. We use large language models, introduce new features into this reranker and . examine different optimisation strategies. We obtain a disfluency detection f-scores of 0.838 which improves upon the current state-of-theart.</p><br/>
<h2>reference text</h2><p>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram  Version 1. Published by Linguistic Data Consortium, Philadelphia. Erik Brill and Michele Banko. 2001. Mitigating the Paucity-of-Data Problem: Exploring the Effect of Training Corpus Size on Classifier Performance for Natural Language Processing. In Proceedings of the First International Conference on Human Language Technology Research. Eugene Charniak and Mark Johnson. 2001 . Edit detec-  irfeuncgFWtlvinyeaors lpbywt.iahrmWngecshidlonagbvse ouftri-alvsgecnothmraeotwdulseahrifncyg.e alodtsu fphaesdrifounemcrtsionafluhdcsei-rCtpW1nioasrgTtlekapsofnhr1t.edc 8r2pi–AaC01rtsn2ei.46odcgPMauFDfetobisrlhvntedgfa,onErsDcbCgtyalhiovemLsNdpnuogTsMtraheinloAchmra.g,DleISLintcpaedPgCrucoKhisnetPvaocipdsrn,-t models reported in literature operating  on  identica7l10  tium, Philadelphia.  Christopher Cieri David, David Miller, and Kevin Walker. 2005. Fisher English Training Speech Part 2 Transcripts. Published by Linguistic Data Consortium, Philadelphia. John J. Godfrey and Edward Holliman. 1997. Switchboard-1 Release 2. Published by Linguistic Data Consortium, Philadelphia. David Graff and Christopher Cieri. 2003. English gigaword. Published by Linguistic Data Consortium, Philadelphia. Martin Jansche. 2005. Maximum Expected F-Measure Training of Logistic Regression Models. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Lan-  guage Processing, pages 692–699, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics. Mark Johnson and Eugene Charniak. 2004. A TAGbased noisy channel model of speech repairs. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 33–39. Mark Johnson, Eugene Charniak, and Matthew Lease. 2004. An Improved Model for Recognizing Disfluencies in Conversational Speech. In Proceedings of the Rich Transcription Fall Workshop. Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf. 2005. Effective Use of Prosody in Parsing Conversational Speech. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 233–240, Vancouver, British Columbia, Canada. Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 18 1– 184. Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3. Published by Linguistic Data Consortium, Philadelphia.  William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-Coverage Parsing using Human-Like Memory Constraints. Computational Linguistics, 36(1): 1–30. Richard Schwartz, Long Nguyen, Francis Kubala, George Chou, George Zavaliagkos, and John Makhoul. 1994. On Using Written Language Training Data for Spoken Language Modeling. In Proceedings of the Human Language Technology Workshop, pages 94–98. Elizabeth Shriberg and Andreas Stolcke. 1998. How model. In Proceedings of the International Conference on Spoken Language Processing, pages 2183– 2186. Elizabeth Shriberg. 1994. Preliminaries to a Theory of Speech Disuencies. Ph.D. thesis, University of California, Berkeley. David A. Smith and Jason Eisner. 2006. Minimum Risk Annealing for Training Log-Linear Models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 787–794. Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2004. A Lexically-Driven Algorithm for Disfluency Detection. In Proceedings of Human Language Technologies and North American Association for Compu-  tational Linguistics, pages 157–160. Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904. Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A progressive feature selection algorithm for ultra large feature spaces. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 561–568. far do speakers  back up in repairs?  A  quantitative71 1</p>
<br/>
<br/><br/><br/></body>
</html>
