<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-216" href="../acl2011/acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">acl2011-216</a> <a title="acl-2011-216-reference" href="#">acl2011-216-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</h1>
<br/><p>Source: <a title="acl-2011-216-pdf" href="http://aclweb.org/anthology//P/P11/P11-1023.pdf">pdf</a></p><p>Author: Chi-kiu Lo ; Dekai Wu</p><p>Abstract: We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost. As machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent. But more accurate, nonautomatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacyjudgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER. We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as HTER despite an even lower labor cost for the evaluation procedure. The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER. 1</p><br/>
<h2>reference text</h2><p>Satanjeev Banerjee and Alon Lavie. METEOR: An Au-  tomatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the 43th Annual Meeting of the Association of Computational Linguistics (ACL-05), pages 65–72, 2005. Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in Machine Translation Research. In Proceedings of the 13th Conference ofthe European Chapter oftheAssociationfor Computational Linguistics (EACL-06), pages 249–256, 2006. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. (Meta-) evaluation of Machine Translation. In Proceedings of the 2nd Workshop on Statistical Machine Translation, pages 136–158, 2007. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further Metaevaluation of Machine Translation. In Proceedings of the 3rd Workshop on Statistical Machine Translation, pages 70–106, 2008. Chris Callison-Burch, Philipp Koehn, Christof Monz, results  suggest future  potential  for a  fully  automatic vari2-28  Kay Peterson,  Mark Pryzbocki, and  Omar  Zaidan.  Findings ofthe 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation. In Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53, Uppsala, Sweden, 15-16 July 2010.  G. Doddington. Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology Research (HLT-02), pages 138–145, San Francisco, CA, USA, 2002. Morgan Kaufmann Publishers Inc. Jes ´us Gim e´nez and M `arquez. Linguistic Features for Automatic Evaluation of Heterogenous MT Systems. In Proceedings of the 2nd Workshop on Statistical Machine Translation, pages 256–264, Prague, Czech Republic, June 2007. Association for Computational Linguistics. Jes ´us Gim e´nez and M `arquez. A Smorgasbord of Features for Automatic MT Evaluation. In Proceedings of the 3rd Workshop on Statistical Machine Translation, pages 195–198, Columbus, OH, June 2008. Association for Computational Linguistics. Philipp Koehn and Christof Monz. Manual and Automatic Evaluation of Machine Translation between European Languages. In Proceedings of the Workshop on Statistical Machine Translation, pages 102–121 , 2006. Gregor Leusch, Nicola Ueffing, and HermannNey. CDer: Efficient MT Evaluation Using Block Movements. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06), 2006. Ding Liu and Daniel Gildea. Syntactic Features for Eval-  Llu´is  Llu´is  uation of Machine Translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, page 25, 2005. Ding Liu and Daniel Gildea. Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation. In Proceedings of the 2007 Conference of the North American Chapter of the Association of Computational Linguistics (NAACL-07), 2007. Chi-kiu Lo and Dekai Wu. Evaluating machine translation utility via semantic role labels. In Seventh International Conference on Language Resources and Evaluation (LREC-2010), pages 2873–2877, Malta, May 2010. Chi-kiu Lo and Dekai Wu. Semantic vs. syntactic vs. n-gram structure for machine translation evaluation. Workshop on Syntax and Structure in Statistical Translation (at COLING 2010), pages 52–60, Beijing, Aug 2010. Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In 22nd International Joint Conference on Artificial Intelligence (IJCAI-11), Barcelona, Jul 2011. To appear. Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. A Evaluation Tool for Machine Translation:  Fast Evaluation forMT Research. InProceedings ofthe 2nd International Conference on Language Resources and Evaluation (LREC-2000), 2000. Karolina Owczarzak, Josef van Genabith, and Andy Way. Evaluating machine translation with LFG dependencies. Machine Translation, 21:95–1 19, 2008. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. BLEU: A Method for Automatic Evaluation ofMachine Translation. In Proceedings ofthe 40th AnnualMeeting oftheAssociationfor ComputationalLinguistics (ACL-02), pages 3 11–3 18, 2002. Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow Semantic Parsing Using Support Vector Machines. In Proceedings of the 2004 Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL-04), 2004. Mark Przybocki, Kay Peterson, S ´ebastien Bronsart, and Gregory Sanders. The NIST 2008 Metrics for Machine Translation Challenge - Overview, Methodology, Metrics, and Results. Machine Tr, 23:71–103, 2010. Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06), pages 223–23 1, 2006.  Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. Accelerated DP Based Search For Statistical Translation. In Proceedings of the 5th European Conference on Speech Communication and Technology (EUROSPEECH-97), 1997. Clare R. Voss and Calandra R. Tate. Task-based Evaluation of Machine Translation (MT) Engines: Measuring How Well People Extract Who, When, Where-Type Elements in MT Output. In Proceedings of the 11th Annual Conference of the European Association for Machine Translation (EAMT-2006), pages 203–212, Oslo, Norway, June 2006. In Dekai Wu, editor, Proceedings  of SSST-4, Fourth229</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
