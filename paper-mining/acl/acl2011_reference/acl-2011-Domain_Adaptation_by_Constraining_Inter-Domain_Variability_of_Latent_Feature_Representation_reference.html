<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-103" href="../acl2011/acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">acl2011-103</a> <a title="acl-2011-103-reference" href="#">acl2011-103-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</h1>
<br/><p>Source: <a title="acl-2011-103-pdf" href="http://aclweb.org/anthology//P/P11/P11-1007.pdf">pdf</a></p><p>Author: Ivan Titov</p><p>Abstract: We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effec- tive on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks.</p><br/>
<h2>reference text</h2><p>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79: 151–175. Yoshua Bengio and Olivier Delalleau. 2007. Justify-  ing and generalizing contrastive divergence. Technical Report TR 13 11, Department IRO, University of Montreal, November. S. Bickel, M. Br ¨ueckner, and T. Scheffer. 2007. Discriminative learning for differing training and test distributions. In Proc. of the International Conference on Machine Learning (ICML), pages 81–88. Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, UK. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of EMNLP. John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. 45th Meeting of Association for Computational Linguistics (ACL), Prague, Czech Republic. John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman. 2008. Learning bounds for domain adaptation. In Proc. Advances In Neural Information Processing Systems (NIPS ’07). Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Proc. of the Conference on Empirical MethodsforNatural Language Processing (EMNLP), pages 285–292.  R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML. Hal Daum e´ and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence, 26: 101–126. Gregory Druck and Andrew McCallum. 2010. Highperformance semi-supervised learning using discriminatively constrained generative models. In Proc. of the International Conference on Machine Learning (ICML), Haifa, Israel. Kuzman Ganchev, Joao Graca, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research (JMLR), pages 2001–2049. Andrea Gesmundo, James Henderson, Paola Merlo, and Ivan Titov. 2009. Latent variable model of synchronous syntactic-semantic parsing for multiple languages. In CoNLL 2009 Shared Task. Zoubin Ghahramani and Michael I. Jordan. 1997. Factorial hidden Markov models. Machine Learning, 29:245–273. G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313:504–507. Geoffrey E. Hinton. 2002. Training Products of Experts  by Minimizing Contrastive Divergence. Neural Computation, 14: 1771–1800. Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proc. of the Annual Meeting of the ACL, pages 264–271, Prague, Czech Republic, June. Association for Computational Linguistics. Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of Machine Learning Research, 11:955–984. Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2008. Domain adaptation with multiple sources. In Advances in Neural Information Processing Systems. Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2009. Domain adaptation: Learning bounds and algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009), Montreal, Canada. 71  Andrew McCallum, Chris Pal, Greg Druck, and Xuerui Wang. 2006. Multi-conditional learning: Generative/discriminative training for clustering and classification. In AAAI. David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In Proc. of the Annual Meeting of the ACL and the International Conference on Computational Linguistics, Sydney, Australia. B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Barbara Plank. 2009. Structural correspondence learning for parse disambiguation. In Proceedings of the Student Research Workshop at EACL 2009, pages 37–45, Athens, Greece, April. Association for Computational Linguistics. Sandeepkumar Satpal and Sunita Sarawagi. 2007. Domain adaptation of conditional probability models via feature subsetting. In Proceedings of 11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Warzaw, Poland. Lawrence K. Saul, Tommi Jaakkola, and Michael I. Jordan. 1996. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61–76.  Paul Smolensky. 1986. Information processing in dynamical systems: foundations of harmony theory. In D. Rumehart and J McCelland, editors, Parallel distributed processing: explorations in the microstructures ofcognition, volume 1: Foundations, pages 194– 281. MIT Press. Ivan Titov and James Henderson. 2007a. Constituent parsing with Incremental Sigmoid Belief Networks. In Proc. 45th Meeting of Association for Computational Linguistics (ACL), pages 632–639, Prague, Czech Republic. Ivan Titov and James Henderson. 2007b. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proc. of the CoNLL shared task, Prague, Czech Republic. G.-R. Xue, W. Dai, Q. Yang, and Y. Yu. 2008. Topicbridged PLSA for cross-domain text classification. In Proceedings of the SIGIR Conference.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
