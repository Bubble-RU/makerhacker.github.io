<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-35" href="../acl2013/acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">acl2013-35</a> <a title="acl-2013-35-reference" href="#">acl2013-35-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</h1>
<br/><p>Source: <a title="acl-2013-35-pdf" href="http://aclweb.org/anthology//P/P13/P13-2119.pdf">pdf</a></p><p>Author: Kevin Duh ; Graham Neubig ; Katsuhito Sudoh ; Hajime Tsukada</p><p>Abstract: Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling un- known word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
