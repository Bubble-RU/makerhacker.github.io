<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-5" href="../acl2013/acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">acl2013-5</a> <a title="acl-2013-5-reference" href="#">acl2013-5-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</h1>
<br/><p>Source: <a title="acl-2013-5-pdf" href="http://aclweb.org/anthology//P/P13/P13-2024.pdf">pdf</a></p><p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><br/>
<h2>reference text</h2><p>Enrique Amig o´, Julio Gonzalo, and Felisa Verdejo. 2012. The heterogeneity principle in evaluation measures for automatic summarization. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 36–43, Montr ´eal, Canada, June. Association for Computational Linguistics. John M. Conroy and Hoa Trang Dang. 2008. Mind  the gap: Dangers of divorcing evaluations of summary content from linguistic quality. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 145–152, Manchester, UK, August. Coling 2008 Organizing Committee. John M. Conroy, Judith D. Schlesinger, and Dianne P. O’Leary. 2011. Nouveau-ROUGE: A Novelty Metric for Update Summarization. Computational Linguistics, 37(1): 1–8. George Giannakopoulos, Vangelis Karkaletsis, George A. Vouros, and Panagiotis Stamatopoulos. 2008. Summarization system evaluation revisited: N-gram graphs. TSLP, 5(3). George Giannakopoulos, George A. Vouros, and Vangelis Karkaletsis. 2010. Mudos-ng: Multidocument summaries using n-gram graphs (tech report). CoRR, abs/1012.2042. Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi Fukumoto. 2006. Automated summarization evaluation with basic elements. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06), pages 899–902. Alistair Kennedy, Anna Kazantseva Saif Mohammad,  Terry Copeck, Diana Inkpen, and Stan Szpakowicz. 2011. Getting emotional about news. In Fourth Text Analysis Conference (TAC 2011). Niraj Kumar, Kannan Srinathan, and Vasudeva Varma. 2011. Using unsupervised system with least linguistic features for tac-aesop task. In Fourth Text Analysis Conference (TAC 2011). N. Kumar, K. Srinathan, and V. Varma. 2012. Using graph based mapping of co-occurring words and closeness centrality score for summarization evaluation. Computational Linguistics and Intelligent Text Processing, pages 353–365. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics. Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without a gold standard. Computational Linguistics, 39:267–300. Ani Nenkova and Annie Louis. 2008. Can you summarize this? identifying correlates of input difficulty for multi-document summarization. In ACL, pages 825–833.  Ani Nenkova, Rebecca J. Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. TSLP, 4(2). Ani Nenkova. 2005. Discourse factors in multidocument summarization. In AAAI, pages 1654– 1655. Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1–9, Montr ´eal, Canada, June. Association for Computational Linguistics. Peter Rankel, John Conroy, Eric Slud, and Dianne O’Leary. 2011. Ranking human and machine summarization systems. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 467–473, Edinburgh, Scotland, UK., July. Association for Computational Linguistics. Peter A. Rankel, John M. Conroy, and Judith D. Schlesinger. 2012. Better metrics to automatically predict the quality of a text summary. Algorithms, 5(4):398–420.  Stephen Tratz and Eduard Hovy. 2008. Summarisation evaluation using transformed basic elements. In Proceedings TAC 2008. NIST. John von Neumann and Oskar Morgenstern. 1953. Theory of games and economic behavior. Princeton Univ. Press, Princeton, NJ, 3. ed. edition. Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics - Volume 2, COLING ’00, pages 947– 953, Stroudsburg, PA, USA. Association for Computational Linguistics. 136</p>
<br/>
<br/><br/><br/></body>
</html>
