<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-308" href="../acl2013/acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">acl2013-308</a> <a title="acl-2013-308-reference" href="#">acl2013-308-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</h1>
<br/><p>Source: <a title="acl-2013-308-pdf" href="http://aclweb.org/anthology//P/P13/P13-2121.pdf">pdf</a></p><p>Author: Kenneth Heafield ; Ivan Pouzyrevsky ; Jonathan H. Clark ; Philipp Koehn</p><p>Abstract: We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
