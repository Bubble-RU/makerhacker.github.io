<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-341" href="../acl2013/acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">acl2013-341</a> <a title="acl-2013-341-reference" href="#">acl2013-341-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</h1>
<br/><p>Source: <a title="acl-2013-341-pdf" href="http://aclweb.org/anthology//P/P13/P13-3007.pdf">pdf</a></p><p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><br/>
<h2>reference text</h2><p>David M. Blei and Andrew Y. Ng and Michael I. Jordan and John Lafferty. 2003. Latent dirichlet allocation, Journal of Machine Learning Research, Sergey Brin and Lawrence Page. 1998. The Anatony of a Large-scale Hypertextual Web Search Engine, Computer Networks and ISDN Systems, pages. 107–1 17. Gunes Erkan, 2004. LexRank: Graph-based Lexical Centrality as Salience in Text Summarization Journal of Artificial Intelligence Research 22, pages.457-479 Gunes Erkan. 2006. Language Model-Based Document Clustering Using Random Walks, Association for Computational Linguistics, pages.479–486. Samer Hassan, Rada Mihalcea and Carmen Banea. 2007. Random-Walk Term Weighting for Improved Text Classification, SIGIR ’07 Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages.829-830. Mario Kubek and Herwig Unger, 2011 Topic Detection Based on the PageRank’s Clustering Property, IICS’ 11, pages.139-148, Rada Mihalcea. 2004. Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization, Proceeding ACLdemo ’04 Proceedings of the 51  ACL 2004 on Interactive poster and demonstration sessions Article No. 20. Rada Mihalcea and Paul Tarau 2004. TextRank: Bringing Order into Texts, Conference on Empirical Methods in Natural Language Processing. David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin, 2010. Automatic evaluation of topic coherence, Human Language Technologies: The 2010 Annual Conference of the North Ametican Chapter of the Association for Computational Linguistics, pages. 100–108, Los Angeles. Christian Scheible, Hinrich Shutze. 2012. Bootstrapping Sentiment Labels For Unannotated Documents With Polarity PageRank, Proceedings of the Eight International Conference on Language Resources and Evaluation. Amarnag Subramanya, Jeff Bilmes. 2008. SoftSupervised Learning for Text Classification Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages. 1090–1099, Honolulu. Wei Wang, Diep Bich Do, and Xuemin Lin. 2005. Term Graph Model for Text Classification, Springer-Verlag Berlin Heidelberg 2005, pages. 19–30. Osmar R. Zaiane and Maria-luiza Antonie. 2002. Classifying Text Documents by Associating Terms with Text Categories, In Proc. of the Thirteenth Australasian  Database Conference (ADC’02), pages.215–222, X Zhu. 2005. Semi-supervised learning with Graphs, Ph.D thesis, Carnegie Mellon University.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
