<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-346" href="../acl2013/acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">acl2013-346</a> <a title="acl-2013-346-reference" href="#">acl2013-346-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</h1>
<br/><p>Source: <a title="acl-2013-346-pdf" href="http://aclweb.org/anthology//P/P13/P13-1071.pdf">pdf</a></p><p>Author: Oliver Ferschke ; Iryna Gurevych ; Marc Rittberger</p><p>Abstract: With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. . We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.</p><br/>
<h2>reference text</h2><p>Revision Structure  Readability  NONGNRGARMANMOWAIKLI • •  •  •  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  •  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  Named Entity  Misc  Flesch-Kincaid Gunning Fog Lix SMOG-Grading # Person entities∗ # Organization entities∗ # Location entities∗ # Characters # Sentences # Tokens Average sentence length Article lead length Lead to article ratio # Discussions  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  •  •  •  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ••  ∗ newly introduced feature # number of instances Table 4: Feature sets used in the experiments Table 4 lists all feature types used in our experiments. Since the feature space becomes large due to the ngram features, we prune it in two steps.  First,  we filter the ngrams according to their document frequency in the training corpus.  We discard all  ngrams that occur in less than x% and more than y% of all documents.  Several values for x and  y have been evaluated in parameter tuning experiments. The best results have been achieved with x=2 and y=90. In a second step, we apply the Information Gain feature selection approach (Mitchell, 1997) to the remaining set to determine the most useful features. Learning Algorithms We evaluated several learning algorithms from the Weka toolkit with respect to their performance on 727  Algorithm SVM RBF Kernel AdaBoost (decision stumps) SVM Poly Kernel RBF Network SVM Linear Kernel SVM PUK Kernel J48 Naive Bayes MultiBoostAB (decision stumps) Logistic Regression LibSVM One Class  Average F1 0.82 0.80 0.79 0.78 0.77 0.76 0.75 0.72 0.71 0.60 0.67  Table 5: Average F1-scores over all flaws on RELP using all features the quality flaw recognition task. Table 5 shows the average F1-score of each algorithm on the RELP dataset using all features. The performance has been evaluated with 10-fold cross validation on 2,000 documents split equally into positive and negative instances. One class classifiers are trained on the positive instances alone. We determined the best parameters for each algorithms in a parameter optimization run and list the results of the best configuration. Overall, Support Vector Machines with RBF kernels yielded the best average results and outperformed the other algorithms on every flaw. We used a sequential minimal optimization (SMO) algorithm (Platt, 1998) to train the SVMs and used different γ-values for the RBF kernel function. In contrast to Ferretti et al. (2012), we did not see significant improvements when optimizing γ for each individual flaw, so we determined one best setting  for each dataset. Since SVMs with RBF kernels are a special case of RBF networks that fit a single basis function to the data, we also used general RBF networks that can employ multiple basis functions, but we did not achieve better results with that approach. One-class classification, as proposed by Anderka et al. (2012), did not perform well within our setup. Even though we used an out-of-thebox one class classifier, we achieve similar results as Anderka et al. in their pessimistic setting, which best resembles our configuration. However, the performance still lacks behind the other approaches in our experiments. The best performing algorithm reported by Ferschke et al. (2012b), AdaBoost with decision stumps as a weak learner, showed the second best results in our experiments. 7  Evaluation and Discussion  The SVMs achieve a similar cross-validated performance on all feature sets containing ngrams, showing only minor improvements for individual flaws when adding non-lexical features. This  suggests that the classifiers largely depend on the ngrams and that other features do not contribute significantly to the classification performance. While structural quality flaws can be well captured by special purpose features or intensional modeling, as related work has shown, more subtle content flaws such as the neutrality and style flaws are mainly captured by the wording itself. Textual features beyond the ngram level, such as syntactic and semantic qualities of the text, could further improve the classification performance of these flaws and should be addressed in future work. Table 6 shows the performance of the SVMs with RBF kernel12 on each dataset using the NGRAM feature set. The average performance based on NOWIKI is slightly lower while using ALL features results in slightly higher average F1-scores. However, the differences are not statistically significant and thus omitted. Classifiers using the NONGRAM feature set achieved average F1-scores below 0.50 on all datasets. The results have been obtained by 10-fold cross validation on 2,000 documents per flaw.  The classifiers trained on reliable positives and random untagged articles (RELP) outperform the respective classifiers based on the BASE dataset for most flaws. This confirms our original hypothesis that using the appropriate revision of each tagged article is superior to using the latest available version from the dump. The performance on the RELALL dataset, in which the topic bias has been factored out, yields lower F1-scores than the two other approaches. Flaws that are restricted to a very narrow set of topics (i.e. Atopic in Fig. 1b is small), such as the in-universe flaw, show the biggest drop in performance. Since the topic bias plays a major role in the quality flaw detection task, as we have shown earlier, the topiccontrolled classifier cannot take advantage of the topic information, while the classifiers trained on the other corpora can make use of these characteristic as the most discriminative features. In the RELALL setting, however, the differences between the positive and negative instances are largely determined by the flaws alone. Classifiers trained on 12γ=0.01 for BASE,RELP and γ=0.001 for RELALL  728  such a dataset therefore come closer to recognizing the actual quality flaws, which makes them more useful in a practical setting despite lower cross-validated scores. In addition to cross-validation, we performed a cross-corpus evaluation of the classifiers for each flaw. Therefore, we evaluated the performance of the unbiased classifiers (trained on RELALL) on the biased data (RELP) and vice versa. Hereby, the positive training and test instances remain the same in both settings, while the unbiased data contains negative instances sampled from Arel and the unbiased data from Arnd (see Figure 1). With the NGRAM feature set, the reliable classifiers outperformed the unreliable classifiers on all flaws that can be well identified with lexical cues, such as Advert or Technical. In the biased case, we found both topic related and flaw specific ngrams among the most highly ranked ngram features. In the unbiased case, most of the informative ngrams were flaw specific expressions. Consequently, biased classifiers fail on the unbiased dataset in which  the positive and negative class are sampled from the same topics, which renders the highly ranked topic ngrams unusable. Flaws that do not largely rely on lexical cues, however, cannot be predicted more reliably with the unbiased classifier. This means that additional features are needed to describe these flaw. We tested this hypothesis by using the full feature set ALL and saw a substantial improvement on the side of the unbiased classifier, while the performance of the biased classifier remained unchanged. A direct comparison of our results to related work is difficult, since neutrality and style flaws have not been targeted before in a similar manner. However, the Advert flaw was also part of the ten flaw types in the PAN Quality Flaw Recognition Task (Anderka and Stein, 2012b). The best system achieved an F1 score of 0.839, which is just below the results of our system on the BASE dataset, which is similar to the PAN setup. 8  Conclusions  We showed  that text classification  based on  Wikipedia cleanup templates is prone to a topic bias which causes skewed classifiers and overly optimistic cross-validated evaluation results. This bias is known from other text classification applications, such as authorship attribution, genre detection and native language detection. We demonFlaw BASE RELP RELALL Advert Confusing Copy edit Essay-like Globalize In-universe Peacock POV Technical Tone Trivia Weasel  .86 .76 .81 .79 .85 .96 .77 .75 .87 .70 .72 .69  .88 .80 .73 .83 .87 .96 .82 .80 .88 .79 .77 .77  .75 .70 .72 .64 .69 .69 .69 .71 .67 .69 .70 .72  ?  .79  .83  .70  ?  Table 6: F1 scores for the 10-fold cross validation of the SVMs with RBF kernel on all datasets using NGRAM features  strated how to avoid the topic bias when creating quality flaw corpora. Unbiased corpora are not only necessary for training unbiased classifiers, they are also invaluable resources for gaining a deeper understanding of the linguistic properties of the flaws. Unbiased classifiers reflect much better the performance of quality flaw recognition “in the wild”, because they detect actual flawed articles rather than identifying the articles that are prone to certain quality due to their topic or subject matter. In our experiments, we presented a system for identifying Wikipedia articles with style and neutrality flaws, a novel category of quality problems that is of particular importance within and outside of Wikipedia. We showed that selecting a reliable set of positive training instances mined from the revision history improves the classification performance. In future work, we aim to extend our quality flaw detection system to not only find articles that contain a particular flaw, but also to identify the flaws within the articles, which can be achieved by leveraging the positional information of in-line cleanup templates.  Acknowledgments This work has been supported by the Volkswagen Foundation as part of the LichtenbergProfessorship Program under grant No. I/82806, and by the Hessian research excellence program “Landes-Offensive zur Entwicklung Wissenschaftlich-O¨konomischer Exzellenz” (LOEWE) as part of the research center ”Digital Humanities”. 729  Maik Anderka and Benno Stein. 2012a. A Breakdown of Quality Flaws in Wikipedia. In 2nd Joint WICOW/AIRWeb Workshop on Web Quality, pages 11–18, Lyon, France. Maik Anderka and Benno Stein. 2012b. Overview of the 1st International Competition on Quality Flaw Prediction in Wikipedia. In CLEF 2012 Evaluation Labs and Workshop Working Notes Papers. –  Maik Anderka, Benno Stein, and Nedim Lipka. 2012.  Predicting Quality Flaws in User-generated Content: The Case of Wikipedia. In 35th International ACM Conference on Research and Development in Information Retrieval, Portland, OR, USA. Julian Brooke and Graeme Hirst. 2011. Native language detection with ’cheap’ learner corpora. In Learner Corpus Research 2011 (LCR 2011). Rich a´rd Farkas, Veronika Vincze, Gy¨ orgy M o´ra, J a´nos Csirik, and Gy¨ orgy Szarvas. 2010. The CoNLL2010 shared task: learning to detect hedges and their scope in natural language text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’ 10: Shared Task, pages 1–12, Stroudsburg, PA, USA. Association for Computational Linguistics. Edgardo Ferretti, Donato Hern a´ndez Fusilier, Rafael Guzm a´n-Cabrera, Manuel Montes y G ´omez, Marcelo Errecalde, and Paolo Rosso. 2012. On the Use of PU Learning for Quality Flaw Prediction in Wikipedia. In CLEF 2012 Evaluation Labs and Workshop Working Notes Papers. –  Oliver Ferschke, Torsten Zesch, and Iryna Gurevych. 2011. Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia’s Edit History. In Proceedings of the 49th Annual Meeting ofthe Associationfor Computational Linguistics: Human Language Technolo-  gies. System Demonstrations, pages 97–102, Portland, OR, USA. Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar. 2012a. Behind the Article: Recognizing Dialog Acts in Wikipedia Talk Pages. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777– 786, Avignon, France. Oliver Ferschke, Iryna Gurevych, and Marc Rittberger. 2012b. FlawFinder: A Modular System for Predicting Quality Flaws in Wikipedia. In CLEF 2012 Evaluation Labs and Workshop Working Notes Papers, Rome, Italy. –  Aidan Finn and Nicholas Kushmerick. 2006. Learning to classify documents according to genre. Journal of the American Society for Information Science and Technology, 57(1 1): 1506–1518. Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1): 10–18. Moshe Koppel and Jonathan Schler. 2003. Exploiting stylistic idiosyncrasies for authorship attribution. In Workshop on Computational Approaches to Style Analysis and Synthesis, pages 69–72.  K. Luyckx and W. Daelemans. 2005. Shallow text analysis and machine learning for authorship attribution. In Proceedings of the Fifteenth Meeting of Computational Linguistics in the Netherlands (CLIN 2004), pages 149–160. Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. George K. Mikros and Eleni K. Argiri. 2007. Investigating topic influence in authorship attribution. In Proceedings of the SIGIR 2007 International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection, PAN 2007, Amsterdam, Netherlands. Thomas Mitchell. 1997. Machine Learning. McGrawHill Education, New York, NY, USA, 1st edition. John C Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, pages 185–208, Cambridge, MA, USA. Besiki Stvilia, Michael B. Twidale, Linda C. Smith, and Les Gasser. 2008. Information Quality Work Organization in Wikipedia. Journal of the American SocietyforInformation Science and Technology, 59(6):983–1001.  Gy¨ orgy Szarvas, Veronika Vincze, Rich a´rd Farkas, Gy¨ orgy M o´ra, and Iryna Gurevych. 2012. Crossgenre and cross-domain detection of semantic uncertainty. Comput. Linguist., 38(2):335–367. Torsten Zesch, Christof M ¨uller, and Iryna Gurevych. 2008. Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco. 730</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
