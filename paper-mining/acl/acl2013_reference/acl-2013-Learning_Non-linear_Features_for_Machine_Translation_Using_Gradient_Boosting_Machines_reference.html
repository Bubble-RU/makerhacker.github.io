<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-221" href="../acl2013/acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">acl2013-221</a> <a title="acl-2013-221-reference" href="#">acl2013-221-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</h1>
<br/><p>Source: <a title="acl-2013-221-pdf" href="http://aclweb.org/anthology//P/P13/P13-2072.pdf">pdf</a></p><p>Author: Kristina Toutanova ; Byung-Gyu Ahn</p><p>Abstract: In this paper we show how to automatically induce non-linear features for machine translation. The new features are selected to approximately maximize a BLEU-related objective and decompose on the level of local phrases, which guarantees that the asymptotic complexity of machine translation decoding does not increase. We achieve this by applying gradient boosting machines (Friedman, 2000) to learn new weak learners (features) in the form of regression trees, using a differentiable loss function related to BLEU. Our results indicate that small gains in perfor- mance can be achieved using this method but we do not see the dramatic gains observed using feature induction for other important machine learning tasks.</p><br/>
<h2>reference text</h2><p>Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. 1984. Classification and Regression Trees. Chapman and Hall. Chris Burges, Tal Shaked, Erin Renshaw, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In ICML. Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In HLTNAACL. David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. 2008a. Decomposability of translation metrics for improved evaluation and efficient algorithms. In EMNLP. David Chiang, Yuval Marton, and Philp Resnik. 2008b. Online large margin training of syntactic and structural translation features. In EMNLP.  D. Chiang, W. Wang, and K. Knight. 2009. 11,001 new features for statistical machine translation. In NAACL. Jonathan Clark, Alon Lavie, and Chris Dyer. 2012. One system, many domains: Open-domain statistical machine translation via feature augmentation. In AMTA. Hal Daum e´ III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In ACL. Jerome H. Friedman. 2000. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29: 1189–1232. Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In EMNLP. Patrick Nguyen, Milind Mahajan, and Xiaodong He. 2007. Training non-parametric features for statistical machine translation. In Second Workshop on Statistical Machine Translation. Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL.  Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu.  2002.  BLEU:  a method for automatic  evaluation of machine translation. TaroWatanabe,  Jun Suzuki,  In ACL.  Hajime  Tsukuda,  and  Hideki Isozaki. 2007. Online large-margin training for statistical machine translation.  In EMNLP.  410 Qiang Wu, Christopher J. Burges, Krysta M. Svore, and  hypotheses (the first hypothesis in each pair), and  4  currently receiving high scores.  1Jmi3a (nt3if)oe,n JgurenGter.aieov.al20m1e0a.suAredsa.ptiIn gfobrom astion gRfoetri envfoarl,- tih avien schroeiageshesorftahpdevrsacnsotearsge sin;mwitoersaek ilefsrowhtyerpakoinethgrethsoyedpseothc aretas ers Appendix A: Derivation of derivatives  Here we express the loss as a function of phraselevel in context scores and derive the derivative of the loss with respect to these scores. Let us number all phrase-pairs in context in all hypotheses in all sentences as p1, . . . , pR and denote their input feature vectors as x1, . . . , xR. We will use F(pr) and F(xr) interchangeably, because the score of a phrase-pair in context is defined by its input feature vector. The loss Ψ(F(xr)) is expressed as follows:  ∑nN=1∑kK=1log(1 + e∑pr∈hjnkF(xr)−∑pr∈hinkF(xr)).  Next∑ we derive the derivatives of the loss Ψ(F(x)) with respect to the phrase scores. Intuitively, we are treating the scores we want to learn as parameters for the loss function; thus the loss function has a huge number of parameters, one for each instance of each phrase pair in context in each translation. We ask the loss function if these scores could be set in an arbitrary way, what direction it would like to move them in to be minimized. This is the direction given by the negative gradient. Each phrase-pair in context pr occurs in exactly one hypothesis h in one sentence. It is possible that two phrase-pairs in context share the same set of input features, but for ease of implementation and exposition, we treat these as different training instances. To express the gradient with respect to F(xr) we therefore need to focus on the terms of the loss from a single sentence and to take into account the hypothesis pairs [hj,k, hi,k] where the left or the right hypothesis is the hypothesis h containing our focus phrase pair pr. is expressed as:  ∂∂ΨF(F(x(xr) )  =  +  ∑k:h=hik−1+e∑e∑prp∈r∈hjnhkjnkFF(x(rx)r−)−∑∑prp∈r∈hinhkinkFF(x(rx)r) ∑k:h=hjk1+e∑e∑prp∈rh∈jnhkjnkFF(x(rx)r−)−∑∑prp∈r∈hinhkinkFF(x(rx)r)  Since in the boosting step we induce a decision tree to fit the negative gradient, we can see that the feature induction algorithm is trying to increase the scores of phrases that occur in better 411</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
