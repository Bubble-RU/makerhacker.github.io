<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-247" href="../acl2013/acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">acl2013-247</a> <a title="acl-2013-247-reference" href="#">acl2013-247-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</h1>
<br/><p>Source: <a title="acl-2013-247-pdf" href="http://aclweb.org/anthology//P/P13/P13-2042.pdf">pdf</a></p><p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><br/>
<h2>reference text</h2><p>Bahl, L., Jelinek, F. & Mercer, R. 1983. A statistical approach to continuous speech recognition. IEEE Trans. Pattern Analysis and Machine Intelligence, 5: 179-190. Bellegarda, J. R. 1998. A multispan language modeling framework for larfge vocabulary speech recognition. IEEE Trans. on Speech and Audio Processing, 6(5): 456-467. Brown, P.F. 1992 Class-based n-gram models of natural language. Computational Linguistics, 18: 467479. Brun, A., Langlois, D. & Smaili, K. 2007. Improving language models by using distant information. In Proc. ISSPA 2007, pp.1-4. Cavnar, W.B. & Trenkle, J.M. 1994. N-gram-based  text categorization. Proc. SDAIR-94, pp. 161-175. Charniak, E., et al. 2000. BLLIP 1987-89 WSJ Corpus Release 1. Linguistic Data Consortium, Philadelphia. Chen, S.F. & Goodman, J. 1996. An empirical study of smoothing techniques for language modeling. In. Proc. ACL ’96, pp. 3 10-3 18. Chelba, C. & Jelinek, F. 2000. Structured language modeling. Computer Speech & Language, 14: 283332. Clarkson, P.R. & Robinson, A.J. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In Proc. ICASSP-97, pp.799802. Coccaro, N. 2005. Latent semantic analysis as a tool to improve automatic speech recognition performance. Doctoral Dissertation, University of Colorado, Boulder, CO, USA. Guthrie, D., Allison, B., Liu, W., Guthrie, L., & Wilks, Y. 2006. A closer look at skip-gram modelling. In Proc. LREC-2006, pp.1222-1225. Huang, X. et al. 1993. The SPHINX-II speech recognition system: an overview. Computer Speech and Language, 2: 137-148.  Katz, S.M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Trans. on Acoustics, Speech, & Signal Processing, 35:400-401 . Klakow, D. 1998. Log-linear interpolation of language model. In Proc. ICSLP 1998, pp.1-4. Kneser, R. & Ney, H. 1993. Improving clustering techniques for class-based statistical language modeling. In Proc. EUROSPEECH ’93, pp.973976. Kuhn, R. & Mori, R.D. 1990. A cache-based natural language model for speech recognition. IEEE Trans. Pattern Analysis and Machine Intelligence, 12(6): 570-583. Lau, R. et al. 1993. Trigger-based language models: a maximum-entropy approach. In Proc. ICASSP-94, pp.45-48. Lv Y. & Zhai C. 2009. Positional language models for information retrieval. In Proc. SIGIR’09, pp.299306. Rosenfeld, R. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10: 187-228.  Simons, M., Ney, H. & Martin S.C. 1997. Distant bigram language modelling using maximum entropy. In Proc. ICASSP-97, pp.787-790. Siu, M. & Ostendorf, M. 2000. Variable n-grams and extensions for conversational speech language modeling. IEEE Trans. on Speech and Audio Processing, 8(1): 63-75. Zhou G. & Lua K.T. 1998. Word association and MItrigger-based language modeling. In Proc. COLING-ACL, 1465-1471. 237</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
