<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-21" href="../acl2013/acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">acl2013-21</a> <a title="acl-2013-21-reference" href="#">acl2013-21-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</h1>
<br/><p>Source: <a title="acl-2013-21-pdf" href="http://aclweb.org/anthology//P/P13/P13-1138.pdf">pdf</a></p><p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><br/>
<h2>reference text</h2><p>Gabor Angeli, Percy Liang, and Dan Klein. 2012. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods for Natural Language Processing (EMNLP 2010), pages 502–512. Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of the 2005 Conference on Empirical Methods for Natural Language Processing (EMNLP 2005), pages 331–338. John Bateman and Michael Zock. 2003. Natural language generation. In R. Mitkov, editor, Oxford Handbook of Computational Linguistics, Research in Computational Semantics, pages 284–304. Oxford University Press, Oxford. Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In  Proceedings of the European Association for Computational Linguistics (EACL’06), pages 3 13–320. Anja Belz. 2007. Probabilistic generation of weather forecast texts. In Proceedings of Human Language Technologies 2007: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT’07), pages 164–171. Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In J. Bos and R. Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277–286. College Publications. Nadjet Bouayad-Agha, Gerard Casamayor, and Leo Wanner. 2011. Content selection from an ontologybased knowledge base for the generation of football summaries. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 72–81. Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation, pages 85–91 .  1414  Pablo A. Duboue and Kathleen R. McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Proceedings of the 2003 Conference on Empirical Methods for Natural Language Processing (EMNLP 2003), pages 2003– 2007. Eduard H. Hovy. 1993. Automated discourse generation using discourse structure relations. Artificial Intelligence, 63:341–385. Blake Howald, Ravi Kondadadi, and Frank Schilder. 2013. Domain adaptable semantic clustering in statistical nlg. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), pages 143–154. Association for Computational Linguistics, March. Thorsten Joachims. 2002. Learning to Classify Text Using Support Vector Machines. Kluwer. Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht. Colin Kelly, Ann Copestake, and Nikiforos Karama-  nis. 2009. Investigating content selection for language generation using machine learning. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG), pages 130–137. Ioannis Konstas and Mirella Lapata. 2012. Conceptto-text generation via discriminative reranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 369– 378. Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expression: A survey. Computational Linguistics, 38(1): 173–218. Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL’98), pages 704–710. Vladimir Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10:707–710. Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the 2011 Conference on Empirical Methods  for Natural Language Processing (EMNLP 2011), pages 1611–1622. Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Natural language generation with tree conditional random fields. In Proceedings of the 2009 Conference on Empirical Methods for Natural Language Processing (EMNLP 2009), pages 400–409. Kathleen R. McKeown. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press. Johanna D. Moore and Cecile L. Paris. 1993. Planning text for advisory dialogues: Capturing intentional and rhetorical information. Computational Linguistics, 19(4):651–694. Kishore Papineni, Slim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40thAnnual Meeting ofthe Associationfor Computational Linguistics (ACL’02), pages 311–3 18. Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press. Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin Yu. 2005. Choosing words in computer-generated  weather forecasts. Artificial Intelligence, 167: 137– 169. Jacques Robin and Kathy McKeown. 1996. Exmpirically designing and evaluating a new revision-based model for summary generation. Artificial Intelligence, 85(1-2). Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2001. A two-stage model for content determination. In Proceedings of the 8th European Workshop on Natural Language Generation (ENLG), pages 1–8. Kees van Deemter, Mari e¨t Theune, and Emiel Krahmer. 2005. Real vs. template-based natural language generation: a false opposition? Computational Linguistics, 31(1): 15–24. Ian Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Techniques with Java Implementation (2nd Ed.). Morgan Kaufmann, San Francisco, CA. 1415</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
