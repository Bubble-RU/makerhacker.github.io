<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-95" href="../acl2012/acl-2012-Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining.html">acl2012-95</a> <a title="acl-2012-95-reference" href="#">acl2012-95-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>95 acl-2012-Fast Syntactic Analysis for Statistical Language Modeling via Substructure Sharing and Uptraining</h1>
<br/><p>Source: <a title="acl-2012-95-pdf" href="http://aclweb.org/anthology//P/P12/P12-1019.pdf">pdf</a></p><p>Author: Ariya Rastrow ; Mark Dredze ; Sanjeev Khudanpur</p><p>Abstract: Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant speed improvements with both N-best and hill climbing rescoring, and show that up-training leads to WER reduction.</p><br/>
<h2>reference text</h2><p>C. Chelba and F. Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332. S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig. 2006. Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing, pages 1596–1608.  J. Cheppalier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice parsing for speech recognition. In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL’99). M Collins, B Roark, and M Saraclar. 2005. Discriminative syntactic language modeling for speech recognition. In ACL. Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In EMNLP. Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. In Proc. HLT-NAACL, number June, pages 742–750. Keith B Hall. 2005. Best-first word-lattice parsing: techniquesfor integrated syntactic language modeling. Ph.D. thesis, Brown University. L. Huang and K. Sagae. 2010. Dynamic Programming for Linear-Time Incremental Parsing. In Proceedings of ACL. Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with Products of Latent Variable Grammars. In Proc. EMNLP, number October, pages 12– 22. S. Khudanpur and J. Wu. 2000. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech and Language, pages 355–372.  S. K ¨ubler, R. McDonald, and J. Nivre. 2009. Dependency parsing. Synthesis Lectures on Human Language Technologies, 2(1): 1–127. Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-Hui Lee. 2002. Discriminative training of language models for speech recognition. In ICASSP. H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and L. Young-Suk. 2009. Syntactic features for Arabic speech recognition. In Proc. ASRU. Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713, Cambridge, MA, October. Association for Computational Linguistics. 183 Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur. 2011a. Efficient discrimnative training of long-span language models. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). Ariya Rastrow, Markus Dreyer, Abhinav Sethy, Sanjeev Khudanpur, Bhuvana Ramabhadran, and Mark Dredze. 2011b. Hill climbing on speech lattices : A new rescoring framework. In ICASSP. Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer  Speech & Language, 21(2). K. Sagae and A. Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proc. ACL, pages 691–698. Association for Computational Linguistics. K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proc. EMNLP-CoNLL, volume 7, pages 1044–1050. Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Kazama. 2011. Learning with Lookahead : Can History-Based Models Rival Globally Optimized Models ? In Proc. CoNLL, number June, pages 238– 246. Ralph Weischedel, Sameer Pradhan, Lance Ramshaw, Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, and Ann Houston, 2008. OntoNotes Release 2.0. Linguistic Data Consortium, Philadelphia.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
