<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2012" href="../home/acl2012_home.html">acl2012</a> <a title="acl-2012-29" href="../acl2012/acl-2012-Assessing_the_Effect_of_Inconsistent_Assessors_on_Summarization_Evaluation.html">acl2012-29</a> <a title="acl-2012-29-reference" href="#">acl2012-29-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 acl-2012-Assessing the Effect of Inconsistent Assessors on Summarization Evaluation</h1>
<br/><p>Source: <a title="acl-2012-29-pdf" href="http://aclweb.org/anthology//P/P12/P12-2070.pdf">pdf</a></p><p>Author: Karolina Owczarzak ; Peter A. Rankel ; Hoa Trang Dang ; John M. Conroy</p><p>Abstract: We investigate the consistency of human assessors involved in summarization evaluation to understand its effect on system ranking and automatic evaluation techniques. Using Text Analysis Conference data, we measure annotator consistency based on human scoring of summaries for Responsiveness, Readability, and Pyramid scoring. We identify inconsistencies in the data and measure to what extent these inconsistencies affect the ranking of automatic summarization systems. Finally, we examine the stability of automatic metrics (ROUGE and CLASSY) with respect to the inconsistent assessments.</p><br/>
<h2>reference text</h2><p>Deen G. Freelon. 2010. ReCal: Intercoder Reliability Calculation as a Web Service. International Journal of Internet Science, Vol 5(1). Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, 78–8 1. Barcelona, Spain. Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating content selection in summarization: The Pyramid method. Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, 145– 152. Boston, MA. Rebecca J. Passonneau, Ani Nenkova, Kathleen McKeown, and Sergey Sigelman. 2005. Applying the Pyramid method in DUC 2005. Proceedings of the 5th Document Understanding Conference (DUC). Van-  couver, Canada. Peter A. Rankel, John M. Conroy, and Judith D. Schlesinger. 2012. Better Metrics to Automatically Predict the Quality of a Text Summary. Proceedings of the SIAM Data Mining Text Mining Workshop 2012.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
