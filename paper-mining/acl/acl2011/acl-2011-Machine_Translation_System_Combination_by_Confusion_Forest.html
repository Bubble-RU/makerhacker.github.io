<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>217 acl-2011-Machine Translation System Combination by Confusion Forest</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-217" href="#">acl2011-217</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>217 acl-2011-Machine Translation System Combination by Confusion Forest</h1>
<br/><p>Source: <a title="acl-2011-217-pdf" href="http://aclweb.org/anthology//P/P11/P11-1125.pdf">pdf</a></p><p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>Reference: <a title="acl-2011-217-reference" href="../acl2011_reference/acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('forest', 0.573), ('confus', 0.382), ('cfv', 0.239), ('network', 0.22), ('earley', 0.181), ('consens', 0.168), ('hypothes', 0.157), ('rost', 0.149), ('skeleton', 0.141), ('transl', 0.114), ('snp', 0.109), ('align', 0.1), ('vp', 0.098), ('hypergraph', 0.095), ('horizont', 0.093), ('hyperedg', 0.087), ('pack', 0.082), ('cf', 0.081), ('npp', 0.077), ('merg', 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="217-tfidf-1" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>2 0.4181833 <a title="217-tfidf-2" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>3 0.39899683 <a title="217-tfidf-3" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>4 0.23587219 <a title="217-tfidf-4" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>5 0.19692317 <a title="217-tfidf-5" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>6 0.1903744 <a title="217-tfidf-6" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>7 0.16038778 <a title="217-tfidf-7" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>8 0.15609662 <a title="217-tfidf-8" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>9 0.14766021 <a title="217-tfidf-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.14154775 <a title="217-tfidf-10" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>11 0.13726062 <a title="217-tfidf-11" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>12 0.12837793 <a title="217-tfidf-12" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>13 0.12629336 <a title="217-tfidf-13" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>14 0.11541461 <a title="217-tfidf-14" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>15 0.11027716 <a title="217-tfidf-15" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>16 0.10711141 <a title="217-tfidf-16" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>17 0.10542937 <a title="217-tfidf-17" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>18 0.10340993 <a title="217-tfidf-18" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>19 0.10182875 <a title="217-tfidf-19" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>20 0.095678575 <a title="217-tfidf-20" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, -0.201), (2, -0.116), (3, 0.085), (4, 0.07), (5, -0.124), (6, -0.074), (7, 0.003), (8, -0.123), (9, 0.009), (10, -0.051), (11, -0.013), (12, -0.039), (13, -0.042), (14, -0.028), (15, 0.054), (16, -0.025), (17, 0.091), (18, 0.003), (19, 0.045), (20, 0.018), (21, -0.138), (22, 0.188), (23, 0.029), (24, 0.092), (25, 0.071), (26, 0.076), (27, -0.051), (28, 0.011), (29, 0.072), (30, -0.066), (31, -0.108), (32, -0.139), (33, 0.0), (34, 0.122), (35, -0.104), (36, 0.007), (37, 0.019), (38, -0.099), (39, 0.085), (40, -0.058), (41, -0.054), (42, -0.13), (43, -0.027), (44, 0.01), (45, 0.108), (46, 0.025), (47, 0.147), (48, 0.059), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90992552 <a title="217-lsi-1" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>2 0.83359838 <a title="217-lsi-2" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>3 0.74972486 <a title="217-lsi-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.73734421 <a title="217-lsi-4" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>5 0.66362894 <a title="217-lsi-5" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>6 0.63785577 <a title="217-lsi-6" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>7 0.61959201 <a title="217-lsi-7" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>8 0.57105201 <a title="217-lsi-8" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>9 0.56609535 <a title="217-lsi-9" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>10 0.54746854 <a title="217-lsi-10" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>11 0.52864134 <a title="217-lsi-11" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>12 0.5078733 <a title="217-lsi-12" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>13 0.44555855 <a title="217-lsi-13" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>14 0.41970804 <a title="217-lsi-14" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>15 0.40497816 <a title="217-lsi-15" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>16 0.40409487 <a title="217-lsi-16" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>17 0.39436844 <a title="217-lsi-17" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>18 0.38836196 <a title="217-lsi-18" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>19 0.38689455 <a title="217-lsi-19" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>20 0.37942797 <a title="217-lsi-20" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.058), (31, 0.092), (43, 0.014), (53, 0.041), (69, 0.062), (78, 0.095), (79, 0.121), (89, 0.013), (90, 0.158), (94, 0.153), (97, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91889435 <a title="217-lda-1" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>2 0.9090755 <a title="217-lda-2" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>same-paper 3 0.88149226 <a title="217-lda-3" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>4 0.86279285 <a title="217-lda-4" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>Author: David Chen ; William Dolan</p><p>Abstract: A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.</p><p>5 0.85905159 <a title="217-lda-5" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>Author: Michael Bendersky ; W. Bruce Croft ; David A. Smith</p><p>Abstract: W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA cro ft @ c s .uma s s .edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@ c s .umas s .edu articles or web pages). As previous research shows, these differences severely limit the applicability of Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an impor- tant part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</p><p>6 0.84765548 <a title="217-lda-6" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>7 0.84591699 <a title="217-lda-7" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>8 0.8453005 <a title="217-lda-8" href="./acl-2011-Extracting_Opinion_Expressions_and_Their_Polarities_-_Exploration_of_Pipelines_and_Joint_Models.html">131 acl-2011-Extracting Opinion Expressions and Their Polarities - Exploration of Pipelines and Joint Models</a></p>
<p>9 0.84383762 <a title="217-lda-9" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>10 0.84060246 <a title="217-lda-10" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>11 0.84046757 <a title="217-lda-11" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>12 0.83920479 <a title="217-lda-12" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>13 0.83592969 <a title="217-lda-13" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>14 0.83582151 <a title="217-lda-14" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>15 0.83419365 <a title="217-lda-15" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>16 0.83318794 <a title="217-lda-16" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>17 0.83215249 <a title="217-lda-17" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>18 0.8318541 <a title="217-lda-18" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>19 0.83102709 <a title="217-lda-19" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>20 0.82992828 <a title="217-lda-20" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
