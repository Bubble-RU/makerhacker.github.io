<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-60" href="#">acl2011-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</h1>
<br/><p>Source: <a title="acl-2011-60-pdf" href="http://aclweb.org/anthology//P/P11/P11-2031.pdf">pdf</a></p><p>Author: Jonathan H. Clark ; Chris Dyer ; Alon Lavie ; Noah A. Smith</p><p>Abstract: In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.</p><p>Reference: <a title="acl-2011-60-reference" href="../acl2011_reference/acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. [sent-7, score-0.087]
</p><p>2 We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. [sent-10, score-1.009]
</p><p>3 1 Introduction The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003). [sent-11, score-0.223]
</p><p>4 In that work, the proposed  method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets. [sent-12, score-0.312]
</p><p>5 However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters. [sent-13, score-0.099]
</p><p>6 In this paper, we present a series of experiments demonstrating that optimizer instability can account for substantial amount ofvariation in translation quality,2 which, if not controlled for, could lead to incorrect conclusions. [sent-18, score-1.008]
</p><p>7 We then show that it is possible to control for this variable with a high degree of confidence with only a few replications of the experiment and conclude by suggesting new best practices for significance testing for ma-  chine translation. [sent-19, score-0.482]
</p><p>8 2  Nondeterminism and Other Optimization Pitfalls  Statistical machine translation systems consist of a model whose parameters are estimated to maximize some objective function on a set of development data. [sent-20, score-0.118]
</p><p>9 , 1-best BLEU, expected BLEU, marginal likelihood) are not convex, only approximate solutions to the optimization problem are available, and the parameters learned are typically only locally optimal and may strongly depend on parameter initialization and search hyperparameters. [sent-23, score-0.129]
</p><p>10 Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al. [sent-24, score-0.191]
</p><p>11 , 2010),3 constitute a second, more obvious source of noise in the optimization procedure. [sent-25, score-0.146]
</p><p>12 This variation in the parameter vector affects the quality of the model measured on both development 2This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. [sent-26, score-0.104]
</p><p>13 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 176–181, data and held-out test data, independently of any experimental manipulation. [sent-32, score-0.117]
</p><p>14 Thus, when trying to determine whether the difference between two measurements is significant, it is necessary to control for variance due to noisy parameter estimates. [sent-33, score-0.134]
</p><p>15 This can be done by replication of the optimization procedure with different starting conditions (e. [sent-34, score-0.175]
</p><p>16 Unfortunately, common practice in reporting machine translation results is to run the optimizer once per system configuration and to draw conclusions about the experimental manipulation from this single sample. [sent-37, score-0.967]
</p><p>17 However, it could be that a particular sample is on the “low” side of the distribution over optimizer outcomes (i. [sent-38, score-0.724]
</p><p>18 The danger here is obvious: a high baseline result paired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. [sent-41, score-0.234]
</p><p>19 3  Related Work  The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. [sent-43, score-0.091]
</p><p>20 Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al. [sent-44, score-0.118]
</p><p>21 Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type Ierrors (i. [sent-46, score-0.164]
</p><p>22 However, these previous methods assume model parameters are elements of the system rather than extraneous variables. [sent-51, score-0.197]
</p><p>23 Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). [sent-52, score-2.218]
</p><p>24 Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. [sent-53, score-0.295]
</p><p>25 However, they only briefly mention the implications  of the instability on significance. [sent-54, score-0.247]
</p><p>26 (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. [sent-59, score-0.143]
</p><p>27 4  Experiments  In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set ofoptimizer samples on two different pairs of systems (4 configurations total). [sent-60, score-0.721]
</p><p>28 The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0. [sent-62, score-0.22]
</p><p>29 7M words), the later of which was decoded with the cdec decoder (Koehn et al. [sent-63, score-0.103]
</p><p>30 4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). [sent-67, score-0.12]
</p><p>31 The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. [sent-68, score-0.078]
</p><p>32 The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al. [sent-70, score-0.144]
</p><p>33 The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al. [sent-72, score-0.141]
</p><p>34 org/wmt 11/  tomatic metrics due to test-set and optimizer variability. [sent-77, score-0.681]
</p><p>35 sdev is reported only for the tuning objective function BLEU. [sent-78, score-0.232]
</p><p>36 1  Extraneous variables in one system  In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. [sent-83, score-0.319]
</p><p>37 We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. [sent-84, score-0.088]
</p><p>38 Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. [sent-86, score-0.477]
</p><p>39 As discussed above, different optimization runs find different local maxima. [sent-87, score-0.146]
</p><p>40 The noise due to this variable can depend on many number of factors, including the number of random restarts used (in MERT), the number of features in a model, the number of references, the language pair, the portion of the search space visible to the optimizer (e. [sent-88, score-0.822]
</p><p>41 Unfortunately, there is no proxy to estimate this effect as with bootstrap resampling. [sent-91, score-0.112]
</p><p>42 To control for this variable, we must run the optimizer multiple times to estimate the spread it induces on the development set. [sent-92, score-0.767]
</p><p>43 Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1. [sent-93, score-0.728]
</p><p>44 178 the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev:  sdev=utvuXi=n1(mni− − m 1)2 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e. [sent-95, score-1.099]
</p><p>45 more random restarts in MERT) could improve system performance. [sent-97, score-0.123]
</p><p>46 Overfitting effects stest As with any optimizer, there is a danger that the optimal weights for a tuning set may not generalize well to unseen data (i. [sent-98, score-0.258]
</p><p>47 For a randomized optimizer, this means that parameters can generalize to different degrees over multiple optimizer runs. [sent-101, score-0.672]
</p><p>48 We measure the spread induced by optimizer randomness on the test set metric score stest, as opposed to the overfitting effect in isolation. [sent-102, score-0.875]
</p><p>49 The computation of stest is identical to sdev except that the mis are the translation metrics calculated on the test set. [sent-103, score-0.542]
</p><p>50 stest  Test set selection ssel The final extraneous variable we consider is the selection of the test set itself. [sent-105, score-0.459]
</p><p>51 A good test set should be representative of the domain or language for which experimental evidence is being considered. [sent-106, score-0.117]
</p><p>52 However, with only a single test corpus, we may have unreliable results because of idiosyncrasies in the test set. [sent-107, score-0.122]
</p><p>53 First, replication of experiments by testing on multiple, non-overlapping test sets can eliminate it directly. [sent-109, score-0.192]
</p><p>54 6 Furthermore, this can be done for each of our optimizer samples. [sent-111, score-0.641]
</p><p>55 optimizer samples, we have a statistic that jointly quantifies the impact of test set effects and optimizer instability on a test set. [sent-113, score-1.766]
</p><p>56 For example, a large ssel indicates that more replications will be necessary to draw reliable inferences from experiments on this test set, so a larger test set may be helpful. [sent-116, score-0.49]
</p><p>57 To compute ssel, assume we have n independent optimization runs which produced weight vectors that were used to translate a test set n times. [sent-117, score-0.207]
</p><p>58 The test set has ‘ segments with references R = hR1, R2, . [sent-118, score-0.105]
</p><p>59 , XhXi‘i is the list ofi translated segments Xfrom the ith optimization run list of the ‘ translated segments of the test set. [sent-129, score-0.315]
</p><p>60 For each hypothesis output Xi, we construct k bootstrap replicates by drawing ‘ segments uniformly, with replacement, from Xi, together with its corresponding reference. [sent-130, score-0.272]
</p><p>61 This produces k virtual test sets for each optimization run i. [sent-131, score-0.224]
</p><p>62 We designate the score of the jth virtual test set of the ith optimization run with mij. [sent-132, score-0.257]
</p><p>63 2 Comparing Two Systems In the previous section, we gave statistics about the distribution of evaluation metrics across a large number of experimental samples (Table 1). [sent-134, score-0.176]
</p><p>64 Because of the large number of trials we carried out, we can be extremely confident in concluding that for both pairs of systems, the experimental manipulation accounts for the observed metric improvements, and furthermore, that we have a good estimate of the magnitude of that improvement. [sent-135, score-0.22]
</p><p>65 However, it is not generally feasible to perform as many replications as we did, so here we turn to the question of how to compare two systems, accounting for optimizer noise, but without running 300 replications. [sent-136, score-0.862]
</p><p>66 We begin with a visual illustration how optimizer instability affects test set scores when comparing two systems. [sent-137, score-0.981]
</p><p>67 Figure 1 plots the histogram of the 300 optimizer samples each from the two  BTEC Chinese-English systems. [sent-138, score-0.79]
</p><p>68 The phrase-based 179  BLEU Figure 1: Histogram of test set BLEU scores for the BTEC phrase-based system (left) and BTEC hierarchical system (right). [sent-139, score-0.159]
</p><p>69 5 BLEU in expectation, there is a non-trivial region of overlap indicating that some random outcomes will result in little to no difference being observed. [sent-141, score-0.181]
</p><p>70 BLEU difference Figure 2: Relative frequencies of obtaining differences in BLEU scores on the WMT system as a function of the number of optimizer samples. [sent-142, score-0.723]
</p><p>71 Crucially, although the distributions are distinct, there is a non-trivial region of overlap, and  experimental samples from the overlapping region could suggest the opposite conclusion! [sent-150, score-0.23]
</p><p>72 To further underscore the risks posed by this overlap, Figure 2 plots the relative frequencies with which different BLEU score deltas will occur, as a function of the number of optimizer samples used. [sent-151, score-0.754]
</p><p>73 To determine whether an experimental manipulation results in a statistically reliable difference for an evaluation metric, we use a stratified approximate randomization (AR) test. [sent-153, score-0.264]
</p><p>74 This is a nonparametric test that approximates a paired permutation test by sampling permutations (Noreen, 1989). [sent-154, score-0.122]
</p><p>75 AR estimates the probability (p-value) that a measured difference in metric scores arose by chance by randomly exchanging sentences between the two systems. [sent-155, score-0.174]
</p><p>76 , the null hypothesis is true), then this shuffling should not change the computed metric score. [sent-158, score-0.121]
</p><p>77 Crucially, this assumes that the samples being analyzed are representative of all extraneous variables that could affect the outcome of the experiment. [sent-159, score-0.247]
</p><p>78 Also, since metric scores (such as BLEU) are in general not comparable across test sets, we stratify, exchanging only  hypotheses that correspond to the same sentence. [sent-161, score-0.185]
</p><p>79 Table 2 shows the p-values computed by AR, testing the significance of the differences between the two systems in each pair. [sent-162, score-0.14]
</p><p>80 The last two lines summarize the results of the test when a small number of replications are performed, as ought to be reasonable in a research setting. [sent-166, score-0.282]
</p><p>81 In this simulation, we randomly selected n optimizer outputs from our large pool and ran the AR test to determine the significance; we repeated this procedure 250 times. [sent-167, score-0.702]
</p><p>82 These indicate that we are very likely to observe a significant difference for BTEC at n = 5, and a very significant difference by n = 50 (Table 2). [sent-169, score-0.1]
</p><p>83 Similarly, we see this trend in the WMT system: more replications leads to more significant results, which will be easier to reproduce. [sent-170, score-0.221]
</p><p>84 Based on the average performance of the systems reported in Table 1, we expect significance over a large enough number of independent trials. [sent-171, score-0.086]
</p><p>85 Nor are metric scores (even if they are statistically reliable) a substitute for thorough human analysis. [sent-173, score-0.084]
</p><p>86 However, we believe that the impact of optimizer instability has been ne180  1nSyshtiegmh ASysltoewm B0B. [sent-174, score-0.888]
</p><p>87 ” For “random,” we simulate an experiments with n optimization replications by drawing n optimized system outputs from our pool and performing AR; this simulation was repeated 250 times and the 95% CI of the AR p-values is reported. [sent-189, score-0.45]
</p><p>88 glected by standard experimental methodology in MT research, where single-sample measurements are too often used to assess system differences. [sent-190, score-0.125]
</p><p>89 In this paper, we have provided evidence that optimizer instability can have a substantial impact on results. [sent-191, score-0.888]
</p><p>90 However, we have also shown that it is possible to control for it with very few replications (Table 2). [sent-192, score-0.268]
</p><p>91 7Source code to carry out the AR test for multiple optimizer samples on the three metrics in this paper is available from http : / / github . [sent-200, score-0.822]
</p><p>92 METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. [sent-216, score-0.143]
</p><p>93 The best lexical metric for phrase-based statistical mt system optimization. [sent-260, score-0.175]
</p><p>94 Online largemargin training of syntactic and structural translation features. [sent-269, score-0.087]
</p><p>95 Extending the METEOR machine translation evaluation metric to the phrase level. [sent-296, score-0.171]
</p><p>96 cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models. [sent-307, score-0.087]
</p><p>97 Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices. [sent-355, score-0.233]
</p><p>98 Random restarts in minimum error rate training for statistical machine  translation. [sent-363, score-0.153]
</p><p>99 On some pitfalls in automatic evaluation and significance testing for MT. [sent-394, score-0.185]
</p><p>100 A study of translation edit rate with targeted human annotation. [sent-405, score-0.127]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('optimizer', 0.641), ('instability', 0.247), ('replications', 0.221), ('sdev', 0.194), ('mert', 0.154), ('extraneous', 0.134), ('bleu', 0.13), ('btec', 0.122), ('bootstrap', 0.112), ('ssel', 0.111), ('stest', 0.111), ('optimization', 0.098), ('translation', 0.087), ('significance', 0.086), ('metric', 0.084), ('samples', 0.08), ('manipulation', 0.08), ('replication', 0.077), ('cdec', 0.073), ('restart', 0.073), ('cer', 0.071), ('dyer', 0.067), ('effects', 0.067), ('resampling', 0.062), ('test', 0.061), ('restarts', 0.06), ('mt', 0.059), ('denkowski', 0.057), ('riezler', 0.057), ('experimental', 0.056), ('deviation', 0.055), ('testing', 0.054), ('outcomes', 0.053), ('minimum', 0.053), ('difference', 0.05), ('mis', 0.049), ('replicates', 0.049), ('ar', 0.048), ('noise', 0.048), ('statistic', 0.048), ('runs', 0.048), ('control', 0.047), ('region', 0.047), ('acknowledged', 0.045), ('arun', 0.045), ('pitfalls', 0.045), ('randomness', 0.045), ('spread', 0.044), ('segments', 0.044), ('bisani', 0.042), ('cahill', 0.042), ('centered', 0.042), ('danger', 0.042), ('randomization', 0.042), ('variable', 0.042), ('rate', 0.04), ('metrics', 0.04), ('optima', 0.04), ('exchanging', 0.04), ('uniformly', 0.04), ('optimized', 0.039), ('lavie', 0.039), ('haddow', 0.038), ('hypergraph', 0.038), ('tuning', 0.038), ('hypothesis', 0.037), ('measurements', 0.037), ('reporting', 0.036), ('reliable', 0.036), ('histogram', 0.036), ('bertoldi', 0.035), ('run', 0.035), ('lopez', 0.034), ('contrasts', 0.034), ('hierarchical', 0.034), ('extrinsic', 0.033), ('plots', 0.033), ('ith', 0.033), ('moses', 0.033), ('variables', 0.033), ('banerjee', 0.033), ('controlled', 0.033), ('affects', 0.032), ('confidence', 0.032), ('system', 0.032), ('controls', 0.032), ('controlling', 0.032), ('parameters', 0.031), ('meteor', 0.031), ('monte', 0.031), ('random', 0.031), ('drawing', 0.03), ('army', 0.03), ('simulation', 0.03), ('virtual', 0.03), ('decoder', 0.03), ('sample', 0.03), ('wmt', 0.03), ('carlo', 0.03), ('moore', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="60-tfidf-1" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>Author: Jonathan H. Clark ; Chris Dyer ; Alon Lavie ; Noah A. Smith</p><p>Abstract: In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.</p><p>2 0.10690181 <a title="60-tfidf-2" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>3 0.10010875 <a title="60-tfidf-3" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>Author: Rafael E. Banchs ; Haizhou Li</p><p>Abstract: This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions. 1</p><p>4 0.094558299 <a title="60-tfidf-4" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>Author: Chi-kiu Lo ; Dekai Wu</p><p>Abstract: We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost. As machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent. But more accurate, nonautomatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacyjudgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER. We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as HTER despite an even lower labor cost for the evaluation procedure. The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER. 1</p><p>5 0.090331376 <a title="60-tfidf-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.087303981 <a title="60-tfidf-6" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>7 0.085493758 <a title="60-tfidf-7" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>8 0.083882965 <a title="60-tfidf-8" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>9 0.081912898 <a title="60-tfidf-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.080564372 <a title="60-tfidf-10" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>11 0.079049967 <a title="60-tfidf-11" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>12 0.077151604 <a title="60-tfidf-12" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>13 0.076581985 <a title="60-tfidf-13" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>14 0.076176398 <a title="60-tfidf-14" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>15 0.070758097 <a title="60-tfidf-15" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>16 0.069945171 <a title="60-tfidf-16" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>17 0.069543369 <a title="60-tfidf-17" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>18 0.069215283 <a title="60-tfidf-18" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>19 0.068748452 <a title="60-tfidf-19" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>20 0.066838175 <a title="60-tfidf-20" href="./acl-2011-They_Can_Help%3A_Using_Crowdsourcing_to_Improve_the_Evaluation_of_Grammatical_Error_Detection_Systems.html">302 acl-2011-They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.093), (2, 0.053), (3, 0.076), (4, 0.005), (5, 0.021), (6, 0.021), (7, -0.01), (8, 0.029), (9, 0.021), (10, 0.012), (11, -0.055), (12, 0.004), (13, -0.072), (14, -0.052), (15, 0.049), (16, -0.047), (17, -0.039), (18, -0.017), (19, -0.012), (20, 0.036), (21, 0.004), (22, 0.022), (23, 0.022), (24, -0.029), (25, -0.008), (26, 0.001), (27, -0.027), (28, -0.003), (29, 0.056), (30, -0.051), (31, 0.047), (32, -0.031), (33, 0.066), (34, 0.023), (35, 0.047), (36, 0.032), (37, 0.021), (38, -0.008), (39, -0.018), (40, 0.101), (41, -0.022), (42, -0.024), (43, 0.023), (44, 0.039), (45, 0.041), (46, -0.038), (47, 0.021), (48, -0.007), (49, -0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93495125 <a title="60-lsi-1" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>Author: Jonathan H. Clark ; Chris Dyer ; Alon Lavie ; Noah A. Smith</p><p>Abstract: In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.</p><p>2 0.79954153 <a title="60-lsi-2" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>3 0.78933251 <a title="60-lsi-3" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>Author: Rafael E. Banchs ; Haizhou Li</p><p>Abstract: This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions. 1</p><p>4 0.76263672 <a title="60-lsi-4" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Alfons Juan ; Francisco Casacuberta</p><p>Abstract: We present minimum Bayes-risk system combination, a method that integrates consensus decoding and system combination into a unified multi-system minimum Bayes-risk (MBR) technique. Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems’ probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary. We introduce expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions. MBR system combination is a general method that is independent of specific SMT models, enabling us to combine systems with heterogeneous structure. Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods.</p><p>5 0.72067338 <a title="60-lsi-5" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>Author: Omar F. Zaidan ; Chris Callison-Burch</p><p>Abstract: Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-toEnglish evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional trans- lators. The total cost is more than an order of magnitude lower than professional translation.</p><p>6 0.71813518 <a title="60-lsi-6" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>7 0.71672744 <a title="60-lsi-7" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>8 0.71596932 <a title="60-lsi-8" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>9 0.70634985 <a title="60-lsi-9" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>10 0.67353547 <a title="60-lsi-10" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>11 0.67162943 <a title="60-lsi-11" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>12 0.66096485 <a title="60-lsi-12" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>13 0.64663172 <a title="60-lsi-13" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>14 0.63489598 <a title="60-lsi-14" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>15 0.62011492 <a title="60-lsi-15" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>16 0.60435069 <a title="60-lsi-16" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>17 0.59879875 <a title="60-lsi-17" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>18 0.58950013 <a title="60-lsi-18" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>19 0.5757075 <a title="60-lsi-19" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>20 0.5655883 <a title="60-lsi-20" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.383), (17, 0.042), (26, 0.013), (31, 0.011), (37, 0.063), (39, 0.047), (41, 0.042), (55, 0.028), (59, 0.04), (72, 0.037), (91, 0.034), (96, 0.186)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97308046 <a title="60-lda-1" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<p>Author: Cecilia Ovesdotter Alm</p><p>Abstract: This opinion paper discusses subjective natural language problems in terms of their motivations, applications, characterizations, and implications. It argues that such problems deserve increased attention because of their potential to challenge the status of theoretical understanding, problem-solving methods, and evaluation techniques in computational linguistics. The author supports a more holistic approach to such problems; a view that extends beyond opinion mining or sentiment analysis.</p><p>2 0.9547978 <a title="60-lda-2" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<p>Author: Ulrich Schafer ; Bernd Kiefer ; Christian Spurk ; Jorg Steffen ; Rui Wang</p><p>Abstract: We describe a novel application for structured search in scientific digital libraries. The ACL Anthology Searchbench is meant to become a publicly available research tool to query the content of the ACL Anthology. The application provides search in both its bibliographic metadata and semantically analyzed full textual content. By combining these two features, very efficient and focused queries are possible. At the same time, the application serves as a showcase for the recent progress in natural language processing (NLP) research and language technology. The system currently indexes the textual content of 7,500 anthology papers from 2002–2009 with predicateargument-like semantic structures. It also provides useful search filters based on bibliographic metadata. It will be extended to provide the full anthology content and en- . hanced functionality based on further NLP techniques. 1 Introduction and Motivation Scientists in all disciplines nowadays are faced with a flood of new publications every day. In addition, more and more publications from the past become digitally available and thus even increase the amount. Finding relevant information and avoiding duplication of work have become urgent issues to be addressed by the scientific community. The organization and preservation of scientific knowledge in scientific publications, vulgo text documents, thwarts these efforts. From a viewpoint of 7 dfki .de / lt a computer scientist, scientific papers are just ‘unstructured information’ . At least in our own scientific community, Computational Linguistics, it is generally assumed that NLP could help to support search in such document collections. The ACL Anthology1 is a comprehensive elec- tronic collection of scientific papers in our own field (Bird et al., 2008). It is updated regularly with new publications, but also older papers have been scanned and are made available electronically. We have implemented the ACL Anthology Searchbench2 for two reasons: Our first aim is to provide a more targeted search facility in this collection than standard web search on the anthology website. In this sense, the Searchbench is meant to become a service to our own community. Our second motivation is to use the developed system as a showcase for the progress that has been made over the last years in precision-oriented deep linguistic parsing in terms of both efficiency and coverage, specifically in the context of the DELPHIN community3. Our system also uses further NLP techniques such as unsupervised term extraction, named entity recognition and part-of-speech (PoS) tagging. By automatically precomputing normalized semantic representations (predicate-argument structure) of each sentence in the anthology, the search space is structured and allows to find equivalent or related predicates even if they are expressed differ- 1http : / /www . aclweb .org/ anthology 2http : //aclasb . dfki . de 3http : / /www . de lph-in . net – DELPH-IN stands for DEep Linguistic Processing with HPSG INitiative. Portland,P Orroecge ondi,n UgSsA o,f 2 th1e J AunCeL 2-H0L1 T. 2 ?0c 1210 1S1ys Atesmso Dcieamtio n s ftorart Cio nms,p puatgaetiso 7n–al1 L3i,nguistics ently, e.g. in passive constructions, using synonyms, etc. By storing the semantic sentence structure along with the original text in a structured full-text search engine, it can be guaranteed that recall cannot fall behind the baseline of a fulltext search. In addition, the Searchbench also provides detailed bibliographic metadata for filtering as well as autosuggest texts for input fields computed from the corpus two further key features one can expect from such systems today, nevertheless very important for efficient search in digital libraries. We describe the offline preprocessing and deep parsing approach in Section 2. Section 3 concentrates on the generation of the semantic search index. In Section 4, we describe the search interface. We conclude in Section 5 and present an outlook to future extensions. – 2 Parsing the ACL Anthology The basis of the search index for the ACL Anthology are its original PDF documents, currently 8,200 from the years 2002 through 2009. To overcome quality problems in text extraction from PDF, we use a commercial PDF extractor based on OCR techniques. This approach guarantees uniform and highquality textual representations even from older papers in the anthology (before 2000) which mostly were scanned from printed paper versions. The general idea of the semantics-oriented access to scholarly paper content is to parse each sentence they contain with the open-source HPSG (Pollard and Sag, 1994) grammar for English (ERG; Flickinger (2002)) and then distill and index semantically structured representations for search. To make the deep parser robust, it is embedded in a NLP workflow. The coverage (percentage of full deeply parsed sentences) on the anthology corpus could be increased from 65 % to now more than 85 % through careful combination of several robustness techniques; for example: (1) chart pruning, directed search during parsing to increase per- formance, and also coverage for longer sentences (Cramer and Zhang, 2010); (2) chart mapping, a novel method for integrating preprocessing information in exactly the way the deep grammar expects it (Adolphs et al., 2008); (3) new version of the ERG with better handling of open word classes; (4) 8 more fine-grained named entity recognition, including recognition of citation patterns; (5) new, better suited parse ranking model (WeScience; Flickinger et al. (2010)). Because of limited space, we will focus on (1) and (2) below. A more detailed description and further results are available in (Sch a¨fer and Kiefer, 2011). Except for a small part of the named entity recognition components (citations, some terminology) and the parse ranking model, there are no further adaptations to genre or domain of the text corpus. This implies that the NLP workflow could be easily and modularly adapted to other (scientific or nonscientific) domains—mainly thanks to the generic and comprehensive language modelling in the ERG. The NLP preprocessing component workflow is implemented using the Heart of Gold NLP middleware architecture (Sch a¨fer, 2006). It starts with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in component JTok, followed by the trigram-based PoS tagger TnT (Brants, 2000) trained on the Penn Treebank (Marcus et al., 1993) and the named entity recognizer SProUT (Dro z˙d z˙y n´ski et al., 2004). 2.1 Precise Preprocessing Integration with Chart Mapping Tagger output is combined with information from the named entity recognizer, e.g. delivering hypothetical information on citation expressions. The combined result is delivered as input to the deep parser PET (Callmeier, 2000) running the ERG. Here, citations, for example, can be treated as either persons, locations or appositions. Concerning punctuation, the ERG can make use of information on opening and closing quotation marks. Such information is often not explicit in the input text, e.g. when, as in our setup, gained through OCR which does not distinguish between ‘ and ’ or “ and However, a tokenizer can often guess (recon- ”. struct) leftness and rightness correctly. This information, passed to the deep parser via chart mapping, helps it to disambiguate. 2.2 Increased Processing Speed and Coverage through Chart Pruning In addition to a well-established discriminative maximum entropy model for post-analysis parse selection, we use an additional generative model as described in Cramer and Zhang (2010) to restrict the search space during parsing. This restriction increases efficiency, but also coverage, because the parse time was restricted to at most 60 CPU seconds on a standard PC, and more sentences could now be parsed within these bounds. A 4 GB limit for main memory consumption was far beyond what was ever needed. We saw a small but negligible decrease in parsing accuracy, 5.4 % best parses were not found due to the pruning of important chart edges. Ninomiya et al. (2006) did a very thorough comparison ofdifferent performance optimization strategies, and among those also a local pruning strategy similar to the one used here. There is an important difference between the systems, in that theirs works on a reduced context-free backbone first and reconstructs the results with the full grammar, while PET uses the HPSG grammar directly, with subsumption packing and partial unpacking to achieve a similar effect as the packed chart of a context-free parser. sentence length −→ Figure 1: Distribution of sentence length and mean parse times for mild pruning In total, we parsed 1,537,801 sentences, of which 57,832 (3.8 %) could not be parsed because of lexicon errors. Most of them were caused by OCR ar- tifacts resulting in unexpected punctuation character combinations. These can be identified and will be deleted in the future. Figure 1 displays the average parse time of processing with a mild chart pruning setting, together with the mean quadratic error. In addition, it contains the distribution of input sentences over sentence length. Obviously, the vast majority of sen9 tences has a length of at most 60 words4. The parse times only grow mildly due to the many optimization techniques in the original system, and also the new chart pruning method. The sentence length distribution has been integrated into Figure 1 to show that the predominant part of our real-world corpus can be processed using this information-rich method with very low parse times (overall average parse time < 2 s per sentence). The large amount of short inputs is at first surprising, even more so that most of these inputs can not be parsed. Most of these inputs are non-sentences such as headings, enumerations, footnotes, table cell content. There are several alternatives to deal with such input, one to identify and handle them in a preprocessing step, another to use a special root condition in the deep analysis component that is able to combine phrases with well-defined properties for inputs where no spanning result could be found. We employed the second method, which has the advantage that it handles a larger range of phenomena in a homogeneous way. Figure 2 shows the change in percentage of unparsed and timed out inputs for the mild pruning method with and without the root condition combining fragments. sentence length −→ Figure 2: Unparsed and timed out sentences with and without fragment combination Figure 2 shows that this changes the curve for unparsed sentences towards more expected characteristics and removes the uncommonly high percentage of short sentences for which no parse can be computed. Together with the parses for fragmented 4It has to be pointed out that extremely long sentences also may be non-sentences resulting from PDF extraction errors, missing punctuation etc. No manual correction took place. Figure 3: Multiple semantic tuples may be generated for a sentence input, we get a recall (sentences with at least one parse) over the whole corpus of 85.9 % (1,321,336 sentences), without a significant change for any of the other measures, and with potential for further improvement. 3 Semantic Tuple Extraction with DMRS In contrast to shallow parsers, the ERG not only handles detailed syntactic analyses of phrases, com- pounds, coordination, negation and other linguistic phenomena that are important for extracting semantic relations, but also generates a formal semantic representation of the meaning of the input sentence in the Minimal Recursion Semantics (MRS) representation format (Copestake et al., 2005). It consists of elementary predications for each word and larger constituents, connected via argument positions and variables, from which predicate-argument structure can be extracted. MRS representations resulting from deep parsing are still relatively close to linguistic structures and contain more detailed information than a user would like to query and search for. Therefore, an additional extraction and abstraction step is performed before storing semantic structures in the search index. Firstly, MRS is converted to DMRS (Copestake, 2009), a dependency-style version of MRS that eases extraction of predicate-argument structure using the implementation in LKB (Copestake, 2002). The representation format we devised for the search index we call semantic tuples, in fact quintuples</p><p>3 0.9254061 <a title="60-lda-3" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>Author: Elijah Mayfield ; Carolyn Penstein Rose</p><p>Abstract: We present a novel computational formulation of speaker authority in discourse. This notion, which focuses on how speakers position themselves relative to each other in discourse, is first developed into a reliable coding scheme (0.71 agreement between human annotators). We also provide a computational model for automatically annotating text using this coding scheme, using supervised learning enhanced by constraints implemented with Integer Linear Programming. We show that this constrained model’s analyses of speaker authority correlates very strongly with expert human judgments (r2 coefficient of 0.947).</p><p>4 0.91598856 <a title="60-lda-4" href="./acl-2011-Language_of_Vandalism%3A_Improving_Wikipedia_Vandalism_Detection_via_Stylometric_Analysis.html">195 acl-2011-Language of Vandalism: Improving Wikipedia Vandalism Detection via Stylometric Analysis</a></p>
<p>Author: Manoj Harpalani ; Michael Hart ; Sandesh Signh ; Rob Johnson ; Yejin Choi</p><p>Abstract: Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams. ,</p><p>same-paper 5 0.84064019 <a title="60-lda-5" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>Author: Jonathan H. Clark ; Chris Dyer ; Alon Lavie ; Noah A. Smith</p><p>Abstract: In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.</p><p>6 0.60633266 <a title="60-lda-6" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>7 0.60024989 <a title="60-lda-7" href="./acl-2011-A_Corpus_of_Scope-disambiguated_English_Text.html">8 acl-2011-A Corpus of Scope-disambiguated English Text</a></p>
<p>8 0.59966445 <a title="60-lda-8" href="./acl-2011-Towards_Tracking_Semantic_Change_by_Visual_Analytics.html">307 acl-2011-Towards Tracking Semantic Change by Visual Analytics</a></p>
<p>9 0.59861779 <a title="60-lda-9" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>10 0.59728289 <a title="60-lda-10" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>11 0.59444082 <a title="60-lda-11" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>12 0.58763015 <a title="60-lda-12" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>13 0.58577394 <a title="60-lda-13" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>14 0.5838306 <a title="60-lda-14" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>15 0.58382165 <a title="60-lda-15" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>16 0.58311361 <a title="60-lda-16" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>17 0.57903945 <a title="60-lda-17" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>18 0.57798719 <a title="60-lda-18" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>19 0.57798088 <a title="60-lda-19" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>20 0.57788503 <a title="60-lda-20" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
