<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-245" href="#">acl2011-245</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</h1>
<br/><p>Source: <a title="acl-2011-245-pdf" href="http://aclweb.org/anthology//P/P11/P11-1066.pdf">pdf</a></p><p>Author: Guangyou Zhou ; Li Cai ; Jun Zhao ; Kang Liu</p><p>Abstract: Community-based question answer (Q&A;) has become an important issue due to the popularity of Q&A; archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A; archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation ofphrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A; data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.</p><p>Reference: <a title="acl-2011-245-reference" href="../acl2011_reference/acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn cai a  Abstract Community-based question answer (Q&A;) has become an important issue due to the popularity of Q&A; archives on the web. [sent-4, score-0.678]
</p><p>2 This paper is concerned with the problem of question retrieval. [sent-5, score-0.296]
</p><p>3 Question retrieval in Q&A; archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. [sent-6, score-1.039]
</p><p>4 In this paper, we propose a novel phrase-based translation model for question retrieval. [sent-7, score-0.62]
</p><p>5 Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation ofphrases as a whole, rather than translating single words in isolation. [sent-8, score-1.23]
</p><p>6 Experiments conducted on real Q&A; data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model. [sent-9, score-0.688]
</p><p>7 1 Introduction Over the past few years, large scale question and answer (Q&A;) archives have become an important information resource on the Web. [sent-10, score-0.638]
</p><p>8 These include the traditional Frequently Asked Questions (FAQ) archives and the emerging community-based Q&A; services, such as Yahoo! [sent-11, score-0.26]
</p><p>9 com/ 653 Community-based Q&A; services can directly return answers to the queried questions instead of a list of relevant documents, thus provide an effective alternative to the traditional adhoc information retrieval. [sent-22, score-0.675]
</p><p>10 To make full use of the large scale archives of question-answer pairs, it is critical to have functionality helping users to retrieve historical answers (Duan et al. [sent-23, score-0.4]
</p><p>11 Therefore, it is a meaningful  task to retrieve the questions that are semantically equivalent or relevant to the queried questions. [sent-25, score-0.57]
</p><p>12 For example in Table 1, given question Q1, Q2 can be returned and their answers will then be used to answer Q1 because the answer of Q2 is expected to partially satisfy the queried question Q1. [sent-26, score-1.297]
</p><p>13 This is what we called question retrieval in this paper. [sent-27, score-0.497]
</p><p>14 uvckaelr most information retrieval models, such as vector space model (VSM) (Salton et al. [sent-30, score-0.24]
</p><p>15 , 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al. [sent-32, score-1.006]
</p><p>16 To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al. [sent-40, score-0.821]
</p><p>17 , 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al. [sent-41, score-0.285]
</p><p>18 Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e. [sent-45, score-0.373]
</p><p>19 However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities. [sent-49, score-0.384]
</p><p>20 , “stuffy”/“cold” and “nose”/“cold”) might have a high translation  probability, the sequence of words “stuffy nose” can be easily translated from a single word “cold” in Q2 with a relative high translation probability. [sent-52, score-0.648]
</p><p>21 In this paper, we argue that it is beneficial to capture contextual information for question retrieval. [sent-53, score-0.366]
</p><p>22 To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al. [sent-54, score-0.285]
</p><p>23 , 2003; Och and Ney, 2004), we propose a phrasebased translation model (P-Trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level. [sent-55, score-1.279]
</p><p>24 This model learns the probability of translating one sequence of words (e. [sent-56, score-0.219]
</p><p>25 , translating a phrase in a historical question into another phrase in a queried question. [sent-60, score-0.953]
</p><p>26 Compared to the traditional word-based translation models that account for translating single words in isolation, the phrase-based translation model is potentially more effective because it captures some contextual information in modeling the translation of phrases as a whole. [sent-61, score-1.21]
</p><p>27 More precise translation can be determined for phrases than for words. [sent-62, score-0.344]
</p><p>28 It is thus reasonable to expect that using such phrase translation probabilities as ranking features is likely to improve the ques-  tion retrieval performance, as we will show in our experiments. [sent-63, score-0.591]
</p><p>29 Unlike the general natural language translation, the parallel sentences between questions and an654 swers in community-based Q&A; have very different lengths, leaving many words in answers unaligned to any word in queried questions. [sent-64, score-0.651]
</p><p>30 Following (Berger and Lafferty, 1999), we restrict our attention to those phrase translations consistent with a good wordlevel alignment. [sent-65, score-0.09]
</p><p>31 Specifically, we make the following contributions: •  •  •  we formulate the question retrieval task as a phrase-based tr thanes qlautieosnti problem by modeling the contextual information (in Section 3. [sent-66, score-0.596]
</p><p>32 we linearly combine the phrase-based translatwioen l imneodaerlyl yfo cro mtheb question part ea-nbda answer part (in Section 3. [sent-68, score-0.497]
</p><p>33 we propose a linear ranking model framework  fwoer question rae ltirnieeavarl r ainn wkinhgich m doidffeelr fernatm mmewodoerlks are incorporated as features because the phrasebased translation model cannot be interpolated with a unigram language model (in Section 3. [sent-70, score-0.856]
</p><p>34 •  finally, we conduct the experiments on community-based Q&A; data for question retrieval. [sent-72, score-0.296]
</p><p>35 Section 3 describes our phrase-based translation model for question retrieval. [sent-76, score-0.62]
</p><p>36 1 Language Model The unigram language model has been widely used for question retrieval on community-based Q&A;  data (Jeon et al. [sent-80, score-0.536]
</p><p>37 , 2008) consistently reported that the wordbased translation models (Trans) yielded better performance than the traditional methods (VSM, Okapi and LM) for question retrieval. [sent-91, score-0.669]
</p><p>38 These models ex-  ploit the word translation probabilities in a language modeling framework. [sent-92, score-0.314]
</p><p>39 (2008), the ranking function can be written as: Score(q, D) =  ∏ (1−λ)Ptr(w|D)+λPml(w|C)  (3)  w∏∈q  Ptr(w|D) =t∑∈DP(w|t)Pml(t|D), Pml(t|D) =#|(Dt,|D) (4)  where P(w|t) denotes the translation probability fwrohemr ew Pord(w wt to w deorndo w. [sent-95, score-0.405]
</p><p>40 (2008) proposed to linearly mix two different estimations by combining language model and word-based translation model into a unified framework, called TransLM. [sent-98, score-0.394]
</p><p>41 The experiments show that this model gains better performance than both the language model and the word-based translation model. [sent-99, score-0.363]
</p><p>42 (2008), this model can be written as: Score(q, D) =  ∏ (1 − λ)Pmx(w|D)  + λPml(w|C)  Pmx(w|D)  w∏∈q (5) = α∑P(w|t)Pml(t|D)+(1−α)Pml(w|D) ∑t∈D (6) 655  D: E: F: M: q:  …  …  …  [for, [for1,  for good cold home remedies good,  best2, (1? [sent-101, score-0.422]
</p><p>43 2)  best home remedy for stuffy nose  document segmentation translation permutation queried question  Figure 1: Example describing the generative procedure of the phrase-based translation model. [sent-108, score-1.82]
</p><p>44 1 Phrase-Based Translation Model Phrase-based machine translation models (Koehn et al. [sent-110, score-0.285]
</p><p>45 Chiang, 2005; Och and Ney, 2004) have shown superior performance compared to word-based translation models. [sent-112, score-0.285]
</p><p>46 In this paper, the goal of phrase-based translation model is to  translate a document4 D into a queried question q. [sent-113, score-0.971]
</p><p>47 Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information. [sent-114, score-0.286]
</p><p>48 For example, we might learn that the phrase “stuffy nose” can be translated from “cold” with relative high probability, even though neither of the individual word pairs (e. [sent-115, score-0.094]
</p><p>49 , “stuffy”/“cold” and “nose”/“cold”) might have a high word translation probability. [sent-117, score-0.285]
</p><p>50 , 2010), we assume the following generative process: first the document D is broken into K non-empty word sequences t1, . [sent-120, score-0.128]
</p><p>51 , tK, then each t is translated into a new non-empty word sequence w1, . [sent-123, score-0.078]
</p><p>52 , wK, and finally these phrases are permutated and concatenated to form the queried questions q, where t and w denote the phrases or consecutive sequence of words. [sent-126, score-0.743]
</p><p>53 To formulate this generative process, let E denote the segmentation of D into K phrases t1, . [sent-127, score-0.142]
</p><p>54 , tK, and let F denote the K translation phrases w1, . [sent-130, score-0.391]
</p><p>55 Finally, l rete fMer d toeno thtee a permutation of K elements representing the final reordering  step. [sent-134, score-0.052]
</p><p>56 Next let us place a probability distribution over rewrite pairs. [sent-136, score-0.042]
</p><p>57 Let B(D, q) denote the set of E, 4In this paper, a document has the same meaning as a historical question-answer pair in the Q&A; archives. [sent-137, score-0.208]
</p><p>58 Equation (8) cannot be used directly for document ranking because q and D are often of very different lengths, leav-  ing many words in D unaligned to any word in q. [sent-140, score-0.149]
</p><p>59 This is the key difference between the communitybased question retrieval and the general natural language translation. [sent-141, score-0.527]
</p><p>60 (2010), document-query translation requires a distillation of the document, while translation of natural language tolerates little being thrown away. [sent-143, score-0.62]
</p><p>61 Thus we attempt to extract the key document words that form the distillation of the document, and assume that a queried question is translated only from the key document words. [sent-144, score-0.952]
</p><p>62 In this paper, the key document words are identified via word alignment. [sent-145, score-0.094]
</p><p>63 aJ, which describe the mapping from a word position j in queried question to a document word position i= aj. [sent-152, score-0.711]
</p><p>64 We assume that the position of the key dPo(cqu,mAe|nDt )w. [sent-154, score-0.058]
</p><p>65 Once the word alignment is fixed, the final permutation is uniquely determined, so we can safely discard that factor. [sent-157, score-0.086]
</p><p>66 Thus equation (8) can be written as:  Aˆ,  P(q|D)  ≈  max  P(F|D, E)  (10)  (E,F,M) ∈B(D,q,Aˆ)  For the sole remaining factor P(F|D, E), we maFkoer rth teh assumption tihnaitn a segmented queried question F = w1, . [sent-158, score-0.675]
</p><p>67 , wK is generated from left to right by translating each phrase t1, . [sent-161, score-0.154]
</p><p>68 , tK indepen-  dently:  ∏K  P(F|D,E) =k∏=1P(wk|tk)  (11)  where P(wk |tk) is a phrase translation probability, the estimation|t will be described in Section 3. [sent-164, score-0.34]
</p><p>69 To find the maximum probability assignment efficiently, we use a dynamic programming approach, somewhat similar to the monotone decoding algorithm described in (Och, 2002). [sent-166, score-0.042]
</p><p>70 We define αj to be the probability of the most likely sequence of phrases covering the first j words in a queried question, then the probability can be calculated using the following recursion: (1) Initialization: α0 = 1 (12)  (2) Iαndju=cjti′ < 0. [sent-167, score-0.533]
</p><p>71 3  Impact of Phrase Length  Our proposed phrase-based translation model, due to its capability of capturing contextual information, is more effective than the state-of-the-art word-based translation models. [sent-170, score-0.64]
</p><p>72 It is important to investigate the  impact of the phrase length on the final retrieval performance. [sent-171, score-0.29]
</p><p>73 Table 5 shows the results, it is seen that using the longer phrases up to the maximum length of five can consistently improve the retrieval performance. [sent-172, score-0.289]
</p><p>74 However, using much longer phrases in the phrase-based translation model does not seem to produce significantly better performance (row 8 and row 9 vs. [sent-173, score-0.474]
</p><p>75 3 897561032  Table 5: The impact of the phrase length on retrieval performance. [sent-176, score-0.29]
</p><p>76 answers are very noisy, it is possible for translation models to contain “unnecessary” translations. [sent-181, score-0.357]
</p><p>77 In this paper, we attempt to identify and decrease the proportion of unnecessary translations in a translation model by using TextRank algorithm. [sent-182, score-0.425]
</p><p>78 This kind of “unnecessary” translation between words will eventually affect the bi-phrase translation. [sent-183, score-0.285]
</p><p>79 Table 6 shows the effectiveness of parallel corpus preprocessing. [sent-184, score-0.062]
</p><p>80 Row 11 reports the average number of translations per word and the question retrieval performance when only stopwords 7 are removed. [sent-185, score-0.532]
</p><p>81 When using the TextRank algorithm for parallel corpus preprocessing, the average number of translations per word is reduced from 69 to 24, but the performance of question retrieval is significantly improved (row 11 vs. [sent-186, score-0.566]
</p><p>82 5 Impact of Pooling Strategy The correspondence of words or phrases in the  question-answer pair is not as strong as in the bilingual sentence pair, thus noise will be inevitably introduced for both P(¯ a|¯ q) and P(¯ q|¯ a). [sent-191, score-0.059]
</p><p>83 To see how much the pooling strategy benefit the question retrieval, we introduce two baseline methods for comparison. [sent-192, score-0.431]
</p><p>84 The first method (denoted as P(¯ a|¯ q)) is used to denote the translation probability aw|¯ qit)h) )th ise question as tohtee source nanslda tiohen answer as 7http://truereader. [sent-193, score-0.811]
</p><p>85 3 A89 71P  Table 7: The impact of pooling strategy for question retrieval. [sent-196, score-0.465]
</p><p>86 th Teh htera snescloatniodn ( probability Pwi(¯ ath| qth)e) answer as the source and the question as the target. [sent-199, score-0.479]
</p><p>87 From this Table, we see that the pooling strategy significantly outperforms the two baseline methods for question retrieval (row  13 and row 14 vs. [sent-201, score-0.723]
</p><p>88 5  Conclusions and Future Work  In this paper, we propose a novel phrase-based translation model for question retrieval. [sent-203, score-0.62]
</p><p>89 Compared to the traditional word-based translation models, the proposed approach is more effective in that it can capture contextual information instead of translating single words in isolation. [sent-204, score-0.513]
</p><p>90 Experiments conducted on real Q&A; data demonstrate that the phrasebased translation model significantly outperforms the state-of-the-art word-based translation models. [sent-205, score-0.688]
</p><p>91 First, question structure should be considered, so it is necessary to combine the proposed approach with other question retrieval methods (e. [sent-207, score-0.793]
</p><p>92 Second, we will try to investigate the use of the proposed approach for other kinds of data set, such as categorized questions from forum sites and FAQ sites. [sent-212, score-0.159]
</p><p>93 Combining lexical semantic resources with question & answer archives for translation-based answer finding. [sent-237, score-0.779]
</p><p>94 Learning the relative usefulness of questions in community QA. [sent-257, score-0.194]
</p><p>95 A generalized framework of exploring category information for question retrieval in community question answer archives. [sent-266, score-0.969]
</p><p>96 Searching questions by identifying questions topics and question focus. [sent-280, score-0.614]
</p><p>97 Clickthrough-based translation models for web search: from word models to phrase models. [sent-287, score-0.34]
</p><p>98 Finding similar questions in large question and answer archives. [sent-295, score-0.596]
</p><p>99 Bridging lexical gaps between queries and questions on large online Q&A; collections with compact translation models. [sent-320, score-0.444]
</p><p>100 A syntactic tree matching approach to finding similar questions in community-based qa services. [sent-385, score-0.159]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('queried', 0.351), ('question', 0.296), ('translation', 0.285), ('pml', 0.267), ('retrieval', 0.201), ('archives', 0.201), ('cold', 0.201), ('stuffy', 0.198), ('jeon', 0.187), ('nose', 0.165), ('questions', 0.159), ('xue', 0.152), ('answer', 0.141), ('okapi', 0.132), ('berger', 0.113), ('pooling', 0.1), ('wk', 0.1), ('translating', 0.099), ('historical', 0.097), ('tk', 0.095), ('row', 0.091), ('home', 0.088), ('phrasebased', 0.079), ('textrank', 0.075), ('answers', 0.072), ('contextual', 0.07), ('pmx', 0.066), ('remedies', 0.066), ('taj', 0.066), ('unnecessary', 0.066), ('document', 0.064), ('sigir', 0.062), ('lafferty', 0.062), ('cao', 0.062), ('duan', 0.062), ('traditional', 0.059), ('phrases', 0.059), ('chasm', 0.058), ('faq', 0.058), ('ptr', 0.058), ('robertson', 0.058), ('phrase', 0.055), ('vsm', 0.053), ('gao', 0.053), ('permutation', 0.052), ('ponte', 0.05), ('distillation', 0.05), ('ranking', 0.05), ('denote', 0.047), ('och', 0.046), ('salton', 0.046), ('triples', 0.042), ('probability', 0.042), ('bridging', 0.04), ('cai', 0.04), ('model', 0.039), ('translated', 0.039), ('croft', 0.039), ('sequence', 0.039), ('yahoo', 0.038), ('bunescu', 0.037), ('generative', 0.036), ('lm', 0.036), ('aj', 0.035), ('bernhard', 0.035), ('strategy', 0.035), ('community', 0.035), ('unaligned', 0.035), ('translations', 0.035), ('parallel', 0.034), ('alignment', 0.034), ('services', 0.034), ('impact', 0.034), ('wj', 0.033), ('linearly', 0.031), ('isolation', 0.03), ('zhai', 0.03), ('semantically', 0.03), ('key', 0.03), ('retrieve', 0.03), ('consistently', 0.029), ('modeling', 0.029), ('qth', 0.029), ('maoxi', 0.029), ('baidu', 0.029), ('translatwioen', 0.029), ('guangyou', 0.029), ('jzhao', 0.029), ('zhongguancun', 0.029), ('jelinekmercer', 0.029), ('ainn', 0.029), ('ndt', 0.029), ('permutated', 0.029), ('pwi', 0.029), ('rwth', 0.029), ('smoothing', 0.029), ('assume', 0.028), ('el', 0.028), ('written', 0.028), ('effectiveness', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="245-tfidf-1" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>Author: Guangyou Zhou ; Li Cai ; Jun Zhao ; Kang Liu</p><p>Abstract: Community-based question answer (Q&A;) has become an important issue due to the popularity of Q&A; archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A; archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation ofphrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A; data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.</p><p>2 0.31672758 <a title="245-tfidf-2" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>Author: Shuguang Li ; Suresh Manandhar</p><p>Abstract: In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users’ information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. We show that translation model can be effectively utilized to predict the information need given only the user’s query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.</p><p>3 0.15346344 <a title="245-tfidf-3" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>4 0.12683775 <a title="245-tfidf-4" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>5 0.11778142 <a title="245-tfidf-5" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>6 0.11624727 <a title="245-tfidf-6" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>7 0.11348195 <a title="245-tfidf-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.10673209 <a title="245-tfidf-8" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>9 0.10211267 <a title="245-tfidf-9" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>10 0.10069816 <a title="245-tfidf-10" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>11 0.097486354 <a title="245-tfidf-11" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>12 0.096399635 <a title="245-tfidf-12" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>13 0.094550647 <a title="245-tfidf-13" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>14 0.093714729 <a title="245-tfidf-14" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>15 0.093109637 <a title="245-tfidf-15" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>16 0.092774674 <a title="245-tfidf-16" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>17 0.091425367 <a title="245-tfidf-17" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>18 0.090813786 <a title="245-tfidf-18" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>19 0.089744404 <a title="245-tfidf-19" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>20 0.08944343 <a title="245-tfidf-20" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.219), (1, -0.101), (2, 0.077), (3, 0.164), (4, 0.0), (5, -0.028), (6, -0.042), (7, -0.027), (8, 0.073), (9, 0.026), (10, 0.055), (11, -0.037), (12, 0.009), (13, -0.096), (14, 0.004), (15, 0.005), (16, -0.013), (17, -0.051), (18, -0.0), (19, 0.003), (20, 0.089), (21, 0.045), (22, -0.015), (23, 0.022), (24, -0.044), (25, -0.084), (26, -0.041), (27, -0.017), (28, 0.005), (29, 0.061), (30, -0.037), (31, -0.059), (32, -0.01), (33, -0.044), (34, 0.031), (35, -0.058), (36, -0.032), (37, -0.126), (38, 0.218), (39, 0.057), (40, -0.245), (41, -0.051), (42, -0.024), (43, 0.065), (44, 0.078), (45, -0.228), (46, -0.037), (47, -0.1), (48, -0.119), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95125252 <a title="245-lsi-1" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>Author: Guangyou Zhou ; Li Cai ; Jun Zhao ; Kang Liu</p><p>Abstract: Community-based question answer (Q&A;) has become an important issue due to the popularity of Q&A; archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in Q&A; archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. In this paper, we propose a novel phrase-based translation model for question retrieval. Compared to the traditional word-based translation models, the phrasebased translation model is more effective because it captures contextual information in modeling the translation ofphrases as a whole, rather than translating single words in isolation. Experiments conducted on real Q&A; data demonstrate that our proposed phrasebased translation model significantly outperforms the state-of-the-art word-based translation model.</p><p>2 0.88537574 <a title="245-lsi-2" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>Author: Shuguang Li ; Suresh Manandhar</p><p>Abstract: In this paper we address the problem of question recommendation from large archives of community question answering data by exploiting the users’ information needs. Our experimental results indicate that questions based on the same or similar information need can provide excellent question recommendation. We show that translation model can be effectively utilized to predict the information need given only the user’s query question. Experiments show that the proposed information need prediction approach can improve the performance of question recommendation.</p><p>3 0.80947292 <a title="245-lsi-3" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>4 0.56516105 <a title="245-lsi-4" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve transla- , tion quality and collectively achieve up to 1 BLEU point over a competitive baseline.</p><p>5 0.52015293 <a title="245-lsi-5" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>6 0.51925361 <a title="245-lsi-6" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>7 0.50193948 <a title="245-lsi-7" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<p>8 0.49264202 <a title="245-lsi-8" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>9 0.48226273 <a title="245-lsi-9" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>10 0.46671787 <a title="245-lsi-10" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>11 0.46332288 <a title="245-lsi-11" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>12 0.46206918 <a title="245-lsi-12" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>13 0.45889664 <a title="245-lsi-13" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>14 0.45792615 <a title="245-lsi-14" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>15 0.45706972 <a title="245-lsi-15" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>16 0.45539841 <a title="245-lsi-16" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>17 0.45527819 <a title="245-lsi-17" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>18 0.45475617 <a title="245-lsi-18" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>19 0.45097321 <a title="245-lsi-19" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>20 0.45024586 <a title="245-lsi-20" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.022), (17, 0.053), (26, 0.031), (37, 0.068), (39, 0.026), (41, 0.033), (55, 0.411), (59, 0.026), (72, 0.018), (91, 0.025), (96, 0.223)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9305011 <a title="245-lda-1" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>Author: Joel Lang</p><p>Abstract: We present a novel probabilistic classifier, which scales well to problems that involve a large number ofclasses and require training on large datasets. A prominent example of such a problem is language modeling. Our classifier is based on the assumption that each feature is associated with a predictive strength, which quantifies how well the feature can predict the class by itself. The predictions of individual features can then be combined according to their predictive strength, resulting in a model, whose parameters can be reliably and efficiently estimated. We show that a generative language model based on our classifier consistently matches modified Kneser-Ney smoothing and can outperform it if sufficiently rich features are incorporated.</p><p>2 0.90934825 <a title="245-lda-2" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<p>Author: Reyyan Yeniterzi</p><p>Abstract: Turkish is an agglutinative language with complex morphological structures, therefore using only word forms is not enough for many computational tasks. In this paper we analyze the effect of morphology in a Named Entity Recognition system for Turkish. We start with the standard word-level representation and incrementally explore the effect of capturing syntactic and contextual properties of tokens. Furthermore, we also explore a new representation in which roots and morphological features are represented as separate tokens instead of representing only words as tokens. Using syntactic and contextual properties with the new representation provide an 7.6% relative improvement over the baseline.</p><p>3 0.90907496 <a title="245-lda-3" href="./acl-2011-Semi-Supervised_Modeling_for_Prenominal_Modifier_Ordering.html">275 acl-2011-Semi-Supervised Modeling for Prenominal Modifier Ordering</a></p>
<p>Author: Margaret Mitchell ; Aaron Dunlop ; Brian Roark</p><p>Abstract: In this paper, we argue that ordering prenominal modifiers typically pursued as a supervised modeling task is particularly wellsuited to semi-supervised approaches. By relying on automatic parses to extract noun phrases, we can scale up the training data by orders of magnitude. This minimizes the predominant issue of data sparsity that has informed most previous approaches. We compare several recent approaches, and find improvements from additional training data across the board; however, none outperform a simple n-gram model. – –</p><p>4 0.9089179 <a title="245-lda-4" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>Author: Viet Ha Thuc ; Nicola Cancedda</p><p>Abstract: Language models based on word surface forms only are unable to benefit from available linguistic knowledge, and tend to suffer from poor estimates for rare features. We propose an approach to overcome these two limitations. We use factored features that can flexibly capture linguistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative lan- guage modeling.</p><p>5 0.85645485 <a title="245-lda-5" href="./acl-2011-Global_Learning_of_Typed_Entailment_Rules.html">144 acl-2011-Global Learning of Typed Entailment Rules</a></p>
<p>Author: Jonathan Berant ; Ido Dagan ; Jacob Goldberger</p><p>Abstract: Extensive knowledge bases ofentailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs.</p><p>same-paper 6 0.82316977 <a title="245-lda-6" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>7 0.8181088 <a title="245-lda-7" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>8 0.6739037 <a title="245-lda-8" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>9 0.67388469 <a title="245-lda-9" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>10 0.65356493 <a title="245-lda-10" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>11 0.64839435 <a title="245-lda-11" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>12 0.64633638 <a title="245-lda-12" href="./acl-2011-Hierarchical_Text_Classification_with_Latent_Concepts.html">150 acl-2011-Hierarchical Text Classification with Latent Concepts</a></p>
<p>13 0.64408267 <a title="245-lda-13" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>14 0.64136297 <a title="245-lda-14" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>15 0.63498974 <a title="245-lda-15" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>16 0.63481152 <a title="245-lda-16" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>17 0.63419604 <a title="245-lda-17" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>18 0.62845242 <a title="245-lda-18" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>19 0.62788391 <a title="245-lda-19" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>20 0.62699097 <a title="245-lda-20" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
