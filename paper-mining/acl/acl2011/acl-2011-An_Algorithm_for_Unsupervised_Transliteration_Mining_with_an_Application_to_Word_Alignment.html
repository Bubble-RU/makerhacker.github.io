<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-34" href="#">acl2011-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</h1>
<br/><p>Source: <a title="acl-2011-34-pdf" href="http://aclweb.org/anthology//P/P11/P11-1044.pdf">pdf</a></p><p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.</p><p>Reference: <a title="acl-2011-34-reference" href="../acl2011_reference/acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  ,  Abstract We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. [sent-3, score-1.068]
</p><p>2 We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. [sent-5, score-0.971]
</p><p>3 Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments. [sent-7, score-1.192]
</p><p>4 1 Introduction Most previous methods for building transliteration systems were supervised, requiring either handcrafted rules or a clean list of transliteration pairs, both of which are expensive to create. [sent-8, score-1.732]
</p><p>5 In this paper, we show that it is possible to extract transliteration pairs from a parallel corpus using an unsupervised method. [sent-10, score-1.109]
</p><p>6 We then filter out a few word pairs (those which have the lowest transliteration probabilities according to the trained transliteration system) which are likely to be non-transliterations. [sent-13, score-1.869]
</p><p>7 This process is iterated, filtering out more and more non-transliteration pairs until a nearly clean list of transliteration word pairs is left. [sent-15, score-1.185]
</p><p>8 We compare our unsupervised transliteration mining method with the semi-supervised systems presented at the NEWS 2010 shared task on transliteration mining (Kumaran et al. [sent-17, score-1.942]
</p><p>9 Transliteration mining on the WIL data sets is easier due to a higher percentage of transliterations than in parallel corpora. [sent-24, score-0.4]
</p><p>10 To this end, we created gold standards in which sampled word pairs are annotated as either transliterations or non-transliterations. [sent-26, score-0.442]
</p><p>11 Ac s2s0o1ci1a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 430–439, Finally we integrate a transliteration module into the GIZA++ word aligner and show that it improves word alignment quality. [sent-30, score-1.073]
</p><p>12 The transliteration module is trained on the transliteration pairs which our mining method extracts from the parallel corpora. [sent-31, score-2.06]
</p><p>13 We evaluate our word alignment system on two language pairs using gold standard word alignments and achieve improvements of 10% and 13. [sent-32, score-0.426]
</p><p>14 In section 2, we describe the filtering model and the transliteration model. [sent-37, score-0.873]
</p><p>15 In section 3, we present our iterative transliteration mining algorithm and an algorithm which computes a stopping criterion for the mining algorithm. [sent-38, score-1.344]
</p><p>16 The first model is a joint character sequence model which we apply to transliteration mining. [sent-42, score-0.897]
</p><p>17 The other model is a standard phrasebased MT model which we apply to transliteration (as opposed to transliteration mining). [sent-44, score-1.709]
</p><p>18 For the mining process, we trained g2p on lists containing both transliteration pairs and non-  transliteration pairs. [sent-60, score-1.914]
</p><p>19 2 Statistical Machine Transliteration System We build a phrase-based MT system for transliteration using the Moses toolkit (Koehn et al. [sent-62, score-0.892]
</p><p>20 We also tried using g2p for implementing the transliteration decoder but found Moses to perform better. [sent-64, score-0.844]
</p><p>21 Moses has the advantage of using Minimum Error Rate Training (MERT) which optimizes transliteration accuracy rather than the likelihood of the training data as g2p does. [sent-65, score-0.877]
</p><p>22 The training data contains more non-transliteration pairs than transliteration pairs. [sent-66, score-0.997]
</p><p>23 Instead we want to optimize the transliteration performance for test data. [sent-68, score-0.844]
</p><p>24 For training Moses as a transliteration system, we treat each word pair as if it were a parallel sentence, by putting spaces between the characters of each word. [sent-71, score-1.009]
</p><p>25 3 Extraction of Transliteration Pairs Training of a supervised transliteration system requires a list of transliteration pairs which is expensive to create. [sent-75, score-1.878]
</p><p>26 In this section, we present an iterative method for the extraction of transliteration pairs from parallel corpora which is fully unsupervised and language pair independent. [sent-77, score-1.144]
</p><p>27 1 We determine the best iteration according to our stopping criterion and return the filtered data set from this iteration. [sent-82, score-0.387]
</p><p>28 The following sections describe the transliteration mining method in detail. [sent-84, score-0.95]
</p><p>29 432  Algorithm 1 Mining of transliteration pairs 1:training data ←list of word pairs 2: tIr ←ain i0n 3: repeat 4: Build a joint source channel model on the training data using g2p and compute the joint probability of every word pair. [sent-97, score-1.283]
</p><p>30 The training data contains mostly nontransliteration pairs and a few transliteration pairs. [sent-100, score-0.997]
</p><p>31 Our results show that at the iteration determined by our stopping criterion, the filtered set mostly contains transliterations and only a small number of transliterations have been mistakenly eliminated (see section 4. [sent-105, score-0.736]
</p><p>32 Algorithm 2 automatically determines the best stopping point of the iterative transliteration mining process. [sent-107, score-1.135]
</p><p>33 For every iteration, it builds a transliteration system on the filtered data. [sent-110, score-0.967]
</p><p>34 The transliteration system is tested on the source side of the other half of the list of word pairs (held-out). [sent-111, score-1.115]
</p><p>35 The output of the transliteration system is matched against the target side of the held-out data. [sent-112, score-0.93]
</p><p>36 The iteration where the output of the transliteration system best matches the held-out data is chosen as the stopping iteration of Algorithm 1. [sent-115, score-1.273]
</p><p>37 Algorithm 2 Selection of the stopping iteration for the transliteration mining algorithm 1:Create clusters of word pairs from the list of word pairs which have a common prefix of length 2 both on the source and target language side. [sent-116, score-1.66]
</p><p>38 11: Choose the iteration with the best median9 score for the transliteration mining process. [sent-123, score-1.074]
</p><p>39 If two variants are distributed over training and held-out data, then the one in the training data may cause the transliteration system to produce a correct translation (but not transliteration) of its variant in the heldout data. [sent-128, score-1.002]
</p><p>40 , steps 4 to 9, we build a transliteration system on  the filtered training data and test it on the source side of the held-out. [sent-137, score-1.055]
</p><p>41 4  Experiments  We evaluate our transliteration mining algorithm on three tasks: transliteration mining from Wikipedia  InterLanguage Links, transliteration mining from parallel corpora, and word alignment using a word aligner with a transliteration component. [sent-148, score-4.021]
</p><p>42 In the evaluation on parallel corpora, we compare our mining results with a manually built gold standard in which each word pair is either marked as a transliteration or as a non-transliteration. [sent-151, score-1.193]
</p><p>43 In the word alignment experiment, we integrate a transliteration module which is trained on the transliterations pairs extracted by our method into a word aligner and show a significant improvement. [sent-152, score-1.383]
</p><p>44 1 Experiments Using Parallel Phrases of Wikipedia InterLanguage Links We conduct transliteration mining experiments on the English/Arabic, English/Hindi, English/Tamil and English/Russian Wikipedia InterLanguage Links (WIL) used in the NEWS10. [sent-155, score-0.95]
</p><p>45 We calculate the F-measure of our filtered transliteration pairs against the supplied gold standard using the supplied evaluation tool. [sent-166, score-1.182]
</p><p>46 For English/Arabic, English/Hindi and English/Tamil, our system is better than most of the semi-supervised systems presented at the NEWS  2010 shared task for transliteration mining. [sent-167, score-0.891]
</p><p>47 In order to examine how well our method performs on parallel corpora, we  apply it to parallel corpora of English/Hindi and English/Arabic, and compare the transliteration mining results with a gold standard. [sent-179, score-1.253]
</p><p>48 434  Table 2: Cognates from English/Russian corpus extracted by our system as transliteration pairs. [sent-180, score-0.87]
</p><p>49 None of them are correct transliteration pairs according to the gold standard. [sent-181, score-1.034]
</p><p>50 We create gold standards for both language pairs by ran-  domly selecting a few thousand word pairs from the lists of word pairs extracted from the two corpora. [sent-186, score-0.52]
</p><p>51 The English/Hindi gold standard contains 180 transliteration pairs and 2084 non-transliteration pairs and the English/Arabic gold standard contains 288 transliteration pairs and 6639 non-transliteration pairs. [sent-188, score-2.23]
</p><p>52 Due to the noise in the held-out data, the transliteration accuracy on the held-out data often jumps from iteration to iteration. [sent-196, score-0.968]
</p><p>53 However,  to check the correctness of the stopping point, we tested the transliteration system on the seed data (available with NEWS 10) for every iteration of Algorithm 2. [sent-198, score-1.218]
</p><p>54 The Moses system used for transliteration will learn to “transliterate” (or actually translate) “change” to “badlao”. [sent-214, score-0.87]
</p><p>55 Such matching predictions of the transliterator which are actually translations lead to an overestimate of the transliteration accuracy and may cause Algorithm 2 to predict a stopping iteration which is too early. [sent-217, score-1.176]
</p><p>56 The dotted line shows unsmoothed heldout scores and solid line shows median9 held-out scores of the transliteration system go up. [sent-228, score-0.924]
</p><p>57 When no more non-transliteration pairs are left, we start filtering out transliteration pairs and the results of the system go down. [sent-229, score-1.139]
</p><p>58 3 Results on Parallel Corpora According to the gold standard, the English/Hindi and English/Arabic data sets contain 8% and 4%  transliteration pairs respectively. [sent-233, score-1.034]
</p><p>59 The English/Hindi gold standard contains 180 transliteration pairs and 2084 non-transliteration pairs. [sent-238, score-1.055]
</p><p>60 The English/Arabic gold standard contains 288 transliteration pairs and 6639 non-transliteration pairs. [sent-239, score-1.055]
</p><p>61 From the English/Hindi data, the mining system has mined 170 transliteration pairs out of 180 transliteration pairs. [sent-240, score-1.94]
</p><p>62 The English/Arabic mined data contains 197 transliteration pairs out of 288 transliteration  pairs. [sent-241, score-1.808]
</p><p>63 The mining system has wrongly identified a few non-transliteration pairs as transliterations (see table 3, last column). [sent-242, score-0.442]
</p><p>64 Most of these word pairs are close transliterations and differ by only one or two characters from perfect transliteration pairs. [sent-243, score-1.182]
</p><p>65 The close transliteration pairs provide many valid multigrams which may be helpful for the mining system. [sent-244, score-1.149]
</p><p>66 3 Integration into Word Alignment Model In the previous section, we presented a method for the extraction of transliteration pairs from a parallel corpus. [sent-246, score-1.068]
</p><p>67 In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the ttable probabilities of the IBM models and the HMM model. [sent-247, score-1.905]
</p><p>68 In this section, we propose a method to modify the translation probabilities of the t-table by interpolating the translation counts with transliteration counts. [sent-260, score-0.917]
</p><p>69 The transliteration module which is used to calculate the conditional transliteration probability is described in Algorithm 3. [sent-263, score-1.73]
</p><p>70 We build a transliteration system by training Moses on the filtered transliteration corpus (using Algorithm 1) and apply it to the e side of the list of word pairs. [sent-264, score-1.947]
</p><p>71 Then, we extract every f that cooccurs with e in a parallel sentence and add it to nbestTI(e) which gives us the list of candidate transliteration pairs candidateTIP(e). [sent-266, score-1.152]
</p><p>72 We combine the transliteration probabilities with the translation probabilities of the IBM models and the HMM model. [sent-270, score-0.93]
</p><p>73 We smooth the alignment frequencies by adding the transliteration probabilities weighted by the factor λ and get the following modified translation probabilities  ˆ p(f|e) =fta(ff,tea)( +e) λ +pt λi(f|e)  (2)  where fta(f, e) = pta(f|e)f(e) . [sent-272, score-1.049]
</p><p>74 λ is the transliteration weight which is optimized for every language pair (see section 4. [sent-275, score-0.864]
</p><p>75 We interpolate translation and transliteration probabilities at different iterations (and different combinations of iterations) of the three models and always observe an improvement in alignment quality. [sent-294, score-1.09]
</p><p>76 In this case, we ran the 5th iteration of Model1, then the first iteration of the HMM and only then stopped for interpola437  rithm 4 shows the interpolation of the transliteration probabilities with IBM Model4. [sent-297, score-1.125]
</p><p>77 5  Previous Research  Previous work on transliteration mining uses a manually labelled set of training data to extract transliteration pairs from a parallel corpus or comparable corpora. [sent-318, score-2.114]
</p><p>78 The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary (Yoon et al. [sent-319, score-1.841]
</p><p>79 , 2006; Lee and Chang, 2003) or just a few carefully selected transliteration pairs (Sherif and Kondrak, 2007; Klementiev and Roth, 2006). [sent-321, score-0.964]
</p><p>80 Our work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. [sent-322, score-2.763]
</p><p>81 Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate injust those iterations of training where we were transitioning from one model to the next. [sent-323, score-1.865]
</p><p>82 Sherif and Kondrak (2007) train a probabilistic transducer on 14 manually constructed transliteration pairs of English/Arabic. [sent-330, score-0.984]
</p><p>83 They iteratively extract transliteration pairs from the test data and add them to the training data. [sent-331, score-1.017]
</p><p>84 Our method is different from the method of Sherif and Kondrak (2007) as our method is fully unsupervised, and because in each iteration, they add the most probable transliteration pairs to the training data, while we filter out the least probable transliteration pairs from the training data. [sent-332, score-1.994]
</p><p>85 The transliteration mining systems of the four NEWS10 participants are either based on discriminative or on generative methods. [sent-333, score-0.95]
</p><p>86 6 For the English/Arabic task, the transliteration mining system of Noeman and Madkour (2010) was best. [sent-339, score-0.976]
</p><p>87 7 Our transliteration extraction method differs in that we extract transliteration pairs from a parallel corpus without supervision. [sent-341, score-1.932]
</p><p>88 We are only aware of one previous work which uses transliteration information for word alignment. [sent-345, score-0.872]
</p><p>89 In order to obtain also negative examples, they generate all possible word pairs from the source and target words in the seed data and extract the ones which are not transliterations but have a common substring of some minimal length. [sent-347, score-0.462]
</p><p>90 438  Hermjakob (2009) proposed a linguistically focused word alignment system which uses many features including hand-crafted transliteration rules for Arabic/English alignment. [sent-350, score-0.991]
</p><p>91 His evaluation did not explicitly examine the effect of transliteration (alone) on word alignment. [sent-351, score-0.872]
</p><p>92 We show that the integration of a transliteration system based on unsupervised transliteration mining increases the word alignment quality for the two language pairs we tested. [sent-352, score-2.082]
</p><p>93 6  Conclusion  We proposed a method to automatically extract transliteration pairs from parallel corpora without supervision or linguistic knowledge. [sent-353, score-1.113]
</p><p>94 We integrated the transliteration extraction module into the GIZA++ word aligner and showed gains in alignment quality. [sent-356, score-1.045]
</p><p>95 We will release our transliteration mining system and word alignment system in the near future. [sent-357, score-1.123]
</p><p>96 Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. [sent-402, score-0.844]
</p><p>97 Whitepaper of news 2010 shared task on transliteration mining. [sent-412, score-0.89]
</p><p>98 Acquisition of English-Chinese transliterated word pairs from parallel-aligned texts using a statistical machine transliteration model. [sent-417, score-1.013]
</p><p>99 Language independent transliteration mining system using finite state automata framework. [sent-427, score-0.976]
</p><p>100 Unsupervised named entity transliteration using temporal and phonetic correlation. [sent-451, score-0.871]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('transliteration', 0.844), ('transliterations', 0.19), ('stopping', 0.155), ('iteration', 0.124), ('pairs', 0.12), ('mining', 0.106), ('parallel', 0.104), ('alignment', 0.093), ('multigrams', 0.079), ('filtered', 0.077), ('gold', 0.07), ('wil', 0.066), ('giza', 0.063), ('moses', 0.062), ('mgiza', 0.058), ('iterations', 0.057), ('badlao', 0.053), ('nbestti', 0.053), ('pmoses', 0.053), ('transliterator', 0.053), ('hmm', 0.051), ('interlanguage', 0.05), ('seed', 0.049), ('fta', 0.046), ('pta', 0.046), ('ibm', 0.045), ('list', 0.044), ('tao', 0.044), ('interpolate', 0.043), ('jiampojamarn', 0.043), ('sherif', 0.043), ('module', 0.042), ('alignments', 0.04), ('heuristic', 0.038), ('kumaran', 0.038), ('aligner', 0.038), ('peaks', 0.036), ('algorithm', 0.036), ('standards', 0.034), ('training', 0.033), ('probabilities', 0.033), ('resume', 0.032), ('yoon', 0.032), ('sproat', 0.032), ('klementiev', 0.031), ('character', 0.031), ('criterion', 0.031), ('target', 0.031), ('iterative', 0.03), ('splitting', 0.03), ('bisani', 0.03), ('cognates', 0.03), ('filtering', 0.029), ('side', 0.029), ('smoothed', 0.029), ('initially', 0.029), ('unsmoothed', 0.029), ('word', 0.028), ('phonetic', 0.027), ('candidateti', 0.026), ('cooc', 0.026), ('eisele', 0.026), ('endi', 0.026), ('noeman', 0.026), ('pti', 0.026), ('sajjad', 0.026), ('teiraitons', 0.026), ('unlabelled', 0.026), ('smooth', 0.026), ('system', 0.026), ('heldout', 0.025), ('corpora', 0.025), ('inflectional', 0.025), ('supplied', 0.025), ('news', 0.025), ('source', 0.024), ('kondrak', 0.024), ('submitted', 0.024), ('statistics', 0.023), ('hile', 0.023), ('eh', 0.023), ('labelled', 0.023), ('repeat', 0.022), ('build', 0.022), ('joint', 0.022), ('variants', 0.021), ('wikipedia', 0.021), ('transliterated', 0.021), ('unsupervised', 0.021), ('ea', 0.021), ('koehn', 0.021), ('shared', 0.021), ('fraser', 0.021), ('standard', 0.021), ('median', 0.02), ('links', 0.02), ('every', 0.02), ('translation', 0.02), ('extract', 0.02), ('manually', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="34-tfidf-1" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.</p><p>2 0.46667951 <a title="34-tfidf-2" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>Author: Masato Hagiwara ; Satoshi Sekine</p><p>Abstract: Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. However, a single model cannot deal with different words from different language origins, e.g., “get” in “piaget” and “target.” Li et al. (2007) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly. This model, however, requires an explicitly tagged training set with language origins. We propose a novel method which models language origins as latent classes. The parameters are learned from a set of transliterated word pairs via the EM algorithm. The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.</p><p>3 0.42399171 <a title="34-tfidf-3" href="./acl-2011-How_do_you_pronounce_your_name%3F_Improving_G2P_with_transliterations.html">153 acl-2011-How do you pronounce your name? Improving G2P with transliterations</a></p>
<p>Author: Aditya Bhargava ; Grzegorz Kondrak</p><p>Abstract: Grapheme-to-phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state-of-the-art G2P systems. We present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate significant accuracy improvements when re-ranking is applied to n-best lists generated by three different G2P programs.</p><p>4 0.27143469 <a title="34-tfidf-4" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<p>Author: Yun Huang ; Min Zhang ; Chew Lim Tan</p><p>Abstract: Machine transliteration is defined as automatic phonetic translation of names across languages. In this paper, we propose synchronous adaptor grammar, a novel nonparametric Bayesian learning approach, for machine transliteration. This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task.</p><p>5 0.10368446 <a title="34-tfidf-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.087887391 <a title="34-tfidf-6" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>7 0.087472498 <a title="34-tfidf-7" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>8 0.07296636 <a title="34-tfidf-8" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>9 0.071626149 <a title="34-tfidf-9" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>10 0.070701607 <a title="34-tfidf-10" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>11 0.068982273 <a title="34-tfidf-11" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>12 0.066698432 <a title="34-tfidf-12" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>13 0.065510094 <a title="34-tfidf-13" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>14 0.065038443 <a title="34-tfidf-14" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>15 0.064591728 <a title="34-tfidf-15" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>16 0.061998796 <a title="34-tfidf-16" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>17 0.060266718 <a title="34-tfidf-17" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>18 0.055213004 <a title="34-tfidf-18" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>19 0.05517932 <a title="34-tfidf-19" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>20 0.054597747 <a title="34-tfidf-20" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.066), (2, 0.027), (3, 0.091), (4, 0.022), (5, -0.026), (6, 0.076), (7, -0.009), (8, -0.035), (9, 0.087), (10, 0.067), (11, 0.091), (12, 0.043), (13, 0.176), (14, 0.021), (15, 0.031), (16, 0.198), (17, 0.148), (18, 0.389), (19, -0.215), (20, -0.32), (21, 0.208), (22, -0.13), (23, -0.084), (24, -0.252), (25, -0.138), (26, 0.047), (27, -0.013), (28, -0.049), (29, 0.098), (30, -0.112), (31, 0.013), (32, -0.027), (33, 0.038), (34, 0.016), (35, -0.028), (36, -0.023), (37, -0.086), (38, 0.02), (39, -0.068), (40, 0.027), (41, -0.034), (42, 0.008), (43, -0.087), (44, 0.059), (45, 0.056), (46, 0.024), (47, -0.001), (48, -0.04), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94632238 <a title="34-lsi-1" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.</p><p>2 0.88680977 <a title="34-lsi-2" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>Author: Masato Hagiwara ; Satoshi Sekine</p><p>Abstract: Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. However, a single model cannot deal with different words from different language origins, e.g., “get” in “piaget” and “target.” Li et al. (2007) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly. This model, however, requires an explicitly tagged training set with language origins. We propose a novel method which models language origins as latent classes. The parameters are learned from a set of transliterated word pairs via the EM algorithm. The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.</p><p>3 0.86506373 <a title="34-lsi-3" href="./acl-2011-How_do_you_pronounce_your_name%3F_Improving_G2P_with_transliterations.html">153 acl-2011-How do you pronounce your name? Improving G2P with transliterations</a></p>
<p>Author: Aditya Bhargava ; Grzegorz Kondrak</p><p>Abstract: Grapheme-to-phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state-of-the-art G2P systems. We present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate significant accuracy improvements when re-ranking is applied to n-best lists generated by three different G2P programs.</p><p>4 0.75354624 <a title="34-lsi-4" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<p>Author: Yun Huang ; Min Zhang ; Chew Lim Tan</p><p>Abstract: Machine transliteration is defined as automatic phonetic translation of names across languages. In this paper, we propose synchronous adaptor grammar, a novel nonparametric Bayesian learning approach, for machine transliteration. This model provides a general framework without heuristic or restriction to automatically learn syllable equivalents between languages. The proposed model outperforms the state-of-the-art EMbased model in the English to Chinese transliteration task.</p><p>5 0.38557193 <a title="34-lsi-5" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<p>Author: Vishal Goyal ; Gurpreet Singh Lehal</p><p>Abstract: Hindi-Punjabi being closely related language pair (Goyal V. and Lehal G.S., 2008) , Hybrid Machine Translation approach has been used for developing Hindi to Punjabi Machine Translation System. Non-availability of lexical resources, spelling variations in the source language text, source text ambiguous words, named entity recognition and collocations are the major challenges faced while developing this syetm. The key activities involved during translation process are preprocessing, translation engine and post processing. Lookup algorithms, pattern matching algorithms etc formed the basis for solving these issues. The system accuracy has been evaluated using intelligibility test, accuracy test and BLEU score. The hybrid syatem is found to perform better than the constituent systems. Keywords: Machine Translation, Computational Linguistics, Natural Language Processing, Hindi, Punjabi. Translate Hindi to Punjabi, Closely related languages. 1Introduction Machine Translation system is a software designed that essentially takes a text in one language (called the source language), and translates it into another language (called the target language). There are number of approaches for MT like Direct based, Transform based, Interlingua based, Statistical etc. But the choice of approach depends upon the available resources and the kind of languages involved. In general, if the two languages are structurally similar, in particular as regards lexical correspondences, morphology and word order, the case for abstract syntactic analysis seems less convincing. Since the present research work deals with a pair of closely related language 1 Gurpreet Singh Lehal Department of Computer Science Punjabi University, Patiala,India gs lehal @ gmai l com . i.e. Hindi-Punjabi , thus direct word-to-word translation approach is the obvious choice. As some rule based approach has also been used, thus, Hybrid approach has been adopted for developing the system. An exhaustive survey has already been given for existing machine translations systems developed so far mentioning their accuracies and limitations. (Goyal V. and Lehal G.S., 2009). 2 System Architecture 2.1 Pre Processing Phase The preprocessing stage is a collection of operations that are applied on input data to make it processable by the translation engine. In the first phase of Machine Translation system, various activities incorporated include text normalization, replacing collocations and replacing proper nouns. 2.2 Text Normalization The variety in the alphabet, different dialects and influence of foreign languages has resulted in spelling variations of the same word. Such variations sometimes can be treated as errors in writing. (Goyal V. and Lehal G.S., 2010). 2.3 Replacing Collocations After passing the input text through text normalization, the text passes through this Collocation replacement sub phase of Preprocessing phase. Collocation is two or more consecutive words with a special behavior. (Choueka :1988). For example, the collocation उ?र ?देश (uttar pradēsh) if translated word to word, will be translated as ਜਵਾਬ ਰਾਜ (javāb rāj) but it must be translated as ਉ?ਤਰ ਪ?ਦਸ਼ੇ (uttar pradēsh). The accuracy of the results for collocation extraction using t-test is not accurate and includes number of such bigrams and trigrams that are not actually collocations. Thus, manually such entries were removed and actual collocations were further extracted. The Portland, POrroecgeoend,in UgSsA o,f 2 t1he Ju AnCeL 2-0H1L1T. 2 ?c 021101 S1y Astessmoc Diaetmioonn fsotr a Ctioonms,p puatagteiosn 1a–l6 L,inguistics correct corresponding Punjabi translation for each extracted collocation is stored in the collocation table of the database. The collocation table of the database consists of 5000 such entries. In this sub phase, the normalized input text is analyzed. Each collocation in the database found in the input text will be replaced with the Punjabi translation of the corresponding collocation. It is found that when tested on a corpus containing about 1,00,000 words, only 0.001 % collocations were found and replaced during the translation. Hindi Text Figure 1: Overview of Hindi-Punjabi Machine Translation System 2.4 Replacing Proper Nouns A great proposition of unseen words includes proper nouns like personal, days of month, days of week, country names, city names, bank fastens words proper decide the translation process. Once these are recognized and stored into the noun database, there is no need to about their translation or transliteration names, organization names, ocean names, river every names, university words names etc. and if translated time in the case of presence in word to word, their meaning is changed. If the gazetteer meaning is not affected, even though this step fast. This input makes list text for the translation is self of such translation. growing This accurate and during each 2 translation. Thus, to process this sub phase, the system requires a proper noun gazetteer that has been complied offline. For this task, we have developed an offline module to extract proper nouns from the corpus based on some rules. Also, Named Entity recognition module has been developed based on the CRF approach (Sharma R. and Goyal V., 2011b). 2.5 Tokenizer Tokenizers (also known as lexical analyzers or word segmenters) segment a stream of characters into meaningful units called tokens. The tokenizer takes the text generated by pre processing phase as input. Individual words or tokens are extracted and processed to generate its equivalent in the target language. This module, using space, a punctuation mark, as delimiter, extracts tokens (word) one by one from the text and gives it to translation engine for analysis till the complete input text is read and processed. 2.6 Translation Engine The translation engine is the main component of our Machine Translation system. It takes token generated by the tokenizer as input and outputs the translated token in the target language. These translated tokens are concatenated one after another along with the delimiter. Modules included in this phase are explained below one by one. 2.6.1 Identifying Titles and Surnames Title may be defined as a formal appellation attached to the name of a person or family by virtue of office, rank, hereditary privilege, noble birth, or attainment or used as a mark of respect. Thus word next to title and word previous to surname is usually a proper noun. And sometimes, a word used as proper name of a person has its own meaning in target language. Similarly, Surname may be defined as a name shared in common to identify the members of a family, as distinguished from each member's given name. It is also called family name or last name. When either title or surname is passed through the translation engine, it is translated by the system. This cause the system failure as these proper names should be transliterated instead of translation. For example consider the Hindi sentence 3 ?ीमान हष? जी हमार ेयहाँ पधार।े (shrīmān harsh jī हष? hamārē yahāṃ padhārē). In this sentence, (harsh) has the meaning “joy”. The equivalent translation of हष? (harsh) in target language is ਖੁਸ਼ੀ (khushī). Similarly, consider the Hindi sentence ?काश ?सह हमार े (prakāsh siṃh hamārē yahāṃ padhārē). Here, ?काश (prakāsh) word is acting as proper noun and it must be transliterated and not translated because (siṃh) is surname and word previous to it is proper noun. Thus, a small module has been developed for यहाँ पधार।े. ?सह locating such proper nouns to consider them as title or surname. There is one special character ‘॰’ in Devanagari script to mark the symbols like डा॰, ?ो॰. If this module found this symbol to be title or surname, the word next and previous to this token as the case may be for title or surname respectively, will be transliterated not translated. The title and surname database consists of 14 and 654 entries respectively. These databases can be extended at any time to allow new titles and surnames to be added. This module was tested on a large Hindi corpus and showed that about 2-5 % text of the input text depending upon its domain is proper noun. Thus, this module plays an important role in translation. 2.6.2 Hindi Morphological analyzer This module finds the root word for the token and its morphological features.Morphological analyzer developed by IIT-H has been ported for Windows platform for making it usable for this system. (Goyal V. and Lehal G.S.,2008a) 2.6.3 Word-to-Word translation using lexicon lookup If token is not a title or a surname, it is looked up in the HPDictionary database containing Hindi to Punjabi direct word to word translation. If it is found, it is used for translation. If no entry is found in HPDictionary database, it is sent to next sub phase for processing. The HPDictionary database consists of 54, 127 entries.This database can be extended at any time to allow new entries in the dictionary to be added. 2.6.4 Resolving Ambiguity Among number of approaches for disambiguation, the most appropriate approach to determine the correct meaning of a Hindi word in a particular usage for our Machine Translation system is to examine its context using N-gram approach. After analyzing the past experiences of various authors, we have chosen the value of n to be 3 and 2 i.e. trigram and bigram approaches respectively for our system. Trigrams are further categorized into three different types. First category of trigram consists of context one word previous to and one word next to the ambiguous word. Second category of trigram consists of context of two adjacent previous words to the ambiguous word. Third category of the trigram consists of context of two adjacent next words to the ambiguous word. Bigrams are also categorized into two categories. First category of the bigrams consists of context of one previous word to ambiguous word and second category of the bigrams consists of one context word next to ambiguous word. For this purpose, the Hindi corpus consisting of about 2 million words was collected from different sources like online newspaper daily news, blogs, Prem Chand stories, Yashwant jain stories, articles etc. The most common list of ambiguous words was found. We have found a list of 75 ambiguous words out of which the most are स े sē and aur. (Goyal V. and frequent Lehal G.S., 2011) और 2.6.5 Handling Unknown Words 2.6.5.1 Word Inflectional Analysis and generation In linguistics, a suffix (also sometimes called a postfix or ending) is an affix which is placed after the stem of a word. Common examples are case endings, which indicate the grammatical case of nouns or adjectives, and verb endings. Hindi is a (relatively) free wordorder and highly inflectional language. Because of same origin, both languages have very similar structure and grammar. The difference is only in words and in pronunciation e.g. in Hindi it is लड़का and in Punjabi the word for boy is ਮੰੁਡਾ and even sometimes that is also not there like घर (ghar) and ਘਰ (ghar). The inflection forms of both these words in Hindi and Punjabi are also similar. In this activity, inflectional analysis without using morphology has been performed 4 for all those tokens that are not processed by morphological analysis module. Thus, for performing inflectional analysis, rule based approach has been followed. When the token is passed to this sub phase for inflectional analysis, If any pattern of the regular expression (inflection rule) matches with this token, that rule is applied on the token and its equivalent translation in Punjabi is generated based on the matched rule(s). There is also a check on the generated word for its correctness. We are using correct Punjabi words database for testing the correctness of the generated word. 2.6.5.2 Transliteration This module is beneficial for handling out-ofvocabulary words. For example the word िवशाल is as ਿਵਸ਼ਾਲ (vishāl) whereas translated as ਵੱਡਾ. There must be some method in every Machine Translation system for words like technical terms and (vishāl) transliterated proper names of persons, places, objects etc. that cannot be found in translation resources such as Hindi-Punjabi bilingual dictionary, surnames database, titles database etc and transliteration is an obvious choice for such words. (Goyal V. and Lehal G.S., 2009a). 2.7 Post-Processing 2.7.1 Agreement Corrections In spite of the great similarity between Hindi and Punjabi, there are still a number of important agreement divergences in gender and number. The output generated by the translation engine phase becomes the input for post-processing phase. This phase will correct the agreement errors based on the rules implemented in the form of regular expressions. (Goyal V. and Lehal G.S., 2011) 3 Evaluation and Results The evaluation document set consisted of documents from various online newspapers news, articles, blogs, biographies etc. This test bed consisted of 35500 words and was translated using our Machine Translation system. 3.1 Test Document For our Machine Translation system evaluation, we have used benchmark sampling method for selecting the set of sentences. Input sentences are selected from randomly selected news (sports, politics, world, regional, entertainment, travel etc.), articles (published by various writers, philosophers etc.), literature (stories by Prem Chand, Yashwant jain etc.), Official language for office letters (The Language Officially used on the files in Government offices) and blogs (Posted by general public in forums etc.). Care has been taken to ensure that sentences use a variety of constructs. All possible constructs including simple as well as complex ones are incorporated in the set. The sentence set also contains all types of sentences such as declarative, interrogative, imperative and exclamatory. Sentence length is not restricted although care has been taken that single sentences do not become too long. Following table shows the test data set: Table 1: Test data set for the evaluation of Hindi to Punjabi Machine Translation DTSWeo nctaruldenmscent 91DN03ae, 4wil0ys A5230,1rt6ic70lS4esytO0LQ38m6,1au5f4no9itg3c5e1uiaslgeB5130,lo6g50 L29105i,te84r05atue 3.2 Experiments It is also important to choose appropriate evaluators for our experiments. Thus, depending upon the requirements and need of the above mentioned tests, 50 People of different professions were selected for performing experiments. 20 Persons were from villages that only knew Punjabi and did not know Hindi and 30 persons were from different professions having knowledge of both Hindi and Punjabi. Average ratings for the sentences of the individual translations were then summed up (separately according to intelligibility and accuracy) to get the average scores. Percentage of accurate sentences and intelligent sentences was also calculated separately sentences. by counting the number of 3.2.1 Intelligibility Evaluation 5 The evaluators do not have any clue about the source language i.e. Hindi. They judge each sentence (in target language i.e. Punjabi) on the basis of its comprehensibility. The target user is a layman who is interested only in the comprehensibility of translations. Intelligibility is effected by grammatical errors, mistranslations, and un-translated words. 3.2.1.1 Results The response by the evaluators were analysed and following are the results: • 70.3 % sentences got the score 3 i.e. they were perfectly clear and intelligible. • 25. 1 % sentences got the score 2 i.e. they were generally clear and intelligible. • 3.5 % sentences got the score 1i.e. they were hard to understand. • 1. 1 % sentences got the score 0 i.e. they were not understandable. So we can say that about 95.40 % sentences are intelligible. These sentences are those which have score 2 or above. Thus, we can say that the direct approach can translate Hindi text to Punjabi Text with a consideably good accuracy. 3.2.2 Accuracy Evaluation / Fidelity Measure The evaluators are provided with source text along with translated text. A highly intelligible output sentence need not be a correct translation of the source sentence. It is important to check whether the meaning of the source language sentence is preserved in the translation. This property is called accuracy. 3.2.2.1 Results Initially Null Hypothesis is assumed i.e. the system’s performance is NULL. The author assumes that system is dumb and does not produce any valuable output. By the intelligibility of the analysis and Accuracy analysis, it has been proved wrong. The accuracy percentage for the system is found out to be 87.60% Further investigations reveal that out of 13.40%: • 80.6 % sentences achieve a match between 50 to 99% • 17.2 % of remaining sentences were marked with less than 50% match against the correct sentences. • Only 2.2 % sentences are those which are found unfaithful. A match of lower 50% does not mean that the sentences are not usable. After some post editing, they can fit properly in the translated text. (Goyal, V., Lehal, G.S., 2009b) 3.2.2 BLEU Score: As there is no Hindi –Parallel Corpus was available, thus for testing the system automatically, we generated Hindi-Parallel Corpus of about 10K Sentences. The BLEU score comes out to be 0.7801. 5 Conclusion In this paper, a hybrid translation approach for translating the text from Hindi to Punjabi has been presented. The proposed architecture has shown extremely good results and if found to be appropriate for MT systems between closely related language pairs. Copyright The developed system has already been copyrighted with The Registrar, Punjabi University, Patiala with authors same as the authors of the publication. Acknowlegement We are thankful to Dr. Amba Kulkarni, University of Hyderabad for her support in providing technical assistance for developing this system. References Bharati, Akshar, Chaitanya, Vineet, Kulkarni, Amba P., Sangal, Rajeev. 1997. Anusaaraka: Machine Translation in stages. Vivek, A Quarterly in Artificial Intelligence, Vol. 10, No. 3. ,NCST, Banglore. India, pp. 22-25. 6 Goyal V., Lehal G.S. 2008. Comparative Study of Hindi and Punjabi Language Scripts, Napalese Linguistics, Journal of the Linguistics Society of Nepal, Volume 23, November Issue, pp 67-82. Goyal V., Lehal, G. S. 2008a. Hindi Morphological Analyzer and Generator. In Proc.: 1st International Conference on Emerging Trends in Engineering and Technology, Nagpur, G.H.Raisoni College of Engineering, Nagpur, July16-19, 2008, pp. 11561159, IEEE Computer Society Press, California, USA. Goyal V., Lehal G.S. 2009. Advances in Machine Translation Systems, Language In India, Volume 9, November Issue, pp. 138-150. Goyal V., Lehal G.S. 2009a. A Machine Transliteration System for Machine Translation System: An Application on Hindi-Punjabi Language Pair. Atti Della Fondazione Giorgio Ronchi (Italy), Volume LXIV, No. 1, pp. 27-35. Goyal V., Lehal G.S. 2009b. Evaluation of Hindi to Punjabi Machine Translation System. International Journal of Computer Science Issues, France, Vol. 4, No. 1, pp. 36-39. Goyal V., Lehal G.S. 2010. Automatic Spelling Standardization for Hindi Text. In : 1st International Conference on Computer & Communication Technology, Moti Lal Nehru National Institute of technology, Allhabad, Sepetember 17-19, 2010, pp. 764-767, IEEE Computer Society Press, California. Goyal V., Lehal G.S. 2011. N-Grams Based Word Sense Disambiguation: A Case Study of Hindi to Punjabi Machine Translation System. International Journal of Translation. (Accepted, In Print). Goyal V., Lehal G.S. 2011a. Hindi to Punjabi Machine Translation System. In Proc.: International Conference for Information Systems for Indian Languages, Department of Computer Science, Punjabi University, Patiala, March 9-11, 2011, pp. 236-241, Springer CCIS 139, Germany. Sharma R., Goyal V. 2011b. Named Entity Recognition Systems for Hindi using CRF Approach. In Proc.: International Conference for Information Systems for Indian Languages, Department of Computer Science, Punjabi University, Patiala, March 9-11, 2011, pp. 31-35, Springer CCIS 139, Germany.</p><p>6 0.27005795 <a title="34-lsi-6" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>7 0.24687405 <a title="34-lsi-7" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>8 0.24663782 <a title="34-lsi-8" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>9 0.24549028 <a title="34-lsi-9" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>10 0.22936882 <a title="34-lsi-10" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>11 0.22654171 <a title="34-lsi-11" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>12 0.22499956 <a title="34-lsi-12" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>13 0.21998979 <a title="34-lsi-13" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>14 0.21092382 <a title="34-lsi-14" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>15 0.20960163 <a title="34-lsi-15" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>16 0.20889072 <a title="34-lsi-16" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>17 0.20728008 <a title="34-lsi-17" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>18 0.20611644 <a title="34-lsi-18" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>19 0.20576644 <a title="34-lsi-19" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>20 0.20419727 <a title="34-lsi-20" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.024), (17, 0.047), (20, 0.04), (26, 0.052), (31, 0.01), (37, 0.091), (39, 0.056), (41, 0.052), (55, 0.027), (59, 0.037), (72, 0.05), (84, 0.201), (91, 0.056), (96, 0.139), (97, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84922338 <a title="34-lda-1" href="./acl-2011-Local_and_Global_Algorithms_for_Disambiguation_to_Wikipedia.html">213 acl-2011-Local and Global Algorithms for Disambiguation to Wikipedia</a></p>
<p>Author: Lev Ratinov ; Dan Roth ; Doug Downey ; Mike Anderson</p><p>Abstract: Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call “global” approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.</p><p>same-paper 2 0.79521185 <a title="34-lda-2" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>Author: Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA++ word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments.</p><p>3 0.69694644 <a title="34-lda-3" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>4 0.69467163 <a title="34-lda-4" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>Author: Stefan Rud ; Massimiliano Ciaramita ; Jens Muller ; Hinrich Schutze</p><p>Abstract: We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.</p><p>5 0.69093847 <a title="34-lda-5" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>6 0.6902712 <a title="34-lda-6" href="./acl-2011-How_do_you_pronounce_your_name%3F_Improving_G2P_with_transliterations.html">153 acl-2011-How do you pronounce your name? Improving G2P with transliterations</a></p>
<p>7 0.69014436 <a title="34-lda-7" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>8 0.68981045 <a title="34-lda-8" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>9 0.68771732 <a title="34-lda-9" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>10 0.68742466 <a title="34-lda-10" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>11 0.687419 <a title="34-lda-11" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>12 0.68736869 <a title="34-lda-12" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>13 0.68710345 <a title="34-lda-13" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>14 0.68582821 <a title="34-lda-14" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>15 0.6849131 <a title="34-lda-15" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>16 0.68466628 <a title="34-lda-16" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>17 0.68452322 <a title="34-lda-17" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>18 0.68440711 <a title="34-lda-18" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>19 0.68415904 <a title="34-lda-19" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>20 0.68371677 <a title="34-lda-20" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
