<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-284" href="#">acl2011-284</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</h1>
<br/><p>Source: <a title="acl-2011-284-pdf" href="http://aclweb.org/anthology//P/P11/P11-1108.pdf">pdf</a></p><p>Author: Elias Ponvert ; Jason Baldridge ; Katrin Erk</p><p>Abstract: We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.</p><p>Reference: <a title="acl-2011-284-reference" href="../acl2011_reference/acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. [sent-4, score-0.624]
</p><p>2 We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. [sent-5, score-0.174]
</p><p>3 These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. [sent-6, score-0.435]
</p><p>4 Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL. [sent-7, score-0.399]
</p><p>5 , 2010) has largely built on the dependency model with valence of Klein and Manning (2004), and is characterized by its reliance on gold-standard part-of-speech (POS) annotations: the models are trained on and evaluated using sequences of POS tags rather than raw tokens. [sent-11, score-0.196]
</p><p>6 CCL established stateof-the-art results for unsupervised constituency pars1077 ing from raw text, and it is also incremental and extremely fast for both learning and parsing. [sent-14, score-0.226]
</p><p>7 Unfortunately, CCL is a non-probabilistic algorithm based on a complex set of inter-relating heuristics and a non-standard (though interesting) representation of constituent trees. [sent-15, score-0.26]
</p><p>8 (2010) explore an alternative strategy of unsupervised partial parsing: directly predicting low-level constituents based solely on word co-occurrence frequencies. [sent-19, score-0.322]
</p><p>9 Essentially, this means segmenting raw text into multiword constituents. [sent-20, score-0.177]
</p><p>10 In fact, simply extracting non-hierarchical multiword constituents from CCL’s output and putting a rightbranching structure over them actually works better than CCL’s own higher level predictions. [sent-22, score-0.232]
</p><p>11 This result suggests that improvements to low-level constituent prediction will ultimately lead to further gains in overall constituent parsing. [sent-23, score-0.52]
</p><p>12 Here, we present such an improvement by using probabilistic finite-state models for phrasal segmentation from raw text. [sent-24, score-0.3]
</p><p>13 The task for these models is chunking, so we evaluate performance on identification of multiword chunks of all constituent types as well as only noun phrases. [sent-25, score-0.499]
</p><p>14 Our unsupervised chunkers extend straightforwardly to a cascade that pre-  dicts higher levels of constituent structure, similar to the supervised approach of Brants (1999). [sent-26, score-0.702]
</p><p>15 This forms an overall unsupervised parsing system that outperforms CCL by a wide margin. [sent-27, score-0.208]
</p><p>16 1: Examples of constituent chunks extracted from syntactic trees 2  Data  We use the standard data sets for unsupervised con-  stituency parsing research: for English, the Wall Street Journal subset of the Penn Treebank-3 (WSJ, Marcus et al. [sent-33, score-0.596]
</p><p>17 In much unsupervised parsing work the test sentences are included in the training material. [sent-41, score-0.208]
</p><p>18 By unsupervised partial parsing, or simply unsupervised chunking, we mean the seg-  mentation of raw text into (non-overlapping) multiword constituents. [sent-48, score-0.458]
</p><p>19 The models are intended to capture local constituent structure the lower branches of a constituent tree. [sent-49, score-0.56]
</p><p>20 For this reason we evaluate 1078 –  ChnkC∩huN nkP s 12 706231K  WSJ NegraChnkC ∩hu NNnPkPsss253933KKK CTBChnkC ∩hu NNnPPksss495326KKK Table 1: Constituent chunks and base NPs in the datasets. [sent-50, score-0.156]
</p><p>21 94  Table 2: Percentage of gold standard constituents and words under constituent chunks and base NPs. [sent-63, score-0.61]
</p><p>22 using what we call constituent chunks, the subset of gold standard constituents which are i) branching (multiword) but ii) non-hierarchical (do not contain subconstituents). [sent-64, score-0.454]
</p><p>23 Examples of constituent chunks extracted from treebank constituent trees are in Fig. [sent-66, score-0.673]
</p><p>24 In English newspaper text, constituent chunks largely correspond with base NPs, but this is less the case with Chinese and German. [sent-68, score-0.416]
</p><p>25 Moreover, the relationship between NPs and constituent chunks is not a subset relation: some base NPs do have internal constituent structure. [sent-69, score-0.676]
</p><p>26 The numbers of constituent chunks and NPs for the training datasets are in Table 1. [sent-70, score-0.388]
</p><p>27 The percentage of constituents in these datasets which fall under these definitions, and the percentage of words under these constituents, are in Table 2. [sent-71, score-0.161]
</p><p>28 For parsing, the standard unsupervised parsing metric is unlabeled PARSEVAL. [sent-72, score-0.208]
</p><p>29 It measures precision and recall on constituents produced by a parser  as compared to gold standard constituents. [sent-73, score-0.25]
</p><p>30 Finally, CCL outperforms most published POS-based models when those models are trained on unsupervised word classes rather than gold POS tags. [sent-82, score-0.233]
</p><p>31 Though punctuation is usually entirely ignored in unsupervised parsing research, Seginer (2007) departs from this in one key aspect: the use of phrasal punctuation punctuation symbols that often mark phrasal boundaries within a sentence. [sent-85, score-0.885]
</p><p>32 These are used in two ways: i) they impose a hard constraint on constituent spans, in that no constituent (other than sentence root) may extend over a punctuation symbol, and ii) they contribute to the model, specifically in terms of the statistics of words seen adjacent to a phrasal boundary. [sent-86, score-0.789]
</p><p>33 2 –  ,  4  ◦  ￿  Unsupervised partial parsing  We learn partial parsers as constrained sequence models over tags encoding local constituent structure (Ramshaw and Marcus, 1995). [sent-91, score-0.528]
</p><p>34 A simple tagset is unlabeled BIO, which is familiar from supervised chunking and named-entity recognition: the tag B 1http : / /www . [sent-92, score-0.232]
</p><p>35 net / ccl 2This set is essentially that of Seginer (2007). [sent-94, score-0.434]
</p><p>36 While it is  clear from our analysis of CCL that it does make use of phrasal punctuation in Chinese, we are not certain whether ideographic comma is included. [sent-95, score-0.302]
</p><p>37 The models we use for unsupervised partial parsing are hidden Markov models, and a generalization we refer to as probabilistic right linear grammars (PRLGs). [sent-99, score-0.313]
</p><p>38 So, the emission and transition emanating from yn would be characterized as a PCFG rule yn → xn yn+1. [sent-108, score-0.515]
</p><p>39 HMMs factor rule probabilities into  emi→ssio xn and transition probabilities: P(yn → xn yn+1)  = P(xn, yn+1 |yn) ≈ P(xn|yn) P(yn+1 |yn). [sent-109, score-0.172]
</p><p>40 So, when we condition emission probabilities on both the current state yn and the next state yn+1, we have an exact model. [sent-111, score-0.206]
</p><p>41 This direct modeling of the right linear grammar rule yn → xn yn+1 is what we call a probabilistic right-linear grammar. [sent-112, score-0.323]
</p><p>42 This approach encoding a chunking problem as a tagging problem and learning to tag with HMMs goes back to Ramshaw and Marcus (1995). [sent-147, score-0.202]
</p><p>43 For unsupervised learning, the expectation is that the model will learn to generalize on phrasal boundaries. [sent-148, score-0.25]
</p><p>44 Using the low-level predictions of CCL as as benchmark, we evaluate the HMM and PRLG chunkers on the tasks of constituent chunk and base NP identification. [sent-154, score-0.558]
</p><p>45 Precision, recall and F-score are reported for full constituent identification brackets which do not match the gold standard exactly are false positives. [sent-160, score-0.403]
</p><p>46 ‘CCL’ refers to the lowest-level constituents extracted from full CCL output, as a benchmark chunker. [sent-162, score-0.202]
</p><p>47 –  As the lowest-level constituents of CCL were not specifically designed to describe chunks, we also  sets. [sent-165, score-0.161]
</p><p>48 CCL refers to the lowest constituents extracted from CCL output. [sent-166, score-0.161]
</p><p>49 checked the recall of all brackets generated by CCL against gold-standard constituent chunks. [sent-174, score-0.33]
</p><p>50 In fact, in Negra, the sequence model chunkers often find NPs embedded in PPs, which are not annotated as such. [sent-181, score-0.195]
</p><p>51 For instance, in the PP “hinter den Kulissen” (behind the scenes), both the PRLG and HMM chunkers identify the internal NP, though this is not identified in Negra and thus considered a false positive. [sent-182, score-0.201]
</p><p>52 Comparing the predictions directly, the two models of1081 POS Sequence # of errors TO VB 673 NNP NNP 450 MD VB 407 DT JJ 368  DT NN  280  Table 5: Top 5 POS sequences of the false positives predicted by the HMM. [sent-187, score-0.223]
</p><p>53 The improved results of the PRLG are based mostly on the fewer overall brackets predicted, and thus fewer false positives: for WSJ the PRLG incorrectly predicts 2241 NP constituents compared to 6949 for the HMM. [sent-189, score-0.3]
</p><p>54 Table 5 illustrates the top 5 POS sequences of the false positives predicted by the HMM. [sent-190, score-0.153]
</p><p>55 5  Constituent parsing with a cascade of chunkers  We use cascades of chunkers for full constituent parsing, building hierarchical constituents bottomup. [sent-206, score-0.966]
</p><p>56 After chunking is performed, all multiword constituents are collapsed and represented by a single pseudoword. [sent-207, score-0.434]
</p><p>57 The sentence is now a  string of symbols (normal words and pseudowords), to which a subsequent unsupervised chunking model is applied. [sent-209, score-0.322]
</p><p>58 Each chunker in the cascade chunks the raw text, then regenerates the dataset replacing chunks with pseudowords; this process is iterated until no new chunks are found. [sent-212, score-0.749]
</p><p>59 level 1NP numbers differ from the NP chunking numbers from Table 3 since they include root-level constituents which are often NPs. [sent-227, score-0.363]
</p><p>60 All chunkers in the cascade have the same settings in terms of smoothing, the tagset and initialization. [sent-230, score-0.326]
</p><p>61 While the first level of constituent analysis has high precision and recall on NPs, the second level often does well finding prepositional phrases (PPs), especially in WSJ; see Table 7. [sent-235, score-0.341]
</p><p>62 Luque,4 we compare on WSJ and Negra to the constituent context model (CCM) of Klein and Manning (2002). [sent-251, score-0.26]
</p><p>63 CCM learns to predict a set of brackets over a string (in practice, a string of POS tags) by jointly estimating constituent and distituent strings and contexts using an iterative EM-like procedure (though, as noted by Smith and Eisner (2004), CCM is deficient as a generative model). [sent-252, score-0.336]
</p><p>64 On the other hand, the other models use punctuation as an indicator of constituent boundaries, but all punctuation is dropped from the input to CCM. [sent-255, score-0.578]
</p><p>65 5 The results from the cascaded PRLG chunker are near or better than the best performance by CCL or CCM in these experiments. [sent-257, score-0.245]
</p><p>66 These and the full-length parsing results suggest that the cascaded chunker strategy generalizes better to longer sentences than does CCL. [sent-258, score-0.333]
</p><p>67 This is due, in part, to the chart structure used by CCM in the calculation of constituent and distituent probabilities: as in CKY parsing, the chart structure entails the trees predicted will  be binary-branching. [sent-261, score-0.341]
</p><p>68 CCL and the cascaded models can predict higher-branching constituent structures, 4http : //www . [sent-262, score-0.421]
</p><p>69 6  Phrasal punctuation revisited  Up to this point, the proposed models for chunking and parsing use phrasal punctuation as a phrasal separator, like CCL. [sent-270, score-0.868]
</p><p>70 Table 9a provides comparison of the sequence models’ performance on the constituent chunking task without using phrasal punctuation in training and evaluation. [sent-272, score-0.765]
</p><p>71 The table shows absolute improvement (+) or decline (−) in precision and recall wmhenent phrasal punctuation )is i rnem porevceidsi fornom an tdhe r edacatal. [sent-273, score-0.325]
</p><p>72 l The punctuation constraint seems to help the chunkers some, but not very much; ignoring punctuation seems to improve chunker results for the HMM on Chinese. [sent-274, score-0.563]
</p><p>73 Overall, the effect of phrasal punctuation on the chunker models’ performance is not clear. [sent-275, score-0.393]
</p><p>74 The results for cascaded parsing differ strongly from those for chunking, as Table 9b indicates. [sent-276, score-0.209]
</p><p>75 Using phrasal punctuation to constrain bracket prediction has a larger impact on cascaded parsing re-  enhtLg23. [sent-277, score-0.478]
</p><p>76 82  b) (Cascade) Parsing Table 9: Effects of dropping phrasal punctuation in unsupervised chunking and parsing evaluations relative to Tables 3 and 6. [sent-326, score-0.679]
</p><p>77 However, within the degrees of freedom allowed by punctuation constraints as described, the chunking models continue to find relatively good constituents. [sent-331, score-0.409]
</p><p>78 While CCL makes use of phrasal punctuation in previously reported results, the open source implementation allows it to be evaluated without this constraint. [sent-332, score-0.269]
</p><p>79 Comparing CCL to the cascaded chunkers when none of them use punctuation constraints, the cascaded chunkers (both HMMs and PRLGs) outperform CCL for each evaluation and dataset. [sent-335, score-0.703]
</p><p>80 For the CTB dataset, best chunking performance and cascaded parsing performance flips from the  HMM to the PRLG. [sent-336, score-0.411]
</p><p>81 More to the point, the PRLG is actually with worst performing model at the constituent chunking task, but the best performing cascade parser; also, this model has the most serious degrade in performance when phrasal punctuation is dropped from input. [sent-337, score-0.866]
</p><p>82 To investigate, we track the performance of the chunkers on the development dataset over iterations of EM. [sent-338, score-0.161]
</p><p>83 6a reveals the average length of the constituents predicted by the PRLG model increases over the course of EM. [sent-342, score-0.209]
</p><p>84 So, the PRLG chunker is predicting constituents that are longer than the ones targeted in the constituent chunking task: regardless of whether they are legitimate constituents or not, often they will likely be counted as false positives in this evaluation. [sent-345, score-0.987]
</p><p>85 This is confirmed by observing the constituent chunking precision in Fig. [sent-346, score-0.491]
</p><p>86 6b, which peaks when the average predicted constituent length is about the same the actual average length of those in the evaluation. [sent-347, score-0.308]
</p><p>87 The question, then, is whether the longer chunks predicted correspond to actual constituents or not. [sent-348, score-0.337]
</p><p>88 6c shows that the PRLG, when con-  strained by phrasal punctuation, does continue to improve its constituent prediction accuracy over the course ofEM. [sent-350, score-0.39]
</p><p>89 These correctly predicted constituents are not counted as such in the constituent chunking or base NP evaluations, but they factor directly into improved accuracy when this model is part of a cascade. [sent-351, score-0.699]
</p><p>90 7  Related work  Our task is the unsupervised analogue of chunking (Abney, 1991), popularized by the 1999 and 2000 Conference on Natural Language Learning shared tasks (Tjong et al. [sent-352, score-0.322]
</p><p>91 In addition to Seginer’s CCL model, the unsupervised parsing model of Gao and Suzuki (2003) and Gao et al. [sent-355, score-0.208]
</p><p>92 Brants (1999) reports a supervised cascaded chunking strategy for parsing which is strikingly similar to the methods proposed here. [sent-362, score-0.411]
</p><p>93 In both, Markov models are used in a cascade to predict hierarchical constituent structure; and in both, the parameters for the model at each level are estimated independently. [sent-363, score-0.435]
</p><p>94 There are major differences, though: the models here are learned from raw text without tree annotations, using EM to train parameters; Brants’ cascaded Markov models use supervised maximum likelihood estimation. [sent-364, score-0.307]
</p><p>95 Secondly, between the separate levels of the cascade, we collapse constituents into symbols which are treated as tokens in subsequent chunking levels; the Markov models in the higher cascade levels in Brants’ work actually emit constituent structure. [sent-365, score-0.85]
</p><p>96 1085 8  Conclusion  In this paper we have introduced a new subproblem of unsupervised parsing: unsupervised partial parsing, or unsupervised chunking. [sent-369, score-0.43]
</p><p>97 We have proposed a model for unsupervised chunking from raw text that is based on standard probabilistic finitestate methods. [sent-370, score-0.452]
</p><p>98 This model produces better local constituent predictions than the current best unsupervised parser, CCL, across datasets in English, German, and Chinese. [sent-371, score-0.41]
</p><p>99 By extending these probabilistic finite-state methods in a cascade, we obtain a general unsupervised parsing model. [sent-372, score-0.232]
</p><p>100 Improving unsupervised dependency parsing with richer contexts and smoothing. [sent-489, score-0.208]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prlg', 0.439), ('ccl', 0.434), ('constituent', 0.26), ('yn', 0.206), ('chunking', 0.202), ('chunkers', 0.161), ('constituents', 0.161), ('negra', 0.159), ('ccm', 0.158), ('punctuation', 0.139), ('cascade', 0.135), ('seginer', 0.13), ('phrasal', 0.13), ('chunks', 0.128), ('hmm', 0.126), ('chunker', 0.124), ('cascaded', 0.121), ('unsupervised', 0.12), ('raw', 0.106), ('parsing', 0.088), ('ctb', 0.081), ('chunk', 0.079), ('nps', 0.078), ('multiword', 0.071), ('rec', 0.071), ('prec', 0.071), ('xn', 0.069), ('wsj', 0.058), ('ponvert', 0.057), ('elsevier', 0.05), ('anig', 0.049), ('prlgs', 0.049), ('predicted', 0.048), ('brants', 0.047), ('moon', 0.043), ('marcus', 0.043), ('brackets', 0.043), ('benchmark', 0.041), ('partial', 0.041), ('pos', 0.04), ('models', 0.04), ('false', 0.04), ('headden', 0.04), ('pseudowords', 0.04), ('spitkovsky', 0.04), ('nnp', 0.039), ('positives', 0.039), ('emissions', 0.037), ('markov', 0.036), ('german', 0.036), ('hmms', 0.036), ('ramshaw', 0.036), ('parseval', 0.035), ('vb', 0.035), ('dt', 0.034), ('reichart', 0.034), ('transition', 0.034), ('sequence', 0.034), ('em', 0.033), ('gold', 0.033), ('chairman', 0.033), ('compuational', 0.033), ('distituent', 0.033), ('ideographic', 0.033), ('nonterm', 0.033), ('relieved', 0.033), ('vinken', 0.033), ('pps', 0.033), ('klein', 0.032), ('headed', 0.031), ('np', 0.031), ('austin', 0.03), ('predictions', 0.03), ('baldridge', 0.03), ('tagset', 0.03), ('smith', 0.029), ('precision', 0.029), ('adriaans', 0.029), ('solan', 0.029), ('subproblem', 0.029), ('constraints', 0.028), ('initialization', 0.028), ('base', 0.028), ('fewer', 0.028), ('recall', 0.027), ('illustrated', 0.027), ('chinese', 0.027), ('krenn', 0.026), ('lari', 0.026), ('levels', 0.026), ('gao', 0.026), ('sequences', 0.026), ('induction', 0.025), ('treebank', 0.025), ('abl', 0.025), ('prepositional', 0.025), ('probabilistic', 0.024), ('tags', 0.024), ('grammar', 0.024), ('schuler', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="284-tfidf-1" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>Author: Elias Ponvert ; Jason Baldridge ; Katrin Erk</p><p>Abstract: We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.</p><p>2 0.10964978 <a title="284-tfidf-2" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>3 0.10369589 <a title="284-tfidf-3" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>Author: Nathan Bodenstab ; Aaron Dunlop ; Keith Hall ; Brian Roark</p><p>Abstract: Efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses. We review prior methods for pruning and then present a new framework that unifies their strengths into a single approach. Using a log linear model, we learn the optimal beam-search pruning parameters for each CYK chart cell, effectively predicting the most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions.</p><p>4 0.093149632 <a title="284-tfidf-4" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>5 0.087892748 <a title="284-tfidf-5" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.</p><p>6 0.085646428 <a title="284-tfidf-6" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>7 0.082199238 <a title="284-tfidf-7" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>8 0.0821364 <a title="284-tfidf-8" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>9 0.080973983 <a title="284-tfidf-9" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>10 0.07997717 <a title="284-tfidf-10" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>11 0.076974735 <a title="284-tfidf-11" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>12 0.073604837 <a title="284-tfidf-12" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>13 0.072530292 <a title="284-tfidf-13" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>14 0.071104057 <a title="284-tfidf-14" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>15 0.069087312 <a title="284-tfidf-15" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>16 0.068574362 <a title="284-tfidf-16" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>17 0.064805366 <a title="284-tfidf-17" href="./acl-2011-Semi-Supervised_Modeling_for_Prenominal_Modifier_Ordering.html">275 acl-2011-Semi-Supervised Modeling for Prenominal Modifier Ordering</a></p>
<p>18 0.064606436 <a title="284-tfidf-18" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>19 0.063832842 <a title="284-tfidf-19" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>20 0.062112566 <a title="284-tfidf-20" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.042), (2, -0.045), (3, -0.112), (4, -0.036), (5, -0.038), (6, 0.013), (7, 0.028), (8, -0.012), (9, 0.024), (10, 0.008), (11, 0.056), (12, -0.006), (13, 0.021), (14, -0.004), (15, 0.034), (16, -0.042), (17, -0.001), (18, 0.043), (19, 0.079), (20, 0.016), (21, 0.051), (22, -0.063), (23, -0.018), (24, 0.035), (25, 0.049), (26, -0.009), (27, 0.024), (28, -0.031), (29, -0.05), (30, 0.053), (31, 0.054), (32, 0.034), (33, -0.057), (34, 0.022), (35, -0.068), (36, -0.022), (37, -0.078), (38, 0.043), (39, -0.061), (40, 0.078), (41, 0.042), (42, 0.091), (43, -0.043), (44, 0.009), (45, -0.047), (46, -0.068), (47, 0.018), (48, -0.063), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92747003 <a title="284-lsi-1" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>Author: Elias Ponvert ; Jason Baldridge ; Katrin Erk</p><p>Abstract: We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.</p><p>2 0.68963444 <a title="284-lsi-2" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>Author: Shu Cai ; David Chiang ; Yoav Goldberg</p><p>Abstract: We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese.</p><p>3 0.65554047 <a title="284-lsi-3" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>4 0.64168382 <a title="284-lsi-4" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>Author: Muhua Zhu ; Jingbo Zhu ; Minghan Hu</p><p>Abstract: For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline.</p><p>5 0.63613254 <a title="284-lsi-5" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>6 0.61723894 <a title="284-lsi-6" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>7 0.61641663 <a title="284-lsi-7" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>8 0.61255985 <a title="284-lsi-8" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>9 0.61088806 <a title="284-lsi-9" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>10 0.60244215 <a title="284-lsi-10" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>11 0.60053957 <a title="284-lsi-11" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>12 0.58309686 <a title="284-lsi-12" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>13 0.55479324 <a title="284-lsi-13" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>14 0.52191782 <a title="284-lsi-14" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>15 0.51942664 <a title="284-lsi-15" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>16 0.51415062 <a title="284-lsi-16" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>17 0.5103671 <a title="284-lsi-17" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>18 0.49569809 <a title="284-lsi-18" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>19 0.48641685 <a title="284-lsi-19" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<p>20 0.48398051 <a title="284-lsi-20" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (17, 0.076), (26, 0.016), (37, 0.109), (39, 0.084), (41, 0.048), (50, 0.239), (55, 0.04), (59, 0.036), (72, 0.031), (91, 0.064), (96, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89249223 <a title="284-lda-1" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>Author: Tony Veale</p><p>Abstract: Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning. In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1</p><p>same-paper 2 0.81580412 <a title="284-lda-2" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>Author: Elias Ponvert ; Jason Baldridge ; Katrin Erk</p><p>Abstract: We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.</p><p>3 0.76303709 <a title="284-lda-3" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Gillick ; Dan Klein</p><p>Abstract: We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.</p><p>4 0.67099917 <a title="284-lda-4" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>5 0.66193599 <a title="284-lda-5" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>6 0.65882111 <a title="284-lda-6" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>7 0.65389055 <a title="284-lda-7" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>8 0.65220523 <a title="284-lda-8" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>9 0.65200818 <a title="284-lda-9" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>10 0.651582 <a title="284-lda-10" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>11 0.65087897 <a title="284-lda-11" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>12 0.64958775 <a title="284-lda-12" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>13 0.64868242 <a title="284-lda-13" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>14 0.64847547 <a title="284-lda-14" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>15 0.64812052 <a title="284-lda-15" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>16 0.64704245 <a title="284-lda-16" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>17 0.6438821 <a title="284-lda-17" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>18 0.64359319 <a title="284-lda-18" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>19 0.64317608 <a title="284-lda-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.64305615 <a title="284-lda-20" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
