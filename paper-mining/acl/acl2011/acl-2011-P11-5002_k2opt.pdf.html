<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>239 acl-2011-P11-5002 k2opt.pdf</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-239" href="#">acl2011-239</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>239 acl-2011-P11-5002 k2opt.pdf</h1>
<br/><p>Source: <a title="acl-2011-239-pdf" href="http://aclweb.org/anthology//P/P11/P11-5002.pdf">pdf</a></p><p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="acl-2011-239-reference" href="../acl2011_reference/acl-2011-P11-5002_k2opt.pdf_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Formal and Empirical Grammatical Inference Jeffrey Heinz, Colin de la Higuera and Menno van Zaa nen heinz@udel . [sent-1, score-0.613]
</p><p>2 nl 1  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Outline of the tutorial I. [sent-4, score-0.55]
</p><p>3 Formal GI and learning theory (de la Higuera) II. [sent-5, score-0.07]
</p><p>4 Empirical approaches to regular and subregular natural language classes (Heinz) III. [sent-6, score-0.173]
</p><p>5 Empirical a pproaches to nonregular natural language classes (van Zaanen) 2  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns I Formal GI and learning theory What is grammatical inference? [sent-7, score-1.054]
</p><p>6 States show the regular expression indicating its “good tails” . [sent-11, score-0.157]
</p><p>7 C,V,i  k  8  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns w kata So for example:  t(w) kata  tkaitka >ttSaikta t. [sent-15, score-0.55]
</p><p>8 i ta>tSi 9  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Our definition Grammatical inference is about learning a grammar given information about a language  Questions Why grammar and not la nguage? [sent-18, score-0.679]
</p><p>9 10  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Why not write “learn a language”? [sent-20, score-0.55]
</p><p>10 Because you always learn a representation of a language  Paradox Take two learners learning a context-free language, one is learning a quadratic normal form and the other a Greibach normal form , they cannot agree that they have learnt the same thing (undecidable question) . [sent-21, score-0.064]
</p><p>11 11  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Our definition Grammatical inference is about learning a grammar given information about a language  How can a become the? [sent-27, score-0.619]
</p><p>12 Ask for the grammar to be the smallest, best (re a score). [sent-28, score-0.03]
</p><p>13 Occam argument Compression argument Kolmogorov complexity M DL argument 13  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Moreover GI is not only about building a grammar from some data . [sent-31, score-0.58]
</p><p>14 It is concerned with saying something about: the quality of the result, the quality of the learning process,  the properties of the process. [sent-32, score-0.036]
</p><p>15 14  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Naive example Suppose you are building a random number generator. [sent-33, score-0.55]
</p><p>16 Empirical approach Experimental approach Formal approach 15  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Empirical approach: using good (safe? [sent-38, score-0.55]
</p><p>17 18  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns What else would we like to say? [sent-45, score-0.55]
</p><p>18 That if the solution we have returned is not good , then that is because the initial data was bad (insufficient, biased)  Idea: Blame the data , not the algorithm 19  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Suppose we cannot say anything of the sort? [sent-46, score-0.55]
</p><p>19 Legal presentation from informant : (λ, +) , (abab, −), (a2b2, +), (a7b7, +), (aab, −), (abab, −),. [sent-65, score-0.048]
</p><p>20 31  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Example: presentation for Spanish Legal presentation Illegal presentation Legal presentation (un,+), (lugar,+) ,  from text: En un lugar de la Mancha. [sent-68, score-0.704]
</p><p>21 from text: Goooool from informant : (en,+) , (whatever,-), (lugor,-) , (xwszrrzt,-) , 32  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns What happens before convergence? [sent-71, score-0.571]
</p><p>22 Charles Babbage 33  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Further definitions Gφ. [sent-75, score-0.55]
</p><p>23 iven a presentation φ, φnis the set of the first n elements in A learning algorithm (learner) A is a function that takes as input a set φn and returns a grammar of a language. [sent-76, score-0.081]
</p><p>24 34  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Convergence to a hypothesis A converges to G with φ if ∀n ∈ N : A(φn) halts and gives an answer ∃n0 ∈ N : n ≥ n0 =⇒ A(φn) = G If furthermore L( G) = Yields(φ) then we have identified . [sent-78, score-0.55]
</p><p>25 35  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Identification in the limit YieldsL Pres(L)  AGL Figure: The learning setting. [sent-79, score-0.596]
</p><p>26 38  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns What about efficiency? [sent-82, score-0.55]
</p><p>27 We can try to bound global time update time errors before converging (IPE) mind changes (MC) queries good examples needed (characteristic samples)  (Pitt 1989, de la Higuera et al . [sent-83, score-0.08]
</p><p>28 2008) 39  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Definition: polynomial number of implicit prediction errors Denote by G = x if G is incorrect with respect to an element x of the presentation (i. [sent-84, score-0.607]
</p><p>29 G is polynomially identifiable in the limit from Pres if there exists an identification learner A and a polynomial p() such that given any G in G, and given any presentation φ of L( G) , ♯i : A(φi) φ(i + 1) ≤ p(| G|). [sent-87, score-0.139]
</p><p>30 Main positive results Can learn Dfa from an informant with polynomial resources (Oncina and Garc´ ıa 1992) ; Can learn Dfa from membership and equivalence queries (Angluin 1987). [sent-91, score-0.098]
</p><p>31 51  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Results Too easy to learn with L∞ Too hard to learn with L1 Both results hold for the same algorithm ! [sent-95, score-0.582]
</p><p>32 53  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns II. [sent-102, score-0.55]
</p><p>33 54  FLor maglGiIca ndl eyarni PgtohesorsyibleCompGIuofRtaegublarPeat PernasternsEmpircalGIandnonregularpaterns  Clements and Keyser 1983  KwBaakciuht 1l9 s7tr5ess 55  FGormIal S GIt anrda lteaerngingi tehesoryGI of Regular Pat ernsEmpircal GI and nonregular pat erns  #1. [sent-108, score-0.641]
</p><p>34 Define “learning” so that large regions can be learned  56  FGormIal S GIt anrda lteaerngingi tehesoryGI of Regular Pat ernsEmpircal GI and nonregular pat erns #2. [sent-109, score-0.641]
</p><p>35 Target non-superfinite cross-cutting classes  (instructor’s bias)  57  FCormoaml GIm ando leanrni gT th eoreymeGI of Regular Pat ernsEmpircal GI and nonregular pat erns 1 Different learning frameworks may better characterize the data presentations learners actually get (strategy #1) . [sent-110, score-0.665]
</p><p>36 2 Classes of formal languages may exist which better  characterize the patterns we are interested in (strategy #2) . [sent-111, score-0.256]
</p><p>37 f Regular Pat ernsEmpircal GI and nonregular pat erns Insights obtained here can be (and have been) applied fruitfully to nonregular classes. [sent-114, score-1.083]
</p><p>38 Angluin 1982 showed a subclass of regular languages (the reversible languages) was identifiable in the limit from positive data by an incremental learner. [sent-115, score-0.263]
</p><p>39 Yokomori’s (2004) Very Simple Languages are a subclass of the context-free languages, but draws on ideas from the reversible languages. [sent-116, score-0.033]
</p><p>40 Similarly, Clark and Eryaud ’s (2007) substituta ble languages (also subclass of context-free) are also based on insights from this paper. [sent-117, score-0.061]
</p><p>41 representations) for regular languages: 1 Regular expressions 2 Generalized regular expressions  3 Finite state acceptors 4 Words which satisfy formulae in monadic second order logic 5 Right or left branching rewrite rules 6  . [sent-120, score-0.332]
</p><p>42 representations) for regular relations: Regular expressions (for relations) Generalized regular expressions (for relations) Finite state transducers . [sent-125, score-0.339]
</p><p>43 s r:epresentations) for distributions over Weighted finite state automata Hidden Markov Models Weighted right or left branching rewrite rules . [sent-130, score-0.056]
</p><p>44 63  FTormhalisGIatndulteaornirngiathleo:ryFinteStGaItoefReAgulaurPtaotemrnsat EmpircalGIandnonregularpaterns Acceptors and subsequential transducers admit canonical forms 1 The smallest deterministic acceptor, syntactic monoids, . [sent-133, score-0.074]
</p><p>45 states represent sets of “good tails” ) 3 In contrast, canonical regular expressions have yet to be determined . [sent-138, score-0.176]
</p><p>46 ) is identifiable No in the limit (Gold 1967) 2 Not even the finite class is PAC-learnable (Blumer et al . [sent-155, score-0.093]
</p><p>47 1989) 3 No superfinite class is identifia ble in the limit with probability p (p > 2/3) (Pitt 1985, Wiehagen et al . [sent-156, score-0.066]
</p><p>48 1986, Angluin 1988)  4 But many subregular classes are learnable in this difficult setting. [sent-157, score-0.042]
</p><p>49 67  FLorema lrGnIaindnlegarninFgrthaeomryeworks:GMIofaRiegnularRPaetesrunsltsEmpircalGIandnonregularpaterns “clDasisstersibution-free” with positive data only: learnable subregular 1 reversible languages (Angluin 1982) 2 strictly local languages (Garcia et al . [sent-158, score-0.173]
</p><p>50 1990) 3 locally testable and piecewise testa ble (Garcia and Ruiz 2004) 4 left-to-right and right-to-left iterative languages (Heinz 2008) 5 strictly piecewise languages (Heinz 2010)  . [sent-159, score-0.18]
</p><p>51 languages are identifiable in the limit from computa ble classes of r. [sent-168, score-0.085]
</p><p>52 80  FIolrmuals GtIr anadlt eiavrni eg th EeoxryampleGI of Regular Pat ernsEmpircal GI and nonregular pat erns Let’s merge states with the same incoming paths of length 2! [sent-199, score-0.693]
</p><p>53 PT(S)  81  FRormeasl GuIl antd l oearfni Sg t heaoryte MerginGgI of Regular Pat ernsEmpircal GI and nonregular pat erns 0 σ´1σ2 σ`σ43--67 σ`σσ5-8 This acceptor is not the canonical acceptor we saw earlier but it recognizes the same language. [sent-200, score-0.691]
</p><p>54 ⇐de⇒f ∃x,y,  u∼ v w such that |w| = k, u = xw, v = yw 2 The algorithm then is simply: G = PT(S)/π∼ 3 This a lgorithm provably identifies in the limit from positive data the Strictly (k + 1)-Local class of languages (Garcia et al. [sent-202, score-0.068]
</p><p>55 In other words, 44 attested stress patterns are Strictly 3-Loca l and 81 are Strictly 6-Local . [sent-207, score-0.108]
</p><p>56 al 1990) 2 recursively eliminate reverse non-determinism (Angluin 1982)  3 m20e0r7g)e states with same “contexts” (Muggleton 1990, Clark and Eryaud 4 merge final states (Heinz 2008) 5 merge states with same “neighborhood” (Heinz 2009)  . [sent-211, score-0.118]
</p><p>57 6 7 merge states to maximize posterior probability (for HM Ms, Stolcke 1994) 8  . [sent-214, score-0.04]
</p><p>58 85  FOormtahl GeI arnd w learani ygs the toroy merge stGIa otfe Resgular Pat ernsEmpircal GI and nonregular pat erns Merge states indiscriminately unless “ill-formedness”  arises  Merge unless something tells us not to 1 unless “onward subsequentiality” Oncina et al . [sent-217, score-0.712]
</p><p>59 86  FSortma ltGIea-ndmleaernirng tihneorgyasinfer GnIocfeRegurlaurPlaetsernsEmpircalGIandnonregularpaterns Strictly k-Local languages (Garcia et al. [sent-221, score-0.033]
</p><p>60 89  FRoremasluGIaltndsleafrnoingrthre orygularlangGuIaofgRe guslarPaternsEmpircalGIandnonregularpaterns Distribution-free with positive data  Identification in the limit from positive data 1 strictly k-local languages (each state corresponds to suffixes of up to length k) (Garcia et al . [sent-226, score-0.103]
</p><p>61 1990) 2 reversible languages (acceptors are both forward and reverse k-deterministic for some k) (Angluin 1982) 3 k-contextual languages (M uggleton 1990) 4  . [sent-227, score-0.083]
</p><p>62 90  FRoremagl GuI alnadr lear niengl tahetoriyonsGI of Regular Pat ernsEmpircal GI and nonregular pat erns Regular relations in CL 1 transliteration 2 translation 3 4 anything with finite state transducers  . [sent-230, score-0.687]
</p><p>63 1GI9 of9 Re3gu)lar Pat ernsEmpircal GI and nonregular pat erns distribution-free with positive data  OSTIA 1didaetnat. [sent-234, score-0.641]
</p><p>64 ifies subsequential functions in the limit from positive 2 Merges states greedily unless subsequentiality is violated 3 If the function is partial , exactness is guaranteed only where the function is defined . [sent-235, score-0.073]
</p><p>65 1GI9 of9 Re3gu)lar Pat ernsEmpircal GI and nonregular pat erns Subsequential relations 1 a subclass of the regular relations, recognizing functions. [sent-237, score-0.814]
</p><p>66 are 2 are those which are recognized by subsequential transducers, which are determinstic on the input and which have an “output” string associated with every state. [sent-238, score-0.032]
</p><p>67 94  FWormealiGgIahndtle adrni gftihneoiryte-staeaGuItof Rmegula rPta ternsEmpircalGIandnonregularpaterns non-distribution-free with positive data  The problem Given a finite multiset of words drawn independently from the target distribution , what grammar accurately describes the distribution? [sent-243, score-0.051]
</p><p>68 Theorem The class of distributions describable with Non-deterministic Probabilistic Finite-State Automata (NPFA) exactly matches the class of distributions describable with Hidden Markov Models (Vidal et al. [sent-244, score-0.098]
</p><p>69 Theorem For a sample S and deterministic finite-state acceptor M, counting the parse of S through M and normalizing at each state optimizes the maximum-likelihood estimate. [sent-248, score-0.042]
</p><p>70 al 2005, de la Higuera 2010)  FSortmralicGItanldylea2rni-nLgthoeocryalDistribuGItofi RegnulsarPa tre rnsbigrammEmopdirceal GsIandnonregularpaterns  96  Figure: The structure of a bigram model . [sent-250, score-0.065]
</p><p>71 97  FSorumabl GrIe angd luearlnainrg th deoirystributionsGI of Regular Pat ernsEmpircal GI and nonregular pat erns 1 When the structure of a Deterministic FSA is known in advance, M LE is easy to do. [sent-252, score-0.641]
</p><p>72 100  FSortmralic GIt anldy lea Prni ge thce orywise languGIa ogf Re gsular Pat ernsEmpircal GI and nonregular pat erns Rogers et al. [sent-258, score-0.654]
</p><p>73 They have several chacterizations in terms of formal language theory, automata theory, logic, model theory, and the 3 algebraic theory of automata (Fu et al . [sent-260, score-0.214]
</p><p>74 2005) 3 Estimation over the factors permits learnability of the patterns like the ones in Samala . [sent-263, score-0.1]
</p><p>75 ) larger class than that which is describable with n-gram distributions or with SP distributions. [sent-281, score-0.049]
</p><p>76 1993, de la Higuera and Thollard 2000, Clark and Thollard 2004, . [sent-285, score-0.046]
</p><p>77 105  FSorumaml GIm anda leraryni g theoryGI of Regular Pat ernsEmpircal GI and nonregular pat erns #2. [sent-288, score-0.641]
</p><p>78 EmpircalGIandnonregularpaterns Research strategy Patterns ⇒ Characterizations ⇒ Learning algorithms 1 Identify the range and kind of patterns (linguistics). [sent-300, score-0.084]
</p><p>79 2 Characterize the range and kind of patterns (computational linguistics). [sent-301, score-0.084]
</p><p>80 2 It has yielded theoretical results in many learning frameworks including both distribution-free and non-distribution-free learning frameworks. [sent-306, score-0.048]
</p><p>81 110  FCormoalnGcIalnudlseaironi ngthteo rysection2GIpofaRergtular2PaternsEmpircalGIandnonregularpaterns 1 Many subclasses of regular languages are learna ble even in the hardest learning settings. [sent-307, score-0.226]
</p><p>82 4 There is a rich literature in GI which speaks to these classes, and how such patterns in these classes ca n be learned . [sent-309, score-0.084]
</p><p>83 (limited) context-free languages “Learning structure” Structure is flexible, restricted by learning algorithm Requires plain sequences as in put Corresponds to e. [sent-319, score-0.057]
</p><p>84 context-free languages 121  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Empirical grammatical inference Choices: What type of grammar are we learning? [sent-321, score-0.65]
</p><p>85 122  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Overview of systems EM ILE Alignment-Based  Learning  ADIOS CCM+DMV U-DOP . [sent-333, score-0.55]
</p><p>86 (ABL) 123  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Underlying approach Given a collection of plain sentences On what basis are we going to assign structure? [sent-336, score-0.55]
</p><p>87 tomorrow what airline is this 147  Formal GI and learning theoryGI of Regular PatternsEmpirical GI and nonregular patterns Results on ATIS Precision Recall F-Score  M icro 47. [sent-346, score-0.55]
</p><p>88 Recognition and learning of a class of context-sensitive languages described by augmented regular expressions. [sent-437, score-0.227]
</p><p>89 A note on the number of queries needed to identify regular languages. [sent-449, score-0.172]
</p><p>90 Learning stochastic regular grammars by means ofa state merging method. [sent-535, score-0.18]
</p><p>91 Polynomial identification in the limit of substitutable context-free languages. [sent-573, score-0.04]
</p><p>92 Identification in the limit with probability one of stochastic deterministic finite automata. [sent-611, score-0.06]
</p><p>93 Learning languages from bounded resources: the case of the DFA and the balls of strings. [sent-622, score-0.033]
</p><p>94 An alge-  braic characterization of strictly piecewise languages. [sent-656, score-0.053]
</p><p>95 Learning k-testable and k-piecewise testable languages from positive data. [sent-663, score-0.058]
</p><p>96 Inference of k-testable languages in the strict sense and application to syntactic pattern recognition. [sent-669, score-0.033]
</p><p>97 On the role of locality in learning stress patterns. [sent-752, score-0.048]
</p><p>98 Cryptographic limitations on learning boolean formulae and finite automata. [sent-789, score-0.045]
</p><p>99 Polynomial-time identification of very simple grammars from positive data. [sent-997, score-0.041]
</p><p>100 Learning mildly context-sensitive languages with multidimensional substitutability from positive data. [sent-1002, score-0.065]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gi', 0.632), ('nonregular', 0.442), ('theorygi', 0.364), ('patternsempirical', 0.357), ('regular', 0.157), ('formal', 0.139), ('pat', 0.121), ('patterns', 0.084), ('erns', 0.078), ('ernsempircal', 0.071), ('higuera', 0.064), ('angluin', 0.05), ('bla', 0.05), ('heinz', 0.039), ('oncina', 0.037), ('thollard', 0.035), ('languages', 0.033), ('subsequential', 0.032), ('substitutability', 0.032), ('polynomial', 0.03), ('la', 0.03), ('grammar', 0.03), ('strictly', 0.029), ('garcia', 0.029), ('dfa', 0.029), ('clit', 0.028), ('icgi', 0.028), ('presentation', 0.027), ('likes', 0.026), ('learnable', 0.026), ('transducers', 0.025), ('pac', 0.025), ('adriaans', 0.025), ('testable', 0.025), ('acceptor', 0.025), ('florema', 0.025), ('stress', 0.024), ('piecewise', 0.024), ('learner', 0.024), ('learning', 0.024), ('grammars', 0.023), ('grammatical', 0.022), ('limit', 0.022), ('clark', 0.022), ('nts', 0.022), ('rogers', 0.022), ('finite', 0.021), ('describable', 0.021), ('informant', 0.021), ('merge', 0.021), ('automata', 0.02), ('zaanen', 0.02), ('al', 0.019), ('states', 0.019), ('vidal', 0.019), ('ccm', 0.019), ('obj', 0.018), ('identification', 0.018), ('hansen', 0.018), ('family', 0.018), ('acceptors', 0.018), ('kearns', 0.018), ('lrgniaindnlegarninfgrthaeomryeworks', 0.018), ('stojonowonowas', 0.018), ('identifiable', 0.018), ('mary', 0.018), ('van', 0.017), ('garc', 0.017), ('reversible', 0.017), ('deterministic', 0.017), ('lecture', 0.016), ('macro', 0.016), ('heidelberg', 0.016), ('de', 0.016), ('theory', 0.016), ('learn', 0.016), ('subclass', 0.016), ('dmv', 0.016), ('learnability', 0.016), ('subregular', 0.016), ('inference', 0.015), ('notes', 0.015), ('queries', 0.015), ('distributions', 0.015), ('atis', 0.014), ('fsortma', 0.014), ('gmiofariegnularrpaetesrunsltsempircalgiandnonregularpaterns', 0.014), ('walks', 0.014), ('editors', 0.014), ('phonology', 0.013), ('abl', 0.013), ('amsterdam', 0.013), ('class', 0.013), ('lea', 0.013), ('ble', 0.012), ('pitt', 0.012), ('something', 0.012), ('subtrees', 0.012), ('paths', 0.012), ('subsequences', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="239-tfidf-1" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>2 0.074818708 <a title="239-tfidf-2" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>Author: Renxian Zhang</p><p>Abstract: In summarization, sentence ordering is conducted to enhance summary readability by accommodating text coherence. We propose a grouping-based ordering framework that integrates local and global coherence concerns. Summary sentences are grouped before ordering is applied on two levels: group-level and sentence-level. Different algorithms for grouping and ordering are discussed. The preliminary results on single-document news datasets demonstrate the advantage of our method over a widely accepted method. 1 Introduction and Background The canonical pipeline of text summarization consists of topic identification, interpretation, and summary generation (Hovy, 2005). In the simple case of extraction, topic identification and interpretation are conflated to sentence selection and concerned with summary informativeness. In . comparison, summary generation addresses summary readability and a frequently discussed generation technique is sentence ordering. It is implicitly or explicitly stated that sentence ordering for summarization is primarily driven by coherence. For example, Barzilay et al. (2002) use lexical cohesion information to model local coherence. A statistical model by Lapata (2003) considers both lexical and syntactic features in calculating local coherence. More globally biased is Barzilay and Lee’s (2004) HMM-based content model, which models global coherence with word distribution patterns. Whilst the above models treat coherence as lexical or topical relations, Barzilay and Lapata (2005, 2008) explicitly model local coherence with an entity grid model trained for optimal syntactic role transitions of entities. 6 . polyu .edu .hk Although coherence in those works is modeled in the guise of “lexical cohesion”, “topic closeness”, “content relatedness”, etc., few published works simultaneously accommodate coherence on the two levels: local coherence and global coherence, both of which are intriguing topics in text linguistics and psychology. For sentences, local coherence means the wellconnectedness between adjacent sentences through lexical cohesion (Halliday and Hasan, 1976) or entity repetition (Grosz et al., 1995) and global coherence is the discourse-level relation connecting remote sentences (Mann and Thompson, 1995; Kehler, 2002). An abundance of psychological evidences show that coherence on both levels is manifested in text comprehension (Tapiero, 2007). Accordingly, an apt sentence ordering scheme should be driven by such concerns. We also note that as sentence ordering is usually discussed only in the context of multi-document summarization, factors other than coherence are also considered, such as time and source sentence position in Bollegala et al.’s (2006) “agglomerative ordering” approach. But it remains an open question whether sentence ordering is non-trivial for single-document summarization, as it has long been recognized as an actual strategy taken by human summarizers (Jing, 1998; Jing and McKeown, 2000) and acknowledged early in work on sentence ordering for multi-document summarization (Barzilay et al., 2002). In this paper, we outline a grouping-based sentence ordering framework that is driven by the concern of local and global coherence. Summary sentences are grouped according to their conceptual relatedness before being ordered on two levels: group-level ordering and sentence-level ordering, which capture global coherence and local coherence in an integrated model. As a preliminary study, we applied the framework to singlePortland, ORP,r UoSceAed 1i9ng-2s4 of Ju tnhee 2 A0C1L1-H. ?Lc T2 0201111 A Ssstuodcieanttio Snes fsoiro Cn,o pmapguesta 6t–io1n1a,l Linguistics document summary generation and obtained interesting results. The main contributions of this work are: (1) we stress the need to channel sentence ordering research to linguistic and psychological findings about text coherence; (2) we propose a groupingbased ordering framework that integrates both local and global coherence; (3) we find in experiments that coherence-driven sentence ordering improves the readability of singledocument summaries, for which sentence ordering is often considered trivial. In Section 2, we review related ideas and techniques in previous work. Section 3 provides the details of grouping-based sentence ordering. The preliminary experimental results are presented in Section 4. Finally, Section 5 concludes the whole paper and describes future work. 2 Grouping-Based Ordering Our ordering framework is designed to capture both local and global coherence. Globally, we identify related groups among sentences and find their relative order. Locally, we strive to keep sentence similar or related in content close to each other within one group. 2.1 Sentence Representation As summary sentences are isolated from their original context, we retain the important content information by representing sentences as concept vectors. In the simplest case, the “concept” is equivalent to content word. A drawback of this practice is that it considers every content word equally contributive to the sentence content, which is not always true. For example, in the news domain, entities realized as NPs are more important than other concepts. To represent sentences as entity vectors, we identify both common entities (as the head nouns of NPs) and named entities. Two common entities are equivalent if their noun stems are identical or synonymous. Named entities are usually equated by identity. But in order to improve accuracy, we also consider: 1) structural subsumption (one is part of another); 2) hypernymy and holonymy (the named entities are in a superordinate-subordinate or part-whole relation). Now with summary sentence Si and m entities eik (k = 1 m), Si = (wf(ei1), wf(ei2), wf(eim)), … … … … … …, 7 where wf(eik) = wk× ×f(eik), f(eik) is the frequency of eik and wk is the weight of eik. We define wk = 1 if eik is a common entity and wk = 2 if eik is a named entity. We give double weight to named entities because of their significance to news articles. After all, a news story typically contains events, places, organizations, people, etc. that denote the news theme. Other things being equal, two sentences sharing a mention of named entities are thematically closer than two sentences sharing a mention of common entities. Alternatively, we can realize the “concept” as “event” because events are prevalent semantic constructs that bear much of the sentence content in some domains (e.g., narratives and news reports). To represent sentences as event vectors, we can follow Zhang et al.’s (2010) method at the cost of more complexity. 2.2 Sentence Grouping To meet the global need of identifying sentence groups, we develop two grouping algorithms by applying graph-based operation and clustering. Connected Component Finding (CC) This algorithm treats grouping sentences as finding connected components (CC) in a text graph TG = (V, E), where V represents the sentences and E the sentence relations weighted by cosine similarity. Edges with weight < t, a threshold, are removed because they represent poor sentence coherence. The resultant graph may be disconnected, in which we find all of its connected components, using depth-first search. The connected components are the groups we are looking for. Note that this method cannot guarantee that every two sentences in such a group are directly linked, but it does guarantee that there exists a path between every sentence pair. Modified K-means Clustering (MKM) Observing that the CC method finds only coherent groups, not necessarily groups of coherent sentences, we develop a second algorithm using clustering. A good choice might be K-means as it is efficient and outperforms agglomerative clustering methods in NLP applications (Steibach et al., 2000), but the difficulty with the conventional K-means is the decision of K. Our solution is modified K-means (MKM) based on (Wilpon and Rabiner, 1985). Let’s denote cluster iby CLi and cluster similarity by Sim(CLi) =SimM,SiinnCLi(Sim( Sim,Sin)), where Sim( Sim,Sin)is their cvMeluin231st.(roCWSD21imLsdhtoIf(=ClaSehbLsimt)l1;(zehS<-amncs,tdoeSa1vn)c;st:el=uMionvrate(lhcSKiosmg-tC;eblLayn)s,c2riuegant wcoelsunathdi cosine. The following illustrates the algorithm. The above algorithm stops iterating when each cluster contains all above-threshold-similarity sentence pairs or only one sentence. Unlike CC, MKM results in more strongly connected groups, or groups of coherence sentences. 2.3 Ordering Algorithms After the sentences are grouped, ordering is to be conducted on two levels: group and sentence. Composed of closely related sentences, groups simulate high-level textual constructs, such as “central event”, “cause”, “effect”, “background”, etc. for news articles, around which sentences are generated for global coherence. For an intuitive example, all sentences about “cause” should immediately precede all sentences about “effect” to achieve optimal readability. We propose two approaches to group-level ordering. 1) If the group sentences come from the same document, group (Gi) order is decided by the group-representing sentence (gi) order ( means “precede”) in the text. gi gj  Gi Gj 2) Group order is decided in a greedy fashion in order to maximize the connectedness between adjacent groups, thus enhancing local coherence. Each time a group is selected to achieve maximum similarity with the ordered groups and the first ordered group (G1) is selected to achieve maximum similarity with all the other groups. G1argmGaxG'GSim( G , G') GiGuanrogrdemred agrxoupsij1 Sim( Gj, G) (i > 1) where Sim(G, G’) is the average sentence cosine similarity between G and G’. 8 Within the ordered groups, sentence-level ordering is aimed to enhance local coherence by placing conceptually close sentences next to each other. Similarly, we propose two approaches. 1) If the sentences come from the same document, they are arranged by the text order. 2) Sentence order is greedily decided. Similar to the decision of group order, with ordered sentence Spi in group Gp: Sp1argSm GpaxS'SSim( S , S') SpiSunorader egd smenteanxces in Gpji1 Sim( Spj,S )(i > 1) Note that the text order is used as a common heuristic, based on the assumption that the sentences are arranged coherently in the source document, locally and globally. 3 Experiments and Preliminary Results Currently, we have evaluated grouping-based ordering on single-document summarization, for which text order is usually considered sufficient. But there is no theoretical proof that it leads to optimal global and local coherence that concerns us. On some occasions, e.g., a news article adopting the “Wall Street Journal Formula” (Rich and Harper, 2007) where conceptually related sentences are placed at the beginning and the end, sentence conceptual relatedness does not necessarily correlate with spatial proximity and thus selected sentences may need to be rearranged for better readability. We are not aware of any published work that has empirically compared alternative ways of sentence ordering for singledocument summarization. The experimental results reported below may draw some attention to this taken-for-granted issue. 3.1 Data and Method We prepared 3 datasets of 60 documents each, the first (D400) consisting of documents of about 400 words from the Document Understanding Conference (DUC) 01/02 datasets; the second (D1k) consisting of documents of about 1000 words manually selected from popular English journals such as The Wall Street Journal, The Washington Post, etc; the third (D2k) consisting of documents of about 2000 words from the DUC 01/02 dataset. Then we generated 100-word summaries for D400 and 200-word summaries for D1k and D2k. Since sentence selection is not our focus, the 180 summaries were all extracts produced by a simple but robust summarizer built on term frequency and sentence position (Aone et al., 1999). Three human annotators were employed to each provide reference orderings for the 180 summaries and mark paragraph (of at least 2 sentences) boundaries, which will be used by one of the evaluation metrics described below. In our implementation of the grouping-based ordering, sentences are represented as entity vectors and the threshold t = Avg( Sim( Sm, Sn))  c , the average sentence similarity in a group multiplied by a coefficient empirically decided on separate held-out datasets of 20 documents for each length category. The “group-representing sentence” is the textually earliest sentence in the group. We experimented with both CC and MKM to generate sentence groups and all the proposed algorithms in 2.3 for group-level and sentence- level orderings, resulting in 8 combinations as test orderings, each coded in the format of “Grouping (CC/MKM) / Group ordering (T/G) / Sentence ordering (T/G)”, where T and G represent the text order approach and the greedy selection approach respectively. For example, “CC/T/G” means grouping with CC, group ordering with text order, and sentence ordering with the greedy approach. We evaluated the test orderings against the 3 reference orderings and compute the average (Madnani et al., 2007) by using 3 different metrics. The first metric is Kendall’s τ (Lapata 2003, 2006), which has been reliably used in ordering evaluations (Bollegala et al., 2006; Madnani et al., 2007). It measures ordering differences in terms of the number of adjacent sentence inversions necessary to convert a test ordering to the reference ordering.   1 4m N( N 1) In this formula, m represents the number of inversions described above and N is the total number of sentences. The second metric is the Average Continuity (AC) proposed by Bollegala et al. (2006), which captures the intuition that the quality of sentence orderings can be estimated by the number of correctly arranged continuous sentences. 9 ACexp(1/(k1) klog(Pn)) n2 In this formula, k is the maximum number of continuous sentences, α is a small value in case Pn = 1. Pn, the proportion of continuous sentences of length n in an ordering, is defined as m/(N – n + 1) where m is the number of continuous sentences of length n in both the test and reference orderings and N is the total number of sentences. Following (Bollegala et al., 2006), we set k = Min(4, N) and α = 0.01. We also go a step further by considering only the continuous sentences in a paragraph marked by human annotators, because paragraphs are local meaning units perceived by human readers and the order of continuous sentences in a paragraph is more strongly grounded than the order of continuous sentences across paragraph boundaries. So in-paragraph sentence continuity is a better estimation for the quality of sentence orderings. This is our third metric: Paragraph-level Average Continuity (P-AC). P-ACexp(1/(k1) klog(PPn )) n2 Here PPn = m ’/(N – n + 1), where m ’ is the number of continuous sentences of length n in both the test ordering and a paragraph of the reference ordering. All the other parameters are as defined in AC and Pn. 3.2 Results The following tables show the results measured by each metric. For comparison, we also include a “Baseline” that uses the text order. For each dataset, two-tailed t-test is conducted between the top scorer and all the other orderings and statistical significance (p < 0.05) is marked with *. In general, our grouping-based ordering scheme outperforms the baseline for news articles of various lengths and statistically significant improvement can be observed on each dataset. This result casts serious doubt on the widely accepted practice of taking the text order for single-document summary generation, which is a major finding from our study. The three evaluation metrics give consistent results although they are based on different observations. The P-AC scores are much lower than their AC counterparts because of its strict paragraph constraint. Interestingly, applying the text order posterior to sentence grouping for group-level and sentence- level ordering leads to consistently optimal performance, as the top scorers on each dataset are almost all “__/T/T”. This suggests that the textual realization of coherence can be sought in the source document if possible, after the selected sentences are rearranged. It is in this sense that the general intuition about the text order is justified. It also suggests that tightly knit paragraphs (groups), where the sentences are closely connected, play a crucial role in creating a coherence flow. Shuffling those paragraphs may not affect the final coherence1. 1 Ithank an anonymous reviewer for pointing this out. 10 The grouping method does make a difference. While CC works best for the short and long datasets (D400 and D2k), MKM is more effective for the medium-sized dataset D1k. Whether the difference is simply due to length or linguistic/stylistic subtleties is an interesting topic for in-depth study. 4 Conclusion and Future Work We have established a grouping-based ordering scheme to accommodate both local and global coherence for summary generation. Experiments on single-document summaries validate our approach and challenge the well accepted text order by the summarization community. Nonetheless, the results do not necessarily propagate to multi-document summarization, for which the same-document clue for ordering cannot apply directly. Adapting the proposed scheme to multi-document summary generation is the ongoing work we are engaged in. In the next step, we will experiment with alternative sentence representations and ordering algorithms to achieve better performance. We are also considering adapting more sophisticated coherence-oriented models, such as (Soricut and Marcu, 2006; Elsner et al., 2007), to our problem so as to make more interesting comparisons possible. Acknowledgements The reported work was inspired by many talks with my supervisor, Dr. Wenjie Li, who saw through this work down to every writing detail. The author is also grateful to many people for assistance. You Ouyang shared part of his summarization work and helped with the DUC data. Dr. Li Shen, Dr. Naishi Liu, and three participants helped with the experiments. I thank them all. The work described in this paper was partially supported by Hong Kong RGC Projects (No. PolyU 5217/07E). References Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. 1999. A Trainable Summarizer with Knowledge Acquired from Robust NLP Techniques. In I. Mani and M. T. Maybury (eds.), Advances in Automatic Text Summarization. 71–80. Cambridge, Massachusetts: MIT Press. Barzilay, R., Elhadad, N., and McKeown, K. 2002. Inferring Strategies for Sentence Ordering in Multidocument News Summarization. Journal of Artificial Intelligence Research, 17: 35–55. Barzilay, R. and Lapata, M. 2005. Modeling Local Coherence: An Entity-based Approach. In Proceedings of the 43rd Annual Meeting of the ACL, 141–148. Ann Arbor. Barzilay, R. and Lapata, M. 2008. Modeling Local Coherence: An Entity-Based Approach. Computational Linguistics, 34: 1–34. Barzilay, R. and Lee L. 2004. Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization. In HLT-NAACL 2004: Proceedings of the Main Conference. 113–120. Bollegala, D, Okazaki, N., and Ishizuka, M. 2006. A Bottom-up Approach to Sentence Ordering for Multidocument Summarization. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, 385–392. Sydney. Elsner, M., Austerweil, j. & Charniak E. 2007. “A Unified Local and Global Model for Discourse Coherence”. In Proceedings of NAACL HLT 2007, 436-443. Rochester, NY. Grosz, B. J., Aravind K. J., and Scott W. 1995. Centering: A framework for Modeling the Local Coherence of Discourse. Computational Linguistics, 21(2):203–225. Halliday, M. A. K., and Hasan, R. 1976. Cohesion in English. London: Longman. Hovy, E. 2005. Automated Text Summarization. In R. Mitkov (ed.), The Oxford Handbook of Computational Linguistics, pp. 583–598. Oxford: Oxford University Press. Jing, H. 2000. Sentence Reduction for Automatic Text Summarization. In Proceedings of the 6th Applied Natural Language Processing WA, pp. 310–315. Conference, Seattle, Jing, H., and McKeown, K. 2000. Cut and Paste Based Text Summarization. In Proceedings of the 1st NAACL, 178–185. 11 Kehler, A. 2002. Coherence, Reference, and the Theory of Grammar. Stanford, California: CSLI Publications. Lapata, M. 2003. Probabilistic Text Structuring: Experiments with Sentence Ordering. In Proceedings of the Annual Meeting of ACL, 545–552. Sapporo, Japan. Lapata, M. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):1–14. Madnani, N., Passonneau, R., Ayan, N. F., Conroy, J. M., Dorr, B. J., Klavans, J. L., O’leary, D. P., and Schlesinger, J. D. 2007. Measuring Variability in Sentence Ordering for News Summarization. In Proceedings of the Eleventh European Workshop on Natural Language Generation, 81–88. Germany. Mann, W. C. and Thompson, S. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8:243–281. Rich C., and Harper, C. 2007. Writing and Reporting News: A Coaching Method, Fifth Edition. Thomason Learning, Inc. Belmont, CA. Soricut, R. and Marcu D. 2006. Discourse Generation Using Utility-Trained Coherence Models. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, 803–810. Steibach, M., Karypis, G., and Kumar V. 2000. A Comparison of Document Clustering Techniques. Technical Report 00-034. Department of Computer Science and Engineering, University of Minnesota. Tapiero, I. 2007. Situation Models and Levels of Coherence: Towards a Definition of Comprehension. Mahwah, New Jersey: Lawrence Erlbaum Associates. Wilpon, J. G. and Rabiner, L. R. 1985. A Modified Kmeans Clustering Algorithm for Use in Isolated Word Recognition. In IEEE Trans. Acoustics, Speech, Signal Proc. ASSP-33(3), 587–594. Zhang R., Li, W., and Lu, Q. 2010. Sentence Ordering with Event-Enriched Semantics and Two-Layered Clustering for Multi-Document News Summarization. In COLING 2010: Poster Volume, 1489–1497, Beijing.</p><p>3 0.067098789 <a title="239-tfidf-3" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>Author: Jeffrey Heinz ; Chetan Rawal ; Herbert G. Tanner</p><p>Abstract: Beginning with Goldsmith (1976), the phonological tier has a long history in phonological theory to describe non-local phenomena. This paper defines a class of formal languages, the Tier-based Strictly Local languages, which begin to describe such phenomena. Then this class is located within the Subregular Hierarchy (McNaughton and Papert, 1971). It is found that these languages contain the Strictly Local languages, are star-free, are incomparable with other known sub-star-free classes, and have other interesting properties.</p><p>4 0.065010801 <a title="239-tfidf-4" href="./acl-2011-Semi-Supervised_SimHash_for_Efficient_Document_Similarity_Search.html">276 acl-2011-Semi-Supervised SimHash for Efficient Document Similarity Search</a></p>
<p>Author: Qixia Jiang ; Maosong Sun</p><p>Abstract: Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance. 1</p><p>5 0.03600838 <a title="239-tfidf-5" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>Author: Manaal Faruqui ; Sebastian Pado</p><p>Abstract: In contrast to many languages (like Russian or French), modern English does not distinguish formal and informal (“T/V”) address overtly, for example by pronoun choice. We describe an ongoing study which investigates to what degree the T/V distinction is recoverable in English text, and with what textual features it correlates. Our findings are: (a) human raters can label English utterances as T or V fairly well, given sufficient context; (b), lexical cues can predict T/V almost at human level.</p><p>6 0.030680604 <a title="239-tfidf-6" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>7 0.027093526 <a title="239-tfidf-7" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>8 0.026701896 <a title="239-tfidf-8" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>9 0.026608925 <a title="239-tfidf-9" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>10 0.026091689 <a title="239-tfidf-10" href="./acl-2011-Grammatical_Error_Correction_with_Alternating_Structure_Optimization.html">147 acl-2011-Grammatical Error Correction with Alternating Structure Optimization</a></p>
<p>11 0.025574742 <a title="239-tfidf-11" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>12 0.024772482 <a title="239-tfidf-12" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>13 0.024408577 <a title="239-tfidf-13" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>14 0.023840621 <a title="239-tfidf-14" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>15 0.023611842 <a title="239-tfidf-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.022839075 <a title="239-tfidf-16" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>17 0.021978861 <a title="239-tfidf-17" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>18 0.021094119 <a title="239-tfidf-18" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>19 0.021001784 <a title="239-tfidf-19" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>20 0.020746863 <a title="239-tfidf-20" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.061), (1, -0.007), (2, -0.022), (3, -0.013), (4, -0.012), (5, -0.005), (6, -0.008), (7, -0.004), (8, -0.025), (9, -0.0), (10, -0.006), (11, -0.002), (12, -0.008), (13, 0.03), (14, -0.02), (15, 0.019), (16, -0.018), (17, 0.026), (18, 0.004), (19, -0.01), (20, 0.007), (21, 0.02), (22, -0.002), (23, -0.001), (24, -0.029), (25, -0.015), (26, -0.017), (27, 0.037), (28, 0.016), (29, -0.014), (30, 0.017), (31, 0.036), (32, 0.032), (33, 0.024), (34, -0.018), (35, 0.038), (36, -0.011), (37, -0.029), (38, -0.059), (39, -0.033), (40, -0.053), (41, -0.024), (42, -0.004), (43, -0.011), (44, -0.01), (45, 0.031), (46, -0.016), (47, -0.0), (48, 0.044), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85793471 <a title="239-lsi-1" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>2 0.63751084 <a title="239-lsi-2" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>Author: Jeffrey Heinz ; Chetan Rawal ; Herbert G. Tanner</p><p>Abstract: Beginning with Goldsmith (1976), the phonological tier has a long history in phonological theory to describe non-local phenomena. This paper defines a class of formal languages, the Tier-based Strictly Local languages, which begin to describe such phenomena. Then this class is located within the Subregular Hierarchy (McNaughton and Papert, 1971). It is found that these languages contain the Strictly Local languages, are star-free, are incomparable with other known sub-star-free classes, and have other interesting properties.</p><p>3 0.52158993 <a title="239-lsi-3" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>Author: Antske Fokkens</p><p>Abstract: When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. Iargue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.</p><p>4 0.49587622 <a title="239-lsi-4" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>Author: Roger Levy</p><p>Abstract: A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</p><p>5 0.47679618 <a title="239-lsi-5" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG.</p><p>6 0.45052472 <a title="239-lsi-6" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>7 0.43834248 <a title="239-lsi-7" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<p>8 0.43749028 <a title="239-lsi-8" href="./acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting.html">113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</a></p>
<p>9 0.42441577 <a title="239-lsi-9" href="./acl-2011-Underspecifying_and_Predicting_Voice_for_Surface_Realisation_Ranking.html">317 acl-2011-Underspecifying and Predicting Voice for Surface Realisation Ranking</a></p>
<p>10 0.42040887 <a title="239-lsi-10" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>11 0.41959971 <a title="239-lsi-11" href="./acl-2011-Semi-Supervised_SimHash_for_Efficient_Document_Similarity_Search.html">276 acl-2011-Semi-Supervised SimHash for Efficient Document Similarity Search</a></p>
<p>12 0.41420794 <a title="239-lsi-12" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>13 0.40935528 <a title="239-lsi-13" href="./acl-2011-Semi-Supervised_Modeling_for_Prenominal_Modifier_Ordering.html">275 acl-2011-Semi-Supervised Modeling for Prenominal Modifier Ordering</a></p>
<p>14 0.39987189 <a title="239-lsi-14" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>15 0.39643136 <a title="239-lsi-15" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>16 0.39634821 <a title="239-lsi-16" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>17 0.39568985 <a title="239-lsi-17" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>18 0.39236814 <a title="239-lsi-18" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>19 0.38883746 <a title="239-lsi-19" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>20 0.38793847 <a title="239-lsi-20" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (17, 0.033), (26, 0.017), (31, 0.012), (37, 0.049), (39, 0.032), (41, 0.045), (50, 0.01), (55, 0.019), (59, 0.026), (72, 0.031), (75, 0.039), (87, 0.394), (91, 0.056), (96, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80555284 <a title="239-lda-1" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>2 0.5644775 <a title="239-lda-2" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>Author: Nathan Bodenstab ; Kristy Hollingshead ; Brian Roark</p><p>Abstract: We present a novel pruning method for context-free parsing that increases efficiency by disallowing phrase-level unary productions in CKY chart cells spanning a single word. Our work is orthogonal to recent work on “closing” chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned. We show that a simple discriminative classifier can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions. Eliminating these unary productions from the search can have a large impact on downstream processing, depending on implementation details of the search. We apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-fine, agenda, and beam-search pruning.</p><p>3 0.5098207 <a title="239-lda-3" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>Author: Yuening Hu ; Jordan Boyd-Graber ; Brianna Satinoff</p><p>Abstract: Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.</p><p>4 0.33514351 <a title="239-lda-4" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>Author: Chung-chi Huang ; Mei-hua Chen ; Shih-ting Huang ; Jason S. Chang</p><p>Abstract: We introduce a new method for learning to detect grammatical errors in learner’s writing and provide suggestions. The method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words, function words, and parts-of-speech (e.g., “play ~ role in Ving” and “look forward to Ving”). At runtime, the given passage submitted by the learner is matched using an extended Levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions. We present a prototype implementation of the proposed method, EdIt, that can handle a broad range of errors. Promising results are illustrated with three common types of errors in nonnative writing. 1</p><p>5 0.33504748 <a title="239-lda-5" href="./acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting.html">113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</a></p>
<p>Author: Benjamin Van Durme ; Ashwin Lall</p><p>Abstract: We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.</p><p>6 0.33403078 <a title="239-lda-6" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>7 0.33292332 <a title="239-lda-7" href="./acl-2011-The_Arabic_Online_Commentary_Dataset%3A_an_Annotated_Dataset_of_Informal_Arabic_with_High_Dialectal_Content.html">299 acl-2011-The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content</a></p>
<p>8 0.33078268 <a title="239-lda-8" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>9 0.32882172 <a title="239-lda-9" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>10 0.32784003 <a title="239-lda-10" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>11 0.32748431 <a title="239-lda-11" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>12 0.32610589 <a title="239-lda-12" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>13 0.32603395 <a title="239-lda-13" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>14 0.32602006 <a title="239-lda-14" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>15 0.32556084 <a title="239-lda-15" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>16 0.32536316 <a title="239-lda-16" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>17 0.32490543 <a title="239-lda-17" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>18 0.32471031 <a title="239-lda-18" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>19 0.3244504 <a title="239-lda-19" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>20 0.32429853 <a title="239-lda-20" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
