<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-27" href="#">acl2011-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</h1>
<br/><p>Source: <a title="acl-2011-27-pdf" href="http://aclweb.org/anthology//P/P11/P11-1139.pdf">pdf</a></p><p>Author: Weiwei Sun</p><p>Abstract: The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is effi- cient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.</p><p>Reference: <a title="acl-2011-27-reference" href="../acl2011_reference/acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. [sent-3, score-0.66]
</p><p>2 First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. [sent-5, score-0.744]
</p><p>3 Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. [sent-6, score-0.264]
</p><p>4 The coarse-to-fine search scheme is effi-  cient, while in the sub-word tagging step rich contextual features can be approximately derived. [sent-7, score-0.315]
</p><p>5 1 Introduction Word segmentation and part-of-speech (POS) tagging are necessary initial steps for more advanced Chinese language processing tasks, such as parsing and semantic role labeling. [sent-9, score-0.495]
</p><p>6 Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation. [sent-11, score-0.387]
</p><p>7 In this paper, we present an effective and efficient solution for joint Chinese word segmentation and POS tagging. [sent-18, score-0.386]
</p><p>8 First of all, a majority of words are easy to identify in the segmentation problem. [sent-20, score-0.286]
</p><p>9 Second, segmenters designed with different views have complementary strength. [sent-23, score-0.186]
</p><p>10 We argue that the agreements and disagreements of different solvers can be used to construct an intermediate sub-word structure for joint segmentation and tagging. [sent-24, score-0.482]
</p><p>11 Since the sub-words are large enough in practice, the decoding for POS tagging over subwords is efficient. [sent-25, score-0.33]
</p><p>12 (2007)  showed the effectiveness of utilizing syntactic information to rerank POS tagging results. [sent-28, score-0.209]
</p><p>13 In this work, we use a representation-efficiency tradeoff through stacked learning, a way of approximating rich non-local feaProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-30, score-0.587]
</p><p>14 Based on the sub-word structure, joint word segmentation and POS tagging is addressed as a two step process. [sent-35, score-0.595]
</p><p>15 In the first step, one word-based segmenter, one character-based segmenter and one local character classifier are used to produce coarse segmentation and POS information. [sent-36, score-0.744]
</p><p>16 The results of the three predictors are then merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. [sent-37, score-0.232]
</p><p>17 In the sub-word tagging phase, the fine-grained tagger mainly considers its POS tag prediction problem. [sent-39, score-0.403]
</p><p>18 For the words that are not consistently predicted, the fine-grained tagger will also consider their bracketing problem. [sent-40, score-0.17]
</p><p>19 Furthermore, in the sub-word tagging step, word features in a large window can be approximately derived from the coarse segmentation and tagging results. [sent-42, score-0.902]
</p><p>20 To train a good sub-word tagger, we use the stacked learning technique, which can effectively correct the training/test mismatch problem. [sent-43, score-0.541]
</p><p>21 17 for the word segmentation task and an f-score of 94. [sent-47, score-0.317]
</p><p>22 , c#c), the task of word segmentation and POS tagging is 1386 to predict a sequence of word and POS tag pairs y = (hw1 ,p1i , hw#y, p#yi), where wi is a word, pi iys i=ts P(hOwS tag, a,hnwd a “#” symbol deereno wtes the number of elements in each variable. [sent-60, score-0.624]
</p><p>23 2  Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. [sent-65, score-0.386]
</p><p>24 In  this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. [sent-67, score-0.213]
</p><p>25 Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. [sent-71, score-0.479]
</p><p>26 This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). [sent-72, score-0.495]
</p><p>27 This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. [sent-75, score-0.262]
</p><p>28 In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece  of continuous characters is a word token and which class it belongs to. [sent-76, score-0.213]
</p><p>29 This word-by-word method for segmentation was first proposed in (Zhang and Clark, 2007), and was then further used in POS tagging in (Zhang and Clark, 2008). [sent-79, score-0.495]
</p><p>30 We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. [sent-81, score-0.316]
</p><p>31 A system combination method that leverages the complementary strength ofword-based and character-based segmentation models was also successfully explored in their work. [sent-82, score-0.322]
</p><p>32 In the machine learning research, stacked learning has been applied to structured prediction (Cohen and 1387 Carvalho, 2005). [sent-121, score-0.608]
</p><p>33 In this work, stacked learning is used to acquire extended training data for sub-word tagging. [sent-122, score-0.541]
</p><p>34 1 Architecture In our stacked sub-word model, joint word segmentation and POS tagging is decomposed into two steps: (1) coarse-grained word segmentation and tagging, and (2) fine-grained sub-word tagging. [sent-124, score-1.453]
</p><p>35 In the first phase, one word-based segmenter (SegW) and one characterbased segmenter (SegC) are trained to produce word boundaries. [sent-126, score-0.353]
</p><p>36 Additionally, a local character-based joint segmentation and tagging solver (SegTagL) is used to provide word boundaries as well as inaccurate POS information. [sent-127, score-0.79]
</p><p>37 Here, the word local means  the labels of nearby characters are not used as features. [sent-128, score-0.185]
</p><p>38 In other words, the local character classifier assumes that the tags of characters are independent of each other. [sent-129, score-0.365]
</p><p>39 In the second phase, our system first combines the three segmentation and tagging results to get sub-words which maximize the agreement about word boundaries. [sent-130, score-0.526]
</p><p>40 Finally, a fine-grained sub-word tagger (SubTag) is applied to bracket subwords into words and also to obtain their POS tags. [sent-131, score-0.18]
</p><p>41 SegWmoerdn-tberasSedgWCseRghawmraecnstenrt-ebnSacesg dCLocSalegscThiafgreaLcter  Figure 1: Workflow of the stacked sub-word model. [sent-132, score-0.541]
</p><p>42 In our model, segmentation and POS tagging interact with each other in two processes. [sent-133, score-0.495]
</p><p>43 Therefore, in the subword generating stage, segmentation and POS tagging help each other. [sent-135, score-0.572]
</p><p>44 Second, in the sub-word tagging stage, the bracketing and the classification of sub-words are jointly resolved as one sequence labeling problem. [sent-136, score-0.318]
</p><p>45 Our experiments on the Penn Chinese Treebank will show that the word-based and character-based segmenters and the local tagger on their own produce high quality word boundaries. [sent-137, score-0.351]
</p><p>46 If a high performance sub-word tagger can be constructed, the whole task can be well resolved. [sent-140, score-0.173]
</p><p>47 As a result, the search space of the sub-word tagging is significantly  shrunken, and exact Viterbi decoding without approximately pruning can be efficiently processed. [sent-142, score-0.277]
</p><p>48 (2006) described a sub-word based tagging model to resolve word segmentation. [sent-145, score-0.288]
</p><p>49 To get the pieces which are larger than characters but smaller than words, they combine a character-based segmenter and a dictionary matching segmenter. [sent-146, score-0.272]
</p><p>50 2 The Coarse-grained Solvers We systematically described the implementation of two state-of-the-art Chinese word segmenters in word-based and character-based architectures, respectively (Sun, 2010). [sent-149, score-0.181]
</p><p>51 Our word-based segmenter is based on a discriminative joint model with a first order semi-Markov structure, and the other segmenter is based on a first order Markov model. [sent-150, score-0.391]
</p><p>52 Statistical segmentation models are then performed on these smaller character sequences. [sent-163, score-0.419]
</p><p>53 Each character can be assigned one of two possible boundary tags: “B” for a character that begins a word and “I” for a character that occurs in the middle of a word. [sent-165, score-0.462]
</p><p>54 We denote a candidate character token ci with a fixed window ci−2ci−1cici+1ci+2. [sent-166, score-0.218]
</p><p>55 The following features are used: •  character uni-grams:  •  character bi-grams:  ck  (i − 2  ckck+1  (i  ≤ k ≤ i+ 2) 2 ≤ k ≤ i+ 1)  −  To resolve the classification problem, we use the linear SVM classifier LIBLINEAR2. [sent-167, score-0.382]
</p><p>56 3  Merging Multiple Segmentation Results into Sub-Word Sequences A majority of words are easy to identify in the segmentation problem. [sent-169, score-0.286]
</p><p>57 If a piece of continuous characters is consistently segmented by multiple segmenters, it will  1This resource is publicly available at http : / /www . [sent-173, score-0.172]
</p><p>58 In particular, if the position between two continuous characters is predicted as a word boundary by any segmenter, this position is taken as a separation position of the sub-word  sequence. [sent-192, score-0.232]
</p><p>59 This strategy  makes sure that it is still possible to re-segment the strings of which the boundaries are disagreed with by the coarse-grained segmenters in the fine-grained tagging stage. [sent-193, score-0.396]
</p><p>60 , c#c), let c[i : j] denote a string that is made up of characters between ci and cj (including ci and cj), then a partition of the sentence can be written as c[0 : e1] , c[e1 + 1 : e2] , . [sent-198, score-0.22]
</p><p>61 Let sk = {c[i : j] } denote the set of all segments of a partition. [sent-202, score-0.172]
</p><p>62 :G ji]v}en d multiple partitions of a character sequence S = {sk}, there pisa one oannsd only one merged partition sS = {c[i : j] } s. [sent-203, score-0.304]
</p><p>63 The first condition makes sure that all segments in the merged partition can be only embedded in but do not overlap with any segment of any partition from 1389 S. [sent-209, score-0.205]
</p><p>64 For the character sequence “ ３ ５ ５ ３ ５ 分居”, the predictions are very different. [sent-216, score-0.195]
</p><p>65 Because there are no word break predictions among the first three characters “ ３ ５ ５ ”, it is as a whole taken as one sub-word. [sent-217, score-0.215]
</p><p>66 The sub-word level allows our system to utilize features in a large context, which is very important for POS tagging of the morphologically poor language. [sent-225, score-0.24]
</p><p>67 A first order Max-Margin Markov Networks model is used to resolve the sequence tagging problem. [sent-242, score-0.292]
</p><p>68 edu/  l l  1390  Algorithm 1: The stacked learning procedure for the sub-word tagger. [sent-250, score-0.541]
</p><p>69 Fs porli te ianchto s uLb seeqtu Sl, the complementary set S − Sl is used to train three coarse sopllevemrse StaergyW sel,t S Se −gCl S and SegTagLl, which process the Sl and provide inaccurate predictions. [sent-275, score-0.193]
</p><p>70 Then the inaccurate predictions are merged into subword sequences and Sl is extended to Sl0. [sent-276, score-0.305]
</p><p>71 Finally, the sub-word tagger is trained on the whole extended data set S0. [sent-277, score-0.173]
</p><p>72 1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the Penn Chinese Treebank (CTB) in experiments. [sent-279, score-0.595]
</p><p>73 Even only locally trained, the character classifier SegTagL still significantly outperforms the two state-of-the-art segmenters SegW and SegC. [sent-298, score-0.32]
</p><p>74 3 Statistics of Sub-Words Since the base predictors to generate coarse information are two word segmenters and a local character classifier, the coarse decoding is efficient. [sent-315, score-0.662]
</p><p>75 the decoding path for sub-word sequences are too long, the decoding of the fine-grained stage is still hard. [sent-318, score-0.171]
</p><p>76 The number of all IOB-style POS tags is 59 (when using 5-fold cross-validation to generate stacked training samples). [sent-323, score-0.582]
</p><p>77 The oracle performance of the final POS tagging on the development data set is shown in Table 4. [sent-331, score-0.209]
</p><p>78 29  Table 4: Upper bound of the sub-word tagging on the development data. [sent-338, score-0.209]
</p><p>79 4 Rich Contextual Features Are Useful Table 5 shows the effect that features within different window size has on the sub-word tagging task. [sent-342, score-0.292]
</p><p>80 65  C:±3T:±2  Table 5: Performance of the stacked sub-word model (K = 5) with features in different window sizes. [sent-372, score-0.624]
</p><p>81 nd T fhui-s ture features are crucial to the Chinese POS tagging problem. [sent-378, score-0.24]
</p><p>82 We think there are two main reasons: (1) The POS information provided by the local classifier is inaccurate; (2) The structured learning of the sub-word tagger can use real predicted sub-word labels during its decoding time, since this learning algorithm does inference during the training time. [sent-386, score-0.336]
</p><p>83 We can see that although it is still possible to improve the segmentation and POS tagging performance compared to the local character classifier, the whole task just benefits only a little from the sub-word tagging procedure if the stacking technique is not applied. [sent-391, score-1.032]
</p><p>84 The stacking  technique can significantly improve the system performance, both for segmentation and POS tagging. [sent-392, score-0.392]
</p><p>85 This experiment confirms the theoretical motivation of using stacked learning: simulating the test-time setting when a sub-word tagger is applied to a new instance. [sent-393, score-0.698]
</p><p>86 The comparison of the accuracy between our stacked sub-word system and the state-of-the-art systems in the literature indicates that our method is competitive with the best systems. [sent-420, score-0.541]
</p><p>87 Our system obtains the highest f-score performance on both segmentation and the whole task, resulting in error reductions of 14. [sent-421, score-0.332]
</p><p>88 5  Conclusion and Future Work  This paper has described a stacked sub-word model  for joint Chinese word segmentation and POS tagging. [sent-438, score-0.927]
</p><p>89 Moreover, the POS tagging could be efficiently and effectively resolved over sub-word sequences. [sent-441, score-0.24]
</p><p>90 To train a good sub-word tagger, we introduced a stacked learning procedure. [sent-442, score-0.541]
</p><p>91 Although the stacked sub-word model is an ad hoc solution for a particular problem, namely joint word  segmentation and POS tagging, the idea to employ system ensemble and stacked learning in general provides an alternative for structured problems. [sent-448, score-1.5]
</p><p>92 These outputs are further merged into an intermediate representation, which allows an extractive system to use rich contexts to predict the final results. [sent-450, score-0.171]
</p><p>93 A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. [sent-480, score-0.386]
</p><p>94 Word lattice reranking for Chinese word segmentation and  part-of-speech tagging. [sent-485, score-0.344]
</p><p>95 An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging. [sent-490, score-0.612]
</p><p>96 Word-based and character-based word segmentation models: Comparison and combination. [sent-519, score-0.317]
</p><p>97 A stacked, voted, stacked model for named entity recognition. [sent-537, score-0.541]
</p><p>98 Joint word segmentation and POS tagging using a single perceptron. [sent-554, score-0.526]
</p><p>99 A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. [sent-559, score-0.386]
</p><p>100 Subword-based tagging by conditional random fields for Chinese word segmentation. [sent-564, score-0.24]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stacked', 0.541), ('segmentation', 0.286), ('pos', 0.226), ('tagging', 0.209), ('segmenter', 0.161), ('segc', 0.153), ('segtagl', 0.153), ('segw', 0.153), ('segmenters', 0.15), ('sk', 0.146), ('character', 0.133), ('solvers', 0.127), ('tagger', 0.127), ('chinese', 0.124), ('lc', 0.119), ('characters', 0.111), ('stacking', 0.106), ('lt', 0.096), ('merged', 0.093), ('coarse', 0.084), ('subword', 0.077), ('sl', 0.074), ('inaccurate', 0.073), ('vv', 0.071), ('cc', 0.07), ('joint', 0.069), ('jiang', 0.069), ('predictors', 0.069), ('decoding', 0.068), ('gk', 0.061), ('idioms', 0.058), ('zhang', 0.054), ('kruengkrai', 0.053), ('subwords', 0.053), ('window', 0.052), ('clark', 0.051), ('si', 0.05), ('tt', 0.05), ('resolve', 0.048), ('whole', 0.046), ('rich', 0.046), ('subtag', 0.044), ('weiwei', 0.044), ('wsun', 0.044), ('nn', 0.044), ('partition', 0.043), ('bracketing', 0.043), ('local', 0.043), ('solver', 0.042), ('cd', 0.042), ('efficiency', 0.042), ('ctb', 0.041), ('tags', 0.041), ('xt', 0.039), ('torres', 0.039), ('penn', 0.038), ('predictor', 0.038), ('yt', 0.038), ('jj', 0.038), ('boundaries', 0.037), ('classifier', 0.037), ('segmentations', 0.037), ('nt', 0.037), ('complementary', 0.036), ('coarsegrained', 0.036), ('sequence', 0.035), ('yue', 0.035), ('sequences', 0.035), ('prediction', 0.035), ('treebank', 0.034), ('xue', 0.034), ('oftheassociation', 0.033), ('ci', 0.033), ('structured', 0.032), ('boundary', 0.032), ('segmented', 0.032), ('outputs', 0.032), ('gl', 0.032), ('pipelined', 0.032), ('tag', 0.032), ('resolved', 0.031), ('rd', 0.031), ('features', 0.031), ('word', 0.031), ('workflow', 0.03), ('theoretical', 0.03), ('continuous', 0.029), ('predicted', 0.029), ('bracketed', 0.029), ('wenbin', 0.029), ('formulated', 0.029), ('contextual', 0.029), ('nianwen', 0.028), ('disadvantage', 0.027), ('positional', 0.027), ('reranking', 0.027), ('ts', 0.027), ('predictions', 0.027), ('segments', 0.026), ('ohio', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="27-tfidf-1" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is effi- cient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.</p><p>2 0.2160296 <a title="27-tfidf-2" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>3 0.17617945 <a title="27-tfidf-3" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>4 0.15580805 <a title="27-tfidf-4" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>5 0.15478019 <a title="27-tfidf-5" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: We present a pointwise approach to Japanese morphological analysis (MA) that ignores structure information during learning and tagging. Despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for Japanese MA, and achieves accuracy similar to that of structured predictors using the same feature set. We also find that the method is both robust to outof-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning.</p><p>6 0.15449174 <a title="27-tfidf-6" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>7 0.1381889 <a title="27-tfidf-7" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>8 0.1190875 <a title="27-tfidf-8" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>9 0.11364617 <a title="27-tfidf-9" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>10 0.097624213 <a title="27-tfidf-10" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>11 0.0939904 <a title="27-tfidf-11" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>12 0.093801498 <a title="27-tfidf-12" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>13 0.092231102 <a title="27-tfidf-13" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>14 0.089722872 <a title="27-tfidf-14" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>15 0.089441255 <a title="27-tfidf-15" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>16 0.084770538 <a title="27-tfidf-16" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>17 0.084604666 <a title="27-tfidf-17" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>18 0.082237706 <a title="27-tfidf-18" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>19 0.073596619 <a title="27-tfidf-19" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>20 0.070540182 <a title="27-tfidf-20" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, -0.042), (2, -0.005), (3, -0.086), (4, -0.07), (5, -0.031), (6, 0.07), (7, -0.017), (8, 0.058), (9, 0.141), (10, 0.005), (11, 0.096), (12, -0.083), (13, -0.007), (14, -0.008), (15, 0.018), (16, 0.01), (17, -0.083), (18, 0.215), (19, 0.257), (20, 0.101), (21, -0.027), (22, -0.069), (23, 0.036), (24, 0.042), (25, 0.006), (26, -0.032), (27, 0.074), (28, 0.039), (29, 0.071), (30, 0.027), (31, 0.057), (32, 0.017), (33, 0.015), (34, -0.047), (35, -0.018), (36, 0.009), (37, -0.01), (38, -0.066), (39, -0.0), (40, 0.129), (41, 0.088), (42, -0.106), (43, 0.006), (44, 0.05), (45, -0.032), (46, -0.014), (47, 0.015), (48, -0.121), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95202768 <a title="27-lsi-1" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is effi- cient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.</p><p>2 0.83122313 <a title="27-lsi-2" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>3 0.82119989 <a title="27-lsi-3" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>4 0.75694877 <a title="27-lsi-4" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>5 0.67500037 <a title="27-lsi-5" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: We present a pointwise approach to Japanese morphological analysis (MA) that ignores structure information during learning and tagging. Despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for Japanese MA, and achieves accuracy similar to that of structured predictors using the same feature set. We also find that the method is both robust to outof-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning.</p><p>6 0.61049438 <a title="27-lsi-6" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>7 0.60346502 <a title="27-lsi-7" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>8 0.58396226 <a title="27-lsi-8" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>9 0.57737821 <a title="27-lsi-9" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>10 0.54490238 <a title="27-lsi-10" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>11 0.54449493 <a title="27-lsi-11" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>12 0.54086739 <a title="27-lsi-12" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>13 0.51319003 <a title="27-lsi-13" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>14 0.50065613 <a title="27-lsi-14" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>15 0.43905526 <a title="27-lsi-15" href="./acl-2011-Semi-supervised_condensed_nearest_neighbor_for_part-of-speech_tagging.html">278 acl-2011-Semi-supervised condensed nearest neighbor for part-of-speech tagging</a></p>
<p>16 0.43362856 <a title="27-lsi-16" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>17 0.42943582 <a title="27-lsi-17" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>18 0.41467786 <a title="27-lsi-18" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>19 0.41221127 <a title="27-lsi-19" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>20 0.41185015 <a title="27-lsi-20" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (17, 0.048), (26, 0.015), (37, 0.078), (39, 0.411), (41, 0.044), (55, 0.036), (59, 0.033), (72, 0.029), (91, 0.049), (96, 0.149)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9714855 <a title="27-lda-1" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>Author: Yoav Goldberg ; Michael Elhadad</p><p>Abstract: We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.</p><p>2 0.95323795 <a title="27-lda-2" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>Author: Jey Han Lau ; Karl Grieser ; David Newman ; Timothy Baldwin</p><p>Abstract: We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.</p><p>3 0.94909501 <a title="27-lda-3" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>Author: Muhua Zhu ; Jingbo Zhu ; Minghan Hu</p><p>Abstract: For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline.</p><p>4 0.91495585 <a title="27-lda-4" href="./acl-2011-%2811-06-spirl%29.html">1 acl-2011-(11-06-spirl)</a></p>
<p>Author: (hal)</p><p>Abstract: unkown-abstract</p><p>5 0.90920699 <a title="27-lda-5" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>Author: Jacob Eisenstein ; Noah A. Smith ; Eric P. Xing</p><p>Abstract: We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</p><p>same-paper 6 0.86844945 <a title="27-lda-6" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>7 0.84982121 <a title="27-lda-7" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>8 0.81588948 <a title="27-lda-8" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>9 0.73609078 <a title="27-lda-9" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>10 0.67702258 <a title="27-lda-10" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>11 0.67129719 <a title="27-lda-11" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>12 0.66669077 <a title="27-lda-12" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>13 0.6666857 <a title="27-lda-13" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>14 0.6637761 <a title="27-lda-14" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>15 0.66348958 <a title="27-lda-15" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>16 0.66219366 <a title="27-lda-16" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>17 0.65999019 <a title="27-lda-17" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>18 0.6574496 <a title="27-lda-18" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>19 0.64959025 <a title="27-lda-19" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>20 0.64702499 <a title="27-lda-20" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
