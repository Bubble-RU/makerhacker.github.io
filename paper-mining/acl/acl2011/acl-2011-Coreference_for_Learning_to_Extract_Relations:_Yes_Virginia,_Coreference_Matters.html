<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-86" href="#">acl2011-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</h1>
<br/><p>Source: <a title="acl-2011-86-pdf" href="http://aclweb.org/anthology//P/P11/P11-2050.pdf">pdf</a></p><p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>Reference: <a title="acl-2011-86-reference" href="../acl2011_reference/acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , Cambridge, MA 02138 The views expressed are those of the author and do not reflect the official policy or position of the Department of Defense or the U. [sent-5, score-0.132]
</p><p>2 Abstract As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. [sent-10, score-1.009]
</p><p>3 Most techniques assume that the examples are based on easily spotted anchors, e. [sent-11, score-0.069]
</p><p>4 Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. [sent-14, score-0.184]
</p><p>5 We explore whether coreference can improve the learning process. [sent-15, score-0.37]
</p><p>6 Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. [sent-18, score-0.14]
</p><p>7 1 Introduction  As an alternative to requiring substantial supervised relation training data (e. [sent-20, score-0.388]
</p><p>8 the ~300k words of detailed, exhaustive annotation in Automatic Content Extraction (ACE) evaluations1) many have explored bootstrapping relation extraction from a few (~20) seed instances of a relation. [sent-22, score-0.757]
</p><p>9 Key to such approaches is a large body of unannotated text that can be iteratively processed as follows: 1. [sent-23, score-0.038]
</p><p>10 Most techniques assume that relation instances, like hasBirthDate(Wolfgang Amadeus Mozart, 1 http://www. [sent-31, score-0.353]
</p><p>11 gov/speech/tests/ace/ 288  texts2  1756), are realized in the corpus as relation with easily spotted anchors like Wolfgang Amadeus Mozart was born in 1756. [sent-33, score-0.504]
</p><p>12 In this paper we explore whether using coreference can improve the learning process. [sent-34, score-0.37]
</p><p>13 That is, if the algorithm considered texts like his birth in 1756 for the above relation, would performance of  the learned patterns be better? [sent-35, score-0.269]
</p><p>14 2  Related Research  There has been much work in relation extraction both in traditional supervised settings and, more recently, in bootstrapped, semi-supervised settings. [sent-36, score-0.421]
</p><p>15 Our work initializes learning with about 20 seed relation instances and uses about 9 million documents of unannotated as a background bootstrapping corpus. [sent-38, score-0.681]
</p><p>16 We use both normalized syntactic structure and surface strings as features. [sent-39, score-0.07]
</p><p>17 Much has been published on learning relation extractors using lots of supervised training, as in ACE, which evaluates system performance in detecting a fixed set of concepts and relations in text. [sent-40, score-0.47]
</p><p>18 text3  2 Throughout we will use relation instance to refer to a fact  (e. [sent-44, score-0.469]
</p><p>19 ORGHasEmployee(Apple, Steve Jobs)), while we will use relation text to refer a particular sentence entailing a relation instance (e. [sent-46, score-0.857]
</p><p>20 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 288–293, Others have explored automatic pattern generation from seed examples. [sent-52, score-0.236]
</p><p>21 Agichtein & Gravano (2000) and Ravichandran & Hovy (2002) reported results for generating surface patterns for relation identification; others have explored similar approaches (e. [sent-53, score-0.681]
</p><p>22 (2009) showed that for macroreading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. [sent-57, score-0.309]
</p><p>23 In all cases, the approaches used surface (word) patterns without coreference. [sent-58, score-0.282]
</p><p>24 Section 3 describes our particular approach to pattern and relation instance scoring and selection. [sent-60, score-0.544]
</p><p>25 , 2008) explores semi-supervised relation learning using the ACE corpus and assuming manual mention markup. [sent-63, score-0.397]
</p><p>26 They measure the accuracy of relation extraction alone, without including  the added challenge of resolving non-specific relation arguments to name references. [sent-64, score-0.914]
</p><p>27 They limit their studies to the small ACE corpora where mention markup is manually encoded. [sent-65, score-0.044]
</p><p>28 Most approaches to automatic pattern generation have focused on precision, e. [sent-66, score-0.075]
</p><p>29 , Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. [sent-68, score-0.469]
</p><p>30 A primary focus on precision allows one to ignore many relation texts that require coreference or long-distance dependencies; one primary goal of our work is to measure system performance in exactly those areas. [sent-72, score-0.862]
</p><p>31 For the majority of entities there will be only a few mentions of that entity in even a large corpus. [sent-74, score-0.093]
</p><p>32 Furthermore, for many information-extraction problems the number documents at runtime will be far less than web-scale. [sent-75, score-0.048]
</p><p>33 3  Approach  Figure 1 depicts our approach for learning patterns  to detect relations. [sent-76, score-0.212]
</p><p>34 At each iteration, the steps are: (1) Given the current relation instances, find possible texts that entail the relation by finding sentenc289 es in the corpus containing all arguments of an instance. [sent-77, score-0.925]
</p><p>35 (2008), induce possible patterns using the context in which the arguments appear. [sent-80, score-0.368]
</p><p>36 Patterns include both surface strings and normalized syntactic Each proposed pattern is applied to the corpus to find a set of hypothesized texts. [sent-81, score-0.145]
</p><p>37 For each pattern, a confidence score is assigned using estimated precision5 and recall. [sent-82, score-0.084]
</p><p>38 The highest confidence patterns are added to the pattern (3) The patterns are applied to the corpus to find additional possible relation instances. [sent-83, score-0.931]
</p><p>39 For each proposed instance, we estimate a score using a Naive Bayes model with the patterns as the features. [sent-84, score-0.212]
</p><p>40 When using coreference, this score is penalized if an instance’s supporting evidence involves lowconfidence coreference links. [sent-85, score-0.37]
</p><p>41 The highest scoring instances are added to the instance set. [sent-86, score-0.285]
</p><p>42 (4) After the desired number of iterations (in these experiments, 20) is complete, a human reviews the  structures. [sent-87, score-0.033]
</p><p>43 6  resulting pattern set and removes those patterns which are clearly incorrect (e. [sent-89, score-0.32]
</p><p>44 7  We ran this system in two versions: –Coref has no access to coreference information, while +Coref (the original system) does. [sent-92, score-0.37]
</p><p>45 Coreference information is provided by BBN’s state-of-the-art information extraction 4 Surface text patterns with wild cards are not proposed until the third iteration. [sent-94, score-0.344]
</p><p>46 5 Estimated recall is the weighted fraction of known instances found. [sent-95, score-0.244]
</p><p>47 Estimated precision is the weighted average of the scores of matched instances; scores for unseen instances are 0. [sent-96, score-0.218]
</p><p>48 6 As more patterns are accepted in a given iteration, we raise the confidence threshold. [sent-97, score-0.294]
</p><p>49 7 This takes about ten minutes per relation, which is less than the time to choose the initial seed instances. [sent-99, score-0.115]
</p><p>50 , 2011; NIST, 2007) in a mode which sacrifices some accuracy for speed (most notably by reducing the parser’s search space). [sent-101, score-0.035]
</p><p>51 +Coref can propose relation instances from text in which the arguments are expressed as either name or non-name mentions. [sent-103, score-0.64]
</p><p>52 When the text of an argument of a proposed instance is a non-name, the system uses coreference to resolve the non-name to a name. [sent-104, score-0.523]
</p><p>53 -Coref can only propose instances based on texts where both arguments are This has several implications: If a text that entails a relation instance expresses one of the arguments as a non-name mention (e. [sent-105, score-0.962]
</p><p>54 ”), -Coref will be unable to learn an instance from that text. [sent-108, score-0.116]
</p><p>55 Even when all arguments are expressed as names, -Coref may need to use more specific, complex patterns to learn the instance (e. [sent-109, score-0.479]
</p><p>56 8  local space of patterns to be a significant advantage of +Coref. [sent-113, score-0.212]
</p><p>57 patterns involving possessives) may also be less likely to be learned by -Coref. [sent-116, score-0.212]
</p><p>58 Finally, +Coref has access to much more training data at the outset because it can find more matching seed instances,9 potentially leading to better and more stable training. [sent-117, score-0.15]
</p><p>59 4  Evaluation Framework  Estimating recall for bootstrapped relation learning is a challenge except for corpora small enough for complete annotation to be feasible, e. [sent-118, score-0.509]
</p><p>60 Yet, with a small corpus, rare relations will be inadequately represented. [sent-122, score-0.122]
</p><p>61 Mitchell, 2009) have not estimated recall, but have measured precision by sampling system output and determining whether the extracted fact is true in the world. [sent-125, score-0.15]
</p><p>62 8 An instance like hasChild(his father, he) would be useful neither during training nor (without coreference) at runtime. [sent-126, score-0.116]
</p><p>63 If multiple mentions expressing an argument occur in one sentence, each match is counted, inflating the difference. [sent-128, score-0.183]
</p><p>64 10 Despite being selected to be rich in the 18 ACE relation  subtypes, the 10 most frequent subtypes account for over 90% of the relations with the 4 most frequent accounting for 62%; the 5 least frequent relation subtypes occur less than 50 times. [sent-129, score-0.941]
</p><p>65 , Here we extend this idea to both precision and recall in a micro-reading context. [sent-134, score-0.19]
</p><p>66 Precision is measured by running the system over the background corpus and randomly sampleing 100 texts that the system believes entail each relation. [sent-135, score-0.142]
</p><p>67 From the mentions matching the argument slots of the patterns, we build a relation instance. [sent-136, score-0.483]
</p><p>68 If these mentions are not names (only possible for +Coref), they are resolved to names using system coreference. [sent-137, score-0.191]
</p><p>69 For example, given the passage in Figure 2 and the pattern ‘(Y, poss:X)’, the system would match the mentions X=her and Y=son, and build the relation instance hasChild(Ethel Kennedy, Robert F. [sent-138, score-0.637]
</p><p>70 During assessment, the annotator is asked whether, in the context of the whole document, a given sentence entails the relation instance. [sent-141, score-0.395]
</p><p>71 We thus treat both incorrect relation extraction and incorrect reference resolution as mistakes. [sent-142, score-0.487]
</p><p>72 To measure recall, we select 20 test relation instances and search the corpus for sentences containing all arguments of a test instance (explicitly or via coreference). [sent-143, score-0.749]
</p><p>73 We randomly sampled from this set, choosing at most 10 sentences for each test instance, to form a collection of at most 200 sentences likely to be texts expressing the desired relation. [sent-144, score-0.217]
</p><p>74 These sentences were then manually annotated in the same manner as the precision annotation. [sent-145, score-0.119]
</p><p>75 Sentences that did not correctly convey the relation instance were removed, and the remaining set of sentences formed a recall set. [sent-146, score-0.614]
</p><p>76 We consider a recall set instance to be found by a system if the system finds a relation of the correct type in the sentence. [sent-147, score-0.577]
</p><p>77 We intentionally chose to sample 10 sentences from each test example, rather than sampling from the set of all sentences found. [sent-148, score-0.074]
</p><p>78 This prevents one or two very commonly expressed instances from dominating the recall set. [sent-149, score-0.288]
</p><p>79 As a result, the recall test set is biased away from “true” recall, because it places a higher weight on  the “long tail” of instances. [sent-150, score-0.108]
</p><p>80 However, this gives a more accurate indication of the system’s ability to find novel instances of a relation. [sent-151, score-0.136]
</p><p>81 name-name pairs far less often than it could (less  Results on precision are mixed. [sent-159, score-0.082]
</p><p>82 While for 4 of the relations +Coref is higher, for the 6 others the  1np 2o t*FCeingmoutirea lfstonw3adm&soead4n; aroegdtnua omiften rctfloeurad etiphnrgesacprieBaslproetinh rDen,va tilheu:waTotsih, esnroef tihsreaiortdn. [sent-160, score-0.087]
</p><p>83 Thus while +Coref pays a price in precision for its improved recall, in many applications it may be a worthwhile tradeoff. [sent-166, score-0.147]
</p><p>84 Though one might expect that errors in coreference would reduce precision of +Coref, such errors may be balanced by the need to use longer patterns in –Coref. [sent-167, score-0.664]
</p><p>85 These patterns often include error-prone wildcards which lead to a drop in precision. [sent-168, score-0.272]
</p><p>86 Patterns with multiple wildcards were also more likely to be removed as unreliable in manual pattern pruning, which may have harmed the recall of –Coref, while improving its precision. [sent-169, score-0.278]
</p><p>87 3  Further Analysis Our analysis thus far has focused on microreading which requires a system find all mentions of an instance relation i,e, in our evaluation OrgLeader(Apple, Steve Jobs) might occur in as many as 20 different contexts. [sent-171, score-0.562]
</p><p>88 finding at least one instance of the relation OrgLeader(Apple, Steve Jobs). [sent-174, score-0.469]
</p><p>89 As a rough measure of this, we also evaluated recall by counting the number of test instances for which at least one answer was found by the two systems. [sent-175, score-0.244]
</p><p>90 With this method, +Coref’s recall is still –  higher for all but one relation type, although the gap between the systems narrows somewhat. [sent-176, score-0.496]
</p><p>91 In addition to our recall evaluation, we measured the number of sentences containing relation instances found by each of the systems when applied to 5,000 documents (see Table 3). [sent-178, score-0.664]
</p><p>92 For almost all relations, +Coref matches many more sentences, including finding more sentences for those relations for which it has higher precision. [sent-179, score-0.124]
</p><p>93 6  Conclusion 292  found relation instances Our experiments suggest that in contexts where recall is important incorporating coreference into a relation extraction system may provide significant gains. [sent-180, score-1.388]
</p><p>94 Despite being noisy, coreference information improved F-scores for all relations in our test, more than doubling the F-score for 5 of the 10. [sent-181, score-0.487]
</p><p>95 Why is the high error rate of coreference not very harmful to +Coref? [sent-182, score-0.37]
</p><p>96 If the only evidence we have for a proposed instance depends on low confidence coreference links, it is very unlikely to be added to our instance set for use in future iterations. [sent-185, score-0.681]
</p><p>97 Second, for both training and runtime, many of the coreference links relevant for extracting the relation set examined here are fairly reliable, such as wh-words in relative clauses. [sent-186, score-0.723]
</p><p>98 The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U. [sent-190, score-0.132]
</p><p>99 Snowball: extract-  ing relations from large plain-text collections. [sent-198, score-0.087]
</p><p>100 Learning surface text patterns for a question answering system. [sent-287, score-0.282]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coref', 0.447), ('coreference', 0.37), ('relation', 0.353), ('patterns', 0.212), ('ace', 0.172), ('boschee', 0.139), ('instances', 0.136), ('weischedel', 0.128), ('instance', 0.116), ('seed', 0.115), ('jobs', 0.109), ('recall', 0.108), ('arguments', 0.107), ('freedman', 0.096), ('mentions', 0.093), ('apple', 0.093), ('relations', 0.087), ('ramshaw', 0.086), ('precision', 0.082), ('anchors', 0.082), ('amadeus', 0.079), ('haschild', 0.079), ('orgleader', 0.079), ('pattern', 0.075), ('subtypes', 0.074), ('surface', 0.07), ('mozart', 0.069), ('spotted', 0.069), ('steve', 0.068), ('extraction', 0.068), ('ravichandran', 0.066), ('sue', 0.064), ('wildcards', 0.06), ('mitchell', 0.059), ('texts', 0.057), ('kennedy', 0.057), ('entail', 0.055), ('son', 0.055), ('expressing', 0.053), ('agichtein', 0.052), ('names', 0.049), ('induce', 0.049), ('runtime', 0.048), ('bootstrapped', 0.048), ('official', 0.047), ('wolfgang', 0.046), ('confidence', 0.046), ('explored', 0.046), ('mention', 0.044), ('expressed', 0.044), ('bbn', 0.043), ('entails', 0.042), ('policy', 0.041), ('bootstrapping', 0.039), ('unannotated', 0.038), ('estimated', 0.038), ('defense', 0.038), ('sentences', 0.037), ('argument', 0.037), ('accepted', 0.036), ('requiring', 0.035), ('pantel', 0.035), ('entailing', 0.035), ('gence', 0.035), ('strand', 0.035), ('edr', 0.035), ('esm', 0.035), ('father', 0.035), ('harmed', 0.035), ('inadequately', 0.035), ('lorna', 0.035), ('marjorie', 0.035), ('moulton', 0.035), ('narrows', 0.035), ('outset', 0.035), ('pays', 0.035), ('sacrifices', 0.035), ('serif', 0.035), ('virginia', 0.035), ('desired', 0.033), ('added', 0.033), ('incorrect', 0.033), ('despite', 0.032), ('errorful', 0.032), ('punyakanok', 0.032), ('cards', 0.032), ('wild', 0.032), ('interconnected', 0.032), ('bob', 0.032), ('populating', 0.032), ('heller', 0.032), ('possessives', 0.032), ('raytheon', 0.032), ('hovy', 0.031), ('measured', 0.03), ('lots', 0.03), ('invited', 0.03), ('worthwhile', 0.03), ('gabbard', 0.03), ('doubling', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="86-tfidf-1" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>2 0.28835607 <a title="86-tfidf-2" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>Author: Ang Sun ; Ralph Grishman ; Satoshi Sekine</p><p>Abstract: We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 1</p><p>3 0.24543357 <a title="86-tfidf-3" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>Author: Harr Chen ; Edward Benson ; Tahira Naseem ; Regina Barzilay</p><p>Abstract: We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small , set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. 1</p><p>4 0.23417728 <a title="86-tfidf-4" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>Author: Sameer Singh ; Amarnag Subramanya ; Fernando Pereira ; Andrew McCallum</p><p>Abstract: Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach.</p><p>5 0.20756209 <a title="86-tfidf-5" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>Author: Dingcheng Li ; Tim Miller ; William Schuler</p><p>Abstract: and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</p><p>6 0.19256859 <a title="86-tfidf-6" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>7 0.16746159 <a title="86-tfidf-7" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>8 0.16308776 <a title="86-tfidf-8" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>9 0.15935294 <a title="86-tfidf-9" href="./acl-2011-Bootstrapping_coreference_resolution_using_word_associations.html">63 acl-2011-Bootstrapping coreference resolution using word associations</a></p>
<p>10 0.15853643 <a title="86-tfidf-10" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>11 0.14975744 <a title="86-tfidf-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.1496311 <a title="86-tfidf-12" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>13 0.13743098 <a title="86-tfidf-13" href="./acl-2011-A_Cross-Lingual_ILP_Solution_to_Zero_Anaphora_Resolution.html">9 acl-2011-A Cross-Lingual ILP Solution to Zero Anaphora Resolution</a></p>
<p>14 0.13269146 <a title="86-tfidf-14" href="./acl-2011-Extending_the_Entity_Grid_with_Entity-Specific_Features.html">129 acl-2011-Extending the Entity Grid with Entity-Specific Features</a></p>
<p>15 0.11617176 <a title="86-tfidf-15" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>16 0.10915493 <a title="86-tfidf-16" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>17 0.10260194 <a title="86-tfidf-17" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>18 0.10059611 <a title="86-tfidf-18" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>19 0.09952607 <a title="86-tfidf-19" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>20 0.099380232 <a title="86-tfidf-20" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, 0.085), (2, -0.231), (3, 0.027), (4, 0.174), (5, 0.08), (6, 0.054), (7, -0.098), (8, -0.275), (9, -0.001), (10, 0.077), (11, -0.081), (12, -0.039), (13, -0.032), (14, -0.026), (15, -0.041), (16, -0.12), (17, -0.166), (18, 0.047), (19, 0.074), (20, -0.116), (21, 0.088), (22, 0.065), (23, 0.07), (24, -0.088), (25, 0.007), (26, 0.118), (27, 0.095), (28, 0.096), (29, -0.102), (30, -0.005), (31, -0.045), (32, -0.015), (33, -0.031), (34, -0.111), (35, -0.003), (36, -0.038), (37, 0.084), (38, 0.077), (39, 0.035), (40, 0.031), (41, -0.064), (42, 0.002), (43, -0.013), (44, 0.007), (45, 0.01), (46, 0.067), (47, -0.022), (48, 0.049), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96193957 <a title="86-lsi-1" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>2 0.8047418 <a title="86-lsi-2" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>Author: Ang Sun ; Ralph Grishman ; Satoshi Sekine</p><p>Abstract: We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 1</p><p>3 0.7615121 <a title="86-lsi-3" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>Author: Tara McIntosh ; Lars Yencken ; James R. Curran ; Timothy Baldwin</p><p>Abstract: State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.</p><p>4 0.72385865 <a title="86-lsi-4" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>5 0.71657634 <a title="86-lsi-5" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>6 0.70887297 <a title="86-lsi-6" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>7 0.69802731 <a title="86-lsi-7" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>8 0.68023109 <a title="86-lsi-8" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>9 0.65067416 <a title="86-lsi-9" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>10 0.61344624 <a title="86-lsi-10" href="./acl-2011-Bootstrapping_coreference_resolution_using_word_associations.html">63 acl-2011-Bootstrapping coreference resolution using word associations</a></p>
<p>11 0.61000222 <a title="86-lsi-11" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>12 0.60844237 <a title="86-lsi-12" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>13 0.57821476 <a title="86-lsi-13" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>14 0.56178653 <a title="86-lsi-14" href="./acl-2011-A_Cross-Lingual_ILP_Solution_to_Zero_Anaphora_Resolution.html">9 acl-2011-A Cross-Lingual ILP Solution to Zero Anaphora Resolution</a></p>
<p>15 0.52796555 <a title="86-lsi-15" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>16 0.4735119 <a title="86-lsi-16" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>17 0.46992525 <a title="86-lsi-17" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>18 0.46286428 <a title="86-lsi-18" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>19 0.45924094 <a title="86-lsi-19" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>20 0.45770049 <a title="86-lsi-20" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.025), (9, 0.03), (17, 0.044), (26, 0.025), (31, 0.013), (37, 0.114), (39, 0.026), (41, 0.072), (55, 0.027), (59, 0.073), (72, 0.054), (89, 0.178), (91, 0.092), (96, 0.127), (97, 0.012), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87518013 <a title="86-lda-1" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<p>Author: Emilia Apostolova ; Noriko Tomuro ; Dina Demner-Fushman</p><p>Abstract: Detecting the linguistic scope of negated and speculated information in text is an important Information Extraction task. This paper presents ScopeFinder, a linguistically motivated rule-based system for the detection of negation and speculation scopes. The system rule set consists of lexico-syntactic patterns automatically extracted from a corpus annotated with negation/speculation cues and their scopes (the BioScope corpus). The system performs on par with state-of-the-art machine learning systems. Additionally, the intuitive and linguistically motivated rules will allow for manual adaptation of the rule set to new domains and corpora. 1 Motivation Information Extraction (IE) systems often face the problem of distinguishing between affirmed, negated, and speculative information in text. For example, sentiment analysis systems need to detect negation for accurate polarity classification. Similarly, medical IE systems need to differentiate between affirmed, negated, and speculated (possible) medical conditions. The importance of the task of negation and speculation (a.k.a. hedge) detection is attested by a number of research initiatives. The creation of the BioScope corpus (Vincze et al., 2008) assisted in the development and evaluation of several negation/hedge scope detection systems. The corpus consists of medical and biological texts annotated for negation, speculation, and their linguistic scope. The 2010 283 Noriko Tomuro Dina Demner-Fushman DePaul University Chicago, IL USA t omuro @ c s . depaul . edu National Library of Medicine Bethesda, MD USA ddemne r@mai l nih . gov . i2b2 NLP Shared Task1 included a track for detection of the assertion status of medical problems (e.g. affirmed, negated, hypothesized, etc.). The CoNLL2010 Shared Task (Farkas et al., 2010) focused on detecting hedges and their scopes in Wikipedia articles and biomedical texts. In this paper, we present a linguistically motivated rule-based system for the detection of negation and speculation scopes that performs on par with state-of-the-art machine learning systems. The rules used by the ScopeFinder system are automatically extracted from the BioScope corpus and encode lexico-syntactic patterns in a user-friendly format. While the system was developed and tested using a biomedical corpus, the rule extraction mechanism is not domain-specific. In addition, the linguistically motivated rule encoding allows for manual adaptation to new domains and corpora. 2 Task Definition Negation/Speculation detection is typically broken down into two sub-tasks - discovering a negation/speculation cue and establishing its scope. The following example from the BioScope corpus shows the annotated hedging cue (in bold) together with its associated scope (surrounded by curly brackets): Finally, we explored the {possible role of 5hydroxyeicosatetraenoic acid as a regulator of arachidonic acid liberation}. Typically, systems first identify negation/speculation cues and subsequently try to identify their associated cue scope. However, the two tasks are interrelated and both require 1https://www.i2b2.org/NLP/Relations/ Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o.c?i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 283–287, syntactic understanding. Consider the following two sentences from the BioScope corpus: 1) By contrast, {D-mib appears to be uniformly expre1ss)e Bdy yin c oimnatrgaisnta,l { dDis-mcsi }b. 2) Differentiation assays using water soluble phorbol esters reveal that differentiation becomes irreversible soon after AP-1 appears. Both sentences contain the word form appears, however in the first sentence the word marks a hedg- ing cue, while in the second sentence the word does not suggest speculation. Unlike previous work, we do not attempt to identify negation/speculation cues independently of their scopes. Instead, we concentrate on scope detection, simultaneously detecting corresponding cues. 3 Dataset We used the BioScope corpus (Vincze et al., 2008) to develop our system and evaluate its performance. To our knowledge, the BioScope corpus is the only publicly available dataset annotated with negation/speculation cues and their scopes. It consists of biomedical papers, abstracts, and clinical reports (corpus statistics are shown in Tables 1 and 2). Corpus Type Sentences Documents Mean Document Size Clinical752019543.85 Full Papers Paper Abstracts 3352 14565 9 1273 372.44 11.44 Table 1: Statistics of the BioScope corpus. Document sizes represent number of sentences. Corpus Type Negation Cues Speculation Cues Negation Speculation Clinical87211376.6%13.4% Full Papers Paper Abstracts 378 1757 682 2694 13.76% 13.45% 22.29% 17.69% Table 2: Statistics of the BioScope corpus. The 2nd and 3d columns show the total number of cues within the datasets; the 4th and 5th columns show the percentage of negated and speculative sentences. 70% ofthe corpus documents (randomly selected) were used to develop the ScopeFinder system (i.e. extract lexico-syntactic rules) and the remaining 30% were used to evaluate system performance. While the corpus focuses on the biomedical domain, our rule extraction method is not domain specific and in future work we are planning to apply our method on different types of corpora. 4 Method Intuitively, rules for detecting both speculation and negation scopes could be concisely expressed as a 284 Figure 1: Parse tree of the sentence ‘T cells {lack active NFkappa B } bPuatr express Sp1 as expected’ generated by cthtiev eS NtanF-fkoaprdp parser. Speculation scope ewxporedcste are gsehnoewrant eind ellipsis. tTanhecue word is shown in grey. The nearest common ancestor of all cue and scope leaf nodes is shown in a box. combination of lexical and syntactic patterns. example, BioScope O¨zg u¨r For and Radev (2009) examined sample sentences and developed hedging scope rules such as: The scope of a modal verb cue (e.g. may, might, could) is the verb phrase to which it is attached; The scope of a verb cue (e.g. appears, seems) followed by an infinitival clause extends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree representations. We first parsed each sentence in the training dataset which contained a negation or speculation cue using the Stanford parser (Klein and Manning, 2003; De Marneffe et al., 2006). Figure 1 shows the parse tree of a sample sentence containing a negation cue and its scope. Next, for each cue-scope instance within the sen- tence, we identified the nearest common ancestor Figure 2: Lexico-syntactic pattern extracted from the sentence from Figure 1. The rule is equivalent to the following string representation: (VP (VBP lack) (NP (JJ *scope*) (NN *scope*) (NN *scope*))). which encompassed the cue word(s) and all words in the scope (shown in a box on Figure 1). The subtree rooted by this ancestor is the basis for the resulting lexico-syntactic rule. The leaf nodes of the resulting subtree were converted to a generalized representation: scope words were converted to *scope*; noncue and non-scope words were converted to *; cue words were converted to lower case. Figure 2 shows the resulting rule. This rule generation approach resulted in a large number of very specific rule patterns - 1,681 nega- tion scope rules and 3,043 speculation scope rules were extracted from the training dataset. To identify a more general set of rules (and increase recall) we next performed a simple transformation of the derived rule set. If all children of a rule tree node are of type *scope* or * (i.e. noncue words), the node label is replaced by *scope* or * respectively, and the node’s children are pruned from the rule tree; neighboring identical siblings of type *scope* or * are replaced by a single node of the corresponding type. Figure 3 shows an example of this transformation. (a)ThechildrenofnodesJ /N /N are(b)Thechildren pruned and their labels are replaced by of node NP are *scope*. pruned and its label is replaced by *scope*. Figure 3: Transformation of the tree shown in Figure 2. The final rule is equivalent to the following string representation: (VP (VBP lack) *scope* ) 285 The rule tree pruning described above reduced the negation scope rule patterns to 439 and the speculation rule patterns to 1,000. In addition to generating a set of scope finding rules, we also implemented a module that parses string representations of the lexico-syntactic rules and performs subtree matching. The ScopeFinder module2 identifies negation and speculation scopes in sentence parse trees using string-encoded lexicosyntactic patterns. Candidate sentence parse subtrees are first identified by matching the path of cue leafnodes to the root ofthe rule subtree pattern. Ifan identical path exists in the sentence, the root of the candidate subtree is thus also identified. The candidate subtree is evaluated for a match by recursively comparing all node children (starting from the root of the subtree) to the rule pattern subtree. Nodes of type *scope* and * match any number of nodes, similar to the semantics of Regex Kleene star (*). 5 Results As an informed baseline, we used a previously de- veloped rule-based system for negation and speculation scope discovery (Apostolova and Tomuro, 2010). The system, inspired by the NegEx algorithm (Chapman et al., 2001), uses a list of phrases split into subsets (preceding vs. following their scope) to identify cues using string matching. The cue scopes extend from the cue to the beginning or end of the sentence, depending on the cue type. Table 3 shows the baseline results. PSFCNalpueingpleciarPutcAlai opbtneisor tacsP6597C348o.r12075e4ctly6859RP203475r. 81e26d037icteF569784C52. 04u913e84s5F2A81905l.2786P14redictCus Table 3: Baseline system performance. P (Precision), R (Recall), and F (F1-score) are computed based on the sentence tokens of correctly predicted cues. The last column shows the F1-score for sentence tokens of all predicted cues (including erroneous ones). We used only the scopes of predicted cues (correctly predicted cues vs. all predicted cues) to mea- 2The rule sets and source code are publicly available at http://scopefinder.sourceforge.net/. sure the baseline system performance. The baseline system heuristics did not contain all phrase cues present in the dataset. The scopes of cues that are missing from the baseline system were not included in the results. As the baseline system was not penalized for missing cue phrases, the results represent the upper bound of the system. Table 4 shows the results from applying the full extracted rule set (1,681 negation scope rules and 3,043 speculation scope rules) on the test data. As expected, this rule set consisting of very specific scope matching rules resulted in very high precision and very low recall. Negation P R F A Clinical99.4734.3051.0117.58 Full Papers Paper Abstracts 95.23 87.33 25.89 05.78 40.72 10.84 28.00 07.85 Speculation Clinical96.5020.1233.3022.90 Full Papers Paper Abstracts 88.72 77.50 15.89 11.89 26.95 20.62 10.13 10.00 Table 4: Results from applying the full extracted rule set on the test data. Precision (P), Recall (R), and F1-score (F) are com- puted based the number of correctly identified scope tokens in each sentence. Accuracy (A) is computed for correctly identified full scopes (exact match). Table 5 shows the results from applying the rule set consisting of pruned pattern trees (439 negation scope rules and 1,000 speculation scope rules) on the test data. As shown, overall results improved significantly, both over the baseline and over the unpruned set of rules. Comparable results are shown in bold in Tables 3, 4, and 5. Negation P R F A Clinical85.5992.1588.7585.56 Full Papers 49.17 94.82 64.76 71.26 Paper Abstracts 61.48 92.64 73.91 80.63 Speculation Clinical67.2586.2475.5771.35 Full Papers 65.96 98.43 78.99 52.63 Paper Abstracts 60.24 95.48 73.87 65.28 Table 5: Results from applying the pruned rule set on the test data. Precision (P), Recall (R), and F1-score (F) are computed based on the number of correctly identified scope tokens in each sentence. Accuracy (A) is computed for correctly identified full scopes (exact match). 6 Related Work Interest in the task of identifying negation and spec- ulation scopes has developed in recent years. Rele286 vant research was facilitated by the appearance of a publicly available annotated corpus. All systems described below were developed and evaluated against the BioScope corpus (Vincze et al., 2008). O¨zg u¨r and Radev (2009) have developed a supervised classifier for identifying speculation cues and a manually compiled list of lexico-syntactic rules for identifying their scopes. For the performance of the rule based system on identifying speculation scopes, they report 61. 13 and 79.89 accuracy for BioScope full papers and abstracts respectively. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. They modeled the scope finding problem as a classification task that determines if a sentence token is the first token in a scope sequence, the last one, or neither. Results of the scope finding system with predicted hedge signals were reported as F1-scores of 38. 16, 59.66, 78.54 and for clinical texts, full papers, and abstracts respectively3. Accuracy (computed for correctly identified scopes) was reported as 26.21, 35.92, and 65.55 for clinical texts, papers, and abstracts respectively. Morante and Daelemans have also developed a metalearner for identifying the scope of negation (2009a). Results of the negation scope finding system with predicted cues are reported as F1-scores (computed on scope tokens) of 84.20, 70.94, and 82.60 for clinical texts, papers, and abstracts respectively. Accuracy (the percent of correctly identified exact scopes) is reported as 70.75, 41.00, and 66.07 for clinical texts, papers, and abstracts respectively. The top three best performers on the CoNLL2010 shared task on hedge scope detection (Farkas et al., 2010) report an F1-score for correctly identified hedge cues and their scopes ranging from 55.3 to 57.3. The shared task evaluation metrics used stricter matching criteria based on exact match of both cues and their corresponding scopes4. CoNLL-2010 shared task participants applied a variety of rule-based and machine learning methods 3F1-scores are computed based on scope tokens. Unlike our evaluation metric, scope token matches are computed for each cue within a sentence, i.e. a token is evaluated multiple times if it belongs to more than one cue scope. 4Our system does not focus on individual cue-scope pair de- tection (we instead optimized scope detection) and as a result performance metrics are not directly comparable. on the task - Morante et al. (2010) used a memorybased classifier based on the k-nearest neighbor rule to determine if a token is the first token in a scope sequence, the last, or neither; Rei and Briscoe (2010) used a combination of manually compiled rules, a CRF classifier, and a sequence of post-processing steps on the same task; Velldal et al (2010) manually compiled a set of heuristics based on syntactic information taken from dependency structures. 7 Discussion We presented a method for automatic extraction of lexico-syntactic rules for negation/speculation scopes from an annotated corpus. The developed ScopeFinder system, based on the automatically extracted rule sets, was compared to a baseline rule-based system that does not use syntactic information. The ScopeFinder system outperformed the baseline system in all cases and exhibited results comparable to complex feature-based, machine-learning systems. In future work, we will explore the use of statistically based methods for the creation of an optimum set of lexico-syntactic tree patterns and will evaluate the system performance on texts from different domains. References E. Apostolova and N. Tomuro. 2010. Exploring surfacelevel heuristics for negation and speculation discovery in clinical texts. In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, pages 81–82. Association for Computational Linguistics. W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, and B.G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of biomedical informatics, 34(5):301–310. A.B. Clegg and A.J. Shepherd. 2007. Benchmarking natural-language parsers for biological applications using dependency graphs. BMC bioinformatics, 8(1):24. M.C. De Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC 2006. Citeseer. R. Farkas, V. Vincze, G. M o´ra, J. Csirik, and G. Szarvas. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on 287 Computational Natural Language Learning (CoNLL2010): Shared Task, pages 1–12. H. Kilicoglu and S. Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC bioinformatics, 9(Suppl 11):S10. H. Kilicoglu and S. Bergler. 2010. A High-Precision Approach to Detecting Hedges and Their Scopes. CoNLL-2010: Shared Task, page 70. D. Klein and C.D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. Advances in neural information processing systems, pages 3–10. D. McClosky and E. Charniak. 2008. Self-training for biomedical parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 101–104. Association for Computational Linguistics. R. Morante and W. Daelemans. 2009a. A metalearning approach to processing the scope of negation. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 21–29. Association for Computational Linguistics. R. Morante and W. Daelemans. 2009b. Learning the scope of hedge cues in biomedical texts. In Proceed- ings of the Workshop on BioNLP, pages 28–36. Association for Computational Linguistics. R. Morante, V. Van Asch, and W. Daelemans. 2010. Memory-based resolution of in-sentence scopes of hedge cues. CoNLL-2010: Shared Task, page 40. A. O¨zg u¨r and D.R. Radev. 2009. Detecting speculations and their scopes in scientific text. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1398–1407. Association for Computational Linguistics. M. Rei and T. Briscoe. 2010. Combining manual rules and supervised learning for hedge cue and scope detection. In Proceedings of the 14th Conference on Natural Language Learning, pages 56–63. E. Velldal, L. Øvrelid, and S. Oepen. 2010. Resolving Speculation: MaxEnt Cue Classification and Dependency-Based Scope Rules. CoNLL-2010: Shared Task, page 48. V. Vincze, G. Szarvas, R. Farkas, G. M o´ra, and J. Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC bioinformatics, 9(Suppl 11):S9. H. Zhou, X. Li, D. Huang, Z. Li, and Y. Yang. 2010. Exploiting Multi-Features to Detect Hedges and Their Scope in Biomedical Texts. CoNLL-2010: Shared Task, page 106.</p><p>2 0.83355439 <a title="86-lda-2" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG.</p><p>same-paper 3 0.8290062 <a title="86-lda-3" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>4 0.75481665 <a title="86-lda-4" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>5 0.74253535 <a title="86-lda-5" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>6 0.73928452 <a title="86-lda-6" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>7 0.73884606 <a title="86-lda-7" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>8 0.73658645 <a title="86-lda-8" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>9 0.7361297 <a title="86-lda-9" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>10 0.73525691 <a title="86-lda-10" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>11 0.7332111 <a title="86-lda-11" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>12 0.73187959 <a title="86-lda-12" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>13 0.731094 <a title="86-lda-13" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>14 0.72959137 <a title="86-lda-14" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>15 0.72873718 <a title="86-lda-15" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>16 0.72871327 <a title="86-lda-16" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>17 0.72785193 <a title="86-lda-17" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>18 0.72586018 <a title="86-lda-18" href="./acl-2011-Grammatical_Error_Correction_with_Alternating_Structure_Optimization.html">147 acl-2011-Grammatical Error Correction with Alternating Structure Optimization</a></p>
<p>19 0.72401488 <a title="86-lda-19" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>20 0.72215605 <a title="86-lda-20" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
