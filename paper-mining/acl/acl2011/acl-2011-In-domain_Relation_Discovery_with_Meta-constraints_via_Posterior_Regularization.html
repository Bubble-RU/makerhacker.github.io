<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-170" href="#">acl2011-170</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</h1>
<br/><p>Source: <a title="acl-2011-170-pdf" href="http://aclweb.org/anthology//P/P11/P11-1054.pdf">pdf</a></p><p>Author: Harr Chen ; Edward Benson ; Tahira Naseem ; Regina Barzilay</p><p>Abstract: We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small , set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. 1</p><p>Reference: <a title="acl-2011-170-reference" href="../acl2011_reference/acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. [sent-2, score-0.87]
</p><p>2 These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. [sent-3, score-0.885]
</p><p>3 We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. [sent-4, score-0.525]
</p><p>4 Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. [sent-5, score-0.488]
</p><p>5 Furthermore, we find that a small  ,  set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. [sent-6, score-0.532]
</p><p>6 The indicator and argument words for the damage relation are highlighted. [sent-15, score-0.899]
</p><p>7 In contrast to previous work, our approach learns from domain-independent meta-constraints on relation expression, rather than supervision specific to particular relations and their instances. [sent-17, score-0.682]
</p><p>8 For instance, consider the damage relation excerpted from earthquake articles in Figure 1. [sent-20, score-0.751]
</p><p>9 ” Syntactically, in two instances the relation instantiation is the dependency child of the word “destroying. [sent-22, score-0.626]
</p><p>10 We capture these regularities using a Bayesian model where the underlying relations are repreProce dinPgosrt olafn thde, 4 O9rtehg Aon ,n Ju anle M 1e9e-2tin4g, 2 o0f1 t1h. [sent-25, score-0.335]
</p><p>11 Each relation instantiation is encoded by the variables as a relation-evoking indicator word (e. [sent-29, score-0.758]
</p><p>12 2 Our approach capitalizes on relation regularity in two ways. [sent-34, score-0.435]
</p><p>13 First, the model’s generative process encourages coherence in the local features and placement of relation instances. [sent-35, score-0.435]
</p><p>14 , 2007) during inference to enforce higher-level declarative constraints, such as requiring indicators  and arguments to be syntactically linked. [sent-37, score-0.304]
</p><p>15 We evaluate our approach on two domains previously studied for high-level document structure analysis, news articles about earthquakes and financial markets. [sent-38, score-0.314]
</p><p>16 In particular, we find that a small set of declarative constraints are effective across domains, while additional domainspecific constraints yield further benefits. [sent-41, score-0.696]
</p><p>17 2We do not use the word “argument” in the syntactic sense— a relation’s argument may or may not be the syntactic dependency argument of its indicator. [sent-52, score-0.486]
</p><p>18 Second, in contrast to work that builds general relation databases from heterogeneous corpora, our focus is on learning the relations salient in a single domain. [sent-54, score-0.637]
</p><p>19 Earlier work in unsupervised information extraction has also leveraged meta-knowledge independent of specific relation types, such as declarativelyspecified syntactic patterns (Riloff, 1996), frequent dependency subtree patterns (Sudo et al. [sent-56, score-0.723]
</p><p>20 Our approach incorporates a broader range of constraints and balances constraints with underlying patterns learned from the data, thereby requiring more sophisticated machinery for modeling and inference. [sent-60, score-0.586]
</p><p>21 Extraction with Constraints Previous work has recognized the appeal of applying declarative constraints to extraction. [sent-61, score-0.412]
</p><p>22 In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative constraints on the output from a set of classifiers trained on local features. [sent-62, score-0.574]
</p><p>23 Recent work has also explored how certain kinds of supervision can be formulated as constraints on model posteriors. [sent-65, score-0.366]
</p><p>24 Such constraints are not declarative, but instead based on annotations of words’ majority relation labels (Mann and McCallum, 2008) and pre-existing databases with the desired output schema (Bellare  and McCallum, 2009). [sent-66, score-0.723]
</p><p>25 In contrast to previous work, our approach explores a different class of constraints that does not rely on supervision that is specific to particular relation types and their instances. [sent-67, score-0.768]
</p><p>26 3  Model  Our work performs in-domain relation discovery by leveraging regularities in relation expression at the lexical, syntactic, and discourse levels. [sent-68, score-1.01]
</p><p>27 A single relation instantiation is a pair of indicator w and argument x; we filter w to be nouns and verbs and x to be noun phrases and adjectives. [sent-70, score-0.951]
</p><p>28 for biasing inference to adhere to declarativelyspecified constraints on relation expression. [sent-71, score-0.778]
</p><p>29 1 Problem Formulation Our input is a corpus of constituent-parsed documents and a number K of relation types. [sent-74, score-0.49]
</p><p>30 The output is K clusters of semantically related relation instantiations. [sent-75, score-0.503]
</p><p>31 We represent these instantiations as a pair of indicator word and argument sequence from the same sentence. [sent-76, score-0.482]
</p><p>32 The indicator’s role is to anchor a relation and identify its type. [sent-77, score-0.435]
</p><p>33 For instance, in the earthquake domain a likely indicator for damage would be “destroyed. [sent-79, score-0.508]
</p><p>34 3  Along with the document parse trees, we utilize a set of features φi(w) and φa(x) describing each potential indicator word w and argument constituent x, respectively. [sent-83, score-0.538]
</p><p>35 2 Generative Process Our model associates each relation type k with a set offeature distributions θk and a location distribution λk. [sent-91, score-0.566]
</p><p>36 Furthermore, we allow at most one instantiation per document and relation, so as to target  relations that are relevant to the entire document. [sent-94, score-0.376]
</p><p>37 Second, an instantiation is selected for every pair of document d and relation k. [sent-97, score-0.685]
</p><p>38 Third, the indicator features of each word and argument features of each constituent are generated based on the relation parameters and instantiations. [sent-98, score-0.946]
</p><p>39 Generating Relation Parameters Each relation k is associated with four feature distribution parameter vectors: θki for indicator words, θbki for nonindicator words, θka for argument constituents, and θbka for non-argument constituents. [sent-100, score-0.822]
</p><p>40 5 By drawing each instance from these distributions, we encourage the relation to be coherent in local lex-  ical and syntactic properties. [sent-104, score-0.524]
</p><p>41 Each relation type k is also associated with a parameter vector λk over document segments drawn from a symmetric Dirichlet prior. [sent-105, score-0.56]
</p><p>42 Documents are divided into L equal-length segments; λk states how likely relation k is for each segment, with one null outcome for the relation not occurring in the document. [sent-106, score-0.87]
</p><p>43 5We use separate background distributions for each relation to make inference more tractable. [sent-108, score-0.543]
</p><p>44 The model can learn, for example, that a particular relation typically occurs in the first quarter of a document (if L = 4). [sent-113, score-0.553]
</p><p>45 Generating Relation Instantiations For every relation type k and document d, we first choose which portion of the document (if any) contains the instantiation by drawing a document segment sd,k from λk. [sent-114, score-0.929]
</p><p>46 Our model only draws one instantiation per pair of k and d, so each discovered instantiation within a document is a separate relation. [sent-115, score-0.376]
</p><p>47 We then choose the specific sentence zd,k uniformly from within the segment, and the indicator word id,k and argument constituent ad,k uniformly from within that sentence. [sent-116, score-0.453]
</p><p>48 We make a Na¨ ıve Bayes assumption between features, drawing each independently conditioned on relation structure. [sent-118, score-0.474]
</p><p>49 , K, using indicator parameters θki if relation k selected w as an indicator word (if id,k = w) and background parameters θbki otherwise. [sent-123, score-0.939]
</p><p>50 4  Inference with Constraints  The model presented above leverages relation regularities in local features and document placement. [sent-126, score-0.693]
</p><p>51 However, it is unable to specify global syntactic preferences about relation expression, such as indicators and arguments being in the same clause. [sent-127, score-0.578]
</p><p>52 6 To overcome these obstacles, we apply declarative constraints by imposing inequality constraints on expectations of the posterior during inference using posterior regularization (Gra ¸ca et al. [sent-129, score-0.997]
</p><p>53 , the indicator and argument modify the same verb), and b is a fixed threshold. [sent-147, score-0.387]
</p><p>54 Given a set C of constraints with functions fc(z) andG tivherensh ao sldest bc, tfh ceo updates fso wr q(θ) anncdti q(z) ffrom equation 1 are as follows:  q(θ) = arqg(mθ)inKL? [sent-148, score-0.344]
</p><p>55 Equation 2 is not affe(zc)ted ∝ by tpheE posterior constraints and is updated by setting q(θ) to q0(θ). [sent-155, score-0.427]
</p><p>56 (4)  With the box constraints of equation 4, a numerical optimization procedure such as L-BFGS-B (Byrd et al. [sent-160, score-0.307]
</p><p>57 For instance, θˆik,φ of relation k and feature is updated by finding the gradient of equation 1 with respect to θˆik,φ and applying L-BFGS. [sent-175, score-0.569]
</p><p>58 5 Declarative Constraints  We now have the machinery to incorporate a variety of declarative constraints during inference. [sent-185, score-0.412]
</p><p>59 • The indicator is a noun and the argument is a Tmhoedi ifniedri or complement. [sent-190, score-0.387]
</p><p>60 • The indicator is a noun in a verb’s subject and tThhee argument i iss i na tnhoeu corresponding object. [sent-191, score-0.387]
</p><p>61 Prevalence For a relation to be domain-relevant, it should occur in numerous documents across the corpus, so we institute a constraint on the number of  times a relation is instantiated. [sent-192, score-1.044]
</p><p>62 Note that the effect of this constraint could also be achieved by tuning the prior probability of a relation not occurring in a document. [sent-193, score-0.518]
</p><p>63 Separation The separation constraint encourages 535 diversity in the discovered relation types by restricting the number of times a single word can serve as either an indicator or part of the argument of a relation instance. [sent-195, score-1.34]
</p><p>64 Finance articles chronicle daily market movements of currencies and stock indexes, and earthquake articles document specific earthquakes. [sent-202, score-0.364]
</p><p>65 We manually annotated relations for both corpora, selecting relation types that occurred frequently in each domain. [sent-204, score-0.597]
</p><p>66 Corpus statistics are summarized below, and example relation types are shown in Table 2. [sent-206, score-0.435]
</p><p>67 Precision is measured by mapping every induced relation cluster to its closest gold relation and computing the proportion of predicted sentences or words that are correct. [sent-221, score-0.906]
</p><p>68 Conversely, for recall we map every gold relation to its closest predicted relation and find the proportion of gold sentences or words that are predicted. [sent-222, score-0.906]
</p><p>69 Note that sentence-level scores are always at least as high as token-level scores, since it is possible to select a sentence correctly but none of its true relation tokens while the opposite is not possible. [sent-224, score-0.477]
</p><p>70 Domain-specific Constraints On top of the cross-  domain constraints from Section 5, we study whether imposing basic domain-specific constraints can be beneficial. [sent-225, score-0.534]
</p><p>71 The finance dataset is heavily quantitative, so we consider applying a single domain-specific constraint stating that most relation arguments should include a number. [sent-226, score-0.678]
</p><p>72 Likewise, earthquake articles are typically written with a majority of the relevant information toward the beginning of the document, so its domain-specific constraint is that most relations should occur in the first two sentences of a document. [sent-227, score-0.528]
</p><p>73 Note that these domain-specific constraints are not specific to individual relations or instances, but rather encode a preference across all relation types. [sent-228, score-0.881]
</p><p>74 For arguments, we use the word, syntactic constituent label, the head word of the parent constituent, and the dependency label of 536 the argument to its parent. [sent-231, score-0.309]
</p><p>75 Clustering (CLUTO): A straightforward way of identifying sentences bearing the same relation is to simply cluster them. [sent-234, score-0.435]
</p><p>76 As with our model, we set the number of clusters K to the true number of relation types. [sent-236, score-0.545]
</p><p>77 The datasets we consider here exhibit high-level regularities in content organization, so we expect that a topic model with global constraints could identify plausible clusters of relation-bearing sentences. [sent-239, score-0.489]
</p><p>78 Again, K is set to the true number of relation types. [sent-240, score-0.477]
</p><p>79 For this comparison, we transform USP’s lambda calculus formulas to relation spans as follows. [sent-250, score-0.571]
</p><p>80 We fix K to the true number of annotated relation types for both our model and USP and L (the number of document segments) to five. [sent-256, score-0.595]
</p><p>81 For earthquake, the far more difficult domain, our base model with only the domainindependent constraints strongly outperforms all three baselines across both metrics. [sent-260, score-0.357]
</p><p>82 When we add such constraints (denoted as model+DSC), we achieve consistently higher performance than all baselines across both datasets and metrics, demonstrating that this approach provides a simple and effective framework for injecting domain knowledge into relation discovery. [sent-263, score-0.797]
</p><p>83 537 The first two baselines correspond to a setup where the number of sentence clusters K is set to the true number of relation types. [sent-265, score-0.585]
</p><p>84 In practice, these outputs are all plausible discoveries, and a practitioner desiring specific outputs could impose additional constraints to guide relation discovery toward them. [sent-280, score-0.727]
</p><p>85 We consider removing the constraints on syntactic patterns (no-syn) and the constraints disallowing relations to overlap (no-sep) from the full domain-independent model. [sent-282, score-0.762]
</p><p>86 8 We also try a version with hard syntac-  tic constraints (hard-syn), which requires that every extraction match one of the three syntactic patterns specified by the syntactic constraint. [sent-283, score-0.482]
</p><p>87 The model’s performance degrades when either of the two constraint sets are removed, demonstrating that the constraints are in fact beneficial for relation discovery. [sent-285, score-0.766]
</p><p>88 Additionally, in the hardsyn case, performance drops dramatically forfinance 8Prevalence constraints are always enforced, as otherwise the prior on not instantiating a relation would need to be tuned. [sent-286, score-0.731]
</p><p>89 This suggests that formulating constraints as soft inequalities on posterior expectations gives our model the flexibility to accommodate both the underlying signal in the data and the declarative constraints. [sent-291, score-0.597]
</p><p>90 To incorporate training examples in our model, we simply treat annotated relation instances as observed variables. [sent-294, score-0.497]
</p><p>91 Forfinance, it takes at least 10 annotated documents (corresponding to roughly 130 annotated relation instances) for the CRF to match the semi-supervised model’s performance. [sent-298, score-0.49]
</p><p>92 For earthquake, using even 10 annotated documents (about 71 relation instances) is not sufficient to match our model’s performance. [sent-299, score-0.49]
</p><p>93 Using a single labeled document (13 relation instances) yields superior performance to either of our model variants for finance, while four labeled documents (29 relation instances) do the same for earthquake. [sent-301, score-1.043]
</p><p>94 This result is not surprising—our model makes strong  domain-independent assumptions about how underlying patterns of regularities in the text connect to relation expression. [sent-302, score-0.662]
</p><p>95 Moreover, being able to annotate even a single document requires a broad understanding of every relation type germane to the domain, which can be infeasible when there are many unfamiliar, complex domains to process. [sent-304, score-0.657]
</p><p>96 8 Conclusions This paper has presented a constraint-based approach to in-domain relation discovery. [sent-306, score-0.435]
</p><p>97 We have shown that a generative model augmented with declarative constraints on the model posterior can  successfully identify domain-relevant relations and their instantiations. [sent-307, score-0.744]
</p><p>98 Furthermore, we found that a single set of constraints can be used across divergent domains, and that tailoring constraints specific to a domain can yield further performance benefits. [sent-308, score-0.57]
</p><p>99 Automatic relation extraction with model order selection and discriminative label identification. [sent-359, score-0.512]
</p><p>100 Unsupervised methods for determining object and relation synonyms on the web. [sent-446, score-0.435]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('relation', 0.435), ('constraints', 0.248), ('usp', 0.23), ('earthquake', 0.199), ('indicator', 0.194), ('argument', 0.193), ('declarative', 0.164), ('relations', 0.162), ('regularities', 0.14), ('instantiation', 0.129), ('eq', 0.129), ('finance', 0.12), ('logp', 0.106), ('posterior', 0.104), ('bki', 0.095), ('kba', 0.095), ('instantiations', 0.095), ('supervision', 0.085), ('document', 0.085), ('earthquakes', 0.084), ('constraint', 0.083), ('lambda', 0.082), ('ik', 0.079), ('damage', 0.077), ('updated', 0.075), ('kbi', 0.071), ('tuesday', 0.071), ('clusters', 0.068), ('constituent', 0.066), ('gra', 0.066), ('banko', 0.065), ('ki', 0.063), ('cluto', 0.063), ('instances', 0.062), ('distributions', 0.061), ('equation', 0.059), ('parameters', 0.058), ('documents', 0.055), ('calculus', 0.054), ('patterns', 0.054), ('crf', 0.054), ('indicators', 0.053), ('domains', 0.053), ('dirichlet', 0.052), ('ka', 0.052), ('financial', 0.052), ('syntactic', 0.05), ('constituents', 0.049), ('arqg', 0.048), ('bka', 0.048), ('declarativelyspecified', 0.048), ('destroying', 0.048), ('forfinance', 0.048), ('germane', 0.048), ('inequalities', 0.048), ('inkl', 0.048), ('mindoro', 0.048), ('harr', 0.047), ('inference', 0.047), ('regularization', 0.046), ('fc', 0.046), ('updating', 0.044), ('extraction', 0.044), ('toward', 0.044), ('true', 0.042), ('poon', 0.042), ('multinomial', 0.042), ('discovering', 0.042), ('homes', 0.042), ('arguments', 0.04), ('baselines', 0.04), ('regina', 0.04), ('drawn', 0.04), ('databases', 0.04), ('articles', 0.04), ('exp', 0.039), ('drawing', 0.039), ('barzilay', 0.039), ('byrd', 0.039), ('variational', 0.038), ('unsupervised', 0.038), ('domain', 0.038), ('hyperparameters', 0.037), ('updates', 0.037), ('location', 0.037), ('dual', 0.037), ('every', 0.036), ('inequality', 0.036), ('sudo', 0.036), ('balances', 0.036), ('sekine', 0.036), ('across', 0.036), ('grouping', 0.035), ('segment', 0.035), ('satoshi', 0.035), ('bellare', 0.034), ('mintz', 0.034), ('explains', 0.034), ('model', 0.033), ('hasegawa', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000025 <a title="170-tfidf-1" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>Author: Harr Chen ; Edward Benson ; Tahira Naseem ; Regina Barzilay</p><p>Abstract: We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small , set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. 1</p><p>2 0.28412136 <a title="170-tfidf-2" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>Author: Ang Sun ; Ralph Grishman ; Satoshi Sekine</p><p>Abstract: We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 1</p><p>3 0.24543357 <a title="170-tfidf-3" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>4 0.22542123 <a title="170-tfidf-4" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>Author: Truc Vien T. Nguyen ; Alessandro Moschitti</p><p>Abstract: In this paper, we extend distant supervision (DS) based on Wikipedia for Relation Extraction (RE) by considering (i) relations defined in external repositories, e.g. YAGO, and (ii) any subset of Wikipedia documents. We show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document.</p><p>5 0.20720874 <a title="170-tfidf-5" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>6 0.19827373 <a title="170-tfidf-6" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>7 0.17273363 <a title="170-tfidf-7" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>8 0.16848202 <a title="170-tfidf-8" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>9 0.15728953 <a title="170-tfidf-9" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<p>10 0.13444969 <a title="170-tfidf-10" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>11 0.12731816 <a title="170-tfidf-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.11791943 <a title="170-tfidf-12" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>13 0.11643475 <a title="170-tfidf-13" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>14 0.11476373 <a title="170-tfidf-14" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>15 0.11266032 <a title="170-tfidf-15" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>16 0.10978208 <a title="170-tfidf-16" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>17 0.10190593 <a title="170-tfidf-17" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>18 0.095686577 <a title="170-tfidf-18" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>19 0.095656581 <a title="170-tfidf-19" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>20 0.092016958 <a title="170-tfidf-20" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.268), (1, 0.077), (2, -0.19), (3, -0.018), (4, 0.119), (5, 0.015), (6, 0.001), (7, 0.067), (8, -0.16), (9, 0.013), (10, 0.11), (11, -0.009), (12, 0.056), (13, 0.047), (14, -0.04), (15, -0.078), (16, -0.209), (17, -0.219), (18, -0.048), (19, 0.052), (20, -0.102), (21, 0.063), (22, -0.016), (23, -0.087), (24, -0.006), (25, -0.015), (26, 0.112), (27, 0.041), (28, 0.178), (29, 0.008), (30, -0.108), (31, 0.107), (32, 0.095), (33, -0.023), (34, -0.046), (35, -0.045), (36, -0.05), (37, 0.093), (38, -0.027), (39, -0.073), (40, -0.033), (41, 0.017), (42, 0.112), (43, -0.089), (44, 0.042), (45, -0.022), (46, 0.003), (47, -0.027), (48, 0.008), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98406649 <a title="170-lsi-1" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>Author: Harr Chen ; Edward Benson ; Tahira Naseem ; Regina Barzilay</p><p>Abstract: We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small , set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. 1</p><p>2 0.8535043 <a title="170-lsi-2" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>Author: Ang Sun ; Ralph Grishman ; Satoshi Sekine</p><p>Abstract: We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 1</p><p>3 0.83770126 <a title="170-lsi-3" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>Author: Tara McIntosh ; Lars Yencken ; James R. Curran ; Timothy Baldwin</p><p>Abstract: State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.</p><p>4 0.82974195 <a title="170-lsi-4" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>5 0.82084918 <a title="170-lsi-5" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents an unsupervised method for deriving inference axioms by composing semantic relations. The method is independent of any particular relation inventory. It relies on describing semantic relations using primitives and manipulating these primitives according to an algebra. The method was tested using a set of eight semantic relations yielding 78 inference axioms which were evaluated over PropBank.</p><p>6 0.78887993 <a title="170-lsi-6" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>7 0.78200787 <a title="170-lsi-7" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>8 0.69224608 <a title="170-lsi-8" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>9 0.67877227 <a title="170-lsi-9" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>10 0.58285815 <a title="170-lsi-10" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<p>11 0.58242244 <a title="170-lsi-11" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>12 0.55245167 <a title="170-lsi-12" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>13 0.5468725 <a title="170-lsi-13" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>14 0.53934413 <a title="170-lsi-14" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>15 0.53290659 <a title="170-lsi-15" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>16 0.52932531 <a title="170-lsi-16" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>17 0.52450091 <a title="170-lsi-17" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>18 0.51717156 <a title="170-lsi-18" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>19 0.51584238 <a title="170-lsi-19" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>20 0.51479286 <a title="170-lsi-20" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.023), (9, 0.016), (17, 0.047), (26, 0.03), (37, 0.11), (39, 0.054), (41, 0.061), (52, 0.162), (53, 0.015), (55, 0.06), (59, 0.092), (72, 0.028), (77, 0.011), (91, 0.051), (96, 0.143), (97, 0.015), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86600959 <a title="170-lda-1" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>Author: Harr Chen ; Edward Benson ; Tahira Naseem ; Regina Barzilay</p><p>Abstract: We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small , set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. 1</p><p>2 0.83032352 <a title="170-lda-2" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>Author: Miao Chen ; Klaus Zechner</p><p>Abstract: This paper focuses on identifying, extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence. Our goal is to find effective features, selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses, and to build automatic scoring models based on the most promising features by using machine learning methods. On human transcriptions with manually annotated clause and sentence boundaries, our best scoring model achieves an overall Pearson correlation with human rater scores of r=0.49 on an unseen test set, whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2. 1</p><p>3 0.79524326 <a title="170-lda-3" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>Author: Kugatsu Sadamitsu ; Kuniko Saito ; Kenji Imamura ; Genichiro Kikui</p><p>Abstract: This paper proposes three modules based on latent topics of documents for alleviating “semantic drift” in bootstrapping entity set expansion. These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsupervised way. Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain.</p><p>4 0.79497516 <a title="170-lda-4" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>Author: Joel Lang ; Mirella Lapata</p><p>Abstract: In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.</p><p>5 0.79157859 <a title="170-lda-5" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>Author: Yuval Marton ; Nizar Habash ; Owen Rambow</p><p>Abstract: We explore the contribution of morphological features both lexical and inflectional to dependency parsing of Arabic, a morphologically rich language. Using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., “broken plurals”) and the related rationality feature improve over form-based features. It is the first time functional morphological features are used for Arabic NLP. – –</p><p>6 0.7810061 <a title="170-lda-6" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>7 0.779338 <a title="170-lda-7" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>8 0.77931112 <a title="170-lda-8" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>9 0.77742487 <a title="170-lda-9" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>10 0.77708316 <a title="170-lda-10" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>11 0.77696431 <a title="170-lda-11" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>12 0.77233291 <a title="170-lda-12" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>13 0.76866716 <a title="170-lda-13" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>14 0.76850283 <a title="170-lda-14" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>15 0.76819783 <a title="170-lda-15" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>16 0.7677148 <a title="170-lda-16" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>17 0.76761609 <a title="170-lda-17" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>18 0.76748425 <a title="170-lda-18" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>19 0.76705313 <a title="170-lda-19" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>20 0.76700979 <a title="170-lda-20" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
