<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-110" href="#">acl2011-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</h1>
<br/><p>Source: <a title="acl-2011-110-pdf" href="http://aclweb.org/anthology//P/P11/P11-1003.pdf">pdf</a></p><p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>Reference: <a title="acl-2011-110-reference" href="../acl2011_reference/acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p i s  Abstract In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. [sent-5, score-0.432]
</p><p>2 Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. [sent-6, score-1.107]
</p><p>3 In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. [sent-7, score-0.575]
</p><p>4 Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. [sent-8, score-0.796]
</p><p>5 On the one hand, there is a tendency to integrate richer syntactic information into a translation rule in order to better express the translation phenomena. [sent-12, score-0.472]
</p><p>6 , 2003), hierarchical phrases (Chiang, 2005), and syntactic tree fragments (Galley et al. [sent-14, score-0.246]
</p><p>7 Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between lan-  guages with substantial structural differences, such as English and Japanese, which is a subject-objectverb language (Xu et al. [sent-22, score-0.242]
</p><p>8 Forest-based translation frameworks, which make use of packed parse forests on the source and/or target language side(s), are an increasingly promising approach to syntax-based SMT, being both algorithmically appealing (Mi et al. [sent-24, score-0.612]
</p><p>9 For example, even a single spurious word alignment can invalidate a large number of otherwise extractable rules, and unaligned words can result in an exponentially large set of extractable rules for the interpretation of these unaligned words (Galley et al. [sent-34, score-0.733]
</p><p>10 c A2s0s1o1ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 2 –31, which there were 237 target function words, which account for 76. [sent-42, score-0.262]
</p><p>11 Following these problematic alignments, we are forced to make use of relatively large English tree fragments to construct translation rules that tend to be ill-formed and less generalized. [sent-46, score-0.503]
</p><p>12 This is the motivation of the present approach of re-aligning the target function words to source tree fragments, so that the influence of incorrect alignments is reduced and the function words can be generated by tree fragments on the fly. [sent-47, score-0.813]
</p><p>13 Therefore, we realign target function words to a packed forest that compactly encodes exponentially many parses. [sent-51, score-0.862]
</p><p>14 Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. [sent-52, score-1.107]
</p><p>15 In order to constrain the exhaustive attachments of function words, we further limit the function words to bind to their surrounding chunks yielded by a dependency parser. [sent-53, score-0.54]
</p><p>16 Using the composed rules of the present study in a baseline forest-to-string translation system results in a 1. [sent-54, score-0.407]
</p><p>17 1 Japanese function words In the present paper, we limit our discussion on Japanese particles and auxiliary verbs (Martin, 1975). [sent-57, score-0.294]
</p><p>18 Alike English, the extra meaning provided by a Japanese auxiliary verb alters the basic meaning of the main verb so that the main verb has one or more of the following functions: passive voice, progressive aspect, perfect aspect, modality, dummy, or emphasis. [sent-62, score-0.235]
</p><p>19 As such, an HPSG parse forest can be considered to be a forest of signs. [sent-72, score-0.817]
</p><p>20 Making use of these signs instead of part-of-speech (POS)/phrasal tags in PCFG results in a fine-grained rule set integrated with deep syntactic information. [sent-73, score-0.283]
</p><p>21 For simplicity, we only draw the identifiers for the signs of the nodes in the HPSG forest. [sent-75, score-0.287]
</p><p>22 html 3The forest includes three parse trees rooted at c0, c1, and c2. [sent-90, score-0.588]
</p><p>23 The chunk-level  3  Composed Rule Extraction  In this section, we first describe an algorithm that attaches function words to a packed forest guided by target chunk information. [sent-98, score-0.857]
</p><p>24 Then, we identify minimal and composed rules from the derivation forest and estimate the probabilities of rules and scores of derivations using the expectation-maximization (EM) (Dempster et al. [sent-100, score-1.127]
</p><p>25 The composed rules are generated by combining a sequence of minimal rules. [sent-112, score-0.386]
</p><p>26 In the alignment of this example, three lines (in dot lines) are used to align was and the with ga (subject particle), and was with ta (past tense auxiliary verb). [sent-117, score-0.32]
</p><p>27 Under this alignment, we are forced to extract rules with relatively large tree fragments. [sent-118, score-0.296]
</p><p>28 , 2004), a rule rooted at c0 will take c7, t4, c4, c19, t2, and c15 as the leaves. [sent-120, score-0.258]
</p><p>29 In order to ensure that this rule is used during decoding, we must generate subtrees with a height of 7 for c0. [sent-122, score-0.26]
</p><p>30 Suppose that the input forest is binarized and that |E| is the average number oesft hyperedges do fa nedac thh node, itshe tnh we emrausgte generate subtrees4 for c0 in the worst case. [sent-123, score-0.462]
</p><p>31 25 the existence of these rules prevents the generaliza-  tion ability of the final rule set that is extracted. [sent-128, score-0.33]
</p><p>32 For example, by ignoring the ambiguous alignments on the Japanese function words, we enlarge the frontier set to include from 12 to 19 of the 24 non-terminal nodes. [sent-130, score-0.361]
</p><p>33 Consequently, the number of extractable minimal rules increases from 12 (with three reordering rules rooted at c0, c1, and c2) to 19 (with five reordering rules rooted at c0, c1, c2, c5, and c17). [sent-131, score-0.927]
</p><p>34 With more nodes included in the frontier set, we can extract more minimal and composed monotonic/reordering rules and avoid extracting the less generalized rules with extremely large tree fragments. [sent-132, score-0.986]
</p><p>35 In the proposed algorithm, we use a target chunk set to constrain the attachment explosion problem because we use a packed parse forest instead of a 1best tree, as in the case of (Galley et al. [sent-136, score-0.79]
</p><p>36 Multiple interpretations of unaligned function words for an aligned tree-string pair result in a derivation forest. [sent-138, score-0.636]
</p><p>37 Now, we have a packed parse forest in which each tree corresponds to a derivation forest. [sent-139, score-0.862]
</p><p>38 Thus,  pruning free attachments of function words is practically important in order to extract composed rules from this “(derivation) forest of (parse) forest”. [sent-140, score-0.845]
</p><p>39 In the English-to-Japanese translation test case of the present study, the target chunk set is yielded by a state-of-the-art Japanese dependency parser, Cabocha v0. [sent-141, score-0.422]
</p><p>40 A chunk contains roughly one content word (usually the head) and affixed function words, such as case markers (e. [sent-144, score-0.254]
</p><p>41 Moreover, we also hope to gain a fine-grained alignment among these syntactic chunks and source tree fragments. [sent-151, score-0.325]
</p><p>42 Thereby, during decoding, we are binding the generation of function words with the generation of target chunks. [sent-152, score-0.297]
</p><p>43 3  The algorithm  Algorithm constructing  1 outlines the proposed  approach to  a derivation forest to include multiple  interpretations of target function words. [sent-156, score-0.875]
</p><p>44 The derivation forest is a hypergraph  as previously  used in  (Galley et al. [sent-157, score-0.526]
</p><p>45 , 2006), to maintain the constraint that one unaligned target word be attached to some node v exactly once in one derivation tree. [sent-158, score-0.595]
</p><p>46 Starting from a triple ⟨FS, T, A⟩, we first tailor the alignment A tao Arip′ by removing t,h we alignments rfo trh target nfmunecnttio An words. [sent-159, score-0.334]
</p><p>47 , During trshee traversal, a ∈fun Pction word fw will be attached to v if 1) t(v) overlaps with the span of the chunk to which fw belongs, and 2) fw has not been attached to the descendants of v. [sent-161, score-0.782]
</p><p>48 We identify translation rules that take v as the root of their tree fragments. [sent-162, score-0.429]
</p><p>49 Each tree fragment is a frontier tree that takes a node in the frontier set PA′ toief FS as tthhaet tr toaokte sno ade n aodnde ninon th-leex ficroanlitzieedr sfreton Ptier nodes or lexicalized non-frontier nodes as the leaves. [sent-163, score-0.978]
</p><p>50 Also, a minimal frontier tree used in a minimal rule is limited to be a frontier tree such that all nodes other than the root and leaves are non-frontier nodes. [sent-164, score-1.101]
</p><p>51 We use Algorithm 1 described in (Mi and Huang, 2008) to collect minimal frontier trees rooted at v in FS. [sent-165, score-0.404]
</p><p>52 rent set of hyperedges forms a minimal frontier tree. [sent-168, score-0.345]
</p><p>53 In the derivation forest, we use ⊕ nodes to manage minimal/composed tr,u lwese uthseat ⊕ ⊕sh naorede tsh teo same node and the same corresponding span. [sent-169, score-0.381]
</p><p>54 Figure 2 shows some minimal rule and ⊕ nodes derived from tshheo example mini Figure u1. [sent-170, score-0.435]
</p><p>55 l Even though we bind function words to their nearby chunks, these function words may still be attached to relative large tree fragments, so that richer syntactic information can be used to predict the function words. [sent-171, score-0.705]
</p><p>56 For example, in Figure 2, the tree fragments rooted at node c00−8 can predict ga and/or ta. [sent-172, score-0.463]
</p><p>57 The syntactic foundation behind is that, whether to use ga as a subject particle or to use wo as an object particle depends on both the left-hand-side noun phrase (kekka) and the right-hand-side verb (kensyou sa re ta). [sent-173, score-0.268]
</p><p>58 This type of node v′ (such as c00−8) should satisfy the following two heuristic conditions: • v′ is included in the frontier set PA′ of FS, and t(v′) covers the function word, or v′ is the root nt(ovde of FS ifthe function word is the beginning or ending word in the target sentence T. [sent-174, score-0.614]
</p><p>59 Starting from this derivation forest with minimal •  Figure 2: Illustration of a (partial) derivation forest. [sent-175, score-0.789]
</p><p>60 Gray nodes include some unaligned target function word(s). [sent-176, score-0.597]
</p><p>61 rules as nodes, we can further combine two or more minimal rules to form composed rules nodes and can append these nodes to the derivation forest. [sent-178, score-1.183]
</p><p>62 3 Estimating rule probabilities We use the EM algorithm to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. [sent-180, score-0.709]
</p><p>63 , 2006) to estimate rule probabilities in derivation forests, is an iterative procedure and prefers shorter derivations containing large rules over longer derivations containing small rules. [sent-182, score-0.581]
</p><p>64 In order to overcome this bias problem, we discount the fractional count of a rule  by the product of the probabilities of parse hyperedges that are included in the tree fragment of the rule. [sent-183, score-0.531]
</p><p>65 , 2008) that makes use of forestbased translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. [sent-186, score-0.53]
</p><p>66 We analyzed the performance of the proposed translation rule sets by 27 TrainDev. [sent-187, score-0.298]
</p><p>67 1, we generated 987,401 1-best trees and 984,73 1 parse forests for the English sentences in the training set, with successful parse rates of 99. [sent-205, score-0.333]
</p><p>68 Using the pruning criteria expressed in (Mi and Huang, 2008), we continue to prune a parse forest by setting pe to be 8, 5, and 2, until there are no more than e10 = 22, 026 trees in a forest. [sent-208, score-0.495]
</p><p>69 jp C3-TM&H-FMin-FC3-F;  free fwYNYY alignment A′ A A′ A′ English side tree forest forest forest # rule86. [sent-214, score-1.397]
</p><p>70 Here, fw denotes function word, and DT denotes the decoding time, and the BLEU scores were computed on the test set. [sent-258, score-0.37]
</p><p>71 5M translation rules from the training set for the 4K English sentences in the development and test sets. [sent-268, score-0.298]
</p><p>72 2  Results  Table 3 lists the statistics of the following translation rule sets: •  C3-T: a composed rule set extracted from the dCe3r-ivT:at aion co fmoproesstesd o rfu e1-sb eetst e xHtrPaSctGed trees t thhaet were constructed using the approach described in (Galley et al. [sent-271, score-0.673]
</p><p>73 The maximum number of internal nodes is set to be three when generating a composed rule. [sent-273, score-0.267]
</p><p>74 We free attach target function words to derivation forests; 28  # of tree nodes in rule  Figure 3: Distributions of the number of tree nodes in the translation rule sets. [sent-274, score-1.454]
</p><p>75 Note that the curves of Min-F and C3-F are duplicated when the number of tree nodes being larger than 9. [sent-275, score-0.289]
</p><p>76 •  M&H-F;: a minimal rule set extracted from HMP&SHG-; Ffo:re asts m using tlh reu extracting algorithm omf (Mi and Huang, 2008). [sent-276, score-0.277]
</p><p>77 3 to attach unaligned words to some node(s) in the forest;  •  •  Min-F: a minimal rule set extracted from the dMeriniv-aFt:io an mfoinreimstsa o ruf HeP sSetG e xfotrraecsttesd th fraotm were constructed using Algorithm 1 (Section 3). [sent-280, score-0.454]
</p><p>78 C3-F: a composed rule set extracted from the dCe3r-ivF:at aion co fmorpeosstse do fr uHlePS seGt feoxtrerastcst. [sent-281, score-0.322]
</p><p>79 the number of rules, the number of reordering rules, and the distributions of the number of tree nodes (Figure 3), i. [sent-284, score-0.324]
</p><p>80 , more rules with relatively small tree fragments are preferred; 2. [sent-286, score-0.37]
</p><p>81 The advantage of using a packed forest for re-alignment is  verified by comparing the statistics of the rules and  C3-T  M&H-F;  Min-F  C3-F  Figure 4: Comparison of decoding time and the number of rules used for translating the test set. [sent-290, score-1.007]
</p><p>82 Using the composed rule set C3-F in our forestbased decoder, we achieved an optimal BLEU score of 28. [sent-292, score-0.324]
</p><p>83 Taking M&H-F; as the baseline translation rule set, we achieved a significant improvement (p < 0. [sent-294, score-0.298]
</p><p>84 In terms of decoding time, even though we used Algorithm 3 described in (Huang and Chiang, 2005), which lazily generated the N-best translation can-  didates, the decoding time tended to be increased because more rules were available during cubepruning. [sent-297, score-0.42]
</p><p>85 Figure 4 shows a comparison of decoding time (seconds per sentence) and the number of rules used for translating the test set. [sent-298, score-0.262]
</p><p>86 (2006) first used derivation forests of aligned tree-string pairs to express multiple interpretations of unaligned target words. [sent-305, score-0.776]
</p><p>87 The EM algorithm was used to jointly estimate 1) the translation probabilities and fractional counts of rules and 2) the scores of derivations in the derivation forests. [sent-306, score-0.544]
</p><p>88 By dealing with the ambiguous word alignment instead of unaligned target words, syntax-  based re-alignment models were proposed by (May 29 and Knight, 2007; Wang et al. [sent-307, score-0.391]
</p><p>89 Free attachment of the unaligned target word problem was ignored in (Mi and Huang, 2008), which was the first study on extracting tree-to-string rules from aligned forest-string pairs. [sent-309, score-0.557]
</p><p>90 This inspired the idea to re-align a packed forest and a target sentence. [sent-310, score-0.641]
</p><p>91 Thus, we focus on the realignment of target function words to source tree fragments and use a dependency parser to limit the attachments of unaligned target words. [sent-312, score-0.917]
</p><p>92 6  Conclusion  We have proposed an effective use of target function words for extracting generalized transducer rules for forest-based translation. [sent-313, score-0.427]
</p><p>93 , 2006) from the 1-best tree to the packed parse forest. [sent-315, score-0.336]
</p><p>94 A simple yet effective modification is that, during rule  extraction, we account for multiple interpretations of both aligned and unaligned target function words. [sent-316, score-0.778]
</p><p>95 That is, we chose to loose the ambiguous alignments for all of the target function words. [sent-317, score-0.343]
</p><p>96 The consideration behind is in order to generate target function words in a robust manner. [sent-318, score-0.262]
</p><p>97 In order to avoid generating too large a derivation forest for a packed forest, we further used chunk-level information yielded by a target dependency parser. [sent-319, score-0.871]
</p><p>98 The present work only re-aligns target function words to source tree fragments. [sent-324, score-0.393]
</p><p>99 It will be valuable to investigate the feasibility to re-align all the target words to source tree fragments. [sent-325, score-0.259]
</p><p>100 Given source parse forests and a target word set for re-aligning beforehand, we argue our approach is generic and applicable to any language pairs. [sent-327, score-0.341]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('forest', 0.375), ('hpsg', 0.201), ('unaligned', 0.177), ('fw', 0.175), ('japanese', 0.171), ('rule', 0.165), ('rules', 0.165), ('galley', 0.161), ('nodes', 0.158), ('derivation', 0.151), ('forests', 0.146), ('frontier', 0.146), ('mi', 0.144), ('packed', 0.138), ('function', 0.134), ('translation', 0.133), ('tree', 0.131), ('target', 0.128), ('fs', 0.119), ('minimal', 0.112), ('composed', 0.109), ('particles', 0.108), ('ga', 0.093), ('rooted', 0.093), ('interpretations', 0.087), ('hyperedges', 0.087), ('aligned', 0.087), ('huang', 0.086), ('alignment', 0.086), ('bleu', 0.083), ('chunk', 0.082), ('alignments', 0.081), ('signs', 0.077), ('fragments', 0.074), ('node', 0.072), ('attached', 0.067), ('verified', 0.067), ('parse', 0.067), ('chunks', 0.067), ('closure', 0.064), ('bind', 0.064), ('extractable', 0.064), ('attachments', 0.062), ('decoding', 0.061), ('passive', 0.057), ('jp', 0.055), ('sag', 0.055), ('ta', 0.053), ('trees', 0.053), ('identifiers', 0.052), ('wu', 0.052), ('auxiliary', 0.052), ('forestbased', 0.05), ('derivations', 0.05), ('aion', 0.048), ('kekka', 0.048), ('kensyou', 0.048), ('mext', 0.048), ('realign', 0.048), ('realignment', 0.048), ('symmetrizing', 0.048), ('xianchao', 0.048), ('height', 0.048), ('knight', 0.048), ('subtrees', 0.047), ('particle', 0.046), ('fractional', 0.045), ('yielded', 0.044), ('qun', 0.044), ('liu', 0.044), ('smt', 0.043), ('eac', 0.042), ('hce', 0.042), ('jst', 0.042), ('em', 0.042), ('verb', 0.042), ('haitao', 0.041), ('chiang', 0.041), ('syntactic', 0.041), ('span', 0.041), ('tsujii', 0.04), ('utiyama', 0.039), ('tailor', 0.039), ('gray', 0.039), ('compactly', 0.039), ('pollard', 0.039), ('markers', 0.038), ('frameworks', 0.037), ('cabocha', 0.037), ('tense', 0.036), ('tv', 0.036), ('translating', 0.036), ('jun', 0.036), ('fragment', 0.036), ('generalization', 0.035), ('reordering', 0.035), ('decoder', 0.035), ('nh', 0.035), ('binding', 0.035), ('dependency', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="110-tfidf-1" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>2 0.33941695 <a title="110-tfidf-2" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>3 0.33286569 <a title="110-tfidf-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.33189252 <a title="110-tfidf-4" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>5 0.28750649 <a title="110-tfidf-5" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>6 0.26005137 <a title="110-tfidf-6" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>7 0.23399195 <a title="110-tfidf-7" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>8 0.20802659 <a title="110-tfidf-8" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>9 0.17210566 <a title="110-tfidf-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.16454519 <a title="110-tfidf-10" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>11 0.16273759 <a title="110-tfidf-11" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>12 0.15694788 <a title="110-tfidf-12" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>13 0.15634091 <a title="110-tfidf-13" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>14 0.15414292 <a title="110-tfidf-14" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>15 0.15044482 <a title="110-tfidf-15" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>16 0.13533597 <a title="110-tfidf-16" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>17 0.12882377 <a title="110-tfidf-17" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>18 0.12233678 <a title="110-tfidf-18" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>19 0.12161637 <a title="110-tfidf-19" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>20 0.1193803 <a title="110-tfidf-20" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.306), (1, -0.297), (2, 0.158), (3, -0.045), (4, 0.062), (5, 0.018), (6, -0.262), (7, -0.075), (8, -0.078), (9, -0.07), (10, -0.047), (11, -0.026), (12, -0.004), (13, 0.046), (14, 0.051), (15, -0.086), (16, 0.046), (17, 0.021), (18, -0.052), (19, 0.02), (20, -0.098), (21, -0.053), (22, -0.019), (23, 0.105), (24, 0.076), (25, -0.056), (26, 0.005), (27, 0.054), (28, -0.043), (29, -0.009), (30, -0.025), (31, -0.096), (32, -0.002), (33, -0.013), (34, -0.042), (35, 0.026), (36, -0.144), (37, -0.105), (38, -0.115), (39, -0.077), (40, -0.06), (41, 0.028), (42, -0.029), (43, 0.063), (44, -0.022), (45, 0.011), (46, 0.051), (47, -0.01), (48, 0.012), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96164411 <a title="110-lsi-1" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>2 0.92603099 <a title="110-lsi-2" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>3 0.87422246 <a title="110-lsi-3" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>4 0.86756444 <a title="110-lsi-4" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>5 0.83959532 <a title="110-lsi-5" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>6 0.79954588 <a title="110-lsi-6" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>7 0.79896468 <a title="110-lsi-7" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>8 0.79010433 <a title="110-lsi-8" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>9 0.7837885 <a title="110-lsi-9" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>10 0.70868212 <a title="110-lsi-10" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>11 0.66061127 <a title="110-lsi-11" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>12 0.63748485 <a title="110-lsi-12" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>13 0.62828064 <a title="110-lsi-13" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>14 0.57040733 <a title="110-lsi-14" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>15 0.57009971 <a title="110-lsi-15" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>16 0.56536007 <a title="110-lsi-16" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>17 0.56476492 <a title="110-lsi-17" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>18 0.54926348 <a title="110-lsi-18" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>19 0.53996462 <a title="110-lsi-19" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>20 0.52666229 <a title="110-lsi-20" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.035), (17, 0.123), (26, 0.035), (28, 0.011), (31, 0.012), (34, 0.01), (37, 0.081), (39, 0.063), (41, 0.064), (53, 0.017), (55, 0.034), (59, 0.034), (67, 0.15), (72, 0.031), (91, 0.032), (96, 0.194)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90886533 <a title="110-lda-1" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>same-paper 2 0.86703956 <a title="110-lda-2" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>3 0.84737575 <a title="110-lda-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.84593374 <a title="110-lda-4" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>Author: Qin Gao ; Stephan Vogel</p><p>Abstract: We present an approach of expanding parallel corpora for machine translation. By utilizing Semantic role labeling (SRL) on one side of the language pair, we extract SRL substitution rules from existing parallel corpus. The rules are then used for generating new sentence pairs. An SVM classifier is built to filter the generated sentence pairs. The filtered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with baseline models. Experimental results on ChineseEnglish machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.</p><p>5 0.84180188 <a title="110-lda-5" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>Author: Yashar Mehdad ; Matteo Negri ; Marcello Federico</p><p>Abstract: This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.</p><p>6 0.8391242 <a title="110-lda-6" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>7 0.8354671 <a title="110-lda-7" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>8 0.83485997 <a title="110-lda-8" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>9 0.83145696 <a title="110-lda-9" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>10 0.8308351 <a title="110-lda-10" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>11 0.82932806 <a title="110-lda-11" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>12 0.82914877 <a title="110-lda-12" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>13 0.82908577 <a title="110-lda-13" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>14 0.82728171 <a title="110-lda-14" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>15 0.82536316 <a title="110-lda-15" href="./acl-2011-Language-independent_compound_splitting_with_morphological_operations.html">193 acl-2011-Language-independent compound splitting with morphological operations</a></p>
<p>16 0.82502794 <a title="110-lda-16" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>17 0.82495487 <a title="110-lda-17" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>18 0.82448244 <a title="110-lda-18" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>19 0.82375574 <a title="110-lda-19" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>20 0.82346565 <a title="110-lda-20" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
