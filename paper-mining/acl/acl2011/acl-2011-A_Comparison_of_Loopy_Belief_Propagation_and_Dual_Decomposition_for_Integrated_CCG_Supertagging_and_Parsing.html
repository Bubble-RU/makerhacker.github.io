<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-5" href="#">acl2011-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</h1>
<br/><p>Source: <a title="acl-2011-5-pdf" href="http://aclweb.org/anthology//P/P11/P11-1048.pdf">pdf</a></p><p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>Reference: <a title="acl-2011-5-reference" href="../acl2011_reference/acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk @ Abstract Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. [sent-5, score-0.867]
</p><p>2 Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. [sent-6, score-0.378]
</p><p>3 To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. [sent-7, score-0.609]
</p><p>4 1 Introduction Accurate and efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics, due to the complexities associated its mild context sensitivity. [sent-11, score-0.116]
</p><p>5 Even for practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants. [sent-12, score-0.17]
</p><p>6 The most successful approach to CCG parsing is based on a pipeline strategy (§2). [sent-17, score-0.166]
</p><p>7 We show experimentally that this pipeline significantly lowers the upper bound on parsing accuracy (§3). [sent-24, score-0.228]
</p><p>8 The same experiment shows that the supertagger prunes many bad parses. [sent-25, score-0.618]
</p><p>9 So, while we want to avoid the error propagation inherent to a pipeline, ideally we still want to benefit from the key insight of supertagging: that a sequence model over lexical categories can be quite accurate. [sent-26, score-0.186]
</p><p>10 Our solution  is to combine the features of both the supertagger and the parser into a single, less aggressively pruned model. [sent-27, score-0.606]
</p><p>11 The challenge with this model is its prohibitive complexity, which we address with approximate methods: dual decomposition and belief propagation (§4). [sent-28, score-0.641]
</p><p>12 In both cases our model significantly outperforms the pipeline approach, leading to the best published results in CCG parsing (§5). [sent-31, score-0.166]
</p><p>13 Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). [sent-43, score-0.393]
</p><p>14 The  posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. [sent-44, score-0.21]
</p><p>15 A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004a). [sent-46, score-0.294]
</p><p>16 It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories 471 only if the parser fails to find an analysis. [sent-47, score-0.834]
</p><p>17 However, the technique is inherently approximate: it will return a lower probability parse under the parsing model if a higher probability parse can only be constructed from a supertag sequence returned by a subsequent iteration. [sent-50, score-0.335]
</p><p>18 In this  way it prioritizes speed over exactness, although the tradeoff can be modified by adjusting the beam step function. [sent-51, score-0.163]
</p><p>19 We will also explore reverse adaptive supertagging, a much less aggressive pruning method that seeks only to make sentences parseable when they otherwise would not be due to an impractically large search space. [sent-53, score-0.23]
</p><p>20 To answer this question we computed oracle best and worst values for labelled dependency F-score using the algorithm of Huang (2008) on the hybrid model of Clark and Curran (2007), the best model of their C&C; parser. [sent-57, score-0.164]
</p><p>21 We computed the oracle on our development data, Section 00 of CCGbank (Hockenmaier and Steedman, 2007), using both AST and Reverse AST beams settings shown in Table 1. [sent-58, score-0.158]
</p><p>22 The results (Table 2) show that the oracle best  accuracy  is more than 3% higher pruning. [sent-59, score-0.139]
</p><p>23 1 In fact, it is al-  for reverse AST  than the aggressive  AST  most as high as the upper bound oracle accuracy of  97. [sent-60, score-0.33]
</p><p>24 73%  obtained using perfect supertags—in  other  words, the search space for reverse AST is theoretically  near-optimal. [sent-61, score-0.115]
</p><p>25 idealized oracle reproduces a result from Clark and Cur-  2This  Condition  Parameter  Iteration 1  2  3  4  5  ASTk (βdic (btieoanmar wyi cduttho)ff)0. [sent-64, score-0.11]
</p><p>26 Parameter β is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words seen less than k times. [sent-75, score-0.134]
</p><p>27 r 3d6 Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle F-scores on CCGbank Section 00. [sent-89, score-0.404]
</p><p>28 The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the the number of lexical categories per word used (from first to last parsing attempt). [sent-90, score-0.224]
</p><p>29 Supertagger beam  Supertagger beam  Figure 1: Comparison between model score and Viterbi F-score (left); and between model score and oracle F-score (right) for different supertagger beams on a subset of CCGbank Section 00. [sent-91, score-0.955]
</p><p>30 worst accuracy is much lower in the reverse setting. [sent-92, score-0.144]
</p><p>31 It is clear that the supertagger pipeline has two effects: while it beneficially prunes many bad parses, it harmfully prunes some very good parses. [sent-93, score-0.728]
</p><p>32 We can also see from the scores of the Viterbi parses that while the reverse condition has access to much better parses, the model doesn’t actually find them. [sent-94, score-0.208]
</p><p>33 Digging deeper, we compared parser model score against Viterbi F-score and oracle F-score at a varan (2004b). [sent-96, score-0.187]
</p><p>34 The reason that using the gold-standard supertags doesn’t result in 100% oracle parsing accuracy is that some of the development set parses cannot be constructed by the learned grammar. [sent-97, score-0.426]
</p><p>35 472 riety of fixed beam settings (Figure 1), considering only the subset of our development set which could be parsed with all beam settings. [sent-98, score-0.268]
</p><p>36 The inverse relationship between model score and F-score shows that the supertagger restricts the parser to mostly  good parses (under F-measure) that the model would otherwise disprefer. [sent-99, score-0.672]
</p><p>37 However, when the supertagger makes a mistake, the parser cannot recover. [sent-101, score-0.606]
</p><p>38 4 Integrated Supertagging and Parsing The supertagger obviously has good but not perfect predictive features. [sent-102, score-0.529]
</p><p>39 An obvious way to exploit this without being bound by its decisions is to incorporate these features directly into the parsing model. [sent-103, score-0.149]
</p><p>40 In our case both the parser and the supertagger are feature-based models, so from the perspective of a single parse tree, the change is simple: the tree is simply scored by the weights corresponding to all of its active features. [sent-104, score-0.64]
</p><p>41 However, since the features of the supertagger are all Markov features on adjacent supertags, the change has serious implications for search. [sent-105, score-0.529]
</p><p>42 Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al. [sent-110, score-0.819]
</p><p>43 , 2010, inter alia), applied to dependency parsing by Koo et al. [sent-112, score-0.116]
</p><p>44 (2010) and lexicalized CFG parsing by Rush et al. [sent-113, score-0.116]
</p><p>45 We apply both techniques to our integrated supertagging and parsing model. [sent-115, score-0.414]
</p><p>46 1 Loopy Belief Propagation Belief propagation (BP) is an algorithm for computing marginals (i. [sent-117, score-0.204]
</p><p>47 3 Our use of belief propagation builds directly on these two familiar algorithms. [sent-123, score-0.311]
</p><p>48 3Forward-backward and inside-outside are formally shown to be special cases of belief propagation by Smyth et al. [sent-124, score-0.311]
</p><p>49 BP is usually understood as an algorithm on bipartite factor graphs, which structure a global function into local functions over subsets of variables (Kschischang et al. [sent-128, score-0.114]
</p><p>50 Variables maintain a belief (expectation) over a distribution of values and  BP passes messages about these beliefs between variables and factors. [sent-130, score-0.36]
</p><p>51 The idea is to iteratively update each variable’s beliefs based on the beliefs of neighboring variables (through a shared factor), using the sum-product rule. [sent-131, score-0.171]
</p><p>52 The message mf→x from a factor to a variable is mf→x(x) =  X ∼X{x}  f(X)  Y  my→f(y)  (2)  y∈Yn(f)\x  where ∼ {x} represents all variables other than x, wXh = n(f) xan}d r f(X) nitss st ahell s veatr oiafb arguments hoafn nth xe, factor function f. [sent-133, score-0.22]
</p><p>53 Making this concrete, our supertagger defines a distribution over tags T0. [sent-134, score-0.529]
</p><p>54 The message fi a variable Ti receives from its neighbor to the left corresponds to the forward prob-  ability, while messages from the right correspond to backward probability bi. [sent-144, score-0.141]
</p><p>55 fi(Ti) =  Xfi−1(Ti−1)ei−1(Ti−1)ti(Ti−1,Ti)  (3)  TXi−1  bi(Ti) =  Xbi+1(Ti+1)ei+1(Ti+1)ti+1(Ti,Ti+1)  (4)  TXi+1  Figure 3: Factor graph for the combined parsing and supertagging model. [sent-145, score-0.417]
</p><p>56 The current belief Bx(x) for variable x can be computed by taking the normalized product of all its in-  coming messages. [sent-146, score-0.179]
</p><p>57 Bx(x) =Z1h∈Yn(x)mh→x(x)  (5)  In the supertagger model, this is just:  p(Ti) =Z1fi(Ti)bi(Ti)ei(Ti)  (6)  Our parsing model is also a distribution over variables Ti, along with an additional quadratic number of span(i, j) variables. [sent-147, score-0.698]
</p><p>58 When a factor graph is a tree as in Figure 2, BP converges in a single iteration to the exact marginals. [sent-152, score-0.181]
</p><p>59 Under certain assumptions this loopy BP it will converge to approximate marginals that are bounded under an in-  terpretation from statistical physics (Yedidia et al. [sent-154, score-0.212]
</p><p>60 The TREE factor exchanges inside ni and outside oi messages with the tag and span variables, taking into account beliefs from the sequence model. [sent-156, score-0.189]
</p><p>61 Once all forward-backward and inside-outside probabilities have been calculated the belief of supertag Ti can be computed as the product of all incoming messages. [sent-159, score-0.296]
</p><p>62 search) algorithm for problems that can be decomposed into exactly solvable subproblems: in our case, supertagging and parsing. [sent-167, score-0.262]
</p><p>63 Formally, given Y as the set of valid parses, Z as the set of valid supertag sequences, and T as the set of supertags, we want to solve the following optimization for parser f(y) and supertagger g(z). [sent-168, score-0.723]
</p><p>64 ayr∈gY,mz∈aZxf(y) + g(z)  (9)  such that y(i, t) = z(i, t) for all (i, t) ∈ I (10) Here y(i, t) is a binary function indicating whether word iis assigned supertag t by the parser, for the set I= {(i, t) : i ∈ 1. [sent-169, score-0.117]
</p><p>65 To enforce the constraint that the parser and supertagger agree on a tag sequence we introduce Lagrangian multipliers u = {u(i, t) : (i, t) ∈ I} and construct a dual objective over )va :ri (aib,tle)s u(i, t). [sent-173, score-0.774]
</p><p>66 (2010) for their tagging and parsing problem (essentially a perceptron update). [sent-176, score-0.116]
</p><p>67 (2010) we used the highest scoring output of the parsing submodel over all iterations. [sent-179, score-0.116]
</p><p>68 We use the C&C; parser (Clark and Curran, 2007) and its supertagger (Clark, 2002). [sent-181, score-0.606]
</p><p>69 Our baseline is the hybrid model of Clark and Curran (2007); our integrated model simply adds the supertagger features to this model. [sent-182, score-0.565]
</p><p>70 The parser relies solely on the supertagger for pruning, using CKY for search over the pruned space. [sent-183, score-0.606]
</p><p>71 We also used a more permissive training supertagger beam (Table 3) than in previous work (Clark and Curran, 2007). [sent-187, score-0.663]
</p><p>72 We  use sections 02-21 (39603 sentences) for training, 4The u terms can be interpreted as the messages from factors to variables (Sontag et al. [sent-191, score-0.122]
</p><p>73 Evaluation is based on labelled and unlabelled predicate argument structure recovery and supertag accuracy. [sent-195, score-0.207]
</p><p>74 We combine the parser and the supertagger over the search space defined by the set of supertags within the supertagger beam (see Table 1); this avoids having to perform inference over the prohibitively large set of parses spanned by all supertags. [sent-200, score-1.44]
</p><p>75 Hence at each beam setting, the model operates over the same search space as the baseline; the difference is that we search with our integrated model. [sent-201, score-0.17]
</p><p>76 1 Parsing Accuracy We first experiment with the separately trained supertagger and parser, which are then combined using belief propagation (BP) and dual decomposition (DD). [sent-203, score-1.177]
</p><p>77 In line with our oracle experiment, these results demonstrate that we can coax more accurate parses from the larger search space provided by the reverse setting; the influence of the supertagger features allow us to exploit this advantage. [sent-206, score-0.82]
</p><p>78 S80T12 Table 4: Results for individually-trained submodels combined using dual decomposition (DD) or belief propagation (BP) for k iterations, evaluated by labelled and unlabelled F-score (LF/UF) and supertag accuracy (ST). [sent-233, score-0.912]
</p><p>79 4  Itera0ons  Figure 4: Labelled F-score of baseline (BL), belief propagation (BP), and dual decomposition (DD) on section 00. [sent-237, score-0.609]
</p><p>80 Next, we evaluate performance when using automatic part-of-speech tags as input to our parser 476 and supertagger (Table 5). [sent-243, score-0.606]
</p><p>81 Hence our combined model represents the best CCG parsing results under any setting. [sent-247, score-0.155]
</p><p>82 Finally, we revisit the oracle experiment of §3 using our lcyo,m wbei rneevdi smito thdeels o (Figure 5). [sent-248, score-0.138]
</p><p>83 2 Algorithmic Convergence Figure 4 shows that parse accuracy converges after a few iterations. [sent-251, score-0.134]
</p><p>84 BP converges when the marginals do not change between iterations, and DD converges when both submodels agree on all supertags. [sent-253, score-0.242]
</p><p>85 DD converges much faster, while BP in the reverse condition converges  quite slowly. [sent-255, score-0.284]
</p><p>86 This is interesting when contrasted with its behavior on parse accuracy—its rate of convergence after one iteration is 1. [sent-256, score-0.13]
</p><p>87 Petrov I-5 is based on the parser output of Fowler and Penn  (2010); we evaluate on sentences for which all parsers returned an analysis (2323 sentences for section  sentences for section 00). [sent-271, score-0.111]
</p><p>88 Supertagger beam  23  and  1834  Supertagger beam Figure 5: Comparison between model score and Viterbi F-score for the integrated model using belief propagation (left) and dual decomposition (right); the results are based on the same data as Figure 1. [sent-272, score-0.913]
</p><p>89 Figure 6: Rate ofconvergence for beliefpropagation (BP) and dual decomposition (DD) with maximum k = 1000. [sent-274, score-0.298]
</p><p>90 3 Parsing Speed Because the C&C; parser with AST is very fast, we wondered about the effect on speed for our model. [sent-278, score-0.109]
</p><p>91 4 Training the Integrated Model In the experiments reported so far, the parsing and supertagging models were trained separately, and only combined at test time. [sent-282, score-0.417]
</p><p>92 A possible reason for this is that we used a stricter supertagger beam setting during training (Clark and Curran, 2007) to make training on a single machine practical. [sent-326, score-0.663]
</p><p>93 Our empirical comparison of BP and DD also complements the theoretically-oriented comparison of marginal- and margin-based variational approxima478 tions for parsing described by Martins et al. [sent-337, score-0.116]
</p><p>94 We have shown that the aggressive pruning used in adaptive supertagging significantly harms the oracle performance of the parser, though it mostly prunes bad parses. [sent-339, score-0.576]
</p><p>95 Based on these findings, we combined parser and supertagger features into a single model. [sent-340, score-0.645]
</p><p>96 Using belief propagation and dual decomposition, we obtained more principled—and more accurate—approximations than a pipeline. [sent-341, score-0.479]
</p><p>97 Models combined using belief propagation achieve very good performance immediately, despite an initial convergence rate just over 1%, while dual decomposition produces comparable results after several iterations, and algorithmically converges more quickly. [sent-342, score-0.766]
</p><p>98 In future work we plan to integrate the POS tagger, which is crucial to parsing accuracy (Clark and  Curran, 2004b). [sent-345, score-0.145]
</p><p>99 Generative models for statistical parsing with Combinatory Categorial Grammar. [sent-484, score-0.116]
</p><p>100 On dual decomposition and linear programming relaxations for natural language processing. [sent-604, score-0.298]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('supertagger', 0.529), ('supertagging', 0.262), ('bp', 0.208), ('ccg', 0.202), ('belief', 0.179), ('dd', 0.169), ('ast', 0.169), ('dual', 0.168), ('curran', 0.162), ('clark', 0.161), ('beam', 0.134), ('propagation', 0.132), ('decomposition', 0.13), ('ti', 0.12), ('supertag', 0.117), ('parsing', 0.116), ('reverse', 0.115), ('oracle', 0.11), ('supertags', 0.105), ('ccgbank', 0.103), ('fowler', 0.088), ('parser', 0.077), ('sutton', 0.075), ('marginals', 0.072), ('converges', 0.071), ('steedman', 0.07), ('messages', 0.069), ('rush', 0.068), ('parses', 0.066), ('loopy', 0.066), ('hockenmaier', 0.063), ('sontag', 0.063), ('factor', 0.061), ('prunes', 0.06), ('beliefs', 0.059), ('np', 0.055), ('categories', 0.054), ('labelled', 0.054), ('bpk', 0.054), ('ddk', 0.054), ('variables', 0.053), ('combinatory', 0.052), ('finkel', 0.052), ('pipeline', 0.05), ('iteration', 0.049), ('beams', 0.048), ('sgd', 0.048), ('penn', 0.048), ('convergence', 0.047), ('viterbi', 0.047), ('message', 0.045), ('aggressive', 0.043), ('converge', 0.042), ('categorial', 0.041), ('pruning', 0.04), ('mcallester', 0.039), ('combined', 0.039), ('koo', 0.039), ('smith', 0.038), ('eisner', 0.038), ('dantzig', 0.036), ('kschischang', 0.036), ('txi', 0.036), ('yedidia', 0.036), ('unlabelled', 0.036), ('integrated', 0.036), ('iterations', 0.034), ('parse', 0.034), ('returned', 0.034), ('lf', 0.033), ('bound', 0.033), ('adaptive', 0.032), ('ei', 0.032), ('approximate', 0.032), ('category', 0.032), ('auli', 0.032), ('diagrams', 0.032), ('edikt', 0.032), ('kummerfeld', 0.032), ('wondered', 0.032), ('dyer', 0.031), ('yn', 0.031), ('petrov', 0.03), ('bangalore', 0.03), ('bx', 0.029), ('exactness', 0.029), ('felzenszwalb', 0.029), ('komodakis', 0.029), ('prioritizes', 0.029), ('accuracy', 0.029), ('expectations', 0.029), ('bad', 0.029), ('approximations', 0.028), ('martins', 0.028), ('dreyer', 0.028), ('revisit', 0.028), ('smyth', 0.028), ('submodels', 0.028), ('backward', 0.027), ('condition', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="5-tfidf-1" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>2 0.49730116 <a title="5-tfidf-2" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>3 0.39508933 <a title="5-tfidf-3" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>Author: Yue Zhang ; Stephen Clark</p><p>Abstract: CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C; parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C.; Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.</p><p>4 0.17780665 <a title="5-tfidf-4" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>Author: Alexander M. Rush and Michael Collins</p><p>Abstract: unkown-abstract</p><p>5 0.12586944 <a title="5-tfidf-5" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>6 0.11465282 <a title="5-tfidf-6" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>7 0.099593788 <a title="5-tfidf-7" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>8 0.095176466 <a title="5-tfidf-8" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>9 0.093637012 <a title="5-tfidf-9" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>10 0.082121991 <a title="5-tfidf-10" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>11 0.080241054 <a title="5-tfidf-11" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>12 0.080063805 <a title="5-tfidf-12" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>13 0.073575273 <a title="5-tfidf-13" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>14 0.071794122 <a title="5-tfidf-14" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>15 0.070221819 <a title="5-tfidf-15" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>16 0.070064321 <a title="5-tfidf-16" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>17 0.064762928 <a title="5-tfidf-17" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>18 0.061790008 <a title="5-tfidf-18" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>19 0.06157095 <a title="5-tfidf-19" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>20 0.061131779 <a title="5-tfidf-20" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.176), (1, -0.076), (2, -0.049), (3, -0.253), (4, -0.024), (5, -0.055), (6, -0.049), (7, 0.076), (8, 0.005), (9, -0.029), (10, 0.04), (11, 0.16), (12, 0.082), (13, -0.064), (14, -0.099), (15, 0.109), (16, 0.105), (17, -0.095), (18, -0.009), (19, -0.097), (20, 0.178), (21, 0.062), (22, 0.22), (23, -0.012), (24, -0.29), (25, 0.208), (26, 0.069), (27, 0.038), (28, 0.018), (29, 0.066), (30, -0.077), (31, -0.166), (32, -0.103), (33, 0.059), (34, -0.195), (35, 0.031), (36, 0.036), (37, -0.002), (38, -0.135), (39, -0.004), (40, -0.095), (41, -0.018), (42, -0.036), (43, 0.031), (44, -0.005), (45, 0.053), (46, -0.122), (47, 0.096), (48, -0.139), (49, -0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93300074 <a title="5-lsi-1" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>2 0.91834188 <a title="5-lsi-2" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>3 0.7693491 <a title="5-lsi-3" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>Author: Yue Zhang ; Stephen Clark</p><p>Abstract: CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C; parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C.; Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.</p><p>4 0.51646882 <a title="5-lsi-4" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>Author: Gisle Ytrestl</p><p>Abstract: This paper describes a backtracking strategy for an incremental deterministic transitionbased parser for HPSG. The method could theoretically be implemented on any other transition-based parser with some adjustments. In this paper, the algorithm is evaluated on CuteForce, an efficient deterministic shiftreduce HPSG parser. The backtracking strategy may serve to improve existing parsers, or to assess if a deterministic parser would benefit from backtracking as a strategy to improve parsing.</p><p>5 0.48483175 <a title="5-lsi-5" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.</p><p>6 0.45895723 <a title="5-lsi-6" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>7 0.36360532 <a title="5-lsi-7" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>8 0.35448381 <a title="5-lsi-8" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>9 0.34164837 <a title="5-lsi-9" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>10 0.32616147 <a title="5-lsi-10" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>11 0.31619853 <a title="5-lsi-11" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>12 0.30761844 <a title="5-lsi-12" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>13 0.30641279 <a title="5-lsi-13" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>14 0.30512437 <a title="5-lsi-14" href="./acl-2011-Dynamic_Programming_Algorithms_for_Transition-Based_Dependency_Parsers.html">107 acl-2011-Dynamic Programming Algorithms for Transition-Based Dependency Parsers</a></p>
<p>15 0.28096604 <a title="5-lsi-15" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>16 0.27725163 <a title="5-lsi-16" href="./acl-2011-Types_of_Common-Sense_Knowledge_Needed_for_Recognizing_Textual_Entailment.html">315 acl-2011-Types of Common-Sense Knowledge Needed for Recognizing Textual Entailment</a></p>
<p>17 0.27346039 <a title="5-lsi-17" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>18 0.26022401 <a title="5-lsi-18" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>19 0.25642133 <a title="5-lsi-19" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<p>20 0.25178048 <a title="5-lsi-20" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.077), (3, 0.16), (5, 0.036), (17, 0.048), (26, 0.014), (37, 0.099), (39, 0.083), (41, 0.051), (55, 0.032), (59, 0.037), (64, 0.038), (72, 0.025), (88, 0.012), (91, 0.039), (96, 0.094), (98, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79743761 <a title="5-lda-1" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>2 0.72904968 <a title="5-lda-2" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>3 0.70760322 <a title="5-lda-3" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>Author: Yue Zhang ; Stephen Clark</p><p>Abstract: CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C; parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C.; Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.</p><p>4 0.68335289 <a title="5-lda-4" href="./acl-2011-Predicting_Relative_Prominence_in_Noun-Noun_Compounds.html">249 acl-2011-Predicting Relative Prominence in Noun-Noun Compounds</a></p>
<p>Author: Taniya Mishra ; Srinivas Bangalore</p><p>Abstract: There are several theories regarding what influences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.</p><p>5 0.67967868 <a title="5-lda-5" href="./acl-2011-Lost_in_Translation%3A_Authorship_Attribution_using_Frame_Semantics.html">214 acl-2011-Lost in Translation: Authorship Attribution using Frame Semantics</a></p>
<p>Author: Steffen Hedegaard ; Jakob Grue Simonsen</p><p>Abstract: We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve clas- sifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.</p><p>6 0.67219913 <a title="5-lda-6" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>7 0.67153847 <a title="5-lda-7" href="./acl-2011-Extracting_and_Classifying_Urdu_Multiword_Expressions.html">134 acl-2011-Extracting and Classifying Urdu Multiword Expressions</a></p>
<p>8 0.66493452 <a title="5-lda-8" href="./acl-2011-A_Probabilistic_Modeling_Framework_for_Lexical_Entailment.html">22 acl-2011-A Probabilistic Modeling Framework for Lexical Entailment</a></p>
<p>9 0.66029215 <a title="5-lda-9" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>10 0.65409404 <a title="5-lda-10" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>11 0.64318192 <a title="5-lda-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.64305753 <a title="5-lda-12" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>13 0.64246571 <a title="5-lda-13" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>14 0.64199138 <a title="5-lda-14" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>15 0.64074212 <a title="5-lda-15" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>16 0.64050728 <a title="5-lda-16" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>17 0.64025223 <a title="5-lda-17" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>18 0.64009893 <a title="5-lda-18" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>19 0.63953668 <a title="5-lda-19" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>20 0.63868004 <a title="5-lda-20" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
