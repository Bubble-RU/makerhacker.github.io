<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-62" href="#">acl2011-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</h1>
<br/><p>Source: <a title="acl-2011-62-pdf" href="http://aclweb.org/anthology//P/P11/P11-4010.pdf">pdf</a></p><p>Author: Sara Stymne</p><p>Abstract: We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part ofMT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.</p><p>Reference: <a title="acl-2011-62-reference" href="../acl2011_reference/acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 se iu  Abstract We present BLAST, an open source tool for error analysis of machine translation (MT) output. [sent-3, score-0.454]
</p><p>2 BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. [sent-7, score-0.236]
</p><p>3 It is designed to be flexible, and can be used with any MT system, language pair, and error typology. [sent-8, score-0.213]
</p><p>4 The annotation task can be aided by highlighting similarities with a reference translation. [sent-9, score-0.275]
</p><p>5 1 Introduction Machine translation evaluation is a difficult task, since there is not only one correct translation of a sentence, but many equally good translation options. [sent-10, score-0.219]
</p><p>6 Thus, we advocate human error analysis of MT output, where humans identify and classify the problems in machine translated sentences. [sent-14, score-0.258]
</p><p>7 In this paper we present BLAST,1 a graphical tool for performing human error analysis, from any MT system and for any language pair. [sent-15, score-0.379]
</p><p>8 BLAST has a graphical user interface, and is designed to be easy 1The BiLingual Annotation/Annotator/Analysis Support Tool, available for download at http : / /www . [sent-16, score-0.153]
</p><p>9 It can aid the user by highlighting similarities with a reference sentence. [sent-20, score-0.318]
</p><p>10 BLAST is flexible in that it can be used with output from any MT system, and with any hierarchical error typology. [sent-21, score-0.303]
</p><p>11 To the best of our knowledge, there is no other publicly available tool for MT error annotation. [sent-23, score-0.335]
</p><p>12 Since we believe that error analysis is a vital complement to MT evaluation, we  think that BLAST can be useful for many other MT researchers and developers. [sent-24, score-0.312]
</p><p>13 0c 1 20 S1y1st Aems Doceiamtio n str foarti oCnosm, p augtaetsio 5n6a–l6 L1in,guistics human-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al. [sent-39, score-0.102]
</p><p>14 2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. [sent-42, score-0.213]
</p><p>15 There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al. [sent-43, score-0.401]
</p><p>16 , 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. [sent-45, score-0.134]
</p><p>17 It is also possible to focus error analysis at a specific problem, such as verb form errors (Murata et al. [sent-46, score-0.316]
</p><p>18 We have not been able to find any other freely available tool for error analysis of MT. [sent-48, score-0.357]
</p><p>19 (2006) mentioned in a footnote that “a tool for highlighting the differences [between the MT system and a correct translation] also proved to be quite useful” for error analysis. [sent-50, score-0.4]
</p><p>20 They do not describe this tool any further, and do not discuss if it was also used to mark and store the error annotations themselves. [sent-51, score-0.5]
</p><p>21 Some tools for post-editing of MT output, a related activity to error analysis, have been described in the literature. [sent-52, score-0.213]
</p><p>22 Font Llitj ´os and Carbonell (2004) presented an online tool for eliciting information from the user when post-editing sentences, in order to improve a rule-based translation system. [sent-53, score-0.304]
</p><p>23 The post-edit operations were labeled with error categories, making it a type of error analysis. [sent-54, score-0.426]
</p><p>24 This tool was highly connected to their translation system, and it required users to post-edit sentences by modifying word alignments, something that many users found difficult. [sent-55, score-0.287]
</p><p>25 (2008) described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns. [sent-57, score-0.122]
</p><p>26 The tool is a pure post-editing tool and the edits are not classified. [sent-58, score-0.244]
</p><p>27 Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and  2Though it does, at least in principle, HTER annotations for more information  seem  possible to mine  57 system comparison (Callison-Burch et al. [sent-59, score-0.246]
</p><p>28 3  System Overview  BLAST is a tool for human annotations of bilingual material. [sent-62, score-0.323]
</p><p>29 Its main purpose is error analysis for machine translation. [sent-63, score-0.259]
</p><p>30 It is not tied to the information provided by specific MT systems, or to specific languages, and it can be used with any hierarchical error typology. [sent-65, score-0.283]
</p><p>31 It has a preprocessing module for automatically aiding the annotator by highlighting similarities between the MT output and a reference. [sent-66, score-0.294]
</p><p>32 BLAST has three working modes for handling error annotations: for adding new annotations, for editing existing annotations, and for searching among annotations. [sent-68, score-0.286]
</p><p>33 BLAST can handle two types of annotations: er-  ror annotations and support annotations. [sent-69, score-0.217]
</p><p>34 Error annotations are based on a hierarchical error typology, and are used to annotate errors in MT output. [sent-70, score-0.483]
</p><p>35 Error annotations are added by the users of BLAST. [sent-71, score-0.211]
</p><p>36 Support annotations are used as a support to the user, currently to mark similarities in the system and reference sentences. [sent-72, score-0.364]
</p><p>37 The support annotations are normally created automatically by BLAST, but they can also be modified by the user. [sent-73, score-0.246]
</p><p>38 Both annotation types are stored with the indices of the words they apply to. [sent-74, score-0.152]
</p><p>39 A segment normally consists of a sentence and the MT output can be accompanied by a source sentence, a reference sentence, or both. [sent-77, score-0.13]
</p><p>40 Error annotations are marked in the segments by bold, underlined, colored text, and support annotations are marked by light background colors. [sent-78, score-0.517]
</p><p>41 The bottom part of the tool, contains the error typology, and controls for updating annotations and navigation. [sent-79, score-0.378]
</p><p>42 The error typology is shown using a menu structure, where submenus are activated by the user clicking on higher levels. [sent-80, score-0.804]
</p><p>43 58 The flexibility of the tool gives users a lot of freedom in how to use it in their evaluation projects. [sent-84, score-0.202]
</p><p>44 However, we believe that it is important within every error annotation project to use a set error typology and guidelines for annotation, but the annotation tool should not limit users in making these choices. [sent-85, score-1.135]
</p><p>45 2  Error Typologies  As described above, BLAST is easily configurable with new typologies for annotation, with the only restriction that the typology is hierarchical. [sent-87, score-0.498]
</p><p>46 Lower subtypologies are only shown when they are activated by the user clicking on a higher level. [sent-92, score-0.247]
</p><p>47 In Figure 1, the subtypologies to Word order were activated by the user first clicking on Word order, then on Phrase level. [sent-93, score-0.247]
</p><p>48 It is important that typologies are easy to extend and modify, especially in order to cover new target languages, since the translation problems to some extent will be dependent on the target language, for instance with regard to the different agreement phenomena in languages. [sent-94, score-0.261]
</p><p>49 The typologies that come with BLAST can serve as a starting point for adjusting typologies, especially to new target languages. [sent-95, score-0.215]
</p><p>50 3 Implementation BLAST is implemented as a Java application using Swing for the graphical user interface. [sent-97, score-0.153]
</p><p>51 BLAST has an object-oriented design, with a particular focus on modular design, to allow it to be easily extendible with new modules for preprocessing, reading and writing to different file formats, and presenting statistics. [sent-99, score-0.21]
</p><p>52 4 File formats The main file types used in BLAST is the annotation file, containing the translation segments and annotations, and the typology file. [sent-103, score-0.7]
</p><p>53 These files are stored in a simple text file format. [sent-104, score-0.187]
</p><p>54 The statistics of an annotation project 3http : / /www . [sent-106, score-0.132]
</p><p>55 html  59 are printed in a text file in a human-readable format (see Section 4. [sent-109, score-0.166]
</p><p>56 The annotation file contains the translation segments for the MT system, and possibly for the source and reference sentences, and all error and support annotations. [sent-111, score-0.674]
</p><p>57 The annotations are stored with the indices of the word(s) in the segments that were marked, and a label identifying the error type. [sent-112, score-0.478]
</p><p>58 The  annotation file is initially created automatically by BLAST based on sentence aligned files. [sent-113, score-0.238]
</p><p>59 It is then updated by BLAST with the annotations added by the user. [sent-114, score-0.165]
</p><p>60 4  Working with BLAST  BLAST has three different working modes: annotation, edit and search. [sent-117, score-0.104]
</p><p>61 The main mode is annotation, which allows the user to add new error annotations. [sent-118, score-0.43]
</p><p>62 The edit mode allows the user to edit and remove error annotations. [sent-119, score-0.548]
</p><p>63 The search mode allows the user to  search for errors of different types. [sent-120, score-0.348]
</p><p>64 BLAST can also create support annotations, that can later be updated by the user, and calculate and print statistics of an annotation project. [sent-121, score-0.184]
</p><p>65 1 Annotation The annotation mode is the main working mode in BLAST, and it is active in Figure 1. [sent-123, score-0.364]
</p><p>66 In annotation mode a segment is shown with all its current error annotations. [sent-124, score-0.448]
</p><p>67 The annotations are marked with bold and colored text, where the color depends on the main type of the error. [sent-125, score-0.247]
</p><p>68 For each new annotation the user selects the word or words that are wrong, and selects an error type. [sent-126, score-0.425]
</p><p>69 In figure 1, the words no television, and the error type Word order→Phrase level→Long are sheele ecrtreodr tiyn poerd Wero rtod a odrdd a new error annotation. [sent-127, score-0.426]
</p><p>70 BLAST ignores identical annotations, and warns the user if they try to add an annotation for the exact same words as another annotation. [sent-128, score-0.212]
</p><p>71 2 Edit In edit mode the user can change existing error annotations. [sent-130, score-0.489]
</p><p>72 In this mode only one annotation at a time is shown, and the user can switch between them. [sent-131, score-0.32]
</p><p>73 For each annotation affected words are highlighted, and  the error typology area shows the type of the error. [sent-132, score-0.626]
</p><p>74 The currently shown error can be changed to a different error type, or it can be removed. [sent-133, score-0.466]
</p><p>75 The edit mode is useful for revising annotations, and for correcting annotation errors. [sent-134, score-0.27]
</p><p>76 3 Search In search mode, it is possible to search for errors of a certain type. [sent-136, score-0.131]
</p><p>77 To search, users choose the error type they want to search for in the error typology, and then search backwards or forwards for error annotations of that type. [sent-137, score-0.922]
</p><p>78 It is possible both to search for specific errors deep in the typology, and to search for all errors of a type higher in the typology, for instance, to search for all word order errors, regardless of subclassification. [sent-138, score-0.248]
</p><p>79 4 Support annotations Error annotation is a hard task for humans, and thus we try to aid it by including automatic preprocessing, where similarities between the system and refer-  ence sentences are marked at different levels of similarity. [sent-142, score-0.393]
</p><p>80 Even if the goal of the error analysis often is not to compare the MT output to a single reference, but to the closest correct equivalent, it can still be useful to be able to see the similarities to one reference sentence, to be able to identify problematic parts easier. [sent-143, score-0.372]
</p><p>81 All these modules work on lower-cased data, so we added a 60 module for exact matching with the original casing kept. [sent-146, score-0.142]
</p><p>82 The paraphrase module is based on an automatic paraphrase induction method (Bannard and Callison-Burch, 2005), it is currently trained for five languages, but the Meteor-NEXT  code for training it for additional languages is included. [sent-149, score-0.203]
</p><p>83 Support annotations are normally only created automatically, but BLAST allows the user to edit them. [sent-150, score-0.362]
</p><p>84 The mechanism for adding, removing or changing support annotations is separate from error annotations, and can be used regardless of mode. [sent-151, score-0.43]
</p><p>85 5  Create Statistics  The statistics module prints statistics about the currently loaded annotation project. [sent-153, score-0.279]
</p><p>86 It contains information about the number of sentences and errors in the project, average number of errors per sentence, and how many sentences there are with certain numbers of errors. [sent-155, score-0.118]
</p><p>87 The main part of the statistics is the number and percentage of errors for each node in the error typology. [sent-156, score-0.301]
</p><p>88 Most importantly we want to add the possibility to annotate two MT systems in parallel, which can be useful if the purpose of the annotation is to compare MT systems. [sent-159, score-0.147]
</p><p>89 We are also working on refining and developing the existing proposals for error typologies, which is an important complement to the tool itself. [sent-160, score-0.409]
</p><p>90 We intend to define a new finegrained general error typology, with extensions to a number of target languages. [sent-161, score-0.213]
</p><p>91 The modularity of BLAST also makes it possible to add new modules, for instance for preprocessing and to support other file formats. [sent-162, score-0.248]
</p><p>92 One example would be to support error annotation of only specific phenomena, such as verb errors, by adding a preprocessing module for highlighting verbs with support annotations, and a suitable verb-focused error typology. [sent-163, score-0.859]
</p><p>93 We are also working on a preprocessing module based on grammar checker techniques (Stymne and Ahrenberg, 2010), that highlights parts of the MT output that it suspects are non-grammatical. [sent-164, score-0.245]
</p><p>94 6  Conclusion  We presented BLAST; a flexible tool for annotation of bilingual segments, specifically intended for error analysis of MT. [sent-168, score-0.53]
</p><p>95 BLAST facilitates the error analysis task, which we believe is vital for MT researchers, and could also be useful for other users of MT. [sent-169, score-0.329]
</p><p>96 Its flexibility makes it possible to annotate translations from any MT system and between any language pairs, using any hierarchical error typology. [sent-170, score-0.259]
</p><p>97 Management of large annotation projects involving multiple human judges: a case study of GALE machine translation post-editing. [sent-205, score-0.176]
</p><p>98 Analysis of machine translation systems’ errors in tense, aspect, and modality. [sent-213, score-0.132]
</p><p>99 Morpho-syntactic information for automatic error analysis of statistical machine translation output. [sent-221, score-0.308]
</p><p>100 A study of translation edit rate with targeted human notation. [sent-225, score-0.157]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('blast', 0.697), ('typology', 0.31), ('error', 0.213), ('mt', 0.209), ('typologies', 0.188), ('annotations', 0.165), ('file', 0.135), ('tool', 0.122), ('user', 0.109), ('mode', 0.108), ('annotation', 0.103), ('module', 0.078), ('menu', 0.077), ('stymne', 0.075), ('translation', 0.073), ('vilar', 0.069), ('highlighting', 0.065), ('preprocessing', 0.061), ('similarities', 0.06), ('errors', 0.059), ('edit', 0.059), ('ahrenberg', 0.057), ('farr', 0.057), ('support', 0.052), ('segments', 0.051), ('activated', 0.049), ('hter', 0.049), ('reference', 0.047), ('jos', 0.046), ('clicking', 0.046), ('users', 0.046), ('working', 0.045), ('denkowski', 0.044), ('graphical', 0.044), ('modules', 0.043), ('flanagan', 0.043), ('glenn', 0.043), ('llitj', 0.043), ('meteornext', 0.043), ('submenu', 0.043), ('subtypologies', 0.043), ('currently', 0.04), ('murata', 0.038), ('aid', 0.037), ('bilingual', 0.036), ('metrics', 0.036), ('search', 0.036), ('popovi', 0.035), ('wmt', 0.034), ('freedom', 0.034), ('flexible', 0.034), ('font', 0.033), ('ping', 0.033), ('modular', 0.032), ('paraphrase', 0.032), ('checker', 0.031), ('printed', 0.031), ('quantitative', 0.031), ('output', 0.03), ('statistics', 0.029), ('normally', 0.029), ('complement', 0.029), ('goals', 0.029), ('screenshot', 0.028), ('modes', 0.028), ('marked', 0.028), ('formats', 0.028), ('colored', 0.028), ('adjusting', 0.027), ('bannard', 0.027), ('lars', 0.027), ('files', 0.026), ('hierarchical', 0.026), ('design', 0.026), ('lrec', 0.026), ('sara', 0.026), ('color', 0.026), ('stored', 0.026), ('targeted', 0.025), ('believe', 0.025), ('adequacy', 0.025), ('meteor', 0.024), ('iu', 0.024), ('purpose', 0.024), ('segment', 0.024), ('name', 0.023), ('amta', 0.023), ('mari', 0.023), ('vital', 0.023), ('indices', 0.023), ('os', 0.023), ('humans', 0.023), ('quantitatively', 0.022), ('analysis', 0.022), ('specific', 0.022), ('indication', 0.021), ('matching', 0.021), ('fluency', 0.021), ('languages', 0.021), ('annotate', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="62-tfidf-1" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>Author: Sara Stymne</p><p>Abstract: We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part ofMT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.</p><p>2 0.24053562 <a title="62-tfidf-2" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>Author: Sara Stymne</p><p>Abstract: In this thesis proposal Ipresent my thesis work, about pre- and postprocessing for statistical machine translation, mainly into Germanic languages. I focus my work on four areas: compounding, definite noun phrases, reordering, and error correction. Initial results are positive within all four areas, and there are promising possibilities for extending these approaches. In addition Ialso focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed.</p><p>3 0.13613738 <a title="62-tfidf-3" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>Author: Chi-kiu Lo ; Dekai Wu</p><p>Abstract: We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost. As machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent. But more accurate, nonautomatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacyjudgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER. We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as HTER despite an even lower labor cost for the evaluation procedure. The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER. 1</p><p>4 0.1083511 <a title="62-tfidf-4" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>Author: Nguyen Bach ; Fei Huang ; Yaser Al-Onaizan</p><p>Abstract: State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improve- ments between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.</p><p>5 0.1029627 <a title="62-tfidf-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.091263443 <a title="62-tfidf-6" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>7 0.080743246 <a title="62-tfidf-7" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>8 0.074748039 <a title="62-tfidf-8" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>9 0.072061725 <a title="62-tfidf-9" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>10 0.071983688 <a title="62-tfidf-10" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>11 0.071879812 <a title="62-tfidf-11" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>12 0.070564993 <a title="62-tfidf-12" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>13 0.068850853 <a title="62-tfidf-13" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>14 0.065784886 <a title="62-tfidf-14" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>15 0.06303706 <a title="62-tfidf-15" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>16 0.062455617 <a title="62-tfidf-16" href="./acl-2011-Monolingual_Alignment_by_Edit_Rate_Computation_on_Sentential_Paraphrase_Pairs.html">225 acl-2011-Monolingual Alignment by Edit Rate Computation on Sentential Paraphrase Pairs</a></p>
<p>17 0.062057257 <a title="62-tfidf-17" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>18 0.061816815 <a title="62-tfidf-18" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>19 0.060874626 <a title="62-tfidf-19" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>20 0.058183555 <a title="62-tfidf-20" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, -0.063), (2, 0.039), (3, 0.082), (4, -0.033), (5, 0.027), (6, 0.082), (7, -0.058), (8, 0.073), (9, -0.031), (10, -0.061), (11, -0.053), (12, -0.008), (13, -0.092), (14, -0.023), (15, 0.076), (16, 0.005), (17, -0.09), (18, -0.024), (19, 0.003), (20, -0.007), (21, 0.097), (22, 0.01), (23, -0.021), (24, -0.015), (25, -0.021), (26, -0.038), (27, -0.023), (28, -0.049), (29, -0.06), (30, -0.044), (31, 0.05), (32, -0.02), (33, -0.009), (34, 0.009), (35, 0.036), (36, 0.055), (37, 0.029), (38, -0.073), (39, 0.036), (40, 0.051), (41, -0.054), (42, -0.01), (43, 0.03), (44, 0.03), (45, 0.086), (46, 0.006), (47, -0.083), (48, 0.065), (49, -0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9468739 <a title="62-lsi-1" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>Author: Sara Stymne</p><p>Abstract: We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part ofMT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.</p><p>2 0.73514509 <a title="62-lsi-2" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>Author: Sara Stymne</p><p>Abstract: In this thesis proposal Ipresent my thesis work, about pre- and postprocessing for statistical machine translation, mainly into Germanic languages. I focus my work on four areas: compounding, definite noun phrases, reordering, and error correction. Initial results are positive within all four areas, and there are promising possibilities for extending these approaches. In addition Ialso focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed.</p><p>3 0.66636842 <a title="62-lsi-3" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>Author: Chi-kiu Lo ; Dekai Wu</p><p>Abstract: We introduce a novel semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as HTER but at much lower labor cost. As machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented MT evaluation metrics such as BLEU, which fail to properly evaluate adequacy, become more apparent. But more accurate, nonautomatic adequacy-oriented MT evaluation metrics like HTER are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in MT output, the non-automatic version of the metric HMEANT achieves a 0.43 correlation coefficient with human adequacyjudgments at the sentence level, far superior to BLEU at only 0.20, and equal to the far more expensive HTER. We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as HTER despite an even lower labor cost for the evaluation procedure. The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than HTER. 1</p><p>4 0.63528228 <a title="62-lsi-4" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>Author: Jonathan H. Clark ; Chris Dyer ; Alon Lavie ; Noah A. Smith</p><p>Abstract: In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.</p><p>5 0.60353291 <a title="62-lsi-5" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>Author: Rafael E. Banchs ; Haizhou Li</p><p>Abstract: This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions. 1</p><p>6 0.5850789 <a title="62-lsi-6" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>7 0.56600684 <a title="62-lsi-7" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<p>8 0.54994357 <a title="62-lsi-8" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>9 0.53993475 <a title="62-lsi-9" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>10 0.53819782 <a title="62-lsi-10" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>11 0.52289057 <a title="62-lsi-11" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>12 0.52052021 <a title="62-lsi-12" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>13 0.51872951 <a title="62-lsi-13" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>14 0.51624584 <a title="62-lsi-14" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>15 0.50397938 <a title="62-lsi-15" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>16 0.49011225 <a title="62-lsi-16" href="./acl-2011-Language-independent_compound_splitting_with_morphological_operations.html">193 acl-2011-Language-independent compound splitting with morphological operations</a></p>
<p>17 0.47728384 <a title="62-lsi-17" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>18 0.46934241 <a title="62-lsi-18" href="./acl-2011-They_Can_Help%3A_Using_Crowdsourcing_to_Improve_the_Evaluation_of_Grammatical_Error_Detection_Systems.html">302 acl-2011-They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems</a></p>
<p>19 0.46178657 <a title="62-lsi-19" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>20 0.45440662 <a title="62-lsi-20" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (10, 0.04), (17, 0.038), (26, 0.039), (31, 0.029), (37, 0.065), (39, 0.047), (41, 0.081), (53, 0.012), (55, 0.015), (59, 0.028), (65, 0.188), (72, 0.055), (91, 0.045), (96, 0.186)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85005122 <a title="62-lda-1" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>Author: Sara Stymne</p><p>Abstract: We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part ofMT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.</p><p>2 0.84356916 <a title="62-lda-2" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>Author: Clinton Burfoot ; Steven Bird ; Timothy Baldwin</p><p>Abstract: This paper explores approaches to sentiment classification of U.S. Congressional floordebate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.</p><p>3 0.78833485 <a title="62-lda-3" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Ting Liu ; Sheng Li</p><p>Abstract: This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1</p><p>4 0.76449132 <a title="62-lda-4" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>5 0.75846922 <a title="62-lda-5" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>6 0.75803125 <a title="62-lda-6" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>7 0.75526774 <a title="62-lda-7" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>8 0.75194257 <a title="62-lda-8" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>9 0.75160015 <a title="62-lda-9" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>10 0.75134641 <a title="62-lda-10" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>11 0.75119448 <a title="62-lda-11" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>12 0.75110865 <a title="62-lda-12" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>13 0.74991262 <a title="62-lda-13" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>14 0.74988449 <a title="62-lda-14" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>15 0.74929845 <a title="62-lda-15" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>16 0.74873281 <a title="62-lda-16" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>17 0.74872947 <a title="62-lda-17" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>18 0.74863595 <a title="62-lda-18" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>19 0.74847555 <a title="62-lda-19" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>20 0.74830401 <a title="62-lda-20" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
