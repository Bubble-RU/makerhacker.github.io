<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-242" href="#">acl2011-242</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</h1>
<br/><p>Source: <a title="acl-2011-242-pdf" href="http://aclweb.org/anthology//P/P11/P11-2008.pdf">pdf</a></p><p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>Reference: <a title="acl-2011-242-reference" href="../acl2011_reference/acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Smith School of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA {kgimpe l n s chne id, brenocon ,dipan j an ,dpmi l s , , l j acobe i ,mhe i s lman , dyogat ama , j flanigan ,nasmith} @ cs . [sent-2, score-0.07]
</p><p>2 edu  Abstract We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. [sent-4, score-0.208]
</p><p>3 We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. [sent-5, score-0.118]
</p><p>4 The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets. [sent-6, score-0.199]
</p><p>5 1 Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. [sent-7, score-0.116]
</p><p>6 The popular microblogging service Twitter (twitter com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al. [sent-8, score-0.09]
</p><p>7 However, the bulk ofthis work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al. [sent-14, score-0.09]
</p><p>8 Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al. [sent-17, score-0.033]
</p><p>9 Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). [sent-19, score-0.058]
</p><p>10 Figure 1 shows three tweets which illustrate these challenges. [sent-20, score-0.259]
</p><p>11 Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAGDICT, METAPH, and DISTSIM). [sent-30, score-0.295]
</p><p>12 $  In this paper, we produce an English POS tagger that is designed especially for Twitter data. [sent-31, score-0.144]
</p><p>13 Our contributions are as follows: we developed a POS tagset for Twitter, • we manually tagged 1,827 tweets, • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we provide our annotated corpus and trained POS tagger to the research community. [sent-32, score-0.506]
</p><p>14 Beyond these specific contributions, we see this work as a case study in how to rapidly engineer a core NLP system for a new and idiosyncratic dataset. [sent-33, score-0.062]
</p><p>15 The success of this approach demonstrates that with careful design, supervised machine learning can be applied to rapidly produce effective language technology in new domains. [sent-36, score-0.062]
</p><p>16 65  Table 1: The set of tags used to annotate tweets. [sent-52, score-0.072]
</p><p>17 The last column indicates each tag’s relative frequency in the full annotated data (26,435 tokens). [sent-53, score-0.064]
</p><p>18 For Stage 0, we developed a set of 20 coarse-grained tags based on several treebanks but with some additional categories specific to Twitter, including URLs and hashtags. [sent-57, score-0.14]
</p><p>19 Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al. [sent-58, score-0.35]
</p><p>20 Heuristics were used to mark tokens belonging to special Twitter categories, which took precedence over the Stanford tags. [sent-61, score-0.11]
</p><p>21 Stage 1was a round of manual annotation: 17 researchers corrected the automatic predictions from Stage 0 via a custom Web interface. [sent-62, score-0.118]
</p><p>22 A total of 2,217 tweets were distributed to the annotators in this stage; 390 were identified as non-English and removed, leaving 1,827 annotated tweets (26,436 tokens). [sent-63, score-0.55]
</p><p>23 The annotation process uncovered several situations for which our tagset, annotation guidelines, and tokenization rules were deficient or ambiguous. [sent-64, score-0.153]
</p><p>24 Based on these considerations we revised the tok-  enization and tagging guidelines, and for Stage 2, two annotators reviewed and corrected all of the English tweets tagged in Stage 1. [sent-65, score-0.501]
</p><p>25 A third annotator read the annotation guidelines and annotated 72 tweets from scratch, for purposes of estimating inter-annotator agreement. [sent-66, score-0.427]
</p><p>26 The 72 tweets comprised 1,021 tagged tokens, of which 80 differed from the Stage 2 annotations, resulting in an agreement rate of 92. [sent-67, score-0.302]
</p><p>27 A final sweep was made by a single annotator to correct errors and improve consistency of tagging decisions across the corpus. [sent-70, score-0.155]
</p><p>28 The released data and tools use the output of this final stage. [sent-71, score-0.045]
</p><p>29 Thus, we sought to design a coarse tagset  that would capture standard parts of speech3 (noun, verb, etc. [sent-76, score-0.178]
</p><p>30 ) as well as categories for token varieties seen mainly in social media: URLs and email addresses; emoticons; Twitter hashtags, of the form #tagname, which the author may supply to categorize a tweet; and Twitter at-mentions, of the form @user, which link to other Twitter users from within a tweet. [sent-77, score-0.145]
</p><p>31 When used in this way, we tag hashtags with their appropriate part of speech, i. [sent-82, score-0.25]
</p><p>32 Of the 418 hashtags in our data, 148 (35%) were given a tag other than #: 14% are proper nouns, 9% are common nouns, 5% are multi-word expresssions (tagged as G), 3% are verbs, and 4% are something else. [sent-85, score-0.3]
</p><p>33 We do not apply this procedure to atmentions, as they are nearly always proper nouns. [sent-86, score-0.05]
</p><p>34 Another tag, ~, is used for tokens marking specific Twitter discourse functions. [sent-87, score-0.144]
</p><p>35 The most popular of these is the RT (“retweet”) construction to publish a message with attribution. [sent-88, score-0.058]
</p><p>36 Wow lmao indicates that the user @USER1 was originally the source of the message following the colon. [sent-92, score-0.197]
</p><p>37 ) at the end of a tweet, indicating a message has been truncated to fit the 140-character limit, and will be continued in a subsequent tweet or at a specified URL. [sent-98, score-0.269]
</p><p>38 Our first round of annotation revealed that, due to nonstandard spelling conventions, tokenizing under a traditional scheme would be much more difficult  hm saetpearriaalte. [sent-99, score-0.146]
</p><p>39 4s  3Our starting point was the cross-lingual tagset presented by Petrov et al. [sent-100, score-0.128]
</p><p>40 Most of our tags are refinements of those categories, which in turn are groupings of PTB WSJ tags (see column 2 of Table 1). [sent-102, score-0.176]
</p><p>41 When faced with difficult tagging decisions, we consulted the PTB and tried to emulate its conventions as much as possible. [sent-103, score-0.161]
</p><p>42 For example, apostrophes are often omitted, and there are frequently words like ima (short for I’m gonna) that cut across traditional POS categories. [sent-106, score-0.031]
</p><p>43 Therefore, we opted not to split contractions or possessives, as is common in English corpus preprocessing; rather, we introduced four new tags for combined forms: {nominal, proper noun} {verb, rp coosmsebsisniveed} f. [sent-107, score-0.122]
</p><p>44 o5 The final tagging scheme (Table 1) encompasses 25 tags. [sent-108, score-0.118]
</p><p>45 For simplicity, each tag is denoted with a single ASCII character. [sent-109, score-0.109]
</p><p>46 The miscellaneous category G includes multiword abbreviations that do not fit in any of the other categories, like ily (Ilove you), as well as partial words, artifacts of tokenization errors, miscellaneous symbols, possessive endings,6 and arrows that are not used as discourse markers. [sent-110, score-0.369]
</p><p>47 Figure 2 shows where tags in our data tend to occur relative to the middle word of the tweet. [sent-111, score-0.072]
</p><p>48 We see that Twitter-specific tags have strong positional preferences: at-mentions (@) and Twitter discourse markers (~) tend to occur towards the beginning of messages, whereas URLs (U), emoticons (E), and categorizing hashtags (#) tend to occur near the end. [sent-112, score-0.288]
</p><p>49 3  System  Our tagger is a conditional random field (CRF; Lafferty et al. [sent-113, score-0.144]
</p><p>50 , 2001), enabling the incorporation of arbitrary local features in a log-linear model. [sent-114, score-0.079]
</p><p>51 Our base features include: a feature for each word type, a set of features that check whether the word contains digits or hyphens, suffix features up to length 3, and features looking at capitalization patterns in the word. [sent-115, score-0.281]
</p><p>52 We then added features that leverage domainspecific properties of our data, unlabeled in-domain data, and external linguistic resources. [sent-116, score-0.041]
</p><p>53 We have features for several regular expression-style rules that detect at-mentions, hashtags, and URLs. [sent-118, score-0.041]
</p><p>54 Microbloggers are inconsistent in their use of capitalization, so we compiled gazetteers of tokens which are frequently capitalized. [sent-120, score-0.11]
</p><p>55 The likelihood of capitalization for a token is computed as NcNap++CαC, where 5The modified tokenizer is packaged with our tagger. [sent-121, score-0.219]
</p><p>56 6Possessive endings only appear when a user or the tokenizer has separated the possessive ending from a possessor; the tokenizer only does this when the possessor is an at-mention. [sent-122, score-0.326]
</p><p>57 Figure  2: Average  position, relative to the middle word in the tweet, of tokens labeled with each tag. [sent-123, score-0.11]
</p><p>58 Most tags fall  between −1 and 1on this scale; these are not shown. [sent-124, score-0.072]
</p><p>59 N is the token count, Ncap is the capitalized token count, and α and C are the prior probability and its prior weight. [sent-125, score-0.098]
</p><p>60 7 We compute features for membership in the top N items by this metric, for N ∈ {1000, 2000, 3000, 5000, 10000, 20000}. [sent-126, score-0.041]
</p><p>61 We add features for all coarse-grained tags that each word occurs with in the PTB8 (conjoined with their frequency rank). [sent-128, score-0.113]
</p><p>62 Unlike previous work that uses tag dictionaries as hard constraints, we use them as soft constraints since we expect lexical coverage to be poor and the Twitter dialect of English to vary significantly from the PTB domains. [sent-129, score-0.109]
</p><p>63 This feature may be seen as a form of type-level domain adaptation. [sent-130, score-0.038]
</p><p>64 When training data is limited, distributional features from unlabeled text can improve performance (Sch u¨tze and Pedersen, 1993). [sent-132, score-0.08]
</p><p>65 9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. [sent-134, score-0.519]
</p><p>66 The suc-  cessor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M ≈ where U is limited to 5co0m mcoplousmitinons. [sent-135, score-0.115]
</p><p>67 EMach ≈ te UrmS’Vs feature vector is its row in U; following Turian et al. [sent-136, score-0.038]
</p><p>68 Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. [sent-140, score-0.103]
</p><p>69 For example, in our  USVT,  7α = 1010, C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0. [sent-142, score-0.079]
</p><p>70 9Via the Apache Commons implementation: http : / / commons . [sent-147, score-0.07]
</p><p>71 Second, we use a feature indicating whether a tag is the most frequent tag for PTB words having the same Metaphone key as the current token. [sent-156, score-0.256]
</p><p>72 (The second feature was disabled in both −TAGDICT and −METAPH ablwaatison d experiments. [sent-157, score-0.038]
</p><p>73 ) 4  Experiments  Our evaluation was designed to test the efficacy of this feature set for part-of-speech tagging given limited training data. [sent-158, score-0.156]
</p><p>74 We randomly divided the set of 1,827 annotated tweets into a training set of 1,000 (14,542 tokens), a development set of 327 (4,770 tokens), and a test set of 500 (7,124 tokens). [sent-159, score-0.291]
</p><p>75 Due to the different tagsets, we could not apply the pretrained Stanford tagger to our data. [sent-161, score-0.144]
</p><p>76 Our tagger with the full feature set achieves a relative error reduction of 25% compared to the Stanford tagger. [sent-167, score-0.182]
</p><p>77 We also show feature ablation experiments, each of which corresponds to removing one category of features from the full set. [sent-168, score-0.228]
</p><p>78 In Figure 1, we show examples that certain features help solve. [sent-169, score-0.041]
</p><p>79 Underlined tokens  σ2 σ2  10We used the following feature modules in the Stanford tagger: bidi rect ional 5words , naacl 2 0 0 3unknowns , wordshape s ( -3 ,3 ) , pre fix ( 3 ) , su ffix ( 3 ) , pre fixsuf fix ( 3 ) . [sent-170, score-0.308]
</p><p>80 2 Table 2: Tagging accuracies on development and test data, including ablation experiments. [sent-194, score-0.107]
</p><p>81 Features are ordered by importance: test accuracy decrease due to ablation (final column). [sent-195, score-0.107]
</p><p>82 82N N 85 ∧ L 93 V , 98 ~∧ & 98 ∧ P 95 R U 97 , ∧ 71 N $ 89 P D∧ 95 ∧ # 89 ∧ O 97 ∧∧ G 26 , A 79 N∧ E 88 , R 83 A T 72 P @ 99 V Z 45 ∧ ~ 91 , Table 3: Accuracy (recall) rates per class, in the test set with the full model. [sent-199, score-0.033]
</p><p>83 (Omitting tags that occur less than 10 times in the test set. [sent-200, score-0.072]
</p><p>84 are incorrect in a specific ablation, but are corrected in the full system (i. [sent-202, score-0.081]
</p><p>85 The −TAGDICT  ablation gets elects, Governor,  andT hneex −t wrong in tweet (a). [sent-205, score-0.273]
</p><p>86 These words appear in the PTB tag dictionary with the correct tags, and thus are fixed by that feature. [sent-206, score-0.109]
</p><p>87 In (b), withhh is initially misclassified an interjection (likely caused by interjections with the same suffix, like ohhh), but is corrected by METAPH, because it is normalized to the same equivalence class as with. [sent-207, score-0.081]
</p><p>88 Finally, s/o in tweet (c) means “shoutout”, which appears only once in the training data; adding DISTSIM causes it to be correctly identified as a verb. [sent-208, score-0.166]
</p><p>89 Substantial challenges remain; for example, despite the NAMES feature, the system struggles to identify proper nouns with nonstandard capitalization. [sent-209, score-0.216]
</p><p>90 This can be observed from Table 3, which shows the recall of each tag type: the recall of proper nouns (ˆ) is only 71%. [sent-210, score-0.198]
</p><p>91 The system also struggles 46 with the miscellaneous category (G), which covers many rare tokens, including obscure symbols and artifacts of tokenization errors. [sent-211, score-0.291]
</p><p>92 Finally, we note that, even though 1,000 training examples may seem small, the test set accuracy when training on only 500 tweets drops to 87. [sent-213, score-0.259]
</p><p>93 5  Conclusion  We have developed a part-of-speech tagger for Twitter and have made our data and tools available to the research community at http : / /www . [sent-216, score-0.189]
</p><p>94 More generally, we believe that our approach can be applied to address other linguistic analysis needs as they continue to arise in the era of social media and its rapidly changing linguistic conventions. [sent-221, score-0.178]
</p><p>95 We also believe that the annotated data can be useful for research into domain adaptation and semi-supervised learning. [sent-222, score-0.032]
</p><p>96 Ways of Analyzing Christiane  Meaning variation of the iconic  Fellbaum. [sent-247, score-0.07]
</p><p>97 Building a large annotated corpus of English: The Penn Treebank. [sent-264, score-0.032]
</p><p>98 From tweets to polls: Linking text sentiment to public  An Electronic  opinion time series. [sent-270, score-0.259]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('twitter', 0.453), ('tweets', 0.259), ('distsim', 0.176), ('metaph', 0.176), ('tagdict', 0.176), ('tweet', 0.166), ('tagger', 0.144), ('hashtags', 0.141), ('tagset', 0.128), ('metaphone', 0.124), ('ptb', 0.121), ('tagging', 0.118), ('tokens', 0.11), ('tag', 0.109), ('ablation', 0.107), ('connor', 0.107), ('lmao', 0.106), ('tokenizer', 0.091), ('stanford', 0.085), ('corrected', 0.081), ('capitalization', 0.079), ('stage', 0.076), ('miscellaneous', 0.073), ('tags', 0.072), ('asur', 0.07), ('commons', 0.07), ('flanigan', 0.07), ('iconic', 0.07), ('moby', 0.07), ('notr', 0.07), ('possessor', 0.07), ('predecessor', 0.07), ('struggles', 0.07), ('thed', 0.07), ('brendan', 0.068), ('rapidly', 0.062), ('deictics', 0.062), ('thelwall', 0.062), ('social', 0.061), ('pos', 0.061), ('urls', 0.059), ('message', 0.058), ('nonstandard', 0.057), ('artifacts', 0.057), ('microblogging', 0.057), ('barbosa', 0.057), ('media', 0.055), ('gimpel', 0.054), ('heilman', 0.054), ('bryan', 0.054), ('phonetic', 0.053), ('annotation', 0.052), ('apache', 0.051), ('coarse', 0.05), ('rt', 0.05), ('proper', 0.05), ('tokenization', 0.049), ('routledge', 0.049), ('turian', 0.049), ('token', 0.049), ('guidelines', 0.047), ('tools', 0.045), ('pre', 0.045), ('truncated', 0.045), ('ritter', 0.044), ('finin', 0.044), ('dipanjan', 0.044), ('confused', 0.044), ('tagged', 0.043), ('conventions', 0.043), ('underlined', 0.043), ('category', 0.042), ('possessive', 0.041), ('emoticons', 0.041), ('features', 0.041), ('fellowship', 0.039), ('distributional', 0.039), ('nouns', 0.039), ('army', 0.039), ('feature', 0.038), ('enabling', 0.038), ('round', 0.037), ('tze', 0.037), ('annotator', 0.037), ('cmu', 0.036), ('categories', 0.035), ('fix', 0.035), ('discourse', 0.034), ('das', 0.034), ('lafferty', 0.033), ('service', 0.033), ('user', 0.033), ('treebanks', 0.033), ('rates', 0.033), ('column', 0.032), ('gaussian', 0.032), ('annotated', 0.032), ('toutanova', 0.032), ('ima', 0.031), ('dicting', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="242-tfidf-1" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>2 0.34564659 <a title="242-tfidf-2" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>Author: Long Jiang ; Mo Yu ; Ming Zhou ; Xiaohua Liu ; Tiejun Zhao</p><p>Abstract: Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification. 1</p><p>3 0.29471698 <a title="242-tfidf-3" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>4 0.2849755 <a title="242-tfidf-4" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>Author: Aditya Joshi ; Balamurali AR ; Pushpak Bhattacharyya ; Rajat Mohanty</p><p>Abstract: Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.</p><p>5 0.25369969 <a title="242-tfidf-5" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>6 0.21934442 <a title="242-tfidf-6" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>7 0.18680175 <a title="242-tfidf-7" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>8 0.18435964 <a title="242-tfidf-8" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>9 0.15470403 <a title="242-tfidf-9" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>10 0.12315402 <a title="242-tfidf-10" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>11 0.11933298 <a title="242-tfidf-11" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>12 0.093995832 <a title="242-tfidf-12" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>13 0.093801498 <a title="242-tfidf-13" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.085194044 <a title="242-tfidf-14" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>15 0.081478588 <a title="242-tfidf-15" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>16 0.07658843 <a title="242-tfidf-16" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>17 0.075093038 <a title="242-tfidf-17" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>18 0.071176261 <a title="242-tfidf-18" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>19 0.070072144 <a title="242-tfidf-19" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>20 0.069522239 <a title="242-tfidf-20" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, 0.152), (2, 0.066), (3, -0.075), (4, -0.036), (5, -0.005), (6, 0.086), (7, -0.211), (8, -0.059), (9, 0.169), (10, -0.324), (11, 0.232), (12, 0.2), (13, -0.097), (14, -0.148), (15, -0.113), (16, -0.052), (17, 0.042), (18, -0.045), (19, 0.014), (20, -0.077), (21, -0.02), (22, -0.017), (23, -0.004), (24, 0.013), (25, 0.03), (26, -0.067), (27, -0.082), (28, 0.009), (29, 0.003), (30, 0.015), (31, -0.011), (32, 0.043), (33, -0.017), (34, -0.009), (35, 0.004), (36, 0.02), (37, -0.019), (38, -0.001), (39, -0.028), (40, 0.023), (41, 0.045), (42, 0.008), (43, -0.065), (44, 0.012), (45, -0.04), (46, -0.012), (47, 0.013), (48, 0.002), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92850202 <a title="242-lsi-1" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>2 0.87819541 <a title="242-lsi-2" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>Author: Roberto Gonzalez-Ibanez ; Smaranda Muresan ; Nina Wacholder</p><p>Abstract: Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well. 1</p><p>3 0.74503118 <a title="242-lsi-3" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>Author: Xiaohua LIU ; Shaodian ZHANG ; Furu WEI ; Ming ZHOU</p><p>Abstract: The challenges of Named Entities Recognition (NER) for tweets lie in the insufficient information in a tweet and the unavailability of training data. We propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to tackle these challenges. The KNN based classifier conducts pre-labeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semi-supervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of KNN and semisupervised learning.</p><p>4 0.71661597 <a title="242-lsi-4" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>5 0.69779301 <a title="242-lsi-5" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>Author: Aditya Joshi ; Balamurali AR ; Pushpak Bhattacharyya ; Rajat Mohanty</p><p>Abstract: Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.</p><p>6 0.64165312 <a title="242-lsi-6" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>7 0.63740265 <a title="242-lsi-7" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>8 0.62123692 <a title="242-lsi-8" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>9 0.56274408 <a title="242-lsi-9" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>10 0.50470501 <a title="242-lsi-10" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>11 0.39727238 <a title="242-lsi-11" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>12 0.34373406 <a title="242-lsi-12" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>13 0.3400991 <a title="242-lsi-13" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>14 0.33395052 <a title="242-lsi-14" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>15 0.32860431 <a title="242-lsi-15" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>16 0.32596654 <a title="242-lsi-16" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>17 0.29829818 <a title="242-lsi-17" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>18 0.29605296 <a title="242-lsi-18" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>19 0.28713316 <a title="242-lsi-19" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<p>20 0.28665373 <a title="242-lsi-20" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (5, 0.032), (17, 0.035), (26, 0.031), (32, 0.263), (37, 0.081), (39, 0.096), (41, 0.057), (55, 0.033), (59, 0.025), (72, 0.077), (91, 0.036), (96, 0.106), (97, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79933101 <a title="242-lda-1" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>2 0.70964342 <a title="242-lda-2" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<p>Author: Vishal Goyal ; Gurpreet Singh Lehal</p><p>Abstract: Hindi-Punjabi being closely related language pair (Goyal V. and Lehal G.S., 2008) , Hybrid Machine Translation approach has been used for developing Hindi to Punjabi Machine Translation System. Non-availability of lexical resources, spelling variations in the source language text, source text ambiguous words, named entity recognition and collocations are the major challenges faced while developing this syetm. The key activities involved during translation process are preprocessing, translation engine and post processing. Lookup algorithms, pattern matching algorithms etc formed the basis for solving these issues. The system accuracy has been evaluated using intelligibility test, accuracy test and BLEU score. The hybrid syatem is found to perform better than the constituent systems. Keywords: Machine Translation, Computational Linguistics, Natural Language Processing, Hindi, Punjabi. Translate Hindi to Punjabi, Closely related languages. 1Introduction Machine Translation system is a software designed that essentially takes a text in one language (called the source language), and translates it into another language (called the target language). There are number of approaches for MT like Direct based, Transform based, Interlingua based, Statistical etc. But the choice of approach depends upon the available resources and the kind of languages involved. In general, if the two languages are structurally similar, in particular as regards lexical correspondences, morphology and word order, the case for abstract syntactic analysis seems less convincing. Since the present research work deals with a pair of closely related language 1 Gurpreet Singh Lehal Department of Computer Science Punjabi University, Patiala,India gs lehal @ gmai l com . i.e. Hindi-Punjabi , thus direct word-to-word translation approach is the obvious choice. As some rule based approach has also been used, thus, Hybrid approach has been adopted for developing the system. An exhaustive survey has already been given for existing machine translations systems developed so far mentioning their accuracies and limitations. (Goyal V. and Lehal G.S., 2009). 2 System Architecture 2.1 Pre Processing Phase The preprocessing stage is a collection of operations that are applied on input data to make it processable by the translation engine. In the first phase of Machine Translation system, various activities incorporated include text normalization, replacing collocations and replacing proper nouns. 2.2 Text Normalization The variety in the alphabet, different dialects and influence of foreign languages has resulted in spelling variations of the same word. Such variations sometimes can be treated as errors in writing. (Goyal V. and Lehal G.S., 2010). 2.3 Replacing Collocations After passing the input text through text normalization, the text passes through this Collocation replacement sub phase of Preprocessing phase. Collocation is two or more consecutive words with a special behavior. (Choueka :1988). For example, the collocation उ?र ?देश (uttar pradēsh) if translated word to word, will be translated as ਜਵਾਬ ਰਾਜ (javāb rāj) but it must be translated as ਉ?ਤਰ ਪ?ਦਸ਼ੇ (uttar pradēsh). The accuracy of the results for collocation extraction using t-test is not accurate and includes number of such bigrams and trigrams that are not actually collocations. Thus, manually such entries were removed and actual collocations were further extracted. The Portland, POrroecgeoend,in UgSsA o,f 2 t1he Ju AnCeL 2-0H1L1T. 2 ?c 021101 S1y Astessmoc Diaetmioonn fsotr a Ctioonms,p puatagteiosn 1a–l6 L,inguistics correct corresponding Punjabi translation for each extracted collocation is stored in the collocation table of the database. The collocation table of the database consists of 5000 such entries. In this sub phase, the normalized input text is analyzed. Each collocation in the database found in the input text will be replaced with the Punjabi translation of the corresponding collocation. It is found that when tested on a corpus containing about 1,00,000 words, only 0.001 % collocations were found and replaced during the translation. Hindi Text Figure 1: Overview of Hindi-Punjabi Machine Translation System 2.4 Replacing Proper Nouns A great proposition of unseen words includes proper nouns like personal, days of month, days of week, country names, city names, bank fastens words proper decide the translation process. Once these are recognized and stored into the noun database, there is no need to about their translation or transliteration names, organization names, ocean names, river every names, university words names etc. and if translated time in the case of presence in word to word, their meaning is changed. If the gazetteer meaning is not affected, even though this step fast. This input makes list text for the translation is self of such translation. growing This accurate and during each 2 translation. Thus, to process this sub phase, the system requires a proper noun gazetteer that has been complied offline. For this task, we have developed an offline module to extract proper nouns from the corpus based on some rules. Also, Named Entity recognition module has been developed based on the CRF approach (Sharma R. and Goyal V., 2011b). 2.5 Tokenizer Tokenizers (also known as lexical analyzers or word segmenters) segment a stream of characters into meaningful units called tokens. The tokenizer takes the text generated by pre processing phase as input. Individual words or tokens are extracted and processed to generate its equivalent in the target language. This module, using space, a punctuation mark, as delimiter, extracts tokens (word) one by one from the text and gives it to translation engine for analysis till the complete input text is read and processed. 2.6 Translation Engine The translation engine is the main component of our Machine Translation system. It takes token generated by the tokenizer as input and outputs the translated token in the target language. These translated tokens are concatenated one after another along with the delimiter. Modules included in this phase are explained below one by one. 2.6.1 Identifying Titles and Surnames Title may be defined as a formal appellation attached to the name of a person or family by virtue of office, rank, hereditary privilege, noble birth, or attainment or used as a mark of respect. Thus word next to title and word previous to surname is usually a proper noun. And sometimes, a word used as proper name of a person has its own meaning in target language. Similarly, Surname may be defined as a name shared in common to identify the members of a family, as distinguished from each member's given name. It is also called family name or last name. When either title or surname is passed through the translation engine, it is translated by the system. This cause the system failure as these proper names should be transliterated instead of translation. For example consider the Hindi sentence 3 ?ीमान हष? जी हमार ेयहाँ पधार।े (shrīmān harsh jī हष? hamārē yahāṃ padhārē). In this sentence, (harsh) has the meaning “joy”. The equivalent translation of हष? (harsh) in target language is ਖੁਸ਼ੀ (khushī). Similarly, consider the Hindi sentence ?काश ?सह हमार े (prakāsh siṃh hamārē yahāṃ padhārē). Here, ?काश (prakāsh) word is acting as proper noun and it must be transliterated and not translated because (siṃh) is surname and word previous to it is proper noun. Thus, a small module has been developed for यहाँ पधार।े. ?सह locating such proper nouns to consider them as title or surname. There is one special character ‘॰’ in Devanagari script to mark the symbols like डा॰, ?ो॰. If this module found this symbol to be title or surname, the word next and previous to this token as the case may be for title or surname respectively, will be transliterated not translated. The title and surname database consists of 14 and 654 entries respectively. These databases can be extended at any time to allow new titles and surnames to be added. This module was tested on a large Hindi corpus and showed that about 2-5 % text of the input text depending upon its domain is proper noun. Thus, this module plays an important role in translation. 2.6.2 Hindi Morphological analyzer This module finds the root word for the token and its morphological features.Morphological analyzer developed by IIT-H has been ported for Windows platform for making it usable for this system. (Goyal V. and Lehal G.S.,2008a) 2.6.3 Word-to-Word translation using lexicon lookup If token is not a title or a surname, it is looked up in the HPDictionary database containing Hindi to Punjabi direct word to word translation. If it is found, it is used for translation. If no entry is found in HPDictionary database, it is sent to next sub phase for processing. The HPDictionary database consists of 54, 127 entries.This database can be extended at any time to allow new entries in the dictionary to be added. 2.6.4 Resolving Ambiguity Among number of approaches for disambiguation, the most appropriate approach to determine the correct meaning of a Hindi word in a particular usage for our Machine Translation system is to examine its context using N-gram approach. After analyzing the past experiences of various authors, we have chosen the value of n to be 3 and 2 i.e. trigram and bigram approaches respectively for our system. Trigrams are further categorized into three different types. First category of trigram consists of context one word previous to and one word next to the ambiguous word. Second category of trigram consists of context of two adjacent previous words to the ambiguous word. Third category of the trigram consists of context of two adjacent next words to the ambiguous word. Bigrams are also categorized into two categories. First category of the bigrams consists of context of one previous word to ambiguous word and second category of the bigrams consists of one context word next to ambiguous word. For this purpose, the Hindi corpus consisting of about 2 million words was collected from different sources like online newspaper daily news, blogs, Prem Chand stories, Yashwant jain stories, articles etc. The most common list of ambiguous words was found. We have found a list of 75 ambiguous words out of which the most are स े sē and aur. (Goyal V. and frequent Lehal G.S., 2011) और 2.6.5 Handling Unknown Words 2.6.5.1 Word Inflectional Analysis and generation In linguistics, a suffix (also sometimes called a postfix or ending) is an affix which is placed after the stem of a word. Common examples are case endings, which indicate the grammatical case of nouns or adjectives, and verb endings. Hindi is a (relatively) free wordorder and highly inflectional language. Because of same origin, both languages have very similar structure and grammar. The difference is only in words and in pronunciation e.g. in Hindi it is लड़का and in Punjabi the word for boy is ਮੰੁਡਾ and even sometimes that is also not there like घर (ghar) and ਘਰ (ghar). The inflection forms of both these words in Hindi and Punjabi are also similar. In this activity, inflectional analysis without using morphology has been performed 4 for all those tokens that are not processed by morphological analysis module. Thus, for performing inflectional analysis, rule based approach has been followed. When the token is passed to this sub phase for inflectional analysis, If any pattern of the regular expression (inflection rule) matches with this token, that rule is applied on the token and its equivalent translation in Punjabi is generated based on the matched rule(s). There is also a check on the generated word for its correctness. We are using correct Punjabi words database for testing the correctness of the generated word. 2.6.5.2 Transliteration This module is beneficial for handling out-ofvocabulary words. For example the word िवशाल is as ਿਵਸ਼ਾਲ (vishāl) whereas translated as ਵੱਡਾ. There must be some method in every Machine Translation system for words like technical terms and (vishāl) transliterated proper names of persons, places, objects etc. that cannot be found in translation resources such as Hindi-Punjabi bilingual dictionary, surnames database, titles database etc and transliteration is an obvious choice for such words. (Goyal V. and Lehal G.S., 2009a). 2.7 Post-Processing 2.7.1 Agreement Corrections In spite of the great similarity between Hindi and Punjabi, there are still a number of important agreement divergences in gender and number. The output generated by the translation engine phase becomes the input for post-processing phase. This phase will correct the agreement errors based on the rules implemented in the form of regular expressions. (Goyal V. and Lehal G.S., 2011) 3 Evaluation and Results The evaluation document set consisted of documents from various online newspapers news, articles, blogs, biographies etc. This test bed consisted of 35500 words and was translated using our Machine Translation system. 3.1 Test Document For our Machine Translation system evaluation, we have used benchmark sampling method for selecting the set of sentences. Input sentences are selected from randomly selected news (sports, politics, world, regional, entertainment, travel etc.), articles (published by various writers, philosophers etc.), literature (stories by Prem Chand, Yashwant jain etc.), Official language for office letters (The Language Officially used on the files in Government offices) and blogs (Posted by general public in forums etc.). Care has been taken to ensure that sentences use a variety of constructs. All possible constructs including simple as well as complex ones are incorporated in the set. The sentence set also contains all types of sentences such as declarative, interrogative, imperative and exclamatory. Sentence length is not restricted although care has been taken that single sentences do not become too long. Following table shows the test data set: Table 1: Test data set for the evaluation of Hindi to Punjabi Machine Translation DTSWeo nctaruldenmscent 91DN03ae, 4wil0ys A5230,1rt6ic70lS4esytO0LQ38m6,1au5f4no9itg3c5e1uiaslgeB5130,lo6g50 L29105i,te84r05atue 3.2 Experiments It is also important to choose appropriate evaluators for our experiments. Thus, depending upon the requirements and need of the above mentioned tests, 50 People of different professions were selected for performing experiments. 20 Persons were from villages that only knew Punjabi and did not know Hindi and 30 persons were from different professions having knowledge of both Hindi and Punjabi. Average ratings for the sentences of the individual translations were then summed up (separately according to intelligibility and accuracy) to get the average scores. Percentage of accurate sentences and intelligent sentences was also calculated separately sentences. by counting the number of 3.2.1 Intelligibility Evaluation 5 The evaluators do not have any clue about the source language i.e. Hindi. They judge each sentence (in target language i.e. Punjabi) on the basis of its comprehensibility. The target user is a layman who is interested only in the comprehensibility of translations. Intelligibility is effected by grammatical errors, mistranslations, and un-translated words. 3.2.1.1 Results The response by the evaluators were analysed and following are the results: • 70.3 % sentences got the score 3 i.e. they were perfectly clear and intelligible. • 25. 1 % sentences got the score 2 i.e. they were generally clear and intelligible. • 3.5 % sentences got the score 1i.e. they were hard to understand. • 1. 1 % sentences got the score 0 i.e. they were not understandable. So we can say that about 95.40 % sentences are intelligible. These sentences are those which have score 2 or above. Thus, we can say that the direct approach can translate Hindi text to Punjabi Text with a consideably good accuracy. 3.2.2 Accuracy Evaluation / Fidelity Measure The evaluators are provided with source text along with translated text. A highly intelligible output sentence need not be a correct translation of the source sentence. It is important to check whether the meaning of the source language sentence is preserved in the translation. This property is called accuracy. 3.2.2.1 Results Initially Null Hypothesis is assumed i.e. the system’s performance is NULL. The author assumes that system is dumb and does not produce any valuable output. By the intelligibility of the analysis and Accuracy analysis, it has been proved wrong. The accuracy percentage for the system is found out to be 87.60% Further investigations reveal that out of 13.40%: • 80.6 % sentences achieve a match between 50 to 99% • 17.2 % of remaining sentences were marked with less than 50% match against the correct sentences. • Only 2.2 % sentences are those which are found unfaithful. A match of lower 50% does not mean that the sentences are not usable. After some post editing, they can fit properly in the translated text. (Goyal, V., Lehal, G.S., 2009b) 3.2.2 BLEU Score: As there is no Hindi –Parallel Corpus was available, thus for testing the system automatically, we generated Hindi-Parallel Corpus of about 10K Sentences. The BLEU score comes out to be 0.7801. 5 Conclusion In this paper, a hybrid translation approach for translating the text from Hindi to Punjabi has been presented. The proposed architecture has shown extremely good results and if found to be appropriate for MT systems between closely related language pairs. Copyright The developed system has already been copyrighted with The Registrar, Punjabi University, Patiala with authors same as the authors of the publication. Acknowlegement We are thankful to Dr. Amba Kulkarni, University of Hyderabad for her support in providing technical assistance for developing this system. References Bharati, Akshar, Chaitanya, Vineet, Kulkarni, Amba P., Sangal, Rajeev. 1997. Anusaaraka: Machine Translation in stages. Vivek, A Quarterly in Artificial Intelligence, Vol. 10, No. 3. ,NCST, Banglore. India, pp. 22-25. 6 Goyal V., Lehal G.S. 2008. Comparative Study of Hindi and Punjabi Language Scripts, Napalese Linguistics, Journal of the Linguistics Society of Nepal, Volume 23, November Issue, pp 67-82. Goyal V., Lehal, G. S. 2008a. Hindi Morphological Analyzer and Generator. In Proc.: 1st International Conference on Emerging Trends in Engineering and Technology, Nagpur, G.H.Raisoni College of Engineering, Nagpur, July16-19, 2008, pp. 11561159, IEEE Computer Society Press, California, USA. Goyal V., Lehal G.S. 2009. Advances in Machine Translation Systems, Language In India, Volume 9, November Issue, pp. 138-150. Goyal V., Lehal G.S. 2009a. A Machine Transliteration System for Machine Translation System: An Application on Hindi-Punjabi Language Pair. Atti Della Fondazione Giorgio Ronchi (Italy), Volume LXIV, No. 1, pp. 27-35. Goyal V., Lehal G.S. 2009b. Evaluation of Hindi to Punjabi Machine Translation System. International Journal of Computer Science Issues, France, Vol. 4, No. 1, pp. 36-39. Goyal V., Lehal G.S. 2010. Automatic Spelling Standardization for Hindi Text. In : 1st International Conference on Computer & Communication Technology, Moti Lal Nehru National Institute of technology, Allhabad, Sepetember 17-19, 2010, pp. 764-767, IEEE Computer Society Press, California. Goyal V., Lehal G.S. 2011. N-Grams Based Word Sense Disambiguation: A Case Study of Hindi to Punjabi Machine Translation System. International Journal of Translation. (Accepted, In Print). Goyal V., Lehal G.S. 2011a. Hindi to Punjabi Machine Translation System. In Proc.: International Conference for Information Systems for Indian Languages, Department of Computer Science, Punjabi University, Patiala, March 9-11, 2011, pp. 236-241, Springer CCIS 139, Germany. Sharma R., Goyal V. 2011b. Named Entity Recognition Systems for Hindi using CRF Approach. In Proc.: International Conference for Information Systems for Indian Languages, Department of Computer Science, Punjabi University, Patiala, March 9-11, 2011, pp. 31-35, Springer CCIS 139, Germany.</p><p>3 0.63126636 <a title="242-lda-3" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>Author: Ning Xi ; Guangchao Tang ; Boyuan Li ; Yinggong Zhao</p><p>Abstract: In this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. Instead of combining word alignments of different models (Xiang et al., 2010), we try to combine word alignments over multiple monolingually motivated word segmentation. Our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. Our combination algorithm is simple, efficient, and easy to implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1</p><p>4 0.57164109 <a title="242-lda-4" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>Author: Ryo Nagata ; Edward Whittaker ; Vera Sheinman</p><p>Abstract: The availability of learner corpora, especially those which have been manually error-tagged or shallow-parsed, is still limited. This means that researchers do not have a common development and test set for natural language processing of learner English such as for grammatical error detection. Given this background, we created a novel learner corpus that was manually error-tagged and shallowparsed. This corpus is available for research and educational purposes on the web. In this paper, we describe it in detail together with its data-collection method and annotation schemes. Another contribution of this paper is that we take the first step toward evaluating the performance of existing POStagging/chunking techniques on learner corpora using the created corpus. These contributions will facilitate further research in related areas such as grammatical error detection and automated essay scoring.</p><p>5 0.56210333 <a title="242-lda-5" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>Author: Michael Bendersky ; W. Bruce Croft ; David A. Smith</p><p>Abstract: W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA cro ft @ c s .uma s s .edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@ c s .umas s .edu articles or web pages). As previous research shows, these differences severely limit the applicability of Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an impor- tant part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</p><p>6 0.55859494 <a title="242-lda-6" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>7 0.55814505 <a title="242-lda-7" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>8 0.55767375 <a title="242-lda-8" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>9 0.5569424 <a title="242-lda-9" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>10 0.55440438 <a title="242-lda-10" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>11 0.55389947 <a title="242-lda-11" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>12 0.55205595 <a title="242-lda-12" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>13 0.55082792 <a title="242-lda-13" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.54984981 <a title="242-lda-14" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>15 0.54929578 <a title="242-lda-15" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>16 0.54925734 <a title="242-lda-16" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>17 0.54885656 <a title="242-lda-17" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>18 0.54870415 <a title="242-lda-18" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>19 0.54845464 <a title="242-lda-19" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>20 0.54815811 <a title="242-lda-20" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
