<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-148" href="#">acl2011-148</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</h1>
<br/><p>Source: <a title="acl-2011-148-pdf" href="http://aclweb.org/anthology//P/P11/P11-2006.pdf">pdf</a></p><p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>Reference: <a title="acl-2011-148-reference" href="../acl2011_reference/acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp  Abstract In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. [sent-3, score-0.74]
</p><p>2 In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. [sent-4, score-0.729]
</p><p>3 The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. [sent-5, score-0.67]
</p><p>4 Experimental results on a variation of the lexical sample task show the effectiveness of our method. [sent-6, score-0.029]
</p><p>5 These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. [sent-9, score-0.386]
</p><p>6 Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al. [sent-14, score-0.604]
</p><p>7 Given seed instances {New York City, wMeabld civoersp Islands}, bootstrapping might learn, ka tC one point ovefs st Ihsel iteration, patterns lgik me “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. [sent-17, score-0.651]
</p><p>8 In this case, a later iteration would likely acquire frequent words co-occurring with these generic patterns, such as Michael Jackson. [sent-18, score-0.055]
</p><p>9 Previous work has tried to reduce the effect of semantic drift by making the stop list of instances that must not be extracted (Curran et al. [sent-19, score-0.626]
</p><p>10 In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). [sent-23, score-0.808]
</p><p>11 The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). [sent-25, score-0.275]
</p><p>12 (2008)  pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. [sent-27, score-0.413]
</p><p>13 cc ia2t0io1n1 f Aors Cocoimatpiounta ftoiorn Caolm Lipnugtuaitsiotincasl:s Lhionrgtpuaisptiecrs , pages 30–36, avoid semantic drift, in this paper we take advantage of this similarity to make better use of Espresso. [sent-31, score-0.054]
</p><p>14 We demonstrate the effectiveness of our approach on a word sense disambiguation task. [sent-32, score-0.107]
</p><p>15 2 Background In this section, we review related work on seed selection and stop list construction. [sent-33, score-0.612]
</p><p>16 We also briefly introduce the Espresso bootstrapping algorithm (Pantel and Pennacchiotti, 2006) for which we build our seed selection and stop list construction methods. [sent-34, score-0.838]
</p><p>17 1 Seed Selection The performance of bootstrapping can be greatly influenced by a number of factors such as the size of the seed set, the composition of the seed set and the coherence of the concept being expanded (Vyas et al. [sent-36, score-0.774]
</p><p>18 (2009) studied the impact of  the composition of the seed sets on the expansion performance, confirming that seed set composition has a significant impact on the quality of expansions. [sent-39, score-0.643]
</p><p>19 They also found that the seeds chosen by non-expert editors are often worse than randomly chosen ones. [sent-40, score-0.295]
</p><p>20 A similar observation was made by McIntosh and Curran (2009), who reported that randomly chosen seeds from the gold-standard set often outperformed seeds chosen by domain experts. [sent-41, score-0.552]
</p><p>21 These results suggest that even for humans, selecting good seeds is a non-trivial task. [sent-42, score-0.235]
</p><p>22 (2002) proposed to run multiple bootstrapping sessions in parallel, with each session trying to extract one of several mutually exclusive semantic classes. [sent-45, score-0.25]
</p><p>23 Thus, the instances harvested in one bootstrapping session can be used as the stop list of the other sessions. [sent-46, score-0.584]
</p><p>24 (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. [sent-48, score-0.264]
</p><p>25 While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. [sent-49, score-0.691]
</p><p>26 To ap-  ply the idea of multi-class bootstrapping to singleclass bootstrapping,  one has to first find appropri-  ate competing semantic classes and good seeds for them, which is in itself a difficult problem. [sent-50, score-0.508]
</p><p>27 , τ do 11: p ← ATi 12: pSc ←ale A p so that the components sum to one. [sent-54, score-0.027]
</p><p>28 13: p ← SELECTKBEST(p,k) 14: ip ← Ap 15: iS ←cale A ip so that the components sum to one. [sent-55, score-0.071]
</p><p>29 16: i← SELECTKBEST(i,m) 17: return iand p 18: function SELECTKBEST(v,k) 19: Retain only the k largest components of v, resetting the remaining components to 0. [sent-56, score-0.089]
</p><p>30 20:  return v  ×  clustering to find competing semantic classes (negative categories). [sent-57, score-0.077]
</p><p>31 3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al. [sent-59, score-0.196]
</p><p>32 Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch u¨tze, 1999) between instances and patterns to evaluate their reliability. [sent-62, score-0.193]
</p><p>33 Let n be the number of all instances in the corpus, and p the number of all possible patterns. [sent-63, score-0.112]
</p><p>34 We denote all pmi values as an n p instance-pattern me daterinxo A, lwli tphm tih vea (i, j) aesle amne nn×t o pf A holding the value of pmi between the ith instance and the jth pattern. [sent-64, score-0.094]
</p><p>35 The input vector i0 (called seed vector) is an ndimensional binary vector with 1 at the ith component for every seed instance i, and 0 elsewhere. [sent-67, score-0.598]
</p><p>36 The algorithm outputs an n-dimensional vector iand an p-dimensional vector p, respectively representing the final scores of instances and patterns. [sent-68, score-0.197]
</p><p>37 Note that for brevity, the pseudocode assumes fixed numbers (k and m) of components in iand p are carried over to the subsequent iteration, but the original Espresso allows them to gradually increase with the number of iterations. [sent-69, score-0.096]
</p><p>38 (2008) pointed out the similarity between Espresso and Kleinberg’s HITS web page ranking algorithm (Kleinberg, 1999). [sent-72, score-0.028]
</p><p>39 Indeed, if we remove the pattern/instance selection steps of Algorithm 1 (lines 13 and 16), the algorithm essentially reduces to HITS. [sent-73, score-0.062]
</p><p>40 In this case, the outputs i and p match respectively the hubness and authority score vectors of HITS, computed on the bipartite graph of instances and patterns induced by matrix A. [sent-74, score-0.241]
</p><p>41 An implication of this algorithmic similarity is that the outputs of Espresso are inherently biased towards the HITS vectors, which is likely to be  the cause of semantic drift. [sent-75, score-0.091]
</p><p>42 Even though the pattern/instance selection steps in Espresso reduce such a bias to some extent, the bias still persists, as empirically verified by Komachi et al. [sent-76, score-0.085]
</p><p>43 In other words, the expansion process does not drift in random directions, but tend towards the set of instances and patterns with the highest HITS scores, regardless of the target semantic class. [sent-78, score-0.485]
</p><p>44 We exploit this observation in seed selection and stop list construction for Espresso, in order to reduce semantic drift. [sent-79, score-0.719]
</p><p>45 First, compute the HITS ranking of instances in the graph induced by the pmi matrix A. [sent-83, score-0.224]
</p><p>46 Next, check the top instances in the HITS ranking list manually, and see if these belong to the target class. [sent-86, score-0.235]
</p><p>47 (a) If the top instances are of the target class, use them as the seeds. [sent-89, score-0.141]
</p><p>48 32 (b) If not, these instances are likely to make a vector for which semantic drift is directed; hence, use them as the stop list. [sent-91, score-0.562]
</p><p>49 In this case, the seed set must be prepared manually, just like the usual bootstrapping procedure. [sent-92, score-0.47]
</p><p>50 Run Espresso with the seeds or stop list found in the last step. [sent-94, score-0.511]
</p><p>51 4  Experimental Setup  We evaluate our methods on a variant of the lexical sample word sense disambiguation task. [sent-95, score-0.136]
</p><p>52 In the lexical sample task, a small pre-selected set of a target word is given, along with an inventory of senses for each word (Jurafsky and Martin, 2008). [sent-96, score-0.118]
</p><p>53 Each word comes with a number of instances (context sentences) in which the target word occur, and some  of these sentences are manually labeled with the correct sense of the target word in each context. [sent-97, score-0.238]
</p><p>54 The goal of the task is to classify unlabeled context sentences by the sense of the target word in each context, using the set of labeled sentences. [sent-98, score-0.097]
</p><p>55 To apply Espresso for this task, we reformulate the task to be that of seed set expansion, and not classification. [sent-99, score-0.274]
</p><p>56 That is, the hand-labeled sentences having the same sense label are used as the seed set, and it is expanded over all the remaining (unlabeled) sentences. [sent-100, score-0.342]
</p><p>57 The reason we use the lexical sample task is that every sentence (instance) belongs to one of the predefined senses (classes), and we can expect the most frequent sense in the corpus to form the highest HITS ranking instances. [sent-101, score-0.21]
</p><p>58 This allows us to completely automate our experiments, without the need to manually check the HITS ranking in Step 2 of Section 3. [sent-102, score-0.028]
</p><p>59 That is, for the most frequent sense (majority sense), we take Step 3a and use the highest ranked instances as seeds; for the rest of the senses (minority senses), we take Step 3b and use them as the stop list. [sent-104, score-0.475]
</p><p>60 1 Datasets  We used the seven most frequent polysemous nouns (arm, bank, degree, difference, paper, party and shelter) in the SENSEVAL-3 dataset, and line (Leacock et al. [sent-106, score-0.143]
</p><p>61 , 1993) and interest (Bruce and Wiebe, Task  arm bank degree difference paper party shelter line interest 80. [sent-107, score-0.382]
</p><p>62 8  Table 1: Comparison of seed selection for Espresso (τ standard deviation). [sent-325, score-0.336]
</p><p>63 For Random, results are reported as (mean  All figures are expressed in percentage term=s. [sent-330, score-0.022]
</p><p>64 ”  ±  rleis trse pthoret evdal auses (m macro-  averaged over the nine tasks. [sent-333, score-0.036]
</p><p>65 , 2008), we used two  types of features extracted from neighboring contexts: collocational features and bag-of-words features. [sent-337, score-0.04]
</p><p>66 For collocational features, we set a window of three words to the right and left of the target word. [sent-338, score-0.069]
</p><p>67 2  Evaluation methodology  We run Espresso on the above datasets using different seed selection methods (for majority sense oftarget words), and with or without stop lists created by our method (for minority senses of target words). [sent-340, score-0.752]
</p><p>68 The output of Espresso may contain seed instances input to the system, but seeds are excluded from the evaluation. [sent-343, score-0.621]
</p><p>69 1 Effect of Seed Selection  We first evaluate the performance of our seed selection method for the majority sense of the nine polysemous nouns. [sent-349, score-0.47]
</p><p>70 Table 1 shows the performance of Espresso with the seeds chosen by the proposed HITS-based seed selection method (HITS), and with the seed sets randomly chosen from the gold standard sets (Random; baseline). [sent-350, score-0.905]
</p><p>71 We set the number of seeds nseed = 7 and number of iterations τ = 5 in this experiment. [sent-352, score-0.325]
</p><p>72 Especially, the MAP reported in Table 1 shows that our approach achieved improvements of 10 percentage points on bank, 6. [sent-354, score-0.056]
</p><p>73 AUC and R-precision mostly exhibit a trend similar to MAP, except R-precision in arm and shelter, for which the baseline is better. [sent-358, score-0.073]
</p><p>74 It can be seen from the P@n (P@30, P@50 and P@ 100) reported in Table 1that our approach performed considerably better than baseline, e. [sent-359, score-0.022]
</p><p>75 , around 17–20 points above Task  Method  MAP  AUC  R-Precision  P@ 10  P@20  P@30  arm bank degree difference paper party shelter line interest 13. [sent-361, score-0.379]
</p><p>76 4  Table 2: Effect of stop lists for Espresso (nstop  = 10, nseed = 10, τ = 20). [sent-804, score-0.322]
</p><p>77 Results are reported as (mean  ±  standard  deviation). [sent-805, score-0.022]
</p><p>78 ” Rshesowulsts st ahere evra elupeors macro-averaged over arldl  nine tasks. [sent-808, score-0.036]
</p><p>79 2  Effect of Stop List  Table 2 shows the performance of Espresso using the stop list built with our proposed method (HITS), compared with the vanilla Espresso not using any stop list (NoStop). [sent-811, score-0.552]
</p><p>80 In this case, the size of the stop list is set to nstop = 10, and the number of seeds nseed = 10 and iterations τ = 20. [sent-812, score-0.646]
</p><p>81 For both HITS and NoStop, the seeds are  selected at random from the gold standard data, and the reported results were averaged over 50 runs of each system. [sent-813, score-0.304]
</p><p>82 Due to lack of space, only the results for the second most frequent sense for each word are reported; i. [sent-814, score-0.093]
</p><p>83 , the results for more minor senses are not in the table. [sent-816, score-0.06]
</p><p>84 As shown in the table, our method (HITS) outperforms the baseline not using a stop list (NoStop), in all evaluation metrics. [sent-818, score-0.276]
</p><p>85 In particular, the P@n listed in Table 2 shows that our method provides about 11percentage points absolute improvement over the baseline on interest, for all n = 10, 20, and 30. [sent-819, score-0.034]
</p><p>86 34 6  Conclusions  We have proposed a HITS-based method for alleviating semantic drift in the bootstrapping algorithm Espresso. [sent-820, score-0.411]
</p><p>87 Our idea is built around the concept of hubs in the sense of Kleinberg’s HITS algorithm, as well as the algorithmic similarity between Espresso and HITS. [sent-821, score-0.105]
</p><p>88 Hub instances are influential and hence make good seeds if they are of the target semantic class, but otherwise, they may trigger semantic  drift. [sent-822, score-0.484]
</p><p>89 We have demonstrated that our method works effectively on lexical sample tasks. [sent-823, score-0.029]
</p><p>90 We are currently evaluating our method on other bootstrapping tasks, including named entity extraction. [sent-824, score-0.219]
</p><p>91 Acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns. [sent-829, score-0.095]
</p><p>92 Minimally supervised learning of semantic knowledge from query logs. [sent-879, score-0.054]
</p><p>93 Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. [sent-883, score-0.411]
</p><p>94 Espresso: Leveraging generic patterns for automatically harvesting semantic relations. [sent-914, score-0.101]
</p><p>95 A bootstrapping method for learning semantic lexicons using extraction pattern contexts. [sent-932, score-0.25]
</p><p>96 Helping editors choose better seed sets for entity set expansion. [sent-936, score-0.297]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hits', 0.451), ('espresso', 0.442), ('nostop', 0.294), ('seed', 0.274), ('seeds', 0.235), ('stop', 0.21), ('bootstrapping', 0.196), ('komachi', 0.165), ('drift', 0.161), ('instances', 0.112), ('nseed', 0.09), ('shelter', 0.09), ('curran', 0.089), ('mcintosh', 0.08), ('kleinberg', 0.078), ('arm', 0.073), ('pennacchiotti', 0.073), ('pantel', 0.07), ('sense', 0.068), ('selectkbest', 0.068), ('list', 0.066), ('auc', 0.066), ('party', 0.063), ('selection', 0.062), ('senses', 0.06), ('mamoru', 0.06), ('bank', 0.057), ('tara', 0.055), ('vyas', 0.055), ('semantic', 0.054), ('pmi', 0.047), ('patterns', 0.047), ('random', 0.047), ('bharat', 0.045), ('hubness', 0.045), ('ittoo', 0.045), ('nstop', 0.045), ('collocational', 0.04), ('thelen', 0.04), ('nara', 0.04), ('disambiguation', 0.039), ('algorithmic', 0.037), ('yuji', 0.037), ('interest', 0.037), ('shimbo', 0.037), ('matrix', 0.037), ('nine', 0.036), ('expansion', 0.035), ('iand', 0.035), ('abe', 0.034), ('cutoff', 0.034), ('pseudocode', 0.034), ('map', 0.034), ('sigir', 0.034), ('mutual', 0.034), ('points', 0.034), ('yarowsky', 0.033), ('porter', 0.033), ('yoshida', 0.033), ('masashi', 0.033), ('exclusion', 0.033), ('riloff', 0.032), ('ellen', 0.031), ('iteration', 0.03), ('composition', 0.03), ('construction', 0.03), ('yangarber', 0.03), ('polysemous', 0.03), ('chosen', 0.03), ('target', 0.029), ('sample', 0.029), ('steedman', 0.029), ('sch', 0.029), ('ranking', 0.028), ('minority', 0.027), ('leacock', 0.027), ('abney', 0.027), ('components', 0.027), ('clark', 0.026), ('ijcnlp', 0.026), ('frequent', 0.025), ('vector', 0.025), ('line', 0.025), ('cooccurrence', 0.024), ('hinrich', 0.024), ('mcclosky', 0.024), ('manning', 0.024), ('james', 0.024), ('competing', 0.023), ('bruce', 0.023), ('entity', 0.023), ('reduce', 0.023), ('deviation', 0.022), ('reported', 0.022), ('conference', 0.022), ('lists', 0.022), ('ip', 0.022), ('rebecca', 0.022), ('helping', 0.022), ('irrelevant', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="148-tfidf-1" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>2 0.19708465 <a title="148-tfidf-2" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>Author: Kugatsu Sadamitsu ; Kuniko Saito ; Kenji Imamura ; Genichiro Kikui</p><p>Abstract: This paper proposes three modules based on latent topics of documents for alleviating “semantic drift” in bootstrapping entity set expansion. These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsupervised way. Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain.</p><p>3 0.19070417 <a title="148-tfidf-3" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>Author: Mitesh M. Khapra ; Salil Joshi ; Arindam Chatterjee ; Pushpak Bhattacharyya</p><p>Abstract: Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.</p><p>4 0.18119211 <a title="148-tfidf-4" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>Author: Tara McIntosh ; Lars Yencken ; James R. Curran ; Timothy Baldwin</p><p>Abstract: State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.</p><p>5 0.14399457 <a title="148-tfidf-5" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>6 0.12783672 <a title="148-tfidf-6" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>7 0.12138372 <a title="148-tfidf-7" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>8 0.1209768 <a title="148-tfidf-8" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>9 0.1168511 <a title="148-tfidf-9" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>10 0.10915493 <a title="148-tfidf-10" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>11 0.10149345 <a title="148-tfidf-11" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>12 0.073771231 <a title="148-tfidf-12" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>13 0.069368854 <a title="148-tfidf-13" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>14 0.064713985 <a title="148-tfidf-14" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>15 0.062746562 <a title="148-tfidf-15" href="./acl-2011-Towards_Tracking_Semantic_Change_by_Visual_Analytics.html">307 acl-2011-Towards Tracking Semantic Change by Visual Analytics</a></p>
<p>16 0.061409831 <a title="148-tfidf-16" href="./acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary.html">158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</a></p>
<p>17 0.058118403 <a title="148-tfidf-17" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>18 0.057108257 <a title="148-tfidf-18" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>19 0.05484065 <a title="148-tfidf-19" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>20 0.053507954 <a title="148-tfidf-20" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.144), (1, 0.055), (2, -0.081), (3, -0.0), (4, 0.018), (5, -0.034), (6, 0.07), (7, 0.006), (8, -0.043), (9, -0.011), (10, -0.016), (11, -0.097), (12, 0.168), (13, -0.002), (14, -0.001), (15, -0.117), (16, 0.056), (17, -0.033), (18, 0.026), (19, 0.029), (20, -0.045), (21, 0.045), (22, 0.06), (23, 0.096), (24, -0.042), (25, 0.032), (26, 0.108), (27, 0.24), (28, 0.087), (29, 0.019), (30, 0.07), (31, -0.038), (32, -0.02), (33, -0.025), (34, 0.089), (35, 0.071), (36, 0.124), (37, 0.037), (38, 0.003), (39, 0.02), (40, 0.007), (41, -0.155), (42, -0.118), (43, 0.019), (44, -0.029), (45, 0.083), (46, 0.159), (47, 0.041), (48, -0.039), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94917494 <a title="148-lsi-1" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>2 0.79168946 <a title="148-lsi-2" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>Author: Mitesh M. Khapra ; Salil Joshi ; Arindam Chatterjee ; Pushpak Bhattacharyya</p><p>Abstract: Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.</p><p>3 0.71327394 <a title="148-lsi-3" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>Author: Dmitriy Dligach ; Martha Palmer</p><p>Abstract: Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</p><p>4 0.590671 <a title="148-lsi-4" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>Author: Ahmed Hassan ; Amjad AbuJbara ; Rahul Jha ; Dragomir Radev</p><p>Abstract: We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.</p><p>5 0.58013904 <a title="148-lsi-5" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>6 0.57812619 <a title="148-lsi-6" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>7 0.53661788 <a title="148-lsi-7" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>8 0.46988741 <a title="148-lsi-8" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>9 0.45585534 <a title="148-lsi-9" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>10 0.44496787 <a title="148-lsi-10" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>11 0.44221026 <a title="148-lsi-11" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>12 0.4300186 <a title="148-lsi-12" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>13 0.40153798 <a title="148-lsi-13" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>14 0.39394298 <a title="148-lsi-14" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>15 0.37178382 <a title="148-lsi-15" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>16 0.35626858 <a title="148-lsi-16" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>17 0.331514 <a title="148-lsi-17" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>18 0.32808927 <a title="148-lsi-18" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>19 0.32378054 <a title="148-lsi-19" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>20 0.32036754 <a title="148-lsi-20" href="./acl-2011-Predicting_Relative_Prominence_in_Noun-Noun_Compounds.html">249 acl-2011-Predicting Relative Prominence in Noun-Noun Compounds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.013), (5, 0.02), (17, 0.039), (25, 0.022), (26, 0.015), (37, 0.069), (39, 0.027), (41, 0.045), (55, 0.019), (59, 0.046), (72, 0.022), (91, 0.45), (96, 0.105), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89672267 <a title="148-lda-1" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>Author: Oliver Schneider ; Alex Garnett</p><p>Abstract: We present ConsentCanvas, a system which structures and “texturizes” End-User License Agreement (EULA) documents to be more readable. The system aims to help users better understand the terms under which they are providing their informed consent. ConsentCanvas receives unstructured text documents as input and uses unsupervised natural language processing methods to embellish the source document using a linked stylesheet. Unlike similar usable security projects which employ summarization techniques, our system preserves the contents of the source document, minimizing the cognitive and legal burden for both the end user and the licensor. Our system does not require a corpus for training. 1</p><p>2 0.87189484 <a title="148-lda-2" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>same-paper 3 0.86485815 <a title="148-lda-3" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>4 0.83900762 <a title="148-lda-4" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>5 0.78151107 <a title="148-lda-5" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>Author: David Chiang ; Steve DeNeefe ; Michael Pust</p><p>Abstract: We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.</p><p>6 0.68105221 <a title="148-lda-6" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>7 0.58949554 <a title="148-lda-7" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>8 0.58879125 <a title="148-lda-8" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>9 0.57099003 <a title="148-lda-9" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>10 0.56387854 <a title="148-lda-10" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>11 0.53333753 <a title="148-lda-11" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>12 0.53302836 <a title="148-lda-12" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>13 0.52221775 <a title="148-lda-13" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>14 0.5133158 <a title="148-lda-14" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>15 0.51124161 <a title="148-lda-15" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>16 0.4874008 <a title="148-lda-16" href="./acl-2011-An_Interface_for_Rapid_Natural_Language_Processing_Development_in_UIMA.html">42 acl-2011-An Interface for Rapid Natural Language Processing Development in UIMA</a></p>
<p>17 0.48107463 <a title="148-lda-17" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>18 0.48102117 <a title="148-lda-18" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>19 0.47990194 <a title="148-lda-19" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>20 0.47712332 <a title="148-lda-20" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
