<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-201" href="#">acl2011-201</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</h1>
<br/><p>Source: <a title="acl-2011-201-pdf" href="http://aclweb.org/anthology//P/P11/P11-1110.pdf">pdf</a></p><p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>Reference: <a title="acl-2011-201-reference" href="../acl2011_reference/acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. [sent-2, score-0.49]
</p><p>2 Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. [sent-3, score-0.423]
</p><p>3 We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. [sent-4, score-0.583]
</p><p>4 We show how different summaries use various phrasal information units (i. [sent-5, score-0.282]
</p><p>5 Finally, we present a ranker that employs distributional similarities to build a net-  work of words, and captures the diversity of perspectives by detecting communities in this network. [sent-8, score-0.433]
</p><p>6 In this paper, we focus on the computational analysis of collective discourse, a collective behavior seen in interactive content contribution and text summarization in online social media. [sent-12, score-0.418]
</p><p>7 One scenario leading to collective reaction to a welldefined subject is when an event occurs (a movie is released, a story occurs, a paper is published) and people independently write about it (movie reviews, news headlines, citation sentences). [sent-17, score-0.401]
</p><p>8 In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009). [sent-24, score-0.328]
</p><p>9 The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maximal Marginal Relevance (MMR). [sent-43, score-0.256]
</p><p>10 These papers try to increase diversity in summarizing documents, but do not explain the type ofthe diversity in their inputs. [sent-46, score-0.386]
</p><p>11 In this paper, we give an insightful discussion on the nature of the diversity seen in collective dis1099 course, and will explain why some of the mentioned methods may not work under such environments. [sent-47, score-0.362]
</p><p>12 Teufel and van Halteren also used 6 DUC1-provided summaries, and annotations from 10 student participants and 4 additional researchers, to create 20 summaries for another news article in the DUC datasets. [sent-50, score-0.369]
</p><p>13 The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. [sent-52, score-0.519]
</p><p>14 These evaluation methods assess the information content in the summaries that are generated automatically. [sent-54, score-0.321]
</p><p>15 Finally, recent research on analyzing online so-  cial media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al. [sent-55, score-0.378]
</p><p>16 The headlines datasets consist of 25 clusters of news headlines collected from Google News2, and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL Anthology Network (AAN)3. [sent-63, score-1.171]
</p><p>17 Each cluster consists of a number of unique summaries (headlines or citations) about the same artifact (non-  evolving news story or scientific paper) written by  1 lists some of the with the number of summaries in them. [sent-64, score-0.747]
</p><p>18 7  Table 1: Some of the annotated datasets and the number of summaries in each of them (hdl  = headlines;  cit  = cita-  tions)  3. [sent-81, score-0.363]
</p><p>19 Different nuggets may all represent the same atomic semantic unit, which we call as a factoid. [sent-87, score-0.401]
</p><p>20 In the following headlines, which are randomly extracted from the redsox dataset, nuggets are manually underlined. [sent-88, score-0.462]
</p><p>21 red sox win 2007 world series boston red sox blank rockies to clinch world series 2news. [sent-89, score-1.142]
</p><p>22 edu/clair/anthology/  boston fans celebrate world series win;  1100 37 arrests re-  ported These 3 headlines contain 9 nuggets, which represent 5 factoids or classes of equivalent nuggets. [sent-94, score-0.991]
</p><p>23 In the former case different nuggets are used to represent the same factoid, while in the latter case different nuggets are used to express different factoids. [sent-103, score-0.721]
</p><p>24 This analogy is similar to the definition of factoids in (van Halteren and Teufel, 2004). [sent-104, score-0.329]
</p><p>25 where the first author mentions “pharaoh” as a contribution of Koehn et al, but the second and third use different nuggets to represent the same contribution: use of trigrams. [sent-110, score-0.373]
</p><p>26 The use ofphrasal information as nuggets is an essential element to our experiments, since some headline writers often try to use uncommon terms to refer to a factoid. [sent-112, score-0.424]
</p><p>27 For instance, two headlines from the redsox cluster are: Short wait for bossox this time Soxcess started upstairs Following these examples, we asked two annotators to annotate all 1, 390 headlines, and 926 citations. [sent-113, score-0.555]
</p><p>28 To avoid such a difficulty, we enforced our annotators to extract non-overlapping nuggets from a summary to make sure that they are mutually independent and that information overlap between them is minimized. [sent-118, score-0.452]
</p><p>29 Finding agreement between annotated welldefined nuggets is straightforward and can be calculated in terms of Kappa. [sent-119, score-0.418]
</p><p>30 However, when nuggets themselves are to be extracted by annotators, the task becomes less obvious. [sent-120, score-0.348]
</p><p>31 These results suggest that  Pr1(a−)P−rP(er)(e)  human annotators can reach substantial agreement when bigram and trigram nuggets are examined, and has reasonable agreement for unigram nuggets. [sent-126, score-0.459]
</p><p>32 4  Diversity  We study the diversity of ways with which human summarizers talk about the same story or event and explain why such a diversity exists. [sent-127, score-0.507]
</p><p>33 4Before the annotations, we lower-cased all summaries and removed duplicates 5Previously (Qazvinian and Radev, 2010) have shown high agreement in human judgments in a similar task on citation annotation 1101 Average κ unigram bigram trigram Human1 vs. [sent-128, score-0.383]
</p><p>34 headlines  citations  P(Xcr)≥1 0 −021 01 c10Pr(2X≥c)P≥cX(r1 0 −120 1cPr(X≥c)102 Figure 1: The cumulative probability distribution for the frequency of factoids (i. [sent-142, score-0.776]
</p><p>35 , the probability that a factoid will be mentioned in c different summaries) across in each category. [sent-144, score-0.274]
</p><p>36 For each factoid in the annotated clusters, we extract its count, X, which is equal to the number of summaries it has been mentioned in, and then we look at the distribution of X. [sent-147, score-0.556]
</p><p>37 , the probability that a factoid will be mentioned in at least c different summaries) in both categories. [sent-150, score-0.274]
</p><p>38 These highly skewed distributions indicate that a large number of factoids (more than 28%) are only  mentioned once across different clusters (e. [sent-151, score-0.474]
</p><p>39 , “poor pitching of colorado” in the reds ox cluster), and that a few factoids are mentioned in a large number of headlines (likely using different nuggets). [sent-153, score-0.807]
</p><p>40 The large number of factoids that are only mentioned in one headline indicates that different summarizers increase diversity by focusing on different aspects of a story or a paper. [sent-154, score-0.77]
</p><p>41 The set of nuggets also exhibit similar skewed distributions. [sent-155, score-0.403]
</p><p>42 If we look at individual nuggets, the reds ox set shows that about 63 (or 80%) of the nuggets get mentioned in only one headline, resulting in a right-skewed distribution. [sent-156, score-0.455]
</p><p>43 The factoid analysis of the datasets reveals two main causes for the content diversity seen in headlines: (1) writers focus on different aspects of the story and therefore write about different factoids (e. [sent-157, score-0.941]
</p><p>44 (2) writer use different nuggets to represent the same factoid (e. [sent-161, score-0.613]
</p><p>45 Intivyezosr142608 01 NFaugctoeids10headlins1023 zIinyvsreto12305 0 10NFuacgtoeids10ctaions102 3 number of summaries  number of summaries  Figure 2: The number of unique factoids and nuggets observed by reading n random summaries in all the clusters of each category 4. [sent-166, score-1.668]
</p><p>46 2 Factoid Inventory The emergence of diversity in covering different factoids suggests that looking at more summaries will capture a larger number of factoids. [sent-167, score-0.804]
</p><p>47 In order to ana-  lyze the growth of the factoid inventory, we perform a simple experiment. [sent-168, score-0.24]
</p><p>48 We shuffle the set of summaries from all 25 clusters in each category, and then look at the number of unique factoids and nuggets seen after reading nth summary. [sent-169, score-1.069]
</p><p>49 This is important to study in order to find out whether we need a large number of summaries to capture all aspects of a story and build a complete factoid inventory. [sent-171, score-0.635]
</p><p>50 1 shows, at each n, the number of unique factoids and nuggets observed by reading n random summaries from the 25 clusters in each category. [sent-173, score-1.104]
</p><p>51 These curves are plotted on a semi-log scale to emphasize the difference between the growth patterns of the nugget inventories and the factoid inven1102 tories6. [sent-174, score-0.341]
</p><p>52 In their work, van Halteren and Teufel indicated that more than 10-20 human sum-  maries are needed for a full factoid inventory. [sent-176, score-0.291]
</p><p>53 However, our experiments with nuggets of nearly 2, 400 independent human summaries suggest that neither the nugget inventory nor the number of factoids will be likely to show asymptotic behavior. [sent-177, score-1.154]
</p><p>54 This means that a lot of the diversity seen in human summarization is a result of the so called different lexical choices that represent the same semantic units or factoids. [sent-179, score-0.281]
</p><p>55 However, a more important question to answer is whether these summaries all cover important aspects of the story. [sent-182, score-0.362]
</p><p>56 Here, we examine the quality of these summaries, study the distribution of information coverage in them, and investigate the number of summaries required to build a complete factoid inventory. [sent-183, score-0.522]
</p><p>57 The information covered in each summary can be determined by the set of factoids (and not nuggets) and their frequencies across the datasets. [sent-184, score-0.386]
</p><p>58 For example, in the reds ox dataset, “red sox”, “boston”, and  “boston red sox” are nuggets that all represent the same piece of information: the red sox team. [sent-185, score-0.937]
</p><p>59 Therefore, different summaries that use these nuggets to refer to the red sox team should not be seen as very different. [sent-186, score-1.004]
</p><p>60 Intuitively, factoids that are mentioned more frequently are more salient aspects of the story. [sent-188, score-0.406]
</p><p>61 Therefore, our pyramid model uses the normalized frequency at which a factoid is mentioned across a dataset as its weight. [sent-189, score-0.512]
</p><p>62 In the pyramid model, the individual factoids fall in tiers. [sent-190, score-0.567]
</p><p>63 If a factoid appears in more summaries, it falls in a higher tier. [sent-191, score-0.24]
</p><p>64 In principle, if the term wi appears |wi | times in the set of 6Similar experiment using individual clusters exhibit similar behavior headlines it is assigned to the tier T|wi| . [sent-192, score-0.586]
</p><p>65 Suppose the pyramid has n tiers, Ti, where tier Tn is the top tier and T1 is the bottom. [sent-194, score-0.33]
</p><p>66 The weight of the factoids in tier Ti will be i(i. [sent-195, score-0.375]
</p><p>67 n Idf Di |i sd etnheo neusm thbeer n uomf bfaecrt ooifd sfa cin-  ××  the summary that appear in Ti, then the total factoid weight for the summary is D = Pin=1 i Di. [sent-199, score-0.354]
</p><p>68 Additionally, the optimal pyramid scPore fori a summary is Max = Pin=1 i |Ti |. [sent-200, score-0.295]
</p><p>69 First, for each set we look at the variation in pyramid scores that individual summaries obtain in their set. [sent-203, score-0.548]
</p><p>70 Figure 3 shows, for each cluster, the variation in the pyramid scores (25th to 75th percentile range) of individual summaries evaluated against the factoids of that cluster. [sent-204, score-0.902]
</p><p>71 This figure indicates that the pyramid score of almost all summaries obtain values with high variations in most of the clusters For instance, individual headlines from redsox obtain pyramid scores as low as 0. [sent-205, score-1.295]
</p><p>72 Additionally, this figure shows that headlines generally obtain higher values than citations when con-  sidered as summaries. [sent-209, score-0.447]
</p><p>73 One reason, as explained before, is that a citation may not cover any important contribution of the paper it is citing, when headlines generally tend to cover some aspects of the story. [sent-210, score-0.528]
</p><p>74 But how many headlines should one read to capture a desired level of information content? [sent-212, score-0.342]
</p><p>75 To answer this question, we perform an experiment based on drawing random summaries from the pool of all the clusters in each category. [sent-213, score-0.398]
</p><p>76 We perform a Monte Carlo simulation, in which for each n, we draw n random summaries, and look at the pyramid score achieved by reading these headlines. [sent-214, score-0.302]
</p><p>77 The pyramid score is calculated using the factoids from all 25 clusters in each cate1103 Each experiment is repeated 1, 000 times to find the statistical significance of the experiment and the variation from the average pyramid scores. [sent-215, score-0.914]
</p><p>78 number of summaries Figure 4: Average pyramid score obtained by reading n random summaries shows rapid asymptotic behavior. [sent-221, score-0.912]
</p><p>79 5  Diversity-based Ranking  In previous sections we showed that the diversity seen in human summaries could be according to different nuggets or phrases that represent the same factoid. [sent-222, score-0.848]
</p><p>80 In this section, we use different state of the art summarization systems to rank the set of summaries in each cluster with re-  spect to information content and diversity. [sent-224, score-0.465]
</p><p>81 To evaluate each system, we cut the ranked list at a constant length (in terms of the number of words) and calculate the pyramid score of the remaining text. [sent-225, score-0.263]
</p><p>82 1 Distributional Similarity We have designed a summary ranker that will produce a ranked list of documents with respect to the diversity of their contents. [sent-227, score-0.316]
</p><p>83 In order to capture the nuggets of equivalent semantic classes, we use a distributional similarity of 7Similar experiment using individual clusters exhibit similar results  Figure 3: The 25th to 75th percentile pyramid score range in individual clusters  words that is inspired by (Lee, 1999). [sent-229, score-0.88]
</p><p>84 1s shheo wnusm part  pocliearb stfiuaonhpsywer pdcsontix2ledporglheivtyjpcnuogmrhypidnbgasetyPlajk  1104  Figure 5: Part of the word similarity graph in the redsox cluster of the word similarity graph in the redsox cluster, in which each node is color-coded with its community. [sent-243, score-0.348]
</p><p>85 In the headlines datasets, where most of the headlines cover some factoids about the story, we expect this method to perform reasonably well since randomization will increase the chances of covering headlines that focus on different factoids. [sent-254, score-1.392]
</p><p>86 However, in the citations  dataset, where a citing sentence may cover no information about the cited paper, randomization has the drawback of selecting citations that have no valuable information in them. [sent-255, score-0.277]
</p><p>87 DivRank uses a vertex-reinforced random walk model to rank graph nodes based on a diversity based centrality. [sent-271, score-0.306]
</p><p>88 The reason is that different nuggets that represent the same factoid  often have no words in common (e. [sent-288, score-0.613]
</p><p>89 3 Experiments We use each of the systems explained above to rank the summaries in each cluster. [sent-292, score-0.311]
</p><p>90 Each ranked list is then cut at a certain length (50 words for headlines, and 150 for citations) and the information content in the remaining text is examined using the pyramid score. [sent-293, score-0.302]
</p><p>91 All methods show similar results in the headlines category, where most headlines cover at least 1 factoid about the story and a random ranker performs reasonably well. [sent-296, score-1.107]
</p><p>92 Table 4 shows top 3 headlines from 3 rankers: word distributional similarity (WDS), CLexRank, and MMR. [sent-297, score-0.424]
</p><p>93 However, the second factoid is absent in the other two. [sent-354, score-0.24]
</p><p>94 6  Conclusion and Future Work  Our experiments on two different categories of human-written summaries (headlines and citations) showed that a lot of the diversity seen in human summarization comes from different nuggets that  may actually represent the same semantic information (i. [sent-355, score-0.911]
</p><p>95 We showed that the factoids exhibit a skewed distribution model, and that the size of the nugget inventory asymptotic behavior even with a large number of summaries. [sent-358, score-0.625]
</p><p>96 We also showed high variation in summary quality across different summaries in terms of pyramid score, and that the information covered by reading n summaries has a rapidly growing asymptotic behavior as n increases. [sent-359, score-1.008]
</p><p>97 ) in order to produce summaries that include diverse aspects of a story. [sent-366, score-0.325]
</p><p>98 Our work has resulted in a publicly available dataset 8 of 25 annotated news clusters with nearly 1, 400 headlines, and 25 clusters of citation sentences with more than 900 citations. [sent-367, score-0.267]
</p><p>99 Examining the consensus between human summaries: initial experiments with factoid analysis. [sent-524, score-0.24]
</p><p>100 Evaluating information content by factoid analysis: human annotation and stability. [sent-529, score-0.279]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nuggets', 0.348), ('headlines', 0.342), ('factoids', 0.329), ('summaries', 0.282), ('sox', 0.257), ('factoid', 0.24), ('pyramid', 0.238), ('diversity', 0.193), ('collective', 0.135), ('divrank', 0.129), ('dj', 0.126), ('red', 0.117), ('redsox', 0.114), ('citations', 0.105), ('nugget', 0.101), ('halteren', 0.098), ('wds', 0.086), ('teufel', 0.083), ('clusters', 0.081), ('headline', 0.076), ('lexrank', 0.074), ('boston', 0.072), ('hdl', 0.071), ('rockies', 0.071), ('story', 0.07), ('citation', 0.069), ('win', 0.067), ('mmr', 0.064), ('summarization', 0.063), ('qazvinian', 0.059), ('radev', 0.059), ('summary', 0.057), ('cit', 0.054), ('series', 0.053), ('communities', 0.053), ('cluster', 0.052), ('van', 0.051), ('adar', 0.05), ('fans', 0.05), ('walk', 0.049), ('inventory', 0.048), ('distributional', 0.048), ('annotators', 0.047), ('behavior', 0.046), ('asymptotic', 0.046), ('vahed', 0.046), ('tier', 0.046), ('wi', 0.046), ('celebrate', 0.043), ('reds', 0.043), ('aspects', 0.043), ('ranker', 0.041), ('dragomir', 0.04), ('world', 0.039), ('content', 0.039), ('perspectives', 0.038), ('erkan', 0.038), ('welldefined', 0.038), ('absorbing', 0.038), ('arrests', 0.038), ('network', 0.037), ('cover', 0.037), ('ranking', 0.036), ('news', 0.036), ('community', 0.036), ('random', 0.035), ('di', 0.035), ('sweep', 0.035), ('similarities', 0.034), ('similarity', 0.034), ('mentioned', 0.034), ('dr', 0.033), ('agreement', 0.032), ('skewed', 0.03), ('ox', 0.03), ('citing', 0.03), ('reading', 0.029), ('cosine', 0.029), ('rank', 0.029), ('leskovec', 0.029), ('pitching', 0.029), ('typhoon', 0.029), ('yule', 0.029), ('variation', 0.028), ('atomic', 0.028), ('datasets', 0.027), ('ti', 0.027), ('movie', 0.027), ('employs', 0.026), ('event', 0.026), ('carbonell', 0.025), ('eytan', 0.025), ('percentile', 0.025), ('summarizers', 0.025), ('exhibit', 0.025), ('scientific', 0.025), ('ranked', 0.025), ('represent', 0.025), ('al', 0.025), ('www', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000018 <a title="201-tfidf-1" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>2 0.20315997 <a title="201-tfidf-2" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>3 0.18917251 <a title="201-tfidf-3" href="./acl-2011-Automatic_Headline_Generation_using_Character_Cross-Correlation.html">51 acl-2011-Automatic Headline Generation using Character Cross-Correlation</a></p>
<p>Author: Fahad Alotaiby</p><p>Abstract: Arabic language is a morphologically complex language. Affixes and clitics are regularly attached to stems which make direct comparison between words not practical. In this paper we propose a new automatic headline generation technique that utilizes character cross-correlation to extract best headlines and to overcome the Arabic language complex morphology. The system that uses character cross-correlation achieves ROUGE-L score of 0. 19384 while the exact word matching scores only 0. 17252 for the same set of documents. 1</p><p>4 0.1564568 <a title="201-tfidf-4" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>5 0.14755856 <a title="201-tfidf-5" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>Author: Hajime Morita ; Tetsuya Sakai ; Manabu Okumura</p><p>Abstract: We propose a new method for query-oriented extractive multi-document summarization. To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. We then formulate the summarization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.3 13, a 36% improvement over a baseline using Maximal Marginal Relevance. 1</p><p>6 0.1255451 <a title="201-tfidf-6" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>7 0.12223788 <a title="201-tfidf-7" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>8 0.11597763 <a title="201-tfidf-8" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>9 0.098138638 <a title="201-tfidf-9" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>10 0.096387804 <a title="201-tfidf-10" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>11 0.095627718 <a title="201-tfidf-11" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>12 0.09290617 <a title="201-tfidf-12" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>13 0.085221343 <a title="201-tfidf-13" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>14 0.078495026 <a title="201-tfidf-14" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>15 0.06967812 <a title="201-tfidf-15" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>16 0.063388497 <a title="201-tfidf-16" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>17 0.062388349 <a title="201-tfidf-17" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>18 0.059476618 <a title="201-tfidf-18" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>19 0.059192624 <a title="201-tfidf-19" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>20 0.0552167 <a title="201-tfidf-20" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.151), (1, 0.099), (2, -0.042), (3, 0.069), (4, -0.051), (5, -0.003), (6, -0.064), (7, 0.13), (8, 0.025), (9, -0.054), (10, -0.076), (11, -0.033), (12, -0.155), (13, 0.021), (14, -0.182), (15, -0.109), (16, 0.017), (17, 0.038), (18, 0.048), (19, -0.017), (20, -0.067), (21, -0.075), (22, 0.113), (23, -0.001), (24, -0.001), (25, 0.018), (26, -0.034), (27, 0.049), (28, -0.015), (29, -0.024), (30, -0.105), (31, -0.023), (32, -0.019), (33, -0.029), (34, 0.07), (35, -0.018), (36, 0.011), (37, -0.012), (38, 0.009), (39, -0.04), (40, -0.015), (41, 0.006), (42, -0.042), (43, 0.082), (44, -0.026), (45, -0.035), (46, -0.01), (47, 0.01), (48, -0.048), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93432313 <a title="201-lsi-1" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>2 0.83401662 <a title="201-lsi-2" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>3 0.82846957 <a title="201-lsi-3" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>4 0.67786199 <a title="201-lsi-4" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>Author: Charles Greenbacker</p><p>Abstract: We propose a framework for generating an abstractive summary from a semantic model of a multimodal document. We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.</p><p>5 0.62348139 <a title="201-lsi-5" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Gillick ; Dan Klein</p><p>Abstract: We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.</p><p>6 0.61076701 <a title="201-lsi-6" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>7 0.60218155 <a title="201-lsi-7" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>8 0.60214341 <a title="201-lsi-8" href="./acl-2011-Automatic_Headline_Generation_using_Character_Cross-Correlation.html">51 acl-2011-Automatic Headline Generation using Character Cross-Correlation</a></p>
<p>9 0.59974396 <a title="201-lsi-9" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>10 0.59864795 <a title="201-lsi-10" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>11 0.58834249 <a title="201-lsi-11" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>12 0.55948508 <a title="201-lsi-12" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>13 0.54324931 <a title="201-lsi-13" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>14 0.5399304 <a title="201-lsi-14" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>15 0.52323353 <a title="201-lsi-15" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>16 0.52116323 <a title="201-lsi-16" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>17 0.49012956 <a title="201-lsi-17" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>18 0.47003144 <a title="201-lsi-18" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<p>19 0.428202 <a title="201-lsi-19" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>20 0.41485995 <a title="201-lsi-20" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.015), (5, 0.043), (11, 0.026), (17, 0.047), (26, 0.021), (31, 0.016), (37, 0.051), (38, 0.256), (39, 0.051), (41, 0.049), (55, 0.028), (59, 0.052), (72, 0.04), (91, 0.024), (96, 0.182)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78891492 <a title="201-lda-1" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>2 0.74402809 <a title="201-lda-2" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>Author: Dingcheng Li ; Tim Miller ; William Schuler</p><p>Abstract: and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</p><p>3 0.71541548 <a title="201-lda-3" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>Author: Shane Bergsma ; David Yarowsky ; Kenneth Church</p><p>Abstract: Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via achieves data and pervised tations. co-training. The co-trained classifier close to 96% accuracy on Treebank makes 20% fewer errors than a susystem trained with Treebank anno-</p><p>4 0.63966131 <a title="201-lda-4" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>5 0.63886112 <a title="201-lda-5" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>6 0.63598806 <a title="201-lda-6" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>7 0.63478076 <a title="201-lda-7" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>8 0.63443136 <a title="201-lda-8" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>9 0.63430321 <a title="201-lda-9" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>10 0.63360143 <a title="201-lda-10" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>11 0.6331315 <a title="201-lda-11" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>12 0.6329025 <a title="201-lda-12" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>13 0.6327194 <a title="201-lda-13" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>14 0.63167512 <a title="201-lda-14" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>15 0.63165963 <a title="201-lda-15" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>16 0.6315186 <a title="201-lda-16" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>17 0.63123757 <a title="201-lda-17" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>18 0.63101155 <a title="201-lda-18" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>19 0.63063395 <a title="201-lda-19" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>20 0.63029873 <a title="201-lda-20" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
