<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>264 acl-2011-Reordering Metrics for MT</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-264" href="#">acl2011-264</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>264 acl-2011-Reordering Metrics for MT</h1>
<br/><p>Source: <a title="acl-2011-264-pdf" href="http://aclweb.org/anthology//P/P11/P11-1103.pdf">pdf</a></p><p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>Reference: <a title="acl-2011-264-reference" href="../acl2011_reference/acl-2011-Reordering_Metrics_for_MT_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. [sent-9, score-0.392]
</p><p>2 This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. [sent-10, score-0.278]
</p><p>3 We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. [sent-11, score-0.423]
</p><p>4 1 Introduction Research in machine translation has focused broadly on two main goals, improving word choice and improving word order in translation output. [sent-15, score-0.292]
</p><p>5 Current machine translation metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture the quality of word order is poor (Birch et al. [sent-16, score-0.425]
</p><p>6 This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. [sent-22, score-0.229]
</p><p>7 They both search for an alignment be-  tween the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences. [sent-25, score-0.267]
</p><p>8 Importantly, none of these metrics capture the distance by which words are out of order. [sent-27, score-0.258]
</p><p>9 Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes. [sent-28, score-0.401]
</p><p>10 This results in a simple, decomposable metric which makes it easy for researchers to pinpoint the effect of their changes. [sent-34, score-0.218]
</p><p>11 In this paper we show that the LRscore is more consistent with human judgements  ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-35, score-0.262]
</p><p>12 We also apply the LRscore during Minimum Error Rate Training (MERT) to see whether information on reordering allows the translation model to produce better reorderings. [sent-38, score-0.463]
</p><p>13 Section 2 describes the reordering and lexical metrics that are used and how they are combined. [sent-44, score-0.53]
</p><p>14 Section 3 presents the experiments on consistency with human judgements and describes how to train the language independent parameter of the LRscore. [sent-45, score-0.333]
</p><p>15 2  The LRscore  In this section we present the LRscore which measures reordering using permutation distance metrics. [sent-48, score-0.559]
</p><p>16 These reordering metrics have been demonstrated to correlate strongly with human judgements of word  order quality (Birch et al. [sent-49, score-0.802]
</p><p>17 The LRscore combines the reordering metrics with lexical metrics to provide a complete metric for evaluating machine translations. [sent-51, score-0.928]
</p><p>18 1 Reordering metrics The relative ordering of words in the source and target sentences is encoded in alignments. [sent-53, score-0.307]
</p><p>19 We can interpret alignments as permutations which allows us to apply research into metrics for ordered encodings to measuring and evaluating reorderings. [sent-54, score-0.32]
</p><p>20 We use distance metrics over permutations to evaluate reordering performance. [sent-55, score-0.665]
</p><p>21 In Figure 1 (a) represents the identity permutation, which would result from a monotone alignment, (b) represents a small reordering consisting of two words whose orders are inverted, and (c) represents a large reordering where the two halves of the sentence are inverted in the target. [sent-58, score-0.844]
</p><p>22 Three permutations: (a) monotone (b) with a small reordering and (b) with a large reordering. [sent-62, score-0.42]
</p><p>23 We choose permutation distance metrics which are sensitive to the number of words that are out of order, as humans are assumed to be sensitive to the number of words that are out of order in a sentence. [sent-77, score-0.463]
</p><p>24 The metrics are normalised so that 0 means that the permutations are completely inverted, and 1means that they are identical. [sent-79, score-0.232]
</p><p>25 All metrics are adjusted so that 100 is the best score and 0 the worst. [sent-93, score-0.244]
</p><p>26 The Hamming distance is the simplest permutation distance metric and is useful as a baseline. [sent-95, score-0.449]
</p><p>27 2 Kendall’s Tau Distance Kendall’s tau distance is the minimum number of transpositions of two adjacent symbols necessary to transform one permutation into another (Kendall, 1938). [sent-99, score-0.368]
</p><p>28 10 iofth πe(riw)i   σ(j) =  (n22− n)  Kendalls tau seems particularly appropriate for measuring word order differences as the relative ordering words is taken into account. [sent-102, score-0.28]
</p><p>29 However, most human and machine ordering differences are much closer to monotone than to inverted. [sent-103, score-0.232]
</p><p>30 This adjusted dk is also more correlated with human judgements of reordering quality (Birch et al. [sent-106, score-0.729]
</p><p>31 We use the example in Figure 1 to highlight the problem with current MT metrics, and to demonstrate how the permutation distance metrics are calculated. [sent-108, score-0.36]
</p><p>32 The metrics are calculated by comparing the permutation string with the monotone permutation. [sent-110, score-0.374]
</p><p>33 BLEU and METEOR fail to recognise that (b) represents a small reordering and (c) a large reordering and they  1029 assign a lower score to (b). [sent-112, score-0.738]
</p><p>34 Both the Hamming distance dh and the Kendall’s tau distance dk correctly assign (c) a worse score than (b). [sent-118, score-0.461]
</p><p>35 Note that for (c), the Hamming distance was not able to reward the permutation for the correct relative ordering of words within the two large blocks and gave (c) a score of 0, whereas Kendall’s tau takes relative ordering into account. [sent-119, score-0.595]
</p><p>36 The reordering component is the average difference of absolute and relative word positions which has no clear meaning. [sent-124, score-0.384]
</p><p>37 This score is not intuitive or easily decomposable and it is more similar to METEOR, with synonym and stem functionality mixed with a reordering penalty, than to our metric. [sent-125, score-0.436]
</p><p>38 2 Combined Metric The LRscore consists of a reordering distance metric which is linearly interpolated with a lexical score to form a complete machine translation evaluation metric. [sent-127, score-0.804]
</p><p>39 The metric is decomposable because the individual lexical and reordering components can be looked at individually. [sent-128, score-0.583]
</p><p>40 The following formula describes how to calculate the LRscore: LRscore = αR + (1 − α)L  (1)  The metric contains only one parameter, α, which balances the contribution of the reordering metric, R, and the lexical metric, L. [sent-129, score-0.574]
</p><p>41 R is the average permutation distance metric adjusted by the brevity penalty and it is calculated as follows:  R =Ps∈S|Sd|sBPs  (2)  Where S is a set of test sentences, ds is the reordering distance for a sentence and BP is the brevity penalty. [sent-131, score-1.019]
</p><p>42 The brevity penalty within the reordering component is necessary as the distance-based metric would provide the same score for a one word translation as it would for a longer monotone translation. [sent-136, score-0.918]
</p><p>43 The 4-gram BLEU score includes some measure of the local reordering success in the precision of the longer n-grams. [sent-140, score-0.402]
</p><p>44 BLEU is an important baseline, and improving on it by including more reordering information is an interesting result. [sent-141, score-0.34]
</p><p>45 The lexical component of the system can be any meaningful metric for a particular target language. [sent-142, score-0.232]
</p><p>46 3 Consistency with Human Judgements Automatic metrics must be validated by comparing their scores with human judgements. [sent-145, score-0.232]
</p><p>47 We train the metric parameter to optimise consistency with human preference judgements across different language pairs and then we show that the LRscore is 1030 more consistent with humans than other commonly  used metrics. [sent-146, score-0.638]
</p><p>48 In total there were 52,265 pairwise rank judgements collected. [sent-152, score-0.214]
</p><p>49 Our reordering metric relies upon word alignments that are generated between the source and the reference sentences, and the source and the translated sentences. [sent-153, score-0.625]
</p><p>50 In an ideal scenario, the translation system outputs the alignments and the reference set can be selected to have gold standard human alignments. [sent-154, score-0.288]
</p><p>51 However, the data that we use to evaluate metrics does not have any gold standard alignments and we must train automatic alignment models to generate them. [sent-155, score-0.267]
</p><p>52 The metric scores are calculated for the test set from the 2009 workshop on machine translation. [sent-162, score-0.235]
</p><p>53 METEOR has 3 parameters which have been trained for human judgements of rank (Lavie and Agarwal, 2008). [sent-168, score-0.261]
</p><p>54 We ascertained how consistent the automatic metrics were with the human judgements by calculating consistency in the following manner. [sent-182, score-0.489]
</p><p>55 We take each pairwise comparison of translation output for single sentences by a particular judge, and we recorded whether or not the metrics were consistent with the human rank. [sent-183, score-0.423]
</p><p>56 we counted cases where both the metric and the human judge agreed that one system is better than another. [sent-186, score-0.227]
</p><p>57 The average Kendall’s tau reordering distance between the test and reference sentences. [sent-194, score-0.618]
</p><p>58 Using multiple language pairs, we train the parameter according to the amount of reordering seen in each test set. [sent-197, score-0.401]
</p><p>59 They can simply calculate the amount of reordering in the test set and adjust the parameter accordingly. [sent-199, score-0.424]
</p><p>60 The amount of reordering is calculated as the Kendall’s tau distance between the source and the reference sentences as compared to dummy monotone sentences. [sent-200, score-0.792]
</p><p>61 The amount of reordering for the test sentences is reported in Table 2. [sent-201, score-0.39]
</p><p>62 GermanEnglish shows more reordering than other language pairs as it has a lower dk score of 73. [sent-202, score-0.472]
</p><p>63 The language independent parameter (θ) is adjusted by applying the reordering amount (dk) as an exponent. [sent-204, score-0.441]
</p><p>64 α represents the percentage contribution of the reordering component in 1031 the LRscore:  α =  θdk  (4)  The language independent parameter θ is trained once, over multiple language pairs. [sent-208, score-0.442]
</p><p>65 2 Results Table 3 reports the optimal consistency of the LRscore and baseline metrics with human judgements for each language pair. [sent-213, score-0.488]
</p><p>66 The LRscore variations are named as follows: LR refers to the LRscore, “H” refers to the Hamming distance and “K” to Kendall’s tau distance. [sent-214, score-0.228]
</p><p>67 This is an important result which shows that combining lexical and reordering information makes for a stronger metric than the  baseline metrics which do not have a strong reordering component. [sent-217, score-1.031]
</p><p>68 Here LR-KB4 is the best metric, which shows that metrics which are sensitive to the distance words are out of order are more appropriate for situations with a reasonable amount of reordering. [sent-220, score-0.329]
</p><p>69 MERT minimises translation errors according to some automatic evaluation metric while searching for the best parameter settings over the N-best output. [sent-223, score-0.317]
</p><p>70 the metric rewards, but will be blind to aspects of translation quality that are not directly captured by the metric. [sent-226, score-0.32]
</p><p>71 We apply the LRscore in order to improve the reordering performance of a phrase-based translation model. [sent-227, score-0.486]
</p><p>72 1 Experimental Design We hypothesise that the LRscore is a good metric for training translation models. [sent-229, score-0.301]
</p><p>73 We used the Moses translation toolkit, including a lexicalised reordering model. [sent-243, score-0.463]
</p><p>74 The parameter setting representing the % impact of the reordering component for the different versions of the LRscore metric. [sent-250, score-0.4]
</p><p>75 The reordering metrics require alignments which were created using the Berkeley word alignment package version 1. [sent-255, score-0.575]
</p><p>76 We first extracted the LRscore Kendall’s tau distance from the monotone for the Chinese-English test set and this value was 66. [sent-259, score-0.308]
</p><p>77 This is far more reordering than the other language pairs shown in Table 2. [sent-261, score-0.358]
</p><p>78 We then calculated the optimal parameter setting, using the reordering amount as a power exponent. [sent-262, score-0.428]
</p><p>79 The optimal amount of reordering for LR-HB4 is low, but the results show it still makes an important contribution. [sent-264, score-0.368]
</p><p>80 2 Human Evaluation Setup Human judgements of translation quality are necessary to determine whether humans prefer sentences from models trained with the BLEU score or with the LRscore. [sent-267, score-0.519]
</p><p>81 Workers who got less than 60% of these gold questions correct were  disqualified and their judgements discarded. [sent-281, score-0.227]
</p><p>82 Three judgements were collected from the trusted workers for each of the 120 test sentences. [sent-289, score-0.286]
</p><p>83 1 Automatic Evaluation of MERT In this experiment we demonstrate that the reordering metrics can be used as learning criterion in minimum error rate training to improve parameter estimation for machine translation. [sent-293, score-0.597]
</p><p>84 isolation, and also as part of the LRscore together with the Hamming distance and Kendall’s tau distance. [sent-306, score-0.228]
</p><p>85 The first thing we note in Table 5 is that we would expect the highest scores when training with the same metric as that used for evaluation as MERT maximises the objective function on the development data set. [sent-308, score-0.241]
</p><p>86 The  reordering component is more discerning than the BLEU score. [sent-310, score-0.367]
</p><p>87 This might make the reordering metric easier to optimise, leading to the joint best scores at test time. [sent-312, score-0.525]
</p><p>88 Although it is interesting to look at the model weights, any final conclusion on the impact of the metrics on training must depend on human evaluation of translation quality. [sent-320, score-0.348]
</p><p>89 2  Human Evaluation  We collect human preference judgements for output from systems trained using the BLEU score and the LRscore in order to determine whether training with the LRscore leads to genuine improvements in translation quality. [sent-327, score-0.529]
</p><p>90 In order to judge how reliable our judgements are we calculate the inter-annotator agreement. [sent-338, score-0.264]
</p><p>91 We expect that more substantial gains can be made in the future by using models which have more powerful reordering capabilities. [sent-346, score-0.34]
</p><p>92 A richer set of reordering features, and a model capable of longer distance reordering would better leverage metrics which reward good word orderings. [sent-347, score-0.979]
</p><p>93 5  Conclusion  We introduced the LRscore which combines a lexical and a reordering metric. [sent-348, score-0.385]
</p><p>94 The main motivation for this metric is the fact that it measures the reordering quality of MT output by using permutation distance metrics. [sent-349, score-0.783]
</p><p>95 It is a simple, decomposable metric which interpolates the reordering component with a lexical component, the BLEU score. [sent-350, score-0.633]
</p><p>96 This paper demonstrates that the LRscore metric is more con-  sistent with human preference judgements of machine translation quality than other machine translation metrics. [sent-351, score-0.766]
</p><p>97 Ultimately, the availability of a metric which reliably measures reordering performance should accelerate progress towards developing more powerful reordering models. [sent-354, score-0.865]
</p><p>98 Meteor, m-BLEU and m-TER: Evaluation metrics for highcorrelation with human rankings of machine translation output. [sent-415, score-0.354]
</p><p>99 ORANGE: a method for evaluating automatic evaluation metrics for  machine translation. [sent-424, score-0.217]
</p><p>100 Measuring machine translation quality as semantic equivalence: A metric based on entailment features. [sent-434, score-0.343]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lrscore', 0.726), ('reordering', 0.34), ('judgements', 0.195), ('bleu', 0.182), ('metrics', 0.165), ('metric', 0.161), ('tau', 0.135), ('translation', 0.123), ('meteor', 0.109), ('kendall', 0.107), ('permutation', 0.102), ('hamming', 0.098), ('distance', 0.093), ('monotone', 0.08), ('dk', 0.075), ('workers', 0.068), ('mert', 0.067), ('ordering', 0.067), ('permutations', 0.067), ('consistency', 0.062), ('brevity', 0.057), ('decomposable', 0.057), ('reference', 0.05), ('penalty', 0.049), ('ter', 0.046), ('human', 0.043), ('adjusted', 0.04), ('humans', 0.04), ('alignments', 0.04), ('score', 0.039), ('objective', 0.039), ('preference', 0.039), ('quality', 0.036), ('preferred', 0.036), ('mechanical', 0.036), ('birch', 0.033), ('parameter', 0.033), ('gold', 0.032), ('hillclimbing', 0.032), ('kittur', 0.032), ('preferring', 0.032), ('unmatched', 0.032), ('aligned', 0.031), ('cer', 0.031), ('alignment', 0.03), ('mt', 0.03), ('evaluating', 0.029), ('null', 0.028), ('amount', 0.028), ('component', 0.027), ('output', 0.027), ('inverted', 0.027), ('calculated', 0.027), ('dh', 0.026), ('translations', 0.025), ('lexical', 0.025), ('judgement', 0.025), ('balances', 0.025), ('consistent', 0.024), ('measures', 0.024), ('scores', 0.024), ('reports', 0.023), ('worker', 0.023), ('interpolates', 0.023), ('subtracting', 0.023), ('trusted', 0.023), ('optimise', 0.023), ('calculate', 0.023), ('turk', 0.023), ('trained', 0.023), ('machine', 0.023), ('longer', 0.023), ('order', 0.023), ('lavie', 0.023), ('judge', 0.023), ('block', 0.022), ('sentences', 0.022), ('blocks', 0.022), ('prefer', 0.022), ('miles', 0.02), ('combines', 0.02), ('matching', 0.02), ('sensitive', 0.02), ('bp', 0.02), ('minimum', 0.019), ('moses', 0.019), ('measuring', 0.019), ('represents', 0.019), ('banerjee', 0.019), ('pairwise', 0.019), ('target', 0.019), ('differences', 0.019), ('necessary', 0.019), ('bulletin', 0.019), ('reward', 0.018), ('pairs', 0.018), ('whereas', 0.018), ('relative', 0.017), ('training', 0.017), ('source', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="264-tfidf-1" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>2 0.23815753 <a title="264-tfidf-2" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Ting Liu ; Sheng Li</p><p>Abstract: This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1</p><p>3 0.17171964 <a title="264-tfidf-3" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>Author: Nadir Durrani ; Helmut Schmid ; Alexander Fraser</p><p>Abstract: We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</p><p>4 0.16603106 <a title="264-tfidf-4" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>Author: Rafael E. Banchs ; Haizhou Li</p><p>Abstract: This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions. 1</p><p>5 0.15769219 <a title="264-tfidf-5" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>6 0.15697135 <a title="264-tfidf-6" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>7 0.15206458 <a title="264-tfidf-7" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>8 0.1445553 <a title="264-tfidf-8" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>9 0.13963225 <a title="264-tfidf-9" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>10 0.13113862 <a title="264-tfidf-10" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>11 0.12591407 <a title="264-tfidf-11" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>12 0.12248671 <a title="264-tfidf-12" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>13 0.10751865 <a title="264-tfidf-13" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>14 0.10690181 <a title="264-tfidf-14" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>15 0.10270482 <a title="264-tfidf-15" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>16 0.10249069 <a title="264-tfidf-16" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>17 0.09804792 <a title="264-tfidf-17" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>18 0.093490772 <a title="264-tfidf-18" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>19 0.088302016 <a title="264-tfidf-19" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>20 0.079741009 <a title="264-tfidf-20" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, -0.154), (2, 0.109), (3, 0.146), (4, 0.035), (5, 0.045), (6, 0.019), (7, -0.0), (8, 0.049), (9, -0.001), (10, 0.015), (11, -0.103), (12, -0.019), (13, -0.178), (14, -0.091), (15, 0.046), (16, -0.057), (17, 0.035), (18, -0.103), (19, -0.019), (20, -0.017), (21, 0.056), (22, -0.039), (23, -0.087), (24, -0.062), (25, 0.05), (26, 0.058), (27, -0.014), (28, 0.043), (29, 0.093), (30, -0.01), (31, -0.019), (32, 0.117), (33, 0.066), (34, 0.05), (35, -0.08), (36, -0.025), (37, 0.053), (38, 0.012), (39, -0.001), (40, 0.161), (41, -0.117), (42, -0.039), (43, 0.074), (44, 0.027), (45, -0.006), (46, -0.033), (47, 0.093), (48, 0.042), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93461639 <a title="264-lsi-1" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>2 0.82801741 <a title="264-lsi-2" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Ting Liu ; Sheng Li</p><p>Abstract: This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1</p><p>3 0.77626216 <a title="264-lsi-3" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.</p><p>4 0.73457789 <a title="264-lsi-4" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>Author: Susan Howlett ; Mark Dras</p><p>Abstract: There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT. An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results. Speculations as to cause have suggested the parser, the data, or other factors. We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?</p><p>5 0.70851678 <a title="264-lsi-5" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>Author: Nadir Durrani ; Helmut Schmid ; Alexander Fraser</p><p>Abstract: We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</p><p>6 0.67940378 <a title="264-lsi-6" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>7 0.65392417 <a title="264-lsi-7" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>8 0.64353192 <a title="264-lsi-8" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>9 0.60142595 <a title="264-lsi-9" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>10 0.58603907 <a title="264-lsi-10" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>11 0.57607132 <a title="264-lsi-11" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>12 0.55798376 <a title="264-lsi-12" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>13 0.55781776 <a title="264-lsi-13" href="./acl-2011-MEANT%3A_An_inexpensive%2C_high-accuracy%2C_semi-automatic_metric_for_evaluating_translation_utility_based_on_semantic_roles.html">216 acl-2011-MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</a></p>
<p>14 0.53775746 <a title="264-lsi-14" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>15 0.51271355 <a title="264-lsi-15" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>16 0.5124321 <a title="264-lsi-16" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>17 0.51088566 <a title="264-lsi-17" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>18 0.5007152 <a title="264-lsi-18" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>19 0.49720445 <a title="264-lsi-19" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>20 0.48095816 <a title="264-lsi-20" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.028), (17, 0.054), (26, 0.018), (31, 0.015), (37, 0.073), (39, 0.044), (41, 0.047), (55, 0.036), (59, 0.04), (72, 0.048), (88, 0.22), (91, 0.026), (96, 0.252)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95092803 <a title="264-lda-1" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>Author: Marjorie Freedman ; Alex Baron ; Vasin Punyakanok ; Ralph Weischedel</p><p>Abstract: For 20 years, information extraction has focused on facts expressed in text. In contrast, this paper is a snapshot of research in progress on inferring properties and relationships among participants in dialogs, even though these properties/relationships need not be expressed as facts. For instance, can a machine detect that someone is attempting to persuade another to action or to change beliefs or is asserting their credibility? We report results on both English and Arabic discussion forums. 1</p><p>same-paper 2 0.86974823 <a title="264-lda-2" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>3 0.82692409 <a title="264-lda-3" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>Author: Ivan Titov</p><p>Abstract: We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effec- tive on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks.</p><p>4 0.8079983 <a title="264-lda-4" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>Author: Chung-chi Huang ; Mei-hua Chen ; Shih-ting Huang ; Jason S. Chang</p><p>Abstract: We introduce a new method for learning to detect grammatical errors in learner’s writing and provide suggestions. The method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words, function words, and parts-of-speech (e.g., “play ~ role in Ving” and “look forward to Ving”). At runtime, the given passage submitted by the learner is matched using an extended Levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions. We present a prototype implementation of the proposed method, EdIt, that can handle a broad range of errors. Promising results are illustrated with three common types of errors in nonnative writing. 1</p><p>5 0.78438711 <a title="264-lda-5" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>6 0.78253162 <a title="264-lda-6" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>7 0.78129637 <a title="264-lda-7" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>8 0.78049695 <a title="264-lda-8" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>9 0.78044021 <a title="264-lda-9" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>10 0.78039682 <a title="264-lda-10" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>11 0.78031337 <a title="264-lda-11" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>12 0.78019339 <a title="264-lda-12" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>13 0.77996969 <a title="264-lda-13" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>14 0.7794919 <a title="264-lda-14" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>15 0.77929491 <a title="264-lda-15" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>16 0.77888632 <a title="264-lda-16" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>17 0.77838427 <a title="264-lda-17" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>18 0.77805322 <a title="264-lda-18" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>19 0.77767366 <a title="264-lda-19" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>20 0.77746189 <a title="264-lda-20" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
